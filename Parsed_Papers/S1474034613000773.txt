@&#MAIN-TITLE@&#Classification of major construction materials in construction environments using ensemble classifiers

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           This study focused on the automatic detection of construction materials in images.


                        
                        
                           
                           We investigated the performance of six single classifiers and ensemble classifiers.


                        
                        
                           
                           We tested the classifiers on three data sets: one each for concrete, steel, and wood.


                        
                        
                           
                           The ensemble classifiers performed better than the single classifiers overall.


                        
                        
                           
                           The ensemble classifier is able to enhance the detection of construction materials.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Ensemble classifier

Color

Construction material detection

Data mining techniques

Image processing

@&#ABSTRACT@&#


               
               
                  The automatic detection of construction materials in images acquired on a construction site has been regarded as a critical topic. Recently, several data mining techniques have been used as a way to solve the problem of detecting construction materials. These studies have applied single classifiers to detect construction materials—and distinguish them from the background—by using color as a feature. Recent studies suggest that combining multiple classifiers (into what is called a heterogeneous ensemble classifier) would show better performance than using a single classifier. However, the performance of ensemble classifiers in construction material detection is not fully understood. In this study, we investigated the performance of six single classifiers and potential ensemble classifiers on three data sets: one each for concrete, steel, and wood. A heterogeneous voting-based ensemble classifier was created by selecting base classifiers which are diverse and accurate; their prediction probabilities for each target class were averaged to yield a final decision for that class. In comparison with the single classifiers, the ensemble classifiers performed better in the three data sets overall. This suggests that it is better to use an ensemble classifier to enhance the detection of construction materials in images acquired on a construction site.
               
            

@&#INTRODUCTION@&#

The automatic detection of construction materials in images acquired on a construction site is essential for a wide range of construction applications, from the generation of a 3D as-built model to progress monitoring (see, for example, [1–5]). With the rapid deployment of image sensors on construction sites, images containing valuable project information are readily available. However, material detection in construction images is non-trivial and difficult. Construction materials in construction images may appear cluttered, occluded, or articulated, and the shapes and positions of construction materials are unpredictable.

Color has been recognized as an efficient feature for distinction of a material of interest from the background. Color has obvious advantages over other features such as texture and shape, especially in complex environments, as it is independent of the shapes and positions of the objects [6–8]. In addition, it is simple and computationally efficient to implement because it requires only the color values of each pixel in an image. It is therefore expected that material detection using color would be more robust and accurate and would overcome problems associated with construction environments compared to other features such as texture and shape.

In past research, color distribution has been used in efforts to detect construction materials in images acquired on a construction site. Neto et al. [9] proposed a method that employs edge detection and color to identify construction materials in images. In their approach, an edge detector algorithm detects edge pixels that belong to construction materials by comparing the RGB values in such pixels with the predetermined RGB values of the construction materials. After the edges have been detected, it groups the interior pixels to each set of edge pixels as an object. At the end of the operation, both of the resulting linked lists (the one for edges and the one for internal pixels) are stored under an object name. Zou and Kim [10] suggested a color-based method for identification of hydraulic excavators on a construction site. They use the hue feature to separate hydraulic excavators of different colors, and the saturation feature to differentiate each excavator of interest from its background, which consists of dark-colored soil and white snow. Simple thresholding methods using the hue and saturation features in conjunction with a method of calculating object centroid coordinates enable their system to produce accurate determinations of excavator idle time and working rate. The changing centroid coordinates of an excavator in successive images taken at constant time intervals are used as indicators of movement. Son and Kim [11] proposed an automated structural component recognition method that employs color and 3D data acquired from a stereo vision system for use in construction progress monitoring. The data processing first relies on color features to effectively extract information on structural components by employing color invariance, 2D object segmentation, and two-stage post-processing of removing unnecessary noise unrelated to the structure of interest and supplementing the image with information that may have been unintentionally eliminated. That information is then utilized to extract 3D coordinates for each color feature. The color image is used to guide the detection of features, while the 3D data are used to compensate for the pose of the feature.

In recent years, data mining methods such as artificial neural networks (ANNs), Gaussian mixture models (GMMs), and support vector machines (SVMs) have been investigated as a way to detect construction materials in images by use of a color model. Zhu and Brilakis [12] applied ANNs to classify regions of concrete in images acquired on a construction site. The images were first divided into regions through image segmentation using color. Then the color and texture features of each region were calculated, and the regions were classified using a pre-trained ANNs classifier. Son et al. [5] performed a comparative analysis of three data mining algorithms (GMMs, ANNs, and SVMs) for detection of concrete regions in images acquired on construction sites. The results show that the accuracy of the SVM they employed is better than that of the GMM or the ANN in dealing with concrete detection.

In previous studies, single classifiers have been employed to detect construction materials—and distinguish them from the background—by using color as a feature. However, a single classifier might not produce the optimal result in construction environments in which the color of one construction material is similar to that of others around it, or the inherent color property of a construction material is altered because of the effects of variation in illumination. For these reasons, the detection of construction materials still remains a challenging problem and there is still room for further improvement of detection performance. In order to solve such complex classification problems, heterogeneous ensemble classifiers (combined into a so-called multiple classifier system) have been proposed and they have been shown to be considerably successful in highly complex domains compared to ones with individual classifiers [13–17].

An ensemble classifier is comprised of a set of individual classifiers whose predictions are combined to obtain a highly accurate classification. Systems of this type have been proposed as a way to achieve better classification performance than with a single classifier [15,17] and are expected to reduce the variance in the estimation errors made by the individual classifiers [18,19]. The effectiveness of ensemble classifiers for detection of materials in complex environments has been demonstrated in various fields (see, for example, [20–24]). To the knowledge of the authors, the applicability of ensemble classifiers to the detection of construction materials has not been explored thus far.

The aim of this study was to improve the accuracy of detection of major construction materials such as concrete, steel, and wood by using ensemble classifiers. It was hypothesized that ensemble classifiers achieve higher accuracy than single classifiers in detecting construction materials in construction environments. This belief is based on the general expectation that ensemble classifiers can outperform individual classifiers [17]. The rest of the paper is organized as follows. Section 2 describes data collection and pre-processing. Section 3 describes the methods employed by the single classifiers and the scheme employed in the proposed ensemble classifier. In Section 4, the results of experiments on the performance of the proposed ensemble classifier are compared with that of single classifiers in terms of average prediction accuracy. Finally, the conclusions are presented in Section 5.

Without a comprehensive data set, it cannot be concluded that an ensemble classifier yields better accuracy than single classifiers in detecting construction materials in construction environment. Because comprehensive data sets for construction material detection were not readily available, a total of three data sets (one each for concrete, steel, and wood) were generated.

The appearance of a construction material’s surface colors can be affected by environmental factors such as changes in the direction and intensity of illumination. Since most construction sites are outdoors, the intensity of illumination varies unpredictably and uncontrollably, depending on the time of day, seasonal variations, and weather conditions (sunny, cloudy, or foggy), thereby resulting in large variations in the appearance of a construction material’s surface colors. To account for such variations, 108 photographs were taken at a total of 50 construction sites for concrete detection, 91 photographs were taken at a total of 80 construction sites for steel detection, and 50 photographs were taken at a total of 14 construction sites for wood detection. Fig. 1
                        (a), (c), and (e) present examples of construction site images for concrete, steel, and wood detection. Digital cameras with resolutions ranging from 3 megapixels to 12 megapixels were used when collecting data. The photographs were intended to contain either of the three structural components (concrete, steel, and wood) in images of actual construction-site scenes in order to validate the effectiveness and robustness of the proposed method for use in applications such as the generation of 3D as-built models and progress monitoring. Therefore, the photographs were taken at a distance from structures so that images contained the entire structures.

Each photograph was then divided into sub-regions of either 25×25 pixels or 50×50 pixels. Each sub-region was categorized and labeled as either a material of interest or the background, or as unable to say whether the sub-region was the material of interest or the background. Fig. 1(b), (d), and (f) display examples of the sub-regions for concrete, steel, and wood data sets. For example, the first, second, and third rows in Fig. 1(b) show the examples of sub-regions that were labeled as the concrete, the background, and unable to say whether the sub-region was the concrete or the background. Of the sub-regions, 48.2% from the concrete data set, 22.8% from the steel data set, and 49.0% from the wood data set were categorized as unable to say whether the sub-region was the material of interest or the background. These sub-regions were then excluded from the data set. As a result, every data set consisted of material or non-material pixels. The former are pixels associated with objects made of materials such as concrete, steel, and wood, while the latter are pixels related to the background. To assess whether the heterogeneous ensemble classifiers perform better than single classifiers, this study made a particular effort to collect and include as many materials as possible with color properties similar to those of the materials of interest. In each data set, the background included all kinds of scenery—bricks, construction equipment, fences, forms, pipes, safety nets, the sky, soil, traffic signs, trees, windows, and other construction-related materials. For example, the background of the concrete data set included objects made of materials such as steel and wood in order to evaluate results in the presence of different construction materials.

In total, the data collected from the concrete, steel, and wood sub-regions and their background sub-regions amounted to over 113million pixels for concrete detection, 95million pixels for steel detection, and 35million pixels for wood detection. The first data set contained approximately 44million pixels of concrete and 69million pixels of background. The second data set consisted of 9million pixels of steel and approximately 85million pixels of background. The third data set had 10million pixels of wood and approximately 25million pixels of background. The characteristics of the three data sets are provided in Table 1
                        . In summary, the data sets were well balanced in terms of time of day, season, and weather. The percentages of data collected during a.m. and p.m. hours were roughly comparable. There was greater variation in the percentages of data collected in different seasons, as well as in the percentages of data collected and under different weather conditions, as shown in Table 1.

One issue that must be carefully considered in order to take full advantage of color-based material detection is how to effectively reduce the effect that variations in illumination which occur in outdoor environments will have on material color. The values of colors in the RGB (red, green, and blue) color space, the most prevalent choice for computer graphics, are particularly subject to deterioration as a result of changes in illumination [25]. Such variations caused by factors in outdoor environments may dramatically affect color properties, potentially impacting detection performance [26]. To deal with these potential artifacts, it is important to represent color in a way that minimizes the effect of variations in illumination. In this study, in order to represent color in an appropriate form, the color values in the RGB color space were converted to the HSI color space, which separates chrominance and luminance information. The HSI color space consists of three components: hue (H), saturation (S), and intensity (I). Hue and saturation are related to color, or chromaticity, and are illumination-independent components. These three components were used as input features to the classifiers. The HSI color space is defined as follows [27]:
                           
                              (1)
                              
                                 H
                                 =
                                 arctan
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      3
                                                   
                                                
                                                (
                                                G
                                                -
                                                B
                                                )
                                             
                                             
                                                (
                                                R
                                                -
                                                G
                                                )
                                                +
                                                (
                                                R
                                                -
                                                B
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (2)
                              
                                 S
                                 =
                                 1
                                 -
                                 
                                    
                                       
                                          min
                                       
                                       (
                                       R
                                       ,
                                       G
                                       ,
                                       B
                                       )
                                    
                                    
                                       I
                                    
                                 
                              
                           
                        
                        
                           
                              (3)
                              
                                 I
                                 =
                                 
                                    
                                       (
                                       R
                                       +
                                       G
                                       +
                                       B
                                       )
                                    
                                    
                                       3
                                    
                                 
                              
                           
                        
                     

We considered six different types of classifiers: a support vector machine (SVM), an artificial neural network (ANN), commercial version 4.5 (C4.5), naïve Bayes (NB), logistic regression (LR), and k-nearest neighbors (KNN). In all of our experiments, we employed the algorithms from WEKA release 3.6.7 [28], which is a Java-based machine learning tool. Default parameter values in WEKA release 3.6.7 were used in these six classifiers. What follows is a brief description of these six classifiers.

The SVM, inspired by statistical-learning theory, is one of the most powerful machine-learning techniques for solving a large number of complex binary classification problems [29]. It acts as a linear classifier in a high-dimensional feature space transformed through a projection from the original input feature space by using non-linear kernel functions of the original data set [30]. The high-dimensional space could theoretically be infinite, in which case linear discrimination is almost possible. In general, however, the resulting classifier is non-linear in the input space. An SVM learns the data in the high-dimensional feature space and achieves good generalization performance by finding a hyperplane that maximizes the margin between the classes with the help of multiplier parameters, for example, Lagrange multipliers [31]. For further details, see Vapnik [32] and Vladimir and Vapnik [33]. In this study, the radial basis function (RBF) was selected as the kernel function. With the RBF kernel function, the parameters C and γ were set to 1 and 0, respectively.

In this study, a back-propagation (BP) neural network is used for classification. This is a feed-forward network that can have one or more hidden layers. It utilizes an iterative gradient-search technique designed to minimize the mean-square error between the actual and desired net outputs. The units in the hidden layers sum their inputs, add a constant, and take a fixed function of the result [34]. The output units are of the same form but with an output function. A three-layer network with one hidden layer was proven to be capable of computing any continuous likelihood function required in a classifier and of solving complex binary classification problems [35,36]. In this study, a three-layer network with one hidden layer was used for experimentation. It has been shown that a multiple hidden-layer neural network can approximate any function [37]. The number of nodes for the hidden layer was set to 3. The sigmoid transfer function was selected as the activation function. The learning rate, which determines the amount of weights that are updated, was set to 0.3; the momentum that was applied to the weights during updating was set to 0.2; and the training time, which indicates the number of epochs through which to train, was set to 500.

C4.5 is a supervised-learning algorithm that is the advanced version of the decision tree algorithm ID3 [38]. It is used to generate a set of rules from the data. As such, it employs a divide-and-conquer approach [39,40]. C4.5 works in three main steps. First, the node at the top of the tree (the “root node”) considers all samples and passes information about them to the “branch nodes” (the nodes that have at least one node of the tree below them). The branch nodes generate rules for a group of samples based on an entropy measure. At this stage of the process, C4.5 constructs a tree by considering all attribute values and finalizes the decision rule by pruning. It uses a heuristic approach for pruning based on the statistical significance of splits. After fixing the best rule, the branch nodes send the final target value to the lowest node on each branch, which is called a “leaf node” [38,41]. In this study, a confidence factor, which is used for pruning, was set to 0.25, and the minimum number of instances per leaf node was set to 2.

The NB classifier is a simple linear classifier that is based on the classical statistical “Bayes theorem.” The term ‘‘naïve’’ refers to the fact that it calculates the maximum posterior probability based on the assumption that the attributes of the training samples are independent and that there are no hidden or latent attributes which influence the prediction procedures [42]. This classifier is based on theoretically well-founded mathematical models used to predict an unseen case given a training sample [43]. The NB classifier will assign the most likely class to a given example as described by its feature vector. It assumes that the decision problem is posed in probabilistic terms and that all of the relevant probability values are known. This type of classifier is simple to implement, as neither numerical optimization nor matrix algebra is required for its use. NB is also efficient to train and use and is easy to update with new data.

The LR is a generalization of linear regression [44]. It is used primarily to predict binary or multi-class dependent variables. Because the response variable is discrete, it cannot be modeled directly by linear regression. Therefore, rather than predicting a point estimate of the event itself, it builds a model to predict the odds of its occurrence. In a two-class problem, odds greater than 50% would mean that the case is assigned to the class designated as 1; otherwise, it is assigned to the class designated as 0. Although logistic regression is a powerful modeling tool, it assumes that the response variable (which is the logarithm of the odds, not the odds per se) is linear in the coefficients of the predictor variables. Furthermore, the modeler, based on his or her experience with the data and data analysis, must choose the right inputs and specify their functional relationship to the response variable.

KNN is an instance-based learning approach. It finds the k-nearest neighbors and uses a majority vote to determine the class label [45]. The basic principle of this algorithm is that each as-yet-unseen instance is always compared with existing instances using a distance metric. Based on the distance metric, the closest existing instance is used to assign the class for the test sample [28]. The most straightforward approach is to assign the majority class among the nearest neighbors to the query [46]. The advantage of the KNN is its robustness when using noisy training data sets [41,47]. The major drawback of the KNN algorithm is the high computational cost, as it has to compute the distance of every query instance for all the training data. In this study, Euclidean distance was used as the distance metric, and k, which indicates the number of neighbors to use, was set to 1.

In this study, an ensemble classifier used a voting scheme which internally makes use of several single (base) classifiers and combines them in an effort to produce better results. Next, a determination was made of the single classifiers to choose for incorporation into a proposed ensemble classifier. The motivation for using a voting-based ensemble classifier was to take advantage of information outputs generated by a number of different base classifiers, instead of relying on the output of a single classifier alone [48]. The output generated by the ensemble classifier was defined as the average of the prediction probabilities generated by the base classifiers. This was done because there is a consensus in the literature that the classification performance of this type of heterogeneous ensemble classifier is better than that of even the best-performing individual classifier, and that biased decisions can be avoided more easily as a result [49,50].

When we design heterogeneous ensemble classifiers, we must select the subset of classifiers that can be combined to achieve better accuracy [51]. It is easy to see that such an optimal subset could be obtained by exhaustive enumeration, that is, by assessing on a validation set the classification accuracy provided by all possible subsets, and then choosing the subset exhibiting the best performance. In our case, there are six single classifiers; thus, the number of possible subsets is equal to 57. If the number of classifiers increases, the number of possible subsets also increases. Unfortunately, the evaluation of all possible combinations of base classifiers is impractical and requires high computational cost. Therefore, different strategies have been proposed in order to limit the computational complexity of the selection process [51]. Accordingly, measures for evaluating the base classifiers forming an ensemble classifier have been proposed for classifier selection purposes [52].

Generally speaking, to have an effective ensemble classifier that is more accurate than any of its base classifiers, one needs base classifiers which are diverse (in the sense that they predict differently) but accurate [23,53–56]. In other words, heterogeneous ensemble classifiers are more effective when the base classifiers are diverse and have acceptable individual performances, and consequently are more likely to have a more robust generalization performance in terms of the principle of bias–variance tradeoff [57,58]. If individual base classifiers are rather inaccurate, their combination is meaningless and might not produce an accurate ensemble classifier. On the other hand, if there is no diversity among the classifiers, (that is, if the base classifiers all produce essentially the same outputs), then their combination will not significantly contribute to improved performance but will serve mainly to increase the complexity of the process.

It is rather difficult to achieve both high diversity and accuracy in base classifiers, since high diversity means substantial disagreement among the outputs of the base classifiers, and high accuracy indicates significant agreement among the outputs of the base classifiers [54,56,59]. The two key ingredients to achieve an effective ensemble classifier are thus in contradiction with each other. Therefore, it is crucial to select base classifiers with a good tradeoff between the two characteristics. In this study, the weighted count of errors and correct results (WCEC) was employed for this purpose. The WCEC is one of the commonly employed measures of diversity of errors.

The weighted count of errors and correct results (WCEC) is one of a number of measures of diversity of errors that are in use in assessing the choice of base classifiers to be used in an ensemble classifier [52]. The measures of diversity of errors emphasize that differences within the errors made by the member classifiers truly affect performance [60,52]. In the previous literature (see, for example, [52,61]), the WCEC measure was proven to be effective in assessing the choice of base classifiers to be used in an ensemble classifier. The WCEC measure not only gives a measure of the correctness of classifiers but also considers the diversity between two classifiers [61]. By using the WCEC, we were able to select base classifiers that are both diverse as well as accurate from among a number of competing classifiers.

The main concept of the WCEC is that adopting an incorrect classifier generally has a negative effect on the ensemble classifier, whereas adopting the correct classifier is beneficial. The agreements generally have a significantly greater positive impact on the ensemble classifier’s performance than the disagreements. Based on this concept, the resulting ensemble classifier often yields more accurate classification results than the single classifiers, because it aggregates the benefits of multiple classifiers [62].

The WCEC takes information on both incorrect results and correct results into account, with more emphasis placed on cases where the classifiers agree on the result, regardless of whether that result is correct or incorrect. One can subdivide the occurrences of the different combinations of outputs of the base classifiers as follows [52]:
                              
                                 (4)
                                 
                                    
                                       
                                          WCEC
                                       
                                       
                                          a
                                          ,
                                          b
                                       
                                    
                                    =
                                    
                                       
                                          N
                                       
                                       
                                          11
                                       
                                    
                                    +
                                    
                                       
                                          1
                                       
                                       
                                          2
                                       
                                    
                                    (
                                    
                                       
                                          N
                                       
                                       
                                          01
                                       
                                    
                                    +
                                    
                                       
                                          N
                                       
                                       
                                          10
                                       
                                    
                                    )
                                    -
                                    
                                       
                                          N
                                       
                                       
                                          different
                                       
                                       
                                          00
                                       
                                    
                                    -
                                    5
                                    
                                       
                                          N
                                       
                                       
                                          same
                                       
                                       
                                          00
                                       
                                    
                                    ,
                                 
                              
                           where N is the total number of instances, N
                           11 is the number of instances in which both classifiers a and b are correct, N
                           10 (resp. N
                           01) is the number of instances in which just the first (resp. second) classifier is correct, and N
                           00 is the number of instances in which both classifiers a and b are incorrect. Furthermore, 
                              
                                 
                                    
                                       N
                                    
                                    
                                       same
                                    
                                    
                                       00
                                    
                                 
                              
                            is the number of instances in which both classifiers a and b are incorrect and have the same output, and 
                              
                                 
                                    
                                       N
                                    
                                    
                                       different
                                    
                                    
                                       00
                                    
                                 
                              
                            is the number of instances in which both classifiers a and b are incorrect but have different outputs. This study deals with a binary classification problem; thus, the value of 
                              
                                 
                                    
                                       N
                                    
                                    
                                       different
                                    
                                    
                                       00
                                    
                                 
                              
                            is zero. Thus N
                           =
                           N
                           11
                           +
                           N
                           10
                           +
                           N
                           01
                           +
                           N
                           00 and 
                              
                                 
                                    
                                       N
                                    
                                    
                                       00
                                    
                                 
                                 =
                                 
                                    
                                       N
                                    
                                    
                                       same
                                    
                                    
                                       00
                                    
                                 
                                 +
                                 
                                    
                                       N
                                    
                                    
                                       different
                                    
                                    
                                       00
                                    
                                 
                              
                           . According to Aksela and Laaksonen [52], who proposed the WCEC measure, the weighting is arbitrary, and the presented values were chosen based on their suitability to penalize errors, and especially the identical errors. This study adopted the weight values, as suggested by Aksela and Laaksonen [52].

Using WCEC as the measure of diversity of errors in order to devise a diverse ensemble classifier, we trained six classifiers (SVM, ANN, C4.5, NB, LR, and KNN) on the three different data sets. Then the WCEC was computed for every combination of two to six of the classifiers for each of the three data sets. For each combination of just two classifiers, the pairwise values were used as inputs to the WCEC. For combinations of three or more classifiers, the mean of all the pairwise values of the classifiers were used as inputs to the WCEC. Then the combination of classifiers that had the maximum value of the WCEC was chosen for the ensemble classifier.

The use of an ensemble classifier based on the voting scheme starts with training of multiple base classifiers (so-called member classifiers). The information from multiple base classifiers is then combined to produce a final decision. Here, the information from the base classifiers included their predictions (binary decisions) for each instance and their prediction probabilities for each target class. In the case of the concrete data set, for example, the information from the base classifiers included a prediction probability that an object is made of concrete a prediction probability that it is not made of concrete, and a decision on whether it is or is not made of concrete. The prediction probabilities convey not only prediction information but also confidence information [63]. Once the base classifiers were selected, based on the measure of the weighted count of errors and correct results, they were combined to produce a final decision (prediction) for each instance and prediction probabilities for each target class.

The average of the prediction probabilities for the different base classifiers was used as the prediction probability for the overall classification result. Use of the average of prediction probabilities, rather than the average of binary decisions, is known to have improved the performance of some ensemble classifiers [64]. In addition, it is a simple, low-cost, but effective approach that has been shown to be quite successful in various classification problems [65–67].

Let D
                           ={D
                           1,…,
                           DL
                           } denote the ensemble classifier, where L is the number of base classifiers of which it is composed. Let Ω={ω
                           1,
                           ω
                           2} be the set of class labels. The individual outputs are estimates of the posterior probabilities; that is, the output di
                           
                           ,
                           
                              j
                           (x) of classifier Di
                           , in support of the hypothesis that an instance x comes from class ωj
                           , is an estimate of P(ωj
                           |x), i
                           =1,…,
                           L; j
                           =1,2 [60]. In this study, the conditional probability μj
                           (x) that x is in class j was defined as the average of the posterior probabilities output by the base classifiers:
                              
                                 (5)
                                 
                                    
                                       
                                          μ
                                       
                                       
                                          j
                                       
                                    
                                    (
                                    x
                                    )
                                    =
                                    
                                       
                                          1
                                       
                                       
                                          L
                                       
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             L
                                          
                                       
                                    
                                    
                                       
                                          d
                                       
                                       
                                          i
                                          ,
                                          j
                                       
                                    
                                    (
                                    x
                                    )
                                 
                              
                           Then instance x was assigned to the class with the larger of the values of μ
                           1(x) and μ
                           2(x).

@&#EVALUATION@&#

Various approaches have been suggested for evaluating the performance of classifiers. Six performance evaluation measures were employed in this study: accuracy, precision, sensitivity, specificity, area under the receiver operating characteristic curve (AUC), and overall average performance score (S). The first five measures have seen extensive use in evaluation of the performance of classification models [68–70]. They can be calculated by computing four quantities: the number of correctly predicted pixels of the material of interest (true positives, TP), the number of correctly predicted pixels that belong to the background (true negatives, TN), the number of pixels that were incorrectly assigned to the material of interest (false positives, FP), and the number of pixels of the material of interest that were incorrectly assigned to the background (false negatives, FN) [71]. The fifth measure (AUC) indicates the area under the receiving operating characteristic (ROC) curve. It is the most common quantitative index describing the ROC curve [72–74]). In this study, AUC is calculated according to Hanley and McNeil [72]. The sixth measure (S) is calculated by averaging the values of the first five measures, which are given by the following formulas:
                           
                              (6)
                              
                                 Accuracy
                                 =
                                 
                                    
                                       
                                          
                                             
                                                TP
                                                +
                                                TN
                                             
                                             
                                                TP
                                                +
                                                TN
                                                +
                                                FP
                                                +
                                                FN
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (7)
                              
                                 Precision
                                 =
                                 
                                    
                                       
                                          
                                             
                                                TP
                                             
                                             
                                                TP
                                                +
                                                FP
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (8)
                              
                                 Sensitivity
                                 =
                                 
                                    
                                       
                                          
                                             
                                                TP
                                             
                                             
                                                TP
                                                +
                                                FN
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (9)
                              
                                 Specificity
                                 =
                                 
                                    
                                       
                                          
                                             
                                                TN
                                             
                                             
                                                FP
                                                +
                                                TN
                                             
                                          
                                       
                                    
                                 
                              
                           
                        For the evaluation of the classification performance of each of the seven classifiers (the six single classifiers and the ensemble classifier), k-fold cross validation was used. This method is known for its tendency to yield minimal bias and variance in comparison to all other validation methods, including the leave-one-out method [75]. Extensive studies on numerous data sets with different classifiers have demonstrated that 10-fold cross validation is optimal in terms of computation and estimation of error, and there is theoretical evidence backing this up [44]. Thus, 10-fold cross validation was used to assess the performance of the seven classifiers.

In 10-fold cross validation, the data set is split into 10 approximately equal “folds” (sets), with each fold in turn being used for testing and the remaining 9 folds being used for training. Thus every instance is used exactly once for testing. The cross validation estimate of the overall performance is then calculated by simply averaging the 10 individual performance evaluation measures.

For an ensemble classifier to be considered superior to its base classifiers in the detection of construction materials, it should be verified that the ensemble classifier outperforms single classifiers in distinguishing those materials from others. Table 2
                         lists the statistical results regarding the effectiveness and usefulness of our ensemble classifier for the detection of concrete, steel, and wood. The results were generated by applying 10-fold cross validation to the six single classifiers on the three data sets (one each for concrete, steel, and wood) separately. Based on the results generated by the six single classifiers, the optimal diverse ensemble classifier for each of three types of material detection turned out to be the same one comprised of the SVM, C4.5, and KNN; this determination was made by choosing the combination with the maximum value the weighted count of errors and correct results. After six single classifiers’ training via 10-fold cross validation was conducted, the selection process of base classifiers for an ensemble classifier using the value of the WCEC on the three data sets (one each for concrete, steel, and wood) took 2.57s, 2.94s, and 2.97s, respectively.

Regardless of the type of classifier, the results imply that concrete, steel, and wood detection can be achieved using color information alone, ranging from 82.28% to 96.70% in accuracy, 74.38% to 96.00% in precision, 87.68% to 98.62% in sensitivity, 66.08% to 95.98% in specificity, 89.39% to 97.24% in AUC, and 82.60% to 96.41% in S. These results strongly indicate that color information is effective and useful in construction material detection for objects made of materials such as concrete, steel, and wood. On all three data sets, the ensemble classifier demonstrated the best performance in terms of accuracy, sensitivity, AUC, and S, except in terms of sensitivity on the wood data set, where LR fared best. In addition, KNN yielded better results in terms of precision and specificity on all three data sets (see Table 2). Among single classifiers, KNN in particular seemed to perform quite well alone except in terms of sensitivity and AUC on all three data sets. Although the differences in performance between the ensemble classifier and the single classifiers were not large, the ensemble classifier performed better overall than the single classifiers, especially in terms of accuracy, sensitivity, AUC, and S. These results confirm that an ensemble classifier can effectively deal with detection of construction materials using color information alone and improve classification performance relative to that of single classifiers.

In this study, the processing time incurred in classification by using the proposed ensemble classifiers was quantified. The test time for the proposed ensemble classifiers was 38.53min, 37.44min, and 41.12min for concrete, steel, and wood detection, respectively. For applications such as the generation of 3D as-built models and progress monitoring, it is important to achieve accurate detection because it is a precedent process in such applications. Poor detection performance may negatively influence the quality of subsequent process steps. While some applications, such as automated equipment control and safety enhancement-related ones, would require less processing time, less than a second or several seconds for real-time and continuous updates. When we consider applications such as the generation of 3D as-built models and progress monitoring, a processing time of about 40min is sufficient and applicable. The results demonstrate that the proposed method is appropriate to the applications by achieving accurate detection as well as reaching an appropriate balance between accuracy and processing time.

For all three data sets, McNemar’s test was used to assess whether the ensemble classifier significantly outperformed the single classifiers. McNemar’s test is a statistical process that can evaluate the significance of the difference in the classification performance between two models [76]. As a non-parametric test for two related samples, this test is particularly useful for detecting differences in outputs [77,78]. Table 3
                         shows the results of McNemar’s test for statistical comparisons of the classification performance of the ensemble classifier to that of each of the six single classifiers on the three datasets. The numbers in parentheses are the p-values. If a p-value is less than 0.05, the difference in the classification performance between two classifiers is statistically significant [79]. As shown in Table 3, all of the p-values are 0 (to three decimal places). Therefore, according to McNemar’s test, the ensemble classifier significantly outperformed the single classifiers.

Receiver operating characteristic (ROC) curves were used to visually compare the classification performances of the six single classifiers (SVM, ANN, C4.5, NB, LR, and KNN) to that of the proposed ensemble classifier. The ROC curve is a two-dimensional graph in which the sensitivity [TP/(TP
                        +
                        FN)], is plotted on the vertical axis and the quantity 1-specificity [FP/(FP
                        +
                        TN)] is plotted on the horizontal axis. Sensitivity is the fraction of pixels of the material of interest that are correctly predicted as pixels of the material of interest, and 1–specificity is the fraction of pixels of the background that are incorrectly predicted as pixels of the material of interest. A larger area under the ROC curve means better classification performance, because a larger value of sensitivity can be achieved for each value of 1-specificity. For each of the three data sets, ROC curves for the seven classifiers are shown in Fig. 2
                        . The ROC curve for the ensemble classifier lies above the ROC curves for the single classifiers. This means that the area under the ROC curve for the ensemble classifier is larger than the areas under the ROC curves for the single classifiers. Furthermore, if one ROC curve completely encloses another ROC curve, the classifier that corresponds to the latter curve can be discarded [80]. Therefore, it has been clearly shown that better classification performance was achieved by the ensemble classifier than by the single classifiers.

This paper proposed the use of a heterogeneous ensemble classifier to improve the detection accuracy of major construction materials such as concrete, steel, and wood on construction sites. In order to take advantage of single classifiers in designing the heterogeneous ensemble classifier and produce better results than any of the single classifiers, this study employed the diversity measure by taking into account the computational efficiency and the combination rule that we have presented. This study demonstrated that the heterogeneous ensemble classifier can be more accurate than an excellent single classifier in the detection of major construction materials; herein lies the main contribution of this study.

To validate the performance of the proposed ensemble model, an experimental comparison of the ensemble model and six single classification models (SVM, ANN, C4.5, NB, LR, and KNN) was carried out. The experimental results validated our initial hypothesis that the ensemble model will achieve greater accuracy than single classifiers in detecting construction materials in a construction environment. Thus, the proposed ensemble strategy can be useful in a variety of construction applications based on the use of images acquired on a construction site.

The results obtained with ensemble classifiers thus far are encouraging and suggest that it would be worthwhile to proceed with this line of research. In this study, the proposed ensemble classifier was validated on images of objects made of materials such as concrete, steel, or wood. In future work, the performance of the proposed ensemble classifier on images of objects made of other construction materials or composite materials made from two or more different types of materials will be investigated. Moreover, recent studies have shown that images acquired on a construction site can be used to detect defects in construction materials (see, for example, [1–5]). Further developments in the use of ensemble classifiers will be directed toward defect detection, such as crack detection in concrete and rust detection in steel. In addition, use of other types of ensemble classifiers, such as bagging classifiers and boosting classifiers, and ensemble classifiers comprised of different combinations of base classifiers other than those employed in this study, will be investigated in future studies.

@&#ACKNOWLEDGEMENTS@&#

This research was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education, Science and Technology (2010-0023229).

@&#REFERENCES@&#

