@&#MAIN-TITLE@&#Efficient online and offline template update mechanisms for speaker recognition

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Created a new database for speaker template updating.


                        
                        
                           
                           MFCC super template for speaker recognition is proposed.


                        
                        
                           
                           Proposed an online and offline MFCC feature and GMM based model update.


                        
                        
                           
                           Secondary template for speaker template (model) update is also suggested.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Template update

Mel frequency Cepstral coefficient

MFCC super template

GMM super model

Online update

Offline update

@&#ABSTRACT@&#


               
               
                  Sample variations are one of the main problems associated with speaker recognition. Most approaches use multiple templates in the gallery database. But, this requires enormous memory space. In order to minimize classification errors and intra-class variations, adaptive online and offline template update methods using vector quantization (VQ) and Gaussian mixture model (GMM) are proposed. Online and offline feature update as well as model update techniques are considered here. Feature update utilizes the vector quantization approach, while Gaussian mixture model approach is considered for model updating. The proposed methods automatically update the feature (model) in accordance with the biometric sample variations over time and they continually adapt the templates (user model) based on semi-supervised learning strategies. Experiments with 50 subjects reveal that the proposed template update strategies, improve the recognition accuracy and reduce the classification errors for voice recognition systems, even under sample variations.
               
            

@&#INTRODUCTION@&#

Need of security is increasing all over the world at every level: countries, societies, corporations and individuals. Biometrics play an important role to overcome the problem of security. Even though the human recognition task seems to be easy and straightforward, automated recognition becomes challenging and difficult. This is primarily due to the inherent variations in the acquisition process, undesired variations in the biometric data and non-ideal operating conditions such as background noise and non-uniform illumination [1]. Moreover, noisy data, intra-class variation, inter-class similarities, non-universality, spoofing, etc., are problems imposed by the biometric systems that tend to increase false accept rate (FAR) and false reject rate (FRR), ultimately reflecting towards poor performance. The new challenge for biometrics arises from trying to circumvent these limitations and developing more robust recognition systems [2]. The core of the biometric system is the ‘template’, which is a set of features (model) extracted from the biometric samples collected during the enrollment phase and stored in the system’s database with a label related to the subject’s identity. The success or failure of a biometric system heavily relies on the representativeness of the templates [3,4]. Template representativeness is a crucial problem with biometrics, as the input biometric data is subjected to on-going changes due to the presence of intra-class variations thus making the initially enrolled templates non-representative of them [5]. As a result, performance degradation arises. Trying to cover these intra-class variations by performing several enrollment sessions is expensive. Template update methods have been proposed in literature in order to account for feature variations due to time, aging, physiological and environmental factors [3,4]. The template update mechanism can automatically update the feature (model) in accordance with the biometric sample variance over time. By doing so, the feature/model in the database becomes an up-to-date representation of the user.

Various approaches are reported in the literature for improving the performance of biometric systems under sample variations[6]. Most of the state-of-the-art commercial systems employ fingerprints, face and voice biometrics. Compared to other modalities, speaker recognition systems have large public acceptance as they are easier to implement and the cost of the acquisition device is very cheap. One of the main advantages of voice biometrics is that, it is the only biometric that can be used for remote identification [2]. The greatest challenge in the design of voice based recognition system is to maintain high accuracy even under intra-class variations. Though a lot of work has been attempted on speaker adaptation [7], to the best of authors’ knowledge, no work is reported for speaker template updating. A little attention was devoted to template update systems in research settings and academic publications, and this topic is not in the current mainstream of basic research in biometrics. We believe that, among the various reasons, the scarcity of appropriate databases, containing a sufficient number of biometric data collected over the time, and the intrinsic difficulty of this topic, also due to the lack of a precise formulation of the problem, hindered the advancement of this research field [4]. Hence, an attempt is made in this direction to address the same.

The primary objective of this work is to develop an adaptive template update mechanism for speaker recognition system, that can efficiently capture the intra-class variations of the user. The system should be capable of improving the recognition accuracy and robustness while reducing the false accept rate and false reject rate, even under sample variations. The performance of the proposed system is analysed in terms of recognition accuracy, classification errors and score density plots. The original contributions of the work are:

                           
                              1.
                              
                                 Creation of a new databases for speaker update: A new database for speaker update; English language database for adaptive speaker recognition (ELDASR) has been developed. The database contains 20 samples of 50 speakers which include both male and female voices. All the speakers are of age limit 20–23 years. The voice samples are collected in uncontrolled environmental settings in order to accommodate for the intra-class variations associated with the users.


                                 ‘MFCC super template’ for speaker recognition: One of the major steps behind template updating is the creation of an ‘MFCC super template’, formed by concatenating the MFCC features of each speaker samples of the individual speaker. The ‘super template’ acts as a representative vector for each speaker.


                                 Online and offline MFCC feature and GMM based model update: Online and offline template update techniques based on ‘self-update’ is successfully implemented for both features as well as model updating.


                                 Secondary template (model) for speaker template (model) update: Adaptive offline template (model) update strategy is implemented for speaker recognition system by introducing an additional secondary template (model) in conjunction with the primary template (model) created during the training phase. Secondary template update method is considered for both feature and model updating.

The rest of the paper is structured as follows. The following section gives a brief survey of the related works available in the literature. Section 3 gives the background of the work; i.e. brief outline of the speaker recognition system and the matching strategies adopted. Proposed online and offline techniques for speaker update is detailed in Section 4. Section 5 describes the database and experimental setup. The results of the experiments are detailed in Section 6. The paper concludes with a brief conclusion in Section 7.

@&#RELATED WORK@&#

The basic steps in template update methods are the assignment of identity labels. Identity label assignment can be done either in completely supervised (i.e. by human expert intervention) manner or by using automatic ‘learning’ (semi-supervised) methodology. The key difference between supervised and semi-supervised learning is the technique followed for the data labelling [3]. In supervised template update methods, the label assignment is manual, while in semi-supervised methods it is automatic. Supervised methods proposed so far are usually offline as they operate on a batch of collected samples. On the other hand, semi-supervised methods are automated methods that assign identity labels to the unlabelled data on the basis of their knowledge, derived through current enrolled templates without the intervention of human supervisor. These methods avoid the cost related to manual assignment of labels [3].

Uludag et al. proposed two methods to select a gallery of representative templates from multiple impressions collected at enrollment [8]. One of the methods was based on a clustering strategy, to choose a template set that best represents the intra-class variations, while the other selected templates that exhibit maximum similarity with the rest of the impressions. Methods which generate superior templates by fusing multiple fingerprint impressions were found in [9,10]. This ‘superior template’ is obtained by fusing information contained in the multiple impressions considered. Uludag et al. did not deal with a strict adaptive biometric system that can improve with use, instead the authors proposed a supervised system [11]. It should be noted, that the template update methods of Uludag et al. [11] can be made adaptive by using semi-supervised clustering algorithms [12] or self-training techniques.

The first work, as per the authors’ knowledge, that described an adaptive system for fingerprint verification is the one of Jiang and Ser [9]. They proposed an algorithm for online template updating. This algorithm could update templates by fusing the impressions acquired online that is recognized as genuine with high reliability. The fusion process recursively generated a ‘superior template’ that removed spurious minutiae while recovering missing minutiae. But it cannot append new minutiae when they belong to the initial template. Ryu et al. proposed an adaptive fingerprint verification system using ‘super template’ [9]. The reported method was more flexible in appending new minutiae, and it also used the local fingerprint quality information to update templates [10]. To the best of the authors’ knowledge, only two works have been reported using fingerprint biometric, that can improve its performance with use [9,10].

Okada et al. described a prototype system for face recognition in video that automatically update galleries, containing different views of the user’s face [13]. Views recognized with high reliability in the input video are added to the galleries. When the identity of the face image is unknown, a new entry, corresponding to a new identity, is added to the person’s gallery. This self-training approach may work well if the classification errors are maintained at a low value. Otherwise, system’s performance may degrade over the time.

Another early work of Weng and Hwang discussed some critical issues in the design of adaptive systems that can improve with use. For example, the ‘forgetting’ issue: how to forget outdated knowledge to save memory space. They proposed a self-organizing approach to face recognition in video [14]. More recently, Liu et al. proposed an algorithm to update the eigen space of a principle component analysis (PCA)-based face recognition system incrementally by exploiting unlabeled data acquired during the system’s operation [12]. In the last few years, Nagy introduced the concept of computer assisted visual, interactive recognition (CAVIAR) that offered a different perspective for the design of adaptive biometric systems [15]. CAVIAR systems overcome the traditional dichotomy between totally manual and totally automatic recognition systems. The goal of the CAVIAR paradigm was to exploit and to integrate the different recognition abilities of humans and machines. Reported results show that this human–computer interaction allows improving performance with use. In particular, the need for human supervision decreases as the automatic recognition algorithm improves by exploiting human feedback. Works related to adaptive face recognition can be found in [16]. Some relevant works on adaptive biometric systems have been done in the field of speech recognition and verification as well [4].

Recently, techniques based on bi-modal updating has been reported in the biometric literature. These methods utilized multimodal biometrics for template updating [17]. They are based on the ‘co-training’ concept of the semi-supervised learning technique [4]. Specifically, two classifiers help each other in the updating process on the assumption that they work on independently and identically distributed (i.i.d.) data [4,17]. The main intuition is that, if matchers are complementary, one matcher operating at high confidence helps other to identify ‘difficult patterns’ [4,17]. The offline version of this method has been proposed in [4] with preliminary experiments. Various approaches are reported in the literature for improving the performance of voice recognition system. The outcome of the literature survey can be summarized as:

                        
                           •
                           Speaker recognition performance could be improved by devising robust and efficient feature extraction algorithms.

Template updating and adaptation techniques are promising to overcome the intra-class variations [3,4].

Improved accuracy can be obtained by using multi-modal and multi-sensor approaches [2].

Recognition performance of the dependent modality (voice) can be improved by combining it with an independent biometric (fingerprint) modality [18].

More research is needed to find out efficient template update strategies [4].

These key issues are the main motivations behind the proposed research.

This section gives a brief description of the speaker recognition system employed in the present work. A speaker recognition system is essentially a pattern recognizer that extracts, characterizes, and classifies the information about the speaker’s identity. Short-time spectral analysis is used to characterize the quasi-stationary speech samples. Mel frequency Cepstral coefficient based feature extraction and vector quantization as well as a Gaussian mixture model based matching strategies have been considered here. To represent the voice samples in a parametric way, we have considered the Cepstral representation as they have proven to be efficient and compact [19,20]. The various steps involved in the MFCC feature extraction include:

                        
                           1.
                           Frame the signal into short frames.

For each frame calculate the periodogram estimate of the power spectrum.

Apply the Mel filter bank to the power spectra, sum the energy in each filter.

Take the logarithm of all filter bank energies.

Take the discrete cosine transform (DCT) of the log filter bank energies.

Keep DCT coefficients 2–13, discard the rest.

The state-of-the-art techniques used for speaker modelling includes dynamic time warping (DTW), hidden Markov model (HMM), vector quantization and Gaussian mixture model. A novel PCA/Linear discriminant analysis (LDA)-based approach for text-independent speaker recognition has been reported in [21]. Both non-parametric (VQ) and parametric (GMM) representations have been considered here, for voice template/model updating, to show that the proposed updating schemes is valid in both domains.

We have considered vector quantization based matching strategy for feature matching. VQ can be thought of a process for redundancy removal that makes the effective use of nonlinear dependency and dimensionality by compression of speech spectral parameters. VQ maps k-dimensional vector space to a finite set 
                           
                              C
                              B
                              =
                              {
                              
                                 C
                                 1
                              
                              ,
                              
                                 C
                                 2
                              
                              ,
                              
                                 C
                                 3
                              
                              ,
                              .
                              .
                              .
                              ,
                              
                                 C
                                 N
                              
                              }
                           
                         (VQ maps, vectors from a large vector space into a finite number of regions in that space). Each region is called a cluster and can be represented by its centre called codevector. The set CB is called a codebook consisting of ‘N’ number of codevectors and each codevector 
                           
                              
                                 C
                                 i
                              
                              =
                              
                                 {
                                 
                                    
                                       c
                                       
                                          i
                                          1
                                       
                                    
                                    ,
                                    
                                       c
                                       
                                          i
                                          2
                                       
                                    
                                    ,
                                    
                                       c
                                       
                                          i
                                          3
                                       
                                    
                                    ,
                                    .
                                    .
                                    .
                                    ,
                                    
                                       c
                                       
                                          i
                                          k
                                       
                                    
                                 
                                 }
                              
                           
                         is of dimension k.

In the training stage, a speaker specific VQ codebook is generated for each known speaker by clustering his/her acoustic vectors. We have considered LBG algorithm, for clustering a set of ‘L’ training vectors into a set of ‘N’ codebook vectors. In the matching stage, the unknown speaker model is compared against the reference model stored in the database to make a valid decision for authentication. The input utterance of an unknown speaker is vector quantized using each trained codebook and the total VQ distortion is computed using the Euclidean distance measure.

                           
                              (1)
                              
                                 
                                    
                                       ∥
                                       x
                                       −
                                    
                                    
                                       m
                                       i
                                    
                                    
                                       ∥
                                       ≡
                                    
                                    
                                       
                                          
                                             
                                                (
                                                x
                                                −
                                                
                                                   m
                                                   i
                                                
                                                )
                                             
                                             T
                                          
                                          
                                             (
                                             x
                                             −
                                             
                                                m
                                                i
                                             
                                             )
                                          
                                       
                                    
                                    <
                                    
                                       ∥
                                       x
                                       −
                                       
                                          m
                                          j
                                       
                                       ∥
                                    
                                    ,
                                    
                                    i
                                    =
                                    j
                                 
                              
                           
                        
                     

The speaker corresponding to the VQ codebook with the smallest total distortion is authenticated.

This is one of the parametric methods for speaker recognition [2]. Gaussian mixture model can be considered for representing the acoustic feature vectors as they are proven to give the most successful likelihood function for representing text-independent speaker samples. Moreover, they effectively model the underlying distribution of acoustic observation and they are insensitive to the temporal aspects of the speech. A Gaussian mixture model is a weighted sum of M component Gaussian densities, given as follows [22]:

                           
                              (2)
                              
                                 
                                    p
                                    
                                       (
                                       x
                                       /
                                       λ
                                       )
                                    
                                    =
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       M
                                    
                                    
                                       a
                                       i
                                    
                                    d
                                    
                                       (
                                       x
                                       /
                                       
                                          μ
                                          i
                                       
                                       ,
                                       
                                          Σ
                                          i
                                       
                                       )
                                    
                                 
                              
                           
                        where x is the D-dimensional feature vector, ai
                        , 
                           
                              i
                              =
                              1
                              ,
                              .
                              .
                              .
                              .
                              .
                              .
                              M
                              ,
                           
                         are the mixture weights such that 
                           
                              
                                 ∑
                                 
                                    i
                                    =
                                    1
                                 
                                 M
                              
                              
                                 a
                                 i
                              
                              =
                              1
                              ,
                           
                           d(x/μi, Σi
                        ),    
                           
                              i
                              =
                              1
                              ,
                              .
                              .
                              .
                              .
                              .
                              .
                              M
                              ,
                           
                         are the component Gaussian densities. Each component density is a D-variate Gaussian function with mean vector μi
                         and covariance Σi
                        . This can be represented as follows:

                           
                              (3)
                              
                                 
                                    d
                                    
                                       (
                                       x
                                       /
                                       
                                          μ
                                          i
                                       
                                       ,
                                       
                                          Σ
                                          i
                                       
                                       )
                                    
                                    =
                                    
                                       1
                                       
                                          
                                             
                                                (
                                                2
                                                π
                                                )
                                             
                                             
                                                D
                                                2
                                             
                                          
                                          
                                             
                                                |
                                                
                                                   Σ
                                                   i
                                                
                                                |
                                             
                                             
                                                1
                                                2
                                             
                                          
                                       
                                    
                                    exp
                                    
                                       {
                                       −
                                       
                                          1
                                          2
                                       
                                       
                                          
                                             (
                                             x
                                             −
                                             μ
                                             )
                                          
                                          
                                             
                                             ′
                                          
                                       
                                       
                                          Σ
                                          
                                             i
                                          
                                          
                                             −
                                             1
                                          
                                       
                                       
                                          (
                                          x
                                          −
                                          μ
                                          )
                                       
                                       }
                                    
                                 
                              
                           
                        
                     

The complete Gaussian mixture model is parametrized by the mean vectors, covariance and the mixture weights. These parameters are collectively represented by [22],

                           
                              (4)
                              
                                 
                                    λ
                                    =
                                    
                                       {
                                       
                                          a
                                          i
                                       
                                       ,
                                       
                                          μ
                                          i
                                       
                                       ,
                                       
                                          Σ
                                          i
                                       
                                       }
                                    
                                    ,
                                    
                                    
                                    i
                                    =
                                    1
                                    .
                                    .
                                    .
                                    .
                                    .
                                    ,
                                    M
                                 
                              
                           
                        
                     

In the training stage itself, each enrolled speakers in 
                           
                              g
                              ,
                           
                         where 
                           g
                        
                        
                           
                              =
                              
                                 {
                                 
                                    
                                       g
                                       1
                                    
                                    ^
                                 
                                 ,
                                 
                                    
                                       g
                                       2
                                    
                                    ^
                                 
                                 ,
                                 .
                                 .
                                 .
                                 ,
                                 
                                    
                                       g
                                       G
                                    
                                    ^
                                 
                                 }
                              
                           
                         is represented by a unique GMM (
                           λ
                        ). In the testing stage, the features from the unknown speaker’s utterances are compared with statistical models of the voices of speakers known to the system. The Bayes rule suggests to allocate the test samples to the class 
                           
                              
                                 
                                    g
                                    k
                                 
                                 ^
                              
                              ,
                           
                         having the highest posterior probability, that is [22],

                           
                              (5)
                              
                                 
                                    
                                       
                                          g
                                          k
                                       
                                       ^
                                    
                                    =
                                    a
                                    r
                                    g
                                    
                                    
                                       max
                                       
                                          1
                                          ≤
                                          k
                                          ≤
                                          G
                                       
                                    
                                    p
                                    
                                       (
                                       
                                          X
                                          |
                                       
                                       
                                          λ
                                          k
                                       
                                       )
                                    
                                 
                              
                           
                        where p(X|λk
                        ) is the a posteriori probability for a given observation sequence.

@&#PROPOSED METHOD@&#

The state-of-the-art ‘self-update’ techniques are based on the concept of ‘self-training’ in semi-supervised learning theory [4,23]. They update the templates in order to reduce intra-class variations using the knowledge gained from current enrolled templates. These systems operate by updating confidently recognized test samples to the template gallery set of the respective user. The rationale is that, the test samples recognized with high ‘reliability’ are used to update the template set, in order to improve its representativeness [3]. The procedure to update the user templates can be either instance based or prototype based [24]. The difficulty with the state-of-the-art template update techniques is with its high ‘decision threshold’ dependency. These methods are very dependent on the threshold value selected for the update, as only the input data whose matching score is above the selected threshold, is added for updating. The serious drawbacks of these approaches are:

                        
                           1.
                           In order to derive better performance the threshold need to be modified manually for each updating.

Rules out the possibility for automatic template updating (as the threshold need to be altered manually).

Increased computational complexity.

Many of these methods [3,4] results only in local update solution as the update lead to non-exploitation of many difficult and informative intra-class variations. Even with the operation at stringent threshold, may be these methods are vulnerable to large intra-class variations due to basic FAR of the system and factors like incorrect threshold estimation, etc. [4]. The present work aims to overcome the aforementioned shortcomings of the state-of-the-art methods in developing a speaker recognition system that can update the user template automatically without any threshold overheads. Here, an adaptive template update mechanism for speaker recognition system, that can effectively capture the intra-class variations of the user is proposed. The system is capable of improving the recognition accuracy and robustness while reducing the classification errors, even under sample variations. In order to reduce the computational complexities and memory space requirements, the concept of ‘MFCC super template’ is proposed. The ‘super template’ is formed by concatenating the MFCC features of speech samples of the individual speaker. This ‘super template’ acts as a representative vector which is further used for testing and updating.

Online and offline mechanisms are two real time strategies which are prominent among the biometric community. Most of the current systems follow offline update method where a batch of samples is collected and template is updated after a particular time period. This method may be very useful where the samples are collected on a daily basis. That is the biometric system will be accessed on a daily basis once or twice. Online methods update the system as soon as the input data arrives, based on whether the data is confidently classified as genuine or not. In this case, the template is updated on each access of the system. This approach will be highly suitable for applications demanding higher security, such as granting access to nuclear facilities where the access of the system only happens occasionally. Hence, in order to reduce the class variability associated with speaker recognition, online and offline template update techniques based on ‘self-update’ are successfully implemented for both features as well as model updating.

The adaptive offline template update is also implemented for speaker recognition by introducing an additional secondary template in conjunction with the primary template created during the training phase. The secondary template update method is considered for both features as well as model update. The ‘self-update’ methods iteratively add the test sample model gt
                      (with unknown class label) to the user model using the class label derived from the previously augmented training set gk
                     . Here, the assumption is that each user i has a model associated with them. In order to arrive at a better conclusion regarding the performance of a speaker recognition system for template updating, feature update mechanism using vector quantization and model update mechanism using Gaussian mixture model has been considered. The following section gives a brief description of the same.

The proposed feature updating algorithms iteratively adds the enrolled templates in order to capture the subject’s intra-class variations. The MFCC features are considered here to best represent the speakers. Sample variations would certainly reflect in the feature vectors. Hence, feature updating thoroughly relies on feature robustness. By concatenating the MFCC features of each speaker samples, ‘MFCC super templates’ are formed. The ‘MFCC super templates’ is an up-to-date (contemporary) representation of the speaker. The ‘MFCC super template’ is further vector quantized (clustered) to yield a compact representation. The codebook (cluster centres) thus formed is a compact and more refined representative vectors for each speaker. The added advantage of this method is that the computational complexity and memory overheads are minimized.

In this method, ‘MFCC super template’ is created during the development (training) stage by concatenating MFCC features of each speaker samples. This super template gives a more compact and timely representation of the individual speakers. During the testing stage, feature vectors of the test samples are compared with the ‘super template’. The process of creation of super template is as follows: let Vi
                           (x) represent the feature vectors of training samples Xi
                            (with 
                              
                                 i
                                 th
                              
                            label) and Vt
                           (x) represent the feature vectors of the test sample (with unknown label). If the test samples are recognized as genuine with high reliability, the feature vectors in Vt
                           (x) that are not contained in the super template are iteratively augmented to Vi
                           (x) (with 
                              
                                 i
                                 th
                              
                            label) using clustering techniques (vector quantization). The augmented super template 
                              
                                 
                                    V
                                    
                                       a
                                       u
                                       g
                                    
                                    
                                       s
                                       t
                                    
                                 
                                 
                                    (
                                    x
                                    )
                                 
                              
                            can be represented mathematically as,

                              
                                 (6)
                                 
                                    
                                       
                                          V
                                          
                                             a
                                             u
                                             g
                                          
                                          
                                             s
                                             t
                                          
                                       
                                       
                                          (
                                          x
                                          )
                                       
                                       =
                                       
                                          V
                                          i
                                       
                                       
                                          
                                             (
                                             x
                                             )
                                          
                                          ∥
                                       
                                       
                                          V
                                          t
                                       
                                       
                                          (
                                          x
                                          )
                                       
                                    
                                 
                              
                           
                        

The horizontal concatenation operator (||) produces a new matrix by horizontally joining Vi
                           (x) and Vt
                           (x) (Matrix
                           1||Matrix
                           2). Matrix
                           1 and Matrix
                           2 must have the same number of rows, which is also the number of rows in the new matrix. The number of columns in the new matrix is the number of columns in Matrix
                           1 plus the number of columns in Matrix
                           2. The vector quantized ‘MFCC super template’ acts as a representative vector which is further used for testing and updating. Template updating further refines this codebook (cluster centres).

In the proposed online update, enrolled templates are updated as soon as a new input sample is encountered, i.e., the process of updating goes along with the recognition process of the system. If the match score between the test sample and enrolled template is above, the decision threshold of the recognition engine, the feature vectors of test sample are used for updating the templates. The test samples that are recognized with high confidence are used for the updating process. The online update process is as follows: let Vi
                           (x) represent the feature vectors (super template) of the speaker Xi
                            and Vt
                           (x) represent the feature vectors of the test sample (with unknown label). If the test samples are recognized as genuine with high reliability, the feature vectors Vt
                           (x) are concatenated with the super template Vi
                           (x). The augmented super template 
                              
                                 (
                                 
                                    V
                                    
                                       a
                                       u
                                       g
                                    
                                    
                                       s
                                       t
                                    
                                 
                                 
                                    (
                                    x
                                    )
                                 
                                 )
                              
                            thus formed is further vector quantized (clustered) to yield a compact representation. Hence, after each access (update), the codebook (cluster centres) is refined to give the contemporary representation of the speaker without any memory overheads. By template updating, the feature (MFCC super templates) in the database becomes an up-to-date representation of the speaker. Thus, concatenation followed by clustering refines the speaker specific codebook without any memory overheads. Fig. 1
                            shows the block diagram for VQ based online update. Online methods update the system as soon as the input data arrives, based on if the data is confidently classified as genuine or not.

In the proposed offline update, a batch of recognized samples is collected, stored and are used to update when the system is not in use. Here, two template sets are considered for offline updating. One is the primary template which is created during the training phase and a secondary template where a batch of test samples recognized with high confidence are stored. The primary database consists of the MFCC super template. During each access of the system the input sample is compared with both these template sets. The primary template will be updated by using a secondary template after a certain period of time. The use of secondary template proves to be efficient for each test sample is compared with both the templates so that the only the sample recognized with higher confidence is used for updating. This improves the robustness of the proposed method. Here also, concatenation of the super template (primary template) with the secondary template followed by vector quantization (clustering) refines the speaker specific codebook without any memory overheads.

In order to improve the speaker recognition performance under sample variations, a novel method for model updating using Gaussian mixtures is also presented here. State-of-the-art techniques use several models for each individual in order to overcome the intra-class variations. But this approach increases the memory space enormously as the database size increases. By using the proposed MFCC super model using Gaussian mixtures, the dynamic features associated with the user could be effectively captured. Moreover, this also reduces the memory cost as well. In model update, the Gaussian mixture parameters, mean (μ), covariance (Σ) and mixture weights (a) are updated. Speaker specific ‘GMM super models’ are stored for each user during the development (training) stage. At the testing stage, if the recognition engine correctly identifies the user with a high reliability, the Gaussian mixture super model previously stored in the database (during training) is updated with the new model parameters associated with the test samples.

In model update, Gaussian mixture model for representing each speaker is created by the virtue of a super template. The resultant model is termed as the ‘GMM super model’. This maps an individual to a model, which represents him/her entirely. The speaker specific GMM super model is obtained as follows: during the development (training) stage ‘super template’ is formed by concatenating the MFCC features of speech samples of the individual speaker. The complete Gaussian mixture model is parametrized by the mean (μ), covariance (Σ) and mixture weights (a). The speaker specific GMM super model (mean, covariance and mixture weights) are estimated from this concatenated feature set. The super model thus obtained acts as a representative vector which is further used for testing and updating. Template updating further refines this super model.

The online model update is based on the state-of-the-art ‘self-update’ method. Here, the template model (template gallery) is updated in order to accommodate the intra-class variations associated with the incoming speech samples. The online model update is performed using Gaussian mixture model, where a ‘super model’ is formed which consists of the representative μ, Σ, a corresponding to each speaker. The test query is analysed, if a match is detected with high reliability, the Gaussian mixture super model previously stored in the database (during training) is updated with the new model parameters associated with the test samples, else the candidate will be rejected. This concept is summarized in Fig. 2
                           . During each successful access of the system, the model corresponding to the speaker is updated. Given two model vectors λi
                           (x) & λt
                           (x),    
                              
                                 x
                                 =
                                 1
                                 ,
                                 2
                                 ,
                                 3
                                 .
                                 .
                                 .
                                 n
                                 ,
                              
                            where λi
                           (x) represent the model vectors of the training samples, (
                              
                                 
                                    λ
                                    i
                                 
                                 
                                    (
                                    x
                                    )
                                 
                                 =
                                 
                                    {
                                    
                                       μ
                                       i
                                    
                                    ,
                                    
                                       Σ
                                       i
                                    
                                    ,
                                    
                                       a
                                       i
                                    
                                    }
                                 
                              
                           ) and λt
                           (x) represent the model vectors of the test samples (
                              
                                 
                                    λ
                                    t
                                 
                                 
                                    (
                                    x
                                    )
                                 
                                 =
                                 
                                    {
                                    
                                       μ
                                       t
                                    
                                    ,
                                    
                                       Σ
                                       t
                                    
                                    ,
                                    
                                       a
                                       t
                                    
                                    }
                                 
                              
                           ). The super model λsm
                           (x) for the two model vectors is mathematically represented by combining operand GMMs into a single model,

                              
                                 (7)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   λ
                                                   
                                                      s
                                                      m
                                                   
                                                
                                                
                                                   (
                                                   x
                                                   )
                                                
                                             
                                          
                                          
                                             =
                                          
                                          
                                             
                                                {
                                                
                                                   μ
                                                   
                                                      s
                                                      m
                                                   
                                                
                                                ,
                                                
                                                   Σ
                                                   
                                                      s
                                                      m
                                                   
                                                
                                                ,
                                                
                                                   a
                                                   
                                                      s
                                                      m
                                                   
                                                
                                                }
                                             
                                          
                                       
                                    
                                 
                              
                           
                           
                              
                                 (8)
                                 
                                    
                                       
                                          
                                             
                                                μ
                                                
                                                   s
                                                   m
                                                
                                             
                                          
                                          
                                             =
                                          
                                          
                                             
                                                m
                                                e
                                                a
                                                n
                                                (
                                                
                                                   μ
                                                   i
                                                
                                                ,
                                                
                                                   μ
                                                   t
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                           
                              
                                 (9)
                                 
                                    
                                       
                                          
                                             
                                                Σ
                                                
                                                   s
                                                   m
                                                
                                             
                                          
                                          
                                             =
                                          
                                          
                                             
                                                m
                                                e
                                                a
                                                n
                                                (
                                                
                                                   Σ
                                                   i
                                                
                                                ,
                                                
                                                   Σ
                                                   t
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                           
                              
                                 (10)
                                 
                                    
                                       
                                          
                                             
                                                a
                                                
                                                   s
                                                   m
                                                
                                             
                                          
                                          
                                             =
                                          
                                          
                                             
                                                m
                                                e
                                                a
                                                n
                                                (
                                                
                                                   a
                                                   i
                                                
                                                ,
                                                
                                                   a
                                                   t
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

In order to refine the GMM parameters and control the size of the model, arithmetic mean of the parameters (mean, covariance and mixture weights) are performed separately after each updating. Hence, the template model is updated in order to accommodate the intra-class variations without any memory overheads.

In offline update, a batch of recognized samples is collected and stored for future references. In the present work, two models have been considered in an offline model updating. One is the primary model which is created during the training phase and a secondary model where a batch of test sample models recognized with high confidence are stored. The secondary models are used for updating the primary model after a particular time period, say when the system is not in use. The primary database consist of a ‘super model’. After each access of the system the test sample is compared with both primary as well as the secondary Gaussian mixture models. The missing parameters in the primarily model is compensated by the secondary model. The arithmetic mean of the primary model with the secondary model is performed to yield a compact and contemporary super model. Experimental results show that the proposed system is capable of improving the recognition accuracy and robustness while reducing the false accept rate and false reject rate, even under sample variations.

The major problem when working with template update is the lack of real-user databases. The lack of appropriate databases, containing a sufficient number of biometric data collected over the time hindered the advancement of research in template update for voice recognition. In this work English language speech database for speaker recognition (ELSDSR) has been considered in the developmental phase of the experiments [25]. The ELSDSR database contains nine text independent speech samples of twenty three persons. Out of this nine samples per biometric, seven samples have been used for training the individual classifiers and two samples for testing. This choice is done to improve the recognition accuracy in order to account for the intra-class variations.

In order to compensate for the scarcity of real user database, a new database for speaker adaptation, ELDASR has been developed. The database at present contains 20 text independent speech samples of 50 speakers which include both male and female voices. All the speakers are of age limit 20–23 years. The voice samples are collected in an uncontrolled environment setting, in order to accommodate for the intra-class variations associated with the users. The speaker samples are collected using unidirectional microphone, Ahuja UTP-30.

Here, the performance of voice recognition system has been considered by varying the number of MFCC features and clusters or Gaussian mixtures. MFCC features of the order 12, 16 and 20 and the GMM with 12 and 16 mixtures (VQ with 12 and 16 clusters) have been considered for the simulation studies, as they are widely used. Different model combinations have been considered to select the best model that gives better recognition accuracy. The selection of the number of MFCC features is based on the Mel Filter banks used. In general, the number of Mel filters (in the filter bank) used for speaker recognition ranges from 12 to 40. However, the effective speaker recognition involves discerning lower frequencies (mimicking the properties of human cochlea), as the lower frequencies capture the intra-class variations. Filter banks with lesser number of Mel filters can capture lower frequencies effectively. Number of Mel filters ranging from 12 to 20 is experimentally evaluated under strict conditions of computational complexity, memory requirements and processing time. Here, 12 Mel filters were found to be optimal and hence 12 MFCC features were considered in the experimental studies.

There exists an ambiguity in literature regarding the selection of clusters in vector quantization. To the best of authors’ knowledge, no unique methods are available in the literature for the same. Optimal cluster selection using complex algorithms (heuristic and meta heuristic) is an ongoing research area which is beyond the scope of this paper. It is experimentally found that the performance of the systems with 12 features and 16 clusters (mixtures) give improved performance with the said database used.

As the research is mainly focused on template update schemes for speaker recognition, the possibilities of the proposed schemes are evaluated with the best available state-of-the art. As one navigates through the literature, it is evident that the parametric and non-parametric approaches for speaker recognition are prominent. The consideration of both parametric GMM based model update) as well as non-parametric (VQ based feature update) approaches for feature and model update respectively is a humble attempt to demonstrate the validity of the proposed updating schemes in the best available domains. Moreover, it is evident from the experimental analysis that the proposed template updating strategies are promising with both model as well as feature update.

@&#RESULTS AND DISCUSSIONS@&#

The performance of the proposed online and offline model updating strategy with VQ as well as GMM is compared with the baseline and state-of-the-art systems. Performance with ELSDSR and ELDASR databases are considered in the evaluation. The performance compared in terms of recognition accuracy, classification errors and score density plots are briefly discussed here.

The performance of baseline speaker recognition systems with two databases have been considered. Speaker recognition system with MFCC features and vector quantization as well as GMM based matching have been considered as the baseline system. Table 1
                         shows the recognition accuracy with the VQ based baseline system. Testing accuracy with ELSDSR database is 97.8261%. Out of 23 speakers with nine samples, seven samples are considered for training and two samples for testing. The samples in ELSDSR database are taken under controlled environment settings. The testing accuracy with ELDASR database is 90%. This reduction in recognition accuracy is quite obvious, as the database consists of 20 samples from 50 speakers. Moreover, the samples in ELDASR database are taken under uncontrolled environment settings. Here, seven samples are considered for training, two samples for testing and the remaining samples for updating. The DET plots and the score density plots for the VQ based baseline systems are shown in Figs. 3
                        (a), (b) and 7, respectively.

The performance of GMM based baseline speaker recognition systems with two databases are shown in Table 2
                        . Speaker recognition system with MFCC features and GMM based matching has been considered. Testing accuracy with ELSDSR database is 100% and that with ELDASR database is 92%. The DET plots for the baseline systems are shown in Fig. 3(c), and (d). The DET plots show that the performance of speaker recognition increases with the parametric representations, as it could effectively capture the temporal variations in the speech as well. The reduction in recognition performance with ELDASR database is due to the above mentioned facts.

The performance of the online feature update method is compared with the ‘MFCC super template’ based speaker matching approach. With this approach, MFCC features of all the training samples from each person are combined together so as to form a ‘super template’. In the testing stage the vector quantized (clustered) test samples are compared to the vector quantized (clustered) trained ‘super templates’ and matching is performed using Euclidean distance metric. This method overcomes the drawback of the baseline systems in terms of memory requirements. The baseline methods use multiple templates for each speaker in order to minimize the intra-class variations. This in turn increases the computational requirements and memory overheads. The recognition performance with the MFCC based ‘super template’ method is tabulated in Table 1. Fig. 4
                        (a) and (b) shows the DET performance plots. Even though the recognition accuracy of both the methods (baseline and super template methods) is comparable, super template approach yield reduced memory and computational requirements.

The performance of the online and offline model updating method is also compared with the ‘super model’ based speaker matching strategy. In this case, during the training phase, MFCC features of all the training samples of each person are combined to form a super template. A single Gaussian mixture (GM) model for this super template is obtained and used for future reference. In the testing stage the test samples are compared with the GM models of each speaker and the test sample is allocated to that class having the maximum posteriori probability. The recognition performance with the GMM based ‘super model’ method (using ELSDSR and ELDASR) is tabulated in Table 2. Fig. 4(c), and (d) shows the DET performance. It is evident from the results that ‘super model’ approach yield reduced memory as well as computational requirements.
                        
                        
                     

The experimental studies with the baseline and the state-of-the art systems using ELSDSR database based on VQ approach yields the same recognition accuracy of 97.8261%. Whereas, the performance with ELDASR database yields a recognition accuracy of 90%. The experimental studies with the baseline and the state-of-the art systems using ELSDSR database based on the GMM approach yields the recognition accuracy of 100%. Whereas, the performance with ELDASR database yields a recognition accuracy of 92% and 94% (for baseline and state-of-the-art). This high recognition accuracy with ELSDSR database can be attributed to the high quality of the standard database collected in the controlled environment. Figs. 3(a), (b) and 4(a), (b) show the DET performance plots with the baseline and the state-of the-art-methods for the VQ based approach. Figs. 3(c), (d) and 4(c), (d) show the DET performance plots with the two databases for the GMM based approaches. In order to minimize the classification errors and intra-class variations associated with voice biometrics, adaptive online and offline template update techniques using vector quantization and Gaussian mixture model are proposed in this paper. The proposed template update mechanisms calls for better incorporation of intra-class variations while maintaining improved recognition performance.

The performance of the proposed online and offline feature update is evaluated using the ELSDSR and ELDASR databases. In online update using VQ, the matched speakers super template is updated with the features of the test samples. The testing accuracy with the proposed method increases with feature updating. This is evident from the testing accuracy depicted in Table 1 and from the DET and score density plots. Figs. 5(a), (b) and 6(a), (b) shows the DET performance with proposed online and offline feature update methods respectively. The proposed system yields a recognition accuracy of 100% and 93% (Table 1) for ELSDSR and ELDASR database respectively. It is worthy to be noted that despite of the variations in the quality of the biometric inputs (from two different databases), the percentage increase in the recognition accuracy is significant for the proposed method. From these experimental results it is evident that the proposed recognition engine could very well accommodate the intra-class variations associated with the biometric samples while maintaining higher recognition accuracy compared to the baseline and the state of the art systems.

Experimental results show that the performance of the recognition system improves with feature updating. The improved performance is due to the fact that new temporal features that is not present in the template database is effectively captured by the online and offline update techniques. The one disadvantage with the proposed approach is that super template size increases with use. This difficulty is managed by replacing the MFCC super template with its vector quantized (clustered) output when the user has more than five accesses.

The performance of the proposed online and offline model update strategy is also analysed using the ELSDSR and ELDASR databases. From Table 2 the recognition accuracy for the proposed system using ELSDSR database is 100%, whereas with the ELDASR database, it is 95%. The testing accuracy with the proposed method increases with the GMM based model updating. This is quite obvious from the testing accuracy depicted in Table 2 and from the DET and score separability plots. Figs. 5(c), (d) and 6(c), (d) show the DET performance with proposed online and offline model updating methods respectively. Experimental results show that the performance of the recognition system improves with the GMM based model update. It is quite evident from the (respective DET plots) Figs. 3(c), (d), 4(c), (d), 5(c), (d) and 6(c), (d), that the proposed system efficiently reduces the classification errors (FAR and FRR) while maintaining improved recognition accuracy compared to the baseline and the state-of-the-art GMM based methods.

The score density plots depicted by Figs. 7 and 8
                         show the genuine and impostor score distribution curves for the baseline and the proposed (offline and online) methods. It is evident from the plots that the reported method shows better separability for genuine and imposture score distribution compared to baseline method. This in turn validates the recognition performance of the reported method. The testing accuracy with the proposed method increases with the GMM based model updating compared to the VQ based feature update. This is quite obvious from the testing accuracy depicted in the Table 2. This again strengthens the fact that template updating using parametric method (GMM) outperforms the non-parametric based approach.

@&#CONCLUSION@&#

An efficient online and offline template update techniques based on ‘self-update’ is successfully implemented for both features as well as model updating. Feature and model update is performed using vector quantization and Gaussian mixture model respectively. Feature update is performed using a ‘super template’ and model update using a ‘super model’. The MFCC based super template is obtained by concatenating the feature vectors of each individual speaker. The ‘super template’ acts as a representative vector for each user. This approach reduces the memory overheads and computational complexities involved in matching. The performance of our system is verified using both ELSDSR and ELDASR database.

In this work, a new database for speaker adaptation, ELDASR has been developed. The database contains 20 samples of 50 speakers which include both male and female voices (age limit 20–23 years). The voice samples are collected in uncontrolled environmental settings in order to accommodate for the intra-class variations associated with the users. Adaptive offline template update strategy is also implemented for speaker recognition system by introducing an additional secondary template in conjunction with the primary template created during the training phase. The secondary template update is considered for both features as well as model updating. It is observed that the reported method provides better recognition accuracies and reduces the classification errors which in turn reinforces the effectiveness of the proposed online and offline feature as well as model update strategies. This work will serve as a baseline for future work to be developed to bring this area of research one step ahead.

@&#ACKNOWLEDGEMENTS@&#

The authors would like to acknowledge the Chairman, Secretary, Director and Principal of M.E.S. College of Engineering Kuttipuram for rendering all help and support needed for the successful completion of this work. Our appreciation also goes to the editors and reviewers as their comments greatly helped to improve the earlier version of this manuscript.

@&#REFERENCES@&#

