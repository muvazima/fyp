@&#MAIN-TITLE@&#Image segmentation via multi-scale stochastic regional texture appearance models

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose multi-scale stochastic regional texture appearance models for image segmentation.


                        
                        
                           
                           An image representation is constructed using an iterative bilateral scale space decomposition.


                        
                        
                           
                           Local texture features are extracted via random projections obtaining stochastic texture features.


                        
                        
                           
                           A texton dictionary is built and is used to represent the global texture appearance model.


                        
                        
                           
                           Based on experiments and comparisons, the method can be effective for textured images.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Image segmentation

Texture segmentation

Stochastic texture models

Regional texture appearance models

@&#ABSTRACT@&#


               
               
                  An ongoing challenge in the area of image segmentation is in dealing with scenes exhibiting complex textural characteristics. While many approaches have been proposed to tackle this particular challenge, a related topic of interest that has not been fully explored for dealing with this challenge is stochastic texture models, particularly for characterizing textural characteristics within regions of varying sizes and shapes. Therefore, this paper presents a novel method for image segmentation based on the concept of multi-scale stochastic regional texture appearance models. In the proposed method, a multi-scale representation of the image is constructed using an iterative bilateral scale space decomposition. Local texture features are then extracted via image patches and random projections to generate stochastic texture features. A texton dictionary is built from the stochastic features, and used to represent the global texture appearance model. Based on this global texture appearance model, a regional texture appearance model can then be obtained based on the texton occurrence probability given a region within an image. Finally, a stochastic region merging algorithm that allows the computation of complex features is presented to perform image segmentation based on proposed regional texture appearance model. Experimental results using the BDSD300 segmentation dataset showed that the proposed method achieves a Probabilistic Rand Index (PRI) of 0.83 and an F-measure of 0.77@(0.92, 0.68), and provides improved handling of color and luminance variation, as well as strong segmentation performance for images with highly textured regions when compared to a number of previous methods. These results suggest that the proposed stochastic regional texture appearance model is better suited for handling the texture variations of natural scenes, leading to more accurate segmentations, particularly in situations characterized by complex textural characteristics.
               
            

@&#INTRODUCTION@&#

Image segmentation is one of the most challenging problems in computer vision, and deals with the partitioning of an image into a set of disjoint segments (or regions), in a way that pixels with similar characteristics are grouped together in homogeneous segments that could be associated with different objects.

Different approaches have been proposed to measure pixel similarity using image characteristics, such as intensity, color, texture, or other features. The similarity criterion to be used generally depends on the context, as well as on the desired details in the image representation.

Texture is one of the most discriminative characteristics for evaluating similarity in image segmentation [1,2]. The main idea of texture is that image features can be better expressed by adjacent groups of pixels instead of isolated ones. In a more formal definition, a texture is a visual pattern (deterministic or stochastic) repeated over some area, in which the basic elements are called texture primitives or textons 
                     [3]. Next, we review some representative methods related to texture feature extraction, representation and region aggregation.

Texture features may be seen as local variations across the image [4–6]. Such variations may be captured by Gabor filters [7], wavelet decomposition [8,9], and combinations of directional filters [10–14]. These approaches may produce accurate texture boundary information for a variety of applications, but accurately representing large regions in an image or handling intra-class texture variability remains challenging.

Texture features can be learned in a way to reduce the number of features while improving class separability. For example, SIFT features and optimal spaces may be learned [15], or convolutional neural networks may be used for learning optimal features [16]. Despite the good performance of the aforementioned techniques in some contexts, supervised training is required to learn adequate parameters, which limits their applicability.

A comprehensive set of local image statistics (i.e. within small adjacent pixel neighborhoods) can be used for representing textures [11,12,17]. However, pixel-wise statistics may not capture the long range correlations existing in some texture classes (e.g. natural scene textures).

Recent research have addressed the apparent paradox of pixel-wise and region-wise texture representations [18–20], and image patches (i.e. small neighborhoods around pixels) have been proposed as feature vectors. This simple approach potentially can be effective to improve texture class separability [21]. Actually, patch features usually are obtained in a supervised manner, and can be made compact and rotationally invariant [19]. The potential of unsupervised schemes for patch features extraction remains largely unexplored, and is one of the contributions of this paper.

Unsupervised image segmentation often relies on clustering texture features by maximizing/minimizing some energy or cost function [22,23]. In this case, most pixel pairs should be considered, but that may be expensive. Alternatively, sparse representation strategies like bag-of-features (BoF) may be used, which involves obtaining a textural feature occurrence histogram [24] by clustering feature vectors (e.g. using K-means [18] or K-SVD [25]) or by identifying textural feature prototypes (e.g. using the nearest neighbor rule or L
                     1-type optimization [15]). The BoF approach can benefit by using many types of features [18,19,24–26], and later we present a new stochastic BoF scheme for unsupervised image segmentation.

In order to compare image segments and obtain an image segmentation, the affinity among segments must be computed. Some methods model affinities and use graph cuts to group segments, obtaining the final image segmentation [22,23]. Alternatively, clustering algorithm may group adjacent segments using spatial constraints [20,27]. Recent research indicate that statistical criteria and simple features such as intensity or color with pixel-wise region initialization can improve segment merging and image segmentation [28–30]. As proposed in [28], adjacent region merging can be based on statistical criteria to discriminate regions sizes and intensities/colors. This concept was later extended in [29] to use a stochastic criteria based on the similarity likelihood of the image regions.

The methods mentioned above perform on single scale image representations. However, Deng and Manjunath [10] suggested that multi-scale region growing, were iterative region growing algorithms are used to segment the image in decreasing level of detail (analogous to multi-scale methods) may be advantageous in image segmentation. In fact, using similarity likelihood of multi-scale regional descriptors can improve on those methods results, as discussed later in this work.

We propose a novel image segmentation method that combines the benefits of discriminative texture features and of region merging-based segmentation approaches. This work introduces a new stochastic regional texture appearance model and a stochastic region-merging segmentation framework. The addition of stochastic elements in these process will better account for intra-class uncertainties as well as for color and illumination variations across the image. Specifically, this paper contributes in the following aspects:

                        
                           1.
                           A bilateral scale space image decomposition technique is presented for the purpose of feature modeling, which will split the image details in levels of granularity (from coarse to fine).

An innovative way of combining the bag-of-features and region merging strategies is presented, allowing a smooth transition from the pixel-wise features to region-wise representation as the regions grow in the region merging stage.

By combining Stochastic Patch Features (SPF) and stochastic region merging, a new unsupervised image scheme is proposed that potentially can provide better results than available methods representative of the state-of-the-art.

The proposed image segmentation technique has three main stages (as shown in Fig. 1
                     
                     ): (i) feature extraction, (ii) texture description, and (iii) region merging. In the first stage, we aim at obtaining the low-level texture information at each point in the image. To do so, we employ a multi-scale bilateral scale space image decomposition strategy (to enhance the separability between coarse and fine textures). From this decomposition, patch features are extracted to construct a set of Stochastic Patch Features (SPF) via random projections. In the texture description stage, the SPF are used to produce a regional texture appearance representation. We construct a feature dictionary (from the SPF) to compose this appearance representation, and choose a bag-of-words technique to describe the segments in the image. The final stage of the proposed method is responsible for detecting the boundaries between texture regions in the input image. In here we employ an iterative region merging segmentation strategy, which uses the previously computed stochastic regional texture appearance representation.

The remaining of this paper is organized as follows. Section 2 presents the proposed multi-scale feature extraction process to obtain the SPF representation. Section 3 describes the proposed region-wise texture appearance representation for characterizing texture properties for regions of arbitrary size and shape. Section 4 details the stochastic region merging algorithm based used to aggregate the pixels into texture regions. In Section 5, we present and discuss experimental results, as well as a comparison of our results with several state-of-the-art segmentation methods. Finally, in the Section 6, conclusions and future work are discussed.

Let I be the input image which we desire to segmented, and consider that it is a 3-channel color image represented in the CIE L*a*b* color space, so there is minimal correlation between the luminance and color channels. The texture features proposed for representing the textural information at each point in the input image are obtained by applying three key steps. The first task is to represent I in a multi-scale manner, where fine to coarse details are decoupled. This is done via an iterative bilateral scale space decomposition. The second task is to extract the patch multi-scale features from the image. The final task of feature extraction is to perform dimensionality reduction in a way that is robust to texture rotation and translation, which is accomplished here via random projections. The pipeline of actions performed to accomplish these tasks can be seen in Fig. 2.

In the proposed work, we wish to not only identify the texture characteristics of the image in matters of luminance and color channels (e.g., a set of texture features for each of the CIE L*a*b* channels), but also at different levels of details (e.g., texture features from coarse scale to fine scale). Therefore, to allow texture analysis and representation at many levels and to improve the discriminatory power of the extracted texture features, the image is decomposed into multiple scales based on the concept of bilateral iterative scale space [31]. The iterative bilateral scale space (BISS) was designed to decompose image details based not only on spatial locality, but also on photometric differences, resulting in a nonlinear multi-scale decomposition where image details at different scales are not only well separated, but also are well localized and well preserved.

Let c denote a channel in {L, a, b} color space. For a given image at a given channel fc
                        (j), where j denotes a pixel location, the multi-scale image decomposition f′
                           c, i
                         constructed using iterative bilateral scale-space can be defined by a family of derived images F′
                           c, i
                        ,

                           
                              (1)
                              
                                 
                                    
                                       
                                          
                                             f
                                             ′
                                          
                                       
                                       
                                          c
                                          ,
                                          i
                                       
                                    
                                    
                                       (
                                       j
                                       )
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                q
                                                ∈
                                                N
                                             
                                          
                                          
                                             
                                                w
                                                p
                                             
                                             (
                                             
                                                j
                                                ,
                                                
                                                   N
                                                   q
                                                
                                             
                                             )
                                             
                                                w
                                                s
                                             
                                             (
                                             
                                                j
                                                ,
                                                
                                                   N
                                                   q
                                                
                                             
                                             )
                                             
                                                
                                                   
                                                      f
                                                      ′
                                                   
                                                
                                                
                                                   c
                                                   ,
                                                   i
                                                   −
                                                   1
                                                
                                             
                                             
                                                (
                                                q
                                                )
                                             
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                q
                                                ∈
                                                N
                                             
                                          
                                          
                                             
                                                w
                                                p
                                             
                                             (
                                             
                                                j
                                                ,
                                                
                                                   N
                                                   q
                                                
                                             
                                             )
                                             
                                                w
                                                s
                                             
                                             (
                                             j
                                             ,
                                             
                                                N
                                                q
                                             
                                             )
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        where 
                           
                              
                                 
                                    
                                       f
                                       ′
                                    
                                 
                                 
                                    c
                                    ,
                                    0
                                 
                              
                              =
                              
                                 f
                                 c
                              
                              ,
                           
                         
                        i ≤ N denotes scale, 
                           
                              N
                              q
                           
                         defines the pixel location within a local neighborhood 
                           
                              N
                              ,
                           
                         and wp
                         and ws
                         denote Gaussian photometric and spatial weights on j, respectively,

                           
                              (2)
                              
                                 
                                    
                                       
                                          
                                             
                                                w
                                                p
                                             
                                             (
                                             j
                                             ,
                                             N
                                             )
                                             =
                                             exp
                                             
                                                [
                                                
                                                   −
                                                   
                                                      1
                                                      2
                                                   
                                                   
                                                      
                                                         (
                                                         
                                                            
                                                               ∥
                                                               
                                                                  
                                                                     
                                                                        
                                                                           f
                                                                           ′
                                                                        
                                                                     
                                                                     
                                                                        c
                                                                        ,
                                                                        i
                                                                        −
                                                                        1
                                                                     
                                                                  
                                                                  
                                                                     (
                                                                     
                                                                        x
                                                                        ̲
                                                                     
                                                                     )
                                                                  
                                                                  −
                                                                  
                                                                     
                                                                        
                                                                           f
                                                                           ′
                                                                        
                                                                     
                                                                     
                                                                        c
                                                                        ,
                                                                        i
                                                                        −
                                                                        1
                                                                     
                                                                  
                                                                  
                                                                     (
                                                                     N
                                                                     )
                                                                  
                                                               
                                                               ∥
                                                            
                                                            
                                                               σ
                                                               p
                                                            
                                                         
                                                         )
                                                      
                                                      2
                                                   
                                                
                                                ]
                                             
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (3)
                              
                                 
                                    
                                       
                                          
                                             
                                                w
                                                s
                                             
                                             (
                                             j
                                             ,
                                             N
                                             )
                                             =
                                             exp
                                             
                                                [
                                                
                                                   −
                                                   
                                                      1
                                                      2
                                                   
                                                   
                                                      
                                                         (
                                                         
                                                            
                                                               ∥
                                                               
                                                                  j
                                                                  −
                                                                  N
                                                               
                                                               ∥
                                                            
                                                            
                                                               σ
                                                               s
                                                            
                                                         
                                                         )
                                                      
                                                      2
                                                   
                                                
                                                ]
                                             
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        where, σp
                         e σs
                         are the standard deviation for the Gaussian model adopted in the photometric and spatial weights, respectively.

Thus, while ws
                         indicates a traditional Gaussian blurring kernel (considering a pixel’s spatial proximity to the central pixel), wp
                         represents an intensity disparity between the neighborhood pixels and the central pixel (i.e., photometric differences). The combination of these two weight functions in Eq. (1) results in an image decomposition where the image representation becomes less detailed with each increasing level (because of the spatial weights), but will also retain edge fidelity and locality (because of the photometric weights).

Given the aforementioned multi-scale image decomposition, we wish to extract texture features and construct descriptors at each scale independently, so that each scale will provide a unique texture description.

While features such as pixel intensity and color provide useful information about the edges (large gradients) within a picture, such information may not suffice for the purpose of automatic image segmentation, particularly when dealing with highly textured regions characterized by large local intensity variances. Achieving a better segmentation in these cases requires the use of more sophisticated texture features. Although there does not exist an universally accepted definition of texture, the main idea is that an identifiable local pattern repeats in some area, and the elements defining this pattern are called texture primitives (or textons) [1,3], that may be either periodic or stochastic.

Some texture representation approaches represent texture patterns using local differences like gradients or Gaussian derivatives [17,18,32]. These texture features detect changes in brightness or color, so by computing these differences in several directions and scales, such as in Gabor filters or wavelet transforms, robust texture features are obtained. From the convolutions used to generate these local differences, Varma and Zisserman [21] observed that these texture features essentially are lower-dimensional projections of an image patch (i.e., a small neighborhood around a pixel). Therefore, instead of computing pixel to pixel differences as texture characteristics, our proposal is to directly use image patches as feature vectors. Such an approach have been proved to be as effective, or even more accurate than the gradients [19], with the advantage of a simple feature extraction process[21].

In this work, the patch features are extracted from the bilateral scale space image decomposition F′. For a given pixel j, we define the patch feature vectors 
                           
                              
                                 N
                                 
                                    c
                                    ,
                                    i
                                 
                              
                              
                                 (
                                 j
                                 )
                              
                           
                         as lexicographically ordered vector representation of a square neighborhood of width 
                           
                              N
                              s
                           
                         centered at j, in the scale f′
                           c, i
                         of the bilateral scale space decomposition. Observe that, for each pixel, a feature vector is extracted from each combination of the neighboring pixels colors and decomposition level (see Fig. 2).

Although simply taking an image patch around each pixel as a feature vector provides a good texture representation, it still can be improved [19,33] for efficiency and robustness purposes. Here, we wish to improve the proposed features in two aspects: (i) invariance in the representation of the rotated textures, and (ii) dimensionality reduction.

The patch features are, by definition, invariant to translation over the image, but since such features depend essentially on the spatial relationship (distance and angle) between neighboring pixels, they tend to become very sensitive to image rotations.

Previous works on the literature have proposed to introduce this property by adding rotated patches to the texture model formulation, or by estimating the dominant gradient orientation in each texture patch [21], but in practice, these methods still present some problems [34]. The dominant orientation estimate tend to be unreliable for some texture regions, and the patch rotations make the feature clustering more expensive. In this work, rotation invariance is achieved by sorting the values of the patch feature vectors

                           
                              (4)
                              
                                 
                                    
                                       N
                                       
                                          c
                                          ,
                                          i
                                       
                                       ′
                                    
                                    
                                       (
                                       j
                                       )
                                    
                                    =
                                    s
                                    o
                                    r
                                    t
                                    
                                       (
                                       
                                          N
                                          
                                             c
                                             ,
                                             i
                                          
                                       
                                       
                                          (
                                          j
                                          )
                                       
                                       )
                                    
                                    ,
                                 
                              
                           
                        where, sort denotes the sorting operation, and 
                           
                              
                                 N
                                 
                                    c
                                    ,
                                    i
                                 
                                 ′
                              
                              
                                 (
                                 j
                                 )
                              
                           
                         is the sorted patch vector for pixel j. As the sorting result is independent of the original positioning of elements inside the patch 
                           
                              
                                 N
                                 
                                    c
                                    ,
                                    i
                                 
                              
                              
                                 (
                                 j
                                 )
                              
                              ,
                           
                         the sorted features 
                           
                              
                                 N
                                 
                                    c
                                    ,
                                    i
                                 
                                 ′
                              
                              
                                 (
                                 j
                                 )
                              
                           
                         will be invariant to changes in texture rotation.

Another important aspect that needs to be considered is the size of the feature vector. Considering a 3-channel color image I, represented in N levels of the BISS, and sorted features extracted from image patches with width w, each pixel will be represented by 
                           
                              3
                              
                              ×
                              
                              N
                              
                              ×
                              
                              
                                 
                                    (
                                    
                                       N
                                       s
                                    
                                    )
                                 
                                 2
                              
                           
                         features (3N vectors of size 
                           
                              
                                 (
                                 
                                    N
                                    s
                                 
                                 )
                              
                              2
                           
                        ). In a numerical example, using 
                           
                              N
                              
                              =
                              
                              3
                           
                         levels and 3 × 3 patches, each pixel will have 81 features; using 
                           
                              N
                              
                              =
                              
                              4
                           
                         levels and 5 × 5 patches will produce 300 features per pixel. This happens because the number of features grow linearly as N increases, but it grows exponentially in relation to w.

Using too many features in unsupervised classification tasks (such as image segmentation) is known to harm the final result due to the lack of information of the feature space, and to the noisy information that is prone to happen in some features. To avoid these complications, dimensionality reduction may be applied to the proposed features. Ideally, a dimensionality reduction should be able to capture the most relevant information in the analyzed data, while decreasing the influence of irrelevant and incoherent channels. In this work, we employ the concept of random projections [19], which is able to efficiently capture the relevant texture information contained in the multiple texture patches.

A random projection (RP) is a simple technique for reducing the dimension of sparse data (i.e., containing few non-zero values), which efficiently captures the most salient information in the source data. The RP works as a linear projection defined by a random matrix 
                           
                              Φ
                              ∈
                              
                                 R
                                 
                                    m
                                    ×
                                    n
                                 
                              
                              ,
                           
                         with m ≪ n. Ideally, this projection must ensure that discriminative information in the original signal is preserved in the new representation, i.e., the distance between two vectors must be approximately the same before and after the projection [33]. It was shown by [35,36] that information preservation is ensured if the projection matrix 
                           
                              Φ
                              =
                              {
                              
                                 ϕ
                                 
                                    
                                       M
                                       i
                                    
                                    
                                       M
                                       j
                                    
                                 
                              
                              }
                           
                         (with 
                           
                              1
                              ≤
                              
                                 M
                                 i
                              
                              ≤
                              m
                           
                         and 
                           
                              1
                              ≤
                              
                                 M
                                 j
                              
                              ≤
                              n
                           
                        ) is defined in a random, stochastic manner as

                           
                              
                                 
                                    
                                       ϕ
                                       
                                          
                                             M
                                             i
                                          
                                          
                                             M
                                             j
                                          
                                       
                                    
                                    =
                                    
                                       {
                                       
                                          
                                             
                                                0
                                             
                                             
                                                
                                                   
                                                      
                                                      with
                                                      
                                                      probability
                                                      
                                                      
                                                         1
                                                         2
                                                      
                                                      ,
                                                   
                                                
                                             
                                          
                                          
                                             
                                                1
                                             
                                             
                                                
                                                   
                                                      
                                                      with
                                                      
                                                      probability
                                                      
                                                      
                                                         1
                                                         2
                                                      
                                                      .
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Random projection (RP) tends to compress sparse vectors while improving the discrimination of vector clusters in the obtained lower dimensionality space [37–39]. As shown in previous work in texture representation [19], an image patch can be seen as a sparse vector. Therefore, it can be represented by RP with reduced dimensionality so image patches clusters are more easily discriminated in feature space (see Section 5). Using the formulation proposed above, we define a stochastic texture representation (STR) v
                        
                           c, i
                        (j) for the proposed features as

                           
                              (5)
                              
                                 
                                    
                                       v
                                       
                                          c
                                          ,
                                          i
                                       
                                    
                                    
                                       (
                                       j
                                       )
                                    
                                    =
                                    Φ
                                    
                                       N
                                       
                                          c
                                          ,
                                          i
                                       
                                       ′
                                    
                                    
                                       (
                                       j
                                       )
                                    
                                    =
                                    Φ
                                    s
                                    o
                                    r
                                    t
                                    
                                       (
                                       
                                          N
                                          
                                             c
                                             ,
                                             i
                                          
                                       
                                       
                                          (
                                          j
                                          )
                                       
                                       )
                                    
                                    ,
                                 
                              
                           
                        where j denotes the pixel location, c the color channel, and i the scale. A practical example of how the RP is applied to the patches is shown in Fig. 3
                        . In this example, a RP matrix (Fig. 3(a)) is stochastically generated as shown in Fig. 3(d), and applied to a sorted patch vector (Fig. 3(b)), producing a projected vector (Fig. 3(e)), which will be the STR feature vector (Fig. 3(c)). It shall be observed that the RM matrices are initialized randomly (using random seeds). Therefore, if the same image patch occurs in distinct image locations, different STR vectors are generated to represent this patch (because of the non-deterministic nature of the stochastic process). However, these distinct STR vectors shall be similar and assigned to the same cluster.

In this fashion we are able to produce a more efficient set of features than the original image patches. Not only are the STR features invariant to the texture rotation, they also have a much lower dimension than before, more robust to the presence of noise, and tend to improve data separability.

Also, the RP matrixes are obtained with a pseudo-random number generator (initialized with pseudo-random seeds), so if the same patch vector occurs in different parts of the image, different STR vectors may be obtained for this patch. But, because of the inherent nature of a stochastic process, the final result will not change significatively by using different seeds.

It may be observed that the multi-scale texture representation used in this work provides local statistics of multiple orders, and the texture is described in multiple levels of details. However, since different statistics are captured when scale changes, our STR features remain relatively sensitive to the size of the texture primitives. This implies that even if multiple order statistics are captured, the texture representation is not scale invariant, and will require a customized setup for segmenting textures at different scales.

As discussed earlier, while simple features such as intensity and color variation may provide sufficient information about the edges of a picture in a general way, they are limited in their usefulness when characterizing the boundaries of highly textured objects. More complex texture features, such as multiple gradients and image patches (including the proposed STR features) can account for such situations, since these local features will capture patterns that are indefinable in the small area of the neighborhood around a pixel. While increasing the size of the image patches would allow the recognition of bigger texture primitives, it would harm the separation between different textures with smaller primitives. Therefore, rather than just taking local features, more comprehensive texture descriptors need to be used to achieve a more precise segmentation by representing not only local features, but also global appearance characteristics [19]. Moreover, since the goal of this paper is to perform reliable segmentation of texture regions within an image, we also need to define a representation for the texture information within a region, which may have many pixels and shall have and unknown arbitrary shape. Motivated by this, one of the key aims of this paper is to introduce a robust multi-scale regional texture appearance model for characterizing textural properties at multiple levels, within a region of arbitrary size or shape.

By the previous definition of the texture primitives, we can consider a texture to be a pattern, stochastic or periodic, that is repeated over some area. Under such consideration the textural information in a region can be described by the statistics of the features occurring in that region. In this work, we introduce a bag-of-features strategy for constructing regional texture appearance models, and therefore describe the textures with the assistance of a dictionary of textons (texture primitives) [19,21].

The bag-of-features strategy aims to represent a feature space with the aid of a finite and well defined set of feature prototypes, which are usually obtained by clustering the available samples. Different sets of data points in this feature space are then described by the histogram counts of all the prototypes. This general strategy has been studied for texture recognition in recent works [19], showing very promising results towards efficient and accurate description of a variety of textures, but remains insufficiently explored in image segmentation applications. In this work, we propose a novel strategy for combining such texture primitive dictionaries as a regional textural appearance model with region merging segmentation techniques.

Most of existing texture recognition methods work with bag-of-features cluster features separately for each known texture class, producing a set of prototypes (i.e., cluster centroids) that will be strongly related with each class. These prototypes are called textons, and the collection of all textons from all classes will be the so called dictionary of textons 
                     [18,40].

In this work, at a particular channel c and scale i, all of the extracted STR feature vectors are clustered using the k-means algorithm [41] to build a texton dictionary, where each texton is a found centroid, as can be seen in Fig. 4
                     , which illustrates the proposed texton dictionary construction method. Previous works on texture recognition via texton dictionaries have shown that dictionaries containing textons equally from all classes will allow a fair representation of all textures in the training set [21]. But, since this work aims to tackle the automatic unsupervised image segmentation problem, there is no information available about the number of texture classes in the image; therefore, the textons will be determined from samples of the whole image regardless of its texture class in building a global texture appearance model. Through the use of such an approach, we are supposing that different texture classes will form different clusters in the feature space, which is the main idea behind such clustering techniques. Also, different textures may have a different number of textons in the dictionary. Here, for the purpose of image segmentation, we intend to show through our experiments that, using a pre-defined number of texton prototypes (centroids), we can build a texton dictionary that can represent all the textural characteristics reliably and efficiently as a global texture appearance model (see Section 5).

Once the texton dictionary has been determined, a regional texture appearance descriptor T for an arbitrary texture region R can now be defined based on the texton occurrence probabilities at the different channels and scales

                        
                           
                              
                                 T
                                 
                                    (
                                    R
                                    )
                                 
                                 =
                                 {
                                 
                                    H
                                    
                                       c
                                       ,
                                       i
                                    
                                 
                                 
                                    (
                                    R
                                    )
                                 
                                 |
                                 1
                                 ≤
                                 c
                                 ≤
                                 3
                                 ,
                                 1
                                 ≤
                                 i
                                 ≤
                                 N
                                 }
                                 ,
                              
                           
                        
                     where, H
                     
                        c, i
                     (R) is the normalized histogram of texton occurrence (probability) in the region R for channel c and scale i. Since, we are dealing with 3-channel color images, represented in N levels of details of the bilateral scale space, a texture is now described by a set of 3N histograms, one for each combination of channel and scale. As such, the proposed regional texture appearance models accounts for color textural characteristics at multiple scales, and is suitable for regions of any shape or size.

Given that the proposed regional texture appearance model is composed of a set of normalized texton occurrence histograms, the dissimilarity between two regions may be computed quantitatively by comparing the set of texton occurence histograms between the regions. Estimating whether the two distributions differ, or are consistent, is a problem that arises frequently, and there is a variety of methods for solving it [42]. In this work, we use the Bhattacharyya distance for measuring the dissimilarity between two normalized texton occurrence histograms. This metric is robust, unbiased, and have become commonly used for the comparison of two probability density functions (PDF). As a normalized histogram can be seen as a discrete estimation of a PDF, the Bhattacharyya distance can be used to compute the dissimilarity between two normalized histograms 
                           
                              p
                              ^
                           
                         and 
                           
                              q
                              ^
                           
                         as

                           
                              (6)
                              
                                 
                                    
                                       D
                                       B
                                    
                                    
                                       (
                                       
                                          p
                                          ^
                                       
                                       ,
                                       
                                          q
                                          ^
                                       
                                       )
                                    
                                    =
                                    −
                                    l
                                    n
                                    
                                       (
                                       
                                          ∑
                                          i
                                       
                                       
                                          
                                             
                                                
                                                   p
                                                   ^
                                                
                                                i
                                             
                                             
                                                
                                                   q
                                                   ^
                                                
                                                i
                                             
                                          
                                       
                                       )
                                    
                                    .
                                 
                              
                           
                        
                     

Since, the texture information within a region is described by a set of histograms, each histogram must be normalized (prior to comparing the Bhattacharyya distance) by dividing each bin by the total number of the samples in that histogram, so every bin indicates the probability of a dictionary texton occurring in a given texture patch at that channel and scale.

Finally, the texture dissimilarity dT
                         between two regions Ra
                         and Rb
                         can be defined as:

                           
                              (7)
                              
                                 
                                    
                                       d
                                       T
                                    
                                    
                                       (
                                       
                                          R
                                          a
                                       
                                       ,
                                       
                                          R
                                          b
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          
                                             W
                                             g
                                          
                                          
                                             [
                                             
                                                
                                                   
                                                      
                                                         
                                                            ∑
                                                            
                                                               i
                                                               =
                                                               1
                                                            
                                                            N
                                                         
                                                         
                                                            D
                                                            B
                                                         
                                                         
                                                            (
                                                            
                                                               H
                                                               
                                                                  L
                                                                  ,
                                                                  i
                                                               
                                                            
                                                            
                                                               (
                                                               
                                                                  R
                                                                  a
                                                               
                                                               )
                                                            
                                                            ,
                                                            
                                                               H
                                                               
                                                                  L
                                                                  ,
                                                                  i
                                                               
                                                            
                                                            
                                                               (
                                                               
                                                                  R
                                                                  b
                                                               
                                                               )
                                                            
                                                            )
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            ∑
                                                            
                                                               i
                                                               =
                                                               1
                                                            
                                                            N
                                                         
                                                         
                                                            D
                                                            B
                                                         
                                                         
                                                            (
                                                            
                                                               H
                                                               
                                                                  a
                                                                  ,
                                                                  i
                                                               
                                                            
                                                            
                                                               (
                                                               
                                                                  R
                                                                  a
                                                               
                                                               )
                                                            
                                                            ,
                                                            
                                                               H
                                                               
                                                                  a
                                                                  ,
                                                                  i
                                                               
                                                            
                                                            
                                                               (
                                                               
                                                                  R
                                                                  b
                                                               
                                                               )
                                                            
                                                            )
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            ∑
                                                            
                                                               i
                                                               =
                                                               1
                                                            
                                                            N
                                                         
                                                         
                                                            D
                                                            B
                                                         
                                                         
                                                            (
                                                            
                                                               H
                                                               
                                                                  b
                                                                  ,
                                                                  i
                                                               
                                                            
                                                            
                                                               (
                                                               
                                                                  R
                                                                  a
                                                               
                                                               )
                                                            
                                                            ,
                                                            
                                                               H
                                                               
                                                                  b
                                                                  ,
                                                                  i
                                                               
                                                            
                                                            
                                                               (
                                                               
                                                                  R
                                                                  b
                                                               
                                                               )
                                                            
                                                            )
                                                         
                                                      
                                                   
                                                
                                             
                                             ]
                                          
                                       
                                       
                                          ∑
                                          
                                             W
                                             g
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        where 
                           
                              
                                 W
                                 g
                              
                              =
                              
                                 [
                                 
                                    w
                                    L
                                 
                                 ,
                                 
                                    w
                                    a
                                 
                                 ,
                                 
                                    w
                                    b
                                 
                                 ]
                              
                           
                         is a weight given to each channel, which will determine the contribution of each channel in the segmentation result (for more details see Section 5).

Once the STR features have been extracted from all pixels in the image, and the multi-scale regional texture appearance descriptors have been properly defined for all regions within the image, the segmentation process is ready to begin. In this section, we focus on defining the procedures involved in the proposed region merging technique, and how they will integrate with the regional texture appearance model for identifying the boundaries between adjacent textures of the input image.

Let J be the discrete lattice where the image is defined, and j ∈ J be a site in this lattice, referring to a pixel location. Also, let 
                           
                              
                                 F
                                 j
                                 P
                              
                              =
                              
                                 {
                                 
                                    f
                                    j
                                 
                                 |
                                 j
                                 ∈
                                 J
                                 }
                              
                           
                         be the texture observed at the image site j, and 
                           
                              R
                              =
                              {
                              
                                 r
                                 s
                              
                              |
                              s
                              ∈
                              S
                              }
                           
                         a texture region label field. The image segmentation can be formulated as a maximum a Posteriori (MAP) estimation problem [29,43]
                        
                           
                              (8)
                              
                                 
                                    
                                       r
                                       ^
                                    
                                    =
                                    
                                       arg max
                                       r
                                    
                                    
                                       {
                                       P
                                       
                                          (
                                          r
                                          |
                                          f
                                          )
                                       
                                       }
                                    
                                    ,
                                 
                              
                           
                        where P(r|f) is the posterior. From the Bayes theorem, solving Eq. (8) is equivalent to solving

                           
                              (9)
                              
                                 
                                    
                                       r
                                       ^
                                    
                                    =
                                    
                                       arg max
                                       r
                                    
                                    
                                       {
                                       P
                                       
                                          (
                                          f
                                          |
                                          r
                                          )
                                       
                                       P
                                       
                                          (
                                          r
                                          )
                                       
                                       }
                                    
                                    ,
                                 
                              
                           
                        where P(f|r) is the likelihood, and P(r) is the label prior. Although this is a simple formulation for this problem, it may be difficult to solve directly due to terms P(r) and P(f|r), that are generally unknown.

The estimation of 
                           
                              r
                              ^
                           
                         is approached as a region merging (RM) problem, as explained next. Conditional random fields (CRF) or Markov Random Fields (MRF) [44] are commonly used for representing relations between pixels. Also, graph cuts can be used to maximize the probabilities in Eq. 8 
                        [45], however this method requires assembling a pixel-wise similarity matrix, which may be quite expensive. We solve the MAP problem in Eq. 8 iteratively, bottom-up, using a stochastic RM strategy, providing an approximate solution at a reasonable cost. The motivation for using this bottom-up strategy is to solve the labeling problem iteratively, by combining MRF-like estimates and region merging. In this way, the locality of the MAP inference is reduced to small pixel neighborhoods, favoring pixel-wise similarities [43]. The a priori probability P(r) is estimated in way that favors pixel-wise similarities [46], and the likelihood P(f|r) is approximated by a merging likelihood function that favors increasing patch-texture class similarity.

More precisely, an approach similar to MRF is used to represent the pixel-to-pixel relations as an adjacency graph, accounting for P(r), and the merge criterion will account for P(f|r), so successively merging the nodes in this graph will maximize the class likelihood. To maintain the process consistent and make it more robust to local variations (e.g. local variations of luminance, color, and noise or artifacts in highly textured regions), the pairs of texture regions are sorted by ascendant dissimilarity. Finally, the segmentation is refined by an iterative merging process.

As mentioned above, one effective way of handling local variations in the image is to model the prior probability P(r) [46] as in MRF. In the proposed approach the prior probability P(r) represents interactions between neighboring pixels, i.e. the probability of neighboring pixels belonging to the same class.

In the beginning of the texture segmentation of a given input image I, with SN
                         × SM
                         pixels, we assume no prior knowledge and make no assumptions about the texture regions in the image, so there is no information on how many distinct texture regions are in the image. So, before we successively aggregate pixels forming larger texture regions, we assign a unique texture region label 
                           
                              R
                              =
                              {
                              1
                              ,
                              ⋯
                              ,
                              
                                 S
                                 N
                              
                              ×
                              
                                 S
                                 M
                              
                              }
                           
                         to each pixel.

In the initial stage of the segmentation process, we build a region adjacency graph 
                           
                              G
                              =
                              (
                              R
                              ,
                              E
                              )
                           
                         representing its current status. In this graph, each vertex represents a region R and the edges E connecting the vertices correspond to dissimilarities between neighboring regions. In the initial stage of the segmentation, G is set up to represent each pixel p as an unique region (vertex), and the edges are the 4-neighborhood of the pixels. The aforementioned scheme for adjacency graph initialization is represented in Fig. 5. At any subsequent state of the segmentation process, each vertex (texture region) may have an arbitrary number of pixels, and be connected to a different number of other vertexes (adjacent texture regions, which may be different of 4).

Once the initial state of the adjacency graph have been set up, the algorithm proposed here begins by sequentially analyzing E and merge edges. Two adjacent regions Rx
                         and Ry
                         are merged with a probability of α(Rx, Ry
                        ), which accounts for P(f|r) in Eq. (9), and is obtained from a likelihood function. In this work, we use a texture region likelihood function that extends upon the stochastic region merging criterion proposed by Wong et al. [29]
                        
                           
                              (10)
                              
                                 
                                    α
                                    
                                       (
                                       
                                          R
                                          x
                                       
                                       ,
                                       
                                          R
                                          y
                                       
                                       )
                                    
                                    =
                                    exp
                                    
                                       [
                                       −
                                       
                                          
                                             
                                                d
                                                T
                                             
                                             
                                                (
                                                
                                                   R
                                                   x
                                                
                                                ,
                                                
                                                   R
                                                   y
                                                
                                                )
                                             
                                          
                                          
                                             Λ
                                             (
                                             
                                                R
                                                x
                                             
                                             ,
                                             
                                                R
                                                y
                                             
                                             )
                                          
                                       
                                       ]
                                    
                                    ,
                                 
                              
                           
                        where dT
                        (Rx, Ry
                        ) is the texture dissimilarity between Rx
                         and Ry
                        , defined in Eq 7, and Λ is a statistical merging penalty, based on the size of the regions, defined as

                           
                              (11)
                              
                                 
                                    Λ
                                    
                                       (
                                       
                                          R
                                          x
                                       
                                       ,
                                       
                                          R
                                          y
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          D
                                          I
                                          2
                                       
                                       
                                          2
                                          Q
                                       
                                    
                                    
                                       [
                                       
                                          
                                             l
                                             n
                                             (
                                             Ψ
                                             
                                                
                                                   (
                                                   I
                                                   )
                                                
                                                2
                                             
                                             )
                                          
                                          
                                             Ψ
                                             (
                                             
                                                R
                                                a
                                             
                                             )
                                          
                                       
                                       +
                                       
                                          
                                             l
                                             n
                                             (
                                             Ψ
                                             
                                                
                                                   (
                                                   I
                                                   )
                                                
                                                2
                                             
                                             )
                                          
                                          
                                             Ψ
                                             (
                                             
                                                R
                                                b
                                             
                                             )
                                          
                                       
                                       ]
                                    
                                    ,
                                 
                              
                           
                        where Ψ(R) is the number of elements (pixels) in the region R, consequently Ψ(I) in the number of pixels in the image; DI
                         represents the range of possible values in I (usually 256), and finally Q is a regularization term, which controls the merging likelihood (see Section 5).

To determine if two regions should be merged, the likelihood function is compared with a random number u, with 
                           
                              P
                              (
                              u
                              )
                              =
                              unit
                              (
                              0
                              ,
                              1
                              )
                           
                        . The regions are merged if the likelihood function value satisfies the following predicate:

                           
                              (12)
                              
                                 
                                    P
                                    
                                       (
                                       
                                          R
                                          x
                                       
                                       ,
                                       
                                          R
                                          y
                                       
                                       )
                                    
                                    =
                                    
                                       {
                                       
                                          
                                             
                                                1
                                             
                                             
                                                
                                                   if
                                                   
                                                   u
                                                   ≤
                                                   α
                                                   (
                                                   
                                                      R
                                                      x
                                                   
                                                   ,
                                                   
                                                      R
                                                      y
                                                   
                                                   )
                                                   ,
                                                
                                             
                                          
                                          
                                             
                                                0
                                             
                                             
                                                
                                                   otherwise
                                                   
                                                   .
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Compared to the stochastic region merging proposed by Wong et.al. [29], the proposed merging criterion is able to handle texture features more robustly. The merging criterion proposed in [29] was limited to simpler features such as color and luminance, in which dissimilarity can be easily measured by the difference of expected value of the regions. With the formulation proposed in Eq. (10), the merging criterion is adequate for more sophisticated texture representations, such as the multi-scale regional texture appearance model proposed here.

During the merging process, whenever a pair of adjacent texture regions is analyzed, they are removed from the queue. Every time a pair is merged, the adjacency graph G is updated and the priority queue is modified to reflect these changes. Once, all the pairs of adjacent regions have been evaluated, the priority queue becomes empty, and the resulting adjacency graph will yield the initial segmentation result.

From the proposed image segmentation algorithm formulation, we can detail some proprieties and advantages that arise from employing the proposed stochastic multi-scale texture model jointly with stochastic texture region merging strategy. Texture representations of small regions, with few pixels, tend to be statistically poor, and texture regions of the same type may be regarded as dissimilar. However, using Eq. (10) the merging probability increases as the texture region size decreases, so the small regions have higher probability of being merged. This not only helps handling noise and artifacts, but also makes the process more robust to over-segmentation (as smaller regions segmented at the early stages of the segmentation process tend to be merged). Taking advantage of this characteristic and to help merging texture regions consistently, the adjacency graph edges are ordered in a priority queue. Each pair of adjacent regions is associated with a unique adjacency graph edge, and this priority queue is used in all merging tests, where the edges are placed in the decreasing order of their weights (i.e. similarities, as discussed next).

Since smaller regions are more likely to be merged, the order of the priority queue is crucial in the region merging process. In the initialization, each pixel j ∈ I is initially assigned to a unique region R in the adjacency graph, and region pairs are sorted based on their pixel-wise differences. In regions with just one pixel, the texture region description would be inaccurate for measuring the texture dissimilarity with Eq. (7), so we initialize the edge weights with the local image gradients. To incorporate some neighborhood information in the initial stage, we compute the gradients of the image multi-scale representation I′ as pixel-to-pixel differences. Since, this image representation describes each pixel by multiple order statistics, it produces a richer representation than the raw pixel values. Still, when computing the merging probability as the texture regions grow in size, the comparison between textons histograms of small regions cannot be avoided. To avoid the inaccuracy of an histogram representation of very small sets, whenever a region has less than 10 pixels, the texture model for that region is obtained from the same region dilated with an 8-neighborhood structure

On the other hand, the more similar two texture regions are the larger the value of α, promoting the homogeneity within the segmented texture regions as the segmentation process develops, even when regions are small. Also, the merging probability heavily depends on the regularization term. Larger values of Q tend to produce smaller α values, reducing the merging probability and increasing the number of texture segments found in the image. Yet, reducing the value of Q will increase the merging probability, consequently reducing the chance of obtaining over-segmentation.

After the priority queue has been emptied, the adjacency graph represents the texture segmentation of the image. To ensure a more reliable segmentation (less sensitive to variations in the stochastic factors), the segmentation result is refined by repeating the stochastic texture region merging process.

As illustrated by Fig. 6, the regions obtained in the first iteration are now used to build the new adjacency graph, and the new pairs of adjacent regions are inserted in the priority queue, which is sorted in the ascending order of the region dissimilarity. Differently from before, when the regions only had one pixel, the regions on the initialization map are larger, allowing the priority queue to be sorted according to the texture dissimilarity dT
                         of the regions pairs. The merging process is then repeated with a smaller regularization term. The value of Q is exponentially decreased in each iteration according to

                           
                              (13)
                              
                                 
                                    
                                       Q
                                       k
                                    
                                    =
                                    
                                       (
                                       Q
                                       −
                                       
                                          Q
                                          min
                                       
                                       )
                                    
                                    *
                                    exp
                                    
                                       (
                                       1
                                       −
                                       k
                                       )
                                    
                                    +
                                    
                                       Q
                                       min
                                    
                                    ,
                                 
                              
                           
                        where k ≥ 1 is the current iteration, 
                           
                              Q
                              
                              =
                              
                              
                                 Q
                                 1
                              
                           
                         is the value of the regularization term in the first iteration, 
                           
                              
                                 Q
                                 min
                              
                              
                              =
                              
                              200
                           
                         is the minimum value for this term, and Qk
                         is the value effectively used in the k
                        th iteration of the region merging process.

At each iteration, the number of regions is expected to be smaller than in the previous iteration, making the segmentation more accurate. These iterations are repeated until the segmentation of the image has converged, i.e., until a iteration produces no alterations in the adjacency graph (in our experiments, we also limit the process to 10 iterations as empirical results showed that it provides strong results). With these iterations we can avoid under-segmentation by setting a higher value for Q, with a lower risk of over-segmentation.


                        Fig. 7 shows the partial results on each iteration of the proposed segmentation technique when applied to an example image.

@&#EXPERIMENTAL RESULTS@&#

To evaluate the quality of the segmentation results obtained by our proposed segmentation method, experiments were conducted on the BSDS300 dataset [47]. This dataset is publicly available and was designed especially for comparison of texture segmentation methods. The BSDS300 image database consists of 300 natural color images, of size 481 × 321 pixels. This dataset contains 200 images that form a training set, and the remaining 100 images form the testing set. Recall that our approach do not have a training stage, so our experiments were applied only to the test images. Since in natural images the ideal texture segmentation can be very subjective, the BSDS300 includes a set with up to 10 handmade segmentations as ground truths for each image.

Using this dataset, we designed and performed a series of experiments for exploring the effects of the parameters of the proposed technique. The objective of such experiments is to determine how the proposed segmentation method described in this work behaves under different configurations. Finally, we also present a comparison between the proposed technique, and a number of representative segmentation methods from the literature, in terms of visual and quantitative segmentation quality.

It is well known that evaluating image segmentation results is a challenging task, since a quantitative quality measure for image segmentation may not fully reflect the effectiveness of the tested method in natural images. Therefore, to have a fair comparison of the different segmentation results, in this work we use the Probabilistic Rand Index (PRI) [12] and the F-measure (or F1-score) [11] for this task.

In these experiments, we aim at investigating the relationship between the parameters of the proposed method and how they affect segmentation quality. Recall that the proposed segmentation method can be configured by defining six parameters: (i) the number of decomposition levels N, (ii) the patch width 
                           
                              
                                 N
                                 s
                              
                              ,
                           
                         (iii) the size of the random projection M, (iv) the number of textons K in the dictionary, (v) the regularization term Q, and (vi) the weight vector Wg
                        .

The first parameter is the number of scales in the bilateral image decomposition, N. Increasing the number of scales allows more comparisons between neighboring pixels, and therefore of neighbor patches. As can be seen in Table 1
                        , while using a few levels will enhance the detection of texture boundaries, a configuration with too many decomposition scales will harm the segmentation, causing under segmentation.

The second parameter, 
                           
                              
                                 N
                                 s
                              
                              ,
                           
                         is the patch width, and it is responsible for controlling the size of the primitives. Even though there is an additional step for creating a global representation of the texture regions appearance, it will be built over the statistics of the STR patch features, so they need to be properly extracted to achieve a good representation. Since the size of the patches is related to the size of the texture primitives, increasing 
                           
                              N
                              s
                           
                         will generate a more coarse analysis of the image, and therefore will eliminate weaker texture boundaries (similar appearance textures in adjacent regions). On the other hand, using too small patches will generate a more detailed analysis, and enforce the creation of weaker texture boundaries.

As shown in Table 2
                        , our experiments have revealed that using smaller patches produce a better segmentation result. This is caused by the integration of the texton dictionary approach for constructing regional texture appearance models, where the smaller patches can reach a more precise texture description due to its better feature space allocation. Also, in accordance to previous works on patch features for texture recognition, the use of very small patches will, in many cases, be enough to achieve the best texture representation [19,21]. In particular, by setting the appropriate number of textons, a better representation of the texture features space will be achieved.

The third parameter to be set is the size of the random projection, defined by M, which is the number of dimensions of the STR features. While an adequate number of dimensions will reduce the computation complexity as well as enhance the texture features representation, a wrongly chosen value will harm the segmentation quality. Using fewer dimensions than the ideal will not preserve enough information about the different textures. On the other hand, using too much features will not efficiently reduce the complexity of the task, and neither will produce the good separability of the data. In our experiments, we have applied different sizes of the random projection, and as shown in Fig. 8
                        (a) the
                         best configuration of M is related to the size of the original patch features.

As discussed in Section 3, the ideal size of the texton dictionary depends on the number of textures classes in the problem (texture images to be segmented). An accurate texture representation usually requires a large number K of textons in the dictionary [48], especially when we have no prior knowledge about the number of textures in the image. However, as K increases, we face higher segmentation complexity.

In fact, our regional texture descriptor does not use textons directly to represent texture regions; instead we use the occurrence probability of all dictionary textons to represent a texture sample. In this way, even if a texture region shares textons with other texture regions, or if it is represented by few textons, this representation still can efficiently discriminate the texture samples as long as the dictionary textons occurrence is not identical in both samples. Therefore, the choice of a value for K should provide a balance between the texture representation quantization error (inversely proportional to K), and the computational cost of a larger texton dictionary (proportional to K). In this work, we have verified the behavior of our method with K ∈ [50; 200]. We have verified that building large dictionaries will reduce the quantization error when computing the texton occurrence histograms. On the other hand, as shown in Fig.8(b), using too many textons may decrease the overall PRI. Since the extra textons are considered not associated exclusively with any texture class, it will reflect in the histogram comparison and the merging probability, therefore causing minor variations on the segmentation results, and a small increase in the number of regions found.

While the previous parameters are directly responsible for determining the precision of the texture representation, the last two parameters control the segmentation (region merging step): (a) the weight vector Wg
                        , and (b) the regularization term Q. The results comparing these experiments are illustred in Figs. 9 and 10.

As can be seen in Fig. 9, the weight vector Wg
                         provides a balance between color and luminance features, and changes in Wg
                         affects how much of the color or luminance information are taken into account in the similarity measurement. In practice, the results of changing values in Wg
                         has an impact on the segmentation, and merging may (or may not) occur depending on how this parameter is set. The experiments realized here explored each 
                           
                              
                                 w
                                 g
                                 c
                              
                              ∈
                              
                                 [
                                 1
                                 ,
                                 2
                                 ]
                              
                           
                        .

As mentioned in Section 4.3, the regularization term Q affects the final number of segmented texture regions. Increasing the value of Q reduces the values of α in Eq. (10), which decreases the merging probability and leads to finding more regions in the final segmentation. On the other hand, lower values of Q will have the opposite effect on the image segmentation, generating fewer segmented regions.

The proposed experiments have demonstrated the segmentation behavior for Q ∈ [200; 800], and Fig. 10 illustrated this discussion with some of these results. The iterations of the stochastic texture region merging method as Q decreases contributes for the segmentation convergence (see Eq. (13)). To obtain a larger number of regions in the final segmentation, larger Q values are needed. Although we can clearly see that this parameter is directly linked to the number of regions in the image (what means that a segmentation can be fine tunned with some specific value), our experiments demonstrates that a default value of Q can be found for the whole dataset without jeopardize the PRI and F-measure.

In this section, we compare the segmentation results produced by the proposed technique with the segmentation results obtained by other state-of-art methods, such as color texture measurement segmentation (CTMS) [7], JSEG [49], CTex [14], hierarchical contours detections (HCD) [12], modal energy image segmentation (MIS) [13], label field fusion (LFF) [27], image segmentation based on cascade classifiers and region aggregation [20], and stochastic region merging (SRM) [29]. Although SRM does not use texture information in image segmentation, we included this method in our comparison because our region-merging approach is an extension of SRM, and because we are interested in state-of-art methods for color image segmentation and region merging algorithms. We also include the stochastic merging of textured regions (SMTR) [30], to have a fair analysis of the improvements brought on by the proposed method over our previous work. In these comparisons, we used the parameters specified in the respective literature. Our evaluation criterion uses the PRI described in [12] and the F-measure described in [11].

We show the overall performance comparison for the BDSD300 dataset in Table 3
                        
                        . These quantitative results show that the texture segmentation method proposed in this paper tends to provide a better performance (PRI
                           
                              =
                              0.83
                              
                                 −
                                 +
                              
                              0.01
                           
                         and 
                           
                              F
                              =
                              0.77
                           
                        @(0.92, 0.68)) in comparison to state of the art methods. This performance was achieved with the default parameter configuration of the proposed method: 
                           
                              N
                              =
                              1
                              ,
                           
                        
                        
                           
                              
                                 N
                                 s
                              
                              =
                              3
                              ,
                           
                        
                        
                           
                              M
                              =
                              5
                              ,
                           
                        
                        
                           
                              K
                              =
                              100
                              ,
                           
                        
                        
                           
                              Q
                              =
                              200
                           
                         and 
                           
                              
                                 W
                                 g
                              
                              =
                              
                                 [
                                 2
                                 ,
                                 1
                                 ,
                                 1
                                 ]
                              
                           
                        .

To demonstrate the effectiveness of the proposed STR features for segmentation, Table 3 also shows a comparison between the proposed feature extraction using random projections and principal component analysis (PCA) (as dimensionality reduction scheme), denoted as Proposed (RP) and Proposed (PCA), respectively. The experiments with PCA used the same range of parameters, where size of the projection M is replaced by the number of coefficients. The analysis of the PRI values in Table 3 suggest a small difference on segmentation quality, but the F-measure shows that using PCA leads to poorer segmentation results, with more fragmented regions.

Some visual results for the BDSD300 dataset can be found in Fig. 11. Visual comparison suggests that the proposed method tends to provide better results than comparable state of art color and texture segmentation methods. In some cases, the comparative methods tend to over-segment the textures with smooth color and luminance variations, or that are highly textured. For example, CTMS tends to incorrectly segment regions near edges or small regions, the JSEG and SRM tend to generate over-segmentation due to the variability of the local image gradients. The proposed method, on the other hand, efficiently deals with most textures in the BSDS300 test set.

This capability of robustly handling intra-texture variations can be attributed to some characteristics of our method. In the texture extraction stage, the combination of the multi-scale representations with the stochastic sorting of images patches to obtain the textures features, and the texton dictionary approach to represent the textures inside each region, provide robust representations of the textures (which becomes more precise as the texture regions grow in size). Also, stochastic texture region merging avoids local optima (minima or maxima), and sorting the priority queue according to regional differences at each iteration makes the merging process more reliable, especially for smaller regions.

In summary, the main advantage of the proposed segmentation over other methods like CTMS and JSEG is an increased robustness to local variations inside the texture regions. Compared to SRM, the use of a more advanced color texture feature model instead of just colors can provide a more adequate representation for the texture features, avoiding over-segmentation of the textures, and providing a better way to deal with local gradient variations inside the texture regions.

@&#CONCLUSIONS@&#

In this paper, we tackle the problem of texture image segmentation. Even though there is a large variety of techniques to solve this problem available in the literature, the use of stochastic texture models remain insufficiently explored, particularly when dealing with regional representations for image segmentation. In this work we introduce a novel stochastic, multi-scale approach for regional texture appearance modeling for the purpose of image segmentation.

Experimental results show that the proposed method tends to provide more accurate texture image segmentations than other methods previously proposed in literature (obtaining a PRI 
                        
                           =
                           0.83
                           
                              −
                              +
                           
                           0.01
                        
                      and 
                        
                           F
                           =
                           0.77
                        
                     @(0.92, 0.68) in our experiments). Visually, the proposed technique tends to be more robust to color and luminance variations inside the textures, as well is able to find the boundaries between similar textures, while avoiding over-segmentation in highly textured regions. Considering both visual and objective analysis and our comparative results, it becomes clear that stochastic texture models are better suited for handling the possible texture variations of natural scenes, and thus, lead to a more accurate segmentation.

Future work include investigating the effectiveness of the proposed technique for recognizing specific textures on natural scenes, as in skin detection, material recognition or skin lesion segmentation. Other possibilities include studying novel manners of contracting an optimal dictionary for image segmentation in an unsupervised manner.

@&#ACKNOWLEDGMENTS@&#

This work was supported by the Coordenaç ao de Aperfeiçoamento de Pessoal de Nível Superior (CAPES), Brazil, the Natural Sciences and Engineering Research Council of Canada, the Canada Research Chairs program, and the Ontario Ministry of Research and Innovation.

Supplementary material associated with this article can be found, in the online version, at 10.1016/j.cviu.2015.06.001
                  


                     
                        
                           
                        
                     
                  

@&#REFERENCES@&#

