@&#MAIN-TITLE@&#Extracting translations from comparable corpora for Cross-Language Information Retrieval using the language modeling framework

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Proposing a language modeling method to extract translations from comparable corpora.


                        
                        
                           
                           Comparing two similarity functions for deriving bilingual word correlations.


                        
                        
                           
                           Improving translation quality by integrating co-occurrence relations into word models.


                        
                        
                           
                           Comparing different estimations of translation probabilities from word correlations.


                        
                        
                           
                           Showing the significant impact of probability estimation methods on CLIR performance.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Translation model

Bilingual lexicon

Comparable corpora

Cross-Language Information Retrieval

Language modeling framework

@&#ABSTRACT@&#


               
               
                  A main challenge in Cross-Language Information Retrieval (CLIR) is to estimate a proper translation model from available translation resources, since translation quality directly affects the retrieval performance. Among different translation resources, we focus on obtaining translation models from comparable corpora, because they provide appropriate translations for both languages and domains with limited linguistic resources. In this paper, we employ a two-step approach to build an effective translation model from comparable corpora, without requiring any additional linguistic resources, for the CLIR task. In the first step, translations are extracted by deriving correlations between source–target word pairs. These correlations are used to estimate word translation probabilities in the second step. We propose a language modeling approach for the first step, where modeling based on probability distribution provides two key advantages. First, our approach can be tuned easier in comparison with heuristically adjusted previous work. Second, it provides a principled basis for integrating additional lexical and translational relations to improve the accuracy of translations from comparable corpora. As an indication, we integrate monolingual relations of word co-occurrences into the process of translation extraction, which helps to extract more reliable translations for low-frequency words in a comparable corpus. Experimental results on an English–Persian comparable corpus show that our method outperforms the previous approaches in terms of both translation quality and the performance of CLIR. Indeed, the proposed method is naturally applicable to any comparable corpus, regardless of its languages. In addition, we demonstrate the significant impact of word translation probabilities, estimated in the second step of our approach, on the performance of CLIR.
               
            

@&#INTRODUCTION@&#

Cross-Language Information Retrieval (CLIR) refers to the retrieval process where documents and queries are in different languages. Some sort of processing is thus needed to match query and document representations. The most general and promising approach for the CLIR task is to use translation resources. Bilingual corpora are widely used in CLIR to obtain statistical translation knowledge which addresses some translation issues such as out of vocabulary, neologism, and ambiguous words. Bilingual corpora are of two types: parallel and comparable. Aligned documents in a parallel corpus are literal translations of each other, while the ones in a comparable corpus cover the same or similar topics. Parallel corpora are generally better translation resources than comparable corpora because of more precise alignments. However, they are not widely available due to the higher cost of creation.

Comparable corpora are important translation resources for both languages and domains with limited linguistic resources. For language pairs with limited translation resources, on one hand, comparable corpora can be built at lower costs from more abundant sources of monolingual texts, such as articles from news agencies, compared to parallel corpora. Comparable corpora for such language pairs help to obtain initial translation knowledge which, in turn, facilitates building more exact and complete translation resources. For a domain with limited translation resources, on the other hand, an in-domain comparable corpus may provide more accurate translations compared to a general parallel corpus (Talvensaari et al., 2008). This is because, the domain of the constituting documents of bilingual corpora impacts the sense of extracted translations for words. Lower cost of building comparable corpora facilitates building several domain-specific corpora. Therefore, comparable corpora are also valuable resources for resource poor domains.

For the purpose of this paper, we focus on extracting translation knowledge from comparable corpora without employing additional linguistic resources. In this paper, we derive high-quality translation models from comparable corpora for CLIR through the following contributions:

                        
                           1.
                           We propose a language modeling approach to extract correlations between each pair of bilingual words from comparable corpora. The intuition behind the proposed method is that words that are translations of each other have similar contributions in generating language models of aligned documents. Our method improves the accuracy of extracted translations and their related words over the previous approaches. Indeed, the proposed approach can be optimized straightforwardly.

We show that integrating monolingual relations of word co-occurrences into word models helps to improve the accuracy of translations by providing more exact estimates of word statistics. In addition, our method provides a principled basis for integrating other sources of lexical and translational relations in order to improve the accuracy of extracted translations from comparable corpora.

We compare different estimations of translation probabilities from word correlations and show the significant impact of this estimation on the performance of CLIR. This reveals a new quality criterion for translation models to be suitable for Cross-Language Information Retrieval.

We evaluated our proposed approach on an English–Persian comparable corpus. Assessment of extracted translations using a machine-readable bilingual dictionary demonstrates that our approach obtains meaningful correlations between words. We further adopted the obtained translation models for English–Persian CLIR. Using translations extracted from only the comparable corpus, we achieve performance of CLIR between 36% and 58% of that of monolingual IR over three standard CLEF datasets.

A preliminary version of our work was presented in Rahimi and Shakery (2013). In the current paper, in addition to a detailed description of the proposed method, we extend our previous work through (1) defining a similarity function to derive bidirectional correlations between source and target word pairs, (2) proposing an approach to better estimate the models of low-frequency words in a comparable corpus, (3) providing a detailed analysis of the effectiveness of our method to demonstrate words for which the proposed method can produce reliable translations, and (4) comparing different functions to estimate word translation probabilities based on word correlations and show how much it impacts the performance of CLIR by empirical evaluation.

The remainder of this paper is organized as follows. In Section 2, we review previous work. Section 3 gives a description of the translation extraction problem. Then, we present our proposed language modeling method for translation extraction in Section 4. Following, experimental design and the results are reported in Sections 5 and 6, respectively. Finally, the paper is concluded in Section 7.

@&#RELATED WORK@&#

The goal of CLIR is to score documents with respect to a query in another language than that of the documents. Due to the different languages of queries and documents, some sort of processing is needed to match document terms with query terms. Cross-lingual retrieval between similar languages (such as Italian–French and Chinese–Japanese (Savoy, 2005)) can be performed without any translation (Buckley et al., 2000; He et al., 2003; Mcnamee & Mayfield, 2004). However, the most general approach for this task is to use translation resources.

Among translation resources, we focus on comparable corpora and discuss the existing methods for obtaining translation knowledge from these corpora. A majority of approaches in this domain extract translations based on the assumption that there is a correlation between collocates of a word and those of its translation (Fung, 1998; Rapp, 1999). Based on this assumption, a context vector is built for each word representing all its collocates in a window-based context. Source-language word vector is transferred into a target-language vector, relying on an existing dictionary. Target words are then ranked based on the similarity of their vectors to the vector of the source word. Chiao and Zweigenbaum (2002) adapt this strategy for extracting translations from a comparable corpus on a specific medical domain. Sadat et al. (2003) build a translation model by merging translation models obtained using this strategy for both directions; from the source language to the target language, and vice versa. Researchers tried to optimize this strategy by proposing different approaches to model context vectors of words or to measure similarity between source and target word vectors. For example, the log-likelihood ratio test is used for weighting collocates in Déjean et al. (2002).

Along the same line, Gaussier et al. (2004) adopt latent semantic analysis for deriving word vectors by presenting a geometric view on this strategy of translation extraction. Dependency trees are also used for modeling context vectors of words in Garera et al. (2009). The window- and syntax-based contexts of words are combined to derive word vectors in Hazem and Morin (2014). Andrade et al. (2011) try to learn the optimal combination of dependency and bag-of-words sentence contexts for modeling word vectors. Furthermore, Laroche and Langlais (2010) compare the impacts of different methods for modeling word contexts on the quality of obtained translations from a comparable corpus.

From a different point of view, improving the quality of translations by increasing the comparability degree of the underlying comparable corpus prior to translation extraction is investigated by Li and Gaussier (2010, 2013) and Li et al. (2011). Tamura et al. (2012) extend the underlying assumption for translation extraction, assuming that a word and its translations have similar direct and indirect co-occurrence relations with seed words of a bilingual dictionary. Based on this, they model word relations as a graph, and extract translations using label propagation. Ji (2009) tries to improve the accuracy of translations of named entities by adopting information extraction techniques and modeling extracted named entities in each language as a graph.

All these approaches use a preliminary bilingual dictionary for the task of translation extraction from comparable corpora. In this regard, Hazem and Morin (2012) study the impacts of the bilingual dictionary on the quality of extracted translations, and introduce a pre-processing step to filter non-useful entries from the bilingual dictionary in order to improve the quality of translations from a specialized comparable corpus.

Another category of approaches use comparable corpora by extracting parallel sentences in order to train machine translation systems, such as Abdul-Rauf and Schwenk (2009a, 2009b), Morin and Prochasson (2011) and Munteanu and Marcu (2005). Due to the difference between machine translation and translation in the CLIR task, these approaches are not in the scope of our study.

In this paper, we focus on extracting translation knowledge from only comparable corpora. Sheridan and Ballerini (1996) are among the first that adopt this strategy for translation extraction. In their approach, the elements of a word vector represent document weights for that term, a variation of TF-IDF weighting scheme. To improve the quality of this approach, Talvensaari et al. (2007) propose Cocot method in which target documents that are aligned with the same source document are grouped into a hyper document. In the Cocot method, cosine normalization is used for source word vectors, while pivoted length normalization is adopted for target word vectors. Similarity of two words is then calculated as the inner product of their corresponding vectors. In another relevant work, Tao and Zhai (2005) extract translations based on correlations between frequency distributions of terms. The more correlated the term distributions in a comparable corpus, the higher the similarity scores. The vectors of source and target words represent normalized frequencies of the words in aligned documents. The similarity between source and target word vectors is then measured using Pearson’s correlation coefficient. Vulić et al. (2011) extract translations based on the assumption that a word and its translations occur in the same bilingual topics, and use word distributions over bilingual topics to find similar words.

Our goal is to extract a high quality translation model from a given comparable corpus. Therefore, methods that, given a translation model, improve the performance of CLIR by different techniques, such as appropriate weighting (Shakery & Zhai, 2013), pre- and post-translation expansion (Ballesteros & Croft, 1997; Levow et al., 2005; McNamee & Mayfield, 2002), and linguistics- or statistical-based translation pruning techniques (Hashemi & Shakery, 2014) are not in the scope of our study, although applying these methods on higher quality translations from comparable corpora using our approach can give higher CLIR performance. Similarly, techniques for enhancing translation knowledge through combination of various translation resources can be adopted on extracted translations using our method to get better performance for CLIR.

We adopt monolingual word relationships in our framework to improve the quality of extracted translations from comparable corpora. Integration of monolingual word relationships in our work differs from that in the previous studies (Cao et al., 2007; Liu et al., 2005; Shakery & Zhai, 2013). Previous studies attempt to refine the estimated translation probabilities by using probabilities of monolingual relationships between extracted translation alternatives. However, we employ monolingual word relationships to provide more accurate estimation of word frequencies in the process of obtaining a translation model. Therefore, previous work in this context can further be adopted on translation models extracted by our approach.

We begin by defining the process of extracting translation knowledge from comparable corpora. Given a comparable corpus, our goal is to estimate a translation model without employing additional linguistic resources such as bilingual dictionaries or machine translation systems. We formally represent a comparable corpus by a vector of triples where each triple corresponds to an alignment. Subsequently, a comparable corpus consisting of n alignments is shown as:

                        
                           (1)
                           
                              
                                 A
                                 =
                                 {
                                 
                                    (
                                    
                                       d
                                       
                                          s
                                          1
                                       
                                    
                                    ,
                                    
                                       d
                                       
                                          t
                                          1
                                       
                                    
                                    ,
                                    
                                       s
                                       1
                                    
                                    )
                                 
                                 ,
                                 …
                                 ,
                                 
                                    (
                                    
                                       d
                                       
                                          s
                                          n
                                       
                                    
                                    ,
                                    
                                       d
                                       
                                          t
                                          n
                                       
                                    
                                    ,
                                    
                                       s
                                       n
                                    
                                    )
                                 
                                 }
                                 ,
                              
                           
                        
                     where the first entry in each triple (
                        
                           d
                           
                              s
                              i
                           
                        
                     ) is a source document ID, the second entry (
                        
                           d
                           
                              t
                              i
                           
                        
                     ) is a target document ID, and the third entry (si
                     ) is the similarity score between the source and target documents of the alignment. The alignments may share the same source or target document according to the general scheme of building comparable corpora, in that for each document in one corpus (referred to as the source document), documents of another corpus (referred to as target documents) are ranked based on their similarities. Then, source and target documents with similarity scores more than a predefined threshold constitutes an alignment. Thus, we may have 
                        
                           
                              d
                              
                                 s
                                 i
                              
                           
                           =
                           
                              d
                              
                                 s
                                 j
                              
                           
                        
                      for some i, j (1 ≤ i, j ≤ n and i ≠ j), which means that the original document is duplicated when it is aligned with several documents in the target language. But, the pair of source and target document IDs in each alignment is unique, i.e., 
                        
                           
                              (
                              
                                 d
                                 
                                    s
                                    i
                                 
                              
                              ,
                              
                                 d
                                 
                                    t
                                    i
                                 
                              
                              )
                           
                           ≠
                           
                              (
                              
                                 d
                                 
                                    s
                                    j
                                 
                              
                              ,
                              
                                 d
                                 
                                    t
                                    j
                                 
                              
                              )
                           
                        
                      for each i, j (1 ≤ i, j ≤ n and i ≠ j).

A translation model consists of word translation probabilities in the form of p(wt
                     |ws
                     ) which indicates the probability of translating a source word ws
                      into a target word wt
                     . In addition, 
                        
                           
                              ∑
                              
                                 
                                    w
                                    i
                                 
                                 ∈
                                 
                                    V
                                    t
                                 
                              
                           
                           
                              p
                              (
                              
                                 w
                                 i
                              
                              |
                              
                                 w
                                 s
                              
                              )
                           
                           =
                           1
                        
                      for each word ws
                     , where Vt
                      denotes the vocabulary of the target language. Two steps of estimating a translation model from a comparable corpus are depicted in Fig. 1
                     . In the first step, correlation between each pair of source–target words is extracted and thus, a ranked list of target words for each source word is created. Then in the next step, a translation model is built upon the ranked lists of target words.

In this section, we describe the proposed approach for extracting translations from comparable corpora using the language modeling framework. The idea behind our approach is that a word and its translations have similar contributions in the language models of aligned documents. Accordingly, each word in the source and the target languages is modeled in such a way that word’s contributions in the language models of documents are captured. Therefore, a word model has the same number of parameters as the number of alignments in a comparable corpus. The KL-divergence between source and target word models then shows how related they are. Indeed, target words can be sorted based on a decreasing function of their KL-divergence distances from a source word. Fig. 2
                      shows the overall structure of the proposed method with different solutions adopted for components. In the following sections, we provide a detailed description of how to model the words and measure their similarity.

To find translations of a source word, target words are first sorted by distance of their models from the source word model. Following the language modeling framework, the distance between word models is measured using the KL-divergence. Subsequently, a function to compute word similarities based on their KL-divergence distances is required. For this purpose, we consider two functions.

The first function applies exponential transformation to the divergence values. In particular, the similarity between a source and a target word is estimated as follows:

                           
                              (2)
                              
                                 
                                    
                                       m
                                       
                                          K
                                          L
                                       
                                    
                                    a
                                    
                                       sim
                                       KL
                                    
                                    
                                       (
                                       
                                          w
                                          s
                                       
                                       ,
                                       
                                          w
                                          t
                                       
                                       )
                                    
                                    =
                                    exp
                                    
                                       (
                                       
                                       −
                                       β
                                       
                                          D
                                          
                                             KL
                                          
                                       
                                       
                                          (
                                          
                                             θ
                                             
                                                w
                                                s
                                             
                                          
                                          ∥
                                          
                                             θ
                                             
                                                w
                                                t
                                             
                                          
                                          )
                                       
                                       )
                                    
                                    ,
                                 
                              
                           
                        where 
                           
                              θ
                              
                                 w
                                 s
                              
                           
                         and 
                           
                              θ
                              
                                 w
                                 t
                              
                           
                         are source and target word models, respectively, 
                           
                              
                                 D
                                 
                                    KL
                                 
                              
                              
                                 (
                                 
                                    θ
                                    
                                       w
                                       s
                                    
                                 
                                 ∥
                                 
                                    θ
                                    
                                       w
                                       t
                                    
                                 
                                 )
                              
                           
                         is their KL-divergence, and β > 0 is the rate parameter of the exponential transformation.

The reason behind adopting exponential transformation is to control the impact of lower ranked translations for each word. Due to the noisy structure of comparable corpora, the reliability of a translation for ws
                         decreases as its rank increases. This leads to a preference for selecting a fewer number of top translations of each word. On the other hand, selecting more translation alternatives of a word has the effect of query or document expansion (based on the translation direction in CLIR) which is desirable for CLIR (Nie, 2010). To balance the benefits and risks of using lower ranked translations, parameter β is introduced. The higher the value of this parameter, the lower the trust in lower ranked translations.

The second similarity function uses word similarities in both translation directions. The KL-divergence measure is not symmetric and p(v|u) can differ from p(u|v) despite using the same modeling approach for words in both sides of a comparable corpus. Estimated translation models by widely used extraction methods from aligned corpora such as the IBM models (Brown et al., 1993) are also asymmetric. However, bidirectional translations by combining translations in both directions are utilized in CLIR to filter out or lower the rank of the translations that exist only in one direction. Accordingly, to acquire more accurate translations, the following similarity function is adopted to build a symmetric translation model:

                           
                              (3)
                              
                                 
                                    
                                       m
                                       
                                          J
                                          S
                                       
                                    
                                    a
                                    
                                       sim
                                       JS
                                    
                                    
                                       (
                                       
                                          w
                                          s
                                       
                                       ,
                                       
                                          w
                                          t
                                       
                                       )
                                    
                                    =
                                    exp
                                    
                                       (
                                       
                                       −
                                       β
                                       
                                          D
                                          
                                             JS
                                          
                                       
                                       
                                          (
                                          
                                             θ
                                             
                                                w
                                                s
                                             
                                          
                                          ∥
                                          
                                             θ
                                             
                                                w
                                                t
                                             
                                          
                                          )
                                       
                                       )
                                    
                                    ,
                                 
                              
                           
                        where D
                        JS(.‖.) is the Jensen–Shannon divergence and is estimated as:

                           
                              (4)
                              
                                 
                                    
                                       
                                          
                                             
                                                D
                                                
                                                   JS
                                                
                                             
                                             
                                                (
                                                
                                                   θ
                                                   
                                                      w
                                                      s
                                                   
                                                
                                                ∥
                                                
                                                   θ
                                                   
                                                      w
                                                      t
                                                   
                                                
                                                )
                                             
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                1
                                                2
                                             
                                             
                                                D
                                                
                                                   KL
                                                
                                             
                                             
                                                (
                                                
                                                   θ
                                                   
                                                      w
                                                      s
                                                   
                                                
                                                ∥
                                                
                                                   θ
                                                   m
                                                
                                                )
                                             
                                             +
                                             
                                                1
                                                2
                                             
                                             
                                                D
                                                
                                                   KL
                                                
                                             
                                             
                                                (
                                                
                                                   θ
                                                   
                                                      w
                                                      t
                                                   
                                                
                                                ∥
                                                
                                                   θ
                                                   m
                                                
                                                )
                                             
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (5)
                              
                                 
                                    
                                       
                                          
                                             θ
                                             m
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                1
                                                2
                                             
                                             
                                                (
                                                
                                                   θ
                                                   
                                                      w
                                                      s
                                                   
                                                
                                                +
                                                
                                                   θ
                                                   
                                                      w
                                                      t
                                                   
                                                
                                                )
                                             
                                             .
                                          
                                       
                                    
                                 
                              
                           
                        
                     

For calculating both similarity functions, we need the KL-divergence of word models which is computed as:

                           
                              (6)
                              
                                 
                                    
                                       D
                                       
                                          KL
                                       
                                    
                                    
                                       (
                                       
                                          θ
                                          
                                             w
                                             s
                                          
                                       
                                       ∥
                                       
                                          θ
                                          
                                             w
                                             t
                                          
                                       
                                       )
                                    
                                    =
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          |
                                          A
                                          |
                                       
                                    
                                    p
                                    
                                       (
                                       
                                          d
                                          
                                             s
                                             i
                                          
                                       
                                       |
                                       
                                          w
                                          s
                                       
                                       )
                                    
                                    log
                                    
                                       
                                          p
                                          (
                                          
                                             d
                                             
                                                s
                                                i
                                             
                                          
                                          |
                                          
                                             w
                                             s
                                          
                                          )
                                       
                                       
                                          p
                                          (
                                          
                                             d
                                             
                                                t
                                                i
                                             
                                          
                                          |
                                          
                                             w
                                             t
                                          
                                          )
                                       
                                    
                                    ,
                                 
                              
                           
                        where A is the alignment vector of a comparable corpus. In the next section, we describe how to estimate word models p(.|.) required in this equation.

To rank target words using the similarity functions of Eqs. (2) and (3), we need to estimate source and target word models, i.e. 
                           
                              
                                 {
                                 p
                                 
                                    (
                                    
                                       d
                                       
                                          s
                                          i
                                       
                                    
                                    |
                                    
                                       w
                                       s
                                    
                                    )
                                 
                                 }
                              
                              
                                 i
                                 =
                                 1
                              
                              
                                 |
                                 A
                                 |
                              
                           
                         and 
                           
                              
                                 
                                    {
                                    p
                                    
                                       (
                                       
                                          d
                                          
                                             t
                                             i
                                          
                                       
                                       |
                                       
                                          w
                                          t
                                       
                                       )
                                    
                                    }
                                 
                                 
                                    i
                                    =
                                    1
                                 
                                 
                                    |
                                    A
                                    |
                                 
                              
                              ,
                           
                         respectively. Using Bayes’ rule, we have:

                           
                              (7)
                              
                                 
                                    p
                                    
                                       (
                                       
                                          d
                                          
                                             s
                                             i
                                          
                                       
                                       |
                                       
                                          w
                                          s
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          p
                                          
                                             (
                                             
                                                w
                                                s
                                             
                                             |
                                             
                                                d
                                                
                                                   s
                                                   i
                                                
                                             
                                             )
                                          
                                          p
                                          
                                             (
                                             
                                                d
                                                
                                                   s
                                                   i
                                                
                                             
                                             )
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             
                                                |
                                                A
                                                |
                                             
                                          
                                          p
                                          
                                             (
                                             
                                                w
                                                s
                                             
                                             |
                                             
                                                d
                                                
                                                   s
                                                   j
                                                
                                             
                                             )
                                          
                                          p
                                          
                                             (
                                             
                                                d
                                                
                                                   s
                                                   j
                                                
                                             
                                             )
                                          
                                       
                                    
                                    
                                    ,
                                    
                                    p
                                    
                                       (
                                       
                                          d
                                          
                                             t
                                             i
                                          
                                       
                                       |
                                       
                                          w
                                          t
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          p
                                          
                                             (
                                             
                                                w
                                                t
                                             
                                             |
                                             
                                                d
                                                
                                                   t
                                                   i
                                                
                                             
                                             )
                                          
                                          p
                                          
                                             (
                                             
                                                d
                                                
                                                   t
                                                   i
                                                
                                             
                                             )
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             
                                                |
                                                A
                                                |
                                             
                                          
                                          p
                                          
                                             (
                                             
                                                w
                                                t
                                             
                                             |
                                             
                                                d
                                                
                                                   t
                                                   j
                                                
                                             
                                             )
                                          
                                          p
                                          
                                             (
                                             
                                                d
                                                
                                                   t
                                                   j
                                                
                                             
                                             )
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

By applying Bayes’ rule, the estimation of word models reduces to the estimation of document language models, i.e. 
                           
                              
                                 {
                                 p
                                 
                                    (
                                    
                                       w
                                       s
                                    
                                    |
                                    
                                       d
                                       
                                          s
                                          i
                                       
                                    
                                    )
                                 
                                 }
                              
                              
                                 i
                                 =
                                 1
                              
                              
                                 |
                                 A
                                 |
                              
                           
                         and 
                           
                              
                                 
                                    {
                                    p
                                    
                                       (
                                       
                                          w
                                          t
                                       
                                       |
                                       
                                          d
                                          
                                             t
                                             i
                                          
                                       
                                       )
                                    
                                    }
                                 
                                 
                                    i
                                    =
                                    1
                                 
                                 
                                    |
                                    A
                                    |
                                 
                              
                              ,
                           
                         and prior probabilities for documents.


                        Document language models: We follow the most used multinomial unigram model for the document language models which can be either the basic or a smoothed Maximum-Likelihood (ML) estimate. The ML language model of document d, denoted by 
                           
                              
                                 θ
                                 d
                                 
                                    ML
                                 
                              
                              ,
                           
                         is given by:

                           
                              (8)
                              
                                 
                                    
                                       
                                          
                                             p
                                             
                                                (
                                                
                                                   w
                                                   |
                                                
                                                
                                                   θ
                                                   
                                                      d
                                                   
                                                   
                                                      ML
                                                   
                                                
                                                )
                                             
                                             =
                                             
                                                
                                                   
                                                      tf
                                                   
                                                   
                                                      (
                                                      w
                                                      ,
                                                      d
                                                      )
                                                   
                                                
                                                
                                                   |
                                                   d
                                                   |
                                                
                                             
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        where tf(w, d) is the count of word w in document d and |d| is the length of document d. The maximum likelihood estimator underestimates probabilities of unseen words in a document. Applying Bayes’ rule on this estimate of document language models produces zeros in word models (Eq. (7)) and thus there may exist an alignment i such that 
                           
                              p
                              (
                              
                                 d
                                 
                                    s
                                    i
                                 
                              
                              |
                              
                                 w
                                 s
                              
                              )
                              >
                              0
                           
                         while 
                           
                              p
                              (
                              
                                 d
                                 
                                    t
                                    i
                                 
                              
                              |
                              
                                 w
                                 t
                              
                              )
                              =
                              0
                           
                        . Therefore, the ML estimate for language models of the target documents causes problems in comparing word models using Eq. (6). We present two techniques to resolve this issue.


                        Smoothing the language models of documents: One solution is to smooth the ML-estimate of document language models. Smoothing methods discount the probabilities of observed words in a document and assign non-zero probabilities to unseen words. Let 
                           
                              θ
                              d
                              S
                           
                         denote a smoothed language model of document d. Two commonly used smoothing methods for estimating 
                           
                              θ
                              d
                              S
                           
                         are Jelinek–Mercer (JM) and Dirichlet prior smoothing methods (Zhai & Lafferty, 2001) that use the collection language model. The JM smoothing method linearly interpolates document language models with the collection language model, and the amount of smoothing is equal for all documents. Using this smoothing method, the word probabilities are estimated as:

                           
                              (9)
                              
                                 
                                    
                                       
                                          
                                             p
                                             
                                                (
                                                
                                                   w
                                                   |
                                                
                                                
                                                   θ
                                                   
                                                      d
                                                   
                                                   
                                                      JM
                                                   
                                                
                                                )
                                             
                                             =
                                             
                                                (
                                                1
                                                −
                                                λ
                                                )
                                             
                                             p
                                             
                                                (
                                                
                                                   w
                                                   |
                                                
                                                
                                                   θ
                                                   
                                                      d
                                                   
                                                   
                                                      ML
                                                   
                                                
                                                )
                                             
                                             +
                                             λ
                                             p
                                             
                                                (
                                                w
                                                |
                                                C
                                                )
                                             
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        where 0 ≤ λ ≤ 1 is the smoothing parameter. In contrast, Dirichlet prior smoothing uses dynamic coefficients for interpolation which yields different amounts of smoothing based on document lengths:

                           
                              (10)
                              
                                 
                                    
                                       
                                          
                                             p
                                             
                                                (
                                                
                                                   w
                                                   |
                                                
                                                
                                                   θ
                                                   
                                                      d
                                                   
                                                   
                                                      Dir
                                                   
                                                
                                                )
                                             
                                             =
                                             
                                                
                                                   |
                                                   d
                                                   |
                                                
                                                
                                                   |
                                                   d
                                                   |
                                                   +
                                                   μ
                                                
                                             
                                             p
                                             
                                                (
                                                
                                                   w
                                                   |
                                                
                                                
                                                   θ
                                                   
                                                      d
                                                   
                                                   
                                                      ML
                                                   
                                                
                                                )
                                             
                                             +
                                             
                                                μ
                                                
                                                   |
                                                   d
                                                   |
                                                   +
                                                   μ
                                                
                                             
                                             p
                                             
                                                (
                                                w
                                                |
                                                C
                                                )
                                             
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        where μ is the smoothing parameter.

We adopt smoothing methods in estimating the language models of the target documents as the first solution. For source documents, both the basic and the smoothed ML estimates are appropriate for estimating source word models. However, considering computational complexity, we use the maximum likelihood estimator and show the effect of using smoothed language models for the source documents in one experiment. Therefore, the following divergence values are calculated in the defined similarity functions (Eqs. (2) and (3)):

                           
                              (11)
                              
                                 
                                    
                                       D
                                       
                                          KL
                                       
                                    
                                    
                                       (
                                       
                                          θ
                                          
                                             w
                                             s
                                          
                                          
                                             ML
                                          
                                       
                                       
                                          ∥
                                       
                                       
                                          θ
                                          
                                             w
                                             t
                                          
                                          S
                                       
                                       )
                                    
                                    ,
                                 
                              
                           
                        where the superscripts of the word models indicate the estimation used for modeling the documents in the corresponding languages. Note that the background collection model for smoothing the language models of the target documents is estimated based on 
                           
                              
                                 {
                                 
                                    d
                                    
                                       t
                                       i
                                    
                                 
                                 }
                              
                              
                                 i
                                 =
                                 1
                              
                              n
                           
                        .


                        Smoothing word models: The problem of zero probabilities in word models can be directly solved by smoothing word models, instead of using smoothed language models for documents. In this solution, language models of source and target documents are estimated using the maximum likelihood estimator and substituted in Eq. (7) to build word models. Thereafter, at least target word models should be smoothed to avoid zero division problem in Eq. (6). The naïve way for smoothing word models, similar to additive smoothing for document language models (Chen & Goodman, 1999), is as:

                           
                              (12)
                              
                                 
                                    p
                                    
                                       (
                                       
                                          d
                                          i
                                       
                                       |
                                       w
                                       )
                                    
                                    =
                                    
                                       
                                          p
                                          (
                                          
                                             d
                                             i
                                          
                                          |
                                          w
                                          )
                                          +
                                          ϵ
                                       
                                       
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             
                                                |
                                                A
                                                |
                                             
                                          
                                          
                                             p
                                             (
                                             
                                                d
                                                j
                                             
                                             |
                                             w
                                             )
                                          
                                          +
                                          
                                             |
                                             A
                                             |
                                          
                                          ϵ
                                       
                                    
                                    ,
                                 
                              
                           
                        where ϵ > 0 is the smoothing parameter. The idea behind this solution is to advance toward smoothing the model of each word based on its features, such as alignment frequency of the word, with the intuition that the model of a word occurring in more alignments needs less amount of smoothing. Herein, we only report the results of first solution, since it gives moderately better results in preliminary experiments.

The second step in the estimation of word models is to derive prior probabilities for documents. We propose to estimate prior probabilities for a source and a target document based on the quality of the alignment containing these two documents. Intuitively, alignments with higher similarity scores are more reliable. To consider alignment qualities, similarity scores of alignments are first normalized as follows:

                           
                              (13)
                              
                                 
                                    p
                                    
                                       (
                                       
                                          a
                                          i
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          
                                             sim
                                          
                                          (
                                          
                                             d
                                             
                                                s
                                                i
                                             
                                          
                                          ,
                                          
                                             d
                                             
                                                t
                                                i
                                             
                                          
                                          )
                                       
                                       
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             
                                                |
                                                A
                                                |
                                             
                                          
                                          
                                             sim
                                          
                                          
                                             (
                                             
                                                d
                                                
                                                   s
                                                   j
                                                
                                             
                                             ,
                                             
                                                d
                                                
                                                   t
                                                   j
                                                
                                             
                                             )
                                          
                                       
                                    
                                    =
                                    
                                       
                                          s
                                          i
                                       
                                       
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             
                                                |
                                                A
                                                |
                                             
                                          
                                          
                                             s
                                             j
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

The prior probability of document 
                           
                              d
                              
                                 s
                                 i
                              
                           
                        /
                           
                              d
                              
                                 t
                                 i
                              
                           
                         is then estimated based on the normalized similarity score of alignment ai
                         that contains the document. Assigning higher prior probabilities to documents in alignments with higher quality implies the stronger impact of these alignments on word similarity. Altogether, word models are estimated as:

                           
                              (14)
                              
                                 
                                    p
                                    
                                       (
                                       
                                          d
                                          
                                             s
                                             i
                                          
                                       
                                       |
                                       
                                          w
                                          s
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          p
                                          
                                             (
                                             
                                                w
                                                s
                                             
                                             |
                                             
                                                
                                                   θ
                                                   ˙
                                                
                                                
                                                   d
                                                   
                                                      s
                                                      i
                                                   
                                                
                                             
                                             )
                                          
                                          p
                                          
                                             (
                                             
                                                a
                                                i
                                             
                                             )
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             
                                                |
                                                A
                                                |
                                             
                                          
                                          p
                                          
                                             (
                                             
                                                w
                                                s
                                             
                                             |
                                             
                                                
                                                   θ
                                                   ˙
                                                
                                                
                                                   d
                                                   
                                                      s
                                                      j
                                                   
                                                
                                             
                                             )
                                          
                                          p
                                          
                                             (
                                             
                                                a
                                                j
                                             
                                             )
                                          
                                       
                                    
                                    
                                    ,
                                    
                                    p
                                    
                                       (
                                       
                                          d
                                          
                                             t
                                             i
                                          
                                       
                                       |
                                       
                                          w
                                          t
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          p
                                          
                                             (
                                             
                                                w
                                                t
                                             
                                             |
                                             
                                                
                                                   θ
                                                   ˙
                                                
                                                
                                                   d
                                                   
                                                      t
                                                      i
                                                   
                                                
                                             
                                             )
                                          
                                          p
                                          
                                             (
                                             
                                                a
                                                i
                                             
                                             )
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             
                                                |
                                                A
                                                |
                                             
                                          
                                          p
                                          
                                             (
                                             
                                                w
                                                t
                                             
                                             |
                                             
                                                
                                                   θ
                                                   ˙
                                                
                                                
                                                   d
                                                   
                                                      t
                                                      j
                                                   
                                                
                                             
                                             )
                                          
                                          p
                                          
                                             (
                                             
                                                a
                                                j
                                             
                                             )
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        where 
                           
                              
                                 θ
                                 ˙
                              
                              
                                 d
                                 i
                              
                           
                         is an appropriate language model of the document, estimated using either the maximum likelihood estimator or a smoothed one. With this estimation of source and target word models, similarity scores of Eqs. (2) and (3) can be calculated for each pair of source–target words.

Most methods for translation extraction are based on word frequencies in aligned documents. Such methods obtain the frequency of a word by counting the exact match(es) of the word in a document. Exact matching of words can make the translation extraction non-optimal, because a concept can be expressed using several words in different documents. Since translations are inferred based on correlation between word frequencies, a more comprehensive estimate of word frequencies can improve the quality of a translation model.

In addition to enhancing the translation quality, considering semantically related words in translation extraction can increase the number of words that can be correctly translated using aligned corpora. Translations from aligned corpora are generally extracted by using statistical approaches on word frequencies where these approaches cannot produce statistically reliable translations for low frequency words. Considering related words in counting the occurrences of words can increase the frequency of some rare words which consequently increases the reliability of extracted translations for them.

We integrate semantically related words into translation extraction by expanding document language models with these words, introduced by Berger and Lafferty (1999) and further studied by Karimzadehgan and Zhai (2010). The expanded document language model, denoted by 
                           
                              
                                 θ
                                 d
                                 
                                    rel
                                 
                              
                              ,
                           
                         is estimated as:

                           
                              (15)
                              
                                 
                                    
                                       
                                          
                                             p
                                             
                                                (
                                                
                                                   w
                                                   |
                                                
                                                
                                                   θ
                                                   
                                                      d
                                                   
                                                   
                                                      rel
                                                   
                                                
                                                )
                                             
                                             =
                                             
                                                ∑
                                                
                                                   v
                                                   ∈
                                                   V
                                                
                                             
                                             
                                                p
                                                
                                                   mono
                                                
                                             
                                             
                                                (
                                                w
                                                |
                                                v
                                                )
                                             
                                             p
                                             
                                                (
                                                
                                                   v
                                                   |
                                                
                                                
                                                   θ
                                                   
                                                      d
                                                   
                                                   
                                                      ML
                                                   
                                                
                                                )
                                             
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        where p
                        mono(w|v) indicates the probability that word v is semantically related to word w in the same language. These word correlations can be calculated based on mutual information defined as:

                           
                              (16)
                              
                                 
                                    a
                                    MI
                                    
                                       (
                                       w
                                       ,
                                       u
                                       )
                                    
                                    =
                                    
                                       ∑
                                       
                                          
                                             X
                                             w
                                          
                                          =
                                          0
                                          ,
                                          1
                                       
                                    
                                    
                                       ∑
                                       
                                          
                                             X
                                             u
                                          
                                          =
                                          0
                                          ,
                                          1
                                       
                                    
                                    
                                       p
                                       
                                          (
                                          
                                             X
                                             w
                                          
                                          ,
                                          
                                             X
                                             u
                                          
                                          )
                                       
                                       log
                                       
                                          
                                             p
                                             (
                                             
                                                X
                                                w
                                             
                                             ,
                                             
                                                X
                                                u
                                             
                                             )
                                          
                                          
                                             p
                                             
                                                (
                                                
                                                   X
                                                   w
                                                
                                                )
                                             
                                             p
                                             
                                                (
                                                
                                                   X
                                                   u
                                                
                                                )
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        where the random variable Xw
                         ∈ {0, 1} represents whether term w occurs in a document (
                           
                              
                                 X
                                 w
                              
                              =
                              1
                           
                        ) or not (
                           
                              
                                 X
                                 w
                              
                              =
                              0
                           
                        ). These correlation values are then normalized for estimating the probability of relatedness as:

                           
                              (17)
                              
                                 
                                    
                                       p
                                       
                                          mono
                                       
                                       
                                          MI
                                       
                                    
                                    
                                       (
                                       w
                                       |
                                       u
                                       )
                                    
                                    =
                                    
                                       
                                          a
                                          MI
                                          (
                                          w
                                          ,
                                          u
                                          )
                                       
                                       
                                          
                                             ∑
                                             
                                                
                                                   w
                                                   ′
                                                
                                                ∈
                                                V
                                             
                                          
                                          
                                             a
                                             MI
                                             (
                                             
                                                w
                                                ′
                                             
                                             ,
                                             u
                                             )
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

Document language models used for estimating word models in Eq. (14) can be extended by related words. Thus, the probability of a word given a document is estimated considering all of its related words appropriately. Related words can be considered in modeling documents of each side of a comparable corpus. Considering computational complexity, Eq. (15) is suitable for expanding language models of source documents with related words. However, as mentioned before, zero probabilities in the models of target words are not acceptable. Therefore, language models of the target documents need to be smoothed. Replacing 
                           
                              p
                              (
                              w
                              |
                              
                                 θ
                                 d
                                 
                                    ML
                                 
                              
                              )
                           
                         with 
                           
                              p
                              (
                              w
                              |
                              
                                 θ
                                 d
                                 
                                    rel
                                 
                              
                              )
                           
                         in estimating the smoothed language models for documents, Eqs. (9) and (10) become:

                           
                              (18)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         p
                                                         
                                                            (
                                                            
                                                               w
                                                               |
                                                            
                                                            
                                                               θ
                                                               
                                                                  d
                                                               
                                                               
                                                                  
                                                                     JM
                                                                  
                                                                  −
                                                                  
                                                                     rel
                                                                  
                                                               
                                                            
                                                            )
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      =
                                                   
                                                
                                                
                                                   
                                                      
                                                         λ
                                                         
                                                            (
                                                            
                                                               ∑
                                                               
                                                                  v
                                                                  ∈
                                                                  V
                                                               
                                                            
                                                            
                                                               p
                                                               
                                                                  mono
                                                               
                                                            
                                                            
                                                               (
                                                               w
                                                               |
                                                               v
                                                               )
                                                            
                                                            p
                                                            
                                                               (
                                                               
                                                                  v
                                                                  |
                                                               
                                                               
                                                                  θ
                                                                  
                                                                     d
                                                                  
                                                                  
                                                                     ML
                                                                  
                                                               
                                                               )
                                                            
                                                            )
                                                         
                                                         +
                                                         
                                                            (
                                                            1
                                                            −
                                                            λ
                                                            )
                                                         
                                                         p
                                                         
                                                            (
                                                            w
                                                            |
                                                            C
                                                            )
                                                         
                                                         ,
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (19)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         p
                                                         
                                                            (
                                                            
                                                               w
                                                               |
                                                            
                                                            
                                                               θ
                                                               
                                                                  d
                                                               
                                                               
                                                                  
                                                                     Dir
                                                                  
                                                                  −
                                                                  
                                                                     rel
                                                                  
                                                               
                                                            
                                                            )
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      =
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            
                                                               |
                                                               d
                                                               |
                                                            
                                                            
                                                               |
                                                               d
                                                               |
                                                               +
                                                               μ
                                                            
                                                         
                                                         
                                                            (
                                                            
                                                               ∑
                                                               
                                                                  v
                                                                  ∈
                                                                  V
                                                               
                                                            
                                                            
                                                               p
                                                               
                                                                  mono
                                                               
                                                            
                                                            
                                                               (
                                                               w
                                                               |
                                                               v
                                                               )
                                                            
                                                            p
                                                            
                                                               (
                                                               
                                                                  v
                                                                  |
                                                               
                                                               
                                                                  θ
                                                                  
                                                                     d
                                                                  
                                                                  
                                                                     ML
                                                                  
                                                               
                                                               )
                                                            
                                                            )
                                                         
                                                         +
                                                         
                                                            μ
                                                            
                                                               |
                                                               d
                                                               |
                                                               +
                                                               μ
                                                            
                                                         
                                                         p
                                                         
                                                            (
                                                            w
                                                            |
                                                            C
                                                            )
                                                         
                                                         .
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

These estimates of expanded language models can be used for documents in the target language in order to integrate related words into translation extraction.

After estimating similarities between source and target word pairs, the next step is to select the appropriate translations and obtain their probabilities from similarity scores (the second component in Fig. 1). For a source word ws
                        , we sort target words by their similarity scores and choose the top m words 
                           
                              
                                 {
                                 
                                    w
                                    
                                       t
                                       i
                                    
                                 
                                 }
                              
                              
                                 i
                                 =
                                 1
                              
                              m
                           
                         each with rank i. The goal is to find the probability that word ws
                         is translated to word 
                           
                              
                                 w
                                 
                                    t
                                    i
                                 
                              
                              ,
                           
                         i.e., 
                           
                              p
                              (
                              
                                 w
                                 
                                    t
                                    i
                                 
                              
                              |
                              
                                 w
                                 s
                              
                              )
                           
                        .

The simplest way is to use linear normalization approach as follows:

                           
                              (20)
                              
                                 
                                    p
                                    
                                       (
                                       
                                          w
                                          
                                             t
                                             i
                                          
                                       
                                       |
                                       
                                          w
                                          s
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          m
                                          a
                                          sim
                                          (
                                          
                                             w
                                             s
                                          
                                          ,
                                          
                                             w
                                             
                                                t
                                                i
                                             
                                          
                                          )
                                       
                                       
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             m
                                          
                                          m
                                          a
                                          sim
                                          
                                             (
                                             
                                                w
                                                s
                                             
                                             ,
                                             
                                                w
                                                
                                                   t
                                                   j
                                                
                                             
                                             )
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

However, due to the noisy structure of comparable corpora, we aim to penalize the lower similarity scores more to reduce the effect of noise words in the retrieval process, similar to Shakery and Zhai (2013). This can be performed using logarithmic or exponential normalization. Logarithmic normalization of similarity scores, by analogy with Normalized Discounted Cumulative Gain (NDCG) evaluation measure, is defined as follows:

                           
                              (21)
                              
                                 
                                    p
                                    
                                       (
                                       
                                          w
                                          
                                             t
                                             i
                                          
                                       
                                       |
                                       
                                          w
                                          s
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          m
                                          a
                                          sim
                                          
                                             (
                                             
                                                w
                                                s
                                             
                                             ,
                                             
                                                w
                                                
                                                   t
                                                   i
                                                
                                             
                                             )
                                          
                                          /
                                          
                                             log
                                             2
                                          
                                          
                                             (
                                             i
                                             +
                                             1
                                             )
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             m
                                          
                                          m
                                          a
                                          sim
                                          
                                             (
                                             
                                                w
                                                s
                                             
                                             ,
                                             
                                                w
                                                
                                                   t
                                                   j
                                                
                                             
                                             )
                                          
                                          /
                                          
                                             log
                                             2
                                          
                                          
                                             (
                                             j
                                             +
                                             1
                                             )
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

Similarly, exponential normalization of similarity scores can be used as:

                           
                              (22)
                              
                                 
                                    p
                                    
                                       (
                                       
                                          w
                                          
                                             t
                                             i
                                          
                                       
                                       |
                                       
                                          w
                                          s
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          
                                             (
                                             
                                                e
                                                b
                                             
                                             )
                                          
                                          
                                             m
                                             a
                                             sim
                                             (
                                             
                                                w
                                                s
                                             
                                             ,
                                             
                                                w
                                                
                                                   t
                                                   i
                                                
                                             
                                             )
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             m
                                          
                                          
                                             
                                                (
                                                
                                                   e
                                                   b
                                                
                                                )
                                             
                                             
                                                m
                                                a
                                                sim
                                                (
                                                
                                                   w
                                                   s
                                                
                                                ,
                                                
                                                   w
                                                   
                                                      t
                                                      j
                                                   
                                                
                                                )
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        where b is a free parameter that controls the amount of penalization applied to the similarity scores. We set 
                           
                              b
                              =
                              2
                           
                         unless otherwise specified.

@&#EXPERIMENTS@&#

In this section, we give an overview of evaluation setup and then present the results of experiments in the next section.

We first discuss how to evaluate the quality of an obtained translation model. We measure the quality of a translation model in terms of the accuracy of its translations as well as its suitability for CLIR. The latter is because the objective of using a translation model in CLIR is to distinguish between relevant and non-relevant documents, which is also affected by other aspects of translation models (than translation accuracy), such as translation probabilities. Following, we mention measures for evaluation of extracted translation models from a comparable corpus.


                        Machine-readable bilingual dictionary: Mean Average Precision (MAP) and F-measure are used to measure the accuracy of translations extracted from a comparable corpus by assuming translations provided by a machine-readable dictionary as ground truth. Herein, we report only the MAP measure since it incorporates the ranks of translations in evaluation. However, this measure does not evaluate three aspects of a translation model. First, translations of OOV words are not evaluated with this measure, since they are not available in dictionaries. Second, the quality of extracted related words is not evaluated. Using comparable corpora for extracting translation knowledge has the advantage of also obtaining non-translations but related words in the target language which have proven to be effective on the IR performance. Therefore, we need to determine how well a translation extraction approach performs in ranking related words in the target language. Third, translation probabilities are not evaluated using this measure. This aspect is important since two translation lists with the same ranking of target words but different assigned probabilities may have different performance in CLIR.


                        Goodness: The goodness measure is defined by Talvensaari et al. (2008) for evaluating translations extracted from an in-domain comparable corpus. The authors assumed a good translation for a word w occurs with high frequency in the documents relevant to the query containing w. Accordingly, Relative Document Frequency (RDF) of translation t in document set S is defined as 
                           
                              
                                 rdf
                              
                              (
                              t
                              ,
                              S
                              )
                              =
                              
                                 df
                              
                              (
                              t
                              ,
                              S
                              )
                              /
                              |
                              S
                              |
                           
                        . Goodness of t for a query word belonging to query q is then measured as:

                           
                              (23)
                              
                                 
                                    g
                                    
                                       (
                                       t
                                       ,
                                       q
                                       )
                                    
                                    =
                                    
                                       rdf
                                    
                                    
                                       (
                                       t
                                       ,
                                       
                                          R
                                          q
                                       
                                       )
                                    
                                    −
                                    
                                       rdf
                                    
                                    
                                       (
                                       t
                                       ,
                                       C
                                       ∖
                                       
                                          R
                                          q
                                       
                                       )
                                    
                                    ,
                                 
                              
                           
                        where Rq
                         is the set of all documents in collection C relevant to query q and C∖Rq
                         is the rest of the documents. Function g (.) has a value in 
                           
                              [
                              −
                              1
                              ,
                              1
                              ]
                           
                         where the larger the value, the higher the quality of the translation. This measure properly evaluates the quality of extracted target words related to the translations of a source word. The main problem with the goodness measure is that it considers neither the ranks of translations, nor the assigned probabilities to translations. Different rankings of a set of target words for a source word yield translation models with totally different qualities. Because of the important role of translation ranks and probabilities (as we will show in Section 6.6) in the quality of a translation model, we do not use this measure for evaluation.


                        Corpus-based CLIR: Corpus-based CLIR models seek to optimize the retrieval effectiveness when translation probabilities are available. Therefore, the CLIR performance using a translation model indicates the quality of that model. For this evaluation, we adopt the language modeling framework for doing CLIR, because it facilitates the integration of translation probabilities into query or document language models (Nie, 2010). Here, we use the query translation approach, since it has so far been the dominant approach due to its efficiency. In the query translation approach, a new query language model is built and documents are ranked according to:

                           
                              (24)
                              
                                 
                                    
                                       
                                          
                                             
                                                Score
                                             
                                             (
                                             Q
                                             ,
                                             D
                                             )
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                ∑
                                                
                                                   
                                                      w
                                                      t
                                                   
                                                   ∈
                                                   
                                                      V
                                                      t
                                                   
                                                
                                             
                                             p
                                             
                                                (
                                                
                                                   w
                                                   t
                                                
                                                |
                                                
                                                   θ
                                                   Q
                                                
                                                )
                                             
                                             log
                                             p
                                             
                                                (
                                                
                                                   w
                                                   t
                                                
                                                |
                                                
                                                   θ
                                                   D
                                                
                                                )
                                             
                                             
                                             ,
                                          
                                       
                                    
                                    
                                       
                                          
                                             p
                                             (
                                             
                                                w
                                                t
                                             
                                             |
                                             
                                                θ
                                                Q
                                             
                                             )
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                ∑
                                                
                                                   
                                                      w
                                                      s
                                                   
                                                   ∈
                                                   
                                                      V
                                                      s
                                                   
                                                
                                             
                                             p
                                             
                                                (
                                                
                                                   w
                                                   t
                                                
                                                |
                                                
                                                   w
                                                   s
                                                
                                                )
                                             
                                             p
                                             
                                                (
                                                
                                                   w
                                                   s
                                                
                                                |
                                                
                                                   θ
                                                   Q
                                                
                                                )
                                             
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        where θQ
                         and θD
                         are query and document language models, respectively, and p(wt
                        |ws
                        ) indicates the probability that the source word ws
                         is translated to the target word wt
                        .

Effectiveness of CLIR is evaluated using MAP, Precision at top 10 documents (P@10), and the percentage of that of monolingual retrieval using manually translated queries available in test datasets.

Datasets: The comparable corpus used as the translation resource in our experiments is UTPECC (University of Tehran Persian–English Comparable Corpora) version 2.0 (Rahimi & Shakery, 2011) which is constructed from 5-year BBC news in English and 5-year Hamshahri news in Persian. Moreover, UTPECC includes 14,979 alignments which aligns 10,724 BBC news articles with 5544 Hamshahri news articles.

For cross-language evaluation, two document collections are used:

                           
                              1.
                              Hamshahri collection (AleAhmad et al., 2009) consists of 166,774 documents in Persian with two sets of topics used in CLEF Persian–English bilingual track: Ham’08 with 50 topics 551–600 used in CLEF 2008 (Agirre et al., 2009) and Ham’09 with 50 topics 601–650 used for evaluation in CLEF 2009 (Ferro & Peters, 2009).

INFILE collection (Besançon et al., 2009) which is used for CLEF 2009 INFILE track and consists of 100,000 documents from Agence France Press (AFP) newswire stories in English. This dataset provides 50 topics in English where 40 of them are translated into Persian in (Hashemi & Shakery, 2014).
                                    1
                                 
                                 
                                    1
                                    Ten queries are omitted in Persian because of two reasons: (1) queries that have less than 4 relevant documents are omitted to avoid quantization noise into MAP, and (2) very specific queries that are difficult to translate into Persian are also removed. Identifiers of the removed queries are: 108, 110, 111, 112, 116, 119, 133, 134, 140, 147.
                                 
                              


                        Evaluation setup: The English words are stemmed, but the Persian words are not, because Persian stemmers have not shown promising results in information retrieval (Dolamic & Savoy, 2009; Hashemi & Shakery, 2014). Persian words are only normalized by replacing all orthographic variations of letters by one form. English and Persian stopwords are removed. In this regard, we used Persian stopword list provided by the CLEF campaign.
                           2
                        
                        
                           2
                           Also available at ece.ut.ac.ir/DBRG/Hamshahri/files/HAM1/persian.stop.txt.
                         Words with frequencies less than 6 are not modeled. Preprocessing steps are the same for documents of all collections and queries. All experiments are done using the Lemur toolkit.
                           3
                        
                        
                           3
                           
                              www.lemurproject.org.
                         We use only the title field of topics for evaluation and retrieve 1000 results per query. For each experiment, MAP and P@10 are reported. Two-tailed paired t-test is used to test whether the differences between performances of approaches are statistically significant. MAP values reported in the tables are marked with ▲ (▼) and △ (▽) to indicate statistical significance at the levels of 0.01 and 0.05, respectively.

To provide insight into the CLIR performance using a comparable corpus, we first report the performance of two baseline methods on the test datasets: (1) performance of monolingual information retrieval, and (2) performance of CLIR using FarsiDic dictionary.
                           4
                        
                        
                           4
                           
                              www.farsidic.com.
                         Monolingual results are obtained using the KL-divergence retrieval model with Dirichlet prior smoothing, in which μ is set to 1000. For cross-lingual information retrieval using the dictionary, we adopt Pirkola’s Structured Query (SQ) method (Pirkola, 1998), since translation alternatives in the dictionary are not associated with probabilities. In the structured query method, translation alternatives of a query word are combined using the INQUERY synonym operator. To provide a complete comparison, we also report the results of English–Persian CLIR using the dictionary by adopting the language modeling framework. To estimate a translation model according to the bilingual dictionary, we assumed uniform distribution on all translations listed for each word in FarsiDic. Cross-lingual results are then obtained by ranking the documents using Eq. (24). Table 1
                         shows the results of all the baseline methods. Although the CLIR performance using a machine-readable bilingual dictionary is good (above 50%) for some language pairs (Nie, 2010), it is relatively low for English–Persian (Hashemi & Shakery, 2014).

@&#RESULTS AND DISCUSSION@&#

In our experiments, we aim to answer the following questions:


                     Q1: What is the quality of translations extracted from the comparable corpus using the proposed approach?


                     Q2: How does the estimation of document language models impact the quality of translation models? How sensitive is our approach to the choice of parameter values?


                     Q3: Which of the KL- or JS-divergence is more suitable for measuring the similarity between source and target word models?


                     Q4: Does the integration of monolingual word co-occurrences into the estimation of document language models improve translation quality?


                     Q5: How does our approach compare to the existing approaches?


                     Q6: How much do different estimates of translation probabilities affect the quality of a translation model for the CLIR task?

The following sections provide answers to these questions respectively. In the experiments, the “ Estimation of Word Translation Probabilities” block in Fig. 1, is fixed to linear normalization using the top 4 ranked target words for each source word, unless otherwise specified.

In the first set of experiments, we answer the first question, Q1, mentioned above. In particular, we investigate the quality of extracted translation models from UTPECC comparable corpus using our proposed method for the CLIR task. Effectiveness of the similarity function defined in Eq. (2) is first evaluated. To compute similarity scores, language models of the source and the target documents are the ML estimate and the JM smoothed models, respectively. Therefore, we report the effectiveness of the following similarity function:

                           
                              (25)
                              
                                 
                                    
                                       m
                                       
                                          K
                                          L
                                       
                                    
                                    a
                                    
                                       sim
                                       KL
                                    
                                    
                                       (
                                       
                                          w
                                          s
                                       
                                       ,
                                       
                                          w
                                          t
                                       
                                       )
                                    
                                    =
                                    exp
                                    
                                       (
                                       
                                       −
                                       β
                                       
                                          D
                                          
                                             KL
                                          
                                       
                                       
                                          (
                                          
                                             θ
                                             
                                                
                                                   w
                                                   s
                                                
                                             
                                             
                                                ML
                                             
                                          
                                          
                                             ∥
                                          
                                          
                                             θ
                                             
                                                
                                                   w
                                                   t
                                                
                                             
                                             
                                                JM
                                             
                                          
                                          )
                                       
                                       )
                                    
                                    ,
                                 
                              
                           
                        which has two parameters that need to be set: β and λ (the JM smoothing parameter). In this regard, we employ 5-fold cross validation method with respect to the MAP measure to play down the impact of tuning. The reported MAP values are the average of the MAP values of 5 test folds. Table 2
                         shows the 5-fold results. The CLIR performance using the comparable corpus outperforms that using the dictionary for all datasets. Using translations of the LM-based method, we achieve CLIR effectiveness of 41.19% of that of monolingual retrieval for Ham’08, which is 50.12% improvement over dictionary-based CLIR.


                        Comparison of LM-based translation models and the dictionary: To investigate the reason of higher performance of CLIR using the comparable corpus than using the dictionary, we compare their translation models based on: (1) coverage of query words, and (2) quality of the translations. To determine coverage of the bilingual dictionary, query words with entries in the dictionary are counted, while for the comparable corpus, query words that occur in the source collection are counted. These statistics are reported in Table 3
                        . The table also includes the number of queries that are completely translated using each translation resource (i.e., all query words are translated). As shown in Table 3, a larger number of words are translated using the comparable corpus compared to the dictionary. Some OOV words which do not exist in the dictionary, such as ‘Wimbledon’, can be translated appropriately by the comparable corpus.

For translation quality comparison, we select queries that are completely translated by both resources and compare the quality of two translation models based on the retrieval performance of these derived query sets. We perform this experiment because the higher CLIR performance using the comparable corpus may be due to its higher coverage, not higher quality of translations. Table 4
                         shows the number of selected queries from each query set according to the mentioned criteria as well as the CLIR performance of selected queries using UTPECC comparable corpus and FarsiDic dictionary. As indicated in the table, using translations from the comparable corpus improves the CLIR performance for Ham’08 and INFILE datasets. To gain more insights into the results, we display the difference in average precision (AP) of each query obtained using translations from the comparable corpus and using the dictionary in Fig. 3
                        . The diagrams indicate that using one translation resource is not superior to the other for all queries, but clearly show that comparable corpora can be valuable complements to dictionaries due to the advantage of obtaining words that co-occur with translations. The machine-readable bilingual dictionary provides better translations for general words such as develop (stem of development in query 587 of Ham’08 query set). However, for query 563 of Ham’08 query set, “mad cow disease”, translation using the comparable corpus has more words related to the context of the query compared to that using the bilingual dictionary.

In this section, we investigate the answer to question Q2. In particular, to gain a better understanding of the similarity function in Eq. (2) using divergence values of Eq. (11), we study the sensitivity of CLIR performance to the parameters of the following function:

                           
                              (26)
                              
                                 
                                    
                                       m
                                       
                                          K
                                          L
                                       
                                    
                                    a
                                    
                                       sim
                                       KL
                                    
                                    
                                       (
                                       
                                          w
                                          s
                                       
                                       ,
                                       
                                          w
                                          t
                                       
                                       )
                                    
                                    =
                                    exp
                                    
                                       (
                                       
                                       −
                                       β
                                       
                                          D
                                          
                                             KL
                                          
                                       
                                       
                                          (
                                          
                                             θ
                                             
                                                
                                                   w
                                                   s
                                                
                                             
                                             
                                                ML
                                             
                                          
                                          
                                             ∥
                                          
                                          
                                             θ
                                             
                                                
                                                   w
                                                   t
                                                
                                             
                                             
                                                S
                                             
                                          
                                          )
                                       
                                       )
                                    
                                    ,
                                 
                              
                           
                        where language models of target documents are smoothed using the JM or Dirichlet prior smoothing method, i.e., 
                           
                              θ
                              
                                 
                                    w
                                    t
                                 
                              
                              
                                 JM
                              
                           
                         or 
                           
                              θ
                              
                                 
                                    w
                                    t
                                 
                              
                              
                                 Dir
                              
                           
                        . Three parameters of this equation that impact the quality of obtained translation models are the amount of smoothing for language models of documents, the noise reduction parameter β, and the number of selected translations for each word. We study the impact of these parameters in the following sections.

First, we vary the noise reduction parameter β over the values 1, 2 to 20 in steps of 2, while fixing all other parameters as 
                              
                                 λ
                                 =
                                 0.6
                                 ,
                              
                           
                           
                              
                                 μ
                                 =
                                 800
                                 ,
                              
                            and selecting the top 4 translations for each word. Fig. 4
                            shows the effect of this parameter on CLIR performance using translation models obtained from two variants of Eq. (26). All curves have an increasing trend at low values of the noise reduction parameter. This indicates the necessity of exponential transformation on the divergence values, which controls the weights of lower ranked translations in retrieval. The increasing trend of curves for Ham’08 and INFILE datasets is followed by a gradual degradation, which does not happen in the curve of Ham’09 dataset. This shows that lower ranked translations for the query words of Ham’08 and INFILE datasets are of higher quality than those for the query words of Ham’09 dataset, since reducing the impact of lower ranked translations causes a decrease in the performance of CLIR from Ham’08 and INFILE datasets.

The performance of CLIR is also sensitive to the number of translations selected for each query word. Thus, we investigate the sensitivity of MAP to this parameter when translations extracted with our approach are used. In particular, we examine the translations extracted using two variants of Eq. (26) with 
                              
                                 λ
                                 =
                                 0.6
                                 ,
                              
                           
                           
                              
                                 μ
                                 =
                                 800
                              
                            and 
                              
                                 β
                                 =
                                 10
                              
                           . Also, selected translations are linearly normalized using Eq. (20). The results are shown in Fig. 5
                           . With increasing the number of selected translations, the MAP curves rise to a level and then stabilize. The curves demonstrate that translation weighting by the defined functions is appropriate and does not allow noise words to pull down MAP. In addition, if we select a few number of extracted translations, MAP decreases as we lose some good translations or expansion words.

To study the impact of the amount of smoothing on the quality of translation models extracted using Eq. (26), we first vary the JM smoothing parameter (λ) and measure the performance of CLIR. In these experiments, β in Eq. (26) is set to 10. Fig. 6
                           (a) shows the effect of the JM smoothing parameter on the performance of CLIR. The results in Fig. 6(a) clearly demonstrate that λ is better to be higher than 0.3, since the probabilities of high entropy words should be less different for all documents. We get acceptable values of MAP for a wide range of λ. Moreover, for λ ≥ 0.8, MAP performance drops sharply, because contents of the documents are ignored in these cases and the reference language model is mainly used.

Similarly, we study the effect of the parameter of the Dirichlet prior smoothing method when similarity scores are estimated using Eq. (26). Fig. 6(b) shows the performance of CLIR in terms of varying the parameter of the Dirichlet prior smoothing technique and using the previous values for the other parameters (the number of selected translations is 4 and 
                              
                                 β
                                 =
                                 10
                              
                           ). The results in Fig. 6(b) confirm that language models of the target documents need smoothing but not substantially.

To further study the impact of the amount of smoothing on translation quality, we plot the sensitivity of average precision for each query to the amount of smoothing in Fig. 7
                           . The bar for each query in Fig. 7 shows the minimum, maximum, and mean of the average precision values obtained by sweeping the JM smoothing parameter (λ). A glance at the three diagrams shows us that the average precision of some queries are not sensitive to the smoothing amount, while that of some others are highly sensitive. To study the reason, we provide the entropy of the query words in Ham’08 query set in Fig. 8
                           . Entropy of words are calculated as (Tao & Zhai, 2005):

                              
                                 (27)
                                 
                                    
                                       H
                                       
                                          (
                                          
                                             w
                                             s
                                          
                                          )
                                       
                                       =
                                       −
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          n
                                       
                                       
                                          p
                                          
                                             (
                                             
                                                d
                                                
                                                   s
                                                   i
                                                
                                             
                                             |
                                             
                                                w
                                                s
                                             
                                             )
                                          
                                          log
                                          p
                                          
                                             (
                                             
                                                d
                                                
                                                   s
                                                   i
                                                
                                             
                                             |
                                             
                                                w
                                                s
                                             
                                             )
                                          
                                       
                                       ,
                                    
                                 
                              
                           where n is the number of alignments in Eq. (1) and 
                              
                                 p
                                 (
                                 
                                    d
                                    
                                       s
                                       i
                                    
                                 
                                 |
                                 
                                    w
                                    s
                                 
                                 )
                              
                            is the normalized frequency of word ws
                            in document 
                              
                                 d
                                 
                                    s
                                    i
                                 
                              
                           . The maximum possible entropy is log (14979) ≃ 9.6.

In Fig. 8, we also highlight the entropy of words of queries 551 and 580 in Ham’08 dataset, which, respectively, have low and high sensitivity to the amount of smoothing according to Fig. 7(a). As indicated in Fig. 8, the entropy of words of query 551 is considerably smaller than that of words of query 580. This shows that the quality of translations for high entropy words is much more sensitive to the smoothing parameter. Models of most such words need higher amounts of smoothing so that their corresponding queries achieve the maximum average precision.


                           Smoothing the language models of source documents: In this experiment, we study the quality of extracted translations using the following function:

                              
                                 (28)
                                 
                                    
                                       
                                          m
                                          
                                             K
                                             L
                                          
                                       
                                       a
                                       
                                          sim
                                          KL
                                       
                                       
                                          (
                                          
                                             w
                                             s
                                          
                                          ,
                                          
                                             w
                                             t
                                          
                                          )
                                       
                                       =
                                       exp
                                       
                                          (
                                          
                                          −
                                          β
                                          
                                             D
                                             
                                                KL
                                             
                                          
                                          
                                             (
                                             
                                                θ
                                                
                                                   
                                                      w
                                                      s
                                                   
                                                
                                                
                                                   JM
                                                
                                             
                                             
                                                ∥
                                             
                                             
                                                θ
                                                
                                                   
                                                      w
                                                      t
                                                   
                                                
                                                
                                                   JM
                                                
                                             
                                             )
                                          
                                          )
                                       
                                       ,
                                    
                                 
                              
                           where language models of source documents are also smoothed using the JM smoothing technique. To investigate the effect of smoothed language models for source documents on translation quality, we fix all the parameters in the above equation except the smoothing parameter for the source documents; β is set to 10 and the smoothing parameter λ for target documents is set to 0.7. Fig. 9
                            shows the performance of CLIR by sweeping the smoothing parameter of the source documents from 0 to 0.9 (step 0.1); 
                              
                                 λ
                                 =
                                 0
                              
                            is equal to the maximum likelihood estimate of language models of source documents in Eq. (25). As expected, a large amount of smoothing for source documents is not appropriate. In addition, a slight increase in MAP performance with a small amount of smoothing for source documents is observed, which can be ignored due to the higher computational complexity of Eq. (28) than Eq. (25).

In this section, we demonstrate the performance of CLIR that can be achieved using the translation model obtained by tuning the parameters in Eq. (25). Values of all three parameters are optimized for best MAP performance. We sweep the parameter λ over the {0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9} set of values, and vary the parameter β from 1 to 10. We also tune the number of selected translations for each source word (from top 1 to top 10). Table 5
                            shows the best achieved results as well as tuned parameter values.

The best performance for Ham’09 dataset is achieved when the number of selected translations for each word is two, which is lower than that for the other two datasets in the optimal settings of parameters. This is compatible with the quality of lower ranked translations for the query words of different datasets which can be inferred from Fig. 4.


                           Fig. 10
                            also shows the sensitivity of CLIR performance to the different combinations of the smoothing and the noise reduction parameter values for three datasets. The heatmap diagrams indicate that by using the default value for the JM smoothing parameter and a relatively wide range of values for the noise reduction parameter, we achieve levels of performance for CLIR not very different from the achievable performance by the tuned parameters.

In these experiments, we answer the third question, that is to compare the KL- and JS-divergence for measuring the similarity of word models, by investigating the effectiveness of the symmetric similarity function defined in Eq. (3). We use the JM smoothing method for estimating language models of the source and target documents; thus translations are extracted using the below similarity function:

                           
                              (29)
                              
                                 
                                    
                                       m
                                       
                                          J
                                          S
                                       
                                    
                                    a
                                    
                                       sim
                                       JS
                                    
                                    
                                       (
                                       
                                          w
                                          s
                                       
                                       ,
                                       
                                          w
                                          t
                                       
                                       )
                                    
                                    =
                                    exp
                                    
                                       (
                                       
                                       −
                                       β
                                       
                                          D
                                          
                                             JS
                                          
                                       
                                       
                                          (
                                          
                                             θ
                                             
                                                w
                                                s
                                             
                                             
                                                JM
                                             
                                          
                                          
                                             ∥
                                          
                                          
                                             θ
                                             
                                                w
                                                t
                                             
                                             
                                                JM
                                             
                                          
                                          )
                                       
                                       )
                                    
                                    ,
                                 
                              
                           
                        which has three parameters: the smoothing parameters in estimating language models of the source and the target documents, henceforth denoted by λs
                         and λt
                        , respectively, and the noise reduction parameter β. Note that setting 
                           
                              
                                 λ
                                 s
                              
                              =
                              0
                           
                         or 
                           
                              
                                 λ
                                 t
                              
                              =
                              0
                           
                         is equivalent to using the maximum likelihood estimator for the source or the target documents, respectively. In the following experiments, we fix the parameter β to 10 and search for the most appropriate values for the pair of the smoothing parameters. In the first experiment, translations obtained using different values of λt
                         are employed for CLIR. This experiment is done for three values of λs
                        : 0, 0.1, and 0.2. The performance of CLIR using the obtained translation models is shown in Fig. 11
                        . The first observation is that the curve for 
                           
                              
                                 λ
                                 s
                              
                              =
                              0.2
                           
                         has a large amount of scatter. This shows that the similarity function based on the JS-divergence is more sensitive to the smoothing parameters compared to that based on the KL-divergence, and there is a general trend to prefer lower values of λs
                        . In addition, the curves for 
                           
                              
                                 λ
                                 s
                              
                              =
                              0
                           
                         and 
                           
                              
                                 λ
                                 s
                              
                              =
                              0.1
                           
                         have a decreasing trend in contrast to the curves of Fig. 6(a). This is due to bidirectional comparison of word models in the JS-divergence. For the similarity function based on KL-divergence, according to Figs. 6(a) and 9, documents in the source language need a less amount of smoothing, while documents in the target language require more smoothing. In the simJS
                         similarity function, a document in one language is considered as both source and target documents, therefore determining the appropriate amount of smoothing for documents is not straightforward.

In the second set of experiments, we fix λt
                         and vary λs
                         from 0 to 0.6 (step 0.1) and obtain translation models. This experiment is repeated for three values of λt
                        . The performance of CLIR using the obtained translation models is depicted in Fig. 12
                         which shows an initial increasing and then decreasing trend for all datasets. The smaller the λt
                        , the more rapid the decrease in MAP. This indicates that the simJS
                         similarity function tends to prefer similar smoothing amounts for the source and the target documents. Finally, the achieved performance of CLIR using JS-divergence for translation extraction is lower than that achieved with KL-divergence which is due to the smoothing.

In this section, we seek to address the forth question: does the integration of monolingual word co-occurrences into the estimation of document language models improve translation quality? Toward this, we study the effect of considering word co-occurrences on the quality of English to Persian translations through the following similarity function:

                           
                              (30)
                              
                                 
                                    
                                       m
                                       
                                          K
                                          L
                                       
                                    
                                    a
                                    
                                       sim
                                       KL
                                    
                                    
                                       (
                                       
                                          w
                                          s
                                       
                                       ,
                                       
                                          w
                                          t
                                       
                                       )
                                    
                                    =
                                    exp
                                    
                                       (
                                       
                                       −
                                       β
                                       
                                          D
                                          
                                             KL
                                          
                                       
                                       
                                          (
                                          
                                             θ
                                             
                                                w
                                                s
                                             
                                             
                                                ML
                                             
                                          
                                          
                                             ∥
                                          
                                          
                                             θ
                                             
                                                w
                                                t
                                             
                                             
                                                JM
                                                −
                                                rel
                                             
                                          
                                          )
                                       
                                       )
                                    
                                    ,
                                 
                              
                           
                        in which language models of the target documents are expanded with related words. We choose to add related words to the models of the target (Persian) documents since we have not used Persian stemmer. Relationships between Persian words are estimated based on mutual information criterion using Hamshahri 2 collection (AleAhmad et al., 2009). This collection is composed of 318,000 documents and is different from the target collection used for CLIR. We estimate probabilities of relationships between Persian words using Eq. (17) by selecting the top 20 words related to each word. To compute the similarity scores in Eq. (30), parameter β and the parameter of the JM smoothing method are set to 10 and 0.5, respectively. The obtained translations are then linearly normalized by selecting different numbers of the top translations for each word in order to study how the quality of translations changes as their rank increases. In this regard, we vary the number of selected translations from 1 to 10. Fig. 13
                         provides a comparison between two types of translation models, obtained using the basic similarity function in Eq. (25) and using Eq. (30), by reporting the performance of CLIR when different numbers of the top translations for each word are used. The figure indicates that integration of word co-occurrences consistently leads to higher MAP values and also shows a more stable behavior.

Moreover, in Table 6
                        , we report the performance of CLIR using translations obtained by Eq. (30) where the number of selected translations per word is tuned. The results show improvements in translation quality over the best achieved results using the basic translation models, though the improvements are not statistically significant. The main reason for better performance of translation models obtained by Eq. (30) is translation of words with low frequencies in the comparable corpus. Average precision of queries that include low-frequency words is increased using translations obtained by Eq. (30). The 3 least-frequent words of Ham’08 query set in the comparable corpus, and the performance of their respective queries using translation models obtained without and with monolingual word co-occurrences (Eqs. (25) and (30), respectively) are: (1) “congest” (congestion) in query 578 where its AP changes from 0.0000 to 0.1018 by addition of monolingual word co-occurrences, (2) “merchandis” (merchandise) in query 590 where its AP changes from 0.3279 to 0.5592 by addition of monolingual word co-occurrences, and (3) “carpet” in query 589 where its AP changes from 0.3785 to 0.4017 by addition of monolingual word co-occurrences.

In this section, we address question Q5 by comparing our translation extraction method with Cocot (Talvensaari et al., 2007), Frequency Correlation-based (FC-based) (Tao & Zhai, 2005), and Spider (Sheridan & Ballerini, 1996) methods.


                        Comparison based on the performance of CLIR: We first compare the effectiveness of translation models obtained using different methods for the CLIR task. In Fig. 14
                        , we compare the performance of CLIR using translation models obtained from Cocot and our approach. Higher MAP values using the LM-based translation model demonstrate that our approach extracts better translations with more appropriate weights. Similarly, Fig. 15
                         depicts the performance of CLIR using the FC-based and the LM-based approaches for translation extraction. Adopting the translation model extracted by the FC-based method shows relatively low performance of CLIR. We believe that the main reason for this observation lies in disregard of shared documents between alignments in the FC-based method. This issue is considered in the Cocot method by creation of hyper documents and in our approach by involving the similarity of aligned documents. In addition, the diagrams in Figs. 14 and 15 reflect the sensitivity of MAP measure to the number of selected translations for each source word. These figures clearly indicate the more stable behavior of our approach in terms of the number of translations selected for each word.


                        Table 7
                         summarizes the results of the Cocot, FC-based, Spider, and LM-based approaches. To give the best possible credit to the Cocot, FC-based, and Spider approaches, we report the best CLIR performance that is achieved by varying the number of selected translations per word for these approaches. For the LM-based approach, the 5-fold results are listed. The results indicate that our approach is better in finding translations and their related words. There is a remarkable difference in the performance of CLIR using translations of the Cocot and Spider methods, although they share the same underlying model. We believe the reason for this difference is the addition of hyper documents in the Cocot method; groping alignments that share the same target documents into one alignment. The substantial impact of hyper documents comes from the fact that the number of unique English documents in UTPECC comparable corpus is almost twice the number of unique Persian documents. This also explains the observation about the performance of FC-based method which does not consider the existence of common documents between alignments. Finally, as demonstrated in Table 7, improvements of our method over FC-based and Spider methods are statistically significant in all cases. Except for INFILE dataset, improvements over Cocot method are also statistically significant.


                        Comparison based on a machine-readable dictionary: To compare the quality of extracted translations from the comparable corpus using different approaches, we use translations of FarsiDic dictionary as reference. The top 5 translations per word, extracted from the comparable corpus using each approach, are then evaluated according to the MAP measure. The results are shown in Table 8
                        , which show that our approach improves the accuracy of translations in both directions over the previous approaches.

In this section, we answer question Q6 of how much different estimates of translation probabilities affect the quality of a translation model for the CLIR task. In this regard, we evaluate different normalization functions defined for the second component of Fig. 1, based on the performance of CLIR. First, translation models obtained by different normalization methods on word similarities extracted by the Cocot approach are used for CLIR and obtained performance is shown in Fig. 16
                        . The figure shows that the similarity scores by the Cocot method do not fully reflect the quality differences among translation alternatives of a word. Thus, using linear normalization (Eq. (20)) is not suitable for estimating translation probabilities from the Cocot scores. In contrast, using exponential transformation in Eq. (22) allows to effectively benefit from the lower ranked translations and subsequently results in significant improvement in the performance of CLIR.

Next, we study the impact of normalization methods on word similarities extracted by the FC-based and our approaches. The obtained translation models are then used in CLIR whose performance is depicted in Figs. 17
                         and 18
                        . For the FC-based method, different normalization methods do not show significantly different performance of CLIR. One possible reason for this observation is almost identical similarity scores of translation alternatives assigned by this method. For our approach, exponential transformation is not suitable for normalizing the word similarities, since similarities are computed by applying an exponential function to the divergence values. However, the logarithmic normalization helps to get better estimates of translation probabilities. These experiments show that the selection of proper normalization method is dependent on the scoring function of translation extraction approaches.

For each translation extraction approach, the curves of different normalization methods per dataset have similar increasing/decreasing trend of MAP. However, for the Cocot and LM-based approaches, the MAP curve of one normalization method is consistently higher than the curves of the other normalization methods. This shows the importance of assigning appropriate translation probabilities to translation alternatives which could result in significantly different performance with the same ranking of translations.

In this paper, we proposed a language modeling approach to extract translation models from comparable corpora. The focus of our paper is to provide a more practical, effective method for the estimation of translation models without requiring additional linguistic resources. Our approach can be tuned easier compared to the existing methods. This is because the main parameter of our approach is related to the adopted smoothing method on document language models, and the default values of smoothing parameters for information retrieval are also suitable for translation extraction from comparable corpora.

The experimental results suggest that source words are better to be modeled using maximum likelihood estimation of document language models, because of much lower computational complexity in measuring word similarities. In addition, we find that ranking target words based on KL-divergence results in more accurate translations compared to that based on JS-divergence. Obtained results demonstrated that integration of monolingual word relationships into document language models improves the quality of translations for low-frequency words. By several experiments, we demonstrated that the proposed method can improve the translation quality as well as the performance of CLIR compared to the existing methods.

There are many possible directions to extend this work. In this paper, we focus on the estimation of word-based translation models. However, our approach provides a basis for translation of phrases by combining unigrams and bigrams in the estimation of document language models similar to Song and Croft (1999) and Srikanth and Srihari (2002). Another direction is to employ domain information in the modeling of documents. This can be helpful since comparable corpora are generally built from news articles that cover different topics. Therefore, to estimate the language model of a document, we can benefit from documents with the same topic. This would help to disambiguate the polysemous words.

@&#ACKNOWLEDGMENTS@&#

The authors would like to thank Prof. Djoerd Hiemstra for his helpful comments. This research was in part supported by a grant from Institute for Research in Fundamental Sciences (No. CS1393-4-43), the National Grand Fundamental Research 973 Program of China (No. 2014CB340405), the Research Grants Council of the Hong Kong Special Administrative Region, China (Project no. CUHK 413213), and Microsoft Research Asia Regional Seed Fund in Big Data Research (Grant no. FY13-RES-SPONSOR-036).

@&#REFERENCES@&#

