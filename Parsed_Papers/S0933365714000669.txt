@&#MAIN-TITLE@&#Cross-hospital portability of information extraction of cancer staging information

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           We evaluate the portability across hospitals of machine learning-based text mining systems for colorectal cancer staging (TNM and ACPS).


                        
                        
                           
                           We present an architecture based on feature selection that allows to build a portable classifier with minimum cost, and we reach state-of-the-art performance.


                        
                        
                           
                           The results show that it is feasible to apply an existing TNM classifier to a new hospital without extra training, given that there is a feature normalisation step.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Machine learning

Text mining

Information extraction

Cancer staging detection

Colorectal cancer

@&#ABSTRACT@&#


               
               
                  Objective
                  We address the task of extracting information from free-text pathology reports, focusing on staging information encoded by the TNM (tumour-node-metastases) and ACPS (Australian clinico-pathological stage) systems. Staging information is critical for diagnosing the extent of cancer in a patient and for planning individualised treatment. Extracting such information into more structured form saves time, improves reporting, and underpins the potential for automated decision support.
               
               
                  Methods and material
                  We investigate the portability of a text mining model constructed from records from one health centre, by applying it directly to the extraction task over a set of records from a different health centre, with different reporting narrative characteristics. Other than a simple normalisation step on features associated with target labels, we apply the models from one system directly to the other.
               
               
                  Results
                  The best F-scores for in-hospital experiments are 81%, 85%, and 94% (for staging T, N, and M respectively), while best cross-hospital F-scores reach 84%, 81%, and 91% for the same respective categories.
               
               
                  Conclusions
                  Our performance results compare favourably to the best levels reported in the literature, and—most relevant to our aim here—the cross-corpus results demonstrate the portability of the models we developed.
               
            

@&#INTRODUCTION@&#

As new technologies for health care are deployed, increasing access to electronic information opens opportunities for improved productivity and decision support. Pathology reports are one rich source of valuable patient information: these contain cell and tissue data and are often critical in determining presence of certain diseases and performing diagnosis. Pathology reports are typically semi-structured, containing distinguishable components but with most information in free text (though often abbreviated or terse). One specific important use of information in certain pathology reports is in determining cancer staging, i.e., describing the extent of cancer within a patient. This is most usually represented via the TNM (tumour-node-metastases) scale (described below), a global standard defined by the American Joint Committee on Cancer (AJCC) and International Union Against Cancer (UICC) [1]. Information to determine cancer staging is typically contained within the text of pathology reports but needs to be extracted, in particular for conversion to the TNM scale. While there is a proposed move towards synoptic (highly structured) reports, few reports (historically, virtually none) are in this format and text processing is required to automatically access the required information.

Identifying cancer stage is a critical clinical and analytic task. For an individual patient, staging information is essential for clinical decision making and determining optimal treatment [2]. For population-based cancer registries, staging data is crucial in determining overall treatment outcomes and planning research and resource allocation [3]. However extracting data from free-text pathology reports is a resource-intensive activity requiring skilled staff to manually process each report. Even when protocols for collection exist, different studies have found serious problems with completeness [4–6], along with manual encoding errors [4,5,7,8]. These problems are considered sufficiently large to be part of the Cancer Australia's National Cancer Data Strategy, where it is noted that “Traditional methods of clinical cancer registration are likely to be too labour-intensive to be sustainable”.
                        1
                     
                     
                        1
                        canceraustralia.gov.au/publications-resources/cancer-australia-publications/national-cancer-data-strategy-australia (accessed 17 April 2014).
                      Consequently, the ability to automatically extract staging data from pathology reports will assist both individual patient care and population-based analysis of outcomes. We explore the application of text mining, a combination of natural language processing (NLP) and machine learning (ML) techniques, to this task.

Text mining tools that perform well on the task of extracting staging information from pathology reports are likely to have important impact on clinical practice and collection of data for cancer registries and population-based studies. They can also be the building blocks of data mining methods that exploit large data for cancer prognosis [9,10]. The main challenges for developing widely-applicable text mining tools are the need for expert domain knowledge (by means of hand-coded rules or manual annotation), and the portability to different environments. However a recent systematic literature review [11] has shown limitations of existing text mining tools for the biomedical domain, which tend to be very context-dependent and not readily portable. The question of portability has been little studied, but is central to developing systems robust enough for wide clinical implementation.

In this paper, we describe a text mining tool based on using machine learning with light training-data annotation needs, for the task of extracting cancer-staging information for colorectal cancer. Our tool relies on manually annotated pathology reports for learning and for evaluation, where domain experts provide the cancer staging codes that correspond to the textual information in each pathology report. Our annotation requirement is at document level, i.e., labels are assigned to full documents (e.g., a full report is given the “staging N1” category) rather than finer linguistic levels (e.g., the phrase “3 positive lymph nodes” is given the “staging N1” category). This considerably alleviates the annotation effort (cf. [12]) and makes the techniques more scalable, given that we can address portability across different styles of pathology reports.

We specifically investigate the issue of portability of our system across distinct data sets from different hospitals. We extend our previous work on this topic [13], and apply an information extraction model that was trained from a collection of reports obtained from one health precinct—Melbourne Health—directly to a collection of reports from another—Barwon Health: these are two distinctly different health centres covering different geographical regions in the state of Victoria in Australia. The data involves differences in pathology reporting formats and authoring patterns, but also different linguistic characteristics in the authored reports. Initial performance was improved by adding a simple term-mapping approach, directed by a feature selection algorithm, resulting in clear performance stability.

Initial work over this multi-site dataset was previously reported in [14]. In the current article, we extend the analysis of the performance of our system, and also compare our results with the use of an open-source tool—MedKAT/P 
                     [15]—for automatically processing pathology reports. Our analysis demonstrates the limitations of off-the-shelf systems, and highlights the importance of using feature selection for normalising different datasets.

@&#BACKGROUND@&#

Our specific focus is on extracting staging information for colorectal cancer tumours from pathology reports. In particular, our main task is to extract values for categories in the widely-used TNM cancer-staging system, which describes the extent of cancer in a patient's body. As well as providing a structure for planning a specific patient's individualised treatment, the use of a globally-adopted scale facilitates the collection of information from cancer-treatment sites and registries around the world for use in analysis.

The categories in the TNM system are as follows: T (size of original (primary) tumour); N (nearby lymph nodes that are involved); and M (distant metastasis (spread to another part of the body)). We also apply the Australian clinico-pathological stage (ACPS) 
                     [16] code for each of the reports: ACPS is a scheme specific to colorectal cancer, based on the original Dukes classification for colorectal cancer, that uses a letter A–C to designate the depth of tumour through bowel wall and the letter D to denote metastatic disease. Each of these categories is assigned a score which can be combined to determine cancer staging, as illustrated in Table 1
                     . The description of each of the categories is given in Fig. 1
                     , as applied by the Royal Melbourne Hospital for encoding colorectal cancer. Our main task, then, is to extract the values for T, N, M, and ACPS from pathology reports.

Work on extracting staging information has previously been performed by Nguyen et al. [17–19], with a focus on lung cancer. Their initial approach used machine learning (ML) techniques, specifically support vector machines, and illustrated the difficulty of primary tumour stage detection (T), with a best accuracy of 64%. In a follow-up paper they explored richer annotation and a combination of ML and rule-based post-processing [18]. They performed fine-grained annotation of stage details for each sentence in order to build their system (e.g., phrases such as chest wall invasion, diaphragm invasion, etc.), and observed improvements over a coarse-grained (document-level) multiclass classifier. However, the authors explain that the annotation cost is high, and the performance for “staging T” was still low (65% accuracy). In more recent work [20], they use a lightweight NLP pipeline to identify and map entities to SNOMED-CT concepts.
                        2
                     
                     
                        2
                        
                           SNOMED-CT (systematized nomenclature of medicine-clinical terms) is a large electronic collection of clinical terminology, including terms, codes, and related items; it is maintained by the International Health Terminology Standards Development Organisation (IHTSDO): http://www.nlm.nih.gov/research/umls/Snomed/snomed_main.html (accessed 17 April 2014).
                      After mapping to concepts, they then apply hand-authored rules to those concepts to extract TNM information, reporting accuracies of 78%, 89%, 95% for extracting T, N, M respectively. Evaluation was performed over a set of pathology reports that were not examined during system development. Given the use of a general-purpose NLP processing pipeline, their performance may transfer better to another corpus of reports, but this has not been explicitly verified.

Patrick et al. [12] outline an end-to-end system for detecting and summarising cancer reports for inclusion in a cancer registry. Their system both classifies reports from radiology departments as being related to cancer, and automatically tags for a number of semantic categories, including T, N, M. They specifically measure the accuracy of their semantic tagger by training on records from one hospital site and testing on another, reporting a performance drop of approximately 5% to 15% as compared to training and testing on data from the same site (which they report to be around 97%; specific accuracy for tagging T, N, M is not reported). A constraint of their approach is the cost of performing fine-grained annotation of the text (i.e., marking the linguistic phenomena).

Other related work includes that of Cohen et al. [15], who define an extensive knowledge model for representing knowledge about cancer, known as “cancer disease knowledge representation”, and link it to hand-authored inference rules to process unseen data. They report high performance over 9 target classes, which do not overlap with our specific staging categories
                        3
                     
                     
                        3
                        Their target classes for the evaluation are the following: anatomical site, histology, dimension, date, grade, gross description part, lymph node, primary tumor, metastatic tumor.
                      for a hand-annotated 300-report dataset. Their system seeks to build a representation of the domain by relying on human experts, and its portability to a different dataset or class-set could be problematic. The authors provide an open-source implementation of their method, and we tested its performance and portability over our datasets. We describe their tool (MedKAT/P) and our application of it in Section 3.4.

Our own previous work [13] developed a system for extracting tumour- and staging-related information from pathology reports provided by the Royal Melbourne Hospital (RMH) in the city of Melbourne, Australia. That system identified specific concepts in pathology reports; these items included both nominal and numeric concepts, corresponding to categories of concepts in a standard structured oncology reporting form used at RMH. Concepts were extracted and assigned to their appropriate categories, including: tumour site; no. of nodes examined; no. of positive nodes; tumour length/depth/width; etc. Best results for the tested staging categories in cross-validation were 82%, 73%, 75% F-scores for T, N, ACPS respectively (we did not evaluate staging M). We provide below a description of the dataset and techniques used for that work, as we will use this dataset for our experimentation.

We set up our challenge of extracting values for each of our categories of interest—i.e., T, N, M, ACPS—as a multiclass document classification problem: for each given pathology report, we classify it according to one of the possible (discrete) values available for each of these categories. Assigning values to each category then allows the cancer staging to be determined, following Table 1. In order to train classifiers and evaluate them, we constructed a corpus with annotated data from two different hospitals. In this section, we first describe the target tasks and the datasets used in detail. We then describe our feature representations and feature selection approach, along with our classification methods, followed by the configuration of the MedKAT/P tool that we used as a baseline comparison. Finally, we describe the evaluation metrics we used.

We work with two distinct collections of de-identified pathology reports
                           4
                        
                        
                           4
                           The radiology reports were checked at the hospitals to verify that no confidential patient information was present in the text. This only required removing the meta-data in the headings, and not any text from the radiology reports.
                        : one from the Royal Melbourne Hospital (RMH) and the other from Barwon Health. The RMH collection contained 193 sample reports while the Barwon collection contained 201. All reports in the collections were annotated as described below.

In the case of the RMH, annotations were obtained via the hospital workflow: for each pathology report in textual form, we were provided with a corresponding structured form, manually completed by hospital staff, that included fields and filled-in values covering the staging information we are interested in extracting automatically (see the example in Fig. 2
                        ). In some cases the staging information is explicit in the text (e.g. “stage=T3N1″) but in other cases it has to be inferred from the textual content by the encoders (e.g. the code N1 from the text “carcinoma is present in one of twenty lymph nodes”). These structured forms thereby effectively serve as a gold-standard for the extraction task for the RMH data.
                           5
                        
                        
                           5
                           During our previous research in [13], we identified some manual encoding errors, which were corrected for this work.
                        
                     

For the Barwon Health data, one clinician annotated the reports manually for the staging TNM and ACPS fields, following the guidelines shown in Fig. 1 and example reports from RMH. The annotator observed that also in this case there were explicit and implicit references to the staging categories, but the required information was always present in the text; his annotation constitutes the Barwon Health gold-standard.

The reports from both sources have a similar structure, with three main sections: macroscopic description; microscopic description; and conclusions (or diagnosis). However the Barwon reports tend to have more structured subheadings (e.g., nodes, margins, etc.) and also tend to be longer (658.6 words per report on average, compared to 438.7 words on average for RMH report). As illustration, we show below the text for the “macroscopic description” from two reports from the different hospitals:
                           
                              
                                 Barwon Health:
                              
                              
                                 “Specimen received pinned out on board and oriented as proximal and distal end. A piece of mucosa 30×25×5
                                 mm showing a sessile polyp 14×23
                                    mm. Assuming proximal margin as 12 o’clock the specimen is inked blue along 3 o’clock half and black along 9 o’clock. 1A-1 proximal margin, 1B-3 1C-1 1D-2 central TS from proximal to distal margin, 1E-1 distal margin. All submitted. (DJ/db)”
                              


                                 “Colon FAP: A colectomy specimen, including terminal ileum 45mm long and 20mm diameter, appendix 50mm long, colon 1100mm long and 20–40mm diameter with attached mesocolic fat up to 50mm wide. The peritoneal reflection is not identified. The mucosa shows 20–30 small nodular polyps, up to 5mm in diameter, located predominantly within the ascending colon. No definite invasive tumour is identified macroscopically. A few lymph nodes are noted in the fat.”
                              

We can see that the language is very dense in both cases, and that the Barwon Health example shows codes that may be specific to the hospital. One of our goals is to measure empirically how well systems trained on data from one hospital perform over data from the other hospital.

When setting our experiments, we observed that a few of the RMH reports had incomplete annotations for one or more of the categories of interest. We removed those cases from our experiments to facilitate the cross-corpora experimentation.
                           6
                        
                        
                           6
                           The removal of these pathology reports accounts for the minor differences to the scores reported in [13] for staging T and staging ACPS.
                         
                        Table 2
                         shows the labels and distribution of reports per hospital. We see that the TNM staging categories have similar distributions, and the most frequent class is the same in both hospitals. For the case of “staging ACPS”, however, the most frequent class is different, depending on the hospital (staging C for Barwon Health and B for RMH).

For feature representation, we use a combination of lexical and semantic features that we previously tested in [13] for document classification. For lexical features, we applied the Genia Tagger [21] to tokenise the text, and registered the occurrences of the word-forms in the sentence, removing punctuation marks. In [13], we also used the Genia Tagger to lemmatise words; however, lemmatisation led to a slight performance degradation and hence we do not apply that processing step here. We normalised all dates into an abstract “DATE” feature, and all numbers into a “NUMBER” feature.

For semantic information, we use the Metathesaurus from the UMLS 
                        [22], which contains a set of ontologies for the biomedical domain with semantic relationships between terms (e.g. synonyms and hypernyms). We apply this resource by parsing each sentence in the document using the MetaMap analyser (version 2012) [23], using default parameter settings, except for the word sense disambiguation module, which provides a single concept for the mapped phrases. As a result we obtain concept unique identifiers (CUIs), which map the text onto ontological concepts. This allows us to identify connections between different word forms for the same concept: e.g., terms “disease” and “disorder” are listed under the same CUI in the UMLS; this connection is potentially useful for measuring text similarity. In [13], we also used matches against SNOMED
                        
                           7
                        
                        
                           7
                           
                              http://www.nlm.nih.gov/research/umls/Snomed/snomed_main.html (accessed 17 April 2014).
                         terms as a semantic feature; however, this did not perform as well as using all of UMLS, hence we do not explore use of this feature here.

Our final feature extraction step targets negated concepts, using the tool Negex [24]. This tool contains a module using context-dependent regular expressions to indicate whether a clinical condition has been negated.

As an example of the feature representation, we show in Table 3
                         the full feature set for the sentence “No evidence of metastatic tumour is seen in any of 11 lymph nodes” from our corpus, before applying the feature selection step. The example illustrates how the concept “tumor (metastatic)” is marked as negative evidence.

In order to deal with feature-space sparseness, we apply a feature selection technique that performed robustly in [13]. The method consists of a correlation-based feature subset selection technique [25] which considers the individual predictive ability of each feature and the redundancy of each subset. We used the Weka toolkit's implementation of this algorithm, using Best-First search, a cache-size of one element, and five levels of backtracking. This technique reduced the number of features from thousands to fewer than a hundred. We present in Table 4
                         an example of the output of the feature selection process for “staging T” when trained over the RMH dataset for testing over Barwon Health. We can see that most of the selected features are lexical, along with some semantic features (all positive). Some explicit category mentions are present, as well as other terms that are found as discriminative features for the task.

We performed automatic document classification of TNM and ACPS staging using an approach based primarily on ML, augmented with a small rule-based system for detecting lymph node counts.

To learn our models, we applied a set of supervised classifiers that has been widely used in the text mining literature in order to compare their performances over our dataset. We used the Weka toolkit [26] (version 3.6.2) for the experiments.

We selected methods from different ML families: Naive Bayes as a simplistic method that assumes independence among features; support vector machines (SVM) as representative of kernel-based methods; Bayesian networks from the graphical models class; and random forests as an example of an ensemble classifier. We used the default parameter settings of Weka for each of the classifiers, which may not be optimal for classifiers such as Bayesian networks, where there are different options for structure learning, or SVM where different kernels and parameter settings can be applied. As a baseline, we used the majority class classifier, whereby the test instances are assigned the label most frequently seen in the training data.

We also applied feature selection as mentioned above, in order to reduce the size of the feature space for the different ML methods. We used the same feature selection method in our error analysis of cross-corpora classification, in order to identify the best features per hospital, and manually normalised the different spellings of the references to categories in the text (see Section 4).

For “staging N”, an indirect way to obtain its value is to detect the number of lymph nodes that have been mentioned as metastatic in the report, and use a deterministic mapping from this number to the staging N code. For instance, if 1–3 lymph nodes are found positive then the corresponding “staging N” code is 1 (see Fig. 1 for the full mapping). For this experiment, we applied a simple rule-based system to detect the number of positive lymph nodes, and then assigned the corresponding code as output for “staging N”. The NLP pipeline we used for this component was separate from the pipeline of the main feature extraction process described above.

The first step in this process is number normalisation, i.e., converting any numbers written in full into numerals—e.g., the text twenty-three would be replaced by 23.
                              8
                           
                           
                              8
                              This may lead to spurious matches not related to lymph nodes, but this does not matter as these will still be unlikely to match the patterns described. Moreover, this transformation is only used in the pipeline for counting lymph nodes, so it does not affect the other parts of our experiments.
                            We then searched for matches to a regular expression (regex) in the report, which was manually constructed on the basis of an examination of 25 randomly selected reports from each hospital. Most statements relating to node counts were of the following forms:
                              
                                 (1)
                                 
                                    1 of 6 mesenteric lymph nodes contains tumour.

Metastatic carcinoma in 1 of 9 lymph nodes
                                 


                                    6 out of 48 lymph nodes contain metastatic adenocarcinoma


                                    3/113 regional nodes, apical nodes clear, …

We thus created a regular expression (regex) pattern to match the bold-faced text in the above examples. The pattern was designed to match a sequence of digits, followed by either a ‘/’ character (with or without spaces), the word of or the sequence out of, then another group of digits followed by any sequence of characters before the word node or nodes in the same sentence. This pattern often matched several times internally in the text, since there were frequently interim counts mentioned within reports. In general, however, the final match was a summary, aggregating all other counts mentioned in a report, so priority was given to the final match. The first set of digits in a match was converted into a count of cancerous nodes, while the second was the count of the total number of nodes examined. When no match was found in a report, both counts were predicted to be zero.

Over the development set, this very simple approach obtained an accuracy of 88% for cancerous node counts, although approximately half of these correct classifications came from the default prediction of zero. We performed an error analysis over the development set, and the failures were due to slight phrasing various variants (such as the presence of a further in 2 of a further 11... nodes), the last match heuristic being invalid when multiple specimens were examined in a single report, or match failures due to node being written in upper case (which could be easily fixed).

We initially investigated an approach to the problem using a dependency parser, then post-processing the output to identify the relevant vertices in the graph. However the nature of clinical language means that the parses produced tend to be noisier than for more polished prose. An initial inspection of some sample parses indicated that the parse inaccuracies would make it difficult to write post-processing rules for determining the relevant vertices which were broad enough to handle all of the parse inconsistencies. Furthermore, the fact that the regex-based approach produced such high accuracy (the high baseline from the default zero prediction is immaterial here as it would affect all approaches equally) indicates that the extra complexity and effort associated with a parser-based approach would be difficult to justify for these datasets, even if the parses were of high quality. On the other hand, this regex-based approach relies somewhat on the fairly homogeneous methods for encoding tumour counts in these reports, so is slightly tuned to the dataset. It is possible that a dependency-based approach would be more generalisable to data from other hospitals. However, since creating regex patterns which worked for both of these two hospitals was fairly straightforward, it seems likely this approach could be extended to handle variations in the phrasing found in new datasets on a case-by-case basis with relatively little effort.

As mentioned in Section 2, we made use of MedKAT/P 
                        [15] to provide a comparison to our techniques. MedKAT/P
                        
                           9
                        
                        
                           9
                           
                              http://sourceforge.net/projects/ohnlp/ (accessed 17 April 2014).
                         is an open-source system for analysis of cancer characteristics in clinical reports; it defines a knowledge representation model for cancer, and automatically populates instances of this model from free-text pathology reports. The knowledge representation includes characteristics that are relevant to our work, such as cancer grade, cancer stage and counts of examined and positive lymph nodes. While our use of MedKAT/P is strictly as baseline comparison, we provide details here of our specific configuration for the purposes of reproducibility.


                        MedKAT/P uses Apache UIMA,
                           10
                        
                        
                           10
                           
                              https://uima.apache.org/ (accessed 17 April 2014).
                         a framework for integrating NLP components, and its semi-structured output uses the UIMA type system. It applies a preconfigured pipeline, including a sentence splitter, tokeniser, POS-tagger and shallow parser. It first identifies “leaf concepts” (according to the knowledge model) using a combination of dictionary lookup (with external resources such as a disease ontology for relevant terms and synonyms) and matching of hand-coded regular expressions. These concepts are then inserted into appropriate slots in the knowledge model using manually-created rules based on the relationship between components of the text.

We applied MedKAT/P using the default configuration from the code repository.
                           11
                        
                        
                           11
                           Actually, the codebase, which had not been updated since 2009, had an unobtainable outdated dependency, so we first updated the code to be compatible with the newer version of the dependency. We therefore cannot guarantee that we are running the same configuration as reported by Coden et al. [15].
                         We supplied our pathology reports in unaltered form (except for replacing tumour with tumor, to match MedKAT/P's assumption of US English spelling), but made no other changes, which may mean that MedKAT/P was operating suboptimally. In particular, it assumes the existence of particular sections in the reports which do not align with those in our dataset. Nonetheless, MedKAT/P detected a large number of annotations for each pathology report.

We used MedKAT/P in two ways: (i) to predict staging values as a baseline comparison for our staging classification, and (ii) as a baseline for the detection of lymph node counts described in Section 3.3.2.

Extracting lymph node counts from MedKAT/P is straightforward: we searched for instances of appropriate features in the output produced by MedKAT/P for each document—i.e., the UIMA feature PositiveNodes attached to a SCRLymphNodesReading type; the node count was set to the final occurrence of a non-zero value for this feature. If there is no match, a value of zero is given. This frequently determined the number of positive nodes successfully, allowing us to determine the “staging N” value.


                        MedKAT/P has in its cancer knowledge model certain specific types which would seem to closely fit our staging classification needs: e.g., the UIMA SCRStage type has features StageType and StageValue. We expected to be able to correlate these values with the various staging TNM and ACPS levels; however, this type was never produced for our corpora. There are other candidate MedKAT/P annotation types for indirectly inferring staging values: e.g., the UIMA types SCRPrimaryTumorReading and MetastaticTumorReading, which can also be associated with a SCRGradeValue type to indicate the severity of the tumour. These types were present to some degree in the MedKAT/P output, but there was no obvious systematic way to directly convert the associated feature values into TNM and ACPS staging classifications.

Thus, to utilise the MedKAT/P output, we used the features in a lightweight machine-learning approach. Decision Lists 
                        [27] allowed us to obtain the degree of association between labels and annotations. We applied the log-likelihood ratio to obtain the association scores, following Eq. (1), where in case of Pr(label|annotation)=1 the denominator is smoothed with the value 1/frequency(annotation):


                        
                           
                              (1)
                              
                                 Log
                                 −
                                 likelihood
                                    
                                 (
                                 annotation
                                 ,
                                    
                                 label
                                 )
                                 =
                                 log
                                 
                                    
                                       
                                          
                                             
                                                Pr
                                                (
                                                label
                                                |
                                                annotation
                                                )
                                             
                                             
                                                Pr
                                                (
                                                ¬
                                                
                                                label
                                                |
                                                annotation
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

This method allowed us to learn which annotations from MedKAT/P are strongly correlated with our target labels. We show in Table 5
                         the top annotations for each category for RMH and Barwon Health in the respective decision lists. They refer to different characteristics of the tumour, and include codes that appear in the pathology reports (e.g. ACPS-B). Note that the full decision lists are long rankings of annotations, which have an associated weight and label for each of the characteristics that are detected in the documents by MedKAT/P.

Once the decision list is built, we annotate each pathology report with MedKAT/P first, then find in the decision list the matching annotation with highest score, and finally assign the corresponding label. In our experiments, we used the decision list learned from one hospital to annotate pathology reports from the other.

As the main evaluation metric, we use micro-averaged F-score. F-score is the harmonic mean of, and thereby balances the contribution of, precision and recall, and is the standard metric of choice for evaluating performance of text mining and other related tasks (e.g., information retrieval). Given our task setup (multiclass classification, with systems always predicting a single class), F-score is equivalent to accuracy for the binary category (staging M). However, for the other categories F-score is always lower than accuracy, since F-score does not take into account true negatives. We considered this metric to provide a better indicator of the practical value of the classifiers.

In order to measure the statistical significance of our comparisons, we implemented a non-parametric test known as random shuffling [28]. The advantage of this kind of test is that it does not require any assumption regarding the underlying distribution of the data.

For the “within-corpus” experiments (experiments using data from a single hospital for both training and testing), we perform 10-fold cross validation without stratification. This technique is widely used as a way to maximise the use of annotated data, in order to robustly evaluate systems over unseen test data while minimising the possibility of bias.

For the cross-corpora experiments, we used the full dataset for each hospital to train a model, and tested over the data from the other hospital. We performed a further experiment by randomly subsampling 90% of the data from each hospital, and repeating the experiment 50 times, in order to measure the effect of introducing small variations in the training and test datasets.

@&#RESULTS@&#

We first present the in-corpus (same hospital) results of our experiments, followed by the cross-corpora evaluation of the classifiers to test the portability across hospitals. Finally we provide the results of MedKAT/P and a rule-based approach as another comparison.

We first perform experiments separately for each hospital. Table 6
                         shows the scores for our first evaluation for the different methods and staging types over the RMH dataset.
                           12
                        
                        
                           12
                           The experiments for some of the classifiers for “staging T” and “staging ACPS” were already reported in [13]; the scores are lower for the latter category in this case, due to the removal of incomplete reports from the corpus, as described earlier.
                         The best results for the set of classifiers are obtained for “staging N”, with significant improvements over the baseline (p
                        <0.01). For “staging M” the systems reach the highest F-score, but the naive baseline is also high for this case and the differences are not significant. For “staging T”, only SVM is able to significantly improve beyond the baseline. Finally “staging ACPS” has much lower performance overall, even though classifiers are significantly above the baseline; we analyse the reasons for this in Section 5.

We performed the same experiment over the Barwon dataset, and the results are shown in Table 7
                        . F-score is above 80% in all cases, and there are significant improvements over the baselines, with gains even over the very strong baseline for “staging M”, for all systems except that using Bayesian networks. The results over both datasets illustrate that, when we have labeled data from the same source, it is possible to build accurate classifiers to find staging information in the text, with the exception of “staging ACPS” for RMH data; as mentioned above, this is analysed in detail in Section 5.

For our second experiment we explored cross-corpora classification, where the models are trained over the Barwon dataset and tested over RMH data. The results are shown in Table 8
                        . We see large drops in performance from the results in Table 6 for all categories and classifiers. There are some cases with statistically significant improvement over the baseline, but the performances are low overall. These low results are confirmed when training over RMH data and testing over Barwon data (see Table 9
                        ). Only “staging M” scores high in F-score, but there is no improvement over the majority class baseline.

For error analysis, we manually analysed the top features resulting from the feature selection process described in Section 3.2, and observed that some of these features explicitly included the labels (e.g. “T2” or “pT2” for “staging T”). This helps explain the strong performance of the in-corpus experiments. However, the way labels are explicitly mentioned in text have differences depending on the corpus, and this is a clear source of error. Table 10
                         shows the features from the automatic feature selection step which are directly linked to target labels: we can see that different codes are used in the different datasets, and our cross-corpora classifiers do not detect these variations.

Following this analysis, we performed another experiment after normalising the equivalent terms in Table 10 into a single representation in both corpora: e.g., labels pT4, pT4b, T4 from the RMH documents were mapped to T4 as used in the Barwon documents. We then repeated the cross-corpora experiments. Tables 11
                         and 12
                         show clear improvement in performance in all cases, even reaching in-corpus performance in some cases. Bayesian networks perform very robustly when tested over RMH data, and SVMs are the best option when tested over Barwon. For the three TNM categories, the performance suggests that we can perform accurate cross-hospital annotation, with “staging ACPS” remaining as the problematic category.

With regards to the classifier choice, we ran pairwise statistical significance tests following the F-score ranking of systems. This allows us to compare if the difference in score between one system and the one ranked below is significant according to the random shuffling test. We present the results in Tables 13
                         and 14
                        . We can see that, as would be expected, the differences between techniques are not statistically significant, and we only see a significant difference (p
                        <0.01) for a single case: Naive Bayes over the Barwon-test data for “staging N” (cf. Table 14). This shows that the different classifiers perform comparably well with default parameters. It could be the case that by optimising parameters some of the techniques would improve further, and we leave this analysis for future work.

Finally, so as to measure the effect of introducing small variations in the training and test datasets, we ran an experiment by randomly subsampling 90% of the data from each hospital, and repeating the experiment 50 times. The results are shown in Tables 15
                         and 16
                        . We can see that the same trends remain, and we obtain statistically significant gains over baseline for all classifiers and staging categories (except for the very biased “staging M”).

As explained in Section 3.3.2, for “staging N” we can also obtain its value in an indirect, rule-based way. Using this approach, we can extract the number of positive lymph nodes found, and map this number to the “staging N” code. We ran this experiment and achieved a strong F-score of 96.2% over RMH data, and 85.1% over Barwon data, which is higher than the in-corpus results. This experiment suggests that this simple method is a good practical alternative to test first over unseen data.

For our final experiment, we evaluate the performance when using MedKAT/P, as explained in Section 3.4. We use decision lists for all categories, and for “staging N” we also infer the code from the detection of lymph nodes from MedKAT/P. The results of the cross-corpora experiments are shown in Table 17
                        . There is a large drop in performance in comparison to our previous approach for all categories. Note also that the indirect approach of detecting lymph nodes performs much worse than our rule-based method. One of the reasons for this could be the low inter-annotator agreement for positive-lymph-nodes reported in [15]. For “staging M”, the results seem high, but the classifier simply mimics the majority class baseline.

@&#DISCUSSION@&#

We have evaluated the performance of various text classifiers for cancer staging identification from pathology reports from two different hospitals, specifically extracting TNM Classification and ACPS information. In particular, we performed a cross-corpora classification experiment. We initially observed a significant drop in performance in comparison to the in-corpus tests. However, we semi-automatically identified text features that are directly linked to staging labels, and repeated the experiment after normalising these references. This simple adjustment led to very high performance for “staging T” and “staging M”, with performance matching (or improving on) the in-corpus results. For “staging N”, we were able to use an indirect method for predicting the correct labels, by first identifying the number of lymph nodes examined. Using this method, we achieved performance that was again higher than in-corpus results. Note that in practice, the normalisation step would only need be performed once on a new document collection by analysing the standard nomenclature used for a small number of concepts.

The stronger performance of our classifiers over a majority class baseline classifier was confirmed by statistical significance tests, and also by our repeated random-subsampling experiment, which showed that we are able to obtain superior performance over different runs. We also measured the performance of MedKAT/P over our datasets, and showed that its portability to our setting is problematic. Our approach of mapping its output into features for a Decision List algorithm did not perform well.

Regarding the choice of classifiers, we observed small differences in their performance, and only Naive Bayes exhibited superior performance to the others for “staging N” when training over RMH and testing over Barwon. This suggests that, when using default parameters, other considerations such as speed of execution could be taken into account when choosing a classifier. However, even if the performances are high, it is possible that parameter optimisation could lead to higher differences between algorithms, and we aim to address this issue in future work. Our main goal for this paper was to evaluate the portability of classifiers for colorectal cancer staging across hospitals, which (to our knowledge) has not been addressed in the literature.

We observed in our experiments that “staging ACPS” is the only category displaying low performance, particularly in cross-validation over the RMH dataset. We manually analysed the annotations in order to find possible sources of error. We verified the classifier errors for our best performing system (Naive Bayes), focusing on cases where there were explicit references to “ACPS” in the text. We found that in 14 of 70 cases (20%), there was a clear mismatch between the explicit label in the pathology test, and the expected category in the form entry (from the hospital workflow) used as our gold standard. The majority of errors (n
                     =13) occur when the pathology report explicitly mentions ACPS-B or ACPS-C, and the goldstandard label is ACPS-D (which does not appear explicitly in the pathology report). Only in one case does the text mention ACPS-B, where the goldstandard is ACPS-A. This could occur because the hospital forms were completed using information other than just the text in the pathology reports.

This high rate of annotation mismatches makes it extremely difficult to build accurate classifiers for “staging ACPS” over the RMH pathology reports. Since this dataset was built using the outcomes of the hospital workflow (as opposed to the controlled annotation performed for Barwon Health data), this can be an illustration of the risks of not manually verifying at least some of the data at early stages, even though for the TNM categories the RMH annotation is robust.

ACPS classification over the Barwon Health dataset performed much more robustly. In this case the annotator reported a variety of cases that would be difficult for automatic classification: multiple tumours which could be classified differently (n
                     =10); borderline cases (n
                     =2); and one possible error in the text of the pathology text (which explicitly referred to ACPS-B instead of ACPS-A). These cases illustrate the challenge of developing automatic classifiers for the “staging ACPS” category.

The results of this paper point to the feasibility of a general text mining tool to automatically extract structured cancer staging information from (mainly unstructured) pathology reports, especially using the widely-used TNM reporting framework. Specifically, the cross-corpora performance results indicate that such a tool could potentially be deployed in a new hospital setting with no extra training and minimal configuration, with only a handful of features used in reporting templates needing to be mapped to a common nomenclature. Such a tool has the potential to greatly increase productivity and accuracy of information provision for critical tasks in cancer treatment, such as determining optimal treatment for patients, as well as helping automatically populate population-based cancer registries, which enable analysis and reporting of overall treatment outcomes.

@&#CONCLUSIONS@&#

We have described and evaluated a text mining approach for extracting staging information from pathology reports. The method uses multi-class classification models developed from training data that was only very lightly annotated: i.e., the main categories were annotated only at document level, and not at the level of clinical concepts or shorter spans of text.

The main focus of this paper was to demonstrate the portability of the constructed models by using corpora from two different hospitals (RMH, Barwon) and training on one corpus and testing on the other. We used feature selection for tuning the classifiers, and simple rules to identify numeric values for “staging N”. A simple rule was used to map feature-labels, but otherwise the models were run as trained on the alternative corpus. We showed strong performance for TNM classification, even across corpora, with F-scores of 81–84% for T, 85–96% for N, and 87–91% for M. Our performance results compare favourably to the best levels reported in the literature, and—most relevant to our aim here—the cross-corpus results demonstrate the portability of the models we developed.

For future work we aim to work on optimising the parameters of the different algorithms we tested, and on applying our framework to different hospitals. We would also like to analyse the annotation processes for “staging ACPS”, which was been the most challenging category in our study.

@&#ACKNOWLEDGMENTS@&#

This work was performed while Martinez, MacKinlay, and Cavedon were employed at NICTA; they are no longer affiliated with NICTA. NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program. We thank Henry Gasko and colleagues at the Royal Melbourne Hospital and to Barwon Health for providing the respective collections of pathology reports.

@&#REFERENCES@&#

