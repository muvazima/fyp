@&#MAIN-TITLE@&#Bayesian segmentation of human facial tissue using 3D MR-CT information fusion, resolution enhancement and partial volume modelling

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Bayesian method provides resolution enhanced segmentation of human head.


                        
                        
                           
                           Segmentation classes incllude muscle, bone, fat, air and skin.


                        
                        
                           
                           Tests were performed on 3D MR and CT images, as well as registered MR-CT images.


                        
                        
                           
                           The most successful results were obtained by the information fusion of MR and CT.


                        
                        
                           
                           Free parameters of the algorithm can be adjusted in a more systematic way.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Image segmentation

Information fusion

Partial volume

Resolution enhancement

Superresolution

Human facial tissue

@&#ABSTRACT@&#


               
               
                  Background
                  Accurate segmentation of human head on medical images is an important process in a wide array of applications such as diagnosis, facial surgery planning, prosthesis design, and forensic identification.
               
               
                  Objectives
                  In this study, a Bayesian method for segmentation of facial tissues is presented. Segmentation classes include muscle, bone, fat, air and skin.
               
               
                  Methods
                  The method presented incorporates information fusion from multiple modalities, modelling of image resolution (measurement blurring), image noise, two priors helping to reduce noise and partial volume. Image resolution modelling employed facilitates resolution enhancement and superresolution capabilities during image segmentation. Regularization based on isotropic and directional Markov Random Field priors is integrated. The Bayesian model is solved iteratively yielding tissue class labels at every voxel of the image. Sub-methods as variations of the main method are generated by using a combination of the models.
               
               
                  Results
                  Testing of the sub-methods is performed on two patients using single modality three-dimensional (3D) image (magnetic resonance, MR or computerized tomography, CT) as well as registered MR-CT images with information fusion. Numerical, visual and statistical analyses of the methods are conducted. High segmentation accuracy values are obtained by the use of image resolution and partial volume models as well as information fusion from MR and CT images. The methods are also compared with our Bayesian segmentation method proposed in a previous study. The performance is found to be similar to our previous Bayesian approach, but the presented methods here eliminates ad hoc parameter tuning needed by the previous approach which is system and data acquisition setting dependent.
               
               
                  Conclusions
                  The Bayesian approach presented provides resolution enhanced segmentation of very thin structures of the human head. Meanwhile, free parameters of the algorithm can be adjusted for different imaging systems and data acquisition settings in a more systematic way as compared with our previous study.
               
            

@&#INTRODUCTION@&#

Segmentation of human head images is an active research area. Result of the segmentation is useful in multiple applications such as simulation of facial expressions for craniofacial surgery planning [1], fusion of motion captured animations and muscle activation signals [2], forming a three dimensional (3D) facial biomechanical model for facial expression animations [3]. Measurement on facial tissue also plays a role in diagnosing, analysing and treating patients with neuromuscular disorders [4]. Calculating muscle volume is also a critical measurement for patients with hemifacial microsomia which is a congenital disorder that affects the lower half of the face [5]. Maxillofacial surgery is a surgery which focuses on treating defects and abnormalities in hard tissue of face. The need for preoperative predictions led to studies that involve person specific models [6]. Although computerized tomography (CT) or magnetic resonance (MR) images are generally used to segment facial tissue, there have also been studies which calculate masseter muscle volume from ultrasonography and analyze its relationship with facial morphology [7]. Craniofacial Surgery Planner software was the product of a study on simulation and prediction of craniofacial surgery [8]. Obtaining facial features also carries importance in forensics for the identification of an unknown body [9]. Measurement on the masseter muscle is critical for parameterizing the properties of head tissue in both forensics and anthropology [10]. Reconstruction of the head from medical images also has a critical role in dental treatments [11].

On human head images, several techniques were applied to obtain tissue segmentation. One of the studies combined thresholding and morphological operations for skull segmentation in MR images [12]. Marching cubes is a method that obtains isosurfaces and it was used in a study to find bone, skin and tissue contours from CT and MR images [13]. Although using single modality or multiple modalities often yields reasonable segmentation accuracy, an extensive review on 3D image fusion processes in orthodontics and orthognathic surgery planning concludes that image fusion is the most accurate method for analysis [14]. Recently, a superresolution technique was applied on 3D MR images from sets of orthogonal images, acquired at a high in-plane resolution for automatic tongue muscle segmentation [15]. Semi-automatic methods were also used [16] to obtain 3D appearance of the lip muscles using MR image and the 3D Slicer software which is a framework for image visualization and processing [17]. Rezaeitabar and Ulusoy [18] suggested using a region growing algorithm which employed a Markov random field (MRF) model and works with manually placed seed points. Bayesian approach was also used. Shattuck et al. [19] proposed a method which includes a partial volume (PV) model to segment MR images after bias correction and skull removal. In their study, a maximum aposteriori (MAP) classifier with a Gibbs prior was used to estimate the voxel labels.

MR has good soft-tissue contrast. However, CT is certainly superior for bone and air segmentation. CT has a better resolution than MR, and it also provides some soft-tissue contrast. MR-CT image fusion has been attempted by a few studies such as [13,14], in which fusion was in the form of extracting bone–air information from CT images and skin/soft-tissue information from MR images. However, considering that both modalities provide soft-tissue information and air–bone information (to a lesser extent by MR), an information fusion scheme where all available information from both modalities is utilized for every tissue type is certainly desirable, which was one of the aims of this study.

The Bayesian MRF segmentation model presented in this study is an extension of the model described in [20]. Kale et al. [20] presented a Level Set and a Bayesian method with MR-CT information fusion and partial volume model. When the Bayesian and level set methods were compared, it was concluded that Bayesian result were better. However, there were shortcomings of the Bayesian method. In segmentation of very thin structures, system resolution (blurring) was a significant challenge. Due to this blurring, mean intensity of very thin structures (1–2 pixel thick) get close to their background intensity level, and hence segmented as background class. For that reason, an adaptive approach was developed using shifted mean tissue intensity values for the very thin structure regions. However, this approach is system and data acquisition setting dependent. Moreover, the difference in resolution of MR and CT was not modelled. Finally, CT image noise was not modelled properly, and the priors which could be potentially more useful were not tested.

In this study, model extensions of our previous Bayesian method [20] were investigated to address all of the above issues: (i) a system resolution model and a resolution enhancement model for MR and CT were incorporated (which eliminates system or scan setting dependant ad hoc parameter tuning for the very thin structures); (ii) a more realistic CT image noise model (correlated) was used; (iii) an adaptive directional prior particularly targeted for very thin structures was adapted; (iv) the first two extensions required a different type of partial volume model which was also addressed. Finally, all methods were tested with and without information fusion of MR and CT images.

@&#METHOD@&#

The Bayesian image segmentation algorithm explained in detail in this section is graphed as a data-flow diagram in Fig. 1
                     .

MR intensity shows have spatial variation because of the magnetic field non-uniformity (bias field). To correct such a variation, there are many successful methods available. In this study, N3 method was used [21]. Next, the CT and MR image intensities were normalized into the range of [0–1] using standard linear approach. Finally, CT and MR images were registered to each other using first a rigid scheme and then hierarchical non-rigid block matching method [22] for fine-tuning.

In this section, standard Bayesian image segmentation problem is stated, and tissue classes used in our segmentation are defined. Next, the resolution enhanced Bayesian segmentation method used is explained in the following section.

Let intensity of pixels in a 3D image, 
                           
                              I
                              →
                           
                        , are written in vector form of size M
                        ×1 where M is the number of pixels. Similarly, the segmentation class labels are denoted by a vector θ. The Bayes’ rule according to this model is
                           
                              (1)
                              
                                 
                                    p
                                    (
                                    
                                       θ
                                       →
                                    
                                    /
                                    
                                       I
                                       →
                                    
                                    )
                                    =
                                    
                                       
                                          p
                                          (
                                          
                                             I
                                             →
                                          
                                          /
                                          
                                             θ
                                             →
                                          
                                          )
                                          p
                                          (
                                          
                                             θ
                                             →
                                          
                                          )
                                       
                                       
                                          p
                                          (
                                          
                                             I
                                             →
                                          
                                          )
                                       
                                    
                                 
                              
                           
                        where 
                           
                              p
                              (
                              
                                 θ
                                 →
                              
                              )
                           
                         is the prior distribution of unknown labels. For MAP estimation, taking the logarithm of Eq. (1) and maximizing leads to
                           
                              (2)
                              
                                 
                                    
                                       
                                          arg max
                                       
                                       
                                          θ
                                          →
                                       
                                    
                                    log
                                     
                                    p
                                    (
                                    
                                       θ
                                       →
                                    
                                    /
                                    
                                       I
                                       →
                                    
                                    )
                                    =
                                    
                                       
                                          arg max
                                       
                                       
                                          θ
                                          →
                                       
                                    
                                    [
                                    log
                                     
                                    p
                                    (
                                    
                                       I
                                       →
                                    
                                    /
                                    
                                       θ
                                       →
                                    
                                    )
                                    +
                                    log
                                     
                                    p
                                    (
                                    
                                       θ
                                       →
                                    
                                    )
                                    −
                                    log
                                     
                                    p
                                    (
                                    
                                       I
                                       →
                                    
                                    )
                                    ]
                                 
                              
                           
                        where 
                           
                              p
                              (
                              
                                 I
                                 →
                              
                              )
                           
                         is independent of 
                           
                              θ
                              →
                           
                         and can be ignored for the maximization. In case of Gaussian data distribution, the likelihood term is a multivariate Gaussian:
                           
                              (3)
                              
                                 
                                    p
                                    (
                                    
                                       I
                                       →
                                    
                                    /
                                    
                                       θ
                                       →
                                    
                                    )
                                    =
                                    g
                                    (
                                    
                                       I
                                       →
                                    
                                    ,
                                    
                                       λ
                                       →
                                    
                                    ,
                                    W
                                    )
                                    =
                                    
                                       1
                                       
                                          
                                             
                                                
                                                   
                                                      (
                                                      2
                                                      π
                                                      )
                                                   
                                                   M
                                                
                                                |
                                                W
                                                |
                                             
                                          
                                       
                                    
                                    
                                       e
                                       
                                          −
                                          
                                             1
                                             2
                                          
                                          
                                             
                                                (
                                                
                                                   I
                                                   →
                                                
                                                −
                                                
                                                   λ
                                                   →
                                                
                                                )
                                             
                                             T
                                          
                                          
                                             W
                                             
                                                −
                                                1
                                             
                                          
                                          (
                                          
                                             I
                                             →
                                          
                                          −
                                          
                                             λ
                                             →
                                          
                                          )
                                       
                                    
                                 
                              
                           
                        where 
                           
                              λ
                              →
                           
                         is the vector of mean values of the segmentation classes (of size M), and W is the covariance matrix of 
                           
                              I
                              →
                           
                        . The constants in Eq. (3) can be ignored during optimization. When a Gibbs MRF prior [23] is used, maximizing posterior probability (MAP estimate, Eq. (2)) is equivalent to the following minimization problem:
                           
                              (4)
                              
                                 
                                    
                                       
                                          arg min
                                       
                                       
                                          λ
                                          →
                                       
                                    
                                    [
                                    
                                       
                                          (
                                          
                                             I
                                             →
                                          
                                          −
                                          
                                             λ
                                             →
                                          
                                          )
                                       
                                       T
                                    
                                    
                                       W
                                       
                                          −
                                          1
                                       
                                    
                                    (
                                    
                                       I
                                       →
                                    
                                    −
                                    
                                       λ
                                       →
                                    
                                    )
                                    +
                                    β
                                    C
                                    (
                                    
                                       λ
                                       →
                                    
                                    )
                                    ]
                                 
                              
                           
                        where the second term is the sum of the prior neighbourhood potentials of all cliques (of the Gibbs prior, 
                           
                              p
                              (
                              
                                 θ
                                 →
                              
                              )
                           
                        ), and β is the weight of the prior term balancing between the likelihood and prior terms.

The human head contains several tissue types such as muscle, skin, tendon, fat, bone, marrow, teeth, vessel, vein, facia, nerve, gland and duct. Some of these tissues have similar intensity on MR and CT images. For example, muscle, skin, vessel, nerve, gland, duct and tendon form a group with similar intensities in both modalities. This group of tissues are gathered under the “muscle” class. Bone, teeth and skull form the “bone” class. Although marrow has a different intensity than bone, it is contained within the bone and therefore included in the bone class. Air and fat are easier to distinguish from these groups and each other. Therefore, their own classes are needed. With these groups, the main tissue classes are: muscle, bone, fat, and air, which are called as pure classes.

It is assumed that the pure class pixels of the original image, 
                           
                              λ
                              →
                           
                        , are composed of pixels with intensity values equal to the mean intensity values of their tissue type. Therefore, the elements of 
                           
                              λ
                              →
                           
                         are allowed only to have the mean intensity values of the pure class labels chosen from the set, S: λ
                        
                           i
                        
                        ∈
                        S, where S
                        ={λ
                        
                           air
                        , λ
                        
                           bone
                        , λ
                        
                           fat
                        , λ
                        
                           muscle
                        }

In addition to the pure class pixels, there are pixels in the image whose intensity values are in between the pure class mean values (mixture of classes) such as on the boundary of pure tissue regions. They are called partial volume pixels. In an accurate segmentation problem, such partial volume pixels have to be treated differently. Detailed description of the method is explained in Section 2.4.

Imaging systems introduce distortion to the ideal image as transformations, blurring, decimation and noise. Image restoration is used to model and rectify these distortions [24]. Image restoration problem is defined as obtaining a high-resolution image from N lower resolution images (defined as the superresolution method). The restoration problem can be carried to an image segmentation context by redefining the components. In the segmentation context, high-resolution image is a vector of class means, 
                           
                              λ
                              →
                           
                        , and low-resolution images acquired (for segmentation) are, 
                           
                              
                                 
                                    I
                                    →
                                 
                                 k
                              
                           
                        . The relation between 
                           
                              λ
                              →
                           
                         and 
                           
                              
                                 
                                    I
                                    →
                                 
                                 k
                              
                           
                         can be written as
                           
                              (5)
                              
                                 
                                    
                                       
                                          I
                                          →
                                       
                                       k
                                    
                                    =
                                    
                                       D
                                       k
                                    
                                    
                                       B
                                       k
                                    
                                    
                                       T
                                       k
                                    
                                    
                                       λ
                                       →
                                    
                                    +
                                    
                                       
                                          E
                                          →
                                       
                                       k
                                    
                                     
                                    (
                                    k
                                    =
                                    1
                                    ,
                                    …
                                    ,
                                    N
                                    )
                                    ,
                                 
                              
                           
                        where N is the number of measurements (different images from the same or different modalities), and T
                        
                           k
                        , B
                        
                           k
                        , D
                        
                           k
                         and 
                           
                              
                                 
                                    E
                                    →
                                 
                                 k
                              
                           
                         are transformation, blurring, decimation matrices and the additive noise term of each image set acquired, respectively [24]. Combining these matrices into a single matrix as A
                        
                           k
                        
                        =
                        D
                        
                           k
                        
                        B
                        
                           k
                        
                        T
                        
                           k
                         yields
                           
                              (6)
                              
                                 
                                    
                                       
                                          I
                                          →
                                       
                                       k
                                    
                                    =
                                    
                                       A
                                       k
                                    
                                    
                                       λ
                                       →
                                    
                                    +
                                    
                                       
                                          E
                                          →
                                       
                                       k
                                    
                                 
                              
                           
                        
                     

Hence, the resolution enhanced image segmentation using MAP estimation can be obtained by substituting 
                           
                              λ
                              →
                           
                         with 
                           
                              
                                 A
                                 k
                              
                              
                                 λ
                                 →
                              
                           
                         in Eq. (4):
                           
                              (7)
                              
                                 
                                    
                                       
                                          λ
                                          →
                                       
                                       
                                          M
                                          A
                                          P
                                       
                                    
                                    =
                                    
                                       
                                          arg min
                                       
                                       
                                          λ
                                          →
                                       
                                    
                                    [
                                    
                                       
                                          (
                                          
                                             I
                                             →
                                          
                                          −
                                          A
                                          
                                             λ
                                             →
                                          
                                          )
                                       
                                       T
                                    
                                    
                                       W
                                       
                                          −
                                          1
                                       
                                    
                                    (
                                    
                                       I
                                       →
                                    
                                    −
                                    A
                                    
                                       λ
                                       →
                                    
                                    )
                                    +
                                    β
                                    C
                                    (
                                    
                                       λ
                                       →
                                    
                                    )
                                    ]
                                 
                              
                           
                        where
                           
                              (8)
                              
                                 
                                    A
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         A
                                                         1
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   ⋅
                                                
                                             
                                             
                                                
                                                   ⋅
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         A
                                                         N
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    ,
                                     
                                     
                                    
                                       I
                                       →
                                    
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            I
                                                            →
                                                         
                                                         1
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   ⋅
                                                
                                             
                                             
                                                
                                                   ⋅
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            I
                                                            →
                                                         
                                                         N
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

In this study, since the number of measurements, N, is two (MR and CT), the image vector, 
                           
                              I
                              →
                           
                        , is the concatenation of two image vectors (after preprocessing):
                           
                              (9)
                              
                                 
                                    
                                       I
                                       →
                                    
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            I
                                                            →
                                                         
                                                         
                                                            M
                                                            R
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            I
                                                            →
                                                         
                                                         
                                                            C
                                                            T
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The covariance matrix of the multivariate Gaussian is block-diagonal since CT and MR images are uncorrelated. When this is applied to the formulation, Eq. (7) simplifies to
                           
                              (10)
                              
                                 
                                    
                                       
                                          λ
                                          →
                                       
                                       
                                          M
                                          A
                                          P
                                       
                                    
                                    =
                                    
                                       
                                          argmin
                                       
                                       
                                          λ
                                          →
                                       
                                    
                                    [
                                    
                                       
                                          (
                                          
                                             
                                                I
                                                →
                                             
                                             
                                                M
                                                R
                                             
                                          
                                          −
                                          
                                             A
                                             
                                                M
                                                R
                                             
                                          
                                          
                                             λ
                                             →
                                          
                                          )
                                       
                                       T
                                    
                                    
                                       
                                          
                                             W
                                             
                                                M
                                                R
                                             
                                          
                                       
                                       
                                          −
                                          1
                                       
                                    
                                    (
                                    
                                       
                                          I
                                          →
                                       
                                       
                                          M
                                          R
                                       
                                    
                                    −
                                    
                                       A
                                       
                                          M
                                          R
                                       
                                    
                                    
                                       λ
                                       →
                                    
                                    )
                                    +
                                    
                                       
                                          (
                                          
                                             
                                                I
                                                →
                                             
                                             
                                                C
                                                T
                                             
                                          
                                          −
                                          
                                             A
                                             
                                                C
                                                T
                                             
                                          
                                          
                                             λ
                                             →
                                          
                                          )
                                       
                                       T
                                    
                                    
                                       
                                          
                                             W
                                             
                                                C
                                                T
                                             
                                          
                                       
                                       
                                          −
                                          1
                                       
                                    
                                    (
                                    
                                       
                                          I
                                          →
                                       
                                       
                                          C
                                          T
                                       
                                    
                                    −
                                    
                                       A
                                       
                                          C
                                          T
                                       
                                    
                                    
                                       λ
                                       →
                                    
                                    )
                                    +
                                    β
                                    C
                                    (
                                    
                                       λ
                                       →
                                    
                                    )
                                    ]
                                 
                              
                           
                        
                     

Eq. (10) is an optimization problem with M unknowns which are the labels of the M pixels in the image (selected from the set S of four possible values). The optimization can be performed by using the iterated conditional modes (ICM) [25].

The blurring matrix in equations above, in ideal case, can model 3D nature of the system resolution (blurring). Hence, the segmentation of all slices of image volume has to be done simultaneously (taking into account the resolution blurring in the axial direction in addition to in-plane). To simplify the computations involved, in this study, the resolution blurring in the axial direction was ignored, and slices of image volume were segmented independent of each other (2D segmentation problem).

To further simplify the calculations involved, CT and MR images were registered to each other and interpolated to the same height and width in terms of pixel numbers (in the preprocessing step). The size of the output label set was also set to this height and width. Therefore, the transformation (T) and decimation (D) matrices in Eq. (5) became identity matrices, and hence, only the effects of the blurring matrix, B
                        
                           k
                        , remained in B
                        
                           k
                        .

This section explains the partial volume model used in this study which enables segmentation output of partial volume class labels in addition to the pure class labels, S, defined in Section 2.2.

In the presence of the partial volume classes, the method of marginalization of the likelihood function [19] was used in our previous study [20]. However, this approach is not possible for the correlated noise and blurring model (Eq. (10)). Therefore, we propose to model the partial classes by introducing a finite set of PV classes with mean values that fall between pure classes. The mean values are calculated by
                           
                              (11)
                              
                                 
                                    
                                       λ
                                       
                                          A
                                          B
                                       
                                    
                                    =
                                    α
                                    
                                       λ
                                       A
                                    
                                    +
                                    (
                                    1
                                    −
                                    α
                                    )
                                    
                                       λ
                                       B
                                    
                                 
                              
                           
                        where A and B are pure tissue classes and α is the fraction of the voxel that is occupied by tissue class A (0≤
                        α
                        ≤1).

Typical mean intensity values of the pure tissue classes in MR and CT (in normalized intensity scale) are as follows. For MR: λ
                        
                           air
                        
                        =0.0, λ
                        
                           bone
                        
                        =0.03, λ
                        
                           muscle
                        
                        =0.2, λ
                        
                           fat
                        
                        =0.6. For CT: λ
                        
                           air
                        
                        =0.0, λ
                        
                           fat
                        
                        =0.2, λ
                        
                           muscle
                        
                        =0.25, λ
                        
                           bone
                        
                        =0.6. Based on these, some partial volume classes overlap (in the single modality case). For example, the bone–fat partial volume classes overlap with the muscle-fat classes, since the bone–fat partial volume intensity zone completely includes the muscle–fat region. As a remedy, we allowed partial volume classes only for the adjacent classes, such as bone–muscle and muscle–fat (Table 1
                        ).

In the CT-MR fusion case, partial volume classes do not overlap. Intensity plot for this case is a 2D graph (Fig. 2
                        ). The counts of partial volume classes defined are given in Table 1 (corresponding to Fig. 2). Defining more partial classes increases the accuracy, but at the expense of computation time. As a compromise, using two PV classes between most combinations of pure tissue classes and six PV classes between the fat and muscle resulted in improvements in segmentation accuracy with acceptable computation cost. Fat and muscle have the longest boundaries with each other, and hence more mixed classes were defined between them. Some partial volume classes such as air–muscle were not defined because they do not have a common boundary (Table 1).

In another Bayesian segmentation study (with a partial volume model) by Shattuck et al. [19], Potts prior was used in the segmentation of brain tissue. The prior is the sum of the weighted label relationship functions for the pixel neighbourhood. The relationship function returns value of −2 for identical labels, −1 for labels that share a tissue type (e.g. muscle–fat and pure muscle) and +1 for not having any common tissue:
                           
                              (12)
                              
                                 
                                    z
                                    (
                                    j
                                    ,
                                    k
                                    )
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      +
                                                      1
                                                      ,
                                                   
                                                
                                                
                                                   
                                                      if
                                                       
                                                      j
                                                       
                                                      and
                                                       
                                                      k
                                                       
                                                      do not have a common tissue,
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      −
                                                      1
                                                      ,
                                                   
                                                
                                                
                                                   
                                                      if
                                                       
                                                      j
                                                       
                                                      and
                                                       
                                                      k
                                                       
                                                      are not of equal type but have a common tissue type,
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      −
                                                      2
                                                      ,
                                                   
                                                
                                                
                                                   
                                                      if
                                                       
                                                      
                                                         j
                                                      
                                                       
                                                      and
                                                       
                                                      
                                                         k
                                                      
                                                       
                                                      are of the same tissue type,
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where j and k are indices of the neighbouring pixels. We call this prior as the uniform prior.

In very thin structures (1–2 pixel thickness), a pixel belonging to a thin structure may be surrounded by mostly background pixels. This will result in the background pixels affecting the cost function more than foreground pixels and the labelling process may become inaccurate. A solution to this problem is using a priori knowledge based on the structural orientation around the pixel. There are variations of MRF priors that depend on the location and shape of the region to be segmented. Most of these priors which favour thin structures use a tensor to enhance the effects of the orientations in the image. Wong et al. [26] used such an MRF prior in a Bayesian segmentation method for tubular structures. The method estimates local structures with an orientation tensor and assign more weight to the direction of orientations in the prior. Descoteaux et al. [27] worked on the segmentation of thin, sheet-like structures such as pituitary gland and thin sinus bones using a Hessian matrix based sheetness measure. In the study by Frangi et al. [28], a vessel enhancement filter was proposed that is constructed from Hessian matrix eigenvalues. The vesselness (or tube-likeness) of each voxel was calculated at multiple scales. Another multi-scale study [29] focused on adaptive orientation selection based on Hessian matrix eigenvalues applied on Gaussian smoothed images. Lorenz et al. [30] segmented line-like structures in 2D and 3D by calculating the measure of similarity of a line structure using eigenvalues of the Hessian matrix. Westin et al. [31] classified the local structures using 3D quadratic filters.

In order to modify the uniform Potts prior for cliques with structural orientation, the method of Wong et al. [26] is adopted and modified as explained next. The Hessian matrix is computed for every pixel by the finite difference method. Then, the eigenvalues e
                        1 and e
                        2 and the eigenvectors 
                           
                              
                                 
                                    v
                                    →
                                 
                                 1
                              
                           
                         and 
                           
                              
                                 
                                    v
                                    →
                                 
                                 2
                              
                           
                         of the Hessian matrix are computed such that |e
                        1|≥|e
                        2|. The orientation of the jth pixel, 
                           
                              
                                 
                                    o
                                    →
                                 
                                 j
                              
                           
                        , is selected as the orthogonal direction to 
                           
                              
                                 
                                    v
                                    →
                                 
                                 1
                              
                           
                         if the following conditions are satisfied:
                           
                              (13)
                              
                                 
                                    e
                                    ≥
                                    0.024
                                     
                                    and
                                     
                                    
                                       
                                          
                                             
                                                
                                                   e
                                                   1
                                                
                                             
                                             
                                                
                                                   e
                                                   2
                                                
                                             
                                          
                                       
                                    
                                    ≥
                                    2
                                    .
                                 
                              
                           
                        
                     

The two thresholds above (Eq. (13)) determine the selection of the prior type and hence the segmentation characteristics as explained below.

Let s
                        
                           j,t
                         be the image mask which stores whether uniform or directional prior is to be used. The elements of this mask are defined as binary values according to the following eigenvalue condition:
                           
                              (14)
                              
                                 
                                    
                                       s
                                       
                                          j
                                          ,
                                          t
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      1
                                                      ,
                                                   
                                                
                                                
                                                   
                                                      if the condition in Eq.
                                                       
                                                      (
                                                      13
                                                      )
                                                       
                                                      true for modality,
                                                       
                                                      
                                                         t
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      0
                                                      ,
                                                   
                                                
                                                
                                                   
                                                      otherwise
                                                      ,
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where t is the image modality (CT or MR). For the jth pixel, if s
                        
                           j,t
                         is one, the directional prior which lies in the orientation direction is to be used. Otherwise (if zero), jth pixel is checked for another orientation condition. This check is done by normalizing all orientation vectors in a 5×5 neighbourhood (25 pixels) to a unit length (angles in the range [0, π)), and by vector averaging. If the magnitude of the average is greater than 0.25, it is decided that directional prior will be used for that pixel. The uniform prior is to be used otherwise (Eq. (12)).

In the information fusion case, if either s
                        
                           j,CT
                         or s
                        
                           j,MR
                         is equal to one, the directional prior is to be used. If both s
                        
                           j,CT
                         and s
                        
                           j,MR
                         are zero, the 5×5 neighbourhood algorithm explained above applies in the same way.

Two threshold values of Eq. (13) and the vector average threshold of 0.25 (above) were chosen experimentally for the following reasons:
                           Lowering the second threshold of Eq. (13) causes thicker and blob like segmented structures, and increasing it results in missed orientations. When the first threshold is chosen a lower value, false positive orientations are detected in homogeneous regions, and higher value decreases the detectability of orientations.
                        
                        
                           Within the 5×5 neighbourhood of 25 pixels, if all the pixels were assigned as the directional prior and all orientations were in the same direction (which is not likely), the vector averaging would produce a magnitude of one. Therefore the threshold value of 0.25 corresponds to one-fourth of the 25 pixels in the neighbourhood to have same direction or approximately one-third of the pixels in similar directions. In such a circumstance, that pixel is considered to have missed the detection (Eq. (13)), due to image noise or numerical reasons, and switched to the directional prior (direction equal to average vector of the neighbourhood pixels). If 0.25 value is increased, the orientation condition would require a much higher number of detected vectors in similar directions. In the case of very thin (e.g. one or two pixel wide) structures, it is not likely to have that many vector detections within the neighborhood.
                        
                     

The orientations are used in obtaining the relationship within the neighbouring pixels through an orientation similarity function 
                        [26]:
                           
                              (15)
                              
                                 
                                    h
                                    (
                                    j
                                    ,
                                    k
                                    )
                                    =
                                    exp
                                     
                                    
                                       
                                          −
                                          
                                             
                                                
                                                   δ
                                                   2
                                                
                                                (
                                                
                                                   
                                                      u
                                                      →
                                                   
                                                   
                                                      j
                                                      k
                                                   
                                                
                                                ,
                                                
                                                   
                                                      o
                                                      →
                                                   
                                                   j
                                                
                                                )
                                             
                                             
                                                2
                                                
                                                   σ
                                                   h
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    u
                                    →
                                 
                                 
                                    j
                                    k
                                 
                              
                           
                         is the unit vector in the direction from pixel j to pixel k, and 
                           
                              
                                 
                                    o
                                    →
                                 
                                 j
                              
                           
                         is the orientation of pixel j. 
                           
                              δ
                              (
                              
                                 u
                                 →
                              
                              ,
                              
                                 o
                                 →
                              
                              )
                           
                         is the orientation discrepancy function which returns decreasing values for increasing compatibility between 
                           
                              u
                              →
                           
                         and 
                           
                              o
                              →
                           
                         (controlled by the parameter σ
                        
                           h
                        ), and is given by Wong et al. [26] as
                           
                              (16)
                              
                                 
                                    δ
                                    (
                                    
                                       u
                                       →
                                    
                                    ,
                                    
                                       o
                                       →
                                    
                                    )
                                    =
                                    1
                                    −
                                    |
                                    
                                       
                                          u
                                          →
                                       
                                       T
                                    
                                    
                                       o
                                       →
                                    
                                    |
                                 
                              
                           
                        
                     

Wong et al. [26] suggested σ
                        
                           h
                         value of 0.2, for which the orientation similarity function reduces to one-eight of its maximum for a neighbouring pixel that deviates from the orientation vector by ±53°.

The combination of the orientation similarity and the Potts prior functions is achieved by multiplication. Summation of the combined directional priors for the neighbourhood of a pixel gives the total effect of the proximity labelling. The use of directional prior aims to enhance the effect of pixels in the orthogonal direction to the eigenvector corresponding to the dominant eigenvalue of the Hessian matrix. Finally, the MRF prior is defined as:
                           
                              (17)
                              
                                 
                                    C
                                    (
                                    
                                       λ
                                       →
                                    
                                    )
                                    =
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       M
                                    
                                    
                                       
                                          ∑
                                          
                                             k
                                             =
                                             1
                                          
                                          
                                             
                                                N
                                                j
                                             
                                          
                                       
                                       
                                          w
                                          (
                                          j
                                          ,
                                          k
                                          )
                                          h
                                          (
                                          j
                                          ,
                                          k
                                          )
                                          z
                                          (
                                          j
                                          ,
                                          k
                                          )
                                       
                                    
                                 
                              
                           
                        where M is the number of pixels, N
                        
                           j
                         is the number of neighbourhood pixels around the jth pixel and z(j, k) is the Potts prior of Eq. (12), and w(j, k) is the reciprocal of the distance between the neighbouring pixels j and k. In the case only uniform prior is to be used, h(j, k) term is equal to 1.0.

A few post-processing steps are taken to increase to accuracy of the segmentation. Most of the steps are morphological operations (binary) performed in 3D on the segmentation result:
                           
                              (1)
                              The skin is removed using a morphological opening operation:Skin intensity is close to muscle class in both modalities. There is mostly a fat layer separating the skin region from the muscles. Very rarely, a one or two pixel wide connections may exist with the superficial muscles. Therefore, after segmentation, the muscle class is eroded by a 3
                                 ×
                                 3
                                 ×
                                 3 cube kernel and then dilated once (open operation). This breaks possible connections between them. Next, the first connected region (volume) in radial direction (from outside towards the image centre of mass) is taken as the skin region.

To correct the false segmentation of bone marrow as muscle, a simple morphological filling algorithm is performed on the bone class.

The pixels on the bone–fat interface are sometimes misclassified as muscle. To rectify this misclassification, the bone pixels are dilated by one voxel. A voting scheme is then applied to change the labelling of the dilated voxels. The labels of the voxels are assigned to the tissue class with the most dominant presence in the neighbourhood.

The final post-process step is the removal of small components. Segmented regions with a connected component size of 1 pixel are removed. This is again performed on 3D segmentation result so that very thin structures with continuity in the axial direction are not removed.

Before solving Eq. (10), certain parameters have to be determined (Fig. 1). Tissue class intensity means and noise covariance values of MR and CT images were calculated directly from data. Resolution sigma of MR and CT can be either determined from device manufacturers or experimentally measured by phantoms or on tissue boundaries (e.g. bone–muscle interface) of the image. The details of the methods are explained in Sections 3.2–3.4.

In the Bayesian framework, the optimum value of the prior term, β, depends on the imaging system or data acquisition setting. Once optimized for a setting, it does not have to be reoptimized for each patient. A low value results in segmentation with over fragmented regions (due to image noise). On the other hand, a high value emphasizes the prior term, and hence results in large and connected segmented regions. In the literature, there are many studies to estimate the optimum value. In the case a number of training image sets are available, a good approach is to run the segmentation algorithm for a range of values and choose the one which gives the best segmentation result. In our study, since the patient size was limited, our approach was to create a training image set (MR and CT) by a realistic simulation, and optimize this parameter on the segmentation results. Using the test data set for prior term estimation (according to segmentation performance) is equivalent to the concept of test_set
                        =
                        training_set in machine learning, which is not desirable.

A realistic 3D synthetic data set was generated which has thin muscle and bone structure regions similar to human anatomy including round and thin regions (for details, see [20]). Muscle regions lay inside fat. Regions were defined in high resolution, and then downsampled to actual pixel size by averaging. The downsampling yield an image with partial volume pixels in the interface of regions. The images were then smoothed by a Gaussian kernel corresponding to the CT and MR resolution. To complete the synthetic data, noise was added to the image (independent noise for MR and correlated noise for CT). CT and MR resolution sigma and noise statistics used in simulation were estimated using the human data as described in Sections 3.3 and 3.4. After creating this synthetic volume, the parameter (β) was optimized using the segmentation of this data. The best parameter was selected based on a score calculated using the metrics explained in the next section.

For segmentation algorithm evaluations and parameter estimations, two metrics were used. The first one is the Dice coefficient (DC), measuring the volume overlap ratio between the ground-truth and algorithmically segmented volumes. The second metric compares the boundaries of two labelled sets: root mean squared symmetric surface distance (RMSSSD) in voxel units. The metrics are normalized and summed to obtain a unified metric [20].

@&#RESULTS@&#

The datasets were obtained from patients whose CT scans were already required [20]. The scans were performed by using standard CT protocols. The MRs of the patients were also acquired after the approval of the institutional ethics committee, and the consent of the subjects. A 16 channel multi-detector computerized tomography device, Philips Mx8000 IDT (Philips Medical Systems, Best, Netherlands) was used as the scanner and the scanning parameters are: 120kV, 221mAs, rotation=0.5s, collimation=16×0.75, slice thickness=1.5mm, reconstructed slice thickness=0.75mm, FOV=250mm, matrix=512×512, reconstructed voxel size=0.49mm×0.49mm×0.75mm. The MR scanner was a Philips(R) Intera 1.5Tesla (Philips Medical Systems, Best, Netherlands). The imaging parameters were as follows [20]: T1 3D FFE, TR=25ms, TE=4.6ms (in-phase), flip angle=30°, FOV=240mm, matrix=256×256, measured voxel size=0.94mm×0.94mm×1.88mm, reconstructed voxel size=0.94mm×0.94mm×0.94mm, slice orientation=transverse, scan duration=15min.

The first subject was scanned with the maxillofacial CT protocol, covering axial slices from orbita inferior to include maxillar and mandibular region. The second subject was scanned with the paranazal sinus CT protocol, covering axial slices starting from upper frontal sinus to include maxillar and mandibular region. 16 regions of interest (ROIs) were selected for evaluation [20], containing a variety of thin and thick layers of tissue: 11 ROIs were from the first subject and the remaining ROIs from the second subject (Table 2
                        ). Each ROI includes five consecutive slices and approximately 40×40 pixels (corresponding to a 37mm×37mm area). In order to calculate the derivatives, the volume corresponding to each ROI was extended by five slices on both ends, but the statistics were presented for five slices only. For ground-truth, a special software (ITK Snap [32]) was used by an expert to label all ROI pixels while viewing both modalities side-by-side [20].

The mean intensity values of four pure classes are measured within large and uniform tissue regions (in normalized intensity scale):
                           
                              
                                 
                                    
                                       
                                          
                                             
                                                For MR
                                                :
                                                 
                                                
                                                   λ
                                                   
                                                      a
                                                      i
                                                      r
                                                   
                                                
                                                =
                                                0.0
                                                ,
                                                 
                                                
                                                   λ
                                                   
                                                      b
                                                      o
                                                      n
                                                      e
                                                   
                                                
                                                =
                                                0.027
                                                ,
                                                 
                                                
                                                   λ
                                                   
                                                      m
                                                      u
                                                      s
                                                      c
                                                      l
                                                      e
                                                   
                                                
                                                =
                                                0.196
                                                ,
                                                 
                                                
                                                   λ
                                                   
                                                      f
                                                      a
                                                      t
                                                   
                                                
                                                =
                                                0.61
                                             
                                          
                                       
                                       
                                          
                                             
                                                For CT
                                                :
                                                 
                                                
                                                   λ
                                                   
                                                      a
                                                      i
                                                      r
                                                   
                                                
                                                =
                                                0.0
                                                ,
                                                 
                                                
                                                   λ
                                                   
                                                      f
                                                      a
                                                      t
                                                   
                                                
                                                =
                                                0.22
                                                ,
                                                 
                                                
                                                   λ
                                                   
                                                      m
                                                      u
                                                      s
                                                      c
                                                      l
                                                      e
                                                   
                                                
                                                =
                                                0.25
                                                ,
                                                 
                                                
                                                   λ
                                                   
                                                      b
                                                      o
                                                      n
                                                      e
                                                   
                                                
                                                =
                                                0.59
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

(For CT, a normalization based on standard Hounsfield values can also be used.)

The autocorrelation matrices W
                        
                           MR
                         and W
                        
                           CT
                         of Eq. (10) are related to the noise models of the MR and CT images. The studies on the measurement noise of the detector bins in the CT devices show that there is no correlation for the data noise in the raw data [33]. Access to raw data is not always possible. Therefore, noise model of the image is needed. In filtered back projection (FBP) which is a CT image reconstruction method, sensor measurements are projected on the image and hence noise of the reconstructed CT image becomes correlated [34].

A homogeneous region of interest is selected for the calculation of the autocorrelation matrix. The autocorrelation for a horizontal shift p and vertical shift q at voxel coordinate (x, y) in this region is calculated by
                           
                              (18)
                              
                                 
                                    
                                       R
                                       
                                          p
                                          q
                                       
                                    
                                    =
                                    
                                       ∑
                                       
                                          x
                                          =
                                          1
                                       
                                       m
                                    
                                    
                                       
                                          ∑
                                          
                                             y
                                             =
                                             1
                                          
                                          n
                                       
                                       
                                          [
                                          I
                                          (
                                          x
                                          ,
                                          y
                                          )
                                          −
                                          μ
                                          ]
                                          [
                                          I
                                          (
                                          x
                                          +
                                          p
                                          ,
                                          y
                                          +
                                          q
                                          )
                                          −
                                          μ
                                          ]
                                       
                                    
                                 
                              
                           
                        where m and n are the width and height of the selected region respectively, and μ is the estimated mean within the region. Some autocorrelations calculated for CT image are (after preprocessing): R
                        00
                        =0.0467, R
                        10
                        =0.0032, R
                        20
                        =0.0012 and R
                        30
                        =0.0003. The magnitude continues to decline rapidly for p and/or q values that are greater than two. In MR, there was no significant correlation between pixels. Therefore, the only significant non-zero term is the variance, which was measured as R
                        00
                        =0.039.

The autocorrelation matrix (W) is in the form of banded diagonal matrix, whose entries are autocorrelation terms. They were generated for a 5×5 region, and used to update a single pixel which is at the centre of 5×5. Therefore, autocorrelation matrix is of size 25×25 (in raster scan pixel representation). The inverse of the autocorrelation matrix (W
                        −1) is computed once and stored for calculations.

The image blurring is modelled as a smoothing matrix (A) which is formed again in raster scan pixel representation of the kernel with which the ideal image is smoothed. This formation is similar to the formation of autocorrelation matrix. The system resolution sigma values are computed as σ
                        
                           CT
                        
                        =0.8 pixels (0.75mm) and σ
                        
                           MR
                        
                        =1.3 pixels (1.22mm) with the assumption of isotropic Gaussian blurring. These are approximate values experimentally measured on tissue boundaries (e.g. bone–muscle interface). They can also be obtained from the device manufacturers or by phantom measurements.

Using the simulation, segmentation metrics were calculated for a number of β values for uniform and directional priors and for different image modalities (Fig. 3
                        ). The optimal value of β is the one which gives the best unified metric (roughly the point where DC score is highest and RMSSD score is lowest) as described in [20]. The optimum values are tabulated in Table 3
                        .

As can be clearly seen in Fig. 3, a range of β values provide optimum or close to optimum scores. Therefore, the optimum values tabulated in Table 3 are not very sensitive to small perturbations.

In the experiments with human data (below), the values given in Table 3 were used. Interestingly, when the metrics calculated and graphed as a function of beta (Fig. 4
                        ), a similar variation was obtained, proving that our simulation and estimation methods were quite successful.

@&#EXPERIMENTS@&#

The method was applied by using different combinations blurring, partial volume, regularization and single image or fusion. The eight main combinations of the models are shown in Table 4
                        . The CT-only and MR-only cases only have regularization type as a variable because the lack of other components yields very low metric values.

Eight Bayesian method cases were evaluated in sixteen ROIs and for each case, DC and RMSSSD combined scores and the standard deviations of these metrics were computed for all ROIs (Table 5
                        ). The ‘air’ and ‘fat’ class results do not differ significantly. Hence, they are not tabulated, but presented in Fig. 5
                        .

Two types of plots are used for illustrating and comparing results. The first one is mean±deviation graph (Fig. 5). Every element on horizontal axis corresponds to a method, and the vertical axis shows the score statistics of 16 ROIs. The second type of plot depicts the combined score of every method versus ROI number in the same graph (Fig. 6
                        ).


                        Fig. 5 shows that CT-only cases (case 3 and 4) had the worst muscle and fat segmentation results. MR-only case (cases 1 and 2) results were better because of the better soft tissue contrast of the modality. CT-MR fusion (cases 5–8) resulted in progressively better scores for muscle and fat, as partial volume and resolution models were added. For bone class, MR-only cases performed the worst as expected. CT images are superior in distinguishing bone from other tissues and this can be observed by the higher scores obtained by the CT-only cases. Although MR images had very poor bone–air contrast, MR contributed slightly (unexpectedly) to bone scores in the fusion cases.

For a sample ROI, the original CT and MR images and overlays of manually segmented muscle (red) and bone (green) classes are shown in Fig. 7
                        . Very thin muscles and muscle-like tissues exist in this ROI. The segmentation results for each of the eight cases are also shown. MR only cases (cases 1 and 2) have inferior bone segmentation accuracy and mostly inaccurate segmentation of bone–air regions. The CT only cases (3 and 4) provide superior bone–air segmentation results, but are unable to identify big parts of thin muscle structures. In the absence of resolution model (blurring matrix), although CT and MR fusion (cases 5 and 6) yields improved results over the cases 1–4, there are still big gaps between thin muscle parts which should form a single structure according to the ground truth segmentation. The cases 7 and 8 (the blurring matrix is non-identity) are quite successful in connecting the previously disconnected very thin muscle structure. The gap is reduced to a single pixel.

Segmentation results in ROI 3 show a similar trend to the results of ROI 6 (Fig. 8
                        ). Very thin (1-pixel wide) muscle strips on the top right portion of the image are not segmented accurately in cases 1 through 6. The accuracy is enhanced with the addition of blurring matrix to the model (cases 7 and 8). ROI 5 and ROI 8 results are also shown in Figs. 9 and 10
                        
                        .

There are various factors affecting the computation time of the methods. The most important ones are the size of the image volume and the number of the partial volume classes used. It takes approximately 10 iterations on average for the methods to converge to a solution. For the standard fusion case, segmentation of 5 slices of 50×50 ROI takes approximately 5s. The computation time increases linearly with image size. Therefore, 50 slices of a 256×256 image volume would take nearly 20min. However, the speed of the algorithm is bound to increase by optimization of the MATLAB codes or by porting the codes to C++.

Using the score value calculated, different methods were evaluated and compared using two sampled parametric and none-parametric statistical tests. Normality of the distributions over 16 ROIs were checked using Shapiro–Wilk test. It was found that all methods had normal distributions (p
                        >0.5) which enables the use of ANOVA test. ANOVA test showed no significant difference among cases 6, 7 and 8 although the means of cases 7 and 8 were slightly better than that of case 6.

Lastly, to demonstrate the effect of post-processing step on the results, segmented images with and without the post-processing are shown in Fig. 11
                        . Significant benefits of this step is observed.

@&#CONCLUSION@&#

The aim of this study was to develop an improved method for human facial tissue segmentation. The MR-CT information fusion, partial volume and system resolution models all improved the segmentation accuracy. The most significant improvement was made by the information fusion of MR-CT images. The results were very consistent in both patients and in all ROIs selected from various anatomical regions. Meanwhile, the free parameters of the algorithm can be adjusted for different imaging systems and data acquisition settings in a more systematic way as compared with our previous study [20].

In congenital diseases such as hemifacial microsomia or in hemifacial tissue loss due to gun injuries, our method can be helpful in the planning of the surgical reconstruction by measuring the tissue volumes in the healthy part of the face. This way, plastic surgeon will be able to do an optimal surgery and save the patient from any further surgeries. In current surgical reconstruction operations of our clinic, neither MR-CT nor MR is used for planning. Only CT (for imaging of the bone) and visual analysis are used. After the surgery, the bone–tissue is reconstructed but the soft-tissue symmetry is not achieved. Another operation is always necessary for fat-tissue replacement in order to get the symmetry.

@&#DISCUSSION@&#

The segmentation results were very similar to the results obtained by the adaptive Bayesian segmentation method proposed by Kale et al. [20]. The adaptive method is 2% better on the combined score scale. Note that the adaptive method results were obtained in 3D and hence the continuity of thin structures in the axial direction was better captured. However, the previous method requires more parameters to be tuned that are medical imaging system and data acquisition setting dependent.

There was not a quantitative difference between the segmentation results using the isotropic and directional prior models. However, additional thin muscle structures which did not exist in the ground truth were obtained in the directional prior case. These may be simply segmentation inaccuracies or ground truth inaccuracies. In either case, the ground truth segmentation should be made by multiple experts since it is not an easy task to manually segment very thin structures. Due to time constraints, the interobserver and intraobserver variability (in ground-truth labelling) were not measured. However, they are expected to be quite significant based on our discussions with clinicians.

The autocorrelation of the CT and MR images were obtained by a manual method and this process needs to be repeated for other imaging systems and data acquisition settings. This process can be switched for a more robust method. The means of class labels and their standard deviations can be calculated by methods such as expectation maximization algorithm. MR intensity standardization [35] (instead of simple normalization) is also expected to further improve the results.

The estimated model of the resolution (σ) is assumed to be constant throughout the volume, where in fact it is typically variable in the field of view for spiral CT systems [36]. However, the results presented here indicate that even a constant value provided improvements in the segmentation output.

Image registration (when multimodal fusion is used) and bias correction are important for the accuracy of the segmentation algorithm. Improvements on the pre-processing methods are expected to improve the segmentation performance. One of the drawbacks of using multimodal images together for segmentation is that any error in the registration will result in loss of accuracy in segmentation. Improvements on automatic registration and small user interactions making the process semi-automatic may result in higher accuracy registration.

Partial volume ratio estimates at each pixel are a natural result of the method. Validation in terms of the partial volume ratios can be performed if these estimates are also available for the ground truth segmentation.

In the future, our research will focus on the following:
                        
                           (i)
                           Collection of more data sets for more through clinical validations.

3D version of the algorithms may further improve the results (to capture continuity of thin structures in the axial direction) but with more expensive computation time. Optimizing the current code and porting to C++ and/or using GPU programming techniques can be investigated. In the case of 3D, the computational cost would increase, but not dramatically since the number of processed voxels would be the same. Only the kernel sizes convolved around the voxels would change.

Keeping the CT/MR images in their original form (instead of interpolating to the same geometry) and using transformation (T) and decimation (D) matrices as in the ideal superresolution formulation (Eq. (5)) can be implemented (with an increase in computation cost).

The advantages and disadvantages of using orientation tensors over Hessian tensors in the directional prior can be investigated.

Neither of the authors have any financial or personal relationships with other people or organizations that could inappropriately influence or bias this work.

@&#ACKNOWLEDGEMENT@&#

This study was partly sponsored by the Scientific and Technological Research Council of Turkey (Project no: 105E128), which had no involvement in study design; in the collection, analysis and interpretation of data; in the writing of the paper; and in the decision to submit the article for publication.

@&#REFERENCES@&#

