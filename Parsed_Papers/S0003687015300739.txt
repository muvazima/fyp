@&#MAIN-TITLE@&#Supporting productive thinking: The semiotic context for Cognitive Systems Engineering (CSE)

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Pierce's Triadic Model of Semiotics provides context for understanding Rasmussen's work.


                        
                        
                           
                           The Triadic Semiotic Model provides common ground for design and evaluation of cognitive systems.


                        
                        
                           
                           Rasmussen emphasized human capabilities as a resource for improving system performance.


                        
                        
                           
                           Components of Rasmussen's approach are evaluated through the lens of the Triadic Semiotic Model.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Cognitive Systems Engineering

Abstraction Hierarchy

Work domain analysis

Decision Ladder

Skills-Rules-Knowledge Model

Ecological Interface Design

Proactive Risk Management

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

In order to fully appreciate Jens Rasmussen's contributions to both safety engineering and to cognitive psychology it is important to start at the beginning, to consider the earliest context motivating his work. He got his M.Sc. degree in electrical engineering in 1950 and in 1956 he joined the Atomic Energy Commission's Research Establishment Risø (now Risø National Laboratory). From 1962 to 1987 he was head of the Electronics Department at Risø. The primary focus of his work throughout this period was on safety in nuclear power and the earliest papers written in English were on aspects of automation and automatic control. In 1968, however, the focus begins to shift to the human operator. This reflects a realization that despite increasing automation, the human operator remained a key component with regards to the ultimate safety of any complex process. It also reflects a conservative attitude toward automation recognizing that the rationality of even very smart computers (e.g. automatic control systems) was bounded by the assumptions of the programmers. Thus, Cognitive Systems Engineering would start from the assumption that for complex processes safety will ultimately depend on the ability of smart machines and smart people to work together.

The first part of this paper will focus on the early motivations behind Rasmussens' research program. In this section, we will draw from his earliest publications that address the design problem with particular focus on the role of the human factor in determining system safety. The second part of the paper will consider four specific aspects of this work that have both practical implications for system safety and theoretical implications for understanding human cognition. These are the Abstraction-Hierarchy (AH), the Decision Ladder (or SRK model), Ecological Interface Design, and Proactive Risk Management. The goal of this paper is to ground these theoretical constructs with respect to the value added with regards to both design and theories of human cognition and to address some of the misconceptions that arise when these constructs are taken out of the Triadic Semiotic Context.


                     
                        In his task of identifying the failure and evaluating its possible consequence for the plant, the operator must therefore be supported by the designer of the system. The information directly available in the form of measuring data and alarm data has to be displayed in a suitable way, and the operator's need for knowledge of the reactions of the plant to failure conditions must be supported as much as possible by incorporating the designer's knowledge into the instrumentation in the form of automatic evaluation procedures that analyse the alarm and measuring data patterns.
                        
                           Rasmussen (1968, p. 7)
                     
                     
                        In the design of automatic control systems for industrial process plants the designer has to realize that he cannot possibly foresee and analyse all relevant operational conditions of the plant; in certain circumstances he must rely upon an intelligent human operator for the identification and correction of abnormal plant behavior. He must consider that the operator, for his diagnosis and decision, needs information describing the anatomy and behavior of the plant that is not needed during normal operation, as well as detailed information on the actual operational state.
                        
                           Rasmussen (1969, p. 3)
                     
                  

The above quotes are taken from the first papers that Rasmussen wrote in English that addressed the human factor with respect to safety. The first paper was titled “On the communication between operators and instrumentation in automatic process plants.” The second paper (Rasmussen, 1969) was titled “Man-machine communication in the light of accident records.” It seems clear that, from the start, Rasmussen framed the design problem as a problem of communication. The quotes suggest that he appreciated that a major issue was the human's ability to determine the state of the plant in order to generate possible interventions and to anticipate the ultimate consequences of those interventions. This involved communication (e.g., feedback) between the human and the processes being controlled. Further, the quotes emphasize that this communication was to a large extent mediated by decisions that designers made in terms of the computational and display processes that mediated that communication. Thus, Rasmussen hypothesized that many of the accidents that were conventionally attributed to ‘human performance’ (e.g., where operators failed to intervene to prevent an accident) were the result of poor interface design (i.e., the interface representation did not specify the ‘states’ of the process in a way that supported productive thinking).

In framing the communication problem, Rasmussen was cognizant of the information processing limitations of humans, but he was more interested in the capacity for meaning processing. For example, consider his citation of G.A. Miller's classic paper.
                        It is well known from everyday life and from experiments, Miller (1956), that the human data input capacity is greatly limited, but also that information input capacity is very great in cases of appropriate coding i.e. when data are arranged in patterns related to the mental model of the system available to the operator.
                        
                           Rasmussen (1969, p. 8)
                     
                  

This quote suggests that the human capacity to ‘code’ information into meaningful chunks, meant that the 7 ± 2 chunk limitation of working memory was rarely a practical limitation on communication for domain experts. Note also that the quote suggests that in the context of process control – the key was not coding in terms of generic mnemonic tricks, but rather it was coding in terms of a ‘model of the system.’ This is critical. From Rasmussen's perspective it was important that the chunks reflected the structure of the underlying processes being controlled. In other words, it was important that the chunks be meaningful with respect to the functional dynamics of the control problem.

Along with the design problem, Rasmussen saw an opportunity. Already in 1968, he anticipated the potential of computers to integrate data into graphical representations:
                        In case a digital computer is used in the instrumentation there are further possibilities of supporting the operator in his survey of the operating conditions. Related measured data can be coded by the designer into series of graphic pictures representative of the plant as a whole and of subsystems, which may supply the operator with a very efficient means of comparing data.
                     
                     
                        By thus elaborating a number of graphs giving surveys of the conditions in representative systems, functions and situations, the designer may ensure that the operator is presented with all relevant data in close relation as a supplement to the alarm information.
                        
                           Rasmussen (1968, p. 8)
                     
                     
                        … the digital process computer and the related new media for information display offer a great capacity for storing information resulting from plant design considerations as well as a highly efficient means of reduction and conditioning of the information to be displayed to the operator.
                        
                           Rasmussen (1969, p. 3)
                     
                  

It is noteworthy that these insights' about the potential value of graphical user interfaces (GUIs) were being articulated well before such interfaces became commonplace. The first GUI interface, the Alto, wasn't developed at Xerox PARC until 1973. One reason that Rasmussen was tuned into the importance of graphical representations was that he recognized the role that analogue representations had for supporting human operators in conventional plants. He wanted to make sure that the benefits of analogue displays were not lost in the process of automating (i.e., digitizing) process control systems, as suggested by the following quotes.
                        This presentation has to be such that the operator only has to run an eye over the instrument panel to ascertain whether the operation is normal. The layout of the control console can facilitate this task to a high degree even in conventional systems: related measuring data are grouped together, normal operating conditions are indicated clearly, for example by edge wise meters arranged so that the hands are normally in line, or meters that can be rotated so that the hands normally point in the same direction. Meters have to be grouped so as to clearly indicate the different subsystems.
                        
                           Rasmussen (1968, p. 5)
                     
                     
                        In conventional control systems the data are presented to the operator by analogue devices (meters, recorders) in a way that has … [evolved] slowly on the basis of operational experience gained during the simultaneous development of the plants themselves. In these systems the operator has direct access to all data; he may perceive the pattern of several indications simultaneously, or he may choose and read individual data accurately.
                        
                           Rasmussen (1969, p. 5)
                     
                  

The key point for this section is that Rasmussen framed the design problem in terms of utilizing information technologies to facilitate communication with human operators. He framed the ‘control’ problem and the ‘human factor’ problem in semiotic terms. That is, he was interested in how the interface (as a representation) impacted how information was coded. He was interested in the extent to which the codes (e.g., signs) mapped both to the physical processes being controlled and to the human mental processes guiding decisions and actions. A recurring theme throughout Rasmussen's writings was the premise that this mapping would have a significant impact on the ability of humans to think productively with respect to complex problems.

In the field of semiotics, there are two competing models of the implications of representations (e.g., signs) for communications as illustrated in Fig. 1
                     . One approach, originating with Ferdinand de Saussure, framed semiotics as a dyadic relation between a sign and an interpretation. This approach emphasizes that the signs (e.g., letters of an alphabet) are arbitrary with respect to any referent object in the ecology (e.g., the sign CAT has no direct correspondence with the animal). In this context, signs are abstract, arbitrary symbols and ‘meaning’ is totally in the head of the observer (i.e., meaning = interpretation). In this context, the sign is meaningless if the observer does not know the language or code. This was appropriate for Saussure's primary interest in the development of alphabets and formal languages.

As illustrated in Fig. 1, the dyadic model is a subset of an alternative approach to communication, originating with Charles Sanders Pierce (see Menand 19xx for convenient access to some of Pierce's writings) that framed semiotics as a triadic relation between an object in the environment or functional problem domain, a sign, and an observer/agent. The two semiotic theorists that Rasmussen cited in his publications (e.g., 1986) each framed the communication problem as a triad (Eco, 1979; Morris, 1971).

The triadic model reflected Pierce's curiosity about the pragmatics of communication and sensemaking that went well beyond de Saussure's interest in language and alphabets. He approached cognition from the perspective of natural selection, and was interested in the selective advantage provided by human cognitive capacities. Thus, he was interested in how signs supported successful interaction, where interactions included adaptations to ecological demands associated with survival. In other words, he was interested in signs (representamen/signifiers) as mediators between an observer's beliefs (interpretant/signified) and a source (referent/object). How is it that the observer/agent's interpretations of the signs come to correspond with the functional realities of the objects that we interact with? In this context, ‘meaning’ refers to the pragmatics with respect to successful interaction or adaptation. Putting this into the context of human-machine systems, this suggests that a sign's meaning is grounded in the functional dynamics of the problem domain. The meaning does not depend on the observer's interpretation. However, the observer's ability to adapt his decisions and behaviors appropriately to the situation dynamics may very well depend on the observer's beliefs/interpretation corresponding with the functional situation.

Note that the choice of semiotic model has clear ontological implications as illustrated in Fig. 1. The dyadic semiotic fits well with a dualistic ontology in which Mind and Matter are considered to be ontologically distinct. The triadic semiotic, however, spans Mind and Matter suggesting a monist ontology of human experience that reflects the dynamic coupling of Mind and Matter. The distinction between these two semiotic models has important implications for the gap between much of the experimental work on cognitive psychology and the problems associated with performance in natural work domains. Due to the strong influence of linguistics, much of experimental cognitive psychology has been at least implicitly framed in the context of a dyadic semiotic model consistent with the assumption that the Mind is a symbol processing system (e.g., classical computer metaphor). The dyadic semiotic model is also at least implicitly associated with much applied work where there is a tendency to focus on interactions between humans and computers/automation or humans and interfaces, rather than on interactions between humans and work domains, that are mediated by interfaces and automation.

It is likely that Rasmussen's choice of a triadic semiotic model was influenced by his prior training in control theory. In Fig. 2
                     , the triadic semiotic system is illustrated in a way that emphasizes the parallels between the triadic semiotic model and the dynamics of control. In this context, the agent (1) is the controller or the source of the control logic corresponding with Peirce's interpretant/signified (operator or automation's internal model, beliefs, expectations or assumptions). For successful control, the controller logic must be tuned appropriately to the functional dynamics or meanings of the work domain (3). The interface (2) corresponds with Peirce's representamen/signifiers, this functions as both a comparator where feedback is compared to intentions to provide an error signal and as the means for action in response to errors (i.e., an actuator). In other words, the interface (2) is the representational medium that links the agent/interpretant (1) to the process/object (3). The third element in Fig. 2 represents the plant, the process being controlled, the problem ecology, or work domain (3). This corresponds with Peirce's referent/object, which is the source of the functional meanings that the agent must discover.

Note that the circle in the control system goes both ways, so that it is also legitimate to think about the agents' intentions as objects with respect to the processes. The control actions of the agent would be the signifiers that are interpreted through the actions of the process (signified). In essence, consider the interaction as a conversation between the agent and the process, where each is a semiotic object that the other is interpreting by means of the interface. Both directions are important for a stable closed-loop dynamic, but Rasmussen's work focused on fault diagnosis, where the primary problem was for the agent to make sense of the controlled processes. For Rasmussen, the semiotic objects were typically industrial work domains (particularly nuclear power processes) and the design challenge was to provide the signals, signs, and symbols (signifiers) so that the operators' interpretations (signified) were consistent with the functional demands for safe and efficient operations.

The parallels between the dynamics of control and the dynamics of semiotics are not an accident. The central question for both Peirce and the pioneers of cybernetics (e.g., Wiener, 1948) was how to use information (signs, feedback) in order to guide successful adaptation. For example, Walter McCulloch (one of the pioneers of cybernetics) observed:
                        Anyone who has the good fortune to listen to Wiener and von Neumann and Rosenblueth and Pitts wrestling with the problems of modern computing machines that know and want, has a strange sense that they are listening to a colloquy of the ancients. But they would be the first to tell you that they themselves are drunk with an American wine of an older vintage. They quote liberally from Charles Peirce and from Josiah Willard Gibbs, these men have altered our metaphysics by altering our physics. It is epistemology that is most affected for it is the physics of communication which is today receiving an adequate theoretical treatment. For the first time in the history of science we know how we know and hence are able to state it clearly. (1954, p. 18)
                     
                  

Thus, Rasmussen saw the problem of safety from the perspective of semiotics and control. He concluded that success in managing complex processes would require cooperation between smart machines and smart people, due to the fact that the rationality of each were bounded. He realized that this cooperation depended on establishing effective communications that were grounded in the functional requirements (i.e., the states of the process or work domain) and that leveraged both the unique capabilities of the smart machines (e.g., data processing, integration, and representation) and the unique capabilities of the smart humans (e.g., pattern recognition, creative adaptation to novelty).


                     
                        When an abnormal state of the plant has been detected, the failure has to be identified with good certainty and its cause and the consequences upon the continued operation must be evaluated. To the operator this is quite a different task from normal optimalization (sic) of the operation. In case of more serious failures the allowable response time may be short; the task will not be to solve a well-defined problem, but rather to formulate the problem corresponding to [the pattern of data presented].2 The situation will be characterized by a set of abnormal data,2 each of which may occur rather frequently during the daily work (e.g., instrument failures),3 but which in just the combination in question may have been caused by a serious or dangerous failure that would have been considered very improbable in advance.3 This means that the operator is not allowed to trust his daily operating experience,1 but has to base his decisions on detailed knowledge
                           1 of the functioning of the plant and its response to the different types of failures.3 He should be imaginative enough to postulate a covering set of n possible causes of the failure situation
                           1 on the basis of the immediately available information
                           2 and then, after processing of more detailed information, to make a well-founded decision.
                     
                     
                        The great difficulty is to ensure – by training, layout of control console and plant
                           2, etc. – that the correct and in many cases very improbable cause of a failure
                           3 is among the possibilities that occur to the operator.1
                        
                        
                           Rasmussen (1968, p. 6, emphasis added)
                     
                  


                     Winograd and Flores (1986, p. x) noted that “all new technologies develop within the background of a tacit understanding of human nature and human work.” This is certainly true of Rasmussen whose primary focus was on the utilization of advanced information technologies to enhance performance of complex process control systems. His work was clearly guided by a tacit understanding of human nature and human work as illustrated in the initial quotes for this section. The underlined phrases are annotated to illustrate how each of the three components of the semiotic triad illustrated in Fig. 2 (1 – operator's beliefs or awareness; 2 – interface representation; 3 – functional work or problem domain) are included in this tacit understanding of what constitutes a ‘well-founded decision.’ The remarkable thing about Rasmussen is that he went on to make his tacit understanding of human nature and human work explicit in ways that have not only shaped the design of information technologies to support human performance, but that have also had a significant impact on cognitive science.


                     Fig. 3
                      is a schematic diagram of operator performance from Rasmussen's 1969 paper. At that point, he had made limited citations to the psychology literature in his writing (Bruner et al., 1956; Gagne, 1962; Miller, 1956), so it is likely that this diagram represents his tacit understanding of human nature and human work. Already, the connections with his later theories of human performance (e.g., the SRK model) are evident.

Of course, Rasmussen began searching for validation of his tacit beliefs about human performance in the psychological literature. However, what he found there was initially disappointing.
                        We have found it very difficult to break down this task into elementary parts which would make it possible to apply data reported from psychological laboratory experiments, and thus we have felt a great need for man-machine experiments in real industrial environments.
                        
                           Rasmussen (1969, p. 10)
                     
                  

Later, Rasmussen would endorse Lopes' (1986) “telling” criticisms of the dominant experimental paradigms for studying human decision-making.
                        The consistent framing of empirical studies in terms of normative models has produced a science in which human behavior is not so much studied as it is graded and in which the goal of describing behavior is replaced by the goal of explaining why the observed behavior is different from some other behavior. It makes no sense, she says, to use a model of behavior as a benchmark for the behavior it is modeling. If there are disparities, then it is the model that has failed to measure up.
                        
                           Rasmussen et al. (1994, p. 228)
                     
                  

Since, the empirical experiments in the general literature did not fit well with his tacit model of human work, Rasmussen and his colleagues began building their own empirical base for testing these intuitions. In the 1969 paper Rasmussen briefly describes a series of experiments in progress at the time to begin exploring human performance in contexts that were, at least somewhat representative of the functional demands of fault diagnosis in process control.
                        The experiments are made in co-operation with a group of technicians who have a reputation as efficient trouble shooters, and as we are interested in their mental activities, the experiments are based upon tape recordings of the technicians thinking aloud during the task. Objects in the troubleshooting tasks are electronic instruments that have some similarity to industrial plants as they are systems with different co-operating subsystems and have a reasonable variety in the way they show their response, so that to some extent the fault can be evaluated directly from “display”: multichannel analysers, oscilloscopes and TV-receivers … [S]ubjects were selected technicians who had a relation to the particular instrument as qualified users, but not as trouble-shooting specialists.
                        
                           Rasmussen (1969, p. 10–11)
                     
                  

In retrospect, it seems very clear that the disconnect between Rasmussen's intuitions about human performance and the literature of his day was that his intuitions were based on a triadic semiotic model, while the experimental literature was dominated by a dyadic semiotic model (e.g., Flach, 2015). Much of the laboratory research in cognitive psychology was based on a view of the human as a ‘symbol’ processing system analogous to a digital computer. In this framework, the symbol was the stimulus, and the research focus was on a sequence of independent processing stages that were involved in responding to (essentially interpreting) the symbols. This reflected a reductionist approach to explanation in which thinking was dissected into components (encoding, decision, action), with research programs focusing on the dynamics of a specific component and very little attention being paid to the integrative dynamics of thinking and the degree to which experimental tasks represented demands analogous to those of typical work domains.

As with the dyadic semiotic framework, laboratory experimental approaches often considered the connection between the symbols (experimental stimuli) and the larger ecology to be arbitrary at best. At worse, the relation between the symbol and any specific natural ecology was considered to be a source of noise or a confound that would potentially limit the generality of research results. Thus, there were strong pressures for experimental psychologists to study ‘toy worlds’ with an emphasis on quantitative measurement, internal validity, and control, rather than to deal with the messy problems that were more representative of the process control industry.

Rasmussen and his colleagues were also out of step with the zeitgeist of experimental psychology of his day with respect to methodology. At that time, verbal protocol methodologies were very much out of fashion. Experimental psychologists were very sensitive to the criticisms of introspective methods leveled by Behaviorists (e.g., Watson, 1913). Thus, they were very keen to demonstrate the potential for an ‘objective’ experimental science. So, there was very little interest in any aspect of performance that could not be objectively quantified (e.g., in terms of reaction time or percent correct). In fact, one of the strong attractors of the Information Processing Model was that it suggested ways to quantify information (bits) and performance (bits/s). However, as the original developers of information theory readily acknowledge, this quantification of information came at the price of meaning:
                        The word information, in this theory, is used in a special sense that must not be confused with its ordinary usage. In particular, information must not be confused with meaning.
                     
                     
                        In fact, two messages, one of which is heavily loaded with meaning and the other of which is pure nonsense, can be exactly equivalent, from the present viewpoint, as regards information.
                        
                           Shannon and Weaver (1963, p. 8)
                     
                  

To Rasmussen it apparently made a great deal of difference whether an interface representation was ‘pure nonsense’ or ‘loaded with meaning’ with respect to shaping operator's thinking in relation to safely operating complex processes. While the experimental psychologists were studying ‘information processing,’ he began taking the first steps toward the study of ‘meaning processing’ (e.g., Flach, 1999a,b, 2001; Flach and Dominguez, 2003; Flach and Rasmussen, 2000). The Triadic Semiotic framework was based on the assumption that meaning was ultimately grounded in the problem ecology. This suggests that an important first step in exploring any problem ecology was to talk to the people who lived there – the domain experts. Thus, Rasmussen came to the conclusion that the verbal protocols of domain experts would provide more insights into the problem ecology than the literature of experimental psychology:
                        To reach a proper human-computer cooperation, it will therefore be necessary to study the diagnostic strategies that are actually used by operators in different situations …. Furthermore, it is very important to analyze the subjective preferences and performance criteria that guide an operator's choice of strategy in a specific situation. Unless these criteria are known, it will not be possible to predict the strategy that an operator will choose, faced with a specific interface design.
                        
                           Rasmussen (1986, p. 25)
                     
                  

Finally, it is important to note that Rasmussen eventually discovered some like-minded people in the social sciences. Two groups in particular were the Ecological psychologists [e.g., Brehmer (1981); Brunswik (1947/1956); Gibson (1966); Hammond et al. (1980); and von Uxkull (1957)] and the Gestalt psychologists [e.g., deGroot (1965), Dunker (1945), Selz (1922), and Wertheimer (1945)]. With respect to the Ecological psychologists he writes:
                        An important feature of Brunswik's theory of perception is that he distinguishes clearly between the term for input of the S-R (stimulus-response) psychologist, which is “stimulus,” pointing from the environment toward the person, and the term “cue,” pointing outward from the person toward the aspects of the environment. Central to the model … is that the environment and the person are described in symmetric terms, hence the name “lens model.” The cues of the task vary in ‘ecological validity” and the person has variation in “cue utilization,” both of which may assume various linear and nonlinear forms. Both sides are analyzed and attempts are made to match ecological validities to cue utilizations as well as ecological function forms to subjective function forms, i.e. to identify the extent to which the principles of organization that control the task system are reflected in the principles of organization that control the cognitive system of the person.
                        
                           Rasmussen (1986, p. 180–181)
                     
                  

With respect to Gestalt Psychology, he writes:
                        … Dunker describes how subjects go from the problem to a solution by a sequence of considerations where the items proposed can be characterized by a “functional value” feature pointing upward to the problem, and a “by means of which” feature pointing downward to the implementation of a solution …
                        
                           Rasmussen (1986, p. 129)
                     
                  

These quotes were picked specifically to emphasize that a common intuition uniting Rasmussen, Ecological Psychology, and Gestalt Psychology was an underlying triadic semiotic model that considered the dual role of information with respect to external ecological structure and with regards to inner problem solving strategies. This suggests that ultimately the achievement of effective, safe control of an industrial process will depend on the coupling between these two sets of constraints (i.e., ecological and mental/strategic).

Now that the context motivating Rasmussen work has been established, the next sections will look at some of the products of his search for a cognitive theory that could guide design. In these sections I will be extrapolating beyond Rasmussen's early intuitions in order to suggest how his earlier motivations have shaped my own orientation as a practitioner of CSE.

Three products of Rasmussen's work are the Abstraction Hierarchy (AH), the Decision Ladder (SRK), and Ecological Interface Design (EID). It can be tempting to identify these three constructs uniquely with the three components of the semiotic model illustrated in Figs. 1 and 2. This is especially true for those who assume a dualistic ontology, where mind and matter are distinct.

From a dualistic ontological perspective the idea that a construct could reflect a functional reality reflecting the conjunction of mind (e.g., agent) and matter (e.g., plant) is not plausible. However, the case will be made that this tendency to map these constructs (e.g., AH) to either mind or matter leads to misconceptions. Rather, it will be argued that these constructs reflect three complementary perspectives on the complete triadic semiotic process as illustrated in Fig. 2. The function of taking three perspectives on the full dynamic is to be able to disambiguate those invariants that are grounded in the triadic semiotic dynamic from those invariants that reflect constraints of a particular research perspective. In other words, the goal of CSE is to use the three perspectives to ‘triangulate on’ or to ‘converge upon’ a deeper understanding of the dynamic of meaning processing (i.e., human experience) that spans the three elements of the triadic model.

The Abstraction Hierarchy (AH) reflects the need to describe a functional problem or work domain in a way that reflects the agents' intentions or functional goals and in a way that suggests appropriate ways to structure representations. The goal is to represent the work domain so that the process states can be organized or chunked in ways that are meaningful with respect to the functional constraints of the process, the constraints of the interface, and the intentional constraints of the human agent.

The Decision Ladder (and associated SRK Model) reflects the need to describe how the internal strategies of human agents interact with the constraints of representations (e.g., signals, signs, and symbols) in order to determine the abilities of the human to respond appropriately to the constraints of the controlled process. The emphasis is on the capabilities of the human to adapt to the demands of complex work and the extent to which those capabilities are supported or handicapped by the structure of representations.

Ecological Interface Design (EID) frames the problem of representation design in terms of both the functional structure of the controlled process/problem domain and the capabilities of the human agent. This reflects the responsibility of designers to make the deep structure of the process accessible to the human agent in order to support productive thinking and to maximize the potential for achievement.

The final construct of Proactive Risk Management extrapolates the intuitions about triadic semiotic dynamics to sociotechnical systems to better understand organizational sensemaking. CSE researchers are becoming increasingly aware that social interactions within and across levels in organizations play a significant role in shaping meaning and performance (e.g. Walker et al., 2009). Thus, in order to achieve the ultimate goal to engineer safe and effective sociotechnical systems, it becomes necessary to consider how authority/control loops and communication networks constrained an organization's pursuit of meaning.

As noted above, the AH provides a view of the triadic semiotic dynamic from the perspective of the functional demands of the work. This reflects the constraints on observation (what do agents need to be able to perceive in order to judge where they are relative to the functional goals) and the constraints on control (what actions are needed in order to move toward the functional goals, while avoiding negative consequences). In other words, the goal of the AH is to provide insight into the ‘deep structure’ of the work domain, problem space, or situation dynamics (e.g., Flach, et al.).

Rasmussen introduced the AH as a framework for visualizing the products of work domain analysis (Vicente, 1999; Naikar, 2013). Thus, to understand the AH it is necessary to appreciate what he meant by work domain analysis and in particular, how it is different than conventional approaches to task analysis (Flach, 2000). Conventionally, task analysis has focused on the activity of workers, considering both physical and mental activities. From a normative perspective, task analysis specifies the procedures that the worker should follow. From a descriptive perspective, task analysis specifies what workers actually do. In contrast, work domain analysis focuses on the functional possibilities or constraints in the work domain. This is a somewhat broader perspective than typically achieved with task analysis. Work domain analysis attempts to describe what a worker could or might do, as a broader context for considering what should be done (normative task analysis), what actually is done (descriptive task analysis) and what other ways are possible (innovation and hazard detection). This focus on possibilities is consistent with Gibson's (1979) construct of affordances that has become popular within the design community (e.g., Norman, 1988, 1990).

For industrial processes, exploring the field of possibilities required considering the basic physics of these processes. However, in the context of the triadic semiotic dynamic, this requires a description that is different than the purely ‘objective’ descriptions that is typically the goal of conventional physical sciences (e.g., thermodynamics). For example, the same physical process (e.g., a nuclear power plant) can have a different ‘functional meaning’ for an energy company than for a terrorist organization. Objectively it is the same system, but functionally it is dramatically different. For example, Rasmussen writes:
                           The way in which the functional properties of a system are perceived by a decision maker very much depends upon the goals and intentions of the person. In general, objects in the environment in fact only exist isolated from the background in the mind of the human, and the properties they are allocated depend on the actual intentions. A stone may disappear unrecognized into the general scenery; it may be recognized as a stone, maybe even a geological specimen; it may be considered an item suitable to scare away a threatening dog; or it may be a useful weight that prevents manuscript sheets from being carried away by the wind – all depending upon the needs or interests of a human subject.
                           
                              Rasmussen (1986, p. 13)
                        
                     

Eventually, Rasmussen (1986) suggested representing the functional meaning or deep structure of a work domain in terms of a hierarchy of 5 layers of constraints that reflected both the top down relations between function and form (that help to answer the question ‘why’ one form might be preferred to another) and the bottom up relations between form and function (that helped to answer the question of ‘how’ that specific form might be used to satisfy a particular function).


                        Fig. 4
                         illustrates one way to think about the 5 layers of constraint and the relations among the layers. Note that Fig. 4 does not use the same labels that Rasmussen and others have used for the different layers. These labels reflect the evolution of my own thinking in struggling to resolve some of the confusions about the AH that have arisen over the years. Note also that while Vicente (1999) and many others have illustrated the AH using hierarchical tree structures to emphasize the links across levels, Fig. 4 suggests different types of visualizations for the different levels. The links across levels are certainly important, as will be discussed. However, we have found that it can be valuable to explore different types of representations to fully appreciate the unique insights into the functional dynamics that are potentially available at the different levels of abstraction.

The middle layer of the AH, labeled Organization in Fig. 4, is illustrated as a typical functional flow or box diagram. At this level the focus is typically on the control or authority structure (which agents control or have command authority or responsibility for what components of the process) and the information flow or structure (what states or process information is sensed and fed back to what agents or what agents report to what other agents) within the organization. Social science and engineering disciplines have conventionally used block diagrams or flow charts to illustrate these aspects of work organizations. In fact, typically task analysis involves refining the description of work at this level. For example, expanding the boxes to provide increasing details about the inputs/processes/activities/outputs associated with any particular box. Another consideration at this level is to evaluate alternative arrangements of the boxes in terms of alternative distributions of authority (flat vs. hierarchical management structures) or alternative communication networks (networks vs. hierarchies).

Rasmussen's work helped us to realize that horizontal analysis in terms of more or less detailed representations of the organization left many questions about the system unanswered. For example, an important question for the design of any control system is the question of state variables. That is, what variables should be used to describe the current status of the process and to anticipate potential trajectories of the process? To answer this question, control engineers typically look to physics to specify the properties of the process (e.g., laws of motion, thermodynamics, aerodynamics). The variables in the equations representing these laws would be the typical candidates as ‘state’ variables. These state variables provide a normative prescription about what types of feedback will be useful for determining the overall state of the system and for predicting future states.

Additionally, in evaluating the quality of a particular box diagram organization, it is necessary to refer to a value system. For example, in selecting the appropriate gain for a control system or in selecting the right decision criteria for an observer (signal detection system) it is necessary to consider the cost function or the payoff matrix. Thus, to address issues associated with determining the appropriate description of the process relative to the feedback loops in the organization and relative to the quality of performance relative to the control and observer logic, a vertical analysis becomes an essential complement to the conventional horizontal reduction. As Leveson (2000) notes these higher layers provide insights into the ‘intent’ of the design that are not typically apparent in the conventional part-whole decompositions typically used by engineers and software designers.

Thus, the top level of the AH focuses on the quality of performance relative to the functional values. At this level the functional goals and risks are specified and the associated tradeoffs among the goals and risks that determine the overall quality or success of the system. This is the top of the AH, because it sets the context for everything below. For example, it determines what physical laws will be relevant and it determine the quality metrics for comparing alternative designs at the organizational level.

The second level in the AH considers the field of possibilities. Physics typically plays an important role at this level in determining what is physically possible. In addition, however, regulatory and other pragmatic constraints are considered at this level to the extent that they place limits or bounds on system performance. Physical constraints are typically represented at this level as algebraic or differential equations. Example A in Fig. 4 is a diagram that was used to represent the energy balance constraints in association with the controls (throttle and stick) for a landing approach (Amelink et al., 2005). For discrete processes the space of possibilities might simply be enumerated in a connected graph. This is illustrated in B in the diagram as an illustration of part of the state space for the classical missionary and cannibals problem (Bennett and Flach, 2011). Finally, example C shows a ‘state space graph’ that control engineers often use to visualize the field of possibilities for physical processes. Each axis in this graph represents a state variable and physical constraints (e.g., laws of motion) can be illustrated as trajectories or boundaries in this space. Also, goals and risks can be illustrated as points or regions within this space (Flach et al., 2011; Stanard et al., 2012).

Note that each of these three top levels (Values, Possibilities, Organization) bring out different dimensions of the semiotic dynamic. However, it should be apparent that the different descriptions are linked. For example, the goals at the Value level will be points or regions within the state space diagrams at the Possibilities level. Also, the states at the Possibilities level will specify feedback requirements at the Organization level.

The fourth layer in the AH, Allocation, considers how the different functions at the Organization level are allocated to different devices or physical types of subsystems. This level brings in constraints that are not as obvious at the other levels of description. For example, consider whether the control logic in one of the control loops at the Organization level is implemented as an automaton or a human. If it is an automaton, then it may require a power source. If it is human, it will require a healthy workspace. Considering the particular needs of different kinds of devices has important implications for design (to figure out how to accomplish a function) and for troubleshooting (to figure out why a function is not being accomplished as intended).

The lowest layer in the AH, Layout, considers the spatial arrangement of the components. This might be represented as a schematic of the work place or as wiring diagrams of components. The key here is that the spatial arrangements place constraints on work that may not be obvious in the other levels of description. For example, the location of a worker relative to the processes being controlled or relative to other workers may have important implications for what information is accessible (e.g., whether the pilot is in the plane or in a ground control station). Or the location of subsystems may result in unintended interactions (e.g., heat exchange) that would not be obvious in the representations used at other levels in the AH.


                        Vicente's (1999) book on cognitive work analysis has played a very important role in drawing attention to the value of work analysis and the AH framework. However, there have been some unintended side effects of the particular example (DURESS) Vicente used to illustrate these constructs. Because DURESS is an relatively isolated process, it was possible to provide a fairly comprehensive representation of the process constraints using hierarchical diagrams. In following Vicente's example, there may be a danger that people are missing the spirit of work analysis that both Rasmussen and Vicente intended.

One danger is that there seems to be a belief that a complete, validated AH is a necessary prerequisite for design. However, work domain analysis is never complete. This is in part due to the complexity of many work domains, there are always new surprises to be discovered. Also, it is due to the fact that work domains are not stationary processes. They are in a constant process of adaptation in order to take advantage of opportunities (e.g., new technologies) and avoid risks (e.g., changing competitive landscapes). By making the AH a prerequisite for design, there is a significant danger that people will never get to the design stage.

Rather, it is suggested that the development of an AH should be considered a co-requisite with design. The AH is a framework for integrating, cataloging, and archiving the things learned in the process of exploring a work domain. Iterative design can be an important part of the exploring and learning process. The AH should not be an obstacle to design, it is intended to be an integral component within a design program. In this context it should be evolving and growing through multiple design cycles. Leveson (2000) provides an excellent discussion and example of how the ‘logic’ of the AH can be integrated into an iterative design process.

The other danger from adhering too tightly to Vicente's specific example is that people tend to fixate on hierarchical tree or network structures as the primary or only representation form. However, one of the important values of the AH is to stimulate alternative ways to visualize work constraints. Thus, it can be very valuable to explore multiple forms of representation both within and across levels. Some of the various forms that have proven useful are illustrated in Fig. 4. In considering alternative representations it is important to consider both the level specific properties that a representation illustrates and the links to other levels. The hierarchical diagrams are particularly good at illustrating links between levels, but they typically do not provide much value with regards to illustrating level specific properties. State space diagrams can be particularly useful for visualizing both level specific constraints (physical boundaries) and connections to other levels (goal states, candidate variables for feedback). Another important representation form to consider is Lind's Multi-level Flow Modeling (MFM) technique (e.g., Lind, 1994), which integrates the higher-level constraints into functional flow diagrams that emphasize the propagation of constraints within multi-level interacting control loops.

The Decision Ladder and the associated distinction between Skill-based, Rule-based, and Knowledge-based interactions (i.e., the SRK model) examines the triadic semiotic dynamic from the perspective of the kinds of demands placed on the control agent. This is typically the role of the human operator, but the SRK model can also be applied to adaptive automatic control systems (Sheridan, 2017). The SRK Model evolved from the empirical work observing trouble-shooting performance in the Risø electronics shop. Rasmussen and his colleagues discovered that the process of trouble-shooting was much more flexible and opportunistic than had been suggested by existing models of information processing (e.g., see Rasmussen, 1986).

Perhaps, of all the constructs suggested by Rasmussen, the SRK Model is the one that has been most eagerly embraced by those who adopt a dyadic, information processing model of human performance. This is because the qualitative performance distinctions associated with the three modes of interaction in the SRK model match well with distinctions that have been associated with levels of expertise in the conventional literature (e.g., Fitts, 1964; Anderson, 1983). This association is reinforced by Rasmussen's own illustrations of the cognitive implications of the SRK model (e.g., Fig. 9.1 in Rasmussen, 1986, p. 101) in terms of three different internal information processing paths.

The Skill-based information path is typically associated with expert performance in perceptual motor skills such as steering a car or landing an aircraft. The skilled driver or pilot seems to respond automatically to continuous feedback (i.e., signals). This automatic processing tends to be almost unconscious in that it is very hard to articulate verbally or to train through verbal instructions. The typical path to expertise is practice, practice, practice. Also, Skilled-based processing is typically associated with relatively low demands on conscious mental processes. For example, it is generally possible to engage in consciously demanding tasks (e.g., carrying on a conversation or way-finding) with minimal disruption.

The Rule-based information path also depends on a degree of expertise or knowledge, but it is knowledge that is easier to articulate in the form of rules or heuristics. The Rule-based path reflects situations where a stored procedure or activity sequence is triggered by a sign. The triggered procedure might be skill-based if there is feedback in terms of appropriate signals. However, often the triggered procedure will be open-loop. That is, the procedure will be carried out to completion, independently from any feedback. With respect to training, rules are easier to communicate through verbal instructions. However, procedures that are practiced consistently can become ‘encapsulated’ and thus, can be carried out automatically with minimal demands on conscious processing.

The Knowledge-based information path, unlike the other two, places heavy demands on conscious processing. This path is used in those situations where there is no direct coupling with the situation through previously established signals or signs. The Knowledge-based path involves symbol processing, where the term symbol refers to patterns in the environment that have no previous association to action through skills or rules. Thus knowledge-based processing requires identification or disambiguation of the meaning of the situation through hypothesis testing. Effective knowledge-based processing requires different kinds of skills than skill- or rule-based processing. Knowledge-based processing requires the skills of a scientist or detective. It requires generation and evaluation of hypotheses or new procedures in response to situations that are not familiar.

A potential problem with the dyadic interpretation of the SRK Model is that the semiotic distinctions that Rasmussen makes between signals, signs, and symbols tend to be treated as internal properties of the human information processing system, rather than mappings across the triadic semiotic system (e.g., see Wickens et al., 2004, p. 171–172). The implication is that the distinction and the associated constraints on information processing are purely a function of the level of expertise (i.e., it's all in the head); and that the only limitation to the achievement of skilled-based information processing is practice. It is accurate that practice is an important factor, but it is a misconception to think that this is the ultimate or only constraint on the style of processing that can be achieved.

The seed for the SRK model can be seen in Fig. 3. Here different couplings between the agent, interface, and process are illustrated. Path I reflects the coupling through signals (e.g., pattern matching). Paths II and III reflect coupling through signs (e.g., triggering procedures). Finally, Path IV reflects the coupling through symbols (e.g., generating new procedures). Fig. 3 makes the implications for the triadic semiotic or the complete control process more explicit. Also, if later discussions of the SRK model are considered in the full context, it becomes clearer that Rasmussen was using the SRK model to explore the full dynamic of situation awareness, not simply awareness (what is happening in the head). For example in the same chapter where he presented the illustration of the implications for information processing, he also discusses the signal, sign, symbol distinction with respect to actions of the human relative to the process. For example, whether the interface allows continuous signals (e.g., a steering wheel) or whether it requires discrete signs (e.g., a button press or switch).

The critical point here is that the limitations on the kinds of processing that are possible depend jointly on properties of the process, interface, and the agent. Thus, we argue that the SRK Model is describing constraints on the triadic semiotic dynamic, NOT mechanisms inside a human's head. For example, in control systems that have long time delays (e.g., remote control of the Mars Rover) proportional control based on continuous feedback will not allow stable control. Thus, skill-based processing is not supported. Under these conditions a discrete style of control in which sequences of control actions are triggered by specific conditions (signs) is the preferred solution. Thus, a rule-based style of control is the best that can be achieved. Expertise in this system depends on learning the appropriate associations between signs and action sequences. This constraint applies whether the remote control system is human or automaton.

Thus, the constraints on the types of processing that are possible depend on all three components of the triadic process. Are the states of the process continuous or discrete? Are the measurements of the states continuous or discrete? Are there phase lags or time delays associated with the processes or the measurements? Are the representations of the measurements on the interface continuous or discrete? Do the representations allow the human to pick-up the relevant space-time patterns? Has the agent discovered the significance of the patterns/signals or the signs? Has the agent learned the requisite procedures? Are the controls analog (allowing continuous adjustment) or are they discrete?

The construct of EID was introduced by Rasmussen and Vicente, 1989, as a framework for considering the role of interface or representation design with respect to achieving systems safety. However, both the 1968 and 1969 papers cited throughout this article end with suggestions for the design of graphical interfaces to facilitate productive thinking with respect to managing nuclear processes (1968, Figs. 1–5; 1969, Figs. 2–4). Thus, as suggested at the start of this paper, already in the late 60s Rasmussen viewed the fundamental design problem from the perspective of communications and control, and he saw the interface as the place where designers could influence this process in positive and negative ways. He understood, as did others that representations had a huge impact for good or ill on human problem solving (Woods, 1995).

It is significant that between 1968 and 1989 there were enormous advances in the power and availability of graphical interfaces (e.g., the glass cockpit). Though even in 1989 the capabilities were extremely expensive and much less generally available relative to today when most of us carry more powerful graphical devices in our pockets than were available in many plants or research labs in 1989.

From the start Rasmussen recognized the computer as both a threat and an opportunity with regards to facilitating communication with humans and supporting productive thinking. On the threat side was the potential loss of information. He worried that signals and signs available to an operator who was directly interacting with a process, might not be available to an operator who was interacting through a computer. For example, there was a potential that analog representations and functional groupings of displays in conventional plants would be replaced by digital representations organized to reflect programming conventions. On the opportunity side was the potential to employ a wider range of graphical conventions to reflect interactions among variables (e.g., using mimic diagrams and graphical objects such as Goodstein, 1984).

In terms of the evolution from 1968 to 1989, perhaps there was a shift in the balance between threat and opportunity. By 1989, the opportunities to utilize the computational and graphical power of computers to build unique visualizations of the process that were much better than those available in conventional plants was a strong impetus behind the introduction of EID as a new framework for interface design. One important aspect of the EID approach was to make the physical laws constraining the interactions among variables more explicit to the operator.

An example where the EID approach enabled skill-based control for an aspect of work that was rule-based with conventional displays is Amelink et al., 2005. The new design for a landing display included a total energy reference path (TERP). Without this reference, the throttle setting for landing was constrained to a rule-based heuristic (e.g., set the throttle to about 75% of cruise power). In the conventional cockpit, there was no direct reference to indicate that the setting was correct. If the setting was not right, the pilot could discover this over time, because it would be very hard to hold the target speed and flight path using only the stick. However, with the total energy reference path, the pilot had continuous feedback about whether the throttle setting was right, since there was a direct link between throttle and total energy. Thus, the addition of the TERP provided a signal to support skill-based control interactions that were not possible in the conventional cockpit.

The EID framework approached the interface design problem through the lenses of AH and SRK, emphasizing that successful communication depended on the dual role of the interface with respect to both the correspondence and the coherence aspects of the triadic semiotic dynamic (Fig. 1). The AH is particularly valuable for addressing the correspondence problem. That is it helps in describing the work domain in ways that are ‘meaningful’ relative to the functional goals of the control agents. The SRK model is useful for addressing the coherence problem with respect to understanding the underlying ways that information can be used to facilitate control of the work processes. Together, the correspondence and coherence problems reflect the potential impact of representations on productive thinking. The interface was not purely an ‘input’ device and the relative cognitive function was not simply ‘encoding’ or ‘perception.’ Rasmussen viewed the representation in terms of the full meaning processing or control problem. In this regards, like Gibson (1979) the cognitive problem was framed in terms of a holistic perception-action system. Thus, in framing the interface problem in the context of EID, we are not simply addressing human-computer interaction, but rather human–work interaction. In this context, the computer is the medium of communication (specifier), but the work domain is the source of meaning (object).

With regards to design, the EID framework might be considered the tip of the spear, with work analysis (AH) and control or strategy analysis (SRK) playing valuable supporting roles. The EID framework has inspired several books (Bennett and Flach, 2011; Burns and Hajdukiewicz, 2004) and recently multiple reviews have assessed the impact of this framework over the last 20 years (Bennett and Flach, 2013; Bennett et al., 2015a; Bennett et al., 2015b; Borst et al., 2014; Flach et al., 2015a; McIlroy and Stanton, 2014).


                        
                           A very fast pace of change of technology is found at the operative level of society within all domains, such as transport, shipping, manufacturing and process industry. This pace of change is much faster than the pace of change of management structures and of safety legislation and regulation. In consequence, the dynamics of change and the interaction between the different levels of society become important considerations for the development of effective risk management strategies.
                           
                              Svedung and Rasmussen (2002, p. 398)
                        
                     

As noted earlier, safety with respect to complex work processes was a principle goal motivating Rasmussen's work. The early focus was on employing computer and display technologies to enhance the operators' visualization of the process to facilitate productive thinking, particularly with regards to hazardous, rare events. However, Rasmussen and many others began to appreciate that the operator-process-control-loop was nested within organizational, regulatory, and political control loops. Thus, it became necessary to take a broader look at the full sociotechnical system as a sensemaking organization or as a triadic semiotic system.

The final component of Rasmussen's work involved exploring ways to help researchers to visualize the dynamics of organizational sensemaking. In essence, this involved meaning processing at an organizational scale and he approached this from the perspective of the triadic semiotic and control frameworks. He illustrated the organization as a collection of nested control loops (e.g., Rasmussen and Svendung, 2000). Typically, decision makers within various loops would have some constraints that were unique to that loop, and at the same time, constraints would propagate across the loops. For example, upper or outer loops might constrain the targets, incentives, and resources for inner or lower loops, and inner loops might constrain the flow of information about the physical processes available to outer loops (e.g., Flach et al., 2015b). Thus, in some sense, each of the control loops was involved in a unique local semiotic dynamic, while at the same time, the system as a whole was involved in a global semiotic dynamic. Since ultimate success (e.g., safety) of the sociotechnical system depended on coordination across the loops, it becomes important that each local semiotic dynamic be understood within the context of the global semiotic.

Rasmussen's last contribution in collaborations with Svedung (Rasmussen and Svendung, 2000; Svedung and Rasmussen, 2002) was to suggest some ways for designers and analysts to visualize the dynamic interactions across the interacting semiotic subsystems in a sociotechnical system. This included the AcciMap, the ActorMap, and the InfoMap.

The AcciMap is designed to make the interactions across levels of an organization explicit in order to better appreciate how decisions and actions at one level might set up the pre-conditions that lead to instabilities or hazards at another level. These preconditions were not always made salient in typical causal analyses, since they are typically not on a direct causal path to an accident. Note that the goal is not to push the ‘blame’ for accidents from the operator to higher levels of the organization, but to identify where interventions might have the greatest impact with respect to improving coordination and increasing global system safety. The function of the ActorMap is to categorize the agents in terms of different control layers within an organization and the InfoMap focused on making the information coupling of a layer with the larger organization explicit.

Rasmussen's work with regards to Proactive Safety Management reflected a new paradigm in the field of safety (e.g., Dekker, 2012, 2015; Flach, 1999a,b; Le Coze, 2015; Leveson, 2012; Snook, 2000) where ‘safety’ was framed in term of the health of the organization or its resiliency (Hollnagel et al., 2007). The problem was not about eliminating errors, but it was about designing organizations that were 1) robust with respect to the normal variance that might be anticipated (e.g., human slips or component failures will not cascade to create global catastrophes); and 2) resilient or adaptive with respect to unanticipated variability (e.g., can flexibly shift strategies when necessary to manage the unexpected).

It is becoming increasingly evident that the robustness and resiliency of organizations, as with all control systems, depends on communications. This included feedback flowing back from the work domain through the various layers within the organization and it includes commands that direct actions with respect to the work domain. In these systems stability depends on the organizational structures that shape these communications, including the distribution of authority for shaping control actions (e.g., hierarchical vs. flat organizations) and the constraints on information flow (e.g., hierarchical vs. network communication systems). Just as computers had created unique opportunities for designing graphical decision support for operators, the advances in information and communication technologies (e.g., the internet) are creating unique opportunities for redesigning organizational structures that have the potential to improve organizational safety and effectiveness.

The central thesis of this paper is that Rasmussen's intuitions about safety and about the role of humans in process control were shaped by a triadic model of semiotics that was particularly compatible with his training in control engineering (Figs. 1 and 2). This perspective aligns well with Ecological and Gestalt approaches to cognition, but it is somewhat at odds with the much laboratory research on human cognition that tends to adopt a dyadic semiotic framework and an open loop causal model of dynamics. For this reason, some of the central constructs of his approach (AH, SRK, EID) are sometimes not universally appreciated, and the potential of these constructs for design and for theories of cognition are not always realized.

It is suggested that AH, SRK, and EID do not represent distinct components of the triadic semiotic dynamic. Rather, these constructs each represent a different perspective on the full semiotic system. The AH views the semiotic triad from the perspective of the functional demands of work domains. The Decision Ladder and associated SRK Model view the semiotic triad from the perspective of different strategies for linking perception and action. The EID approach views the semiotic triad from the perspective of representations and the mappings both to work domains and to strategies.

The perspective provided by the triadic semiotic model allowed Rasmussen to anticipate new waves with respect to how we think about safety (e.g., resilient systems), interface design to support productive thinking, and how we think about cognition (e.g., evolutionary psychology, and embodied cognition). Today, we are gradually catching up with the intuitions that guided Rasmussen's thinking over 40 years ago. The paradigm shift from a dyadic view of semiotics to a triadic perspective is gaining momentum. As this shift continues, it is likely that appreciation for Rasmussen's contributions will grow wider and stronger, while the gap between basic cognitive theory (and associated experimental work) and the demands associated with designing safer more efficient sociotechnical systems will narrow.

@&#REFERENCES@&#

