@&#MAIN-TITLE@&#Recommendations for recognizing video events by concept vocabularies

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Characterizing a universal concept vocabulary suited for representing video events.


                        
                        
                           
                           The concept vocabulary should contain more than 200 specific and general concepts.


                        
                        
                           
                           Be diverse by covering object, action, scene, people, animal and attribute concepts.


                        
                        
                           
                           Increase the number of concepts rather than improve quality of individual detectors.


                        
                        
                           
                           Contain detectors that are appropriately normalized.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Event recognition

Concept representation

Concept vocabulary

@&#ABSTRACT@&#


               
               
                  Representing videos using vocabularies composed of concept detectors appears promising for generic event recognition. While many have recently shown the benefits of concept vocabularies for recognition, studying the characteristics of a universal concept vocabulary suited for representing events is ignored. In this paper, we study how to create an effective vocabulary for arbitrary-event recognition in web video. We consider five research questions related to the number, the type, the specificity, the quality and the normalization of the detectors in concept vocabularies. A rigorous experimental protocol using a pool of 1346 concept detectors trained on publicly available annotations, two large arbitrary web video datasets and a common event recognition pipeline allow us to analyze the performance of various concept vocabulary definitions. From the analysis we arrive at the recommendation that for effective event recognition the concept vocabulary should (i) contain more than 200 concepts, (ii) be diverse by covering object, action, scene, people, animal and attribute concepts, (iii) include both general and specific concepts, (iv) increase the number of concepts rather than improve the quality of the individual detectors, and (v) contain detectors that are appropriately normalized. We consider the recommendations for recognizing video events by concept vocabularies the most important contribution of the paper, as they provide guidelines for future work.
               
            

@&#INTRODUCTION@&#

We consider the problem of recognizing events in arbitrary web video, such as the ones depicted in Fig. 1
                     . Among the many challenges involved, resulting from the uncontrolled recording condition of web videos and the large variations in the visual appearance of events, probably one of the most fundamental questions in event recognition is what defines an event in video? The Oxford English dictionary defines an event as “anything that happens”. With such a broad definition it is not surprising that the topic has been addressed in the computer vision and multimedia retrieval community by many researchers from diverse angles [4,55,44,5,25,54,36].

In this paper, we study representations that contribute to defining events for automatic recognition. We are inspired by findings from cognition, where research has repeatedly shown that humans remember events by their actors, actions, objects, and locations [46]. Studying event representation based on such high-level concepts is now within reach because of the continued progress in supervised concept detection [48] and the availability of labeled training collections like the ones developed in benchmarks such as TRECVID [47], ImageNet [7] and several other venues [34,11]. Different from concepts, which represent a single person, object, scene or action in videos, events are commonly defined as a more complex interaction of several persons, objects, and actions happening in a specific scene [31]. In this paper, we name the set of available concept detectors as the vocabulary and we study how to construct a vocabulary suited for effective recognition of events in video.

The state-of-the-art in event recognition represents a video in terms of low-level audiovisual features [16,38,50,35,15,19,37]. In general, these methods first extract from the video various types of static and/or dynamic features, e.g., color SIFT variations [53], MFCC [15], and Dense Trajectories [38]. Second, the descriptors are quantized and aggregated [38]. The robustness and efficiency of various low-level features for recognizing events are evaluated in [50,15,33]. Despite their good recognition performance, especially when combined together [35,15,38,33], low-level features are incapable of providing an understanding of the semantic structure present in an event. Hence, it is not easy to derive how these event definitions arrive at their recognition. Therefore, essentially different representations are needed for events. We focus on high-level representations for event recognition.

Inspired by previous work in object recognition [51,22], scene recognition [22,40] and activity recognition [41], many have explored high-level representations for recognition of events [28,31,2,58,14,30,8,24]. All these works follow a general pipeline consisting of three consecutive steps to arrive at a high-level video representation. First, frame extraction, where the video is decoded and a subset of frames is extracted. Second, concept detection, where each extracted frame is represented by a vector of predictions from vocabulary concept detectors. Finally, video pooling, where the frame representations are averaged and aggregated into the video level representation. The obtained high-level representation is not only semantically interpretable, but is also reported to outperform the state-of-the-art low-level audiovisual features in recognizing events [31,33]. Rather than training vocabulary concept detectors and event detectors separately, recent work aims for jointly learning the vocabulary concept and event detectors [26,57,1,27]. In these works, the vocabulary concept detectors are trained to optimize the event detection, without explicitly optimizing the individual concept detector accuracy. As a consequence, the vocabulary concepts do not necessarily have a semantic interpretation needed to explain the video content. In this paper, we follow [31,14,58,30] and train concept and event detectors separately.

Identifying a universal vocabulary of concepts suited for representing events is an important question that has been ignored in the literature. To the best of our knowledge, all the previous work on high-level representations for event recognition relies on an arbitrary set of concepts as the vocabulary. By contrast, we focus in this paper on characterizing the vocabulary which is most effective for representing events. We investigate the concept vocabulary from two perspectives: first by characterizing the composition, where we investigate what concepts should be included in the vocabulary. Second by characterizing the concept detectors, where we study how to create vocabulary concept detectors that are most suited for representing events. Before detailing our research questions, we discuss related work that we consider most relevant to these two perspectives.

@&#RELATED WORK@&#

Our study is inspired by the pioneering work of Hauptmann et al. [9] who focus on construction of concept vocabularies for broadcast news video retrieval. They examined how big the concept vocabulary should be and what concepts should be part of the vocabulary for effective shot retrieval. For this purpose, they used a pool of concepts to create and evaluate vocabularies under different circumstances. In their work, the presence and absence of 320 human-annotated concepts was used as the main source for the investigations. To make the experiments more realistic they insert noise into the human annotations to simulate the behavior of automatic concept detectors. They concluded that 5000 detectors with modest quality would be sufficient for general-purpose broadcast news video retrieval. Regarding the important question what concepts to include in the vocabulary, Hauptmann et al. [9] conclude that frequent concepts contribute more to overall news video retrieval performance than rare concepts, so they are preferred to be included in the vocabulary. However, it is not clear whether their conclusion generalizes to event recognition on the challenging domain of unconstrained web video.

In this paper, we start from the analysis by Hauptmann et al. [9] and adopt their research questions for event recognition. Our work is different with respect to the following five aspects. First, we focus exclusively on events, whereas [9] considers news use cases like Find shots of US Maps depicting the electoral vote distribution (blue vs. red state) and Find shots of Refugee Camps with women and children visible. Second, our domain of study is unconstrained web video, rather than the highly structured broadcast television domain. Third, we place special emphasis on the importance of various concept types in representing events (e.g., objects, scenes, actions, etc.), rather than considering all concepts equally important. Fourth, we evaluate retrieval accuracy on video-level rather than shot-level. Finally, in our analysis we do not rely on human concept annotations directly, but instead we use real detector predictions with varying levels of accuracy per concept. Using the real detectors to represent videos leads to surprising new findings, as we will show in the experiments.

Automatic detection of concepts in videos is a well studied topic in computer vision and multimedia for which many algorithms have been proposed [49,52,10,17]. These include descriptors, e.g., SIFT variations [53] and STIP [20], descriptor quantization strategies, e.g., Bag-of-Words, VLAD [13] and Fisher vector coding [42], the use of spatial pyramids [21] and various types of kernels to train classifiers, e.g., RBF, 
                           
                              
                                 
                                    χ
                                 
                                 
                                    2
                                 
                              
                           
                         and Histogram Intersection [29,59]. Choosing among these options provides us with a wide range of concept detectors with varying accuracies and computational costs. In this paper, we investigate how to create vocabulary concept detectors that are most suited for representing events by considering detectors of varying accuracy.

The state-of-the-art in video concept detection employs an SVM classifier to train detectors. SVM predictions are real-valued numbers that could be positive or negative. To perform the subsequent processing steps on the prediction scores, they should be normalized. The general SVM normalization approach in the literature [31,58,14] is to fit a sigmoid function on top of the prediction scores to estimate the posterior probabilities of concept presence [39,23]. The sigmoid function parameters are estimated from a held out partition of the concept detectors training data. In case the training and test data distributions differ, a common scenario when using pre-trained concept detectors for event recognition in arbitrary video, the detector reliability suffers [56]. Hence, the normalization should be executed with care. In this paper, we examine the influence of normalizing the predictions of vocabulary concept detectors on video event recognition accuracy. For this purpose, we consider several existing score normalizations [12].

We consider supervised normalization, which relies on labeled training data to fit the normalization function, e.g., Sigmoid normalization, and unsupervised normalization that does not require any labeled training data. To circumvent supervision, some unsupervised normalizations make assumptions about the distribution of scores. Z-score normalization, for example, assumes the scores have a Gaussian distribution, so the scores are normalized by shifting and scaling by their mean and standard deviation [12]. Others do not make any assumption about the distribution of scores. For example the recent W-score normalization, which models the tails of score distributions by the Extreme Value Theory [43] from statistics and then uses the models to estimate the concept presence probabilities. We assess the influence of normalizing detectors in a concept vocabulary for event recognition.

Our study on the effectiveness of concept vocabularies for video event recognition, is directed by five research questions. The first three questions investigate the ideal concept vocabulary composition, while the last two questions consider the creation of the vocabulary concept detectors. A preliminary version of this study has been published in [8]. Here we put more emphasis on characterizing the vocabulary concept detectors. Our five research questions are:
                           
                              
                                 RQ1 
                                 How many concepts to include in the vocabulary?
                              


                                 RQ2 
                                 What concept types to include in the vocabulary?
                              


                                 RQ3 
                                 Which concepts to include in the vocabulary?
                              


                                 RQ4 
                                 How accurate should the concept detectors be?
                              


                                 RQ5 
                                 How to normalize the concept detectors?
                              

As humans remember events by the high level concepts they contain, viz., actors, actions, objects, and locations [46], studying the characteristics of the concepts that humans use to describe events could be inspirational for automated event recognition. Therefore, before describing our experimental protocol to address the research questions, we first study the vocabulary that humans use to describe events in videos.

To analyze the vocabulary that humans use to describe events, we utilize a set of textual descriptions written by humans to describe web videos containing events. We process textual descriptions for 13,265 videos, as provided by the TRECVID 2012 Multimedia Event Detection task corpus [47]. For each web video in this corpus a textual description is provided that summarizes the event happening in the video by highlighting its dominant concepts. Fig. 2
                      illustrates some videos and their corresponding textual descriptions.

After removing stop words and stemming, we end up with 5433 distinct terms from the 13,265 descriptions making up a human vocabulary for describing events. Naturally, the frequency of these terms varies, as also observed by [9]. Most of the terms seldom occur in event descriptions. Whereas, only a few terms have high term-frequencies. To be precise, 50% of the terms occur once in the descriptions and only 2% occurs more than five times. Terms like man, girl, perform and street appear most frequent, while bluefish, conductor, Mississippi and Bulgarian are instances of less frequent terms. Looking into the vocabulary, we observe that the terms used in human descriptions can be mapped to five distinct concept types, as typically used in the computer vision and multimedia literature: objects, actions, scenes, visual attributes and non-visual concepts. We manually assign each vocabulary term into one of these five types. After this exercise we observe that 44% of the terms refer to objects. Moreover, we note that a considerable number of objects are dedicated to various types of animals and people; i.e., lion, and teen. About 21% of the terms depict actions, like walking. Approximately 10% of the concept types are about scenes, such as kitchen. Visual attributes cover about 13% of the terms; i.e., white, flat, and dirty. The remaining 12% of the terms belong to concepts, which are not visually depictable; i.e., poem, problem, and language. We summarize the statistics of our human event descriptions in Fig. 3
                     .

We observe that when describing video events, humans use terms with varying generalizations. Some terms are very specialized and refer to specific objects; like, salmon, cheesecake and sand castle. While other terms are more general and refer to a broader set of concepts; like human, vegetation and outdoor. We analyze the generalization of the vocabulary terms using their depth in the WordNet hierarchy [32]. In this hierarchy, the terms are structured based on their hypernym/hyponym relations, so the more specialized terms are placed at the deeper levels. Our study shows that the 5433 vocabulary terms have an average depth of 9.07±5.29. The high variance in term depths indicates that the human vocabulary to describe events is composed of both specific and general terms.

To summarize, analyzing the available event descriptions, we observe that the vocabulary that humans use to describe events is composed of a few thousand words, derived from five distinct concept types: objects, actions, scenes, visual attributes and non-visual concepts. Moreover, we observe that the vocabulary contains both specific and general concepts. Strengthened by these observations about the human vocabulary for describing events, we design five experiments to answer our research questions on the ideal vocabulary for recognizing events in arbitrary web video.

To answer the research questions raised in Section 2.3, we create a rigorous empirical setting. First, we introduce the video datasets used to evaluate the event recognition experiments. Then we explain the pool of concept detectors, which we employ to create vocabularies. Finally, the pipeline used for event recognition using concept vocabularies is presented.

For the event recognition experiments, we rely on two publicly available video collections: the TRECVID Multimedia Event Detection [47] and the Columbia Consumer Video [18] datasets. TRECVID MED 
                        [47] consists of 13,274 event videos sampled from the TRECVID 2012 Multimedia Event Detection task corpus, as used in [8]. This dataset consists of over 400h of user-generated video with a large variation in quality, length and content. Moreover, it comes with ground-truth annotations at video level for 25 real-world events, including life events, instructional events, sport events, etc. Following [8], the dataset is divided into a training set (66%) and a test set (34%).


                        Columbia CV 
                        [18] contains 9317 user-generated videos from YouTube. This dataset consists of over 210h of videos in total, where each video has an average length of 80s. Moreover, the dataset contains ground-truth annotations at video level for 20 semantic categories, where 15 of them are events. The other five categories are objects and scenes, which are excluded from the dataset in our experiments: “bird”, “cat”, “dog”, “beach” and “playground”. We use the training and test set divisions as defined in [18].

We summarize the training and test set statistics for both video datasets per event in Table 1
                        .

To create the vocabularies, we need a comprehensive pool of concept detectors. We build this pool of detectors using the human annotated training data from two publicly available resources: the TRECVID 2012 Semantic Indexing task [47,3] and the ImageNet Large-Scale Visual Recognition Challenge 2011 [6]. The former has annotations for 346 semantic concepts on 400,000 keyframes from web videos. The latter has annotations for 1000 semantic concepts on 1,300,000 photos. The categories are quite diverse and include concepts from various types; i.e., object, scene and action. Note that the training data is different from the TRECVID MED videos and their textual descriptions, which are used for studying the human vocabulary, as discussed in Section 3.

Leveraging the annotated data available in these datasets, we train 1346 concept detectors in total. We follow the state-of-the-art for our implementation of the concept detectors. We use densely sampled SIFT, OpponentSIFT and C-SIFT descriptors [53] with Fisher vector coding [42]. The codebook used has a size of 256 words. As a spatial pyramid we use the full image and three horizontal bars [21]. The feature vectors representing the training images form the input for a fast linear Support Vector Machine [45].

As summarized in Fig. 3, the concepts that humans use to describe events are derived from object, action, scene, attributes and non-visual concept types. It is hard to imagine that non-visual concepts can be detected by their visual features, so we exclude them from our study. With respect to the importance of the actors in depicting events [46], as well as their high frequency in human descriptions, we consider people and animal as extra concept types in our experiments. Inspired by this composition, we divide our concept pool by manually assigning each concept to one of the six types. Consequently, we end up with the following concept types: object containing 706 concepts, action containing 36 concepts, scene containing 135 concepts, people containing 83 concepts, animal containing 338 concepts and attribute containing 48 concepts. Fig. 4
                         provides an overview of the concept types and shows example instances.

In the event recognition experiments, we follow the common pipeline as used in the literature [31,58,14,30]. Unless noted otherwise we use the following implementation. We decode the videos by uniformly extracting one frame every two seconds. Then all the concept detectors are applied on the extracted frames. After concatenating the detector outputs, each frame is represented by a concept vector. Finally, the frame representations are pooled into a video level representation by averaging and normalizing as proposed in [43]. On top of this concept vocabulary representation per video, we use again a linear SVM classifier to train the event recognizers.

@&#EXPERIMENTS@&#

We perform five experiments to address our research questions. Each concept vocabulary used in the experiments is evaluated based on its performance in recognizing events using the datasets, pipeline and evaluation protocol described in Section 4. Moreover, the vocabularies are all derived from the concept pool introduced in Section 4.2.
                        
                           •
                           
                              
                                 Experiment 1: How many
                               
                              concepts to include in the vocabulary? To study this question, we create several vocabularies with varying sizes and evaluate their performance for recognizing events. Each vocabulary is made of a random subset of the concept detectors from the concept pool. To compensate for possible random effects, all experiments are repeated 50 times and the results are averaged.


                              
                                 Experiment 2: What concept types
                               
                              to include in the vocabulary? We look into this question by comparing two types of vocabularies: (i) single type vocabularies, where all concepts are derived from one type and (ii) joint type vocabularies, where concepts are derived from all available concept types. We perform this experiment for six kinds of single type vocabularies: object, action, scene, people, animal and attribute types respectively.

To make the single type and joint type vocabularies more comparable, we force the vocabularies to be of equal size. We do so by randomly selecting the same number of concepts from the concept pool. All the experiments are repeated 500 times to balance possible random effects.


                              
                                 Experiment 3: Which concepts
                               
                              to include in the vocabulary? In this experiment, we investigate whether the concept vocabulary for event recognition should be made of general concepts, specific concepts, or their mixture. We manually label and select two sets of general and specific concepts from the concept pool. The former contains 149 general concepts, i.e., vegetation, human and man made thing, and the latter contains 619 specific concepts, i.e., religious figure, emergency vehicle and pickup truck. The rest of the concepts, which are not clearly general or specific, are not involved in this experiment. Using these sets we compare three types of vocabularies: (i) a general vocabulary in which all the concepts are general, (ii) a specific vocabulary in which all the concepts are specific and (iii) a mixture vocabulary in which the concepts are randomly selected from both general and specific concept sets. We repeated this experiment for different vocabulary sizes and found that the results remained stable. The reported results are obtained for a vocabulary size of 70, averaged over 500 repetitions.


                              
                                 Experiment 4: How accurate
                               
                              should the concept detectors be? We look into this question by decreasing the detector accuracies and measuring how the event recognition performance responds. To decrease the detector accuracies we follow two different approaches: the first approach trains less sophisticated concept detectors, and the second approach imposes noise into the concept prediction scores.

In the first approach, we train four versions of our vocabulary concept detectors at different levels of sophistication: (i) 100%-3SIFT-SP is the most sophisticated version, where the detectors are implemented as described in Section 4.2. In this version, detectors are trained on all available training data. (ii) 30%-3SIFT-SP is similar to 100%-3SIFT-SP, but the detectors are trained on a random subset of 30% of the available concept training examples. (iii) 30%-SIFT-SP is similar to 30%-3SIFT-SP, but does not include any color SIFT and only relies on standard intensity SIFT. (iv) 30%-SIFT is the same as 30%-SIFT-SP, but without using any spatial pyramid. The four versions of the detectors are trained for the 346 semantic concepts from the TRECVID Semantic Indexing dataset.

In the second approach, we make the concept detectors inaccurate by gradually imposing increasing amounts of noise into their predictions. The output of each concept detector, as an SVM classifier, is a real-valued number which is supposed to be larger than +1 and smaller than −1 for positive and negative samples. However in practice, the SVM only assigns these values to the samples which are confidently classified, while other samples are assigned to the unconfident area in between −1 and 1. Looking into the concept detector predictions, we observe that most of them are agglomerated in the unconfident area. The less accurate a concept detector is, the more samples are assigned to the unconfident area. To simulate the detector accuracy changes, we randomly select predictions and shift them towards the center of the unconfident area, which has the least decision confidence. We gradually increase the amount of noise and repeat the experiments 50 times to compensate for possible random factors.


                              
                                 Experiment 5: How to normalize
                               
                              the concept detectors? In this experiment, we investigate the effect of normalizing concept vocabularies on video event recognition accuracy. We compare the representation obtained from un-normalized predictions with the representations obtained by applying several normalizations as introduced in Section 2.2: supervised, unsupervised, assumption-based and assumption-free normalization. We apply sigmoid normalization as a supervised method and compare it with Z-score and W-score, as instances of unsupervised normalizations. Moreover, to study the effect of making assumptions on the distribution of concept detector predictions we compare Z-score, which assumes a Gaussian distribution, with W-score normalization, which is an assumption-free method.

Each experiment results in a ranking of the videos from both the test sets based on the probability that the video contains the event of interest. As the evaluation criterion for these ranked lists, we employ average precision (AP) which is in wide use for evaluating visual retrieval results [47]. We also report the average performance over all events as the mean average precision (MAP).

@&#RESULTS@&#

As shown in Fig. 5
                        , adding more concept detectors to the vocabulary improves the event recognition performance. The improvement gain is particularly prevalent for small vocabularies. When increasing the vocabulary from 50 to 300, on TRECVID MED for example, the MAP increases from 0.125 to 0.221. The improvement is less prevalent when more than 1000 detectors are part of the vocabulary. When increasing the vocabulary from 1000 to 1346 the absolute MAP improvement is only 0.012 on average. We observe similar behavior on Columbia CV. We speculate that the improvement comes from the finer gain partitioning of the event feature space, which in our case is caused by the concept annotations, but is also achievable along other means [57].

However, by looking into individual event recognition results we observe that not all events behave similar when increasing the vocabulary size. For some events, i.e., “flash mob gathering”, a relatively high average precision of 0.34 is obtained by including only 50 concepts. We observe that there are some concepts within the vocabulary which are very discriminative for this event i.e., group of people, dancing and people marching. In contrast, for some other events i.e., “giving directions to a location”, the event recognition performance is not improved by increasing the vocabulary size. Apparently, there is no concept in the vocabulary, which can effectively discriminate this event from others. It demonstrates that besides the vocabulary size, the relevance of vocabulary concepts should be considered.

The error bars plotted in Fig. 5 indicate the variance in MAPs for various vocabularies. The variance demonstrates that with the same number of concept detectors, some vocabularies perform better than others. In the next two experiments, we study the characteristics of these optimal vocabularies.

Small vocabularies have poor performances in recognizing events. In addition, their effectiveness could be rapidly increased by adding a few more concept detectors. So, in general we recommend to include at least 200 concept detectors in the vocabulary.


                        Tables 2 and 3
                        
                         compare single type and joint type vocabularies for recognizing events. Comparing the MAPs for both datasets, we conclude that joint type vocabularies outperform single type vocabularies for all six concept types on average. It demonstrates that when creating the vocabulary, it is better to sample the concept detectors from diverse types. Hence, we need to detect the objects, people, actions and scenes occurring in the video jointly to recognize the event properly. In other words, all of the concept types contribute to the recognition of events.

When we analyze individual event recognition results, we observe a few cases exist where a single type vocabulary outperforms the joint type because of the tight connection between the event description and specific concepts. For example, using a single type vocabulary made of animals only, we achieve a higher average precision for “feeding animal”, “grooming animal” and “dog show” events in comparison to a joint type vocabulary on the TRECVID MED dataset. Similarly, the “Ice Skating” and “Skiing” events from the Columbia CV dataset are recognized better by the action concepts than by the joint vocabulary. Nevertheless, joint type vocabularies do better than single type vocabularies on average. Therefore, we consider joint type vocabularies more suited for general purpose event recognition. The performance difference between the single type and joint type vocabularies varies per concept type. For some types, like animal, the difference is substantial (0.158 vs. 0.239 and 0.310 vs. 0.265 on the TRECVID MED and the Columbia CV dataset respectively), while for others, like action, it is almost negligible (0.067 vs. 0.076 and 0.197 vs. 0.217 on the TRECVID MED and the Columbia CV dataset respectively). We attribute the performance difference to at least two reasons. First, our concept detectors are trained on a global image level, so they contain a considerable amount of contextual information. Consequently, some single types may contain a wide sample of contextual information including ‘semantic overlap’ from other concept types. The action pool, for example, may contain action detectors in varying scenes using various objects. Second, when creating several concept detectors for a similar type, it is likely that the detectors will be correlated, especially for the less diverse types, e.g., People and Animal. To clarify this observation we plot the correlation between concept detectors within a concept type in Fig. 6
                        . As shown in this figure, the highly correlated concepts tend to belong to the same concept type. Therefore, including too many concepts from the same type in a vocabulary, especially from the less diverse concept types like animal and people, leads to correlated concepts and should be avoided.

We recommend to make the vocabulary diverse by including concepts from various concept types and to limit the number of concepts for the less diverse types.


                        Tables 4 and 5
                        
                         compare three types of vocabularies: specific, general and mixture. According to the MAPs on both datasets, the general vocabulary performs better than the specific vocabulary, but the mixture vocabulary is in both cases the best overall performer.

We observe that for a few events a specific vocabulary outperforms the others, e.g., “repairing appliance” and “music performance”. For these events, there are some specific and discriminative concepts available in the vocabulary. For example, the washing machine, refrigerator and microwave concepts for “repairing appliance” and music stool, instrumental musician and acoustic guitar concepts for “music performance”. While the specific concepts may be distinctive for recognizing some events, the concepts typically occur in only few videos. Hence, they are absent in most videos and do not contribute much to generic event recognition. Therefore, if the vocabulary consists of specific concepts only, it will perform well in recognizing the events relevant to those concepts, but it will perform poor for other events. In contrast to the specific concepts, general concepts occur in a large numbers of videos. Although these concepts are not discriminative individually, taking several of them together into a vocabulary makes the event recognition better than using a specific vocabulary. Since it is able to simultaneously utilize distinctive specific concepts and general concepts, the best performance is obtained when the vocabulary contains a mixture of both specific and general concepts.

We recommend to insert both general and specific concepts into the event recognition vocabulary.


                        Tables 6 and 7
                        
                         demonstrate the effect of training less accurate vocabulary concept detectors on event recognition performance. Comparing 100%-3SIFT-SP and 30%-3SIFT-SP demonstrates the effect of using less examples to train vocabulary concept detectors. It shows that training concept detectors on 30% of the available training data does not substantially degrade the event recognition performance. More specifically on the TRECVID MED dataset, the performance is degraded only by a relative 8% in terms of MAP, and on the Columbia CV dataset the event recognition performance is not degraded at all. Comparing 30%-3SIFT-SP and 30%-SIFT-SP demonstrates the effect of using fewer descriptor types in training the detectors. It shows that using only SIFT descriptors, rather than concatenation of SIFT, Opponent-SIFT and C-SIFT descriptors, degrades the MAP is only by a relative 4% and 5% for the TRECVID MED and the Columbia CCV datasets, respectively. Furthermore, comparing 30%-SIFT-SP and 30%-SIFT demonstrates that including spatial pyramids in the detectors does not improve the event recognition. To summarize, the results demonstrate that the more sophisticated detectors do not substantially improve the event recognition performance.

Rather than training less sophisticated detectors we also perform the experiment with degrading the concept detector accuracies by imposing noise into their prediction scores. As expected, the results in Fig. 7
                         demonstrate that event recognition performance degrades by adding more noise to the concept detector predictions in the vocabulary. When the noise amount is rather small, i.e., up to 30%, the event recognition remains relatively robust. For a vocabulary containing 1346 concepts, on the TRECVID MED for example, the relative performance drops by only 3% when the noise amount is 30%. When 50% noise is inserted into the concept detection results for the full vocabulary, the performance drops by 11%. It means that even if 50% of the detector predictions are distorted, the event recognition performance will be degraded by only 11%. We observe even more robust behavior against the imposed noise on the Columbia CV. Interestingly, it implies that improving the current level of concept detector accuracy has at best a limited influence on the overall event recognition performance.

What is more, improving the detector accuracies has the same effect on event recognition performance as adding more detectors to the vocabulary. If we insert 50% noise into the vocabulary made of 50 concept detectors, on the TRECVID MED for example, the event recognition performance is 0.10 in terms of MAP. We may improve the accuracy by removing the noise again, or by adding 50 more (noisy) concept detectors to the vocabulary. In both cases the event recognition performance increases to 0.13 in terms of MAP. We observe similar behavior on Columbia CV. Considering the wide availability of large amounts of training data for concept detectors [11], adding more concept detectors seems to be more straightforward than improving the detector accuracies for event recognition vocabularies.

Our experiments confirm the observation by Hauptmann et al. [9]: effective video retrieval can be achieved even when concept detector accuracies are modest, if sufficiently many concepts are combined. As a conclusion, we recommend to increase the size of the concept vocabulary rather than improving the quality of the individual detectors.

The results of this experiment, are shown in Tables 8 and 9
                        
                        . Both tables demonstrate that the representation obtained from un-normalized detector predictions is outperformed by all the normalized representations. More specifically on the TRECVID MED, normalizing the detector predictions by sigmoid, Z-score, and W-score normalization improves the event recognition performance, in terms of MAP, by 13%, 68% and 89%, where on Columbia CV the numbers are 27%, 164% and 169%, respectively. This substantial improvement is achieved because normalization boosts the event representation by making the predictions of different concept detectors comparable. Looking into the distribution of detector predictions, as illustrated in Fig. 8
                        , we observe that different detectors generate different predictions distributions, which are not directly comparable. For some detectors a prediction score might indicate absence of the concept, while for some other detectors exactly the same score might indicate concept presence. Normalizing the predictions makes the vocabulary concept detectors more comparable, which leads to a better event recognition. Comparing the performance of supervised sigmoid normalization, with unsupervised Z-score and W-score normalizations, we observe that unsupervised methods are more effective in representing events. This contradicts the common practice in the literature to rely on sigmoid normalization e.g., 
                        [31,58,14]. As shown in Table 8 for the TRECVID MED dataset, using supervised score normalization we obtain an event recognition accuracy of 0.181, in terms of MAP. But with unsupervised normalization we achieve an MAP of 0.267 and 0.300 for Z-score and W-score, respectively. Similarly the supervised normalization is substantially outperformed by unsupervised normalizations on the Columbia CV dataset. The lower performance of supervised normalization is mainly caused by the fact that it assumes the distribution of concept presence on training and test data are similar. But this assumption is violated when the concept detectors are applied, as a vocabulary, on arbitrary videos that could have different concept presence distribution from the doctors training data. For example, the concept Military Vehicle might have a high probability of presence in its training data but it might never be present in the event videos. The difference in concept presence distributions between concept detector training data and event videos degrades the normalization performance, leading to a less effective event representation. As Table 8 shows, despite its simplicity Z-score normalization performs well in normalizing the detector outputs and achieves an event recognition accuracy of 0.267 in terms of MAP. We explain this by the observation that many concept detectors generate bell-shaped score distributions that could be modeled as Gaussian distribution. However, this Gaussian assumption is not valid for all the score distributions. Some concept detector distributions have high skewness and some others are not even bell-shaped, which violates the Gaussian distribution assumption, so degrades the normalization effectiveness. In our experiments, the best event recognition performance is obtained after applying the unsupervised and assumption-free W-score normalization. We explain it by two reasons. First, W-score, as an unsupervised normalization, does not suffer from the possible incompatibilities between the concept distributions in the concept detector training data and the event training data. Second, W-score does not make any assumption about the overall distribution of concept detector scores, leading to better generalization.

As a conclusion, we recommend to normalize the detector predictions in a concept vocabulary, preferably by unsupervised and assumption-free normalizations.

In this paper we study concept vocabularies for event recognition by characterizing the concept vocabulary composition and vocabulary concept detectors. We consider five research questions related to the number, the type, the specificity, the quality and the normalization of the detectors in concept vocabularies. From the analysis of our experiments using 1346 concept detectors, two large public video datasets containing 40 events and a general event recognition pipeline, we arrive at the following five recommendations:
                        
                           •
                           
                              Recommendation 1: In general, use vocabularies containing more than 200 concepts.


                              Recommendation 2: Make the vocabulary diverse by including various concept types: object, action, scene, people, animal and attributes. However, selecting too many concepts from the same type, especially the less diverse concept types, leads to correlated concepts and should be avoided.


                              Recommendation 3: Include both general and specific concepts into the vocabulary.


                              Recommendation 4: Increase the size of the concept vocabulary rather than improve the quality of the individual detectors.


                              Recommendation 5: Normalize the predictions of vocabulary concept detectors, preferably by an un-supervised and assumption-free normalization.

The recommendations may serve as guidelines to compose the appropriate concept vocabularies for future endeavors aiming for recognizing and, ultimately, explaining the semantics of complex events in video.

@&#ACKNOWLEDGMENTS@&#

This research is supported by the STW STORY project, the Dutch national program COMMIT, and by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center contract number D11PC20067. The US Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/NBC, or the US Government. The authors thank Dennis Koelma and Koen E.A. van de Sande for providing concept detectors.

@&#REFERENCES@&#

