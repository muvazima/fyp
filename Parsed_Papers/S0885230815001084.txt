@&#MAIN-TITLE@&#Weighted hierarchical archetypal analysis for multi-document summarization

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           We formalize the problem of MDS as the weighted hierarchical archetypal analysis problem.


                        
                        
                           
                           The weighted hierarchical archetypal analysis shows the ability to select “the best of the best” summary sentences.


                        
                        
                           
                           The paper demonstrates how four different summarization tasks can be treated by the proposed framework.


                        
                        
                           
                           Experimental results show that the framework is an effective summarization method.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Multi-document summarization framework

Weighted hierarchical archetypal analysis

General

Query-focused

Update

Comparative summarization

@&#ABSTRACT@&#


               
               
                  Multi-document summarization (MDS) is becoming a crucial task in natural language processing. MDS targets to condense the most important information from a set of documents to produce a brief summary. Most existing extractive multi-document summarization methods employ different sentence selection approaches to obtain the summary as a subset of sentences from the given document set. The ability of the weighted hierarchical archetypal analysis to select “the best of the best” summary sentences motivates us to use this method in our solution to multi-document summarization tasks. In this paper, we propose a new framework for various multi-document summarization tasks based on weighted hierarchical archetypal analysis. The paper demonstrates how four variant summarization tasks, including general, query-focused, update, and comparative summarization, can be modeled as different versions acquired from the proposed framework. Experiments on summarization data sets (DUC04-07, TAC08) are conducted to demonstrate the efficiency and effectiveness of our framework for all four kinds of the multi-document summarization tasks.
               
            

Multi-document summarization is an integral tool for document understanding. MDS enables better information benefits by creating brief and descriptive summaries for a large collection of documents. It has been found various uses in many real world applications. For example, multi-document summarization can be applied to summarize user-interested news events from a huge pool of news. A vast amount of available online texts are responsible for serious difficulties in development of the question answering or information retrieval systems. Fortunately, by extracting the vital information from documents, multi-document summarization can make possible development of the question answering or information retrieval systems. Note that the generated summary can be: (a) the general, non-restricted or typical (Mani, 2001); (b) the query-focused, most typical or archetypal summary which is explicitly biased toward a user defined query; (c) the comparative summary (Wang et al., 2012) which recaps dissimilarities between corresponding document groups; (d) and the update summary (Dang and Owczarzak, 2008) which produces very brief extract of the latest documents to apprehend novel information distinct from earlier documents.

In this paper, we propose a new framework for MDS using the weighted hierarchical archetypal analysis (wHAASum). Four different and known summarization tasks namely general, query-focused, update, and comparative summarization tasks, can be modeled as different versions acquired from the proposed framework. An effective foundation to promote their differences while settling the affinities among different summarization tasks is served by this framework.

Generally, the main topic of a document set is composed of some sub-topics. The assumption is that capturing the sub-topic structure is essential for a successful MDS system. These sub-topics can be treated using cluster-based methods by modeling the logic structure of the topics and sub-topics. However, the implicit structure of the topic can not only be represented by the explicit features such as statistical term distributions. In this paper, we argue that sentence selection in a MDS can be based on the implicit structure of the topic covered in the document set. By representing the relationship information with graph, we research the use of sub-topics as a model of the document collection, where sub-topics are represented as sub-archetypes. In this way, we investigated ways of selecting the most salient sentences from important sub-topic originating from significant topics by utilizing the hierarchical archetypal analysis.

In our summarization framework, the multi-document summarization problem is firstly generalized to the weighted hierarchical archetypal analysis problem. Then several useful properties of the wHAA are identified and taken into consideration for the greedy summarization algorithm. The latter is further shown to have the ability of addressing the multi-document summarization problem. We finally use this algorithm to propose the framework for different multi-document summarization tasks.

Our work displays benefits from two perspectives:
                     
                        1.
                        It proposes a new generic framework to address different summarization problems.

It proposes a novel version of the well-known archetypal analysis algorithm, namely the weighted hierarchical archetypal analysis algorithm.

The rest of the paper is organized as follows. In Section 1, we review the related work about different multi-document summarization tasks and the submodular function. After introducing the original archetypal analysis (AA) and weighted archetypal analysis (wAA) algorithms and after proposing the novel hierarchical version of wAA in Section 2, we propose the hierarchical wAA based summarization method in Section 3. Section 4 presents the framework for multidocument summarization, and shows how to model the four aforementioned summarization tasks. Section 5 presents experimental results of our framework on well accepted summarization data sets. Finally, Section 6 concludes the paper.

@&#RELATED WORK@&#

In a broad sense there are two types of summarization schemes: extractive and abstractive. Extractive summarization reduces the problem of summarization into the problem of selecting a most significant subset of the sentences in the original document set. Abstractive summarization is harder since it requires the composition of novel sentences, unseen in the original sources.

In the non-restricted, general summarization each sentence is scored with a significance value, and then sentences are ranked based on the significance score. The significance scores are commonly calculated as a combination of syntactic, semantic and statistical characteristics. The well-known baselines for extractive multi-document summarization can be categorized into one of the following general models: Centrality-based (Radev et al., 2004; Ribeiro and de Matos, 2011; Erkan and Radev, 2004), maximal marginal relevance (MMR) (Carbonell and Goldstein, 1998; Sanner et al., 2011), coverage-base methods (Lin and Hovy, 2000; Sipos et al., 2012), and hybrids (centrality+coverage-based) (Marujo et al., 2015; Ribeiro et al., 2013) Centrality-as-relevance methods base the detection of the most salient passages on the identification of the central passages of the input source(s). MMR methods are based on a measure for quantifying the extent of dissimilarity between the sentences being considered and those already selected. Coverage-based summarization defines a set of concepts that need to occur in the sentences selected for the summaries. Although wHAASum is not a typical example of any of this groups, it mostly belongs to centrality based methods.

In last years, many approaches for sentence extraction building upon the algebraic methods, more precisely matrix decomposition methods have emerged. Common approaches used in MDS spread from low rank approximations such as singular value decomposition (SVD) (Steinberger and Ježek, 2005), principal component analysis (PCA) (Lee et al., 2003), latent semantic indexing (LSI/LSA) (Yeh, 2015), non-negative matrix factorization (NMF) (Lee et al., 2009), symmetric-NMF (Wang et al., 2008), probabilistic LSA (Hennig and Labor, 2009) to soft clustering approaches such as the EM-algorithm for clustering (Ledeneva et al., 2011) and hard assignment clustering methods such as K-means (Wang et al., 2008), even to eigen decomposition which are also known as the graph based methods (Erkan and Radev, 2004). These techniques can be jointly seen as a factor analysis description of input data exposed to different constraints. An advantage of low-rank approximations is that they have a great degree of flexibility, but the features can be harder to interpret. While clustering approaches extract features that are similar to actual data, making the results easier to interpret, on the other hand the binary assignments reduce flexibility. Archetypal Analysis and its different versions presented in this work are typical matrix factorization techniques. But in contrast to other factorization methods, which extract prototypical, characteristic, even basic sentences, AA selects distinct (archetypal) sentences and thus induces variability and diversity in produced summaries. Another significant property of AA is its ability to directly combine the virtues of clustering and the flexibility of matrix factorization. wHAASum is also a graph-based method since it can be directly applied to sentence similarity graph (matrix).

Extracting summary for answering user's stated information need expressed via the given topic or query is the main constraint of the query focused summarization task. In it the given query should be incorporated into very nature of the summarization method. Many methods for the generic summarization can be extended to incorporate the query information, including singular value decomposition (SVD) (Arora and Ravindran, 2008), latent semantic indexing (LSA) (Yeh, 2015), non-negative matrix factorization (NMF) (Park et al., 2006) and symmetric-NMF (Wang et al., 2008). The graph-based ranking algorithms were also used in query-focused summarization when it became a popular research topic. For instance, a topic-sensitive version of LexRank is proposed by (Otterbacher et al., 2009).

Update summarization was first presented at the Document Understanding Conference (DUC) 2007. Then it was a main task of the summarization track at the Text Analysis Conference (TAC) 2008. The update summarization task aims to summarize a set of documents presuming that the first set of documents has already been read and summarized as the main summary. For generating the update summary, some clever solutions are required to capture the temporally evolving information and avoid the redundant information which has already been covered by the main summary. The timestamped graph model (Lin et al., 2007), motivated by the evolution of citation network, tries to model the temporal aspect of update summarization. A novel graph based sentence ranking algorithm, namely PNR2, for update summarization as presented in Wenjie et al. (2008), is inspired by the intuition that “a sentence receives a positive influence from the sentences that correlate to it in the same collection, whereas a sentence receives a negative influence from the sentences that correlates to it in the different (perhaps previously read) collection”.

Comparative document summarization is first proposed in Wang et al. (2012) to summarize differences between comparable document groups. Wang et al. (2012) presents a sentence selection strategy modeled by means of conditional entropy, which precisely discriminates the documents in different groups.

One of the rather rare papers on summarization frameworks describes the principled and versatile framework for different multidocument summarization tasks using submodular functions (Li et al., 2012).

Archetypal analysis originates from the Greek word “archetype” which stands for the prototype based on which others are formed. As follows, identifying these prototypes, archetypes, within a given data set is the primary purpose of archetypal analysis. Archetypal analysis is a general unsupervised learning technique that can be used in many research areas, such as in economics, text mining, statistics, and also in pattern recognition (Bauckhage and Thurau, 2009). The problem of discovering the archetypes in a data set can be described as a problem of finding a few cases (archetypes) in a set of multivariate cases so that all the data can be well represented as convex combinations of the archetypes. The cases do not have to be necessarily from the original data, but can be computed from them. Usefulness of AA for feature extraction and dimensionality reduction for a large variety of machine learning problems taken from computer vision, neuro imaging, chemistry, text mining and collaborative filtering, is vastly presented by Mørup and Hansen (2012). The concept of AA was originally formulated in Cutler and Breiman (1994). Cutler and Breiman (1994) formalized AA as the problem of learning the convex hull of a data set, and solved it using alternating non-negative least squares method, see Fig. 1
                        b. They defined the problem as a nonlinear least-squares problem and presented an alternating minimizing algorithm to solve it.

The weighted version of the archetypal analysis was first introduced in a paper by (Eugster and Leisch, 2011).

It is important to note that the archetypal analysis approach to document summarization has been investigated in our previous work (Canhasi and Kononenko, 2014a,b, 2011). There are some fundamental differences between this work and our previous work: (i) In (Canhasi and Kononenko, 2014a), the general (unrestricted) document summarization problem is treated by means of a plain archetypal analysis problem. Sentences from original documents have been modeled as the joint matrix of a content and a similarity graph. The obtained matrix is then further analyzed by AA in order to simultaneously cluster and rank the original sentences. (ii) In (Canhasi and Kononenko, 2014b), the query focused summarization problem is formalized as a weighed version of the archetypal analysis problem. Input data, i.e. sentences from the original documents and the given query, are modeled as a multi-element graph of documents, sentences, terms and the query.

This paper, differently from our previous work, does not treat only the general or query focused summarization task but rather proposes a framework for all known summarization tasks and the proposed framework is based on a hierarchical version of wAA. To the best of our knowledge, the problem of hierarchical wAA has not been proposed or studied before. Therefore, another significant contribution of this work is the presentation of the wHAA technique with its application to summarization.

In order to motivate the wAA algorithm and to trace the development process to the point of wAA, in this section we first summarize the archetypal analysis as presented in its original form (Cutler and Breiman, 1994) and then we briefly present the weighted version of AA(wAA) mainly inherited from Eugster and Leisch (2011).

Let as consider a matrix X
                        =[x
                        1, …, x
                        
                           n
                        ] in 
                           
                              
                                 
                                    ℝ
                                 
                              
                              
                                 n
                                 ×
                                 m
                              
                           
                         representing a multivariate data set with n observations and m variables. Archetypal analysis for given archetype number p
                        ≪
                        n factorizes a given matrix X into stochastic matrices 
                           P
                           ∈
                           
                              
                                 
                                    ℝ
                                 
                              
                              
                                 n
                                 ×
                                 p
                              
                           
                         and 
                           Q
                           ∈
                           
                              
                                 
                                    ℝ
                                 
                              
                              
                                 n
                                 ×
                                 p
                              
                           
                         as defined by Eq. (1)
                        
                           
                              (1)
                              
                                 X
                                 ≈
                                 
                                    PQ
                                    T
                                 
                                 X
                              
                           
                        
                     

More exactly, the archetypal problem is to calculate two matrices Q and P which minimize the residual sum of squares


                        
                           
                              (2)
                              
                                 
                                    
                                       
                                          RSS
                                          (
                                          k
                                          )
                                          =
                                          ‖
                                          X
                                          −
                                          
                                             PV
                                             T
                                          
                                          
                                             ‖
                                             2
                                          
                                          
                                          with
                                          
                                          V
                                          =
                                          
                                             X
                                             T
                                          
                                          Q
                                       
                                    
                                    
                                       
                                          
                                             s
                                             .
                                             t
                                             .
                                          
                                          
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             p
                                          
                                          
                                             P
                                             ij
                                          
                                          =
                                          1
                                          ,
                                          
                                             P
                                             ij
                                          
                                          ≥
                                          0
                                          ;
                                          
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             m
                                          
                                          
                                             Q
                                             ij
                                          
                                          =
                                          1
                                          ,
                                          
                                             Q
                                             ij
                                          
                                          ≥
                                          0
                                       
                                    
                                 
                              
                           
                        
                     

The feature matrix V is ensured to be a convex combination of the archetypes via the constraints 
                           
                              ∑
                              
                                 i
                                 =
                                 1
                              
                              p
                           
                           
                              P
                              ij
                           
                           =
                           1
                         and P
                        
                           ij
                        
                        ≥0. Similarly, the constraints 
                           
                              ∑
                              
                                 i
                                 =
                                 1
                              
                              m
                           
                           
                              Q
                              ij
                           
                           =
                           1
                         and Q
                        
                           ij
                        
                        ≥0, enforce that each archetype is a meaningful combination of data points. ||·||2 denotes the Euclidean matrix norm.

Eq. (2) defines the bases of the estimation algorithm presented by pioneering AA paper (Cutler and Breiman, 1994). It alternates between finding the best P for given archetypes V and finding the best archetypes V for given Q; where at each step many convex least squares problems are solved until the overall RSS is reduced successively.

The inclusive framework for archetypal analysis in step by step description is the following:

Given the number of archetypes p:
                           
                              1.
                              Pre-processing: scale data.

Initialization: initialize Q in a way the constrains are satisfied to calculate the starting archetypes V.

Repeat while a stopping criterion is not met, i.e. stop when RSS is small enough or the maximum number of iteration is reached:
                                    
                                       3.1.
                                       Find best P for the given set of archetypes V, i.e. solve n convex least squares problems, where i
                                          =1, …, n
                                          
                                             
                                                
                                                   
                                                      min
                                                      
                                                         
                                                            P
                                                            i
                                                         
                                                      
                                                   
                                                   =
                                                   
                                                      1
                                                      2
                                                   
                                                   ‖
                                                   
                                                      X
                                                      i
                                                   
                                                   −
                                                   
                                                      VP
                                                      i
                                                   
                                                   
                                                      ‖
                                                      2
                                                   
                                                   
                                                   
                                                      s
                                                      .
                                                      t
                                                      .
                                                   
                                                   
                                                   
                                                      ∑
                                                      
                                                         j
                                                         =
                                                         1
                                                      
                                                      z
                                                   
                                                   
                                                      P
                                                      ij
                                                   
                                                   =
                                                   1
                                                   ,
                                                   
                                                      P
                                                      ij
                                                   
                                                   ≥
                                                   0
                                                   .
                                                
                                             
                                          
                                       

Recalculate archetypes 
                                             
                                                V
                                                ˆ
                                             
                                           by solving a system of linear equations 
                                             X
                                             =
                                             P
                                             
                                                
                                                   
                                                      V
                                                      ˆ
                                                   
                                                
                                                T
                                             
                                          .

Find best Q for the given set of archetypes 
                                             
                                                V
                                                ˆ
                                             
                                          , i.e, solve p convex least squares problems where j
                                          =1, …, p
                                          
                                             
                                                
                                                   
                                                      min
                                                      
                                                         
                                                            Q
                                                            j
                                                         
                                                      
                                                   
                                                   =
                                                   
                                                      1
                                                      2
                                                   
                                                   ‖
                                                   
                                                      
                                                         
                                                            
                                                               V
                                                               j
                                                            
                                                         
                                                         ˆ
                                                      
                                                   
                                                   −
                                                   
                                                      XQ
                                                      j
                                                   
                                                   
                                                      ‖
                                                      2
                                                   
                                                   
                                                   
                                                      s
                                                      .
                                                      t
                                                      .
                                                   
                                                   
                                                   
                                                      ∑
                                                      
                                                         i
                                                         =
                                                         1
                                                      
                                                      m
                                                   
                                                   
                                                      Q
                                                      ij
                                                   
                                                   =
                                                   1
                                                   ,
                                                   
                                                      Q
                                                      ij
                                                   
                                                   ≥
                                                   0
                                                   .
                                                
                                             
                                          
                                       

Recalculate archetypes V
                                          =
                                          X
                                          
                                             T
                                          
                                          Q.

Recalculate RSS.

Post-processing: rescale archetypes.


                        Initialization: Cutler and Breiman point out that some attention should be given in choosing the initial mixtures that are not too close together because this can cause slow convergence or convergence to a local optimum. To ensure the Breiman's point on choosing initial mixtures (archetypes) we use the following method. The method proceeds by randomly selecting a data point as an archetype and selecting subsequent data points x
                        
                           i
                         (archetypes) the furthest away from already selected ones x
                        
                           j
                        . Such a new data point is selected according to
                           
                              (3)
                              
                                 
                                    a
                                    new
                                 
                                 =
                                 
                                    max
                                    i
                                 
                                 
                                    
                                       
                                          
                                             ∑
                                             j
                                          
                                          
                                             ‖
                                             
                                                x
                                                i
                                             
                                             −
                                             
                                                x
                                                j
                                             
                                             ‖
                                             ,
                                             j
                                             ∈
                                             G
                                          
                                       
                                    
                                 
                              
                           
                        where ||·|| is a given norm and G is a set of indices of current selected points.


                        Convergence: Cutler and Breiman (1994) show that the algorithm converges in all cases, but not necessarily to a global minimum. They also note that, like many alternating optimization algorithms, their algorithm results in a fixed point of an appropriate transformation, but there is no guarantee that this will be a global minimizer of RSS. For further details on convergence see Cutler and Breiman (1994).

The original archetypal problem as defined by Eq. (1), requires that each data point and hence each residual participate in the minimization with the same weight. Assuming W is a complementing n
                        ×
                        n square weight matrix, the weighted version of the archetypal problem (Eugster and Leisch, 2011) can be formulated as the minimization of:


                        
                           
                              (4)
                              
                                 RSS
                                 (
                                 k
                                 )
                                 =
                                 ‖
                                 W
                                 (
                                 X
                                 −
                                 
                                    PV
                                    T
                                 
                                 )
                                 
                                    ‖
                                    2
                                 
                                 
                                 with
                                 
                                 V
                                 =
                                 
                                    X
                                    T
                                 
                                 Q
                              
                           
                        
                     

Since weighting the residuals is equivalent to weighting the data set:


                        
                           
                              
                                 W
                                 (
                                 X
                                 −
                                 
                                    PV
                                    T
                                 
                                 )
                                 =
                                 W
                                 (
                                 X
                                 −
                                 P
                                 
                                    
                                       (
                                       
                                          X
                                          T
                                       
                                       Q
                                       )
                                    
                                    T
                                 
                                 )
                                 =
                                 W
                                 (
                                 X
                                 −
                                 
                                    PQ
                                    T
                                 
                                 X
                                 )
                                 =
                                 WX
                                 −
                                 W
                                 (
                                 
                                    PQ
                                    T
                                 
                                 X
                                 )
                                 =
                                 WX
                                 −
                                 (
                                 WP
                                 )
                                 (
                                 
                                    Q
                                    T
                                 
                                 
                                    W
                                    
                                       −
                                       1
                                    
                                 
                                 )
                                 (
                                 WX
                                 )
                                 =
                                 
                                    
                                       X
                                       ˆ
                                    
                                 
                                 −
                                 
                                    
                                       P
                                       ˆ
                                    
                                 
                                 
                                    
                                       Q
                                       ˆ
                                    
                                 
                                 
                                    
                                       X
                                       ˆ
                                    
                                 
                                 =
                                 
                                    
                                       X
                                       ˆ
                                    
                                 
                                 −
                                 
                                    
                                       P
                                       ˆ
                                    
                                 
                                 
                                    
                                       
                                          V
                                          ˆ
                                       
                                    
                                    T
                                 
                              
                           
                        
                     

The problem can be rewritten as minimizing


                        
                           
                              (5)
                              
                                 RSS
                                 (
                                 k
                                 )
                                 =
                                 ‖
                                 
                                    
                                       X
                                       ˆ
                                    
                                 
                                 −
                                 
                                    
                                       
                                          P
                                          ˆ
                                       
                                    
                                    
                                       
                                          V
                                          ˆ
                                       
                                    
                                 
                                 
                                    ‖
                                    2
                                 
                                 
                                 with
                                 
                                 
                                    
                                       V
                                       ˆ
                                    
                                 
                                 =
                                 
                                    
                                       Q
                                       ˆ
                                    
                                 
                                 
                                    
                                       X
                                       ˆ
                                    
                                 
                                 
                                 and
                                 
                                 
                                    
                                       X
                                       ˆ
                                    
                                 
                                 =
                                 WX
                              
                           
                        
                     

Using the original algorithm in calculating the weighted AA is possible based on this reformulation with the supplementary pre-processing step to calculate 
                           
                              X
                              ˆ
                           
                         and the additional post-processing step to recalculate P and Q for the data set X given the archetypes 
                           
                              
                                 V
                                 ˆ
                              
                           
                           =
                           Q
                           
                              
                                 X
                                 ˆ
                              
                           
                        . The weight matrix W can represent different intentions, see Fig. 1c.

We have already shown how AA and wAA are able to identify the extreme data points which are lying on the convex-hull of the given data set. In document summarization, given a matrix representation of a set of documents, positively and/or negatively salient sentences are values on the data set boundary. These extreme values, archetypes, can be computed using AA. Similarly, in query-focused summarization given a graph representation of a set of sentences, weighted by similarity to the given query, positively and/or negatively salient sentences are values on the weighted data set boundary. Weighted AA can be used to compute these extreme values, archetypes, and hence to estimate the importance of sentences in the target documents set. Unfortunately, matrices representing sentence similarity graphs can be often very complex. They can have complex inner structure, i.e. clusters of sentences representing different topics. Although AA and wAA are successful in treating situations where data is convex and when the “outer” extreme values are of the interest, they have some clear limitations when their usage in summarization is considered. These concerns include (1) how can AA and wAA be used when data sets are complex, as it is with sentence similarity matrices, (2) and how to use AA and wAA for finding not only outer but also the inner extreme values. As an illustrative (already low-dimensional) example consider Fig. 2
                        . It depicts a typical “non-convex data” situation. We have drawn data points from 3 randomly positioned data clusters in 2D and were interested in computing inner extreme values. By design, AA assigns clusters to the “extreme” data groups and not to “inner” ones, as illustrated by Fig. 2a. Although we can still reconstruct each data point perfectly, this is discouraging. The intrinsic (low dimensional) structure of the data is not captured and, in turn, the representation of the data is not as meaningful as it could be.

As solution to mentioned limitations we propose in this paper hierarchical version of AA(wAA), namely wHAA. wHAA automatically adapts to the low intrinsic dimensionality of data as illustrated in Fig. 2b–d. The algorithm design of the wHAA is based on the well known Divisive Analysis (DIANA) (Kaufman and Rousseeuw, 2009). This variant of hierarchical clustering is also known as “top down” approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy. wHAA is based on a hierarchical decomposition of 
                           
                              
                                 
                                    ℝ
                                 
                              
                              D
                           
                         in the form of a tree. Note that the standard wAA corresponds to a tree of depth zero.

Let us now first outline the proposed wHAA algorithm (as we implement and use in summarization) very generally. In subsequent subsections, we describe parts of the algorithm in more details and discuss different options for its usage in summarization.

Consider a n
                           ×
                           m matrix X. Given the number of archetypes p and the number of levels l, begin by running the wAA on the entire data set. Repeat the following steps at each level of the tree:
                              
                                 1.
                                 
                                    Partitioning: Assign each data point to only one partition based on the archetypal membership value. At the end of the step there should be p subsets of the parent data set.


                                    Processing: run the wAA algorithm on each of the subsets.


                                    Ordering: Order the new archetypes

Stop splitting the node when the number of levels in the tree is equal to l or when there are fewer data points than p. The final level is an ordered list of archetypes. Partitioning step: Since the goal is to divide the data set to smaller subsets, in this step we use wAA output from previous iteration to cluster all data points to distinct archetypes. The wAA produces two stochastic matrices P and Q. The latter one consists of rows representing archetypes and columns denoting the archetypal membership values of each data point. Therefore, assigning each data point to one archetype, (i.e. splitting the data set) is straightforward. The data point is assigned to only one archetype for which it has the highest membership value.


                           Processing step: In order to split data for the next iteration, we run wAA on each sub-dataset embedded in internal nodes. When stopping conditions are reached this step is executed for the last time and the wAA results are embedded in the corresponding leaf.

The connection between the weighted archetypal analysis (consequently between the weighted hierarchical archetypal analysis) and the multidocument summarization can be easily identified. As discussed in Section 2.2 weighted AA can be used in identifying the “best” summary sentences for query focused summarization. The same reasoning applies to generic summarization task where we simply do not use weight matrix, and ergo weighted AA becomes standard AA. Besides that AA(wAA) is generally able to select the “best” summary sentences, it has many other useful properties, including: (1) it is an unsupervised method; (2) in contrast to other factorization methods which extract prototypical, characteristic, even basic sentences, AA selects distinct (archetypal) sentences, thus induces variability and diversity in produced summaries; (3) extracted sentence can be represented as convex combination of archetypal sentences, while the archetypes themselves are restricted to being very sparse mixtures of individual sentences and thus supposed to be more easily interpretable; and finally (4) it readily offers soft clustering, i.e. simultaneous sentence clustering and ranking.

Unfortunately, despite all of its advantages, AA(wAA) lacks in a few fundamental aspects already described in previous section. There, as a solution, we proposed weighted hierarchical archetypal analysis. Since the wHAA method basically inherits all the pros of AA(wAA) and introduces some new summarization-suitable properties, we use wHAA for the multidocument summarization task.

Let us now delve into multidocument summarization task from the perspective of summarization algorithm based on wHAA, namely wHAASum.

For a pool of sentences formed from the given document set, the problem is how to pick up the most representative sentences as a summary of this document set. For accomplishing stated problem we present a relatively simple approach where sentences are hierarchically soft-clustered into weighted archetypes in order to generate the sentence ranking where the top ranked sentences are consequently one by one extracted, until the length constraint (l sentences) is reached. The framework of the proposed method, wHAASum, consists of the following steps:
                           
                              1.
                              Construct the input matrix X depending in the summarization task.

Generate the input weight matrix W.

Perform wHAA on matrix X given the W.
                                    
                                       i.
                                       Build the tree of hierarchical decomposition where internal nodes contain splitting criteria and the leafs consist of the final decomposition matrices Q
                                          
                                             i
                                           and P
                                          
                                             i
                                          .

Select l sentences to form the final summary.
                                    
                                       i.
                                       Start with the leftmost (the most significant) leaf and extract sentence(s) with the highest archetypal membership value (according to their values in Q
                                          
                                             i
                                          ). Continue with next leftmost leaf until the summary length constraint is met. That is, sentences with the highest archetype membership value in each leaf's most significant archetype (row of matrix Q
                                          
                                             i
                                          ) are selected and if the summary length is not met then the extraction step continues with the next leaf, and so on.

Previously selected sentences are than compared to newly selected sentence in order to avoid extracting the significantly similar ones.

The fourth step in the above algorithm starts the sentence extraction from the leftmost leaf and follows with the next leftmost leaf until the desired summary length is reached. Worth to mention is the technique of actual sentence selection used at each leaf. Leaves contains P and Q decomposition matrices. Since P matrices has been previously used in splitting, here Q matrices can be used specifically for sentence ranking. Columns of this matrix denote sentences whereas rows represent ordered archetypes. Selecting sentence(s) from Q is therefore straightforward, the sentence with the highest membership value in the first row of the matrix is selected as the most outstanding one.

The two crucial aspects of the summarization, namely the relevance and the content coverage, are, in our understanding, successfully treated by this approach. The another vital result of the presented method is diversity optimization. Even thought diversity is to some extent provided by the definition of archetypal analysis in order to more effectively remove redundancy and increase the information diversity in the summary, we use a greedy algorithm presented in the last step (4.ii) of the above algorithm. In the following subsection we present the usage of wHAASum on an illustrative example.

For evidencing the benefits of using the wHAASum as the method for hierarchically simultaneous sentence clustering and ranking with respect to the sentence weights, a simple example is given in Fig. 3
                        . In it the artificial data set is given in the form of undirected sentence similarity graph, where nodes denote sentences and edges represent similarity between connected nodes. To better present the idea of wHAASum and even more simplify presentation in this example, we do not consider the sentence node weights (sentence to query similarity). This could be easily realized by connecting each sentence to the given query with the edge weighted by the corresponding sentence to query similarity.

In our artificial data set we present two clusters of sentences, where {S
                        1, S
                        9} are the central sentences of the first and S
                        6 of the second cluster. We purposely simulated a topic drift situation in the first cluster right in the neighborhood of S
                        4.

By setting the number of archetypes p
                        =3, the number of levels l
                        =3 and then by applying wHAASum we obtain the decomposition tree and the accompanied set of matrices P
                        
                           i
                        , Q
                        
                           i
                         and XQ
                        
                           i
                        . In Fig. 4
                        , for the sake of simplicity, we only show the Q matrices obtained by repeated use of wAA in hierarchical archetypal analysis. wHAA can be interpreted as a kind of top-down clustering or divisive clustering. We start at the top with all sentences in one cluster. The cluster is then split using a flat wAA algorithm. This procedure is applied recursively until the number of sentences is less than the required archetype number in each cluster (archetype). As one can see from Fig. 4, at the beginning all sentences are in one cluster. Then by applying wAA on the entire set of sentences three archetypes are obtained. Table (a) in Fig. 4 represents matrix Q produced after the fist cycle of sequential wAA calls. Each row of this matrix is one of the estimated archetypes, and rows are presented in decreasing order of the archetypal significance. Let us now further analyze this intermediate result. In fact, one can successfully extract summary sentences by plainly relaying on this first result, as it is presented in Canhasi and Kononenko (2014a). The possible difficulty occurs when the plain wAA is trapped in a local minimum. It is obvious that sentences {S
                        9, S
                        1, S
                        12} could be selected as summary sentences since those have high archetypal membership values in the most significant archetype (3rd row). The local minimum issue in this problem is manifested when there is not enough information to make a right decision on sentence picking. A typical situation is reached at the second archetype (row) in table (a). Here one cannot easily decide which of the sentences S
                        6, S
                        7 or S
                        8 to pick. From the original graph it is obvious that sentence S
                        6 should be picked. Similar issue appears in the first row of the same table. Here, the sentence S
                        4 can be picked as the most significant, but still the sentence S
                        12 is ranked lower, which does not reflect the reality. In our previous work those issues were treated by increasing the number of archetypes. In this work we continue the summarization process by further dividing the data set to sub-archetypes. As described in previous sections, the complete set of sentences are now separated in three groups. Splitting is based on results from table (a). The first group includes {S
                        1, S
                        9}, the second contains {S
                        6, S
                        7, S
                        8} and the last holds the rest of sentences, i.e. {S
                        2, S
                        3, S
                        4, S
                        5, S
                        10, S
                        11, S
                        12} In the second level of Fig. 4 three child nodes are presented. Each one is in fact an archetype from previous level, i.e. from the root.

The table Res(1) denotes the first leaf of the decomposition tree. From previous level the third archetype, also the most significant one, has been used in forming the next leftmost sub-tree. But since the newly formed node has the number of elements (2) lower than the number of desired archetypes (3), the further splitting is stopped at this point and the node is converted to a leaf.

For interpretation, we identify the internal nodes as intermediate clustering results, leaves as final clustering and ranking results and the final archetypal sentences found in leaves as different types with different degree of potentially “good” and “bad” summary sentences. Then we set the observations in relation to them. Under this circumstances, the first produced leaf is the “best” sub-archetype containing ranking of the “best” archetypal sentences. The sentence S
                        9 is the best of the best, which is followed by the sentence S
                        1. The same procedure continues with other child-archetypes. Note that only the leftmost leaves are shown in Fig. 4. This is due to logic of the original summarization algorithm, where the goal is to extract only the “best of the best” archetypal sentences. Table (b) from the second level is further clustered into three new child archetypes, but only the leftmost leaf is used for actual sentence selection. Table Res(2) shows the second selected sentence S
                        6. Note that this sentence could not be selected at the first level of the tree however at this point it is the “best” summary sentence of the second archetype. Table (c) represents the largest archetype produced by previous splitting and therefore it is further divided into three new child archetypes. Again, the leftmost leaf is used for direct sentence selection while the second and third archetypes are additionally processed. This example shows that the output decomposition tree and matrices produced by wHAA describe the data structure well and at different levels of details.

Our proposed wHAA-based summarization framework can be directly used for different multi-document summarization tasks, including generic, query-focused, update and comparative summarization. To adapt the wHAASum to different summarization task but still use the same method we model the summarization tasks differences by means of input matrix modeling. In this section, we formulate each summarization task by defining different matrix generation functions, henceforth labeling functions. Table 2 summarizes these labeling functions for different summarization tasks, and Table 1
                     
                      presents the notations. The general procedure of methods for different summarization tasks is described in Section 3.1, while the only difference resides in the labeling functions.

The fundamental idea of labeling functions used in this work is based on representing the input sentences via sentence similarity graph. Note that the principal data structures used for graph representation are matrices, such as adjacency, incidence, etc. Therefore, the rest of this section describes the basic sentence similarity matrix generation process.

Given the set of documents D one can produce a set of sentences D
                     ={s
                     1, s
                     2, …, s
                     
                        n
                     }, where n denotes the number of sentences and s
                     
                        i
                      denotes the ith sentence in D. First, through representing each sentence as a vector we form the sentence to sentence similarity matrices. The vector space model is the most known representation scheme for textual units via which the terms are represented by their counts. Let T
                     ={t
                     1, t
                     2, …, t
                     
                        m
                     } represent all the unique terms occurring in the collection, where m is the number of different terms. The standard vector space model (VSM) using the bag of the words approach represents the text units of a corpus as vectors in a vector space. Traditionally, a whole document is used as a text unit, but in this work for this purpose we used only sentences. Each dimension of a vector corresponds to a term that is present in the document. Term weighting used to weight terms is frequency-inverse sentence frequency (tf-isf) weighting scheme. It combines local and global weighting of a term. The local term weighting measures the significance of a term within a sentence. The global term weighting or the inverse sentence frequency isf measures the importance of a term within the sentence collection. As a result the tf-isf weighting scheme can be formulated as:
                        
                           (6)
                           
                              
                                 w
                                 ik
                              
                              =
                              
                                 tf
                                 ik
                              
                              ×
                              
                                 isf
                                 ik
                              
                              =
                              
                                 freq
                                 ik
                              
                              ×
                              log
                              
                                 
                                    
                                       
                                          n
                                          
                                             
                                                n
                                                k
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     here the weight 
                        
                           w
                           ik
                        
                      of a term t
                     
                        k
                      in a sentence s
                     
                        i
                      is defined by the product of the local weight of term t
                     
                        k
                      in sentence s
                     
                        i
                      and the global weight of term t
                     
                        k
                     . A popular similarity measure is the cosine similarity which uses the weighting terms representation of the sentences. According to the VSM the sentence s
                     
                        i
                      is represented as a weighting vector of the terms, 
                        
                           s
                           i
                        
                        =
                        [
                        
                           w
                           
                              i
                              1
                           
                        
                        ,
                        
                           w
                           
                              i
                              2
                           
                        
                        ,
                        …
                        ,
                        
                           w
                           im
                        
                        ]
                     , where 
                        
                           w
                           ik
                        
                      is the weight of the term t
                     
                        k
                      in the sentence s
                     
                        i
                     . This measure is based on the angle α between two vectors in the VSM. The closer the vectors are to each other the more similar are the sentences. The calculation of an angle between two vectors 
                        
                           s
                           i
                        
                        =
                        [
                        
                           w
                           
                              i
                              1
                           
                        
                        ,
                        
                           w
                           
                              i
                              2
                           
                        
                        ,
                        …
                        ,
                        
                           w
                           im
                        
                        ]
                      and 
                        
                           s
                           j
                        
                        =
                        [
                        
                           w
                           
                              j
                              1
                           
                        
                        ,
                        
                           w
                           
                              j
                              2
                           
                        
                        ,
                        …
                        ,
                        
                           w
                           jm
                        
                        ]
                      can be derived from the Euclidean dot product. Hence, the product of two vectors is given by the product of their length multiplied by the cosine of the angle α between them:


                     
                        
                           (7)
                           
                              sim
                              
                                 
                                    
                                       
                                          s
                                          i
                                       
                                       ,
                                       
                                          s
                                          j
                                       
                                    
                                 
                              
                              =
                              cos
                              α
                              =
                              
                                 
                                    (
                                    
                                       s
                                       i
                                    
                                    ,
                                    
                                       s
                                       j
                                    
                                    )
                                 
                                 
                                    |
                                    
                                       s
                                       i
                                    
                                    |
                                    ·
                                    |
                                    
                                       s
                                       j
                                    
                                    |
                                 
                              
                              =
                              
                                 
                                    
                                       ∑
                                       
                                          l
                                          =
                                          1
                                       
                                       m
                                    
                                    
                                       w
                                       il
                                    
                                    
                                       w
                                       jl
                                    
                                 
                                 
                                    
                                       
                                          
                                             ∑
                                             
                                                l
                                                =
                                                1
                                             
                                             m
                                          
                                          
                                             w
                                             il
                                             2
                                          
                                          ·
                                          
                                             ∑
                                             
                                                l
                                                =
                                                1
                                             
                                             m
                                          
                                          
                                             w
                                             jl
                                             2
                                          
                                       
                                    
                                 
                              
                              ,
                              
                              i
                              ,
                              j
                              =
                              1
                              ,
                              2
                              ,
                              …
                              ,
                              n
                           
                        
                     
                  

One can also define the normalized sentence similarity as:


                     
                        
                           (8)
                           
                              
                                 sim
                                 norm
                              
                              (
                              
                                 s
                                 i
                              
                              ,
                              
                                 s
                                 j
                              
                              )
                              =
                              
                                 
                                    sim
                                    (
                                    
                                       s
                                       i
                                    
                                    ,
                                    
                                       s
                                       j
                                    
                                    )
                                 
                                 
                                    
                                       ∑
                                       
                                          
                                             s
                                             k
                                          
                                          ∈
                                          D
                                          ∩
                                          k
                                          ≠
                                          i
                                       
                                    
                                    sim
                                    (
                                    
                                       s
                                       i
                                    
                                    ,
                                    
                                       s
                                       k
                                    
                                    )
                                 
                              
                           
                        
                     
                  


                     Example of the vector representation for documents and sentences. Here is a simplified example of the vector space retrieval model. Consider a very small collection F that consists in the following three sentences(documents): s
                     1: “new york times”; s
                     2: “new york post”; s
                     3: “los angeles times”.

Some terms appear in two sentences, some appear only in one sentence. The total number of sentences is N
                     =3. Therefore, the idf values for the terms are: “angeles”: log
                     2(3/1)=1.584; “los”: log
                     2(3/1)=1.584; “new” log
                     2(3/2)=0.584; “post”: log
                     2(3/1)=1.584; “times”: log
                     2(3/2)=0.584; “york”: log
                     2(3/2)=0.584. For all the sentences, we calculate the tf scores for all the terms in F. We assume the words in the vectors are ordered alphabetically. We also multiply the tf scores by the idf values of each term, obtaining the following matrix of documents-by-terms:


                     
                        
                           
                              
                              
                              
                              
                              
                              
                              
                              
                                 
                                    
                                    angeles
                                    los
                                    new
                                    post
                                    times
                                    york
                                 
                              
                              
                                 
                                    
                                       s
                                       1
                                    
                                    0
                                    0
                                    0.584
                                    0
                                    0.584
                                    0.584
                                 
                                 
                                    
                                       s
                                       2
                                    
                                    0
                                    0
                                    0.584
                                    1.584
                                    0
                                    0.584
                                 
                                 
                                    
                                       s
                                       3
                                    
                                    1.584
                                    1.584
                                    0
                                    0
                                    0.584
                                    0
                                 
                              
                           
                        
                     
                  

Given the following new sentence: s
                     4: “new new times”, we calculate the tf-idf vector, and compute the score of each sentence in F relative to this query, using the cosine similarity measure. When computing the tf-idf values for the query terms we divide the frequency by the maximum frequency (2) and multiply with the idf values.


                     
                        
                           
                              
                              
                              
                              
                              
                              
                              
                              
                                 
                                    
                                       s
                                       4
                                    
                                    0
                                    0
                                    (2/2)*0.584=0.584
                                    0
                                    (1/2)*0.584=0.292
                                    0
                                 
                              
                           
                        
                     
                  

We calculate the length of each sentence:


                     
                        
                           
                              
                                 Length
                                    
                                 of
                              
                                 
                              
                                 s
                                 1
                              
                              =
                              sqrt
                              (
                              0
                              .
                              
                                 584
                                 2
                              
                              +
                              0
                              .
                              
                                 584
                                 2
                              
                              +
                              0
                              .
                              
                                 584
                                 2
                              
                              )
                              =
                              1.011
                              …
                              
                                 Length
                                    
                                 of
                              
                                 
                              
                                 s
                                 4
                              
                              =
                              sqrt
                              (
                              0
                              .
                              
                                 584
                                 2
                              
                              +
                              0
                              .
                              
                                 292
                                 2
                              
                              )
                              =
                              0.652
                           
                        
                     
                  

Then the similarity values are: sim(s
                     1, s
                     4)=(0*0+0*0+0.584*0.584+0*0+0.584*0.292+0.584*0)/(1.011*0.652)=0.776…
                     sim(s
                     3, s
                     4)=(1.584*0+1.584*0+0*0.584+0*0+0.584*0.292+0*0)/(2.316*0.652)=0.112.

According to the similarity values, we connect sentences into sentence similarity graph.

The general summarization task is to select a set of sentences from a given data set where the extracted sentences can screen the overall understanding of the document set. Given the length limit to the summary, the generic summarization problem can be resolved by using the wHAA. In fact, for general summarization the algorithm wHAASum from Section 3.1 is used. Since there is no need for query incorporation, the labeling function is in its simplest possible form:


                        
                           
                              (9)
                              
                                 Gs
                                 =
                                 
                                    
                                       [
                                       
                                          sim
                                          norm
                                       
                                       (
                                       
                                          s
                                          i
                                       
                                       ,
                                       
                                          s
                                          j
                                       
                                       )
                                       ]
                                    
                                    
                                       m
                                       ×
                                       m
                                    
                                 
                              
                           
                        where sim
                        
                           norm
                        (.) is the normalized similarity function for computing the similarity between sentences s
                        
                           i
                         and s
                        
                           j
                        . Resulting 
                           Gs
                           ∈
                           
                              
                                 
                                    ℝ
                                 
                              
                              
                                 m
                                 ×
                                 m
                              
                           
                         denotes a general sentence similarity matrix where m is the total number of sentences in document set D.

Query-focused multi-document summarization is a special case of generic multi-document summarization. Given a query, the task is to produce a summary which can respond to the information required by the query. Different from generic summarization which needs to preserve a typical semantic essence of the original document(s), the query-focused summarization purposely demands the most typical (archetypal) summary biased toward an explicit query.

Given a document set and a query q, we define the labeling functions as
                           
                              (10)
                              
                                 Gs
                                 =
                                 
                                    
                                       [
                                       
                                          sim
                                          norm
                                       
                                       (
                                       
                                          s
                                          i
                                       
                                       ,
                                       
                                          s
                                          j
                                       
                                       )
                                       ]
                                    
                                    
                                       m
                                       ×
                                       m
                                    
                                 
                              
                           
                        
                        
                           
                              (11)
                              
                                 W
                                 =
                                 
                                    
                                       [
                                       
                                          sim
                                          norm
                                       
                                       (
                                       q
                                       ,
                                       
                                          s
                                          i
                                       
                                       ,
                                       )
                                       ]
                                    
                                    
                                       m
                                       ×
                                       m
                                    
                                 
                              
                           
                        where the first function represents the general information coverage, and the second function, the diagonal matrix W, represents the query-focused information coverage.

The multi-document update summarization was introduced by Document Understanding Conference (DUC) in 2007. It aims to produce a summary describing the majority of information content from a set of documents under the assumption that the user has already read a given set of earlier documents. This type of summarization has been proved extremely useful in tracing news stories: only new and update contents should be summarized if we already know something about the story.

Mostly, this summarization task is based on the following situation: Suppose that there is an open interest in a specific news topic and a need for tracking the related news as they emerge over time. In order to fulfill the information need of the users who are either overloaded with too much related news or are occasional readers of the given topic, the update summarization can provide summaries that only talk about what is new or different about this topic.

We formulate such problem as follows:

Given a query q (representing the users interested topic) and two sets of documents D
                        1 (previously read ones) and D
                        2 (new ones), the update summarization aims to generate a summary of D
                        2 related to the query q, given D
                        1.

First of all, the general summary of D
                        1, referred as to S
                        1, is generated by using the general summarization method. Then, the update summary of D
                        2 related to q, referred to as S
                        2 is generated using the following labeling functions:
                           
                              (12)
                              
                                 Us
                                 =
                                 
                                    
                                       [
                                       
                                          sim
                                          norm
                                       
                                       (
                                       
                                          s
                                          i
                                       
                                       ,
                                       
                                          s
                                          j
                                       
                                       )
                                       ]
                                    
                                    
                                       m
                                       ×
                                       m
                                    
                                 
                              
                           
                        
                        
                           
                              (13)
                              
                                 
                                    W
                                    1
                                 
                                 =
                                 
                                    
                                       [
                                       1
                                       −
                                       
                                          sim
                                          norm
                                       
                                       (
                                       
                                          S
                                          1
                                       
                                       ,
                                       
                                          s
                                          i
                                       
                                       )
                                       ]
                                    
                                    
                                       m
                                       ×
                                       m
                                    
                                 
                              
                           
                        
                        
                           
                              (14)
                              
                                 
                                    W
                                    2
                                 
                                 =
                                 
                                    
                                       [
                                       
                                          sim
                                          norm
                                       
                                       (
                                       q
                                       ,
                                       
                                          s
                                          i
                                       
                                       )
                                       ]
                                    
                                    
                                       m
                                       ×
                                       m
                                    
                                 
                              
                           
                        
                        
                           
                              (15)
                              
                                 W
                                 =
                                 
                                    W
                                    1
                                 
                                 +
                                 
                                    W
                                    2
                                 
                              
                           
                        Here, the first function represents the general sentence similarity graph. The second function, diagonal matrix W
                        1, represents the dissimilarity between summary S
                        1 and sentences from document set D
                        2. Eq. (14) represents the query to sentence similarity diagonal matrix. Eq. (15) denotes the way of combining weight matrices W
                        1 and W
                        2.

The main idea of S
                        2 should be different from the main idea of S
                        1. This is ensured by weighting the archetypal analysis with dissimilarity of sentences to the first summary while producing the second summary. Since the normalized similarity is a value in range [0, 1], its inverse is obtained by subtracting its value from 1. By penalizing the sentences similar to the first summary we reward the novel sentences and in this way attempt to model the update summarization problem. Also, S
                        2 should cover all the aspects of the document set D
                        2 as many as possible, which is again optimized using the general wHAA approach.

In this section we investigate one of the recent summarization tasks, first proposed by Wang et al. (2009) and referred to as Comparative Multi-document Document Summarization (CMDS).

CMDS is mainly about summarizing the diversities among related document groups. Formally, given a set of document groups, the comparative summarization is to produce condense summary expressing the differences of these documents by extracting the most distinct sentences in each document group. While the goal of the classic document summarization is to extract the central information usually by taking into account the similarities among document collections, on contrary the aim of the comparative summarization is to capture differences among them.

We model the comparative summarization as follows: Extract the summaries S
                        1, S
                        2, …, S
                        
                           N
                         from the given N groups of documents G
                        1, G
                        2, …, G
                        
                           N
                        . Extracted summaries should be as divergent as possible from one another on the topic level while still expressing central themes of corresponding groups.

We propose a different labeling function for the comparative summarization to generate the discriminant summary for each group of documents. The labeling function for the comparative summarization is defined as:


                        
                           
                              (16)
                              
                                 Cs
                                 =
                                 
                                    
                                       
                                          
                                             (
                                             i
                                             ,
                                             j
                                             )
                                             ∣
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  sim
                                                                  norm
                                                               
                                                               (
                                                               
                                                                  s
                                                                  i
                                                               
                                                               ,
                                                               
                                                                  s
                                                                  j
                                                               
                                                               )
                                                            
                                                            
                                                               if
                                                                  
                                                               G
                                                               (
                                                               
                                                                  s
                                                                  i
                                                               
                                                               )
                                                               =
                                                               G
                                                               (
                                                               
                                                                  s
                                                                  j
                                                               
                                                               )
                                                            
                                                         
                                                         
                                                            
                                                               −
                                                               
                                                                  sim
                                                                  norm
                                                               
                                                               (
                                                               
                                                                  s
                                                                  i
                                                               
                                                               ,
                                                               
                                                                  s
                                                                  j
                                                               
                                                               )
                                                            
                                                            
                                                               if
                                                                  
                                                               G
                                                               (
                                                               
                                                                  s
                                                                  i
                                                               
                                                               )
                                                               ≠
                                                               G
                                                               (
                                                               
                                                                  s
                                                                  j
                                                               
                                                               )
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       t
                                       ×
                                       t
                                    
                                 
                              
                           
                        where G(s
                        
                           i
                        ) is the document group containing sentence s
                        
                           i
                        , sim
                        
                           norm
                        (s
                        
                           i
                        , s
                        
                           j
                        ) is the normalized sentence similarity, and t
                        =
                        m
                        1
                        +
                        m
                        2
                        +⋯+
                        m
                        
                           N
                         is the total number of sentences from all groups of documents.

Sentences from the same group are weighted by normalized similarity, mainly in order to grasp the centrality of the same. Sentences from different groups are valued with an inverse normalized similarity. Since the normalized similarity is a value in range [0, 1], the inverse is obtained by subtracting it from 0. By rewarding intra-group sentence similarity and by penalizing the inter-group sentences we attempt to model the Comparative Extractive Multi-document Document Summarization (CMDS).

@&#EXPERIMENTS@&#

The experiments are conducted on the four summarization tasks to evaluate our summarization framework. Results show that it outperforms many existing approaches.

The DUC04 data set is used for the general (unrestricted) summarization task. As for the query-focused summarization, the DUC05 and the DUC06 data sets are used. The DUC07 and the TAC08 data set are used for the experiments on update summarization task. The compact view of the data sets can be found in Table 3
                     . For the comparative summarization, we use the subset of the DUC07 corpora to test our comparative summarization method.

In our experiments for evaluation of all but the comparative summarization we used Recall-Oriented Understudy for Gisting Evaluation (ROUGE) package (Lin, 2004), which compares various summary results from several summarization methods with summaries generated by humans. ROUGE is adopted by DUC as the official evaluation metric for text summarization. It has been shown that ROUGE is very effective for measuring document summarization. It measures how well a machine summary overlaps with human summaries using N-gram co-occurrence statistics, where an N-gram is a contiguous sequence of N words. Multiple ROUGE metrics are defined according to different N and different strategies, such as ROUGE-N, ROUGE-L, ROUGE-W, ROUGE-S, and ROUGE-SU. The ROUGE-N measure compares N-grams of two summaries, and counts the number of matches. This measure is computed by formula (Lin, 2004):


                     
                        
                           (17)
                           
                              ROUGE
                              -
                              N
                              =
                              
                                 
                                    
                                       ∑
                                       
                                          S
                                          ∈
                                          
                                             Summ
                                             ref
                                          
                                       
                                    
                                    
                                       ∑
                                       
                                          N
                                          -
                                          gram
                                          ∈
                                          S
                                       
                                    
                                    
                                       Count
                                       match
                                    
                                    (
                                    N
                                    -
                                    gram
                                    )
                                 
                                 
                                    
                                       ∑
                                       
                                          S
                                          ∈
                                          
                                             Summ
                                             ref
                                          
                                       
                                    
                                    
                                       ∑
                                       
                                          N
                                          -
                                          gram
                                          ∈
                                          S
                                       
                                    
                                    Count
                                    (
                                    N
                                    -
                                    gram
                                    )
                                 
                              
                           
                        
                     where N stands for the length of the N-gram, Count
                     
                        match
                     (N-gram) is the maximum number of N-grams co-occurring in candidate summary and the set of reference-summaries. Count(N-gram) is the number of N-grams in the reference summaries. Here, we report the mean value over all topics of the recall scores of ROUGE-1, ROUGE-2, and ROUGE-SU4 (skip-bigram plus unigram) (Lin and Hovy, 2003). For the comparative summarization, we provide some exemplar summaries produced by our summarization method. The comprehensive descriptions for each set of experiments and experimental results are given in the next subsections.

For the general summarization, we use DUC04 as the experimental data. We observe through the experiment that the summary result generated by our method is the best when the archetype number and level number are set as l
                        =
                        k
                        =3. Consequently, we set l
                        =
                        k
                        =3 when performing comparative experiments with other existing methods. We work with the following widely used or recent published methods for general summarization as the baseline systems to compare with our proposed method wHAASum: (1) BaseLine: the baseline method used in DUC2004; (2) Best-Human: the best human-summarizers performance (3) System-65: The best system-summarizer from DUC2004; (4) System-35: The second best system-summarizer from DUC2004; (5) AASum-w2: Archetypal analysis summarization system. Here we purposely compare with “w2” version of the AASum since it uses the same data modeling as wHAASum, which is the sentence similarity graph. (6) Lex-PageRank: by calculating the eigenvector centrality given the sentence to sentence similarity graph the method extracts the most significant sentences (7) Centroid: which selects sentences based on the positional value and the first sentence overlap and the distance to centroid value; (8) latent semantic analysis (LSA): by employing latent semantic analysis method extracts “semantically” important sentences; (9) non-negative matrix factorization (NMF): the method calculates NMF on the sentence-term matrix and based on factorization matrices extracts the high ranked sentences; (10) SNMF [27]: the method forms the sentence to sentence similarity matrix then clusters the sentences via the symmetric nonnegative matrix factorization, and finally extracts the sentences based on the clustering result.


                        Table 4
                         shows the ROUGE scores of different methods using DUC04. The higher ROUGE score indicates the better summarization performance. The number in parentheses in each table slot shows the ranking of each method on a specific data set. Based on the results, our method apparently performs better than the other peers and is even better than the DUC04 best team work.

We conduct our query-focused summarization experiments on DUC05 and DUC06 data sets since the main task of both was query-focused summarization. We compare our system with some effective, widely used and recently published systems: (1) Avg-Human: average human-summarizer performance on DUC2005/06; (2) Avg-DUC05/06: average system-summarizer performance on DUC2005/06; (3) System-15/24: The best system-summarizer from DUC2005/06; (4) System-4/12: The second best system-summarizer from DUC2005/06; (5) wAASum-w2: weighted archetypal analysis summarization system. We examine only “w2” version of the wAASum for the same reason as in general summarization task; (6) non-negative matrix factorization (NMF); (7) latent semantic analysis (LSA); (8) PLSA: employs the probabilistic latent semantic analysis approach to model documents as mixtures of topics; (9) biased-LexRank: the method first constructs a sentence connectivity graph based on the cosine similarity and then selects important sentences biased toward the given query based on the concept of eigenvector centrality; (10) SNMF: symmetric non-negative matrix factorization;

As indicated in Lee et al. (2009), Wang et al. (2008) and Canhasi and Kononenko (2014b), LSA, NMF, SNMF and wAASum-w2 are few competing matrix factorization techniques. From Tables 5
                         and 6
                         we can see that NMF shows better performance than LSA. This is in consistency with results reported in Lee et al. (2009) and it can be mainly contributed to the property of NMF to select more meaningful sentences by using more intuitively interpretable semantic features and by better grasping the innate structure of documents. From the same tables we see that hierarchical version of wHAA performs better than its flat counterpart, SNMF, NMF and LSA. This is due to the weighted hierarchical archetypal analysis, which can detect the hierarchical archetypal structure, and hence cluster and rank sentences into sub-archetypes more effectively than above mentioned approaches. Since LSA, NMF and SNMF are matrix factorization methods, the improvement of wHAASum compared with them can be mainly attributed to wHAA's ability to combine the clustering and the matrix factorization. The advantages of our approach are clearly demonstrated in Tables 5 and 6. It produces very competitive results, which apparently outperforms many of the methods in both years. More important, it is the best automatic system in DUC2006, and ranks among the bests in DUC2005. Notice also that all the results of wAASum are produced based on a simple similarity measure.

For update summarization we used the DUC07 and TAC08 update task datasets. The DUC07 update task goal is to produce brief (100 words long) multi-document update summaries of newswire articles supposing that the user has already read a set of earlier articles. Each update summary should update the reader of new information about a particular topic. Given a DUC topic and its 3 document clusters: A, B and C, the task is to create from the documents three short summaries such as: (1) A summary of documents in cluster A. (2) An update summary of documents in B, under the assumption that the reader has already read documents in A. (3) An update summary of documents in C, under the assumption that the reader has already read documents in A and B. Within a topic, the document clusters must be processed in chronological order; i.e. we cannot look at documents in cluster B or C when generating the summary for cluster A, and we cannot look at the documents in cluster C when generating the summary for cluster B. However, the documents within a cluster can be processed in any order.

The main task of TAC08 summarization track is composed of 48 topics and 20 news wire articles for each topic. Twenty articles are grouped into two groups. The update summarization task is to produce two summaries, using the initial summary (TAC08A), which is the standard query focused summarization, and the update summary (TAC08B) under the assumption that the reader has already read the first 10 documents.


                        Table 7
                         shows the comparative experimental results on the update summarization. In Table 7, “DUC/TAC Best” and “DUC/TAC Median” represent the best and median results from the participants of the DUC2007 and TAC2008 summarization tracks (Dang, 2007; Dang and Owczarzak, 2008).

As seen from the results, the ROUGE scores of our method are higher than the median results. Superior results of some teams are usually due their employment of the advanced natural language processing techniques. Those techniques include shallow NLP methods like lemmatization, part-of-speech tagging, and Named Entity recognition and a degree of deeper linguistic processing methods such as the clustering noun-phrases, comparing clauses instead of sentences, using graphs representing Named Entities connected by dependency relations, syntactic parsing, and sentence compression combined with syntactic (Dang, 2007; Dang and Owczarzak, 2008). Although we can also utilize this kind of techniques in our solution, our goal here is to demonstrate the effectiveness of formalizing the update summarization problem using the weighted hierarchical archetypal analysis and therefore we do not use any advanced NLP technique. Experimental results demonstrate that our simple update summarization method merely based on the wHAA can result in competitive performance for the update summarization.

We used randomly selected six clusters of documents from the DUC07 corpora to generate comparative summaries using the wHAASum summarization method. The data set contains six clusters as follows: 1. Steps toward introduction of the Euro; 2. Burma government change 1988; 3. Basque separatist movement 1996–2000. 4. Unemployment in France in the 1990s; 5. Obesity in the United States and possible causes for US obesity; 6. After “Seinfeld” TV series;

Looking at the results by our wHAASum sentence selection method in Table 8
                        , each of the sentences represents one cluster respectively and summarizes well specific topics of each cluster. In Table 8, we also highlight some keywords representing the unique features of each topic. Note that the sentence extracted by wHAASum for each topic are not just discriminative but they also present the essence of the topic. For example, the summary of topic 3 defines the acronym ETA and clearly explains their demands. Another complex example is the summary of topic 6 where we are interested in what became of the cast and others related to the “Seinfeld” TV series after it ended. Again, the selected summary sentence answers well the concerns given by the query and at the same time it is completely dissimilar to the summaries of other topics. Note also how successfully the summary No.1 defines and explains the issues of introducing the Euro.

For the qualitative analysis we show that sub-topics inferred by wHAA do correspond to sub aspects of a given document set. We compare the quality of sub-topics obtained by wHAA with topics discovered by the standard wAA approach. To perform qualitative experiments we used a subset of document sets from DUC05 and DUC06. Since our primary purpose in this experiment was to compare the non-hierarchical to hierarchical version of wAA, we manually categorized 9 sets of documents into 3 manually formed super topics. The initial sub-topics and their labels are obtained form the queries matched to each data set in DUC collection. Those sets are carefully chosen to form a following hierarchical structure of topics:
                           
                              1.
                              Automobiles
                                    
                                       i.
                                       American automobile manufacturers strike; 25 documents; DUC06; D0636I

Automobile safety; 25 documents; DUC06; D0608H

Electric automobiles; 28 documents; DUC05; d385g

Brain diseases
                                    
                                       i.
                                       ADHD; DUC06; 25 documents; D0628A

Autism; DUC06; 25 documents; D0612C

OCD; DUC06; 25 documents; D0648C

Viruses
                                    
                                       i.
                                       Malaria virus; DUC06; 25 documents; D0618I

West Nile virus; DUC06; 25 documents; D0622D

Computer viruses; DUC06; 25 documents; D0629B

We run wAA and wHAA on the same hierarchical structure of document sets combined into single document set. In doing so we measure how well can wAA and wHAA model the document sets with hidden hierarchical structure. For wAA we selected archetype number to be the number of super topics, which is three, since in real world settings one cannot know in advance the number and the implicit structure of embedded sub-topics. For wHAA we set the number of archetypes to three and the number of levels to two. In this way our comparison was as fair to wAA as possible. Top words for the discovered sub-topics and the global topics of wAA and wHAA models are presented in Tables 9
                         and 10
                        , one topic per line, along with selected topics from our models. We manually assigned labels to coherent super topics to reflect our interpretation of their meaning. Note that in Table 9 the topic labels for first two archetypes and the top words from them seem to correspond well to each other. The third archetype even that corresponds well to topic labeled as “Viruses”, it still contains many words from other different topic. Table 10, in similar way, presents the results of wHAA used on same dataset as in Table 9. In Table 10 along the original sub-topic labels we present the super topic labels and top words from each sub-topic. Fortunately, top words in each line correspond very well to each embedded subtopic and its super topic. The most of the results are far better than those presented in previous table.

Although parameter selection does not play a central role yet it has an important function in our research. The main idea of parameter selection is to choose a subset of relevant parameters for building robust archetypal analysis based summarization models. In our current research we use standard ad hoc parameter selection techniques. Since the most important parameter in our methods is the number of archetypes, we have mainly projected the general parameter selection problem to choosing the best value for the number of archetypes to be computed. For the plain and the weighted archetypal analysis we used a simple approach for choosing the value of the number of archetypes: we run the algorithm for different numbers of the archetypes where the selection criteria were the maximization of the summary evaluation outcome. In this way we found some values for which we believe are the most adequate. Similarly, in order to define the two most important parameters in hierarchical weighted archetypal analysis, we followed the same direction of thoughts, where we decided on some ad hoc values for the archetype number and the number of levels on decomposition tree. Yet, at the end, one can still argue our decisions and arguments, hence we believe that a deeper, more comprehensive, and accurate analysis on parameter selection and tuning with even more detailed experiments should be done as the part of future work.

In this subsection we present the complexity analysis of the proposed summarization methods. We first present the theoretical time and space complexity for the preprocessing step. Then we continue with the time complexity analysis for archetypal analysis itself. And finally, we conclude the subsection with theoretical and empirical complexity analysis of the summarization methods proposed in the earlier sections.

As it was presented earlier, in order to obtain the sentences similarity graph one needs to compute the similarity values for all possible pairs of sentences in order to connect them in the sentence similarity graph. We used the vector space model to represent sentences from given documents. The vector space model is an algebraic model for representing sentences as vectors of terms. Computing the similarity of sentences then reduces to computing the cosine similarity. The cosine similarity is a measure of similarity between two vectors of an inner product space that measures the cosine of the angle between them.

Assuming that multiplication and addition are constant-time operations, the time complexity of computing the cosine similarity where m is the biggest number of terms is therefore O(m)+
                           O(m)=
                           O(m). The only auxiliary space we require during the computation is to hold the ‘partial dot-product so far’ and the last product computed. Assuming we can hold both values in constant-space, the space complexity is therefore O(1)+
                           O(1)=
                           O(1). But since we need to compute the sentence similarity for every pair of sentences then the time and space complexity of generating the sentence similarity graph becomes O(n(n
                           −1)/2), here n is the number of sentences. Even though a quadratic complexity can be seen as problematic, given that typical number of sentences in summarization is not too big, then the overall time consumption is on an acceptable level. In order to give better grips on the time complexities in Table 11
                            we give some empirical measurements.

The convex hull or envelope of a data matrix X is the minimal convex set containing X. While the problem of finding the convex hull is solvable in linear time (i.e. O(n)) (McCallum and Avis, 1979) the size of the convex set increases dramatically with the dimensionality of the data. The expected size of the convex set for n points in general position in m dimensional space grows with dimension as O(log(n
                           (m−1)) (Dwyer, 1988). In the paper of Cutler and Breiman (Cutler and Breiman, 1994) the model was estimated by non-negative least squares such that the linear constraints were enforced by introducing quadratic penalty terms in the alternating updates of P and Q. Alternatively, the standard non-negative quadratic programming solvers with linear constraints can be invoked (Bauckhage and Thurau, 2009) for each alternating sub problem solving P for fixed Q and vice versa. We found however, that the following projected gradient method inspired by the projected gradient method for NMF (Lin, 2007) using the normalization invariance approach worked efficiently in practice. By using the later approach, the overall computational complexity is the same for AA as for NMF (Lin, 2007). When only considering T candidate points used to define the archetypes as proposed in Bauckhage and Thurau (Bauckhage and Thurau, 2009) this latter complexity can be further reduced to O(zmn), where z is the number of archetypes, m the dimensionality of the feature space and n the number of observations.

Given that AA based document summarization system consists of preprocessing and processing steps then the overall time complexity can be found by summing the latter two complexities, i.e: O(O(n(n
                        −1)/2)+
                        O(zmn)), where n is the number of sentences, m is the number of terms, and z is the number of archetypes.

Although the overall time complexity seems to be quadratic, since the number of archetypes z is usually very low (maximum 16 in our case) and the number of sentences and therefore number of terms are bounded to the length of input documents the overall time spent for producing the summary is time affordable. In order to give a better gist of the time complexity we report the elapsed time in producing the summary for different document(s) lengths in Table 11. The time elapsed in computing the summaries are measured on processor with following specifications: Intel(R) Core(TM) i5 CPU M 450 @ 2.45GHz with 4GB RAM memory. The first two columns (document(s) length in KB, and the total number of sentences) represent the input values, while the rest three columns (pre-processing, archetypal analysis based sentence selection and total time spent in seconds) are the measured times. Overall quadratic time complexity can be noticed also in the experimental results, but one should bear in mind that this methods are intended to be used in summarizing the news articles. The document lengths in real situations are not expected to be more than 100KB.

In this paper, we present a novel summarization framework based on weighted hierarchical archetypal analysis, namely wHAASum. We used the weighted hierarchical archetypal analysis to select “the best of the best” summary sentences by producing the hierarchical decomposition in the form of a tree. All known summarization tasks, including general, query-focused, updated, comparative summarization can be treated by this framework. The empirical results show that this framework outperforms the other related methods in generic summarization and that it is competitive in other summarization tasks. The skill to address these summarization problems is build on weighted hierarchical archetypal analysis problem itself, and on various input matrix modeling functions for corresponding summarization tasks.

The proposed algorithm is evaluated using several automatic summarization methods (ROUGE-1,2,SU) on general (using DUC04), query focused (using DUC05/06), and update summarization (using DUC07, TAC08 datasets). The results show that the proposed method outperforms the previously used non-hierarchical method (on generic and query-focused) summarization. It also outperforms most of the other methods used by the systems participating in document summarization competitions. For update summarization, the proposed method outperformed median score of the systems participating in the competition. Superior results of other teams usually are due their usage of the advanced natural language processing techniques, such as deep structure parsing, semantic labeling and entity name recognition. For the comparative summarization task we do not present ROUGE results and baselines comparisons mainly because there is currently no dataset/methodology available to carry out a quantitative evaluation.

We believe that in the future, the performance of wHAASum would possibly be further improved. There are few limitations and weakness of presented methods such as: (1) AA based sentence selection methods tend to extract fewer number of longer sentences which is contrary to the human summarizers who tend to produce larger number of shorter sentences; (2) the elementary data units in our methods are sentences, working with sub-sentential entities could potentially increase the quality of produced summaries.

Our future work resides still in the area of summarization, but we would like to widen the area of wHAASums usability by exploring the possibilities of utilizing it in other fields such as opinion and biomedical summarization.

@&#REFERENCES@&#

