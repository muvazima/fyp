@&#MAIN-TITLE@&#Temporal mapping of surveillance video for indexing and summarization

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A comprehensive scheme of temporal profile of surveillance video is proposed.


                        
                        
                           
                           It provides the sampling selection, analyzes properties of shape and motion of the temporal profile, develop the algorithms to detect dynamic targets, and measure their motion direction.


                        
                        
                           
                           Finally, the visualization of targets with multiple temporal slices with transparency is provided.


                        
                        
                           
                           Further, robust object detection based on background updating, and real time data transmission of is given.


                        
                        
                           
                           A comparison of our method with other video indexing methods is also given.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Surveillance video

Temporal profile

Image projection

Target counting

Crowd analysis

Video summarization

Video indexing

Video retrieval

Video compression

@&#ABSTRACT@&#


               
               
                  This work converts the surveillance video to a temporal domain image called temporal profile that is scrollable and scalable for quick searching of long surveillance video by human operators. Such a profile is sampled with linear pixel lines located at critical locations in the video frames. It has precise time stamp on the target passing events through those locations in the field of view, shows target shapes for identification, and facilitates the target search in long videos. In this paper, we first study the projection and shape properties of dynamic scenes in the temporal profile so as to set sampling lines. Then, we design methods to capture target motion and preserve target shapes for target recognition in the temporal profile. It also provides the uniformed resolution of large crowds passing through so that it is powerful in target counting and flow measuring. We also align multiple sampling lines to visualize the spatial information missed in a single line temporal profile. Finally, we achieve real time adaptive background removal and robust target extraction to ensure long-term surveillance. Compared to the original video or the shortened video, this temporal profile reduced data by one dimension while keeping the majority of information for further video investigation. As an intermediate indexing image, the profile image can be transmitted via network much faster than video for online video searching task by multiple operators. Because the temporal profile can abstract passing targets with efficient computation, an even more compact digest of the surveillance video can be created.
               
            

@&#INTRODUCTION@&#

A video shows location (where), time (when), people/objects (who/what), and actions/events (how). A surveillance video, however, has a relatively fixed area to monitor and simple actions (passing of object and people) as compared to the entertainment and sports videos. The time is much longer for identifying who and what passed through in the field of view. It is a major task to screen surveillance videos captured day and night to find target objects and persons. Surveillance cameras have been located everywhere for monitoring targets and events, and video data are constantly collected day and night. Searching targets in large video volumes from distributed cameras is not a trivial task. An automatic scanning of candidates in videos and the confirmation by human operators are eventually necessary.

This paper generates a profile image to index the entire surveillance videos emphasizing when targets passed (time) critical locations. The indexing means that an examiner of surveillance video can click at the identified targets or suspects in such a profile image for further investigation in the video frames. The profile image can also be used efficiently for long term survey and direct counting of large group or passing flow. In this paper, we make efforts to preserve the target shape in the temporal profile of video for target identification (who/what). Detecting and screening targets and their motion in the profiles (when/how) are easier and faster than watching the video, and so as to visualizing the target moving directions and positions (where) for understanding events. Figs. 1
                         and 2
                         show the preliminary framework of obtaining temporal image from a video sequence [25].

@&#RELATED WORKS@&#

Automatic identification of pedestrians and cars has been one of the hottest topic in video surveillance [12,32]. Background and foreground separation have been extensively explored [10]. Among detected foreground regions, pedestrian detection is carried out by using shape [36] and motion [34,47] information based on supervised learning techniques [33,35]. Motion tracking further extracts trajectories of targets for event understanding [37]. However, these methods have not achieved close to perfect results; many of them have accuracy around 80% depending on the training data. Therefore, current video screening still requires involvement of human operators. In addition, all of the methods require intensive computation that is much longer than playing video in real time.

To reduce the time of searching video exhaustively, there have been many works categorized as temporal condensing of frames in spatial domain in order to shorten the video lengths. One is to remove video segments without foreground object events [39] or even compress different temporal events together [[26]–
[28]] into short video clips. Another way along the same line is to condense events in different frames into key frames as index by mosaicing foreground patterns [41]. Although these compressions of frames to the same spatial domain have reduced data sizes, they lose temporal information completely either across clips or within clips. The visual space may also get cluttered quickly by crowded scenes over prolonged time periods such that non-uniformed clip selection and sampling are required to be done offline based on overall target density and event complexity in the video [40].

An alternative way to index video is to focus on temporal domain 
                        [25,29] at critical positions in the field of view where the targets of interest will pass through, because the surveillance cameras always watch at fixed locations (where is known). Similar as the linear CCD camera in the early stage of digital imaging [3,4,6,7,25], we set a fixed pixel line in the video frame for collecting temporal data as used in traffic monitoring [11,25] with the camera mounted at a high position. In many applications such as counting passing people in a park, station, store or exhibition site [21], alarming invasion of a critical facility or cross of border, and monitoring a traffic flow, the target moving directions and speeds are already obvious. We can thus focus on counting targets and recording shapes. If penetrating objects are mainly pursuing a translational movement through some sampling line, the resulting 2D temporal slice image contains information on time, shape and identities [3–6,23,25] but requires less redundant processing than normal video [12,15]. It is also more intuitive to examine the history of passing entities than a video and discrete synopsis clips or key frames, because it is temporally continuous and extendible to long periods, which serves the purpose to index temporal data in surveillance videos.

The third method is combining spatial-temporal presentations by copying the moving targets to shifted positions along the time axis in the video volume of a clip [44–46], after the segmentation process to separate foreground from background as synopsis. The display of video volume allows viewer's interaction to examine the targets and actions from different angles. This method is more powerful but more costly in visualization, searching and retrieval of videos. A comprehensive comparison of the three methods is given in the discussion section later on.

In this work, we introduce an intermediate image representation named temporal profile created from the surveillance video for saving computation cost and data size. The resulting data have one dimension as time; it corresponds to watching the video volume from right side as Fig. 3
                         depicted. We provide a much faster approach than playing video for finding when some targets (who/what) pass through specified critical locations (where), since it is the primary task for watching surveillance video. We also enhance the visualization on how these targets passed through according to their motion directions and velocities, and even more detailed positions in the monitored field of view.

We focus on the sensing modes of a temporal profile in the aspects of projection and visualization for counting target flows through spatial channels or guard lines. We propose how to set pixel sampling lines in video and analyze what effects or information can be observed in the temporal profile. Multiple temporal slices are further fused properly to show the spatial information missed in a single temporal slice. Several critical problems such as the sampling line alignment for capturing shapes, robust moving object extraction and motion direction estimation are solved. Practical problems such as background updating and data transmission over the wired/wireless network are also addressed.

In Section 2, we explore the acquisition of temporal slice from a surveillance camera by setting the viewing angle, sampling line, and sampling rate to obtain quality video profile. We investigate the essential properties of projected shape in the profile image for target identification.

In Section 3, the crowd counting problem will be addressed by using the temporal slices and extracting targets through critical positions. We show a powerful function of the temporal slice for long term target flow counting particularly on large crowds, which can only be achieved so far with much heavier matching and tracking methods on redundant data between frames. Our motion direction estimation of individual targets is carried out within a small data block around the sampling line to show target activities in groups.


                        Section 4 addresses the visualization of spatial information and flow motion in the multi-line temporal profiles. We blend targets at a critical location with different transparencies in case of sparse passing of targets. This is further extended to all-position sampling over the entire field of view by integrating the targets according to their depth cue, which brings in more spatial information to the temporal profile. Background information is further embedded into the results for understanding the global structure and layout of environments that targets pass through.


                        Section 5 extends our method to camera panning videos, which is from a more general camera motion in surveillance. The global motion is condensed in the field of view and the camera rotation is detected at every moment in order to perform the temporal profiling. The result is stable for sparse targets with unknown the camera rotation parameter, and is certainly precise if the camera control parameter will be provided.

In
Section 6 we deal with long-term surveillance videos robust to outdoor illumination changes. Adaptive background removal is achieved so that the surveillance systems can work day and night continuously. A median filtering algorithm with constant complexity is developed for real time processing. We also discuss the transmission of such profile data via network in real time, which makes it possible to collect the surveillance data over a large distributed camera network.

Finally, a general discussion of temporal profiling is given to compare with other video index and summary methodologies across different aspects. Although this temporal profiling is incapable of capturing non-transitive complex movements of targets, and its image quality cannot compete with that in a normal video frame, it is still a good choice of video indexing on pedestrians and fast screening of traffic on roads and construction sites, invasion detection at critical facilities and borders, crowd counting in stores, airports, and event sites, because of its non-redundant data size and low computation cost. The property in profiling passing flows significantly reduces the computation as compared to the processing of 3D video volume such that it can be deployed widely for the purpose of security, disaster preparation, environment assessment, etc.

Cameras used for the surveillance purpose monitor large field of views for prolonged periods of time to monitor dynamic objects. Set at relatively high positions, they usually monitor gateways, roads, rivers, border, or scenes with large crowds. Most of the dynamic targets have a directional motion (image velocity 
                           v≠
                         0), at least when they enter or leave the field of view. The exceptional motion without directional movement are in the situations of object 3D rotations, human articulate motion, swaying and waving, tree or flag waving, and translation motion towards the camera (tele-lens directing exactly along a hallway, highway, tunnel, etc.). It is obvious that surveillance videos are aimed at monitoring certain “critical” locations in the field of view, where dynamic objects known as foreground are passing. The background portion in the field of view is unchanged throughout the entire video and is not in our interest to monitor.
                     

We can form a plane of sight 
                        
                           N
                         in the 3D space by fixing a sampling line, 
                           l
                        
                        
                           s
                        , in the video frame with the camera focus. We create an image I(t,l) named the temporal slice by sampling pixels on this line continuously and stacking the sampled 1D arrays consecutively, where l ∈ [0, h] is the coordinate on 
                           ls
                           
                         and t is the time or image frame [3,8,14]. As targets move across the plane of sight formed by the sampling line, they will leave shapes or traces in the temporal slice, because different parts of the object are exposed to the sampling line in order. Plane N must be set to capture moving flow in order for the temporal slice to contain meaningful shapes, i.e., 
                           l
                        
                        
                           s
                         should intersect the optical flow of target motion to record target shape segments consecutively; otherwise targets will leave traces rather than recognizable shapes. If objects are moving along a confined path in the space such as a road, path, stairway or a river, the translational motion will be clear and hence it is easy to capture shapes into the temporal slice.

Since the sampling line is projecting the video volume onto a plane, the static background pixels will appear as parallel patterns dragging along the time axis as the background colors do not change over time. Fig. 2 gives an example to record people passing through a yard. The profile of people passing through the yard is clearly projected in the temporal slice and it can be stored and used as an index to the original frame in the video. Each column of the temporal slice I(t,l) will refer to a single frame in the video and the exact passing time of targets can be found and explored further in the video frame. This temporal slice is a compact representation of video. Although it has shortcomings in representing perfect shapes, it has remarkably reduced data size, which will facilitate real time data transmission and processing for mobile devices over a wireless network.


                        Fig. 4 shows the temporal slices obtained from sample clips in the CUHK crowd dataset [38]. Some video has heavy compression, but still shows meaningful target shapes. The motion of dynamic targets is mainly horizontal and vertical. Fig. 5 shows the temporal slice from our own videos, which are sufficient for brief target identification based on color, shape, height, etc.

As we are sampling a line on a pixel grid, setting the line at a diagonal or arbitrary angle will require interpolation on the grid to obtain accurate color information. Hence a sampling line, set on purely horizontal or vertical direction, will have better quality than a line set otherwise [2,7,17]. However, such a line may not orthogonal to the translational flow direction which may introduce other deformation into the temporal slice. In this section we will study sampling characteristics of the temporal slice in order to take advantage of it for various applications.

In order to generate the best possible shapes in the temporal slice, we study the field of view of the camera to determine the poses of objects in the 3D space. We examine three sets of orthogonal vectors in the 3D space that align with object poses in the video frame I(x,y,t). Often times when humans or vehicles are moving along a pathway, they have an obvious translation direction 
                           V
                         on the ground. There is also a principal pose direction 
                           L
                         associated with target objects in the 3D space. For example, 
                           L
                         can be the standing direction for humans, or can be a horizontal direction on a vehicle orthogonal to its heading or path direction. In general, 
                           V
                        ⊥
                           L
                         as shown in Fig. 6. Their projections in the image are 
                           v=
                        (u,v) and 
                           l
                         respectively. The secondary pose direction denoted by 
                           D
                         is orthogonal to 
                           V
                         and 
                           L
                         and is named depth direction. Fig. 6 shows two examples of principal pose directions either horizontal or vertical in the 3D space associated with the secondary pose directions 
                           D
                        .

We propose to sample a line along one of principal pose directions, i.e., the plane of sight 
                           N
                         will pass through a line in 
                           L
                        . Under such setting condition, sampling line 
                           ls
                           
                         aligns with 
                           l
                        . Further, we select the 
                           l
                         that is more orthogonal to velocity 
                           v
                         in the video frame. Consecutive lines in the video volume I(x,y,t) form a temporal slice I(t,l) as

                           
                              (1)
                              
                                 
                                    I
                                    
                                       (
                                       
                                          t
                                          ,
                                          l
                                       
                                       )
                                    
                                    =
                                    I
                                    
                                       
                                          (
                                          
                                             x
                                             ,
                                             y
                                             ,
                                             t
                                          
                                          )
                                       
                                       
                                          (
                                          x
                                          ,
                                          y
                                          )
                                          ∈
                                          l
                                          s
                                       
                                    
                                    =
                                    
                                       {
                                       I
                                       
                                          (
                                          
                                             x
                                             ,
                                             y
                                             ,
                                             t
                                          
                                          )
                                       
                                       |
                                       
                                          (
                                          
                                             x
                                             ,
                                             y
                                          
                                          )
                                       
                                       ∈
                                       
                                          l
                                          s
                                       
                                       }
                                    
                                 
                              
                           
                        where l is the coordinate on line 
                           ls
                           
                        . As long as 
                           ls
                           
                         is not set along local motion direction 
                           v
                         in the frame, the foreground will leave shapes in the temporal slice; otherwise, flow traces will be captured. Our strategy guarantees flow 
                           V
                         penetrating the plane of sight 
                           N
                        . Moreover, aligning the sampling line with a principal pose direction 
                           L
                         provides shapes very similar to those obtained by a perspective projection. This means that the majority of target's pixels will be sampled at the same time instance rather than with delays, as shown in Fig. 6(b). 
                           L
                         can also be horizontal, as in Fig. 6(a), to monitor a road or river in an overlook view during the targets moving.
                        
                     

Many surveillance videos monitor areas from a vantage point located at above a pathway or point of interest. Parallel lines in 3D space may pass through a vanishing point after their perspective projection to the video frames. The projection of the vertical pose l passes through another vanishing point far below the frame. If a purely vertical sampling line is set on the grid as x=a, the shape of objects, normally not be purely-vertical in the frame, will be slightly skewed along t axis after being captured in the temporal slice. This is due to object parts being scanned asynchronously. We avoid this shape distortion by following the above strategy. By observing several poles, columns or building rims in the FOV, the vanishing point QL
                         can be computed at the crossing of these extended lines. The sampling line is thus required to pass through the vanishing point QL
                        , crossing the path of moving objects. In case of a close-to-vertical motion of objects in the video, the principal pose will be chosen horizontally to be more orthogonal to v through another vanishing point.

In general, the moving, principal, and depth directions are projected to the camera frame in perspective projection as depicted in the left parts of Fig. 6. 3D lines parallel to each direction of V, L and D have their image projections v, l, and d converging to corresponding vanishing points denoted by QV, QL
                        , and QD
                        , respectively, which may be out of the image frame and even at infinity. Our derived sampling line ls
                         (or its extension) is through vanishing point QL
                         in the image plane, along with a critical position to intersect a path or channel of target flow. Therefore, the sampling line ls
                         will scan line set L in order when objects pass through. As shown in the left column of Fig. 6, if two principal directions are possible to select on a vehicle in orthogonal to flow V, either horizontally as in Fig. 6(a) or vertically as in Fig. 6(b), we choose L whose projected l is more orthogonal to the projected motion v in the image.

We sample the line at each frame to obtain an array of pixels, and the arrays from consecutive frames are connected along time axis. For simplicity, 
                           ls
                           
                         can be parameterized by y (or x) only so that the temporal slice becomes I(t,y) (or I(t,x)) with y mapped from l. For a video volume, a temporal slice is viewed from side (or top) of the volume showing accurate temporal information horizontally. Under this assumption, the shape characteristics in the temporal slice image are summarized as follows:

                           
                              1.
                              
                                 Speed and distance adjusted aspect ratio: The length of an object along the time axis is inversely proportional to object image velocity |
                                    v|. The vertical scale of the object depends on its distance from the camera. The faster the speed, the narrower the yielded shape is. If |
                                    V
                                 | = 0 as for the background pixels, the projected pixels will stretch horizontally [22].


                                 Side face preserved: The lines parallel to 
                                    V
                                  on moving objects are imaged as horizontal lines in I(t,y), while the line set parallel to 
                                    L
                                  is projected to vertical lines 
                                    l
                                  perpendicular to the t axis in I(t,y), because each line 
                                    l
                                  passes 
                                    l
                                 
                                 
                                    s
                                  instantly. The side face then preserves its shape on linearity in the temporal slice possible for target recognition by humans.


                                 Non-linearity mapping in depth: Line set parallel to 
                                    D
                                  direction on objects is projected to a set of hyperbolas 
                                    d
                                  in the temporal slice if 
                                    v
                                  is constant. Moreover, these hyperbolas approach to a horizontal asymptotic line, y = yq
                                 , in the temporal slice. If we connect vanishing points QV
                                  and QD
                                  in Fig. 6, the linking line intersects line 
                                    ls
                                    
                                  at a point q. Its coordinate yq
                                  on 
                                    ls
                                    
                                  determines the position of the asymptotic line in the temporal slice. This curved effect in the temporal slice is not significantly different from the line set 
                                    d
                                  in the perspective image because the line segments are short, if 
                                    ls
                                    
                                 is selected along the major principal direction with long lines 
                                    L
                                 .


                                 Lacking motion direction: A single sampling line does not acquire the direction of object penetrating in the temporal slice. All the forward moving objects have their head facing left in the temporal slice. We can only heuristically infer the direction from some face features associated with objects, as shown in Fig. 7.
                                 
                              

Properties 1 and 2 can be derived easily. After setting the sampling line, the change of sampling rate alters the object length along the time axis in the temporal slice. A low sampling rate may obtain insufficient object resolution horizontally and thus is difficult for visualization and object recognition. Due to the time necessary for digital cameras to digitize and accumulate irradiance, the sampling rate has an upper limit. Depending on the hardware used, sampling rate ranges from 60 to several thousand lines per second. Assume that the maximum sampling rate available is r lines per second, and the image velocity 
                           v
                         is measured by pixel/frame. The object length T in the captured temporal slice image is then

                           
                              (2)
                              
                                 
                                    T
                                    =
                                    
                                       r
                                       
                                          60
                                          v
                                       
                                    
                                    l
                                    e
                                    n
                                    g
                                    t
                                    h
                                 
                              
                           
                        where length is the object length projected in the frame, and the video has 60 frames per second if we capture it in interlaced format. According to property (2), the shapes on the side faces are briefly preserved in resulting I(t,y), except the aspect ratio changed by property (1). The proof of Property (3) is omitted here. The curves may generate some shapes stretching in depth (d curves in Fig. 6), which may deform the shape as shown in Fig. 8
                        . We can apply a local skew operation to partially rectify the shape for easy identification.

As visible in Fig. 8, all shapes in the temporal slice will face the same direction as they cross the sampling line first while moving forward. Property (4) mentions that as the temporal aspect of video is emphasized, information such as direction is omitted. In Fig. 7, the side surfaces of a car all look the same, except the front and back surfaces visible in the depth direction. Because the lighting direction in this case is from right in the 3D space, the shadows are casted at the left of the vehicle regardless of the vehicle moving directions. The vehicle moving direction can be inferred in the motion slices from the relation between shadow and vehicle only. Similarly, depending on the relation of visible front or back surface and side surface, vehicle direction can be inferred. In general, the moving direction is not preserved in a single temporal slice if no object model or illumination is available. We will provide a solution to reveal the object motion direction in later sections.

In monitoring a wide road or a river from a high position for counting people, moving vehicles or floating boats, the principal direction can be set horizontal to avoid occlusion of objects viewed from side. After identifying a flow, we examine object sizes and aspect in the image to determine the principal direction 
                           L
                        . A sampling line can be set manually over a path to capture target flow. The flexibility in selecting the sampling line in the frame allows us to set a camera freely far away from the monitoring location. The line can even monitor a narrow area where objects do not show their entire shape as in Fig. 9.
                        
                     

Counting target flow is difficult for large groups of moving targets as shown in Fig. 10
                        , which needs to correspond targets over frames to avoid duplicated counting through frames. A lot of dispute arises regarding how many people exactly have attended an event. Non-redundant counting is required in such a circumstance based on tracking targets [37]. The temporal slice, however, has the advantages in achieving this task: the targets are non-redundant in the temporal slice when passing through a path cut by a sampling line. The number counted in the temporal slice is the exact number of passing targets. Multiple cameras can be located at critical entrances and paths for temporal slices.

The color, shape and structure of the objects revealed in the temporal slice easily distinguish them from the background. Hence, it is visually easy to detect events and extract dynamic shapes. Knowing that the background pixels extend as horizontal stripes, automatic extraction of targets to further reduce the profile size becomes feasible. In order to extract patterns from background stripes, we follow the procedure as depicted in Fig. 11
                        (a), much easier than background subtraction techniques performed on the entire video frame [42]. We apply an efficient method on the temporal slices for removing background in order to satisfy the real time obligation. Both the temporal differentiation and background colors are taken into consideration. At each height y, we differentiate the color value along the time axis by

                           
                              (3)
                              
                                 
                                    
                                       I
                                       t
                                    
                                    
                                       (
                                       
                                          t
                                          ,
                                          y
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          ∂
                                          I
                                          
                                             (
                                             
                                                t
                                                ,
                                                y
                                             
                                             )
                                          
                                       
                                       
                                          ∂
                                          t
                                          
                                       
                                    
                                 
                              
                           
                        
                     

which is a differential image of the temporal slice I(t,y) producing the boundaries of passing objects. On the other hand, by subtracting all lines in I(t,y) with a single-line background color distribution b(y) as

                           
                              (4)
                              
                                 
                                    
                                       I
                                       b
                                    
                                    
                                       (
                                       
                                          t
                                          ,
                                          y
                                       
                                       )
                                    
                                    =
                                    
                                       |
                                       
                                          |
                                          
                                             I
                                             
                                                (
                                                
                                                   t
                                                   ,
                                                   y
                                                
                                                )
                                             
                                             −
                                             b
                                             
                                                (
                                                y
                                                )
                                             
                                          
                                          |
                                       
                                       |
                                    
                                 
                              
                           
                        
                     

We obtain another image Ib
                        (t,y), in which high value pixels belong to a passing object. The collection of those pixels in Ib
                        (t,y) shows the dynamic object occupied regions. Subtracting a single background array from the entire temporal slice might not produce ideal results since some objects may contain similar colors as the background (yielding small values in Ib
                        (t,y)). Detected edges, on the other hand, only reveal object boundaries and leave the uniform inner pixels undetected.

For a period of time that the sampling line scans background pixels only, both It
                        (t, y) and Ib
                        (t, y) values are close to zero. If an object passes, however, It
                        (t, y) is non-zero at boundaries and Ib
                        (t, y) is non-zero in the object occupied region except some holes. We thus fill such regions to enclose dynamic objects according to the conditions

                           
                              (5)
                              
                                 
                                    
                                       |
                                       
                                          |
                                          
                                             
                                                I
                                                t
                                             
                                             
                                                (
                                                
                                                   t
                                                   ,
                                                   y
                                                
                                                )
                                             
                                          
                                          |
                                       
                                       |
                                    
                                    >
                                    
                                       δ
                                       1
                                    
                                    
                                    or
                                    
                                    
                                       |
                                       
                                          |
                                          
                                             
                                                I
                                                b
                                             
                                             
                                                (
                                                
                                                   t
                                                   ,
                                                   y
                                                
                                                )
                                             
                                          
                                          |
                                       
                                       |
                                    
                                    >
                                    
                                       d
                                       2
                                    
                                 
                              
                           
                        where δ1
                         and δ2
                         are thresholds roughly at same scale, which are not difficult to set empirically as normal edge detection. Because the background will be updated overtime according to the illumination changes [25], this allows us to fix the thresholds at small values in our algorithm. A detected object thus contains information on arriving time and shape for identification. At the boundary of a dynamic object, the two thresholds are basically consistent, i.e., they are the difference from the background intensity.

With an updated background, the selected thresholds can work for a wide range of illumination. For detecting the one-dimensional background pattern b(y) that extends to horizontal stripes in I(t, y), we calculate a temporal section Ts
                         = {t, t+||Ts
                        ||} without any dynamic object as shown in Fig. 11(b), where the accumulations of It
                        (t, y) along the sampling line are close to zero, i.e., for every t ∈ Ts
                        ,

                           
                              (6)
                              
                                 
                                    
                                       T
                                       s
                                    
                                    =
                                    
                                       {
                                       t
                                       |
                                       
                                          Σ
                                          y
                                       
                                       
                                          |
                                          
                                             |
                                             
                                                
                                                   I
                                                   t
                                                
                                                
                                                   (
                                                   
                                                      t
                                                      ,
                                                      y
                                                   
                                                   )
                                                
                                             
                                             |
                                          
                                          |
                                       
                                       /
                                       h
                                       <
                                       
                                          δ
                                          3
                                       
                                       }
                                    
                                 
                              
                           
                        where δ3
                         is a tight threshold. Averaging I(y,t) horizontally in the scope of Ts
                        , the background color on the sampling line 
                           l
                        
                        
                           s
                         is obtained as

                           
                              (7)
                              
                                 
                                    b
                                    
                                       (
                                       y
                                       )
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             t
                                          
                                          
                                             I
                                             
                                                (
                                                
                                                   t
                                                   ,
                                                   y
                                                
                                                )
                                             
                                          
                                       
                                       
                                          ∥
                                          
                                             T
                                             s
                                          
                                          ∥
                                       
                                    
                                    ,
                                    t
                                    ∈
                                    
                                       T
                                       s
                                    
                                    
                                    and
                                    
                                    y
                                    ∈
                                    
                                       [
                                       0
                                       ,
                                    
                                    h
                                    ]
                                 
                              
                           
                        where ||Ts
                        || is the length of Ts
                        , and h is the height of image I(t,y).
                     

As depicted in Fig. 12
                        , the foreground regions are extracted along with their corresponding time stamps and can be sent via wire/wireless network for browsing. The surveillance camera in Fig. 13
                         is hidden at a distant location overlooking a road and a vertical sampling line generates a temporal slice that shows passing vehicles. The algorithm detects vehicle sections and sends to a server, where the segmented sections are shown in Fig. 13(c). Dynamic regions with smaller vertical sizes are neglected during the object extraction. Such object will be considered as noise and could include waving leaves and ripples on a river. On the other hand, shadows and reflections of objects are considered foreground as they move along with the objects. Foreground extraction on temporal slice is much less expensive than that done on a video frame as it only involves operations on several 1D pixel lines.

To update the background along time, we apply median filter with a large window to temporal domain in the temporal slice to find stable values of b(y) over time, assuming the background occupies the larger portion of the temporal slice temporally, i.e., foreground targets sparsely appear in long term surveillance. This guarantees the dynamic foreground to be ignored as noise and the background color is correctly obtained from the dominant median values in the large window over time. On the other hand, for crowd scenes in some events lasting for predictable periods such as several hours, background does not have to be updated constantly in real time. To ensure the prompt response of the large size median filter, we have proposed a constant time filtering algorithm [25] regardless of the window size. The idea is to adjust the median value according to newly entered and dropped pixels only in the large window, as the window moves along the time axis of the temporal slice. Thus, the large size window of median filter can be implemented to obtain a stable background color distribution.

Our next goal is to estimate the moving direction and velocity of targets within a narrow region around the sampling line, as illustrated in Fig. 14
                        , for keeping the low computational cost of line sampling that is affordable in real time surveillance. We are not using optical flow because of the following reasons: (1) we found the fundamental optical constraint may not be satisfied due to auto-exposure function of cameras; auto-exposure function in some surveillance cameras may change the background intensity when some target enters the field of view; (2) optical flow assumes smooth flow constraints, which is only correct for surface marks on objects but incorrect for occluding boundary. The moving targets have discontinuous motion from the background at boundary and the flow at background should be zero. However, it is incorrectly obtained after flow propagation from a close target approaching to the sampling line. For articulate human motion, each limb may further move against body in a non-smooth fashion [47]; (3) at the narrow region around the sampling line, there may not have sufficient number of edges to provide evident motion flow. For crowded target groups with small moving particles, this flow propagation over different moving directions blurs the correct motion; and (4) the weakness of noisy flow values between two consecutive frames. To avoid these problems, we observe multiple frames in a longer period, e.g., 10 frames, to obtain the robust target motion from their trajectories.

In the field of view, targets may pass the sampling line in various directions. For simplicity, we compute the horizontal and vertical components (u,v) of 
                           v
                         for horizontally or vertically set sampling lines. Assume image velocity 
                           v
                        (u,v) is more orthogonal to a selected vertical sampling line 
                           ls
                           
                         (x = xs
                        ) than the other principal pose direction as illustrated in Fig. 14(a). The penetrating velocity u achieves a scanning of target shape by line 
                           ls
                           
                        , and v yields a shift vertically along the sampling line. u can be estimated from the motion of an object edge non-orthogonal to line 
                           ls
                           
                         (i.e., ∂I/∂x ≠ 0). Such an edge shows the trajectory in the horizontal temporal-spatial slice. Thus, u can be observed and estimated from the tangent of object trajectory. However, if motion v ≠0, a close to horizontal edge (∂I/∂y ≠ 0) on object also generates a fake trace in the horizontal temporal spatial slice when it penetrates the slice. This type of edges has to be avoided in computing the tangents of edge trajectories for u.

We work our way around these “false edges” by averaging the horizontal temporal-spatial slices in the video volume to get a condensed slice showing real object motion (Fig. 14(a)). The close to horizontal edges in the frames would appear strongly as they penetrate horizontal temporal-spatial slices with a vertical velocity component v, but will be blurred and become weaker in the condensed slice by averaging the color along 
                           ls
                           
                         direction. The real close-to-vertical edges, however, are preserved in their strength in this averaging as they present the true moving direction of the whole target. We select the vertical scope between 10 and 30 pixels for data condensing to blur such fake edges within a narrow data block (20 pixel wide) around 
                           ls
                           
                        .

The algorithm of motion computation is as follows:

                           
                              (1)
                              Smoothing the color in the narrow block around 
                                    ls
                                    
                                  vertically by an averaging filter:


                        F(y) = 1/m for y ∈ [-m/2, m/2] and F(y) = 0 for other y, in a large scope m.

A condensed slice is generated by convolution, Ic
                        (x,y,t) = F(y) ∗ I(x,y,t), in a scope of x around line 
                           ls
                              .
                           
                        
                        
                           
                              (2)
                              At each condensed slice Ic
                                 (t,x) at height y, compute gradient 
                                    
                                       g
                                       
                                          (
                                          
                                             t
                                             ,
                                             x
                                          
                                          )
                                       
                                       =
                                       
                                          (
                                          
                                             
                                                
                                                   ∂
                                                   
                                                      I
                                                      c
                                                   
                                                
                                                
                                                   ∂
                                                   t
                                                   
                                                
                                             
                                             ,
                                             
                                                
                                                   ∂
                                                   
                                                      I
                                                      c
                                                   
                                                
                                                
                                                   ∂
                                                   x
                                                   
                                                
                                             
                                          
                                          )
                                       
                                    
                                  at strong traces (||
                                    g
                                 (t,x)|| > δ to better remove fake edge). The tangent direction of trace is then obtained as 
                                    
                                       t
                                       =
                                       
                                          (
                                          
                                             
                                                s
                                                t
                                             
                                             ,
                                             
                                                s
                                                x
                                             
                                          
                                          )
                                       
                                       =
                                       
                                          (
                                          
                                             
                                                
                                                   ∂
                                                   
                                                      I
                                                      c
                                                   
                                                
                                                
                                                   ∂
                                                   x
                                                   
                                                
                                             
                                             ,
                                             
                                             −
                                             
                                                
                                                   ∂
                                                   
                                                      I
                                                      c
                                                   
                                                
                                                
                                                   ∂
                                                   t
                                                   
                                                
                                             
                                          
                                          )
                                       
                                    
                                 , if 
                                    
                                       
                                          
                                             ∂
                                             
                                                I
                                                c
                                             
                                          
                                          
                                             ∂
                                             x
                                          
                                       
                                       >
                                       0
                                    
                                 , and 
                                    
                                       (
                                       
                                          −
                                          
                                             
                                                ∂
                                                
                                                   I
                                                   c
                                                
                                             
                                             
                                                ∂
                                                x
                                                
                                             
                                          
                                          ,
                                          
                                          
                                             
                                                ∂
                                                
                                                   I
                                                   c
                                                
                                             
                                             
                                                ∂
                                                t
                                                
                                             
                                          
                                       
                                       )
                                    
                                  otherwise, to guarantee a positive st
                                 .

Then, u is normalized to 
                                    
                                       u
                                       =
                                       −
                                       
                                          
                                             ∂
                                             
                                                I
                                                c
                                             
                                          
                                          
                                             ∂
                                             t
                                             
                                          
                                       
                                       /
                                       
                                          
                                             ∂
                                             
                                                I
                                                c
                                             
                                          
                                          
                                             ∂
                                             x
                                             
                                          
                                       
                                    
                                  for all points within the block around 
                                    ls
                                    
                                 .

The computed u is further median filtered within the block to obtain uc
                                 (t,y) = median(u(x,y,t)) to remove isolated errors due to some complex shape and occlusion between articulate motion if targets are humans.

The sign of u gives the passing direction and magnitude of image velocity of target through the sampling line.


                        Fig. 15
                         shows the results of the algorithm in separating pedestrians in groups according to their moving directions through the sampling line. The parameter m is 30 pixels in this case and it is selected depending on object size. The x scope for this computation is 20 pixels. The optical flow method is much noisier than our method on limbs of pedestrians and object boundaries. It may yield different flow directions during the walking period of a pedestrian when arm and leg have relatively static moment [34].

With the detected motion, we can also diminish the shape deformation of targets in visualization. We further determine the transparency according to the scale of motion through the sampling line. Therefore, the temporal stay, sway, or slow motion of a target will be more transparent or even disappeared in the temporal slice, while prompt motion will be displayed more opaque. A vehicle may stay for a signal or in a traffic jam, and a pedestrian may stop for chat. If such a time is short, the target should still be considered as a dynamic object but displayed with transparency as Fig. 16
                         computed. Otherwise, it is merged into the background in order to separate other passing objects in front of it.

The temporal slice can work well with non-occlusion targets moving in different directions. An overlooking camera or a side viewing camera watching at a narrow road is effective to capture non-occlusion flows of targets. For occlusion targets, the sampling line can only captures the shape of occluding target; the occluded target has to be visualized by profiles described in the following sections. In motion direction computation, however, the thin data brick around the sampling plane in the volume may still catch the motion of occluded target partially. Fig. 14(b) (left) depicts a case in which multiple targets are crossing the sampling line in different directions at the same time. The spikes that changes motion direction in close frames are actually from occluding and occluded targets, respectively.

In order to add more spatial information in the temporal slices, we set multiple sampling lines at the spatial locations sufficiently far apart to get the direction of target movements and add some locations in a temporal profile. We sample pixel lines at multiple critical locations with the distances at least as wide as target widths so that a target will not pass multiple lines simultaneously. As discussed in Section 2.2, these lines will also pass the vanishing point so as to sample the principal pose at each location correctly. Because of the delays of the foreground flow crossing these sampling lines, the shapes appearing in the individual temporal slices will not overlap in time. This proposed approach overcomes one of the shortcomings of the temporal slice by providing a motion direction to the apparent targets. It is achieved by blending the multiple temporal slices together according to their spatial locations to create a combined temporal profile of video that shows the dynamic flow of foreground clearly.

As shown in the diagram of Fig. 17
                        , a temporal profile is integrated from three different temporal slices sampled on selected lines at a critical location. Combining multiple temporal slices will cause occlusion in the final profile as the background contained in the slice may overlap with the sampled targets in another slice. Since the analysis of a single temporal slice can yield sufficient information for background identification in Section 3.1, we remove background regions in the ith temporal slice by utilizing a mask maski
                        (t,y) extracted from the foreground (refer to Fig. 12). We blend multiple slices at their foreground regions in different transparencies. Denote I0
                        (t,y), I1
                        (t,y) and I2
                        (t,y) as the temporal slices obtained with the sampling lines from left to right in the video frame respectively, each slice has a blending coefficient αi
                         that determines its contribution to the final temporal profile. In general, assume the video is sampled at n different locations, where n ≥ 3, e.g., as depicted in Fig. 18
                        , we blend profiles Pi
                        (t,y) by

                           
                              (8)
                              
                                 
                                    
                                       
                                       
                                       
                                          
                                             
                                                P
                                                i
                                             
                                             
                                                (
                                                t
                                                ,
                                                y
                                                )
                                             
                                             =
                                             
                                                [
                                                
                                                   1
                                                   −
                                                   
                                                      α
                                                      i
                                                   
                                                   m
                                                   a
                                                   s
                                                   
                                                      k
                                                      i
                                                   
                                                   
                                                      (
                                                      t
                                                      ,
                                                      y
                                                      )
                                                   
                                                
                                                ]
                                             
                                             
                                                P
                                                
                                                   i
                                                   −
                                                   1
                                                
                                             
                                             
                                                (
                                                t
                                                ,
                                                y
                                                )
                                             
                                             +
                                             
                                                α
                                                i
                                             
                                             m
                                             a
                                             s
                                             
                                                k
                                                i
                                             
                                             
                                                (
                                                t
                                                ,
                                                y
                                                )
                                             
                                             
                                                I
                                                i
                                             
                                             
                                                (
                                                t
                                                ,
                                                y
                                                )
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                       
                                          
                                             
                                                P
                                                0
                                             
                                             
                                                (
                                                t
                                                ,
                                                y
                                                )
                                             
                                             =
                                             
                                                I
                                                0
                                             
                                             
                                                (
                                                t
                                                ,
                                                y
                                                )
                                             
                                             ,
                                             
                                             
                                             
                                             i
                                             =
                                             1
                                             ,
                                             …
                                             ,
                                             n
                                          
                                       
                                    
                                 
                              
                           
                        where P0
                         is the slice at location 0, which contains background stripes to provide the profile with a context. If the value of maski
                         at a background position is zero, the color value in previous Pi-1
                         is used for the newly integrated profile Pi
                        . As long as the profiles are blended with a predefined spatial order, the motion direction of target becomes clear in the resulting profile. Examples of final P2
                        (t,y) are shown in Fig. 18, where α1
                         and α2
                         for profiles I1
                        (t,y) and I2
                        (t,y) are set as 0.7 and 0.5, respectively. Therefore, one can easily understand that, for a target moves rightward to across the sampling lines, the transparencies of shapes increase in the profile. Inversely, a leftward motion generates shapes more and more opaque in the temporal profile [31].

Resulting from blending multiple slices together, this temporal profile not only allows us to locate time instances of passing targets, but also helps us understand the motion direction of targets. Figs. 18–20
                        
                         demonstrate the use of this profile in different surveillance settings. Passing through the sampling line are shown by three copies of the targets. Even if the foreground extraction contains some error due to color similarity between parts and background, at least one copy of the target is intact.

To generate a more representative profile that reflects all target motion in the entire field of view, we extend the multi-line sampling scheme to cover all the locations in the video frame for a temporal profile. The close-to-vertical lines passing through the vanishing point as discussed in Section 2.2 will capture all horizontal motions of targets on paths in the video. Similarly, if we choose to capture vertical motion on a stretching path, we can set the sampling lines horizontally through the corresponding vanishing point. Lines are set at equidistant positions to capture events, and are integrated into a single profile to show the events in the video as complete as possible. A target may pass one or more sampling lines and will leave a series of copies in the temporal profile, unless it is static or it performs local non-translational motion. Although events in between the sampling lines may be omitted, a significant translational motion is impossible to be missed. Fig. 21
                            shows a result where pedestrians and vehicles move at an intersection. It is possible to estimate the image velocity of objects in such a temporal profile under the condition that objects do not occlude each other. By matching consecutive copies of an object, its image velocity can be approximated. The hyperbolas fitted to the copies of a vehicle show the vehicle speed through the intersection.

We blend all the slices together according to their spatial locations to create a temporal profile of video that shows the foreground flow clearly. If these lines are spatially apart from each other with intervals wider than target sizes, a target will not pass them simultaneously. Although we can chose the interval of the sampling lines to be arbitrary small, it is a good idea to leave some distance between sampling lines to avoid cluttering the temporal profile. We choose the number of lines from six to ten for capturing a combined temporal profile. Because of the delays for a foreground to across these sampling lines, their temporal order in the slices helps determine the motion direction. To reflect more spatial information in this dimension-reduced profile, we also use different blending factors for slices according to their spatial positions in the video frame, i.e., αi
                           (x) ∼ x/W or αi
                           (x) ∼ 1- x/W, where W is the width of frame. The change of blending factor creates a haze effect or transparency for visualizing the target distance from a frame margin in video volume as proposed in Fig. 3, either left or right depending on the 3D scenes.

Assume Ii
                           (t,y), i= 0,1, …n, are the slices sampled at xi
                            from one margin in the video frame, each slice has blending coefficient αi
                            that determines its contribution to the final temporal profile Pn
                           . Unlike the localized temporal profile discussed in Section 4.1, the transparency coefficients αi
                            are determined by the sampling positions in the frame; sampling lines at two extreme values of x axis will have the minimum and maximum transparencies. Using this order while blending the slices together will ensure the direction of movement being preserved. The blending formula used in Eq. (8) guarantees that if a sampling slice contains background stripes it will not occlude the extracted foreground objects (utilizing a mask noted maski
                           ). If the value of maski
                            is zero at a position, the color value in Pi-1
                            is used. Fig. 22
                            includes some examples where 7 slices are blended to create a complete profile of the scene. The copies of a pedestrians show their position by transparency.

To eventually determine in which order to blend slices, we integrate spatial information from the 3D space into the profile. With all the field of view sampled by the distributed lines, there may involve depth differences at different orientations. The size of a target varies at these depths as well as in the obtained temporal slices. Let us take the example of a video with close-to-vertical sampling lines roughly capturing horizontal motions on a path. The camera is possibly orientated in such a way that one side of the frame or one end of the path is closer than the other side (targets on close side are larger in size), as depicted in Fig. 23
                           . We can classify the camera orientations to be (a) right facing, (b) orthogonal, or (c) left facing, with respect to a path regardless of the camera tilt. They correspond to paths of left-close scene denoted as LC layout, horizontal path, and right-close scene denoted as RC layout, respectively. The transparency assigned to different slices should reflect the target depth consistent with its size changes. In addition, a lower position in the frame normally has a closer depth for an overlooking camera, and targets on the ground thus have larger sizes.

Our strategy thus selects the left margin to be opaque for LC layout, and right margin to be opaque for RC layout respectively. For orthogonal case (b) or multiple paths without a significant depth difference, either side can be selected as the opaque margin. The transparency then increases (α decreases) towards the other margin such that target appearances tend to be far away. Using this strategy, the generated temporal profile shows a close target at a low position with a high opacity. If a target is at a far distance, it becomes small at a higher position in the temporal profile, and is rendered more transparent as shown in LC layout in Fig. 22. Based on this strategy, the transparent-to-opaque change of target copies in the temporal profile indicates a leftward motion in LC layout, and a rightward motion in RC layout, as if the video volume is observed from side. Inversely, the moving direction presented by an opaque-to-transparent transition can also be derived for LC and RC layouts easily.

To further improve the profile's readability and increase its perceptiveness of spatial layout, we blend a frame of background into the profile to help understand the positions and environment. In the first step, we will create a frame with only background pixels to avoid blending false targets into the final profile. To accomplish this, we stitch together the position different pixels in a period of video segment without foreground activities. This can be cut out a diagonal slice across all the background traces in such a static period of video volume as shown in Fig. 3. The direction of diagonal cutting is rightward in the frame uniformly. This not only makes background visible from sideways, but also keeps the definition of temporal profiling, i.e., for any point visible in I(t,y), it is in frame t in the original video feasible for frame retrieval. We then embed such an entire frame into the profile, which adds even more spatial reference to targets, while keeping the temporal accuracy along the time axis in the profile. This embedded background will show the layout in the camera space, the order of blending, and even the depth of targets in the case of RC and LC layouts. The opacity is proportional to the image position x changing from opaque to transparent along the time axis for LC layout, and from transparent to opaque for RC layout, respectively. That is

                              
                                 (9)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   α
                                                   i
                                                
                                                
                                                   (
                                                   x
                                                   )
                                                
                                                =
                                                1
                                                −
                                                x
                                                /
                                                W
                                                
                                                for
                                                
                                                L
                                                C
                                                
                                                layout
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   α
                                                   i
                                                
                                                
                                                   (
                                                   x
                                                   )
                                                
                                                =
                                                x
                                                /
                                                W
                                                
                                                for
                                                
                                                R
                                                C
                                                
                                                layout
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

which is consistent to the order above for temporal slices at varied depths. Fig. 24(
                           c) and (d) shows the results with LC background embedding.

In this section, we extend our temporal profiling to the camera panning motion. In addition to static surveillance cameras, panning cameras are most commonly used for surveillance. Panning is usually achieved by means of a motor rotating periodically around an axis with a constant velocity, or controlled by operators in a piecewise smooth motion to enlarge the monitoring field of view. To obtain a temporal slice from a constantly panning camera, we analyze the motion of the camera, transform the panning camera video to a panorama-like static video, and then use above mentioned techniques to cut along the time axis for obtaining a temporal profile. With this approach, the temporal slices in a wider field of view can be extracted in real time and transmitted to a monitoring center promptly, while detailed video can still be recorded to the vast storage space with panning focused on different directions in the scenes.

For simplicity, we focus on cameras with smooth panning motion around a vertical axis, and the background scenes are the majority of the field of view as compared to the dynamic foreground targets passing through. This study can be easily generalized to cameras along an arbitrary axis. Let us assume that a camera overlooking a scene is smoothly panning with the constant angular velocity ωc
                     . The movement of objects in the frames will be on the curve segments of ellipses symmetric to the y axis in the opposite to the camera panning direction. This means the background flow 
                        v
                     (u,v) at each x position has the same horizontal component u but slightly different y components v. If we obtain a condensed image of the video, C(x, t), by averaging pixels vertically in the entire frame over the period of clip (as described in Section 3.2, Figs. 1 and 14), we can obtain homogeneous motion traces regardless of their small difference in vertical motion. All of the background traces are along the direction opposite to the camera motion in the images. This can be observed in the condensed image in Fig. 25
                     , obtained by averaging the pixels vertically in the video volume I(x, y, t).

According to Section 3.2, we can infer the velocity of the camera from the slopes of the traces in the condensed image. Fig. 26
                      shows a diagram of detecting the camera pan speed. The gradient of the condensed image is computed and the pixels with high magnitude of gradient are collected for computing their detections at each moment t. The camera velocity averaged or median filtered at each moment yields displacement dx of scenes between two frames, which is used to correct the position of the video frames so that the video will virtually appear as static. Fig. 27
                      shows such a computation in the condensed image of a panning video. Because of the larger background area than foreground targets in the field of view, the background traces are dominant in the condensed image, even if dynamic targets have occasional traces against background traces. The detected background motion values can be majority, and thus can be guaranteed for extraction by median filtering the motion values. If we further know a height in the field of view without target passing (most surveillance video has such a part) or even a single solid vertical feature at such a height, the condensed image can be obtained from such a specified height. The motion trace of background hence can be followed robustly and precisely just from that feature.

Once we have detected the motion of camera in opposite to the background motion, we compensate for it by zero padding the missing pixels in the video. This would mean the field of view will shrink as we are panning out. We translate the pixels back by the amount of motion, transforming the volume into a static video with zero-padded pixels where the field of view is changed. Fig. 28
                      is an example of the corrected video by our method, where the frames are rectified in their position according to the fixed scene orientation. The further video profiling is shown in Fig. 29
                     , where Fig. 29(a) shows the results of the camera motion detection in the condensed image. After the correction we can obtain a temporal slice in Fig. 29(b) as described above by cutting the regions that stay in the field of video after stabilizing the video. The vertical shrink of scenes at the center of the temporal slice is caused from the vertical component changes of optical flow at the sampling locations in the frames during the camera panning from right to left. Although the accuracy error of the estimated motion may cause some disturbance on the background in the temporal slice, it has no difficulty for human operators to identify the sharp images of passing targets against the horizontally blurred background.

For other camera motion such as translation, this temporal profile may not be able to separate dynamic targets from static background in general, because the entire background scenes are moving with different motion parallax according to their depths. The temporal profile thus may work for some other purposes such as archive passed scenes by a camera for the geographic information system [19] and general video indexing [43].

For the purpose of verifying the temporal profiles, we have tested many video clips with all shots captured using an HD video recorder. These clips include indoor scenes as well as outdoor environments with static and panning camera shots. Subjects with different motion characteristics such as humans, bicycles, and cars were recorded for study. We have also developed a GUI on windows to facilitate sampling and advance our tests. Emgu CV library was utilized in most of our code to accelerate the process and help with our proof of concepts. Our tests were run on a Dell XPS desktop with 16 GB of RAM and i7-3770 3.40 GHz. For all of the algorithms above, a non-parallel implementation has been provided that leads to a linear run time with regards to the video length. For a two minute video, any of the above mentioned profiles will take less than 10 seconds to generate. The image resolution of the temporal profile is compatible to that of video frame in spatial domain. The temporal deformation does not affect the identification of targets by viewers significantly. This leads to the further examination of original video frame in its full size. The image quality of temporal profile still allows the recognition of target identity to some extent.
                     
                  

We have also tested our algorithms on various online and offline video streams lasting hours. For each long video, sampling locations are set manually at critical locations according to the site layout in the field of view and target motion directions. For a distributed camera network, our experiments were carried out with several cameras connected to computers with image grabbers. These computers further communicate with a server via wireless Internet. The experiments can virtually last day and night if permanent systems are mounted.

If a sampling line is set at an angle not parallel to the imagining grid, the sampled pixels are interpolated by varying height y. In the data processing, we collect temporal profile with a large circular memory M[t,y], where t ∈ [0,1000], i.e., M[t,y] = I(t mod 1000, y). The horizontal filtering and pixel subtraction are all with the complexity of O(1) for a fixed length h of the sampling line; the processing has no accumulative latency. We use video cameras to read data on the sampling line at the rate of 60 Hz, which is adequate to capture walking people at distance. A five minute video results in a temporal profile with the length of 18,000 pixels in storage. Only extracted segments of dynamic objects are transmitted as an index from the computers to a central server via wired and wireless network, which achieves a great data reduction from original video. Degrading one dimension of the visual field can thus reveal targets in a complicated background without heavy processing. A low-end computer (for example, a tested computer has 400 MHz CPU and thus is more workable for other communication devices and units) can undertake the processing of surveillance data. This flexible camera network can cover various spots in a large area. A camera node can be added in a room for capturing an object flow at a distant street through window, if the camera zooms up the scene. The computers send detected targets to the server by socket connections (TCP/IP based) via Wi-Fi network. The image sections (y∈[0,h]) containing passing objects are displayed on the server and the number is counted.

Different from a vertical setting of sampling line in many cases, Fig. 30 shows a temporal profile cut from the principal direction in horizontal, because it is more orthogonal to the motion direction along the escalator. The background updating works robustly and this improved the dynamic foreground segmentation. The dynamic objects staying less than 5 s are extracted stably, which benefits to the multi-line sampling and temporal profile fusion. In a dim night, we only capture headlights of vehicles and leave bodies undetected. The threshold for recognizing an object is lowered down. Similarly, Fig. 31 profiles a low intensity distribution in a snow day. Fig. 32
                     
                     
                      shows a real time experiment with cameras facing indoor and outdoor environments in a campus to count passing people. Only the time periods with targets are collected and transmitted via network.

Surveillance videos are taken day and night so that the video profiling is required to work for changeable background due to the change of illuminations (from shining to cloudy, from day to night), object casting shadows, waving backgrounds (water, tree, rain, snow), and temporally static objects. We needs to deal with low contrast, salt noise, and ripple texture in the background, and unpredictable object stop in temporal profile. Overall, the background must be adapted over time [10,15,18].

We assume that, in the temporal scale, vehicles and people pass a location in a shorter time than changes of weather and illumination in general. The instant temporal differentiation applied over several pixels (<150 ms) for detecting people and vehicles is hence insensitive to the slow changes in ambient lighting conditions. The key issue now is the update of background over time under a changeable illumination so that the algorithms can work robustly in thresholding foreground from the backgrounds. If dynamic objects passing 
                        ls
                        
                      are not overcrowded temporally, the majority of intensities collected over a long period will be dominant by background pixels. We can thus update each pixel in the background distribution b(y) by a simple median filter filtering pixels along the time axis [1,20,24,25]. A static period Ts for background distribution b(y) is obtained initially in the temporal profile, and then b(t,y) is updated over time by a median filter with a window width τ, i.e.,

                        
                           (10)
                           
                              
                                 b
                                 
                                    (
                                    
                                       t
                                       ,
                                       y
                                    
                                    )
                                 
                                 =
                                 median
                                 
                                    (
                                    
                                       I
                                       
                                          (
                                          
                                             
                                                t
                                                s
                                             
                                             ,
                                             y
                                          
                                          )
                                       
                                    
                                    )
                                 
                                 ,
                                 
                                    t
                                    s
                                 
                                 ∈
                                 
                                    T
                                    s
                                 
                                 
                                    [
                                    t
                                    ,
                                    
                                    t
                                    −
                                    τ
                                    ]
                                 
                              
                           
                        
                     
                  

Dynamic objects are considered as outliers or noises to be removed from the period of Ts
                     . The larger the window, the more stable the filtered result is. However, the sorting cost in median filter increases tremendously for a largeτ window. Many works have improved the computation order of median filter by using the quick selection algorithm with the computational complexity O(τlogτ) [20], which is still not affordable in real time video scanning and multiple line profiling. We have developed an efficient sorting algorithm on temporal data to achieve background updating in linear complexity. The algorithm is based on Radix sorting working on a stream of data [25]. It adjusts median value in a sequence only based on newly included and excluded pixels at two ends of the filter window, when the window shifts forward continuously. The common data elements are handled in histograms. This saves the cost of sorting of the entire window in a constant order; the large window for stable results does not increasing the computational cost. We have experimented the window size between 640 and 2000 pixels (frames), depending on the size of circular memory opened for the constant generation of temporal profile. Fig. 33 shows the updating of background in a certain period due to weather and illumination change while extracting dynamic vehicles.

In computing the motion direction of target, our method is more global and accurate in pixel level as compared to the optical flow computation between frames and then copied to the sampled temporal profile. In details, our proposed method is better in accuracy than the results based on optical flow computation between frames by using OpenCV modules. Fig. 34 shows the results of motion direction detection using three methods on various video clips. Table 1 also shows the accuracy of motion direction in pixels among the dynamic moving pixels. The accuracy is measured by comparing the detected motion direction on each pixel with the ground truth, normalized by the number of pixels in the regions of dynamic targets. The motion direction is binary here through the sampling line, though a more detailed value of penetrating velocity is obtained from the filtering of target traces.

In the visualization aspect, the temporal profiles are viewed by human operators in scrollable windows for fast target searching. The 2D profile has a much compact data size as compared to synopsis video and is further shortened by removing long periods without target motion. Scrolled in a video track, a temporal profile is continuous, in which moving targets stand out against monotonic background stripes so that target screen is easier than watching discrete synopsis. Clicking on the targets can lead to exact frames for further examination. By incorporating the layout and temporal information in the temporal profiles, we can further perceive global motion directions of targets in the video. The density or target copy in the temporal profile is controlled by the interval of sampling lines, and is also influenced from target moving velocity and depth. With close target intervals in the temporal profile, we can even show target actions in the video. The aspect ratio of close target is thin and tall because the fast image velocity of close object, even if the target is walking. Fortunately, the video track in video software under the video frame display is long with low height. This allows us to reduce the scale in spatial domain only to compensate the distortion in target aspect ratio. It is possible to cut multiple pixel lines during the sampling of temporal profile for improving the aspect ratio in video index visualization, if the path/channel or target depth as well as the target speed are known. We are not making this effort because we are only using the temporal profile as an index of video for further examination of details at frames, rather than a complete and perfect visualization in this paper. This video presentation is not necessary to be the optimal video summary for general video, but is particularly efficient for surveillance video browsing. Although this paper is more focused on surveillance, the temporal profile as a video index can be compared to other video summary methods either in spatial or in spatial-temporal domains in the all aspects as given in Table 2. Boldfaces are functions superior than other methods in each aspect. Overall, each style of video indexing has its own advantages, which can be applied selectively in surveillance.

@&#CONCLUSION@&#

This paper proposed a method to map a surveillance video to a temporal profile for indexing and searching. The profile provides not only an intuitive summary of moving targets in a compact image but also the accurate time and speed information recorded in the video. Because of the reduction of one dimension data, it is inexpensive and efficient to visualize shapes of passing targets in a scrolling window for further examination in video frames. We align sampling lines in the principal pose directions of targets more orthogonal to their motion for better shape acquisition in the temporal profile. Moreover, we have achieved target group extraction, flow motion estimation, background
                      updating, and invasion alarming with constant complexity, and transmitted dynamic data via wired/wireless Internet in real time. Further, we integrate multiple slices from video into the temporal profile by blending foreground as well as background in different opacities. This reflects the target moving direction and position in the space. Different from spatial indexing methods, the proposed profile is continuous in time domain without length limitation, and displays shape and spatial relationship to a certain extent. It can be generated in real time and has reduced the amount of data significantly for indexing and searching of surveillance video.

@&#REFERENCES@&#

