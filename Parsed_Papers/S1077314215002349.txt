@&#MAIN-TITLE@&#A scalable and flexible framework for smart video surveillance

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We developed a scalable and flexible framework to video surveillance analysis.


                        
                        
                           
                           By using the framework, the researcher can focus only on the design of his/her specific task.


                        
                        
                           
                           The framework allows the creation of a high-level semantic representation of the scene and scalable feature extraction.


                        
                        
                           
                           We report results demonstrating the framework performance and the viability of its usage.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Smart surveillance framework

Surveillance systems

Computer vision

Video analysis

Video surveillance

@&#ABSTRACT@&#


               
               
                  In the last years, the number of surveillance cameras placed in public locations has increase vastly and as consequence, a huge amount of visual data is generated every minute. In general, this data is analyzed manually, a challenging task which is labor intensive and prone to errors. Therefore, automatic approaches must be employed to enable the processing of the data, so that human operators only need to reason about selected portions. Computer vision problems focused on solving problems in the domain of visual surveillance have been developed aiming at finding accurate and efficient solutions. The main goal of such systems is to analyze the scene focusing on the detection and recognition of suspicious activities performed by humans in the scene, so that the security staff can pay closer attention to these preselected activities. However, these systems are rarely tackled in a scalable manner. Before developing a full surveillance system, several problems have to be solved, which are usually solved individually. However, in a real surveillance scenario, these problems have to be solved in sequence considering only videos as the input. With that in mind, this work proposes a framework for scalable video analysis called Smart Surveillance Framework (SSF) to allow researchers to implement their solutions to the surveillance problems as a sequence of processing modules that communicates through a shared memory.
               
            

@&#INTRODUCTION@&#

Due to the reduction in prices of cameras and the increase in network connectivity, the number of surveillance cameras placed in several locations increased significantly in the past few years. If on one hand, a distributed camera network provides visual information in real time covering large areas, on the other hand, the number of images acquired in a single day can be easily in the order of billions, preventing their manual processing and posing an intricate problem for monitoring such areas [1].

While the ubiquity of video surveillance provides safer environments, the monitoring of large amount of visual data is a challenging task when performed manually by human operators since most of the visual data do not present interesting events from the surveillance standpoint, turning it into a repetitive and monotonous task for humans [2,3]. Hence, automatic understanding and interpretation of activities performed by humans in videos are of great interest since such information can assist the decision making process of security agents [2].

The addition of automatic understanding and interpretation to surveillance systems does not entail the replacement of human operators as foreseen on several Sci-Fi movies, on the contrary, it aims at supplying information to the operator. For instance, instead of a security agent monitoring continually up to 50 screens with live security video feed (task which humans do not present high performance due to the lack of important events during most of the time [4]), an automated system might perform a filtering in the videos and indicate only those video segments more likely to contain interesting activities, such as suspicious activities that might lead to a crime.

In the last two decades, professionals of industry and researchers have dedicated their studies to improve surveillance systems. To understand the increase of works related to video surveillance and inspired by Huang [5] study, we searched the keywords video and surveillance in IEEE Xplore Digital Library
                        1
                     
                     
                        1
                        
                           http://ieeexplore.ieee.org/
                        
                      (within metadata only) and the IEEE Computer Society Digital Library
                        2
                     
                     
                        2
                        
                           http://www.computer.org/csdl
                        
                      (by exact phrase). The findings are shown in Fig. 1
                     
                      as a function of the publication year. The large number of publications in the past ten years indicates that research on surveillance video has been very active.

Smart visual surveillance systems deal with the real-time monitoring of objects within an environment. The main goal of these systems is to provide automatic interpretation of scenes and understand activities and interactions of the observed agents based on the visual information being acquired. Current research regarding these automated visual surveillance systems tend to combine multiple disciplines, such as computer vision, signal processing, telecommunications, management and socio-ethical studies.

One of the great challenges of automatic surveillance systems is that to interpret what is happening in the scene, a sequence of problems need to be solved, which is highly prone to noise generated during the process. Among the problems are the background subtraction [6], pedestrian detection [7], face recognition [8], gesture recognition [9], pose estimation [10], person tracking [11] and re-identification [12], action recognition [13] and activity recognition [14]. Even thought each one of these problems present a vast literature, they are usually considered independently such as in currently available evaluation data sets, e.g., the evaluation of face recognition methods is performed using already detected, cropped and aligned faces [15], which cannot be accomplished in real surveillance scenarios where the only inputs are video feeds without annotations. Therefore, dealing with the problems individually does not allow one to identify what are the effects of the results obtained by solving one problem on the following steps in the processing sequence.

Although visual surveillance has been subject to a huge growth, there is still a lack of contributions from the field of system engineering to the area [16]. The small number of frameworks that are open and focus on visual surveillance usually require a steep learning curve. In addition, with the contemporary advances in video sensors and increasing availability of network cameras allowing the deployment of large-scale surveillance systems, distributed in a wide coverage area, the design of smart and scalable surveillance system remains a research problem: how to design scalable video surveillance systems considering aspects related to processing power, memory consumption and network bandwidth?

Motivated by the presented issues, this work proposes a framework for a scalable video analysis able to readily integrate different computer vision algorithms into a functional surveillance system. This framework, called Smart Surveillance Framework (SSF), aims at bringing several improvements providing scalability and flexibility, allowing the users (researchers) to focus only on their application by treating the sequence of problems as a set of processing modules that communicates through data streams, stored in a shared memory. This framework was presented in a preliminary and reduced version focusing only on its main concepts in [17] and its feature extraction server was described in [18]. However, this work presents the SSF in details and focuses on the evaluation of its several components
                        3
                     
                     
                        3
                        The Smart Surveillance Framework is available for download at http://www.ssig.dcc.ufmg.br/ssf/.
                      In addition, this work also presents new features, such as the data stream and the image and feature managers.

More specifically, the Smart Surveillance Framework is a development environment in which the researcher can implement and evaluate his/her algorithms related to surveillance in an integrated manner, as illustrated in Fig. 2. It is based on execution modules that communicate to each other using data streams controlled by a shared memory. The framework provides the following features to aid the researcher: memory management to allow handling large amounts of data in regular computers; communication control among execution modules; predefined data structures specifically designed for surveillance environment; management of multiple data input, such as cameras or stored videos; feature extraction server to maximize the usage of the processing power available to compute local descriptors; query server to allow high level reasoning and scene understanding; and a configuration interface to help setting up sequences of execution. These features and the framework architecture will be discussed in details in Section 4.

The main contributions provided by the development of the SSF are the following: (i) A novel framework to allow the processing of large amounts of data provided by multiple surveillance network cameras; (ii) A platform to compare and exchange research results in which researchers can contribute with modules to solve specific problems; 
                     (iii) A framework to allow fast development of new video analysis techniques since one can focus only on his/her specific task; (iv) Creation of a high-level semantic representation of the scene using data extracted by low-level modules to allow the execution of video event analysis based on individual or group activities; (v) A testbed to allow further development on activity understanding since one can focus directly on using real data, instead of annotated data that may prevent the method from working on real environments; (vi) A platform to allow scalable feature extraction that uses the full power of multi-core architectures;

This work is organized as follows. Section 2 discusses briefly the most common problems tackled in visual surveillance. Section 3 presents a review of published papers in recent years that discuss the issues and challenges involved in the deployment of modern visual surveillance systems and discusses works similar to the proposed framework. Then, Section 4 describes the proposed Smart Surveillance Framework (SSF). While Section 5 presents our experimental evaluation, Section 6 describes how a surveillance application can be designed in the SSF. Finally, Section 7 points our final remarks.

According to the taxonomy proposed by Nazare et al. [17], problems considered in the surveillance domain might be divided into four groups: visual information representation, regions of interest location, tracking and identification, and knowledge extraction. Fig. 3 shows these groups and the relationship among the problems within each. While modules located at the top of the diagram define low-level problems, in the sense that they present low dependency to solutions obtained by other problems, e.g., background subtraction and pedestrian detection, modules at the bottom comprise high level problems since they depend on the results of other problems, e.g., action and activity recognition.

The arrow in the right-hand side of Fig. 3 represents the dependencies among the problems. For example, to solve the action recognition, one first needs to correctly detect and track the person who is executing an action. Tasks composing this process might be affected by errors propagated along the task chain (e.g., detection errors will affect the tracking of a person, which will prevent the recognition of the action executed by this person). Therefore, it is necessary to solve the tasks in an accurate manner be able to solve problems presenting several dependencies, such as the activity recognition, responsible for making inferences regarding the activities being executed in a scene (e.g., loitering, identification of suspicious collaborations or carjacking).

In the diagram shown in Fig. 3, Visual Information Representation comprehends tasks aiming at representing the information contained in the visual data, e.g., converting pixel information to a feature space which is more robust to noise and transformations taking place in the video. The goal of the Regions of Interest Location is to narrow down efficiently the locations of the scene where information regarding activities taking place can be extracted. Then, once the tasks in the previous category have located the relevant regions in the scene for each frame, the problems in the Tracking and Identification category will estimate their trajectories and identify the agents based on information including their appearance or their faces. Finally, after the objects and agents have been located, identified and their trajectories have been estimated, their actions and activities can be recognized, these problems refer to the Knowledge Extraction category. All the information collected by executing the tasks will be used to generate a knowledge representation regarding the scene, so that one can use such information to make inferences and perform scene analysis.

The framework developed in this work has been designed to allow researchers to tackle the problems shown in Fig. 3 in such a way that the results achieved by solving these problems might feed an inference system used to understand the scene and the activities performed by the agents (persons in the scene).

Nowadays, there is an increasing interest in surveillance applications because of the availability of low-cost sensors and processors. There is also an emerging need from the public for improving safety and security in urban environments and the significant utilization of resources in public infrastructure. These two factors associated with the growing maturity of algorithms and techniques, enable the application of technology in public, military and commercial sectors [19].

Security surveillance systems are becoming crucial in situations in which personal safety could be compromised resulting from criminal activity. For this, video cameras are constantly being installed for security reasons in prisons, parks, banks, automatic teller machines, gas stations, and elevators, which are the most susceptible for criminal activities [20].

In general, images acquired by a set of cameras may be monitored in real time from a command center, where exists many display screens from which security personnel constantly monitor suspicious activities. However, the burden of watching the entire video, detecting threats, and locating suspects is assigned to the human operator. This process of manually watching video is known to be tedious, ineffective, and expensive [2], because the attention span of human observers is inevitably limited [4]. Therefore, the addition of computational intelligence to alert the observers to the infrequent image feed which contained events of possible importance was thus a natural development as computing resources became both cheaper and more powerful.

In the following sections, we overview the state-of-the-art in smart visual surveillance systems and introduce the evolution of these systems, as well as, their challenges.

According to Valera [16] and Räty [20], the technological evolution of surveillance systems can be divided into three generations, summarized in Table 1
                        .

The first generation of surveillance systems started with analogue Closed-Circuit Television (CCTV). These systems consist of a number of cameras placed in multiple locations and connected to a set of monitors, usually placed in a single control room via switches (a video matrix). The main disadvantages of these systems concern the reasonably small attention span of operators that may result in a significant miss rate of the events of interest. The advantage is that the technology is mature. To perform computational processing on this type of system conversion from analog to digital video is required which may cause quality degradation.

The advent of digital CCTV and high performance computers have led to the development of semi-automatic systems, known as second generation of surveillance systems. This generation benefited from the early progress in digital video communications, e.g., digital compression, robust transmission and bandwidth reduction. The advances of the second generation are that the surveillance efficiency of CCTV is enhanced. The difficulties lie within the robust detection and tracking algorithms needed for behavioral analysis.

Most of the research on the second generation of surveillance systems is based on the creation of computer vision algorithms aiming at improving results for identification, tracking of multiple objects in complex scenes, human behavior comprehension, and multi-sensor data fusion. The second generation also improved intelligent human-machine interfaces, performance evaluation of video processing algorithms, signal processing for video compression and multimedia transmission for video-based surveillance systems [20].

In the third generation, the technology revolves around wide-area surveillance systems, dealing with a large number of cameras, geographically distributed resources and several monitoring points. Such factors allowed the acquisition of more accurate information by combining different types of sensors and the distribution of the information. The difficulties are in achieving efficient information integration and communication, the establishment of design methodologies, and the task of designing and deploying multi-sensor platforms.

The current research on the third generation concentrates on distributed and centralized intelligence, data fusion, probabilistic reasoning frameworks, and multi-camera surveillance techniques [16]. According to Räty [20], the main objective of the fully third generation system is to provide efficient data communication, management, and extraction of events in real-time video from a large collection of sensors. To achieve this goal, improvements in automatic recognition functionalities and digital multi-user communications are required.

Several surveillance systems of the third generation have been designed and developed both in the industry and in the academia. These systems can be classified into two groups: general purpose and specialized in a certain function. The framework proposed in this work can be classified as general-purpose system because the user (researcher) has the freedom to develop his/her modules (as described in Section 4.6) and use them for any purpose involving surveillance.

Several technologies for video-based surveillance have been developed under a United States government funded program called Video Surveillance and Monitoring (VSAM) [21]. This project, which can be considered one of the pioneers among the third-generation systems, designed a testbed system to demonstrate how automated video understanding technology can be combined into a coherent surveillance system that enables a single human operator to monitor a wide area. It looked at several fundamental issues in detection, tracking, auto-calibration, and multi-camera systems. The goal of VSAM testbed system was to develop efficient wide-area video surveillance systems using a distributed network of cameras. Similar to other newer systems, the SSF incorporates several concepts based on VSAM project, such as scalability, modularization and code reuse.

Knight [22] is a fully automated system with multiple surveillance cameras that detects, categorizes and tracks moving objects in the scene using computer vision techniques. Although it can be used in various types of surveillance environments, the Knight is a closed framework that does not allow the implementation of new methods to replace or extend to the existing ones.

Another system is the IBM Smart Surveillance System (S3) [23], which is among the most advanced surveillance systems nowadays. It provides the following capabilities: automatic monitoring of a scene, management of surveillance data, perform event based retrieval and receive real-time event alerts. In S3, computer vision routines are not implemented directly into the system, but as plugins. One of its disadvantages is that it requires the use of technologies from IBM, such as IBM DB2 and IBM WebSphere, which reduces its applicability for research purposes.

San Miguel et al. [24] and Suvonvorn [25] proposed two general-purpose frameworks for processing and analyzing surveillance videos. Similarly to the SSF, they enable the development of modules for processing images and videos. However, they have adopted a different approach for data communication between the modules. In [24], the communication between modules is mapped through a database system, while in [25], the modules communicate directly, where a buffer is used as an exchange zone. In contrast, modules in the SSF do not communicate directly, but through a shared memory, which allows modules to be launched in an asynchronous way and the dependency among them can be defined as parameters, making the SSF flexible.

The work proposed by Afrah et al. [26] addresses two aspects in the development of vision-based systems that are not fully exploited in many current frameworks: abstraction above low-level details and high-level module reusability. They proposed a systematic classification of subtasks in vision-based system development. However, this framework is inflexible in according to the exchange of modules, preventing researchers from comparing results obtained by different methods, which would be an important feature for the academic community.

With a proposal similar to the SSF, the work proposed by Wang et al. [27] presents a vision system architecture that can readily integrate computer vision processing and make application modules share services and exchange messages transparently. The model of computation assumed by the authors is the same used in the SSF. In this model, modules communicate with each other through a shared memory and are executed independently and in parallel.

Despite their similarities, there are some key difference between the two approaches: (i) In Wang et al. [27] the processing is centralized for some tasks, such as capturing sensor data, encoding and decoding video streams, and transforming different types of data, but on the SSF all processing is performed in parallel on modules, which allows a better use of the processing power; (ii) the shared memory on the SSF stores the scene information in a hierarchy based on the necessary structures for surveillance environment to avoid data redundancy, allowing low memory consumption (for more details, see Section 4.2); (iii) the SSF allows one to perform complex queries on data in shared memory through the Complex Query Server (CQS) (Section 4.4).

Another aspect that differentiates SSF from other systems is that SSF implements the Feature Extraction Server (FES), described in Section 4.3, which allows the feature extraction to be performed using the entire computational power available in the system with the objective of maximizing the performance (one can use all available CPU cores). Even though feature extraction plays a central role on the surveillance algorithms, it does not receives special treatment in the other systems reviewed, being under the user‘s responsibility.

To design efficient systems, it is necessary that researchers understand the nature of the environments in which the systems will be used. Another issue is to be able to interpret the requirements of the end user. Several authors [16,19,28,29] classified real-world applications into the following monitoring categories: public areas, interior and exterior of buildings, transport, military, entertainment and efficiency improvement.

There are several papers published on specific-purpose surveillance systems. The work of Xia et al. [30] that focuses on wide-area traffic monitoring for highway roads. Odobez et al. [31], in turn, designed a metro station monitoring system that aims at automatically detecting dangerous situations which may lead to accidents or violence. The system proposed by Thornton et al. [32] allows an operator to search through large volumes of airport surveillance video data to find persons that match a particular attribute profile. Siebel et al. [33] especially deal with the problem of multi-camera tracking and person handover, on metro stations. A framework for people searching, where the user can specify personal attributes through queries such as “Show me the bald people who entered a given building last Saturday wearing a red shirt”, was proposed by Vaquero et al. [34]. It is important to note that, many surveillance applications are of commercial license, and thus, there are no scientific sources that describe them.

As mentioned earlier, surveillance systems of the third generation contribute significantly to the design of various types of secure environments. Meanwhile, along with improvements, several challenges have emerged, causing many researchers devote their studies to do so. The work published by Liu et al. [35] discusses some challenging issues faced by researchers. Other papers addressing the challenges of smart surveillance systems have also been published recently [19,20,29,36].The next paragraphs present an overview on these challenges.

Images are not always perfect in such systems. For instance, objects of interest can be partially occluded, camera lenses maybe covered or damaged, the person being identified may have covered him self/herself by purpose. Even when these problems do not exist, there are other aspects causing decreasing the image quality, such as, poor illumination, sensor noise, particularly in poor lighting conditions and low resolution of the cameras.

A large-scale video surveillance system comprises many video sources distributed over a large area, transmitting live video streams to a central location for monitoring and processing. Contemporary advances in video sensors and the increasing availability of networked digital cameras have allowed the deployment of large-scale surveillance systems over existing network infrastructure. However, designing a smart and scalable surveillance system remains a research problem: how to design scalable video surveillance systems according to aspects related to processing power, memory consumption and network bandwidth?

According to Fleck and Strasser [37], the privacy is a fundamental and very personal property to be respected so that each individual can maintain control of the flow of information about himself/herself. According to Gilbert [38], privacy comprises confidentiality, anonymity, self-determination, freedom of expression, and control of personal data. In the surveillance environment, it is important to guarantee privacy, as persons within a perimeter covered by cameras have very little choice of being filmed or not, whereas e.g., in the case of cell phone tracking the user still has the choice to turn his phone off. An example of functionality able to maintain the privacy of individuals is mask out some portions of the image [39].

According to Haering et al. [36], one of the major challenges of developing a smart surveillance system is that it has to operate robustly during the entire time in a in wide range of scenarios. The only way to ensure robust and reliable performance is to perform extensive testing.

The following questions are relevant for system evaluation. Is it possible to establish a repository containing some common surveillance scenarios? Who are the people providing these scenarios, and what are the evaluations criteria? To answer these questions, Venetianer and Deng [40] discuss some of the major challenges involved and provides a case study for addressing the evaluation problem.

For algorithms in other areas, such as machine learning, there are standard data sets to validate, evaluate and compare the algorithms. However, for visual surveillance systems, each security concern is different, the objects being recognized and events being detected are more specific according to the application. Therefore, it is a very difficult task to evaluate a complete surveillance system from a case awareness viewpoint [35].

The performance evaluation of video analysis systems requires significant amount of annotated data. Typically, annotation is a very expensive and tedious process. Additionally, there can be significant errors in annotations and part of the evaluation of the surveillance systems depends on what the system operator considers as relevant action since they are not objective. All of these issues make performance evaluation a significant challenge [29].

Written in C/C++, using Open Source Computer Vision Library (OpenCV) and C++ Standard Template Library (STL), the Smart Surveillance Framework (SSF) is a tool built to provide a set of functionalities to aid researchers not only on the development of surveillance systems, but also on the creation of novel algorithms for problems related to video surveillance, such as those presented in Section 2.

The proposed framework allows research works to focus only on their problems of interest without the necessity of creating an infrastructure for every problem that will be tackled, as it is done in the majority of cases nowadays. By using the SSF, the researcher can concentrate only on the problem at hand without concerning with the design of data representation, storage, communication and parallelism.

The SSF has been designed to allow the development of third generation surveillance systems, providing features as tools to perform scene understanding, scalability, real-time operation, distributed multi-sensor environment and communication control, as discussed in Section 3.1. The next sections describe the design choices of the SSF to provide such desirable features.

The architecture of framework can be divided into two main parts: user modules and SSF kernel, as illustrated in Fig. 4
                        . While the former is where the user implements his/her surveillance and computer vision algorithms, the latter, responsible form controlling data communication, parallelism and data structures, lies outside of the user domain, being accessible only through configuration parameters.

The SSF kernel is composed of the following components. (i) shared memory: the backbone of the SSF, it allows the communication among all other components and stores the data generated by user; (ii) Feature Extraction Server (FES): it processes feature extraction requests and return feature vectors to user modules to maximize the occupancy of the processing units available; (iii) Complex Query Server (CQS): this component allows user modules to search for specific data in the shared memory by using Prolog or queries in Structured Query Language (SQL) databases, for instance; (iv) execution control: this component controls the execution of the others SSF components and is responsible for the SSF initialization. In addition, this component has a graphical interface to aid the user to configure the run-time environment.

The user modules are components written by the researchers to solve surveillance and computer vision problems (in fact, any algorithm can be implemented in the user modules). These modules use a well-defined interface to communicate with the the kernel components and the concept of data stream to communicate to other modules through specific data types (SMData).

To the user’s perspective, the communication between modules does not exist directly because when implementing his/her module, the user requests data types as input without specifying which module will provide it and provides data types as output also without specifying target modules. The actual communication, controlled by the shared memory, is only set in the execution time when the user specifies the input and output modules. This communication scheme, known as publish-subscribe messaging pattern [41], allows the reuse of modules as components of applications with different goals and increases the flexibility of the framework once the modules with the same purpose are interchangeable.

To achieve a flexible and modular software architecture, it is necessary that the modules be designed independently without knowing each other interfaces, which would reduce the flexibility when integrating a set of modules to solve a given task. Therefore, to address this constraint, the SSF provides a resource to store data and control of the data communication between the user modules. Such feature is referred to as shared memory and is responsible for the data communication control. This way, the modules only need to know the interfaces provided by the shared memory and not each other specific interfaces.

The shared memory was designed to enable the development of different types of applications, including applications that are not in the visual surveillance purpose (outside of the scope of this work). To accomplish that goal, the shared memory is composed of four components as illustrated in Fig. 5
                         and described as follows (the last component, the image and feature managers will be described in details in Section 4.2.3).

The first component, called Specialized Shared Memory, is a specialization of the shared memory, with surveillance purposes. This component provides methods and specific data types for the surveillance domain (the data types are described in Section 4.2.1) and is available when the researcher is developing user modules.

The second component is the Basic Shared Memory, responsible for the functions to access the data. This component does not depend on the context of the application, that is, their interface functions are general (i.e., functions to read and write data items) and have no knowledge of the data type being manipulated.

The third component, called Memory Management, is responsible for the storage and management of the handled data. In the SSF, the data items are created by user modules and their references are passed on to the shared memory and the Memory Manager becomes responsible for the management of these references and their contents, releasing the user from this tedious and intricate task.

To focus on the surveillance domain and to avoid data redundancy, the shared memory stores the data scene information in a hierarchy manner, as illustrated in Fig. 6
                           . All data information are store in lists and only their references on the lists are stored in the hierarchy elements, which not only reduces the data redundancy, but also avoid the need for updating the information when data structures are changed.

The hierarchical data structures available on SSF to represent the scene under surveillance are described in Table 2
                           . All these data structures inherit from the class called SMData. This class contains the creation timestamp and a unique and unchangeable identifier, the latter is used as reference between data entries.

For the information that cannot be represented as one of the aforementioned data structures, the SSF allows the creation of new data structures by inheriting from SMData, referred as User data. This special type enables specific data definition such as sensors output (audio, temperature, multi-spectral images) or exchange of specific data types between modules.


                           Fig. 5 also illustrates the references between the data stored in shared memory. For instance, the samples of a frame are not stored directly within the frame data structure but only references between the frame data structure and their samples are used. One might also note the presence of links in frame data structure, that is because the content of images and feature vectors is passed to managers that provide a better memory management, as it will be discussed in Section 4.2.3.

As described in Section 4.1, the SSF implements the publish-subscribe messaging pattern for communication. In this pattern, the senders (publishers) do not send messages (data items) to specific receivers, but instead, their messages are characterized into classes defined by the sender identification and the data type being transmitted, e.g., the tuple (Input, Frame) defines uniquely the output of module Input, showed in Fig. 7
                           . Then, the receivers (subscribers) define the class of messages that they are interested in receiving, e.g., in Fig. 7, module Display expresses its interest in receiving messages defined by tuples (Input, Frame) and (Detector, Sample), this way it can shows in the screen the input frames overlaid with the bounding boxes provided by the Detector module.

Since the publish-subscribe messaging pattern does not perform direct communication between the modules, i.e., the subscriber has to express its interest for a piece of data, the data provided by the publishers are stored in the shared memory and the actual communication between modules is performed by using the concept of data stream. First, when a module is designed, the researcher must define the input and output streams of module. A data stream is simply the definition of a class of messages, e.g., Stream(Input, Frame), in Fig. 7, indicates that module Input will provide frames. In a later moment, when the framework will be executed, the user must configure the connection between the inputs and outputs of the modules, according to data type compatibility.

The great advantage of using data streams defined by the publisher and the data type is the flexibility brought to the framework. For instance, an user module that performs face recognition receives samples (regions of the image containing a face) as input. Using the SSF, this module only has to define Stream(publisher, Sample), in which the variable publisher will be instantiated only on the execution time with the identification of the face detector module that will be used. Therefore, multiple face detection approaches could be easily evaluated without any change on the source-code.

Despite the communication between modules using data streams might appear direct from the user’s point of view, it is in fact performed through the shared memory. First, the data is written in the shared memory by the publishers and then, the subscribers request specific data. Therefore, all communication control and synchronization of the streams is performed by the shared memory.

The use of a shared memory avoids the data replication. But no only that, another benefit of this design decision is that it is incremental in the sense that when a new data item is stored, it receives a new and unique identifier together with a creation time stamp. This allows one to trace back the entire lifecycle of stored data. For instance, one could verify when tracklets were merged and when new objects were created, which might be useful in the development of novel object tracking and recognition approaches.

Given that user modules are executed asynchronously, it is the responsibility of shared memory to perform data synchronization because a module might consume information faster than another module can provide. Fig. 8
                            illustrates an example of synchronization between two modules (M
                           1 and M
                           2) and the shared memory (SM). In the first instant of time (t
                           1), M
                           1 writes a new data item to the SM while M
                           2 reads and processes the current data item in the memory. At time t
                           2, the module M
                           1 is processing, while M
                           2 reads and processes the only available data item. Since there is no more data to read in t
                           3, M
                           2 is locked until a new information is made available, which occurs in t
                           4. Finally, at t
                           5, M
                           2 is unlocked and performs a new reading. Therefore, by using locking mechanisms in the data reading, the SM is able to synchronize dependencies among modules without compromising the performance of independent modules.

Since surveillance systems must handle large volumes of data, the memory on the SSF host machine can be easily filled. The data types with higher memory consumption in a surveillance system are the images generated by cameras and the extracted features. For instance, a video feed being recorded for hours can easily use the entire RAM memory on a common desktop. In addition, another significant factor is that depending on the problem to be solved, the number of extracted features can be large, which also consumes large amounts of computer memory. The other data types usually only have metadata or small integers such as the location of a bounding box or the identifier of the object. Therefore, their impact in the memory consumption is low.

To deal with the high memory consumption by images, the shared memory has a mechanism, called Image Manager, capable of storing the captured images into the disk when necessary. To improve the computation performance, the Image Manager has a cache (with size set by the user), that from the principle of temporal locality
                              4
                           
                           
                              4
                              Once a location is referenced, there is a high probability that it will be referenced again in the near future.
                            
                           [42], stores the last manipulated images.

To control the amount of allocated memory, each frame data has a reference to the manager, instead of having the actual data, as illustrated by the links in Fig. 5. The Image Manager, basically has two operations. The first is responsible for storing the image on the disk every time a new frame data is inserted into the shared memory. While the second operation, retrieves the disk image corresponding to a frame, when requested by the user module. Even though simple, these operations are performed in an intelligent way. For instance, if the image is already stored somewhere in the disk, instead of saving a copy, it just point to that original image.

Since feature descriptors also consume large amounts of memory, the shared memory has a similar mechanism to Image Manager, called Feature Manager to handle the extracted feature vectors. It is important to note that both managers work transparently for the user, which manipulates images and feature descriptors as usual.

The proposed managers not only dramatically reduce memory consumption, as described in Section 5.3, but they also allow the user to control the amount of memory consumed by the framework. The lack of such mechanisms would prevent the tracing of the entire lifecycle of stored data, which might be important when developing surveillance algorithms and applications as mentioned earlier.

Feature extraction is critical for surveillance systems since several algorithms require feature descriptors as input. However, most feature extraction algorithms are highly time consuming and not suitable for real time applications. Researchers have also devoted their studies to optimize the feature extraction methods. One of the early works was proposed by Viola and Jones [43], the integral image, an intermediate representation that allows faster computation of rectangle features. Dollar et al. [44] proposed linear and non-linear transformations to compute multiple registered image channels, called Integral Channel Feature. Authors employed these descriptors into their ChnFtrs detector achieving state-of-the-art results in pedestrian detection. Another approach is the use of parallel architectures, as multi-core processors and Purpose Graphics Processing Unit (GPGPU), for feature extraction. For instance, Prisacariu and Reid [45] showed in their work efficient ways to extract Histogram of Oriented Gradients (HOG) descriptors using GPGPU, achieved speedups of over 67 × from the standard sequential code.

To address the feature extraction problem, the SSF provides a powerful tool: the Feature Extraction Server (FES). It allows the feature extraction to be performed using the entire computational power available in the system to maximize the performance (one can use all available CPU cores). More specifically, researchers implement their feature extraction methods based on a template class and the feature extraction server will be responsible for splitting the task among the available processing units.

The feature extraction server relies on an asynchronous approach to receive requests, process them and return feature vectors to the user modules with the objective of maximizing the occupancy of the processing units available. Once a request has been sent to the FES, it does not block the processing being executed in the module, which can continue working while the request is been processed by the FES. For instance, the module might be processing the feature vectors already extracted while others are being extracted. Therefore, all features vectors do not need to be stored in memory before processing, preventing from high memory consumption. In fact, the maximum amount of allocated memory can be set to avoid the process from using the virtual memory.


                        Fig. 9
                         illustrates the main components of the feature extraction server: request control, extraction method and feature extraction memory. Using FES, a feature extraction request is performed as follows. First, a module sends extraction requests by passing image regions from which the features will be extracted by a given extraction method. Such requests are sent to a queue in the request control, which allows the module to make all requests for an image and continue its processing while the features are extracted. Then, the request control selects the extraction method chosen by the module and forwards the requests to the extraction method, which process them using N instances (N is defined by the user). First, it checks the memory availability in the feature extraction memory, if no memory is available, the extraction method waits until some memory has been released. Finally, once the feature extraction is completed, the feature vector is pushed to the output queue and it is ready to be retrieved by the requesting module.

The request control is responsible for screening the requests made by the modules. It is composed of an input queue a data structure for storing information regarding the feature extraction methods available. Once a request enters the queue, the request control forwards it to the correct feature extraction method. The request control is useful in the sense that the feature extraction becomes centralized, such that two modules requiring the same feature extraction method will use the same instance of the extraction method, which will allow the usage of cached features if two modules request feature extraction for the same image region.

The extraction method manages the feature extraction for a specific feature descriptor, such as HOG, Gray-Level Co-occurrence Matrix (GLCM) and others [46]. When the extraction method receives a request, it first verifies in the cache if the same request had been made before and the feature descriptors are already available, if so, return them, otherwise it checks in the feature extraction memory whether there is memory available in the feature extraction memory (experiments show that the usage of cache reduces greatly the computational cost for feature extraction, see Section 5.4.2.).

The feature extraction memory allows the FES to set a limit of memory that can be used for the feature extraction process, otherwise the entire memory available in the machine could be consumed quickly compromising the execution. If there is no memory available, the extraction method is blocked until some memory is released (some module retrieves an extracted feature vector from the output queue, process it, and sets it as released), otherwise, it sends the request to one of its instances to perform the actual feature extraction for an image region.

The advantages provided by the feature extraction server include the following. Besides of using methods already implemented, the user can implement his/her own feature extraction methods which will have their processing distributed according to the computational power at hand or according to the parameter setting chosen by the user. In addition, it allows users to develop novel feature descriptors and evaluate them easily on problems related to surveillance, such as detection and recognition. Finally, this centralization approach based on a server to extract features allows the caching of features vectors so that several modules might share the same vectors for different purposes.

To search for specific data, such as actions being performed in a given time interval or tracklets intersection of two given subjects, one may retrieve data from the shared memory by implementing the query in a module. However, such approach may be inefficient since the architecture of the shared memory is optimized for simple write and read requests. To allow user modules to search efficiently for specific data in the shared memory, the SSF provides the Complex Query Server (CQS).

CQS is independent of the underlying query/inference solution, for instance Relational or Big Data Databases and logic programming such as Prolog. Therefore, the user modules are not required to know how to write a query in a specific solution. To achieve this independence, the CQS defines a common interface with modules so that each complex query solution underlying must implement this CQS common interface which either may be simplified to allow easily integration with as many underlying solutions as possible or may also be complete enough to easily allow complex queries. Any implementation of an underlying query/inference solution in the CQS common interface can be performed by implementing initialization, storing and querying methods. The following paragraphs describe how these methods are used in the SSF.

The initialization method requires that the user informs which fields for instance, time-stamp of image, location of sample and time interval of a tracklet, will be stored in the CQS for future search. This information is given at the definition of each data type and allows the framework to grow in a scalable way, i.e., without modifying CQS structure when new data types are incorporated to the framework.

At execution time, the CQS initializes by iterating over each data type and registering the searchable fields. This initialization is required in some solutions to create underlying structures such as tables in SQL Databases. Then, the CQS retrieves data items from the shared memory and passes them to storing methods so, they can be registered in the underlying structure, a row in SQL Database or a fact in Prolog, for instance.

For the querying methods, a user module retrieves a copy of a CQS instance with access only to query methods. Query methods are subdivided into filter and retrieve methods: filter methods are simple operations (“equal to”, “less than”, “or”, among others) that receive field and data types and change the internal state of the CQS instance by building a partial filter of the field and integrating it with the previously state; retrieve methods return data to the user considering the filtered state of the CQS instance.

As an example, suppose that one is interested in recognizing a fighting activity between two subjects by analyzing the output of an identity recognition module and an action recognition module. The fighting activity is characterized by two subjects, close together, facing each other, and at least one of them is performing punching actions. Examples of queries to identify this fighting activity are given in Prolog, SQL and in CQS query format in Fig. 10
                        . In this example, tracklets are represented by horizontal lines and the action being performed is shown inside a rectangle. A fighting activity may be characterized by any two subjects that are close together facing each other and at least one subject is performing punching actions. The query result R is the reference to the video segment containing the action.

The SSF components that will be used for an execution are chosen by parameter settings, which increases the customization of the framework. The parameters might be supplied via a configuration file or assigned through the Graphical User Interface (GUI). Once the configuration file is provided, the Execution Control is responsible for initializing the remaining components and for assigning values to the parameters.

The SSF first initializes the internal components (i.e., FES and CQS), by assigning values to its parameters. Then, the instantiation and configuration of the parameters of the user modules is performed. It is worth noting that only the modules that are listed on the configuration are initialized. The execution control also defines data flows referred to as data streams, between modules and shared memory. These streams are declared in the configuration file (or in the GUI), in which the user defines how the modules will communicate with shared memory, stating which types of data will be transmitted, according to those that are implemented in the user module.

Due to the large number of parameters, the configuration file becomes complex and difficult to maintain. Thus, to deal with this problem, the SSF provides a Graphical User Interface (GUI) component. Its goal is help the user to configure the runtime environment for the SSF. Through it, we can perform the following tasks: (a) configure modules defining the parameters values; (b) create and setup pipelines; (c) define the data flow between the modules and/or pipelines; (d) configure the SSF internal components, such as the shared memory and CQS.

The user modules are the framework mechanism where the researcher implements his/her algorithms of typical routines of a surveillance system, such as person detection, background subtraction, face recognition, person tracking and re-identification, and action and activity recognition.

Every module follows the same standard interface, in which the user (researcher) defines its input and output data types and its parameters without specifying which module will provide or receive them. This is done later, in execution time by reading the dependencies from a parameter file (or the GUI), which makes the framework highly flexible and versatile. Once the module is launched, an execution routine (where the user implement his/her method), is called and executed.

To illustrate the design of an user module, Fig. 11
                         shows the source-code for a background subtraction module. The user must create a new class, in this example called ModBGS, inheriting from the UserModule class and implement a set of methods, described as follows.


                        Class Constructor (lines 6–10): This is where the user declares which parameters are used and which data types are required and provided by the module. The example module uses an integer parameter (lines 3 and 7), requires a frame as input (line 8) and provides another frame as output (line 9). In execution time, after reading the parameters, the execution control (Section 4.5) will set the values of the variable inMod with the name of the module that will generate the input frame. Therefore, with such information and the type of the data (frame in this example), it is possible to define the data streams for this module, as discussed in Section 4.2.2.


                        Setup (lines 12–15): This method performs consistency checks. The example shows a test where the only module parameter must be a positive integer, in case of failure, an error message will be showed to the user and the execution will be aborted until the parameter values be corrected.


                        Execute (lines 17–27): This is the main method of the module and is where the user implements his/her algorithm. The example first instantiates the data streams for the input and output. Line 18 creates an input stream, whereas the line 19 creates an output stream, both of frames (SMFrame type) – note that the output stream does not specify the module name, meaning that any module can read the frames provided by this module. Then, specific smart pointers to manipulate SSF data types are declared. These pointers will make reference to the input image, as well as the image resulting from operation. Finally, the iteration loop of lines 22–26 performs the background subtraction.

In the main loop of the source-code showed in Fig. 11, for each image received from the input stream (line 23), the background subtraction operation is applied (line 24) and then the resulting image, bgsMask, is sent to the output stream. The auxiliary function BGSAuxFunction should be implemented in advance by the user. The iteration loop is finished when there is no more input images (input module stop providing frames).

It is important to note that when the execution control (Section 4.5) calls the Execute method, it first launches a new execution thread, which increases the performance of the system, and the lifetime of this thread is valid while its is within the Execute. After that, the thread will be terminated and the execution of the module will be finished. Therefore, the user is responsible for reading the data received in the input, for instance as performed in line 22 of the source-code in Fig. 11, in which the module keeps reading the input stream while there is data available.

Another important feature related to modules, is the creation of execution pipelines – collections of user modules behaving as a single module. A pipeline allows one to group several modules of individual methods in a sequence. Once defined, multiple instances of the pipeline can be launched just by changing their inputs. For instance, one pipeline can be launched to process data from each surveillance camera attached to the system. Such a feature also makes the framework more scalable.

A demand that the framework user might have during the development of surveillance algorithms and applications is the creation of cyclic pipelines, as illustrated in Fig. 12
                           . An example is the on-line learning, where results of the execution of the algorithm are re-used by the learning method to improve results. However, such synchronization might not be easy in a system based on data streams because before updating, a module might need to know if some another module has finished processing some piece of data.

To make the necessity for synchronization clear, let us consider part of the pipeline in Fig. 12, assuming that module M
                           3 receives frames and performs object detection, module M
                           4 receives samples and performs non-maximum suppression, and module M
                           5 performs filtering to identify which samples present very high confidence as object and passes to M
                           3 only those samples, which will be used to update the object model in module M
                           3 after the detection be performed for each frame. However, the problem is that module M
                           3 does not know when it should update the model because it outputs n samples and it will receive back m samples, where m ≤ n. To allow this type of synchronization, we developed a special mechanism called pulse.

The pulse works as follows. When a module, referred to as Feedback Module, needs to wait for other modules in the pipeline to finish part of the processing, it will generate a pulse that will be propagated through the pipeline and returned back to the feedback module. When the pulse is propagated, the processing for the feedback module is locked until it receives the pulse back.

Using the previous example, after all samples have been generated by M
                           3 for a frame f
                           1, M
                           3 sends a pulse and stop processing waiting for the pulse to return. The shared memory will only deliver the pulse to M
                           4 when all samples of f
                           1 have been processed, i.e., when M
                           4 tries to read a sample related to the next frame. At this point, the pulse will be propagated forward and will reach module M
                           5 only after all samples related to frame f
                           1 have been propagated to M
                           5. Based on the same idea, the pulse will be propagated to M
                           3 only after all samples related to f
                           1 have been processed by M
                           5. Therefore, when the pulse reaches M
                           3, the samples for the frame f
                           1 will already be processed and the processing of M
                           3 can be resumed.

The modules comprising the cyclic pipeline does not need to have knowledge of the pulse, because the pulse is automatically transmitted between modules through the shared memory. In addition, the pulse is propagated to other modules in the pipeline, e.g., M
                           6 and M
                           7, which are not in the cycle and will not interfere in the processing. The reason is that the feedback module is not aware of the existing pipeline (the data could come back to module M
                           3 through module M
                           7). Thus, this design choice maintains the flexibility of the framework to add and remove modules from cyclic pipelines.

@&#EXPERIMENTAL RESULTS@&#

This section evaluates important aspects of the framework proposed in this work. Section 5.1 explores the framework scalability, Section 5.2 evaluates the communication latencies caused by the architecture based on the shared memory, Section 5.3 discusses the benefits of employing the image and feature storage servers, and Section 5.4 discusses the performance of the Feature Extraction Server.

All experiments were conducted using computer with two Intel® Xeon™ 2.40GHz processor with 6 physical cores each, 32GB of RAM memory and running Windows™ 7 operating system.

This section presents two experiments to demonstrate the scalability of the framework. The SSF allows the user to perform parallelization of methods by decomposing a problem into independent sub-problems. Two general methodologies are commonly used. The first, termed data decomposition, assumes that the overall problem consists in executing computational operations to one or more data structures and, further, that these data structures may be divided and operated upon. The second, called task decomposition, divides the work based on different operations or functions.

The SSF supports both aforementioned decomposition approaches. The task decomposition by the decomposition of a large problem into smaller parts, which in turn, are implemented through the user modules and the data decomposition by the instantiation of several modules, with the same purpose, where, each user module will handle a portion of the data [47].


                        Fig. 13
                        a shows a problem being treated in the traditional way, i.e., sequentially. In this approach, each input data is processed sequentially and individually. In the SSF, this approach is equivalent to solve the entire problem inside a single user module containing all problem tasks, for instance, a module performing the detection and recognition tasks.


                        Fig. 13b shows a task decomposition of the problem shown in Fig. 13a. In this case, each sub-problem is addressed in parallel and various subsets of the data are processed simultaneously in a pipeline composed of specific modules for each task. The implementation of specific user modules for each task enables the implementation of this decomposition type on the proposed framework. From the above example, the detection and recognition would be implemented in separate modules allowing them to run in parallel, in which as soon as partial results are available from the first module, they will be processed by the second module, while another part of the data is already been processed by the first.


                        Fig. 13c illustrates the data decomposition approach for the same problem. Here, the entire problem is replicated, the data is partitioned and each replica is responsible for a subset of the data. The SSF enables the decomposition of data through the pipeline replication.

The aforementioned decomposition techniques are not exclusive and can often be combined, a decomposition commonly implemented in the SSF. It allows not only data decomposition but also task decomposition at the same time, taking fully advantage of the processing power of multi-core processors.

The following experiments demonstrate the application of parallelism on the SSF through the approaches of problem decomposition described. As a starting problem (sequential method), twelve series of similar image processing operations were implemented (Canny Edge Detector from OpenCV). This number of sequential operations was chosen because of the number of cores available on the test machine. Therefore, the sequential method has as input an image I and a series of operations pi
                        , where the computational cost C of all operations is similar. In other words, 
                           
                              C
                              
                                 (
                                 
                                    p
                                    i
                                 
                                 )
                              
                              =
                              C
                              
                                 (
                                 
                                    p
                                    j
                                 
                                 )
                              
                              ,
                           
                         for all values of 1 ≤ i, j ≤ 12. A data set containing 10, 000 images of 640 × 480 pixels was used.

First, we evaluate the data decomposition on SSF. In this experiment, the sequential method has been implemented as a single SSF module and replicated n times, as illustrated in Fig. 13c, where n represents the number of cores used. Each instance of the method is responsible for processing 10, 000/n images from the data set, where n was varied from 1 to 12 and each experiment was executed ten times. Fig. 14
                            reports the average execution time and speedup achieved by the data decomposition approach.

According to Fig. 14, it is advantageous to use of the framework to parallelize the processing of a considerable number of images. The speedup obtained by the data decomposition approach is very close to linear, demonstrating that the communication overhead caused by the SSF is minimal (Section 5.2 presents a detailed evaluation of the communication overhead caused by the SSF).

This experiment demonstrates whether the use of task decomposition approach is advantageous in the framework. The sequential method is divided into n sub-problems, each implemented as a SSF module. Then, these modules are interconnected forming a pipeline, similar to Fig. 13b. The value of n was varied from 1 to 12, i.e., the sequential method was divided into up to 12 sub-problems. The division of the sequential method in sub-problems was conducted in two distinct ways. In the first, the n modules had the same computational complexity, i.e., the problem was equally divided and the computational load of modules were balanced. In the second way, at least one of the n modules had at least 25% of the operations of sequential method while the other 75% were equally distributed among the 
                              
                                 n
                                 −
                                 1
                              
                            remaining modules.

According to the results presented in Fig. 15
                           
                           , one can observe that the task decomposition is successful when there is a balance between the parts (an almost linear speedup was achieved when the operations were divided equally among the modules). However, when the division into sub-tasks was unbalanced, there is no cost reduction starting from a certain number of instances. Since 25% of the operations are under the responsibility of a single module, the time improvement was observed only for up to 4 instances. After that, there was no performance gain because the time spent by the module that was overloaded will always be greater than or equal to 25% of the total time. This experiment demonstrates that the task decomposition in SSF can be scalable if conducted in a balanced manner.

To evaluate the overhead caused by the communication between the modules and the shared memory, we conducted an experiment in which an image (SSF frame data type) was transmitted through a certain number of modules. For that, a pipeline with n modules was created and each module just forwards the image frame (without performing any processing) to the next module. The time elapsed between the instant at which the first and the last module of the pipeline (Modules 01 and n, respectively) performed the reading of the image was computed to estimate the data latency. Fig. 16 illustrates this experiment.

This experiment considered pipelines with sizes of 1, 3, 5, 7, 10 and 15 modules. In addition, for each pipeline size, executions with 1, 3 and 5 simultaneous pipelines were tested. To perform this experiment, a total of 100 different images were transmitted and the average time spent for each move across the pipeline was calculated. The results are showed in Fig. 17
                        .

According to results shown in Fig. 17, the overhead caused by the increased number of modules connected to the shared memory at the same time is low, even when multiple pipelines are instantiated simultaneously. Although this overhead exists, it is negligible when compared with the processing time of the data, usually orders of magnitude higher.

As mentioned in Section 4.2.3, the Image Manager is a mechanism that aims to control and limit the memory consumed by the SSF. To demonstrate its importance, this section presents a example of how the mechanism allows the framework execution for long periods of time.

Suppose a camera recording images with High Definition (HD) resolution (1280 × 720 pixels) and a rate of 20 frames per second, a reality for the cameras found in the market nowadays. According to the information presented in Section 3.4.2, this camera will generate a total of 2GB/hour. Thus, a current computer desktop with 8GB of memory, would not be able to process more than 4 h of video generated by this camera. Using the Image Manager the memory consumption is limited in accordance with the size of the cache memory configured by user, thereby enabling the same computer to process the images for a much longer period.

This section describes the experiments conducted to evaluate the performance of the Feature Extraction Server (FES). The evaluation was conducted using two widely employed features extraction methods: Histogram of Oriented Gradients (HOG) [48] and Gray-Level Co-occurrence Matrix (GLCM) [49]. Even though there are many other feature extraction methods, we have chosen these two methods because they present different computational cost, allowing us to evaluate different aspects of the Feature Extraction Server (FES). While HOG fast computes gradients for each image pixel and a histogram using integral image, GLCM tabulates an accumulation matrix and extract computational intensive statistics that are used as feature descriptors.

The experiments consist in extracting feature descriptors of an image with a resolution of 640 × 480 pixels, using the aforementioned methods. To represent a realistic scenario, we employ the sliding window algorithm [50], widely used in object detection, to sample the image regions from which the feature descriptors are extracted. This algorithm works by exhaustively scanning an input image to generate a set of coordinates of several detection windows in multiple scales. For this work, we follow the block setup used in [48], in which each detection window is split into 105 blocks and we set the stride and scales parameters to generate a total of 48, 495 detection windows per image. We evaluate the FES regarding two aspects: the performance of parallel feature extraction by increasing the number of extraction instances and improvements obtained by the cache memory in the feature extraction.

To demonstrate the performance of parallelism provided by FES, we conducted experiments based on the number of instances used in the extraction. Each experiment consisted in the execution of a method repeated ten times and varying the number of instances from 1 to 12.

According to Fig. 18
                           , it is possible to observe an improvement in the computational performance as a function of the number of instances used in the FES, which demonstrates the advantage of its usage in multi-core environments. While GLCM presented a proportional reduction in run time on all experiments, HOG achieved a reduction only until six instances and presented a slightly increase in the run time when using more than eight instances.

The speedup achieved with the GLCM method presents a linear growth, demonstrating the scalability of the FES for computationally expensive methods. For HOG, the speedup presented a linear growth up to only three instances due to the overhead present in the FES which is more evident when the method is not very computational expensive. This behavior can be explained by the Amdahl’s Law 
                           [51]. This law, used to find the maximum improvement possible by improving a particular part of a algorithm, states that the maximum speedup (S) achieved by a computer running an algorithm with n threads is given by

                              
                                 (1)
                                 
                                    
                                       S
                                       ≤
                                       
                                          (
                                          
                                             
                                                1
                                                
                                                   s
                                                   f
                                                   +
                                                   
                                                      
                                                         
                                                            p
                                                            f
                                                         
                                                         n
                                                      
                                                   
                                                
                                             
                                          
                                          )
                                       
                                       ,
                                    
                                 
                              
                           where the sf is the fraction of the algorithm that is strictly serial and 
                              
                                 p
                                 f
                                 =
                                 1
                                 −
                                 s
                                 f
                              
                            is the parallel fraction. In other words, the Amdahl’s Law states that a fraction of sequential operations, even in small numbers, can significantly limit the speedup achieved by a multi-core computer. In our case, the extraction method corresponds to parallel fraction (pf) of the program and the speedup provided by the FES is inversely proportional to its value. Therefore, the speedup presented by HOG is worse than the presented by GLCM because the computational complexity of this method is smaller when compared to GLCM.

This set of experiments evaluates the performance gain obtained when the cache memory is used for the feature extraction method as a function of the number of cache entries allowed, the cache size. We performed experiments where each extraction method is individually executed for a different cache with at most C entries, where C ∈ {0, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536} by varying the number of instances in 1, 2, 4, and 8. Each experiment was executed ten times and we report the average.


                           Fig. 19
                           a shows a significant reduction in time for the GLCM compared to results achieved without using cache (near 80% for a cache of size 512), for every number of instances. The improvement is also observed for 1024, 2048, 4096, and 8192 cache sizes. However, starting from 16834 entries, the runtime does not decrease. This is because the number of extracted features is not enough to fill the entire cache.

According to Fig. 19b, the cache utilization also significantly contributed to the performance of HOG. However, this contribution is only observed when the HOG is performed in one or two instances are executed. Unlike the previous results, experiments with 4 and 8 instances for HOG increased the run time due to the overhead caused by competition for cache access since the low computational cost of HOG yields the instances to quickly compute the features and consequently making them wait to have access to write to the cache memory.

In this section, we develop a real-world visual surveillance application based on the proposed framework and evaluate its performance. The whole application was built using Smart Surveillance Framework (SSF) components, such as user modules, Image Manager and Feature Extraction Server (FES). The goal of this application is to monitor an entrance and identify the persons entering the environment trough face recognition.

Face recognition [52] aims at providing the identity to an individual at different locations and times, where the same individual has been previously observed and entered into a database (person gallery). This research field has been one of the main challenges studied in the visual surveillance due to its widespread usage in real problems, such as criminals identification, access control, person tracking and others [8].

The development of this application intends to demonstrate the usefulness and flexibility of the SSF to aid the user/researcher to design solutions to visual surveillance problems. Since our main purpose is to demonstrate how the application can be developed in the SSF, we will not present details of the used algorithms (for more details regarding the algorithms, please refer to [43,53–55]).

While Section 6.1 presents the application design, describing the user modules and components used, as well as the data flow transmitted between them, Section 6.2 discusses the experimental results of application performance tests.

In a face recognition solution, after capturing the frame, it is necessary to locate the subject in the scene and obtain his/her identification [8]. Thus, to design a solution for this problem, our application consists of the tasks described bellow (illustrated in Fig. 20
                        ). For each task, we also present the corresponding user module for them. The details of the modules and the data types used are described in Table 3
                        .

                           
                              i)
                              
                                 Data acquisition (ModVideo): Video frames are captured from a video file and stored into the Shared Memory;


                                 Face detection (ModHaar): The received frames are processed by an detection algorithm to indicate the subject location (samples) in the scene;


                                 Tracking (ModKalman and ModUnenrolled): The trajectory of each individual in the scene is defined. In other words, an inter-frame relationship is created between samples of the same person.


                                 Face recognition (ModPLS): Using a classification algorithm, each individual detected and tracked (object) is compared with others present in the gallery
                                    5
                                 
                                 
                                    5
                                    For this application, we consider that all individuals tested were already enrolled in the gallery.
                                  and its receives a label identification.


                                 Result output (ModScreen): Finally, the recognition results (a rectangle indicating the person face and an identification tag) is displayed to the user.

All user modules and extraction methods used in the design of this application were implemented by members of our research group, the Smart Surveillance Interest Group (SSIG) 
                           6
                        
                        
                           6
                           
                              http://ssig.dcc.ufmg.br
                           
                        , to be used in their research. Thus, it was not necessary to development of any module to build this application, which demonstrates the re-usability of the framework.

In addition of the user modules and SSF data types, this application employs other framework components. The Feature Extraction Server (FES), described in Section 4.3, is used to extract feature descriptors needed by the classification module. Despite of being transparent to the user, the Image Manager (Section 4.2.3) stores the image data into the hard-drive when the predefined memory limit is achieved. The Section 6.2 presents a comparison of the performance when the Image Manager has been activated. The cyclical communication between ModKalman and ModUnenrolled requires the synchronization provided by the pulse mechanism (Section 4.6.1).

Some aspects of the application can be changed by the exchange of employed algorithms. The flexibility provided by SSF enables this type of customization be performed without much effort because it allows the exchange of available user modules just by modifying the execution configuration (through the configuration file or by the Graphical User Interface (GUI)).


                        Fig. 20 shows some examples of user modules exchange. For instance, instead of capturing frames by decoding a video file (ModVideo), we could use a module to reads frame from an image folder (ModImage) or capture the video stream provided by a video camera (ModCamera). The detection module (ModHaar) could be replaced by the ModHOGDetect, which implements the detection algorithm proposed by Dalal and Triggs [48]. Similarly to the user modules, the extraction methods can also be exchanged. For instance, the recognition module could use other available feature extraction method, such as LBP [56] or SIFT [57].

@&#PERFORMANCE EVALUATION@&#

To demonstrate the performance of SSF in the designed application, we conducted two computational experiments. The first evaluates the framework scalability by executing the entire application pipeline varying the number of input videos and the number of processor cores. The second experiment measures the impact, in terms of memory consumption, of Image Manager usage in real-time processing.

As the previous evaluations (Section 5), both experiments were conducted using computer with two Intel® Xeon™ 2.40GHz processor with 6 physical cores each and 32GB of RAM memory. To simulate distinct hardware configurations, we created a virtual machine
                        
                           7
                        
                        
                           7
                           A Virtual Machine (VM) is an operating system or application environment that is installed on software which mimics dedicated hardware.
                         running on XenServer virtualization platform. It allows us to decrease and increase the number of processor cores and the amount of memory available in the experiments.

In the first experiment, we measured the execution time required to run the application with n replicas of the execution pipeline shown in Fig. 20, for 
                           
                              n
                              =
                              {
                              1
                              ,
                              3
                              ,
                              5
                              ,
                              7
                              ,
                              10
                              }
                           
                        . To create the pipeline replicas, it was enough to make minor changes in the configuration file. For each execution pipeline, we used a recorded video with 6120 frames and approximately 10.2 min at 10 frames per second from persons entering in a research laboratory
                           8
                        
                        
                           8
                           To enable multiple executions of the experiments, we use recorded videos instead of live camera feeds. That does not make difference for this experiment since we are only interested in measuring the execution time to evaluate the framework.
                        . We also varied the number of processors of our virtual machine used during the experiment in 2, 4, 6, 8, 10 and 12 and fixed the amount of memory to the total of available memory (32GB).

According to results shown in Fig. 21
                        , it is possible to observe an increase in the speedup as a function of the number of processors used during the execution application execution. The speedup achieved with three or more feeds processed presents a linear growth, demonstrating the scalability of the SSF for this application. When only a single feed is processed, the speedup presented a linear growth up to only four processors because the number of parallel running modules is smaller than the number of available processors. This behavior can also be explained by the Amdahl’s Law 
                        [51], described in Section 5.4.1
                     

The second experiment evaluated the performance gain when we run the application in a low-memory host machine using the Image Manager (Section 4.2.3). We set our virtual machine to 4 GB of available memory and 12 processors. For this test, we switch the video file reader module (ModVideo) by a user module to capture the live video camera feed (ModCamera). In this way, we ensure that the SSF would be fed until the execution was over and at some time the available memory would be all filled. Table 4
                         shows the results of this experiment. It was first measured the frames processed per second before the memory limit has been reached (less than 4GB of memory was used). In this case, the use of the Image Manager (9.04 frames/second) caused a small overhead compared to running without its use (9.22 frames/second). However, when the memory limit has been reached (more than 4GB of memory was used), the employment of Image Manager proved advantages – its achieved 8.56 frames/second compared to 7.31 frames/second without its use (the virtual memory of the operating system was the sole responsible for managing the data), a performance gain of 17%. Demonstrating, therefore, the contribution of the Image Server to manage visual data.

The design of this application along with the performance tests demonstrate the flexibility and scalability of the Smart Surveillance Framework (SSF) in the development and execution of real-world surveillance application. The framework components enable the design various types of projects, from simple prototypes to complex applications, making it into a support tool for the development of visual surveillance algorithms and solutions.

@&#CONCLUSIONS@&#

This work proposed a novel framework to allow further development on computer vision methods and surveillance applications. The architecture of the Smart Surveillance Framework (SSF) allows the simultaneous execution of multiple user modules that can be developed independently since they have communication and synchronization through a shared memory, which contributes to the scalability and flexibility. The framework also provides two important components, the feature extraction server and the complex query server, these components maximize the computational resource usage and facilitate the scene understanding, respectively.

The proposed framework will be made publicly available and besides of making surveillance research using real data and in real-time processing easier, it will also allow researchers to provide their methods (implemented as modules) to be used by other researchers to compare how results. Nowadays, it is difficult to compare results to previously published works since the code is not always available or it is necessary to adapt the code to work on new data sets. By using the SSF, one can provide the source-code (or just its compiled version) of the module to solve a computer vision problem and when another researcher proposes a novel solution, he/she can use that module to compare the results different data sets or to compare the computational cost in the same machine. Therefore, the SSF might also contribute to a more accurate validation of computer vision algorithms, mainly those related to surveillance.

@&#ACKNOWLEDGMENTS@&#

The authors would like to thank the Brazilian National Research Council - CNPq, the Minas Gerais Research Foundation - FAPEMIG and the Coordination for the Improvement of Higher Education Personnel - CAPES for the financial support.

@&#REFERENCES@&#

