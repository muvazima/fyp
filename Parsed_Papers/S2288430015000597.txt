@&#MAIN-TITLE@&#E-quality control: A support vector machines approach

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           This work uses innovative methods in e-quality with the application of SVM.


                        
                        
                           
                           Many potential benefits of e-quality control have been presented and instantiated.


                        
                        
                           
                           The classifier equations are built on the data obtained from the experiments.


                        
                        
                           
                           A detailed analysis is presented for six different case studies.


                        
                        
                           
                           The results indicate the robustness of proposed SVM classification.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Support vector machines

Part classifications

Remote inspection

Networked robotic station

e-quality

@&#ABSTRACT@&#


               
               
                  The automated part quality inspection poses many challenges to the engineers, especially when the part features to be inspected become complicated. A large quantity of part inspection at a faster rate should be relied upon computerized, automated inspection methods, which requires advanced quality control approaches. In this context, this work uses innovative methods in remote part tracking and quality control with the aid of the modern equipment and application of support vector machine (SVM) learning approach to predict the outcome of the quality control process. The classifier equations are built on the data obtained from the experiments and analyzed with different kernel functions. From the analysis, detailed outcome is presented for six different cases. The results indicate the robustness of support vector classification for the experimental data with two output classes.
               
            

@&#INTRODUCTION@&#

It is more likely that the rapid advancements in sensor, computer, communication, and information technologies are bringing about the fundamental changes in manufacturing settings. This includes fully-automated, 100% quality inspection that can process a large amount of measurement data [1–6]. Other production related activities and business functions will be also integrated into the company information management network, which guarantees the instant access to critical production data for enhanced decision making [7–9]. This new approach is referred to as e-quality control, and one of the enabling tools is the ability to predict the variations and performance losses during the various production stages. This means that the traditional quality control scheme, which relies on sampling techniques, would be replaced by the sensor-based, automatic, computerized inspection methods that provide the unprecedented level of data processing and handling. Since the production equipment is integrated into the network, the condition of the machines can be monitored, while the product quality from specific machines can be instantly identified. In order to test the new quality control approach, the authors have developed a networked quality control station. This includes two network-accessible assembly robots, two networked vision sensors, and other ancillary equipment which constitutes the cell. The overall setting of the system is presented in 
                     Fig. 1.

The vision sensors see the part and measure the dimensions. The captured image has 640×480 pixel size and the analysis results are produced by the computer algorithms. Part gauging is established by using a pattern matching technique. For each part, the vision sensor conducts the pre-defined quality control tasks, and sends the information to the awaiting robot. If the part passes the quality standard, it will be picked up by the robot and dropped into the bin. Otherwise, the bad parts will be carried away by the conveyor belt. The picture of entire setup is shown in 
                     Fig. 2.

In the context of e-quality control, the objective of this paper is to apply the machine learning approach in the form of support vector machines (SVMs) to predict the outcome of the part classification. Data obtained from the remote inspection experiments will be analyzed using SVM classifier equations to build a model, which can be used for predictions (i.e., good vs. bad quality). The motivation behind this work is to build robust classifiers, which can sort the incoming parts based on the vision-sensor generated dimensional data into the predefined groups in an automated way.

@&#LITERATURE REVIEW@&#

Data mining, which is also referred to as knowledge discovery in databases, means a process of nontrivial extraction of implicit, previously unknown and potentially useful information (such as knowledge rules, constraints, regularities) from data in databases. In modern manufacturing environments, vast amounts of data are collected into the database management systems and data warehouses from all involved areas, such as product and process design, assembly, materials planning and control, order entry and scheduling, maintenance, recycling, and so on. Different researchers tried to solve the quality control and inspection using various machine learning approaches in an effort to address different types of problems. Automated diagnosis of sewer pipe defects was done using support vector machines (SVMs), where the results showed that the diagnosis accuracy using SVMs was better than that derived by a Bayesian classifier [10]. A combination of fuzzy logic and SVMs was used in the form of Fuzzy support vector data description (F-SVDD) for the automatic target identification for a TFT-LCD array process, where the experimental results indicated that the proposed method ensemble outperformed the commonly used classifiers in terms of target defect identification rate [11]. Independent component analysis (ICA) and SVMs were used as a combination for intelligent faults diagnosis of induction motors, where the results show that the SVMs achieved high performance in classification using multiclass strategy, one-against-one and one-against-all [12]. Fault diagnosis was also done based on the particle swarm optimization and support vector machines, where the new method can select the best fault features in a short time and has a better real-time capacity than the method based on principal component analysis(PCA) and SVMs [13]. Multi-class support vector machines were used for the fault diagnostics of roller bearing using kernel based neighborhood score multi-class support vector machine, where it was shown the multi-class SVM was effective in diagnosing the fault conditions and the results were comparable with binary SVM [14]. Artificial neural networks were used for addressing quality control issue as a non-conventional way to detect surface faults in mechanical front seals, which achieved good results in comparison with the deterministic system which was already implemented [15]. Fuzzy association rules were used to develop an intelligent quality management approach with the research providing a generic methodology with knowledge discovery and the cooperative ability for monitoring the process effectively and efficiently [16]. An automatic optical inspection was adopted for on-line measurement of small components on the eyeglasses assembly line, which was designed to be used at the beginning of the assembly line and is based on artificial vision, exploits two CCD cameras and an anthropomorphic robot to inspect and manipulate the objects [17].

In fact, the very insightful resources are abounded in terms of fuzzy learning with kernels and SVMs. One example includes the learning of one-class SVM, which requires non-labeled data [18,19]. Other studies also utilized the method of non-labeled data, hence being able to operate in a fully unsupervised manner [20,21]. Fuzzy analytical hierarchy process was used to select unstable slicing machines to control wafer slicing quality, where the results of exponentially weighted moving average control chart demonstrated the feasibility of the proposed algorithm in effectively selecting the evaluation outcomes and evaluating the precision of the worst performing machines [22]. Logistic Regression and PCA were the data mining algorithms used for monitoring PCB assembly quality, where the results demonstrated that the statistical interpretation of solder defect distributions can be enhanced by the intuitive pattern visualization for process fault identification and variation reduction [23]. Fuzzy logic was used for the fault detection in statistical process control of industrial processes and the comparative rule-based study has shown that the developed fuzzy expert system is superior to the preceding fuzzy rule-based algorithm [24]. SVMs were used for an intelligent real-time vision system for surface defect detection, where the proposed system was found to be effective in detecting the steel surface defects based on the experimental results generated from over one thousand images [25]. SVMs were also used as a part of the optical inspection system for the solder balls of ball grid array, where the system also gives the training model adjustment judgment core SVM which is efficient for the image comparison and classification [26]. SVMs were used for quality monitoring in robotized arc welding, where the results show that the method can be feasible to identify the defects online in welding production [27]. A defect classification algorithm for the rolling system surface inspection was developed using Neural Networks and SVMs with good classification ability and generalization performance [28]. SVMs along with the wavelet feature extraction based on vector quantization and SVD techniques were used for improved defect detection with the results outlining the importance of judicious selection and processing of 2D DWT wavelet coefficients for industrial pattern recognition applications as well as the generalization performance benefits obtained by involving SVM neural networks instead of other ANN models [29]. Radial basis function (RBF) neural networks (NNS) and SVMs were used for quality monitoring in a plastic injection molding process, where the experimental results obtained thus far indicate improved generalization with the large margin classifier as well as better performance enhancing the strength and efficacy of the chosen model for the practical case study [30]. Two very different studies involve the surface inspection applications, where different approaches can be used within the similar domains of quality inspection [31,32]. 
                     Table 1 summarizes the applications listed in the manuscript.

Although significant amount of literature is published on solving quality related issues using data mining techniques or support vector machines in particular, the concept of addressing e-quality using SVMs remains unexplored. This is due to the fact that the whole idea of e-quality is still in its developmental stages. However, some researchers developed the idea to address e-quality for manufacturing within the framework of internet-based systems. The researchers designed the setup to perform quality control operations over the Internet using Yamaha robots and machine vision cameras. The present work is an extension to this type of work, where the data obtained from these experiments is analyzed using SVMs for predictions. The idea behind using SVMs for this work is solely based on the fact that the performance of SVMs on binary output data is better, when compared to other widely used approaches like the neural networks, principal component analysis and independent component analysis. Most literatures support that SVMs outperformed better than other methods in many quality applications. Note that the objective of this paper is to focus on determining a better classification model, based on the tuning of the parameters among different SVM kernels.

@&#METHODOLOGY@&#

This section explains the model selection for running the experiments using the support vector classifiers, the values of training parameters selected, and the values of parameters used for different kernel functions. 
                     Fig. 3 shows the conceptual framework used as a part of this work.

In training SVMs, we need to select a kernel and set a value to the margin parameter C. To develop the optimal classifier, we need to determine the optimal kernel parameter and the optimal value of C. A k-fold (a value of k=10) cross validation approach is adopted for estimating the value of training parameter. The minimum value considered was 0.1 and the maximum value was 500. The value of C with the high level training accuracy percentage was identified as the optimal value (highlighted in bold characters). The performance of the classifiers is evaluated by using different kernel functions in terms of testing accuracy, training accuracy, a number of support vectors, and validation accuracy. Four different kernel functions are identified for this research based on the knowledge gained from the literature review. They include (1) Linear Kernel, (2) Polynomial Kernel, (3) Radial Basis Function (RBF) Kernel, and (4) Sigmoid Kernel. Polynomial and RBF kernels are by far the most commonly used kernels in the research world. The following section identifies the different parameters involved in all kernels, and also discusses the range for each parameter.


                           
                           
                              
                                 (1)
                                 
                                    k
                                    
                                       (
                                       
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          ,
                                          
                                             
                                                x
                                             
                                             
                                                j
                                             
                                          
                                       
                                       )
                                    
                                    =
                                    
                                    
                                       
                                          x
                                       
                                       
                                          i
                                          
                                       
                                    
                                    ,
                                    
                                    
                                       
                                          x
                                       
                                       
                                          j
                                       
                                    
                                 
                              
                           
                        

It is the inner product of 
                              
                                 
                                    x
                                 
                                 
                                    i
                                    
                                 
                              
                              ,
                              
                              
                                 
                                    x
                                 
                                 
                                    j
                                 
                              
                           , so there is no gamma and no bias.


                           
                           
                              
                                 (2)
                                 
                                    k
                                    
                                       (
                                       
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          ,
                                          
                                             
                                                x
                                             
                                             
                                                j
                                             
                                          
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          (
                                          γ
                                          
                                             
                                                x
                                             
                                             
                                                i
                                                
                                             
                                          
                                          
                                             
                                                x
                                             
                                             
                                                j
                                                
                                                
                                             
                                          
                                          +
                                          coefficient
                                          )
                                       
                                       
                                          degree
                                       
                                    
                                 
                              
                           where 
                              (
                              degree
                              ∈
                              
                              ℕ
                              ,
                              
                              coefficient
                              
                              ≥
                              0
                              ,
                              
                              
                              γ
                              
                              >
                              0
                              
                              )
                           
                        

Two cases are designed based on this kernel, which include the degrees of 2 and 3. Gamma as 2, coefficient as 1, are chosen based on the data. Trial and error method was adopted to find the optimal values, which gives high testing data classification accuracy.


                           
                           
                              
                                 (3)
                                 
                                    k
                                    
                                       (
                                       
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          ,
                                          
                                             
                                                x
                                             
                                             
                                                j
                                             
                                          
                                       
                                       )
                                    
                                    =
                                    e
                                    x
                                    p
                                    
                                       (
                                       
                                          −
                                          γ
                                          |
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          −
                                          
                                             
                                                x
                                             
                                             
                                                j
                                             
                                          
                                          
                                             
                                                |
                                             
                                             2
                                          
                                       
                                       )
                                    
                                 
                              
                           where 
                              (
                              γ
                              
                              >
                              0
                              )
                           .

Two cases are also designed based on this kernel with gamma values of 0.5 and 2. Trial and error method was adopted to find the optimal values, which gives high testing data classification accuracy.


                           
                           
                              
                                 (4)
                                 
                                    k
                                    
                                       (
                                       
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          ,
                                          
                                             
                                                x
                                             
                                             
                                                j
                                             
                                          
                                       
                                       )
                                    
                                    =
                                    
                                    tan
                                    
                                    h
                                    
                                       (
                                       
                                          γ
                                          
                                             
                                                x
                                             
                                             
                                                i
                                                
                                             
                                          
                                          
                                             
                                                x
                                             
                                             
                                                j
                                                
                                                
                                             
                                          
                                          +
                                          coefficient
                                       
                                       )
                                    
                                 
                              
                           where 
                              (
                              γ
                              
                              >
                              0
                              ,
                              
                              coefficient
                              
                              ≥
                              0
                              )
                           .

Values of gamma 0.2 and coefficient 0.1 were chosen for this work based on the data.

After the acquisition of the data from the experiments, the following characteristics were identified. Total number of cases=138 (note: a total number of times that the experiment ran. It includes ‘auto’ and ‘manual’ modes.). Number of input features=5 (note: five features include the five different dimensions of the test piece.). Number of output features=2 (note: two outputs include the cases where the test piece is ‘compliant’ or ‘non-compliant.). As SVMs are a part of the supervised learning methods, the data are divided into the training set and testing set. Going with the standard approach, two-thirds of the data are divided into the training set and the remaining one thirds into a testing set. Accordingly, it has been set as the training data sample size of 92, and the testing data sample size of 46. According to the literature review, in obtaining the support vectors, one needs to solve the equation:
                           
                              (5)
                              
                                 
                                    
                                       maximize
                                    
                                    
                                       α
                                       ∈
                                       
                                          
                                             ℝ
                                          
                                          m
                                       
                                    
                                 
                                 
                                 
                                 W
                                 
                                    (
                                    α
                                    )
                                 
                                 =
                                 
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    m
                                 
                                 
                                    
                                       α
                                    
                                    
                                       i
                                    
                                 
                                 −
                                 1
                                 /
                                 2
                                 
                                    ∑
                                    
                                       i
                                       ,
                                       j
                                       =
                                       1
                                    
                                    m
                                 
                                 
                                    
                                       α
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       α
                                    
                                    
                                       j
                                    
                                 
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       y
                                    
                                    
                                       j
                                    
                                 
                                 k
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       x
                                    
                                    
                                       j
                                    
                                 
                                 )
                              
                           
                        subject to the constraints
                           
                              (6)
                              
                                 0
                                 ≤
                                 
                                    
                                       α
                                    
                                    
                                       i
                                    
                                 
                                 ≤
                                 C
                                 
                                 for
                                 
                                 all
                                 
                                 i
                                 =
                                 1
                                 ,
                                 …
                                 .
                                 ,
                                 m
                                 ,
                                 
                                 and
                                 
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    m
                                 
                                 
                                    
                                       α
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 o
                              
                           
                        Accordingly, one has to estimate the value of the training parameter C. After closely following the literature and going through the published materials, it is decided that the k-fold cross validation method is used to estimate the value of training parameter C.

In this section, the various steps involved in the process of conducting experiments are presented. The results generated from using the classifiers in six different cases are also shown in the form of tables. In addition, the comparison between the performances of different kernel functions on the given data is made and the findings are reported.

The experimental setup considered for this research is in the development stage and as of now it cannot be completely commercialized and used for real-world problems. Due to this reason, a similar scenario is designed, which depicts the real world cases. Instead of parts dealt in a typical production line, small test pieces are designed to conduct the experiment. These pieces are smaller in size, less complicated in dimensions and shapes when compared to the parts used in various types of industries. 
                        Fig. 4 shows the geometry of the test piece used in the experiment, indicating that L: length of the piece; W: width of the piece; D
                        1 and D
                        2: diameters of two circles; and CCL: distance between the circle centers. There are about 20 pieces made. Each differs from the original in at least one dimension, in order to depict that there are defective products in the production line.

The experiment includes recording real-time measurements on sample work pieces (products) that are passed around on a conveyor belt, compare these dimensions to the required specifications, make a decision on the quality of the product (i.e. if it is compliant or non-compliant), and take an appropriate action on the product. We used a Cognex DVT 540 vision sensor for making inspections and measurements on the object under test. We have set the camera to have an image resolution of 640×480 bits with an exposure time of 4ms and a frequency of 2 snapshots per second. The camera is initially trained to learn the profile of the object being tested and make the required measurements on it. Once trained, it can detect the presence of the same kind of object under different orientations. The camera can be addressed using an IP address and is capable of exchanging information with other entities over a data network. Subsequently, during inspections, the camera makes measurements on the objects passing on the conveyor belt and reports it back along with the objects׳ position to an application server over the network. The application is software written in VB6 and runs in a PC. It communicates with the camera over the network and receives the measurement and position information about the object. It then uses this information to decide whether the product adheres to the required specifications. Once a decision is made, it communicates with the robot over the network, and instructs it to it stop the belt and do the proper pick and place operation on the object. The robot places the compliant and non-compliant objects on to two different stacks. The camera placed at the inspection site also allows for visual monitoring of the ongoing process from a remote location. 
                        Table 2 summarizes the specifications for the different type of work pieces used for the experiment. Type 1 constitutes objects that adhere to the required specifications. The other types deviate from these specifications in one dimension.

Based on the previous phase, the experiment is conducted allowing the user to test the different samples. The user has two options to perform this type of experiment, which includes dealing with the robot in manual and auto modes. The data used for this experiment consists of data recorded in two modes. (1) Manual mode: after the test piece is placed on the conveyor system, it stops as soon as it comes right below the DVT vision sensor, which records the dimensions. Through a VB interface, the user is able to see the values and has to make a judgment whether the test piece is compliant or non-compliant. After the user takes a decision the robot is programmed to pick up the object and place it in the respective stack. The process continues until all test samples are put on the conveyor. The significance of this mode lies on the users׳ ability to make the correct decision to classify the object. (2) Auto mode: the process is almost similar to the manual mode except after the dimensions are recorded by the DVT vision sensor camera, the application itself takes the decision, whether the piece is compliant or non-compliant. The VB application has the option to write all the data recorded along with the action taken into an excel file. Most analysis part is done using a statistics software package, called STATISTICA, which provides a selection of data analysis, data management, data mining, and data visualization procedures. 
                        Fig. 5 shows the screen shot of the input data used for the analysis in the STATISTICA 8.0 data mining module.

Results obtained from six different cases are presented in this section. All cases vary in the type of kernel functions used for classification. The cases include: Case 1. Radial Basis Function Kernel (gamma=0.5); Case 2. Radial Basis Function Kernel (gamma=2.0); Case 3. Polynomial Kernel (degree=2, gamma=2, coefficient=1); Case 4. Polynomial Kernel (degree=3, gamma=2, coefficient=1); Case 5. Linear Kernel; and Case 6. Sigmoid Kernel (gamma=0.2, coefficient=0.1).

The classifier equations are tested with the radial basis function kernel using a gamma value of 0.5 and different runs are made using different values of the training parameter. For each run, a number of support vectors generated, while the accuracy of classifying the data in the training and testing sets are noted. The summary is presented in 
                           Table 3.

Values from the table are plotted in the form of a graph shown in 
                           Fig. 6 with training parameter values on the X-axis and accuracy percentages on the Y-axis. Optimal cross validation value is found and highlighted in the graph.

The classifier equations are tested with the radial basis function kernel using a gamma value of 2 and different runs are made using different values of the training parameter. For each run, a number of support vectors generated the accuracy of classifying the data in the training and testing sets are noted. The summary is presented in 
                           Table 4.

The values from the table are plotted in the form of a graph shown in 
                           Fig. 7 using Statistica with training parameter values on the X-axis and accuracy percentages on the Y-axis. Optimal cross validation value is found and highlighted in the graph.

The classifier equations are tested with the polynomial kernel and different runs are made using different values of the training parameter. For each run, a number of support vectors are generated, and the accuracy of classifying the data in the training and testing sets are noted. The summary of all these observations is presented in 
                           Table 5.

The values from the table are plotted in the form of a graph shown in 
                           Fig. 8 using Statistica with training parameter values on the X-axis and the accuracy percentages on the Y-axis. Optimal cross validation value is found and highlighted in the graph.

The classifier equations are tested with the Polynomial Kernel and different runs are made using different values of the training parameter. For each run, a number of support vectors generated, and the accuracy of classifying the data in the training set and testing set are noted. The summary of all these observations is presented in 
                           Table 6.

The values from the table are plotted in the form of a graph shown in 
                           Fig. 9 using Statistica with training parameter values on the X-axis and the accuracy percentages on the Y-axis. Optimal cross validation value is found and highlighted in the graph.

The classifier equations are tested with Linear Kernel and different runs are made using different values of the training parameter. For each run, a number of support vectors are generated, and the accuracy of classifying the data in the training set and testing set are noted. The summary of all these observations are presented in 
                           Table 7.

The values from the table are plotted in the form of a graph shown in 
                           Fig. 10. Optimal cross validation value is found and highlighted in the graph.

The classifier equations are tested with the Sigmoid Kernel and different runs are made using different values of the training parameter. For each run, a number of support vectors are generated, and the accuracy of classifying the data in the training set and testing set are noted. The summary is presented in 
                           Table 8.

The values from the table are plotted in the form of a graph shown in 
                           Fig. 11. Optimal cross validation value is found and highlighted in the graph.

This section compares the results obtained from the different cases. After executing the model with different kernel functions, the results that a specific kernel gives the best accuracy are identified and used for comparison along with other kernels. 
                        Table 9 summarizes the finding.

Additionally, to show the advantage of the SVMs, two other methods that are commonly used have been tested. The results are shown in 
                        Table 10.

@&#CONCLUSIONS@&#

This section presents the important findings that can be drawn from the analyses. The purpose of this research was to develop a support vector classifier model based on the experimental data in order to facilitate the process of e-quality control. The study was conducted under the following assumptions. (1) The data used for analysis contained 138 different cases, which were obtained by running the experiment with different test samples. (2) The model selection for training parameter C was based on the v-fold cross validation approach. (3) The range of training parameter C values included 0.01 to 500, where much higher values in the order of four digits and five digits can also be used based on the characteristics of data. (4) The parameters of the kernel functions were assumed based on the trial and error, to obtain the best accuracy level. After analyzing the data obtained using SVM classifiers and testing the accuracy levels using different kernels, the following conclusions can be drawn. Since the SVMs produced good classification results for data with binary outcome, the results achieved for this data were significant. The highest testing rate of 93% was achieved, when using Linear, Polynomial and RBF kernels in different cases. Polynomial kernel of second degree and RBF kernel with gamma values had slightly higher training rate values. Among all cases, the RBF kernel with a gamma value of 2 is identified as the best performer, as it has the lowest number of support vectors used in the classification method. Heuristically, a less number of support vectors signifies the robustness of the classifier. However, this might not be true in all cases, since it also depends on the number of bounded support vectors, which are located between the margins. The value of the training parameter ‘C’ identified as 376 for the RBF kernel also satisfies the basic necessity for selecting the ideal training parameter. If ‘C’ is too small, the insufficient stress will be placed on fitting the training data. If it is too large, the algorithm leads to over fitting the data. As to the data size, even though the available data are not large, it is adequate for the research. It may have a better result (i.e., a better predict accuracy) with the larger data sets. Basically, there are also problems in dealing with the big data, such as over fitting and outliers. Moreover, the proposed model may be insensitive only given by certain data sets. In other words, different data sets may lead various “optimal” models. Consequently, it is suggested that pre-processing effort could focus on eliminating bias, particularly pre-existing pattern data prior the use of SVM classification. One of the future works could dedicate to develop a better model, which may be hybrid in nature through combining different approaches (i.e., SVM and non-SVM) and/or fusing different kernel functions under feasible conditions. Another future work might be to realize the equality in the dynamic environments. The new algorithm will be affected by the dynamic data, when setting the parameters automatically to optimize the models.

@&#ACKNOWLEDGMENTS@&#

This work was partially supported by the National Science Foundation, United States (Award # DUE-1246050) and the US Department of Education (Award #P031S120131 and #P120A130061). This research was also supported by the Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (Grant no. NRF-2013R1A1A2006108). The authors wish to express sincere gratitude for their financial support received the duration of the research.

@&#REFERENCES@&#

