@&#MAIN-TITLE@&#Factorized appearances for object detection

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We present an object detection framework based on an advanced part learning.


                        
                        
                           
                           A generic learning method of the relationships between part appearances is proposed.


                        
                        
                           
                           We treat part locations as well as their appearance as latent variables


                        
                        
                           
                           We modify the weakly-supervised learning to handle a more complex structure.


                        
                        
                           
                           Our model yields to state-of-the-art results for several object categories.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Object recognition

Deformable part models

Learning and sharing parts

Discovering discriminative parts

@&#ABSTRACT@&#


               
               
                  Deformable object models capture variations in an object’s appearance that can be represented as image deformations. Other effects such as out-of-plane rotations, three-dimensional articulations, and self-occlusions are often captured by considering mixture of deformable models, one per object aspect. A more scalable approach is representing instead the variations at the level of the object parts, applying the concept of a mixture locally. Combining a few part variations can in fact cheaply generate a large number of global appearances.
                  A limited version of this idea was proposed by Yang and Ramanan [1], for human pose dectection. In this paper we apply it to the task of generic object category detection and extend it in several ways. First, we propose a model for the relationship between part appearances more general than the tree of Yang and Ramanan [1], which is more suitable for generic categories. Second, we treat part locations as well as their appearance as latent variables so that training does not need part annotations but only the object bounding boxes. Third, we modify the weakly-supervised learning of Felzenszwalb et al. and Girshick et al. [2], [3] to handle a significantly more complex latent structure.
                  Our model is evaluated on standard object detection benchmarks and is found to improve over existing approaches, yielding state-of-the-art results for several object categories.
               
            

@&#INTRODUCTION@&#

Pictorial structures (PSs) [4,5] and their modern variants such as the deformable part models (DPMs) [2] are probably the most popular models for object category detection. A PS is a collection of independent object parts whose spatial configuration is constrained by a system of elastic connections (springs). A DPM is a particular example of a PS that is learned by a discriminative method (latent SVM) and that uses linear classifiers on top of HOG features to describe the part appearance.

By design, DPMs model variations of the object that can be expressed as an independent motion of the object parts, which excludes, in particular, all the effects that cannot be expressed as an image deformation. Examples are appearance variations due to the self occlusion of a three dimensional object rotating out-of-plane. Other examples are three dimensional articulations or deformations: the appearance of a horse tail or of a scarf can change quite dramatically with motion. Since the linear HOG filters used in DPMs represent, by their very nature, a unimodal distribution of appearances, none of these variations can be modelled effectively by a DPM.

A simple way of incorporating multi-modal statistics in a DPM is to give up the linearity of the filters. For a discriminatively trained model, this means using a kernel other than a linear one, for example a radial basis function (RBF) kernel [6,7]. Unfortunately, non-linear kernels have a major impact on the learning and testing complexity of the model [6]. In fact, if the bottleneck of a standard DPM is searching object parts at all image locations and scales [8], with a non-linear kernel this is further exacerbated by the need of comparing each candidate part appearance to a large number of support vectors (typically in the order of thousands [6]). Recent techniques for the efficient “linearization” of non-linear kernels [7,9] do not help much here because they are limited to additive kernels, which, unlike the RBF ones, cannot be used to express multi-modal functions. Approximating RBF kernels very efficiently are still an open issue [10].

The alternative and more common approach for modeling multi-modal statistics with a DPM is to use a mixture of multiple DPMs [2,11,12], one for each object aspect (e.g., the front, three-quarter, and side views of a car, as in Fig. 1
                     ). The multiple DPMs are “glued” together by a latent variable that selects which component to use for each given candidate object instance. Compared to using non-linear kernels, the increase in complexity is bounded (linear in the number of components), and the latent variable explicitly captures which appearance variant is active, which may have a well defined semantic (e.g., the object viewpoint).

Mixtures of DPMs are usually learned jointly to calibrate their scores and to determine which component to use for each training object instance [2,11]. Other than that, the components are independent computationally and statistically. The latter issue is particularly severe as it limits the number of components that can be added to the model before overfitting starts to kick in. In practice, mixtures of DPMs can only model a handful of different object aspects. A more effective modeling scheme must exploit the fact that the various object aspects are by and large statistically dependent.

In this paper we extend the mixture-of-parts proposed in Yang and Ramanan [1] for pose estimation to general object class detection. In essence, we investigate the simplest extension to DPMs that allows exploiting the statistical dependencies between different object aspects. In particular, we apply the notion of a mixture of object appearances at the level of the object parts, rather than to the object as a whole. In object detection, the class structure for any object and the part locations are generally unknown, and only bounding boxes are available. Therefore, the fully supervised method of Yang and Ramanan [1] cannot be used. In contrast, our model considers the object parts and their appearances as latent variables that should be jointly estimated during training. In order to properly constraint the latent variables, we adapt the weakly-supervised latent SVM algorithm [2,3], with a hierarchical regularization as explained in Section 3. In this way, local part appearances can be learned in an unsupervised way.

To illustrate our model, consider a standard mixture of DPMs [2]. Graphically, this can be represented by the AND-OR tree of Fig. 1(i). The root node represents an OR node, and entails selecting one of a number of possible DPM models (corresponding to the three-quarter, side, and front views of the car). Each of these nodes is in turn connected to a small number of parts by an AND node, meaning that all those parts should be detected for the corresponding DPM. Our extension associates to each part a pool of different appearances to choose from, connected by an OR node. These multiple part appearances can represent local variations such as different styles of the wheel of a car, different shapes of the tail of a horse, or different rotations of the head of a person.

The key insight is that the model can now represent a much broader range of object variations combinatorial rather than linear in the number of model components, with a very modest increase in the number of model parameters (e.g., just twice as many if two appearance models per part are considered). As we will see in Section 5, the impact on the inference and learning costs is also very modest.

Nevertheless, selecting parts independently from each other can yield unreasonable configurations (e.g., two different wheel styles for the same car). To improve the model specificity and ultimately its precision, we consider on top of the AND-OR graph a mechanism to constrain the part activations to be pairwise compatible. (see Fig. 1(iii)). While in Yang and Ramanan [1] the structure of the compatibility constraints have the same structure used for deformations, i.e. a tree, since our goal is to generic object categories whose structure may be unknown a-priori, local appearance compatibility is enforced on a planar graph instead, where each part is connected to its neighborhoods. This structure is a loopy conditional random field (CRF) and it can be optimized efficiently with combinatorial techniques [29]. In this way, the actual structure of the object is learned during training by associating a weight to each pairwise term.

@&#RELATED WORK@&#

This section briefly summarizes some of the main development in the vast literature on object detection, highlighting the methods that are most related to our contribution, see Zhang et al. [13] for an extensive survey on object detection.

The simplest approach to improve the quality of an object detection system such as DPM is to improve the underlying image features. For example, Zhang et al. [14] adds LBP features on top of the standard HOG representation, Khan et al. [15] incorporates color and Chen et al. [16] integrates local bag-of-features models and an object mask. However, a conscious study on the effect of adding more data and changing the structure model based only on HOG [17], reflects the necessity that more complex structures can better represent the objects and therefore increase the recognition performance. A counter example is found in Ott and Everingham [18], which allows sharing of parts between different components, an approach orthogonal to ours. Unfortunately their results are well below the state-of-the-art in some international benchmarks. A possible reason is that, in our experience, sharing the same linear part filters between different DPMs yields serious calibration issues.

The concept of mixtures-of-parts is first introduced in Yang and Ramanan [1]. Here the authors propose a tree-structured model for human pose estimation using multiples interchangeable mixtures for each part. Unfortunately, their model is valid only for articulated objects, where the structure and the degree-of-freedom of the parts are known. Furthermore, part locations are known which make the problem easier and a standard learning procedure, like SVMs can be used. Recently, other methods have also explored the case of fully-supervised training, where the part location is known [19,20]. These seem to trade a higher cost of annotations for a better detection performance.

Other works have proposed to use multiple part appearances in contexts other than DPMs, but they usually require a significant amount of supervision [21], use AND–OR graphs to parse articulated objects, but the position of the parts (limbs) is known beforehand. Similarly, in Rothrock and Zhu [22], the authors make use of production scores to capture the co-occurrence costs. Poselets [23] learn a large mixture of human parts, each with his own appearance, and associate them to “fragments of pose”. These methods have some interesting properties but require a very large quantity of annotated data.

In Viola et al. [24], the authors introduced multiple instance learning for object modeling by learning automatically the object parts and their locations from a set of object bounding boxes. The same mechanism, but implemented by means of latent variables, has been used extensively in the learning of DPMs [2], including determining object bounding boxes, parts, and aspects, and is further extended in this work to capture multiple part appearances. Finally, the layout of our baseline model is a simplified version of Zhu et al. [11] where we use a single layer of parts in a regular grid, still obtaining similar performance.

The grammar framework described in Girshick et al. [3] does not require ground-truth annotations on the position of the parts. However, that grammar needs to be carefully hand-tuned to represent the object of interest (humans). Since grammars cannot yet be learned automatically, we prefer to choose a model that can be adapted to any type of class, so we select a general structure based on simple pairwise connections between the parts, forming a CRF over parts appearance.

CRFs and latent variables have been used in the modeling of object categories in Quattoni et al. [25]. There the authors model an object as a set of patches and activate them by computing a minimum-spanning tree. However, the representation is too weak to obtain satisfactory performance on challenging international benchmarks such as the PASCAL VOC.

While in our work we use multiple appearances to render complex object configurations, rank constraints during learning [26] or a sparse representation on the learned model [27] are used to represent the object parts as a linear combination of a reduced set of basis. These methods contribute to make the inference faster having to evaluate a reduced set of parts, but does not help on improving detection, as instead our model does. A similar idea is used in Fidler et al. [28] to speed up multi-class object detection, by using a coarse-to-fine taxonomy of parts among classes.

This section introduces our deformable object model combining: (i) a small number of global components that capture radically different object viewpoints (e.g., the front and side of a car), (ii) a number of movable parts for each component to model deformations and (iii) a number of appearance models for each part, to represent multiple variations of their appearance. Next, we give a formal definition of the model, and we specify the score obtained by matching the model to an image for a given configuration of the parts.

Let x be an image. The score Aj
                              (y; x, w) of matching a single part given its location/scale at rest y = (yx, yy, ys
                              ) is obtained by trading off the cost of a part displacements z = (zx, zy, zs
                              ) with the quality of the resulting appearance match:

                                 
                                    (1)
                                    
                                       
                                          A
                                          
                                             (
                                             
                                                y
                                             
                                             ;
                                             
                                                x
                                             
                                             ,
                                             
                                                w
                                             
                                             )
                                          
                                          =
                                          
                                             max
                                             
                                                z
                                             
                                          
                                          
                                             〈
                                             
                                                ψ
                                                A
                                             
                                             
                                                (
                                                
                                                   w
                                                
                                                )
                                             
                                             ,
                                             
                                                ϕ
                                                A
                                             
                                             
                                                (
                                                
                                                   x
                                                
                                                ,
                                                
                                                   y
                                                
                                                +
                                                
                                                   z
                                                
                                                )
                                             
                                             〉
                                          
                                          +
                                          
                                             〈
                                             
                                                ψ
                                                D
                                             
                                             
                                                (
                                                
                                                   w
                                                
                                                )
                                             
                                             ,
                                             
                                                ϕ
                                                D
                                             
                                             
                                                (
                                                
                                                   z
                                                
                                                )
                                             
                                             〉
                                          
                                          .
                                       
                                    
                                 
                              Here ϕA
                              (x, y + z) is the HOG descriptor extracted from image x at location y + z and ϕD
                              (z) is a descriptor of the deformation (for example defining ϕD
                              (z) as the vector of the squared displacements implements a quadratic spring). The vector w collects the parameters for the part and the operators ψA
                               and ψD
                               simply extract the blocks of parameters corresponding respectively to the appearance and the deformation.

Next, we extend w to include multiple part parameters (appearance and deformation) and introduce corresponding operators 
                                 
                                    
                                       ψ
                                       k
                                       A
                                    
                                    
                                       (
                                       
                                          w
                                       
                                       )
                                    
                                 
                               to extract them. We can therefore associate each 
                                 
                                    
                                       ψ
                                       k
                                       A
                                    
                                    
                                       (
                                       
                                          w
                                       
                                       )
                                    
                                 
                               to a learned appearance for a certain part as represented in Fig. 1. The appearance with the highest score is used to match the part to the image:

                                 
                                    (2)
                                    
                                       
                                          P
                                          
                                             (
                                             
                                                y
                                             
                                             ;
                                             
                                                x
                                             
                                             ,
                                             
                                                w
                                             
                                             )
                                          
                                          =
                                          
                                             max
                                             k
                                          
                                          A
                                          
                                             (
                                             
                                                y
                                             
                                             ;
                                             
                                                x
                                             
                                             ,
                                             
                                                ψ
                                                k
                                                A
                                             
                                             
                                                (
                                                
                                                   w
                                                
                                                )
                                             
                                             )
                                          
                                          .
                                       
                                    
                                 
                              This has the function of an OR node as shown in Fig. 1. Summing over a number of parts 
                                 
                                    j
                                    ∈
                                    P
                                 
                               results in the score for the aspect:

                                 
                                    (3)
                                    
                                       
                                          C
                                          
                                             (
                                             
                                                y
                                             
                                             ;
                                             
                                                x
                                             
                                             ,
                                             
                                                w
                                             
                                             )
                                          
                                          =
                                          
                                             ∑
                                             j
                                          
                                          
                                             P
                                             j
                                          
                                          
                                             (
                                             
                                                y
                                             
                                             +
                                             
                                                
                                                   h
                                                
                                                j
                                             
                                             ;
                                             
                                                x
                                             
                                             ,
                                             
                                                ψ
                                                j
                                                P
                                             
                                             
                                                (
                                                
                                                   w
                                                
                                                )
                                             
                                             )
                                          
                                          .
                                       
                                    
                                 
                              This is equivalent to an AND node in Fig. 1. 
                                 
                                    ψ
                                    j
                                    P
                                 
                               contains therefore the model parameters of the multiple appearances and deformations of a part j, while h
                              
                                 j
                               = (hx, hx, hs
                              ) is the part anchor, i.e. the location of the part with respect to the object center. Finally, w is extended one last time to include multiple aspects and the score of the whole model is given by of the best matching aspect:

                                 
                                    (4)
                                    
                                       
                                          O
                                          
                                             (
                                             
                                                y
                                             
                                             ;
                                             
                                                x
                                             
                                             ,
                                             
                                                w
                                             
                                             )
                                          
                                          =
                                          
                                             max
                                             i
                                          
                                          
                                             C
                                             i
                                          
                                          
                                             (
                                             
                                                y
                                             
                                             ;
                                             
                                                x
                                             
                                             ,
                                             
                                                ψ
                                                i
                                                C
                                             
                                             
                                                (
                                                
                                                   w
                                                
                                                )
                                             
                                             )
                                          
                                          .
                                       
                                    
                                 
                              Again, this correspond to a logical OR over the object aspects modelled by 
                                 
                                    ψ
                                    i
                                    C
                                 
                               as represented at the top of the AND-OR tree in Fig. 1. To summarize, the score of the model is given by

                                 
                                    (5)
                                    
                                       
                                          O
                                          
                                             (
                                             
                                                y
                                             
                                             ;
                                             
                                                x
                                             
                                             ,
                                             
                                                w
                                             
                                             )
                                          
                                          =
                                          
                                             max
                                             i
                                          
                                          
                                             ∑
                                             j
                                          
                                          
                                             max
                                             k
                                          
                                          A
                                          
                                             (
                                             
                                                y
                                             
                                             +
                                             
                                                
                                                   h
                                                
                                                
                                                   i
                                                   ,
                                                   j
                                                
                                             
                                             ;
                                             
                                                x
                                             
                                             ,
                                             
                                                ψ
                                                
                                                   i
                                                   ,
                                                   j
                                                   ,
                                                   k
                                                
                                             
                                             
                                                (
                                                
                                                   w
                                                
                                                )
                                             
                                             )
                                          
                                       
                                    
                                 
                              where for compactness we defined 
                                 
                                    
                                       ψ
                                       
                                          i
                                          ,
                                          j
                                          ,
                                          k
                                       
                                    
                                    
                                       (
                                       
                                          w
                                       
                                       )
                                    
                                    =
                                    
                                       ψ
                                       i
                                       C
                                    
                                    
                                       (
                                       
                                          ψ
                                          j
                                          P
                                       
                                       
                                          (
                                          
                                             ψ
                                             k
                                             A
                                          
                                          
                                             (
                                             
                                                w
                                             
                                             )
                                          
                                          )
                                       
                                       )
                                    
                                 
                               and we denoted by h
                              
                                 i, j
                               the anchor of the part j of the aspect i.

In order to limit the number of possible part combinations to the ones that are meaningful, a set of additional constraints in the form of a CRF with loops is introduced. These constraints encourage neighbor parts to be assigned as a compatible appearance, as automaticaly estimated from the frequency of co-occurrences on the training set. This set of part relations is modeled by a graph 
                                 
                                    G
                                    ⊂
                                    P
                                    ×
                                    P
                                 
                               with an edge per constraint. For each constraint, consider a matrix v
                              
                                 mn
                               where 
                                 
                                    
                                       v
                                    
                                    
                                       
                                          k
                                          1
                                       
                                       ,
                                       
                                          k
                                          2
                                       
                                    
                                 
                               is the cost of activating the appearance k
                              1 of the first part together with the appearance k
                              2 of the second part. Consider also the scoring function

                                 
                                    (6)
                                    
                                       
                                          B
                                          
                                             (
                                             
                                                k
                                                1
                                             
                                             ,
                                             
                                                k
                                                2
                                             
                                             ;
                                             
                                                v
                                             
                                             )
                                          
                                          =
                                          
                                             ∑
                                             m
                                          
                                          
                                             ∑
                                             n
                                          
                                          I
                                          
                                             (
                                             
                                                k
                                                1
                                             
                                             =
                                             m
                                             )
                                          
                                          I
                                          
                                             (
                                             
                                                k
                                                2
                                             
                                             =
                                             n
                                             )
                                          
                                          
                                             
                                                v
                                             
                                             
                                                m
                                                ,
                                                n
                                             
                                          
                                          ,
                                       
                                    
                                 
                              where 
                                 I
                               is the indicator function of an event. Instead of maximising independently over each part appearance as in (2), now the model optimizes jointly over all parts, while accounting for the pairwise constraints:

                                 
                                    (7)
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      C
                                                      CRF
                                                   
                                                   
                                                      (
                                                      
                                                         y
                                                      
                                                      ;
                                                      
                                                         x
                                                      
                                                      ,
                                                      
                                                         w
                                                      
                                                      )
                                                   
                                                
                                             
                                             
                                                =
                                             
                                             
                                                
                                                   
                                                      max
                                                      
                                                         k
                                                      
                                                   
                                                   
                                                      ∑
                                                      
                                                         j
                                                         ∈
                                                         P
                                                      
                                                   
                                                   A
                                                   
                                                      (
                                                      
                                                         y
                                                      
                                                      +
                                                      
                                                         
                                                            h
                                                         
                                                         j
                                                      
                                                      ;
                                                      
                                                         x
                                                      
                                                      ,
                                                      
                                                         ψ
                                                         
                                                            j
                                                            ,
                                                            
                                                               k
                                                               j
                                                            
                                                         
                                                      
                                                      
                                                         (
                                                         
                                                            w
                                                         
                                                         )
                                                      
                                                      )
                                                   
                                                
                                             
                                          
                                          
                                             
                                             
                                             
                                                
                                                   +
                                                   
                                                   
                                                      ∑
                                                      
                                                         (
                                                         j
                                                         ,
                                                         l
                                                         )
                                                         ∈
                                                         G
                                                      
                                                   
                                                   B
                                                   
                                                      (
                                                      
                                                         k
                                                         j
                                                      
                                                      ,
                                                      
                                                         k
                                                         l
                                                      
                                                      ;
                                                      
                                                         ψ
                                                         
                                                            j
                                                            ,
                                                            l
                                                         
                                                         B
                                                      
                                                      
                                                         (
                                                         
                                                            w
                                                         
                                                         )
                                                      
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              where k = [k
                              0, k
                              1, …, kn
                              ] is a vector appearance labels, one for each part, and 
                                 
                                    
                                       ψ
                                       
                                          j
                                          ,
                                          k
                                       
                                    
                                    
                                       (
                                       
                                          w
                                       
                                       )
                                    
                                    =
                                    
                                       ψ
                                       j
                                       P
                                    
                                    
                                       (
                                       
                                          ψ
                                          k
                                          A
                                       
                                       
                                          (
                                          
                                             w
                                          
                                          )
                                       
                                       )
                                    
                                 
                              . Finally 
                                 
                                    ψ
                                    
                                       j
                                       ,
                                       l
                                    
                                    B
                                 
                               are the parameters of the pairwise constraints between the parts (j, l), represented with a dashed line in Fig. 1.

Rewriting the final score for the formulation with pairwise appearance constraints gives

                                 
                                    (8)
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      O
                                                      CRF
                                                   
                                                   
                                                      (
                                                      
                                                         y
                                                      
                                                      ;
                                                      
                                                         x
                                                      
                                                      ,
                                                      
                                                         w
                                                      
                                                      )
                                                   
                                                
                                             
                                             
                                                =
                                             
                                             
                                                
                                                   
                                                      max
                                                      
                                                         i
                                                         ,
                                                         
                                                            k
                                                         
                                                      
                                                   
                                                   
                                                      ∑
                                                      j
                                                   
                                                   A
                                                   
                                                      (
                                                      
                                                         y
                                                      
                                                      +
                                                      
                                                         
                                                            h
                                                         
                                                         
                                                            i
                                                            ,
                                                            j
                                                         
                                                      
                                                      ,
                                                      
                                                         x
                                                      
                                                      ,
                                                      
                                                         ψ
                                                         
                                                            i
                                                            ,
                                                            j
                                                            ,
                                                            
                                                               k
                                                               j
                                                            
                                                         
                                                      
                                                      
                                                         (
                                                         
                                                            w
                                                         
                                                         )
                                                      
                                                      )
                                                   
                                                
                                             
                                          
                                          
                                             
                                             
                                             
                                                
                                                   +
                                                   
                                                   
                                                      ∑
                                                      
                                                         (
                                                         j
                                                         ,
                                                         l
                                                         )
                                                         ∈
                                                         G
                                                      
                                                   
                                                   B
                                                   
                                                      (
                                                      
                                                         k
                                                         j
                                                      
                                                      ,
                                                      
                                                         k
                                                         l
                                                      
                                                      ;
                                                      
                                                         ψ
                                                         
                                                            i
                                                            ,
                                                            j
                                                            ,
                                                            l
                                                         
                                                         B
                                                      
                                                      
                                                         (
                                                         
                                                            w
                                                         
                                                         )
                                                      
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              Inferring the model at location y amounts to maximising (8). To do so efficiently, 
                                 G
                               is restricted to have a planar structure, where each part is connected with its horizontal and vertical neighbors (as in Fig. 1(iii)). Dynamic programming is used to estimate the optimal displacement of each part first, and sequential reweighted trees [29] are used to solve the loopy CRF model and jointly estimate the optimal appearance of the parts. Considering that the number of parts is generally quite small, this does not compromise detection speed compared to a standard DPM.

Learning uses weak supervision and, similarly to Felzenszwalb et al. [2], requires only bounding boxes around instances of the object category of interest. The aspect, part locations, and part appearance components are not provided and are instead estimated automatically during learning as latent variables.

In detail, given a set of input images 
                        
                           X
                           =
                           (
                           
                              
                                 x
                              
                              0
                           
                           ,
                           
                              
                                 x
                              
                              1
                           
                           ,
                           …
                           ,
                           
                              
                                 x
                              
                              l
                           
                           )
                           ,
                        
                      a set of object locations 
                        
                           Y
                           =
                           (
                           
                              
                                 y
                              
                              0
                           
                           ,
                           
                              
                                 y
                              
                              1
                           
                           ,
                           …
                           ,
                           
                              
                                 y
                              
                              p
                           
                           )
                           ,
                        
                      and the locations of the negative samples 
                        
                           N
                           =
                           (
                           
                              
                                 n
                              
                              0
                           
                           ,
                           
                              
                                 n
                              
                              1
                           
                           ,
                           …
                           ,
                           
                              
                                 n
                              
                              n
                           
                           )
                        
                      (i.e. , locations that do not overlap with the ground truth object bounding boxes), the goal is to optimize the empirical risk

                        
                           (9)
                           
                              
                                 
                                    
                                       
                                          f
                                          (
                                          
                                             w
                                          
                                          )
                                       
                                    
                                    
                                       =
                                    
                                    
                                       
                                          
                                             1
                                             2
                                          
                                          R
                                          
                                             (
                                             
                                                w
                                             
                                             )
                                          
                                          +
                                          C
                                          
                                             ∑
                                             
                                                i
                                                =
                                                0
                                             
                                             p
                                          
                                          L
                                          
                                             (
                                             
                                                max
                                                
                                                   
                                                      s
                                                   
                                                   ∈
                                                   
                                                      S
                                                      i
                                                   
                                                
                                             
                                             O
                                             
                                                (
                                                
                                                   s
                                                
                                                ;
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      l
                                                      (
                                                      i
                                                      )
                                                   
                                                
                                                ,
                                                
                                                   w
                                                
                                                )
                                             
                                             )
                                          
                                       
                                    
                                 
                                 
                                    
                                    
                                    
                                       
                                          +
                                          
                                          C
                                          
                                             ∑
                                             
                                                i
                                                =
                                                0
                                             
                                             
                                                n
                                             
                                          
                                          L
                                          
                                             (
                                             −
                                             O
                                             
                                                (
                                                
                                                   
                                                      n
                                                   
                                                   i
                                                
                                                ;
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      l
                                                      (
                                                      i
                                                      )
                                                   
                                                
                                                ,
                                                
                                                   w
                                                
                                                )
                                             
                                             )
                                          
                                          ,
                                       
                                    
                                 
                              
                           
                        
                     where 
                        
                           L
                           (
                           z
                           )
                           =
                           max
                           {
                           0
                           ,
                           1
                           −
                           z
                           }
                        
                      is the hinge loss, x
                     
                        l(i) is the image corresponding to the object location y
                     
                        i
                     , and s denotes a small correction applied to the ground truth location estimated to better fit the model to the training data, similar to Felzenszwalb et al. [2]. In particular, the adjustment is encoded by the (latent) variable s, which is constrained to be in the vicinity of the ground truth locations, i.e.
                     
                        
                           (10)
                           
                              
                                 
                                    S
                                    i
                                 
                                 =
                                 
                                    {
                                    
                                       s
                                    
                                    ∈
                                    
                                       
                                          x
                                       
                                       
                                          l
                                          (
                                          i
                                          )
                                       
                                    
                                    :
                                    ovr
                                    
                                       (
                                       
                                          s
                                       
                                       ,
                                       
                                          
                                             y
                                          
                                          i
                                       
                                       )
                                    
                                    >
                                    T
                                    }
                                 
                                 ,
                              
                           
                        
                     where 
                        
                           ovr
                           
                              (
                              
                                 s
                              
                              ,
                              
                                 y
                              
                              )
                           
                           =
                           
                              
                                 area
                                 (
                                 
                                    B
                                    
                                       s
                                    
                                 
                                 ∩
                                 
                                    B
                                    
                                       y
                                    
                                 
                                 )
                              
                              
                                 area
                                 (
                                 
                                    B
                                    
                                       s
                                    
                                 
                                 ∪
                                 
                                    B
                                    
                                       y
                                    
                                 
                                 )
                              
                           
                        
                      is the overlap score between the bounding boxes at locations s and y respectively, and T is a threshold. Note that besides s, the other latent variables are not shown in Eq. (9), but are still maximized inside O, as shown in the derivation of O in Section 2.

Since the objective (9) is equivalent to a standard linear SVM (except for the treatment of the latent variables, as discussed below), optimization uses the fast stochastic gradient descent technique of Singer and Srebro [30]. However, since the number of negative examples is extremely large (there is one negative for each image location that does not contain the object), the model is learned in stages, by collecting more and more hard negative examples based on the current version of the model. This procedure, known as constraint generation, cutting plane, or mining of hard negatives [2], can be shown to converge to the optimum of the objective function (9) in polynomial time.

The scoring function O(y; x, w) of the model implicitly maximizes over a number of parameters (aspect, part locations, part appearance selections) energies that are, ultimately, linear in w. Since O(y; x, w) is the max of convex functions, is itself convex in w, and so is the composition with the hinge loss 
                           
                              L
                              (
                              −
                              O
                              
                                 (
                                 
                                    
                                       n
                                    
                                    i
                                 
                                 ;
                                 
                                    
                                       x
                                    
                                    
                                       l
                                       (
                                       i
                                       )
                                    
                                 
                                 ;
                                 
                                    w
                                 
                                 )
                              
                              )
                           
                         for the negative examples. Unfortunately, for the positive examples the loss turns the sign the other way around and the composition is not convex. To address this issue, we follow the standard approach of converting the parameters that O(y; x, w) marginalizes over (aspect, part locations, part appearances) into latent variables and use the Concave–Convex Procedure (CCP) [2,11,31] to find a model w which is at least locally optimal. The CCP alternates estimating the latent parameters of the positive object instances and the model w; in particular, the latent estimation step can be seen as hallucinating/estimating the model parameters that would be provided by an annotator in case of strong supervision.

In our model the latent variables are applied at two different levels. For the parts location, the latent variable is applied at feature level. That is, the model displaces each part to select the features that maximize its score. Instead, for aspect and part appearance, the latent variable is applied at model level, because the model selects which component for aspect and parts appearance better describe the features (i.e. maximizes the score).

While latent variables at the feature level can be regularized with standard SVM 
                           
                              R
                              
                                 (
                                 
                                    w
                                 
                                 )
                              
                              =
                              
                                 
                                    ∥
                                    
                                       w
                                    
                                    ∥
                                 
                                 2
                              
                           
                         regularization, for latent variables at model level the standard approach would fail. This is because when a latent variable at model level selects the best component, the others would be set to zero to force them to not contribute to the scoring. This procedure allows the model to represent OR-like nodes, but it is intrinsically unstable. Imagine that a component, during an iteration of latent variables estimation get assigned more samples than another. This would produce a new model, where the corresponding component has gained importance (i.e. its norm is higher). Thus, in the next iteration of latent variables estimation the component will probably get assigned even more samples. This would tend to produce a sparse representation with few strong components and the rest set to zeros and thus ineffective.

We can counterbalance this instability by using as regularizer the maximum of the squared norm of the parameters of each component rather than their sum. In Felzenszwalb et al. [32] this procedure was used to better balance the final score among the different aspects of a model. Here, as the object parts are totally free to choose any appearance, this procedure becomes fundamental. We found that when using latent variables, balancing the various model components (aspects, part appearances) is very important. If the latent, using the standard SVM regulariser 
                           
                              R
                              
                                 (
                                 
                                    w
                                 
                                 )
                              
                              =
                              
                                 
                                    ∥
                                    
                                       w
                                    
                                    ∥
                                 
                                 2
                              
                           
                         tends in fact to kill entire components by pushing their parameters to zero, ultimately lowering the performance of the model. Felzenszwalb et al. [32] alleviate this problem by using as regularizer the maximum of the squared norm of the parameters of each component rather than their sum. In this way, there is no advantage in lowering the weights of any of the components with respect to any other. Since our model includes components at two levels (object and parts), we found that the appropriate extension of this idea involves maximizing over components at both levels, as follows:

                           
                              (11)
                              
                                 
                                    R
                                    
                                       (
                                       
                                          w
                                       
                                       )
                                    
                                    =
                                    
                                       max
                                       i
                                    
                                    
                                       ∑
                                       j
                                    
                                    
                                       max
                                       k
                                    
                                    
                                       〈
                                       
                                          ψ
                                          
                                             i
                                             ,
                                             j
                                             ,
                                             k
                                          
                                       
                                       
                                          (
                                          
                                             w
                                          
                                          )
                                       
                                       ,
                                       
                                          ψ
                                          
                                             i
                                             ,
                                             j
                                             ,
                                             k
                                          
                                       
                                       
                                          (
                                          
                                             w
                                          
                                          )
                                       
                                       〉
                                    
                                    .
                                 
                              
                           
                        Due to the recursive definition of ψ
                        
                           i, j, k
                        (w), (11) must be computed recursively, for example by using dynamic programming. Other than that, incorporating it in the SGD solver is trivial as it suffices to compute a sub-gradient with respect to w.

The CCP procedure is a local optimization method therefore the initialization is very important in order to obtain a good solution. This amounts to finding a good initial value for the latent variables. As in the proposed model the latent variables are extended also to part appearance, their initialization is fundamental for good results. We propose a two steps approach to produce a good initialization for the parts appearance.

The location of the positive instances (s in (9)) is chosen to maximize the overlap between the ground truth bounding box and the one associated to the model. Initially deformations are set to be null. As in Felzenszwalb et al. [32], the model has a flag indicating whether the object is facing left or right; this is an additional latent variable which is initialized by pre-clustering the training examples (denoted as FLIP).

We explore two initialization procedures for learning the local appearances. In the first, we learn all latent variables at the same time, by randomly assigning each local appearance to one of the labels. With this naive approach, we found that the model can easily get stuck in a local minima.

A better strategy is to using a two step sequential procedure (denoted as SEQ). A standard (one appearance) DPM model is first learned. Then, the learned model is applied again to the training images so that a precise and aligned localization of the parts can be obtained. For each part a k-means clustering on the feature space is effectuated, where k is the number of appearances that we want to model. Each cluster is then used as initialization for the appearance of the multi-appearance model.

The appearance compatibility parameters are initialized to zero so that initially, any appearance can be chosen. After this, their value is estimated by the SVM optimization so that compatible parts will obtain a positive weight while incompatible parts a negative one.

@&#IMPLEMENTATION DETAILS@&#

We implement our model using HOG features for the object appearance and quadratic cost for the deformation features. Specifically, we define the features of an object part as

                        
                           (12)
                           
                              
                                 
                                    ϕ
                                    I
                                 
                                 
                                    (
                                    
                                       x
                                    
                                    ,
                                    
                                       y
                                    
                                    +
                                    
                                       z
                                    
                                    )
                                 
                                 =
                                 H
                                 
                                    (
                                    
                                       x
                                    
                                    ,
                                    
                                       y
                                    
                                    +
                                    
                                       z
                                    
                                    )
                                 
                              
                           
                        
                     where H is a function that given the image x extracts a vector of HOG features [2] from the given location y + z. The deformation features are defined as:

                        
                           (13)
                           
                              
                                 
                                    ϕ
                                    D
                                 
                                 
                                    (
                                    
                                       z
                                    
                                    )
                                 
                                 =
                                 
                                    [
                                    
                                       z
                                       x
                                       2
                                    
                                    ,
                                    
                                       z
                                       y
                                       2
                                    
                                    ,
                                    
                                       z
                                       x
                                    
                                    ,
                                    
                                       z
                                       y
                                    
                                    ]
                                 
                              
                           
                        
                     to account for the displacement magnitude and direction. Due to these choices, the maximization in Yang and Ramanan (1) can be done efficiently by using the distance transform [4]. Local appearances are selected in the same maximization, after applying its own displacement penalties.

For detection, the score O(y; x, w) is evaluated at a discrete set of locations y which match to the layout of the underlying HOG features. To speed-up the evaluation of the appearance compatibility we produce an initial set of detection hypotheses without considering the pair-wise compatibility scores (5), rank them, and compute the full but more expensive score (8) only at the top 1000 candidates. This reduces the computational cost of the method without affecting the detection accuracy.

To get a final list of candidate detections, non-maxima suppression is run over the candidate list of bounding boxes of the different model aspects sorted by decreased confidence score. This procedure is greedy: after selection a new detection, any other detection that overlaps with it by more than a threshold is removed from the candidate pool.

The time required to detect an object is dominated by the number of part filters that need to be evaluated. For example, a model with two aspects, left-right flipping, and two appearances per part, requires 8 × N
                     parts filtering operations. On a single core Xeon 2.4 GHz a model with N
                     parts = 9, evaluating the cost on a PASCAL VOC image takes an average of ten seconds.

@&#EXPERIMENTS@&#

We evaluate our approach on two standard datasets: INRIA Person Dataset [33] and Pascal VOC 2007 [34]. The variety of the classes helps to identify the classes where more benefit is obtained by the use of multiple local appearances. For evaluation, we use the comparison framework of Dollár et al. [35] for INRIA, and the average precision (AP) with the standard Pascal VOC 2007 criterion.

First, we evaluate the two initializations of the appearance explained in Section 3 for the horse and motorbike classes. We begin with a model with two components. Although the simplicity of the random initialization, the method is able to find two different appearances per part. As shown in Fig. 2(i), a model of horse with 2 local appearances (named 2app) with this random initialization gain five points over the one appearance model (1app).

Using the same initialization with the left–right models, the method gain is not as high as expected, and only improves in one point with respect to the flipped version. This is because the left-right orientation and the local appearances compete each other to estimate the same object appearance. An interesting example of this is shown in Fig. 3
                        (i), where it is illustrated the object model of a horse with random initialization. Local appearances and left–right model try to represent the same appearance, finally resulting in impossible model configurations (i.e. horse with two heads in the top-right model). Instead, with the SEQ initialization, which sequentially learn the left–right prediction and then two latent estimations of local appearances, obtain a much nicer model. In this way, as the model orientation has already been learned, the local appearances can now learn different views of the object (namely, a quiet or a running horse).

This is shown in Fig. 3(ii). We add the two appearances to the model, once the flip variables has been estimated, which represents the current state of the art for deformable HOG based models. Again, the multiple local appearances increase the performance, pushing the AP up to 60.1% which is already four points over the state of the art. Finally, learning the compatibility of the local appearances further increase the AP of more than one point reaching an AP of 61.7%. This is mainly due to less false detections are found, hence higher precision is achieved.

We next evaluate our approach on the INRIA Person Dataset [33]. For evaluation, we use the framework of Dollár et al. [35].

In Table 1
                        , we evaluate different configurations of our model. The baseline is shown in column one and is a model with only one appearance per part and one global component with left–right facing (like a traditional DPM). In the first row we show the effect of increasing the number components. It produces a slight decrement on AP, probably due to the statistical independence of each component. In practice increasing the global components reduces the number of samples available for each component and therefore the generalization capability of the learned model. In contrast, using more local appearances yields better accuracy, and the model reaches an AP of 88% when using a model with three appearances for each part.

In Fig. 4
                        
                        
                         we compare the model with three appearances with the current state of the art in pedestrian detection. As pedestrians assume different poses, local deformations are quite important. For this reason the DPM model (in the table is referred as LatSvm-V2) performs relatively well, even if other models use multiple and more expensive features like color or convolutional features. However, pedestrians can also wear different clothes or assume very specific positions that cannot be explained with simple parts displacement. Under this condition the proposed model is better indicated. This is reflected on the evaluation, where our method combining deformation and a multimodal representation of the object parts clearly outperform DPM and is on par with most of the state of the art approaches which are specifically optimized for the task of pedestrian detection.

Our method is general enough to be used to learn any object class, not only pedestrians. In this sense we perform several experiments on the challenging VOC 2007 [34], where 20 different classes should be learned using the same settings.

We evaluate for each class the importance of the two main contributions of this work: (i) we compare local parts versus global components and (ii) we evaluate the effect of the pairwise constraints on the parts appearance. As reference we consider the AP of our model trained using two components and two local appearances as reported in the first row of Table 2.

The second row of the table reports the AP for a new model trained with exactly the same configuration of our reference model but without using the pairwise appearance constraints. In average the model without appearance constraints is inferior to the complete one. This confirms our hypothesis that learning the pairwise compatibility between parts helps to improve the detection accuracy, see Fig. 5. Although the average difference between the two models is relatively small (0.7) for certain classes enforcing compatibility among parts can provide a neat improvement of more than two points.
                        
                        
                        
                     

In the third row of the table we train a new model with four global components. Doing so, the number of parameter to learn is similar to the model with two components and two appearances. However, in the four components model, each component is totally separate from the other. This can be considered an advantage because the model avoids to mix-up appearances and it will probably generates fewer false positives. Still, the model cannot share parts which reduces its capability to generalize. In this experiment the advantage of using multiple local parts is evident. The four components model obtains a lower AP in almost every class and it has a mAP more than three points lower than the model with multiple appearances.

In Fig. 6 we visualize the occurrence of each possible configuration of parts for the class car. As the model is composed by nine parts and each has two local appearances, a total of 29 different configurations can be expressed. This is much higher than the 4 representations of a traditional DPM with independent components. From the histogram we can see that there are few configurations that are most used. However, most of the configurations are used at least one time. This shows that the model is really using its capability to combine different part appearances to represent the different instances of a class.

In Table 3 we report the AP of the DPM model as reference and the AP of our deformable model with one, two and three appearances. Our baseline is around two points below the DPM score form [32]. This is mainly due to our implementation choice. A strategic placement of the parts can highly enhance the performance of the detector. However, as we use connections among nearby parts, we prefer to use a uniform distribution of the parts. Instead, in DPM they use a greedy procedure to find the best placement for the parts. Furthermore, the DPM is composed of a low-resolution and rigid model and on top of it several parts.

In our simplified model, we do not use the low resolution representation because we are mainly interested in the role of the parts and their interaction. Assuming that, we consider ours a strong baseline. Our baseline obtains a mAP of 29.6. The performance of the same model with two appearances per each part and pairwise compatibility constraints scores a mAP of 31.6 such that the gap with DPM is already almost canceled.

Moving to three appearances per part leads to an additional improvement of 0.6 points as reported in the last row of the table. In Figs. 7 and 8, we show the different appearances learned for the parts of cars and horses, respectively, together with the top five best scoring detections for each part. We can see how, despite describing the same object, each appearance learns a quite different model.

@&#DISCUSSION@&#

In this paper we have shown how to increase the representational capability of DPM by adding multiple local part appearances that can be combined in an exponential number of possible representations with a limited computational cost. However, to obtain this model to work properly some important parts of the DPM algorithm had to be modified and improved.

First of all, as explained in Section 3.2, when dealing with multiple competing representations, especially at multiple levels, (as aspects and parts in our case), it is fundamental to apply a regularization that tends to keep the corresponding models balanced, so that one does not “steal” all samples. We notice that this problem becomes more and more important while increasing the number of appearances. In our setting we limit our experiments to three part appearances mostly for resources reasons (i.e. memory). However, we believe that further increasing the number of appearances can give a limited improvement also due to the competing representations problem.

Another very important factor for good results is initialization. In particular, clustering aligned parts can make a big difference in the final results (see Fig. 2(i) in Section 5). Applying the clustering to fixed part locations (before alignment) would produce splits that represent the different displacements that the part can assume in different object instances. Therefore the resulting multi-appearance model of each part would represent almost the same appearance multiple times (would learnt displaced parts) which lead to a poor initialization, and the model would get also stuck. Instead, with our aligned initialization we assure that the split in the clustering would model different appearances of the same part. Even though the proposed initialization performs already much better than a naive one, we still believe that the initialization of the parts is one of the key points to further improve results. Specifically, for classes where the body deformation are relevant, like cats and dogs, a better initialization based on the real part location can produce much better results as shown in [19].

Finally, it is interesting to notice that, as the model capacity increases, for example in our case allowing combination of parts, the space of search of the negative examples also increases, which directly translates into a slower convergence. For example a training of a deformable model with one appearance needs an average of 4−5 iterations of negatives to converge in the first iteration. If we move to two local appearances the number of iterations grows to 5−15 while for three appearances it is necessary from 10 to 20 iterations. Despite the training time increases, during testing time, the method grows linearly with the number of appearances. In this sense methods like [26,27,36–38] can be used to reduce the computational cost for detection.

@&#CONCLUSIONS@&#

We have presented a new extension of the deformable parts model that can be used to learn multiple local appearances at a reasonable computational cost.

Compared to a traditional mixture of DPMs, our model (i) can express a very large set of different object appearances with a very small increase in the number of parameters, (ii) can learn the same amount of variation from far less training data by better exploiting the statistical dependencies between different object appearances, and (iii) is still very discriminative because the CRF constraints can reject unlikely part configurations.

Compared with multiple independent models, our approach can approximate an exponentially rich combination of appearances maintaining the same model representation. In addition, to limit our representation to only the feasible configuration of local parts, we introduce pairwise potential between appearances. We are also investigating the possibility to introduce the concept of occluded parts to the model as another local appearance, which can help on learning clearer parts.

@&#ACKNOWLEDGMENTS@&#

Andrea Vedaldi was supported by the EU Project FP6 VIDI-Video IST-04554, ONR MURI N00014-07-1-0182, and the Violette and Samuel Glasstone Research Fellowships in Science. The other authors would acknowledge the support of the Spanish projects TIN2009-14501-C02-02 and TIN2012-39051.

@&#REFERENCES@&#

