@&#MAIN-TITLE@&#A talking profile to distinguish identical twins

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We prove that twins look alike, but they behave differently.


                        
                        
                           
                           A talking profile consisting of usual facial motions is proposed as a biometric.


                        
                        
                           
                           Our model counts more on abnormal action and gains the superior performance.


                        
                        
                           
                           Our experiments show that the talking profile is robust to some aging effect.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Talking profile

Identical twins

Abnormal motions

EMRM

@&#ABSTRACT@&#


               
               
                  Identical twins pose a great challenge to face recognition due to high similarities in their appearances. Motivated by the psychological findings that facial motion contains identity signatures and the observation that twins may look alike but behave differently, we develop a talking profile to use the identity signatures in the facial motion to distinguish between identical twins. The talking profile for a subject is defined as a collection of multiple types of usual face motions from the video. Given two talking profiles, we compute the similarities of the same type of face motion in both profiles and then perform the classification based on those similarities. To compute the similarity of each type of face motion, we give higher weights to more abnormal motions which are assumed to carry more identity signature information.
                  Our approach, named Exceptional Motion Reporting Model (EMRM), is unrelated with appearance, and can handle realistic facial motion in human subjects, with no restrictions of speed of motion or video frame rate. We first conduct our experiments on a video database containing 39 pairs of twins. The experimental results demonstrate that identical twins can be distinguished better by the talking profiles over the traditional appearance based approach. Moreover, we collected a non-twin YouTube dataset with 99 subjects. The results on this dataset verified that the talking profile can be the potential biometric. We further conducted an experiment to test the robustness of talking profile to the time. Videos from 10 subjects which span across years or even decades in their lives are collected. The results indicated the robustness of talking profile to the aging process.
               
            

@&#INTRODUCTION@&#

The occurrence of twins has progressively increased in the past decades as twin birth rate has risen to 32.2 per 1000 birth with an average 3% growth per year since 1990 [1]. With the increase of twins, identical twins are becoming more common as well. This, in turn, is urging biometric identification systems to accurately distinguish between twin siblings. Although identical twins represent a minority (0.2% of the world's population), it is worth noting that they equal the whole population of countries like Portugal or Greece. Therefore failing to identify them is a significant hindrance for the success of biometric systems. Identical twins share the same DNA code and therefore they look extremely alike. Nevertheless, some biometrics depend not only on the genetic signature but also on the individual development in the womb. As a result, identical twins have some different biometrics such as fingerprint and retina. Several researchers have taken advantage of this fact and have shown promising results in automatic recognition systems that use these discriminating traits: fingerprint [2], palmprint [3], iris [4] and combinations of some of the above biometrics [5]. However, these biometrics require the cooperation of the subject. Thus, it is still desirable to identify twins by pure facial features, since they are non-intrusive, they do not require explicit cooperation of the subject and are widely available from photos or videos captured by ordinary camcorders. Unfortunately, the high similarity between identical twins' appearance is known to be a great challenge for face recognition systems. The performance of various face appearance based approaches on recognizing identical twins has recently been questioned [5,6]. They both confirmed the difficulties encountered by appearance based face recognition systems on twin databases, and strongly suggested the need for new ways to improve the performance of recognizing identical twins.

In psychology, it has been demonstrated that the human visual system utilizes both appearance and facial motion for face recognition [7,8]. Appearance information provides the first route for face recognition, while the dynamic signature information embedded in facial motion are processed in the superior temporal sulcus and provide a secondary route for face recognition. This secondary route for face recognition, also known as the supplemental information hypothesis, is supported by many works both in psychology and computer vision [7,9].

The traditional appearance based face recognition simulates the first route to recognize faces, but fails to effectively distinguish between identical twins. Considering both studies support that facial motion contains identity information, and the observation that twins may look alike but behave differently, we propose to use talking profiles which consists of multiple type of usual face motions to recognize twins. Our intention is to simulate the secondary route for face recognition. The flowchart of our approach, named Exceptional Motion Report Model (EMRM), is illustrated in Fig. 1
                     .
                        
                           1)
                           Given a video, the talking profile is extracted. The profile consists of 6 types of usual face motions, such as pose change and 2D in-plane movement. Each type of motion in the profile is represented as a sequence of local motions between two adjacent frames.

From the computed two talking profiles, we compute the similarity of each pair of corresponding face motions of the same type from both talking profiles. Given a pair of corresponding face motions, since they may be unsynchronized, i.e. different frame rate or speed, we perform a motion alignment in advance. The alignment is achieved by minimizing the abnormality function. A Gaussian mixture model is employed to estimate the abnormality of each local motion. This abnormality scheme is inspired from [10] in psychology which proves that humans use exceptional motions to identify faces. This is also observed in real life that humans always use a person's peculiar head motion (e.g. tilt) rather than common head movement to aid recognition. After alignment, the similarity of this pair of corresponding face motions is computed as the weighted sum of local motion similarity.

From the similarities of every pair of corresponding face motion sequences in the talking profiles, we perform a SVM classification on those similarities. Also, our model is set in verification mode, that is, to claim that the faces in two videos are genuine or imposters.

We test our algorithm by conducting several experiments on a free talking video database with 39 pairs of identical twins. Our experimental results shows that compared with traditional appearance based approaches, the talking profile can be used to accurately distinguish between identical twins. We further apply talking profile on a free talking video database of 99 subjects from YouTube. Results from our second set of experiments are in agreement with the psychological findings that facial motion contains identity signatures and demonstrate that the talking profile has a potential to be used in biometrics. To test the robustness of talking profile against time, we further collected an cross-age video database with 10 subject whose videos are set across years and conducted the experiments on this database. The results indicated that the talking profile is relatively robust to the time change.

The contributions of our work are four fold: 1) we show that talking profiles can be used to distinguish between identical twins. 2) We propose a novel EMRM to analyze facial motion in video, which also provides a general framework of using abnormality for recognition. 3) Our experiments on YouTube dataset support the psychological findings that facial motion does provide identity signatures. 4) Our experiments on cross-age video database demonstrate the robustness of talking profile against the aging process. We continue by introducing existing twin recognition and motion based face recognition works in Section 2. In Section 3, we describe the details of our model. We present our dataset and experiments in Section 4, concluding in Section 5.

@&#RELATED WORK@&#

There are limited works on identical twin recognition using 2D face biometrics [5,6,11]. Sun et al. [5] were the first to evaluate the performance of appearance based face recognition to distinguish between twins. They compared with performances of iris, fingerprint and a fusion of them. Their database was collected in 2007 at the fourth Annual Festival of Beijing Twins Day. The face subset used in the experiments contained 134 subjects, each having around 20 images. All images were collected during a single session over a short interval. Experiments were conducted using the commercial matcher and showed that identical twins are a challenge to current face recognition systems. Phillips et al. [6] thoroughly extended the analysis of the performance of face recognition systems in distinguishing between identical twins on another database collected at the Twins Days festival in Ohio in 2009 and 2010. It consisted of images of 126 pairs of identical twins collected on the same day and 24 pairs with images collected one year apart. Facial recognition performance was tested using three of the top submissions to the Still Face Track at Multiple Biometric Evaluation 2010. Based on their experimental results, the best performance was observed under ideal conditions (same day, studio lighting and neutral expression). But under more realistic conditions, distinguishing between identical twins was very challenging. Klare et al. [11] analyzed the features of each facial component to distinguish identical twins from the same database in [6]. They also analyzed the possibility of using facial marks to distinguish identical twins. They also confirmed the challenge of recognizing identical twins merely based on appearance. All these works showed the need for new approaches to help improve performance when recognizing identical twins.

Psychological studies have shown that humans better recognize faces with expressive motion. Hill and Johnston [7] showed in their experiments that humans utilize rigid head movements in face recognition. Thornton and Kourtzi [12] observed that showing moving face images rather than static face images in training sessions improved human subjects' performances in face recognition. Pilz et al. [9] claimed further that moving images not only increased recognition rate, but also reduced reaction time. These psychological findings imply that face motions contain considerable identity information which is fairly reliable for face recognition. There are some recent attempts [8,13] in computer vision to use face motions for face recognition. They computed either a dense or sparse displacement on tracked points and used it to identify general human subjects. Tulyakov et al. [8] manually marked some landmark points and used their displacement as features for face recognition. Ye and Sim [13] proposed to compute the dense optical flow from the neutral to the apex of a smile, then used the optical flow field as a feature for recognition. Later, they designed a local deformation pattern as a feature and tried to find identity signature between different expressions. All these works achieved some breakthrough in motion based face recognition, but all of them only considered the general population and required the cooperation of subjects. Moreover, among all possible face motions, only facial expressions are considered in these works, while many other types of face motions remain unexplored.

One recent work tried to apply motion based face recognition on the identical twin problem which only focused on expression and required a fixed head position without any movement. This work is different from [14] in three aspects. First, in our model, we do not require any face alignment while they required very accurate face alignment. Second, our work uses 6 different types of usual face motions while they only used facial expressions. Third, our algorithm can handle the occurrence of multiple and unknown motions in a single video, while they can only handle the existence of one expression per video.

In this section we describe the details of our model, Exception Motion Reporting Model (EMRM), to use talking profiles for face recognition. We follow the setting in Labeled Face in the Wild [15] to set EMRM in the verification model, that is, to claim that the faces in two videos are imposters or genuine. There are in total five steps in the EMRM. The first step is to extract the talking profile from each video. The second and third steps are to assign the abnormality weight and compute local motion similarity. The fourth step is to align two face motion sequences of the same type from the two talking profiles and compute their similarity. The final step is to use the similarities from all the corresponding pairs in talking profiles for classification using support vector machine.

For each video, the talking profile is comprised of multiple types of usual face motions. In our work, it includes 6 types: 2D in-plane translation, pose change, gaze change, pupil movement and eye/mouth opening–closing magnitude (i.e. the extent that the eyes/mouths are open). Various tools have been released to extract these information from the video, such as Luxand, Pitpatts and Omron SDK. In our implementation, we use Omron SDK to extract the talking profile. Note that each type of face motion in talking profile is a sequence of local motions between two adjacent frames. We perform a sampling before processing to test the robustness when the probe and the gallery have different frame rates and also save computation workload. Assume that the sample rate is FPS (i.e. the local motion is computed between each FPS frames). We define talking profiles as TP
                        ={ϕ
                        
                           head
                        ,ϕ
                        
                           gaze
                        ,ϕ
                        
                           pose
                        ,ϕ
                        
                           pupil
                        ,ϕ
                        
                           eye
                        ,ϕ
                        
                           mouth
                        }. Assume ϕ
                        
                           i
                        
                        ∈
                        TP is one type of face motion, then it can be expressed as follows in the temporal order, where ς
                        
                           i
                         is a local motion:
                           
                              (1)
                              
                                 
                                    
                                       ϕ
                                       i
                                    
                                    =
                                    
                                       
                                          ς
                                          1
                                       
                                       
                                          ς
                                          2
                                       
                                       …
                                       
                                          ς
                                          t
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

For a talking profile, there are 6 different types of face motions, and each type of face motion is a sequence of local motions. In this section, we address how to encode the abnormality weight to each local motion. The motivation is from psychological studies [10] which prove that the human visual system uses visual abnormality for recognizing faces.

Considering human variations (e.g. ethnicity, gender, etc.), we employ a Gaussian mixture model, G
                        ={g
                        1,g
                        2,g
                        3, …,g
                        
                           τ
                        }∀
                        i, g
                        
                           i
                        
                        ~
                        N(μ
                        
                           i
                        ,σ
                        
                           i
                        ), to estimate the local motion distribution in each type of face motion space. μi
                         and σi
                         are estimated by using expectation maximization [16]. In our implementation, we will run the Gaussian mixture model six times as we have 6 types of face motions. Then given a local motion ς
                        ∈
                        ϕ
                        
                           i
                        , we use maximum likelihood to find its intrinsic Gaussian distribution as κ, where 
                           
                              ς
                              ∈
                              
                                 g
                                 κ
                              
                              ⇔
                              κ
                              =
                              
                                 
                                    arg
                                    max
                                 
                                 i
                              
                              P
                              
                                 
                                    ς
                                    |
                                    
                                       g
                                       i
                                    
                                 
                              
                           
                        . After knowing its Gaussian distribution, the probability of this local motion can be computed as P(ς|g
                        
                           κ
                        ). Then, we approximate its abnormality, ω, as ω(ς)=1−
                        P(ς|g).

Given two talking profiles, each with 6 types of face motions, we need to compute the similarity of corresponding face motions from both talking profiles. In our work, six similarities in total will be computed, each for one type of face motion. Here, we choose pose change as an example. Assume that there are two pose change sequences from the two talking profiles, ϕ
                        ={ς
                        1,ς
                        2, …,ς
                        
                           n
                        } and φ
                        ={ς
                        1′,ς
                        2′, …,ς
                        
                           m
                        ′}, we first define how to compute the local motion similarity between two local motions ς
                        
                           i
                        
                        ∈
                        ϕ, ς
                        
                           j
                        ′∈
                        φ and then perform motion alignment.We define the local motion similarity W in Eq. (2).
                           
                              (2)
                              
                                 
                                    W
                                    
                                       
                                          ς
                                          i
                                       
                                       
                                          
                                             ς
                                             j
                                             ′
                                          
                                       
                                    
                                    =
                                    
                                       
                                          ω
                                          
                                             
                                                ς
                                                i
                                             
                                          
                                          ∗
                                          ω
                                          
                                             
                                                
                                                   ς
                                                   j
                                                   ′
                                                
                                             
                                          
                                          ∗
                                          
                                             
                                                Sim
                                                
                                                   
                                                      ς
                                                      i
                                                   
                                                   
                                                      
                                                         ς
                                                         j
                                                         ′
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                D
                                                RAD
                                             
                                             
                                                
                                                   
                                                      g
                                                      s
                                                   
                                                   
                                                      
                                                         g
                                                         t
                                                         ′
                                                      
                                                   
                                                
                                                2
                                             
                                             +
                                             
                                                C
                                                1
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (3)
                              
                                 
                                    ω
                                    
                                       
                                          ς
                                          i
                                       
                                    
                                    =
                                    
                                       
                                          1
                                          −
                                          P
                                          
                                             
                                                
                                                   ς
                                                   i
                                                
                                                |
                                                g
                                             
                                          
                                       
                                    
                                    ;
                                    
                                       ς
                                       i
                                    
                                    ∈
                                    g
                                 
                              
                           
                        
                        
                           
                              (4)
                              
                                 
                                    s
                                    =
                                    
                                       
                                          arg
                                          max
                                       
                                       k
                                    
                                    P
                                    
                                       
                                          
                                             ς
                                             i
                                          
                                          |
                                          
                                             g
                                             k
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (5)
                              
                                 
                                    t
                                    =
                                    
                                       
                                          arg
                                          max
                                       
                                       k
                                    
                                    P
                                    
                                       
                                          
                                             
                                                ς
                                                j
                                                ′
                                             
                                          
                                          |
                                          
                                             g
                                             k
                                          
                                       
                                    
                                 
                              
                           
                        where ω(ς
                        
                           i
                        ) and ω(ς
                        
                           j
                        
                        ′) are the abnormality for each local motion, ς
                        
                           i
                         and ς
                        
                           j
                        
                        ′, respectively. Sim(ς
                        
                           i
                        ,ς
                        
                           j
                        
                        ′) is the similarity of the motion in Euclidean space. D
                        RAD(g
                        
                           s
                        ,g
                        
                           t
                        
                        ′) is the difference between two Gaussian distributions, gs
                         and g
                        
                           t
                        
                        ′. gs
                         is the intrinsic Gaussian distribution of ς
                        
                           i
                        , and g
                        
                           t
                        
                        ′ is the intrinsic Gaussian distribution of ς
                        
                           j
                        
                        ′. Note gs
                         and g
                        
                           t
                        
                        ′ can be the same or different. To estimate the difference between two Gaussian distributions, the common way is the Kullback–Leibler Divergence [17], defined as in Eq. (6).
                           
                              (6)
                              
                                 
                                    
                                       D
                                       KL
                                    
                                    
                                       
                                          p
                                          ∥
                                          q
                                       
                                    
                                    ≐
                                    ∫
                                    
                                       p
                                       
                                          x
                                       
                                       
                                          log
                                          2
                                       
                                       
                                          
                                             
                                                p
                                                
                                                   x
                                                
                                             
                                             
                                                q
                                                
                                                   x
                                                
                                             
                                          
                                       
                                       dx
                                    
                                 
                              
                           
                        where p(x) and q(x) are distributions. KL distance is non-negative and equal to zero if p(x)≡
                        q(x), however, it is asymmetric. Thus, we use its symmetrical extension, Resistor-average Distance (RAD), defined as in Eq. (7).
                           
                              (7)
                              
                                 
                                    
                                       D
                                       RAD
                                    
                                    
                                       p
                                       q
                                    
                                    =
                                    
                                       
                                          
                                             
                                                D
                                                KL
                                             
                                             
                                                
                                                   
                                                      p
                                                      ∥
                                                      q
                                                   
                                                
                                                
                                                   −
                                                   1
                                                
                                             
                                             +
                                             
                                                D
                                                KL
                                             
                                             
                                                
                                                   
                                                      p
                                                      ∥
                                                      q
                                                   
                                                
                                                
                                                   −
                                                   1
                                                
                                             
                                          
                                       
                                       
                                          −
                                          1
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

Similar to KL, RAD is non-negative and equal to zero if p(x)≡
                        q(x). Moreover, it is symmetric.

From Eq. (2), several points can be concluded. First, we reward the more abnormal local motions, seen as ς
                        
                           i
                         and ς
                        
                           j
                        
                        ′. The larger ς
                        
                           i
                         and ς
                        
                           j
                        
                        ′ are, the more abnormal W(ς
                        
                           i
                        ,ς
                        
                           j
                        
                        ′) will be. Secondly, the higher the similarity of the local motions in the Euclidean space is, the higher the final local motion similarity, seen as Sim(ς
                        
                           i
                        ,ς
                        
                           j
                        
                        ′). In our implementation, we define the similarity in the Euclidean space as the inverse of the Euclidean distance between ς
                        
                           i
                         and ς
                        
                           j
                        
                        ′. Note that ς
                        
                           i
                         and ς
                        
                           j
                        
                        ′ are local motions from the same type of face motion, thus they are vectors of the same length in the Euclidean space. Thirdly, if the intrinsic distribution of ς
                        
                           i
                         and ς
                        
                           j
                        
                        ′ is the same, then D
                        RAD(g
                        
                           s
                        ,g
                        
                           t
                        
                        ′) is equal to zero, otherwise it will be larger than 0. Therefore, W(ς
                        
                           i
                        ,ς
                        
                           j
                        
                        ′) penalizes the situation when ς
                        
                           i
                         and ς
                        
                           j
                        
                        ′ are from different Gaussian distributions. C
                        1 is a constant to avoid zero division. We set it to C
                        1
                        =1 in our implementation.

In the previous section, we describe how to compute the similarity between two local motions of the same type. Next we need to align these two motion sequences in temporal order (i.e. find the best matches between two sequences). Mathematically, we maximize the total local motion similarity score ϒ(ϕ,φ) between ϕ and φ as follows:
                           
                              
                                 
                                    
                                       
                                          max
                                          ϒ
                                          
                                             ϕ
                                             φ
                                          
                                          =
                                          
                                             
                                                
                                                   ς
                                                   
                                                      i
                                                      1
                                                   
                                                
                                                :
                                                
                                                   
                                                      ς
                                                      
                                                         j
                                                         1
                                                      
                                                      ′
                                                   
                                                
                                                ,
                                                
                                                   ς
                                                   
                                                      i
                                                      2
                                                   
                                                
                                                :
                                                
                                                   
                                                      ς
                                                      
                                                         j
                                                         2
                                                      
                                                      ′
                                                   
                                                
                                                ,
                                                …
                                                
                                                   ς
                                                   ik
                                                
                                                :
                                                
                                                   
                                                      ς
                                                      jk
                                                      ′
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                          s
                                          .
                                          t
                                          .
                                          ∀
                                          
                                             i
                                             s
                                          
                                          ,
                                          
                                             i
                                             t
                                          
                                          ,
                                          
                                             j
                                             s
                                          
                                          ,
                                          
                                             j
                                             t
                                          
                                          ,
                                          
                                             i
                                             s
                                          
                                          >
                                          
                                             i
                                             t
                                          
                                          ⇒
                                          
                                             j
                                             s
                                          
                                          >
                                          
                                             j
                                             t
                                          
                                       
                                    
                                 
                              
                           
                        where i
                        
                           s
                        , i
                        
                           t
                        , j
                        
                           s
                        , j
                        
                           t
                         represent the frame number in temporal order. Temporal consistency is imposed in the constraint. This maximization problem is similar to finding the longest common sub-sequence, with the addition of an element continuous match scoring, instead of a binary 1/0 match scoring. Based on the local motion similarity described above, we propose a dynamic programming script to calculate the maximum matching score given two feature sequences. The rules of this dynamic programming are described in Algorithm 1. The general idea of Algorithm 1 is to continuously update the current best match up to action ς
                        
                           i
                        
                        ∈
                        ϕ in Table γ(ϕ,φ). As a convention, we use |V| to denote the length of vector V. Rule 1 is the initialization. Based on Rule 2, if there is only one local motion to match (i.e. either |ϕ|=1 or |φ|=1), then the matching weight would be W(ma
                        
                           i
                        , ς
                        
                           j
                        
                        ′). Based on Rule 3 (when there are more local motions to match), at each step we either abandon previous matches to local motion ς
                        
                           j
                        ′ and decide to match local motion ς
                        
                           i
                         with ς
                        
                           j
                        
                        ′ (i.e. W(ς
                        
                           i
                        ,ς
                        
                           j
                        
                        ′)) or prefer to keep the previous match to local motion ς
                        
                           j
                        
                        ′ (i.e. γ
                        
                           i
                           −1,j
                        (ϕ,φ)) and match the ς
                        
                           i
                         to one of the next local motions in the temporal order (i.e. 
                        
                           
                              W
                              
                                 
                                    ς
                                    i
                                 
                                 
                                    
                                       ς
                                       
                                          j
                                          ′
                                       
                                       ′
                                    
                                 
                              
                              s
                              .
                              t
                              .
                              
                                 j
                                 ′
                              
                              =
                              j
                              +
                              1
                              ,
                              …
                              ,
                              m
                           
                        ). Finally, based on the outcome of Rule 3, all of the next rows of the γ Table will be updated.

Through the above steps, we align two face motion sequences of the same type from both talking profiles in ϒ(ϕ,φ). Then we can compute the final similarity for pose change in talking profile scores as the summation of corresponding local motion similarity in ϒ(ϕ,φ). We repeat those steps to compute the similarity for gaze change, 2D in-plane change, pupil movement, eye/mouth opening-closing, respectively. Finally we employ support vector machine to perform classification [18] by using those similarities for verification.
                           Algorithm 1
                           Dynamic programming to align two sequences by maximizing total local motion similarities.


                              ϒ(ϕ,φ): Table γ(ϕ,φ) is an n
                              ×
                              m table where |ϕ|=
                              n and |φ|=
                              m.


                              i and j are row index and column index of table γ.
                                 
                                    Initialize i
                                       =0

Start Loop i
                                    

Rule 1: if i
                                       ≤0 then, γ
                                       
                                          i,j
                                       (ϕ,φ)=0

Rule 2: else if i
                                       =1 then, γ
                                       
                                          i,j
                                       (ϕ,φ)=
                                       W(ς
                                       
                                          i
                                       ,ς
                                       
                                          j
                                       ′)

Rule 3: else if i
                                       ≤
                                       n
                                    

Start Loop j


                                       
                                          
                                             
                                                γ
                                                
                                                   i
                                                   ,
                                                   j
                                                
                                             
                                             
                                                ϕ
                                                φ
                                             
                                             =
                                             max
                                             
                                                
                                                   W
                                                   (
                                                   
                                                      ς
                                                      i
                                                   
                                                   ,
                                                   
                                                      
                                                         ς
                                                         j
                                                         ′
                                                      
                                                   
                                                
                                             
                                             ,
                                             max
                                             
                                                
                                                   
                                                      γ
                                                      
                                                         i
                                                         −
                                                         1
                                                         ,
                                                         j
                                                      
                                                   
                                                   
                                                      
                                                         ϕ
                                                         ,
                                                         φ
                                                         )
                                                         +
                                                         W
                                                         
                                                            
                                                               ς
                                                               i
                                                            
                                                            
                                                               
                                                                  ς
                                                                  
                                                                     j
                                                                     ′
                                                                  
                                                                  ′
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       , j′ varies from j
                                       +1, …, m
                                    


                                       γ
                                       
                                          i
                                          +1,j
                                       (ϕ,φ)=
                                       γ
                                       
                                          i
                                          +1,j
                                       (ϕ,φ)+
                                       γ
                                       
                                          i,j
                                       (ϕ,φ)

End Loop j

End Loop i
                                    


                                       
                                          
                                             ϒ
                                             
                                                ϕ
                                                φ
                                             
                                             =
                                             
                                                argmax
                                                j
                                             
                                             
                                                
                                                   
                                                      γ
                                                      
                                                         n
                                                         ,
                                                         j
                                                      
                                                   
                                                   
                                                      ϕ
                                                      φ
                                                   
                                                
                                             
                                          
                                       
                                    

@&#EXPERIMENTS@&#

In experiments, we collected an identical twin free talking database at the Sixth Mojiang International Twins Festival held on 1st May 2010 in China. This database includes Chinese, Canadians and Russians. There are 39 pairs of twins (78 subjects) and each subject has at least 2 video clips at approximately 45s. A Sony HD color video camera is used to capture the video clips. They do not constrain the face position while speaking, so the expression, head and neck movement of each participant is realistic in each clip. Fig. 2
                         shows 6 subjects (3 pairs of identical twins) from this database. To the best of our knowledge, this is the largest twin video database in the entire research community.

We chose six facial appearance approaches, Eigenface [19], Local Binary Pattern [20], Gabor [21], Independent Component Analysis [22,23], Locality Preserving Project [24] and a commercial face matcher “Luxand faceSDK”, as baseline to compare our approach with the performance of using appearance to distinguish between identical twins. For each twin subject, we randomly select 8 images from the talking videos. The images are then registered by eye positions detected by STASM [25] and resized to 160 by 128. For Eigenface, we vectorized gray intensity in each pixel as feature and performed principle component analysis to reduce the dimension. For LBP, we divided the image into 80 blocks. For each block, we extract the 59-bin histogram. For Gabor, we used 40 Gabor (5 scales, 8 orientations) filters and set the kernel size for each Gabor filter to 17 by 17. Principle component analysis is performed to reduce the feature dimension for LBP and Gabor. For Independent Component Analysis (ICA), we use Gabor as representation of the image. As suggested by [22], we employ the architecture I which we find a set of statistically independent basis image. For LPP, we also use Gabor as representation of the image. LPP is a dimension reduction technique that preserves the locality after projection. For the commercial face matcher, we use Luxand faceSDK. Luxand, Inc. is a private hi-tech company formed in 2005. Luxand faceSDK can output a similarity score range from 0 to 1 given two images. Here, 0 represents the most dissimilar, while 1 represents the most similar.

The experimental result is shown in Fig. 3
                        . From this figure, we can see that identical twins indeed pose a great challenge to appearance based approaches. If the threshold is set when the false accept rate is equal to the false reject rate, the accuracy is 0.644 for Eigenface, 0.654 for LBP, 0.658 for Gabor, 0.656 for ICA, 0.666 for LPP and 0.674 for Luxand faceSDK separately. We can also clearly see that there is no huge difference between Intensity, LBP, Gabor, ICA, LPP and Luxand faceSDK for twin verification. This result verifies the twin challenge. To better illustrate the result, we also demonstrate the score distribution of twin imposter and genuine of Luxand faceSDK in Fig. 4
                        . From this figure, we can clearly see that twin imposter can have very high similarity score even though they are not same subject.

In this experiment, we evaluate the performance of using talking profile to distinguish between identical twins. The sample rate for probe and gallery are set to be the same. For classification, we use 60% of the videos for training and the remaining 40% for testing. The training data and testing data are mutually exclusive and the training and testing videos are from different recordings. Our experiment tests the performance when FPS is equal to 2, 3, 4, 5, 7, 10, 15, 20, 30, 40, 50, and 60, respectively. For example, if the FPS is 25 and the video frame rate is 50, then only 2 frames are sampled per second. The verification accuracy is shown in Fig. 5
                        .

From Fig. 5, several points can be observed. First, the verification accuracies on identical twins are above 0.90. Compared with the best accuracy in [14] where it is 0.82 using facial expression and the best accuracy using appearance in the aforementioned section, our proposal shows a great improvement. These experimental results again verify the hypothesis that twins look similar but behave differently. Secondly, we can see three local maximum along the entire ranges of FPS rate: the first local maximum is at FPS
                        =6, then the second maximum is at FPS
                        =9, and the last one is around 30. These three peaks suggest that identical twins can be better recognized by talking profile with different speeds (fast, medium, and slow). The three ROCs for these three different sample rates in Fig. 3 clearly demonstrate the superiority of our proposal against traditional appearance based approaches on identical twins.

In these experiments, we consider the scenario when the probe and gallery have different FPS. Among the gallery, we assume that the FPS for all videos is the same. It is practical in real applications because the gallery is usually pre-collected. Our motivation to use different FPS rates between probe and gallery is to test the robustness of our algorithm when the video frame rate is not fixed. For example, the gallery video may be captured by a 50fps camera, while the probe video may be captured by a 100fps camera. Given a FPS in gallery, we compute the average accuracy of various FPS in probe to evaluate the performance. For example, if the FPS in the gallery video is 2, then we test the accuracy for each FPS except 2 in the probe video, and use the average accuracy to represent the performance. The performance in such setting is presented in Table 1
                        . The variance distance between the largest and the smallest is also listed. In terms of accuracy, for the identical twin database, the best performance can be achieved when the FPS in gallery is 5 or 7. When the FPS in gallery is either too large or small, the performance degrades significantly. We think the reason is because if the FPS is in the middle, the local motions in the gallery video at least have some overlap with the local motions in the probe video, otherwise the local motions in the gallery would jump too much or too little.

It would be interesting to see the individual discriminating ability of each type of face motion in talking profile to distinguish between identical twins. Hence, we conducted experiments with the same FPS in gallery and probe videos. Various FPSs, such as FPS
                        =2, 3, 4, 5, …, 10 are tested and we compute the average performance to evaluate the discriminating ability of each type of facial motions on the same FPS settings. We also use the average accuracy of each single type of face motion for evaluation. The final result is shown in Table 2
                        . We can see that the best performance of a single type of face motion i.e., 2D in-plane face translation, is less than 0.50. This result shows that even though the individual discriminating ability of each type of facial motion is low, together they convey enough identity specific information for recognition, as shown in Experiment 1. The reason may be because identical twins may have different face motions but are not restricted to one single type. For example, for pair A, their pose changes are different and gaze changes are the same, while for pair B, it is reversed.

Several points can be concluded from the experiments:
                           
                              1)
                              Twins can be distinguished by talking profiles. As shown in our first experiment, our algorithm obtains the best accuracy to recognize identical twins up to now. This conclusion verifies the observation that parents of twins prefer to use the motion of their children for recognition. The proposal in this work presents a new way to recognize identical twins, as suggested in previous research [5,6,11].

Though the proposed talking profile can provide enough identity information for twin recognition, each type of face motion in talking profile lacks such discriminating power. This proves that there exist some differences of face motions between identical twins, but those differences only occurs on some types of face motions and such difference may be subject dependent.

Synchronization of motion sequence is also an important factor affecting recognition performance. Different FPS can significantly degrade the accuracy, as seen in experiments 1 and 2. With the same FPS, the accuracy can be as high as 0.90, while for different FPS, it reduces to at best, around 0.70, when FPS in probe is 5 or 7.

Besides the identical twin database, we further investigate the possibility of using talking profiles for the non-twin population. To verify it, we collected a moderate database from YouTube. It contains 99 subjects in 228 clips of 45s each. These videos only have a single person talking. The quality of the video ranges from medium to high due to the variation of webcams. The person is either sitting or standing still and the environment can be either indoor or outdoor without controlled lightings. These types of videos range from speeches, technical talks to interviews. The gender and ethnicity of the speakers are diverse. Some examples from this database are shown in Fig. 6
                        .

To conduct the experiment, we use 60% of the videos for training, and 40% for testing. The training and testing are mutually exclusive. Based on our preliminary works on 20 subjects, the sample rate equal to 5 is a good balance between efficiency and accuracy. Therefore, we use the sample rating equal to 5 in the following experiments. The verification accuracy for the YouTube non-twin database is around 0.87, the corresponding ROC curve is shown in Fig. 7
                        .

Although our focus in this work is on distinguishing between identical twins, this experiment on the YouTube non-twin dataset indicates the potential of talking profile to be used as a biometric for generic (non-twin) populations. We also conduct the experiment to investigate the performance of each type of face motion on the YouTube dataset, similar to Experiment 4. The accuracies for face 2D in-plane translation, gaze change, pose change, pupil movement, eye open–close and mouth open–close magnitude are 0.45, 0.34, 0.32, 0.32, 0.45 and 0.40. The result is also consistent with the identical twin database. From this experiment, we can see that even though a single type of facial motion cannot provide enough signature information for recognition, their combination can be used for recognition.

To test the robustness of talking profile against time, we further collected a moderate database from YouTube. The videos of the subjects are all across many years. An example is shown in Fig. 8
                        . There are in total 10 subjects in the database due to the difficulty of collecting.

In our experiments, we also set 60% of the data as training and the remaining 40% as testing. The subjects inside the training and testing are mutually exclusive. The accuracy is 0.715 when the sampling rate is set to 5. The corresponding ROC curve is shown in Fig. 9
                        . From this experiment, we can see that even though the time degrades the performance of our talking profile from 0.87 to 0.715, our talking profile can still provide some signature identity information through the years. This further verified that our proposed model is invariant to the aging process. We will conduct experiments on a larger dataset in the future to further validate the robustness of our approach for unconstrained verification in the general population.

@&#CONCLUSION@&#

Distinguishing between identical twins is a challenging problem in face recognition. In this paper, we verify that the talking profile can be used to distinguish between identical twins. To use talking profiles, we proposed a framework, EMRM, to effectively use identity-related abnormalities in face motions, with explicit focus on temporal information in the motion sequences. The experimental results on 2 databases, collected under free talking scenario, verified the robustness of our algorithm with both fixed and variable frame rates. We also suggest the most discriminating face motion type and best gallery video sampling rates for archival to achieve best performance for twin and non-twin subjects. Finally, our results on the YouTube non-twin database show the potential of talking profile to be used for general subject recognition.

For future works, several points need more efforts. Firstly, at the current stage, during data collection setting, we are encouraging the subjects to perform face and head motion in a natural free talking, while discouraging face and head motion with some specific task, such as slapping one's face. In such way, the data mainly contains the natural motions instead of accidental motions. In practical scenario, it is very important yet very challenging to separate the natural motion with accidental motion. Secondly, the video length in our current database is around 45 to 60s. We set such length via the empirical observation that enough face and head motion have been captured. It is still desired to analyze the optimal video length for our algorithm. Thirdly, though the current experimental results are promising, we would investigate the scalability of our model on an even larger dataset, to explore the scalability and stability of face motion features. Fourth, we will test the stability of talking profiles with increased time intervals. Finally, we will explore if the accuracy can be boosted by cascading face motions at different sample rates.

@&#REFERENCES@&#

