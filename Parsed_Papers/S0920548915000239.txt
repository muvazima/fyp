@&#MAIN-TITLE@&#Interoperable architecture for joint real/virtual training in emergency management using the MPEG-V standard

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           An architecture for joint real/virtual training in emergency management is proposed.


                        
                        
                           
                           Deployment of a middleware using MPEG-V where different applications interoperate.


                        
                        
                           
                           Deployment and integration of a subsystem for video management


                        
                        
                           
                           The proposed system improves the ways of actuation and response in crisis events.


                        
                        
                           
                           The system has a satisfactory performance and that the use of resources is adequate.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Command and control

Emergency management

Interconnection

MPEG-V standard

Virtual

@&#ABSTRACT@&#


               
               
                  Operatives' training is crucial in emergency management. Traditional exercises to improve procedures interoperability and harmonization between agencies are complex and expensive. Research on command and control systems specifically designed for emergency management and on virtual reality use leads towards enhancing real world applications' capabilities, facilitating management and optimizing resource usage.
                  This paper proposes a new architecture for a training system based on the interconnection between real and virtual worlds extending the MPEG-V standard; allowing real and virtual units' simultaneous and real-time training using commercial off-the-shelf equipment, and including a novel subsystem for video management from both real and virtual sources.
               
            

@&#INTRODUCTION@&#

A virtual world (VW) is a computer-based simulated environment where users interact through avatars and intelligent agents. The current applications go beyond merely entertaining purposes [1,2] as they have become a powerful tool to enhance the capabilities of real world (RW) applications and have been widely used for training and learning in different areas, including military units; vehicle driving and flight simulators. Trainees can learn and practice how to perform tasks while working toward animated agents that can collaborate with human trainees in the virtual worlds [1]. Furthermore, the capabilities of traditional virtual systems can be enhanced and extended if a hybrid system is implemented including data from real sources [3,4].

In the case of crisis management, the use of virtual reality to create immersive training exercises for human beings allows personnel to operate with modern computer equipment; respond rapidly to unforeseen events in situations under stress, and to perform joint exercises with a significant reduction on costs and complexity. However, the interaction of real and virtual worlds with new standards (i.e., MPEG-V), allowing data streaming between both worlds and using commercial off-the-shelf (COTS) equipment, has not yet been exploited and means going a step further in the state of the art.

This paper presents a new architecture for emergency management training that connects virtual worlds and command and control systems operating in the real world. An innovative and key feature is the use of the MPEG-V standard, which has been extended to develop a middleware, called “interconnection gateway”, where different applications can interoperate, contributing to an extendable and scalable solution.

Another significant contribution is the inclusion of a subsystem for streaming, displaying and recording video flows from both real world cameras and virtual world live streams.

A continuous training in a virtual environment based on the proposed architecture helps the staff that intervenes in real crisis resolution enhance their actuation protocols, considerably reduce the response time in the case of an emergency, and improve their effectiveness. Any user can gather information about its state through sensor values, video streaming or specific messages. Any actions performed in the real world are represented in the virtual world (and vice versa).

The main technical challenges and goals are:
                        
                           •
                           Represent the reality with high fidelity for crisis managers and first responders (those who first assist in an emergency situation) in order to help develop actuation strategies and homogenize procedures.

Enable collaboration among teams of users, software agents and avatars. Crisis management personnel from different organizations and locations should be able to collaborate in developing strategies.

Provide interoperability, interaction and overlapping between real and virtual worlds using the MPEG-V standard to stream real data in a virtual world and vice versa. It is necessary to make an extension of the standard to fit the particularities of the project and the tools used.

The paper is structured as follows: first, in Section 2, the related work core concepts and literature gaps are summarized. The system architecture is described in Sections 3 and 4; Section 5 contains the data model, and video insertion is explained in Section 6. A performance evaluation is carried out in Section 7. Finally, in Section 8 the conclusions from the study and future work will be commented.

@&#RELATED WORK@&#

Research related with the ongoing work can be divided into three main topics: use of virtual worlds (VW) for training, command and control systems, and the MPEG-V standard.

This addresses several areas.

Models for virtual world representation describe the worlds in such a manner that browsers can efficiently visualize the geometry of the worlds and, in some cases, support low level interactivity. However, in order to support richer interactions, and to improve agent reasoning [5], a high level representation model including semantic information [6,7] is desirable, as well as the use of ontologies to model such information. A possible way to address these issues is to use the MPEG-V standard [8], which, besides aiming at interoperability, sets a base data model to represent information and define interactions with virtual entities. We will be discussing more about MPEG-V throughout the paper.

There are also several studies on how to reconstruct scenes and users from the real world to create a real-time full 3D representation that can be placed inside a shared virtual world [4,9,10], and even with a focus on tactical information [11].

Different holistic architectures have been used to test semantic models. Normally a middle layer has been used as an interface between the agents and the virtual world and it models the world through a semantic representation built by instantiating a set of ontologies. This kind of architecture is reusable and allows decoupling the graphical representation from the semantic representation of the environment; thus, agents can interact directly with the semantic layer and process the semantic representation of the virtual world [6]. Others [1] propose an architecture with separate components running in parallel and which communicate by exchanging messages through a dispatcher, increasing modularity.

However, architectures going beyond representation, and aiming to allow direct interaction between the real and virtual worlds are much fewer, and even more if the focus is emergency management. We intend to fill this gap, and propose a layered architecture and a middleware connecting a command and control system from the real world, currently used in emergency management, with a virtual world representing the same operation field.

Users often experience difficulties and can easily become disoriented or lost for a variety of reasons, including those common to real world environments (e.g., getting trapped inside a building), as well as some problems unique to virtual worlds such as lack of landmarks and reduced level of detail [6]. Many methods and techniques may be combined to solve these issues depending on the application; for instance, animated agents can serve as navigation guides preventing users from becoming lost [1]; or social network analysis may be used to adjust the level of detail to represent an avatar based on its interactions with other entities [12].

Authors in [6] deepen further on this matter and provide a summary of techniques for navigation support. We have implemented artificial intelligence algorithms to determine the best possible path to go from one point to the other and preventing agents from getting lost.

The addition of life (populate the environment with autonomous agents that behave in a life-like manner [6], such as humans and animals) improves the user experience and realism of the simulation. Authors in [13] developed semantic applications that model the real world by associating ontology concepts with objects and locations. Also, augmented reality is another interesting field of research; it increases the sensations of the user by taking computer generated information (audio, video, haptic, etc.) and overlaying it, in real-time, in the real world environment [14]. In the case of an emergency management training exercise, it would be enriched by adding surprise elements, for example sudden appearance of injured people, and contribute to analyzing actuation protocols.

A key aspect that must be considered when designing new implementations where different systems interoperate is the use of standards [15]. However, efforts on developing standards for virtual worlds are relatively recent, beginning with the Metaverse 1 project [16], and only until 2011 an official document was published by ISO/IEC. Under the general title Information technology — Media context and control, MPEG-V is an emerging standard for interfacing with virtual worlds [8,17] that sets the base layer for ensuring interoperability at the level of data representation [18]. It will be commented in more detail in Section 2.3.

There is a variety of tools (both open source and proprietary) for powering virtual worlds and developing new applications. However, there is yet none based on MPEG-V. Hence, the equivalent data model in MPEG-V format must be defined. In particular for this project, we have used a proprietary software platform, OLIVE (On-Line Interactive Virtual Environment) [19], and its equivalent MPEG-V data format, particularized for our project, will be commented' in Section 5.2.

C4ISR systems (Command Control, Computers and Communications Information Surveillance and Reconnaissance) comprise a wide number of architectures and computer and communication systems. Their main goal is to obtain information from the operations field and deliver it, conveniently formatted, to those in command of a military or safety operation so that they build an adequate vision of it and can make the right decisions. Also, they must act as communication platform to transmit command orders and any other information that might be considered relevant in real time, that is, in a valid time with respect to the timescale of the events taking place.

There are several command and control applications and architectures, both in the civilian and military fields; pointing to areas as military operations, air traffic management, space operation management, detection and actuation systems for natural disasters, operations in events of emergency; and even in the corporate field and the structure of organizations.

In emergency management, the first responders have the first contact with the situation and all information gathered through personal media, sensors and other devices is gathered and sent to the command and control system to determine the course of action according to the point of reference (where do we want to go) defined. Hence, the location of the devices is a key point in order to improve the situational awareness, a fundamental concept in the theory of command and control systems, which refers to the shared understanding of the situation; it implies analysis and feedback on what is happening and is essential for proper decision making. Authors in [11] propose an interesting visualization architecture to improve it.

One of the C4ISR systems that implement emergency management is SIMACOP (SIstema de MAndo y COntrol de Pequeñas unidades, Command and Control System for Small Units), developed by the research group at UPV [46] and currently used by the Spanish MoD, SIMACOP [20,21] constitutes a command and control system of tactical and operational level especially suitable for emergency management because it emphasizes, on the operational side, on ensuring the safety of the people involved in a first intervention; and, on the technical side, on the integration of sensors of all types, including video, and wireless data networks [20] as communications support for the command and control system. Fig. 1
                         shows the architecture of SIMACOP.

Applications where virtual and real worlds interoperate require a strong connection between them to reach simultaneous reactions in both worlds to any changes in the environment [9]. Thus, standardized, efficient, effective and intuitive interfaces between the real and virtual worlds are crucial for inter-virtual and virtual-real world communication [9,17]
                     

Standardization efforts in virtual worlds for connecting real and virtual worlds, particularly in creating a standardized format of representation, started with the Metaverse1 project in 2008 [22]. The main goal was to create a “Global Standard among Real and Virtual Worlds” to enhance interoperability between virtual worlds and applications. The target was MPEG-V.

The standard ISO/IEC 23005, MPEG-V, first published in 2011
                           1
                        
                        
                           1
                           Revisions to Parts 2–6 were published in early 2013.
                         under the general title Information technology — Media context and control, aims to offer a solid technical background for application and multimedia immersive and multidimensional services. It provides an architecture and specifies associated information representations to enable the interoperability between virtual worlds [8]. The standard represents the solution for integrating multisensory contents in user environments, ensuring interoperability as it offers a wide range of tools to represent such contents. In particular for this project, MPEG-V has been used and extended to develop an interconnection gateway where virtual and real worlds can interoperate and exchange data with the goal of offering an interoperable, scalable and with a standardized basis. This will be further explained in Section 4.3.

The MPEG-V standard consists of 7 parts [8]: Part 1: Architecture, Part 2: Control information, Part 3: Sensor information, Part 4: Virtual World Object Characteristics, Part 5: Formats for interaction devices, Part 6: Common Types and Tools and Part 7: Conformance and Reference Software. A brief summary of each is presented below.
                           
                              Part 1: Architecture.
                              Efficient, effective, intuitive and entertaining interfaces between users and virtual worlds are of crucial importance for their wide acceptance and use. To improve the process of creating virtual worlds a better design methodology and better tools are indispensable [8].

The MPEG-V architecture for object control and information exchange inside the real world, between virtual worlds or between real and virtual worlds is shown in Fig. 2
                                 . This architecture allows simultaneous reactions in both worlds to changes in the environment and human behavior.

Specifies syntax and semantics of the tools required to provide interoperability in controlling devices in the real as well as in the virtual world [8]. The control information can be used to fine tune the sensed information and the device command for the control of virtual/real world by providing extra information to the adaptation engine [23].

The basic structure of the control information is defined using CIDL (Control Information Description Language) and a set of tools is used for describing capabilities and preferences.

This is the part that deals with the representation of sensory effects, which are effects additional to the audio–visual content that the user perceives and that aim to enhance the experience by providing effects such as light, wind, vibration, etc. [24].

The media processing engine may adapt both the media resource and the metadata according to the capabilities of the rendering devices and effects are defined in a way to abstract from the authors' intention and be independent from the end user's device setting.

Authors in [25–27] focus their research on this part of the standard and propose a very interesting open source toolset for authoring and rendering sensory effects.

Defines attributes and characteristics of virtual world objects [23], which can be classified into avatars and generic virtual objects. They serve two purposes: to characterize various kinds of objects within the Virtual Environment (VE) and to provide interaction with it [8].

An avatar is, in general, a computer based graphic representation of a user [28] and by which the user exists and interacts in the virtual world. MPEG-V avatar metadata serves three purposes [8]: make visible the presence 
                                 [29] of a real user into the VE; characterize the user within the VE; and provide interaction with the VE.

Virtual objects are all other objects in the virtual world. Metadata defined in the standard for these objects allow characterizing them in the VE and provide interaction between virtual objects and users [23].

Contains the tools for exchanging information for interaction devices. The adaptation engine (which is not within the scope of standardization), performs bi-directional communications using data formats specified in this part of the standard and may also use tools related to Control Information [8,23].

Specifies syntax and semantics of the data types and tools common to the tools defined in the other parts of the standard.

Provides means for conformance testing. That is, bit-streams – XML descriptions – that conform or do not conform to the normative clauses of the other parts of MPEG-V and informative descriptions thereof including scripts for automated conformance testing [23].

There are many usage scenarios, depending on the type of media exchange, which can be: (i) information adaptation from real world to virtual world, (ii) information adaptation from virtual world to real world and (iii) information exchange between virtual worlds [8]. On the other hand, several instantiations are proposed in the standard; below, some examples of applications that have been developed based on these instantiations are briefly described.
                           
                              4D broadcasting/theater
                              It is possible to create and distribute sensory effect metadata. After all corresponding processes of binarization multiplexing and demultiplexing, the receiver extracts the metadata to configure devices and generate commands. Such device commands, defined by the MPEG-V normative, are delivered to sensory effects rendering devices. Several authors have made developments in this area. For instance, a framework for 4-D broadcast is presented in [30,31] for home entertainment; while in [32] the authors propose a method to generate 4D images with a smartphone application and [33] introduces a haptic-enabled system.

This is an example of how to take advantage of the potential of cameras beyond simply taking images and that can also be used as interaction devices, e.g., the authors in [34] make a study on the use of MPEG-V to control virtual objects by real world devices, focusing on the facial animation of avatars controlling an avatar's expression by facial movement instead of using featured points.

One of the goals pursued when connecting real and virtual worlds is to achieve interoperability seamlessly; in [35] the authors propose a method to transform sensed information from the real world to MPEG-V formatted data and to control virtual world objects.

The three base hypotheses to develop the proposed system are:
                        
                           •
                           Traditional training simulated-based systems are expensive and offer little flexibility.

Real training in the field of emergency management is expensive and complicated regarding the harmonization of procedures between agencies.

A network-based system that integrates reality and simulation (including virtual reality) will facilitate the interoperability and harmonization of procedures among agencies. On the other hand, it will be relatively cheap.

We are proposing a new architecture for an interoperable system that allows the interconnection of command and control systems, operating in the real world, with virtual world systems; particularly, the inclusion of real sensors on the virtual side of the system and the access to virtual sensors from real command posts deployed in the operating field. Thus, users will be able to train in virtual environments with the same tools that they would use during the real crisis mitigation.

An important consideration is the deployment of equipment on the field in the training area:
                        
                           •
                           Units act as mobile sensors and actuators in the mitigation of the simulated crisis, feeding the system with real data and responding to the orders of the crisis managers.

In the real world these units are connected to the operative command center through some communications system, such as a MESH network, WiMAX or a satellite terminal.

Sensors deployed in the real world will be accessible from the virtual world and vice versa: the position and identifier of the units created and spread in the real world will be accessible from terminals of the command and control system unfolded in the virtual world.

The proposed architecture has three main components (see Fig. 3
                     ):
                        
                           •
                           Tactical Trainer Server (TTS), where the global configuration of the system is set. It includes the virtual world server and the interconnection gateway, the key element to achieve interoperability between real and virtual worlds and which will be further explained in Section 4.3.

Tactical Trainer Client (TTC), client subsystem used by the training nodes; includes both a command and control system of the real world and a virtual world client.

Virtual Video Server (VVS), an innovative system for management, distribution and playback of video generated by the sensors involved in the operations field, from both real and virtual sources.

To achieve interoperability between real and virtual worlds, an interconnection gateway based on the MPEG-V standard has been developed. The main functionality of this gateway is to interconnect two systems (whether they are virtual worlds, command and control systems from the real world, or a combination of both), translate and adapt the relevant information following the MPEG-V guidelines and map the adapted data between them. The gateway must be able to take the information from the source system, translate it into MPEG-V and then represent it properly in the target system. Communication is bidirectional. The location of the interconnection gateway in the C4ISR system for emergency training is shown in Fig. 4
                     .

It is worth to stress the fact that our system is constrained to fit the most commonly used architectures in command and control systems, as well as in virtual worlds. We begin with a very specific target, interconnecting our own command and control system, SIMACOP, with a virtual world but, beyond this, our goal is to offer a modular and scalable architecture that can be easily customized to fit in new applications.

Thus, to define the architecture of the system, the first necessary step was to analyze the architectures commonly used in both command and control systems (particularly SIMACOP, as it has been the base application for developing the system) and virtual worlds, since the existing applications that we aim to interconnect are of one of these types.

Command and control systems, such as SIMACOP, usually have an architecture (Fig. 5
                        ) that reflects two main dimensions: the organizational structure, hierarchically divided (N0, N1, N2 …); and the communication network structure, with different network meshes (M0, M1 …).

The main properties of this architecture are:
                           
                              •
                              Fully distributed systems

Distributed database

Consistency based in a replication mechanism

Non-strict synchronization (usually in communications in tactical environments, since data has an expiration date, it is possible, and acceptable, that some pieces of information are lost).

On the other hand, for virtual world systems, a centralized architecture (see Fig. 6
                        ) is commonly used. This kind of architecture offers advantages such as facilitating control and dabatase management. However, a system based on this scheme is more exposed to collapse if one component fails, or if a security attack occurs.

If we also consider load balancing and scalability with respect to the number of users and the use of resources, an architecture with associated servers would be a better approach (Fig. 7
                        ), as it offers many interesting features:
                           
                              •
                              Centralized server (although federated servers are used)

Centralized database

Rigorous consistency that determines the temporal and events granularity

Strict synchronization. This means that all critical data must be updated within the same time interval.

Next, it was necessary to address how data should be mapped between the real world and a virtual world. First, we defined the semantic mapping; this is how to adjust entities, their taxonomy, functionalities and interrelations in a world into the entities of the other. Table 1
                         shows the equivalence of the main entities from each world.

In order to adapt the temporal dynamics from both worlds and to solve possible incoherencies, mechanisms must be adapted. Also, the update rate from the command and control system must be adapted to a faster update speed of information in the virtual world. It is important to point out that this is an open line of research and possible future line of work.

Taking into account the main features from both C4ISR and virtual world architectures that better fit the sought functionalities and requirements; we defined the architecture for our system. As commented previously in Section 3, it has three subsystems: Tactical Trainer Server (TTS), Tactical Trainer Client (TTC), and Virtual Video Server (VVS). Fig. 8
                         shows the deployed components consisting of one TTS; an indeterminate number of TTC nodes and a VVS system.

The system has been implemented in such a way that the interconnection is completely transparent to the distributed replication schema between nodes and without affecting the regular functioning conditions of the existing systems, both from the real and virtual worlds. Thus, data (from position, threats, units, etc.) routed in the gateway to/from the RW nodes follows the schema:
                           
                              
                                 R
                                 W
                                 →
                                 V
                                 W
                                 
                                 from
                                 
                                 
                                    
                                       T
                                       T
                                       C
                                    
                                    i
                                 
                                 –
                                 R
                                 W
                                 
                                 t
                                 o
                                 
                                 T
                                 T
                                 S
                              
                           
                        
                        
                           
                              
                                 V
                                 W
                                 →
                                 R
                                 W
                                 
                                 from
                                 
                                 T
                                 T
                                 S
                                 
                                 t
                                 o
                                 
                                 the
                                 
                                 
                                    
                                       T
                                       T
                                       C
                                    
                                    i
                                 
                                 –
                                 R
                                 W
                                 
                                 that
                                 
                                 produces
                                 
                                 such
                                 
                                 information
                                 
                                 in
                                 
                                 the
                                 
                                 R
                                 W
                                 .
                              
                           
                        
                     

The information flows that are transported between RW and VW are: information related to the nature and location of units/avatars; threats and objects; and video flows. As future work, new modules are being developed to integrate messaging [36] and new sensor types.

One of the most outstanding contributions of the work conducted by the research group is the use of the MPEG-V standard for interconnecting virtual and real worlds. An extension of the standard was made to fit the command and control systems from the real world that will be used, as well as to the virtual world systems and the peculiarities of the data models usable in an emergency management scenario. Thus, a MPEG-V-based data model was proposed. It allows mapping information between RW and VW and will be further explained in Section 5.

The model is implemented in the interconnection gateway following a two-layer modular architecture (Fig. 9
                        ) which includes adaptation and translation processes also based on the standard. Following this schema, any application integrated into the system, whether it is real or virtual, would act as a separate module implementing a customized data model. This makes possible connecting any kind of C4ISR system with any virtual world where the emergency management training system is implemented.

Data is streamed following a communication mechanism based on the use of web services and query–response patterns that will be explained further in Section 4.4. This way, any command and control system from the real world could use the whole developed architecture if a data mapping and communication module was implemented according to the specifications of the developed model.

A basic conceptual scheme of how the gateway operates is shown in Fig. 10.

The data flow can be described with an example: data from the command and control real system (e.g., SIMACOP) is translated into MPEG-V format. Next, it must be translated to the OLIVE (or any other virtual world software platform that is being used) format to, finally, deliver it to the application and represent it properly. The reverse path would be similar. Data generated in OLIVE is translated into MPEG-V and sent for representation in the real world. The difference lies in that in the first step the data flow will not be translated from real into virtual but virtual to virtual.

Following the MPEG-V architecture (Fig. 2), the architecture of the interconnecion gateway is proposed in Fig. 11
                        
                        . Translation RV, VR or VV (depending on the case) will be carried out in the gateway using an MPEG-V extended data model (see Section 5) as an intermediate translation format.

The basic architecture of the communication mechanism is shown in Fig. 12
                        . Interaction between components is performed through web services.

When a client makes a request (1), whether it is from the real or a virtual world, the request is sent as an XML object through a web service (2) and delivered to the translation and adaptation layer in order to perform the VR, RV or VV adaptation (depending on the case) and to convert the data into MPEG-V format. The resulting data is again encapsulated in an XML object which – again via web services (4) – will be delivered to the corresponding module (5), (6) and awaits for a response. The response will travel in the reverse path. The web service delivers the response to the adaptation/translation layer, where the appropriate actions will take place, and afterwards the result is delivered to the client that started the request.

It is critical to verify that information both in the real and virtual world is updated and data is valid all the time, to ensure that generated XML data are correct. Also, in future stages, control parameters will be defined related to an adequate management of a real-time system considering the heterogeneous and distributed nature of the system [37,38].

As has been commented before, the use of the MPEG-V standard for data exchange is one of the key features of the proposed architecture. Therefore, and considering the needs of the particular project, an extension of MPEG-V data format has been made to define a base data model for the information that will be exchanged between the modules of the system and which could be easily customized when integrating new applications into the system.

It was necessary to make an abstraction of the syntax and semantics of each system, and adapt it to MPEG-V. To simplify this task (and having in mind that other applications may be integrated in the future), a generic data model (common intermediate format) was proposed. See Fig. 13
                     .

First of all, a base wrapper element is declared with a root node that has, in the initial version, four elements: (1) VirtualObjects: defines the information regarding avatars and virtual world objects. (2) SysElements: specifies the generic elements of a C4ISR system. (3) Control: describes the control information (according to the second part of the standard). (4) Interaction: commands to control devices and description of the information acquired by sensors.

Data definition for elements 1, 3 and 4 is based on the data definition templates from the MPEG-V standard. On the other hand, a separate schema has been created for the generic elements of the system, considering the most important characteristics of a command and control system and the information that will be exchanged.

From the analysis of SIMACOP syntax and semantics and the abstraction of what information should be sent to other modules to ensure data consistency and a proper implementation of the training system for emergency management, the equivalent data model for SIMACOP in MPEG-V was proposed (Fig. 14
                        ). The root element is SimacopNode and the children nodes: Alarm, Threat, Object, Position, Sensor and Message.

The elements Alarm, Threat, Position, Sensor and Message are defined based on the generic schema while Object is defined as shown in Fig. 15
                        .

Following a similar process, the virtual world platform OLIVE was analyzed to determine which elements were most relevant and should be exchanged with other applications. The equivalent data model was defined. The resulting schema (Fig. 16
                        ), as expected, is very similar to SIMACOP's.

Again, the elements Alarm, Threat, Position, Sensor and Message appear (since they are generic objects of the system). Besides these, four new elements are proposed: Avatar, VWO, Inventory and Object.


                        AvatarType describes the information associated with an avatar (Fig. 17
                        ). Its basic elements are: user identifier (UserID), name (AvatarName), avatar identifier (AvatarDoid), position (Position), associated template (TemplateId) and AvatarFullDesc; this last will contain additional describers that are defined in the MPEG-V standard and can be applied in OLIVE too.

In the same order of ideas, a type to describe virtual world objects (Fig. 18
                        ) is defined. It has an object identifier and type, the id of the object's owner, its attributes (name and type, grouped in AttributesType) and some additional descriptors based on MPEG-V.

Finally, elements Inventory (items associated with a virtual object, see Fig. 19
                        ) and Object. The last one has the same definition as a SIMACOP object.

Having access to a continuous video feed of what is going on in the operation hot spot enhances considerably the training process and also contributes to the improvement of the situational awareness. Moreover, information from media streaming can be used to create sensory effects and improve the user experience.

There are several other researches related to this subject. E.g., the authors in [39] make a summary of the existing technologies adapted to virtual worlds for video streaming and propose a protocol to stream video from IP cameras in the real world into virtual worlds developed under the Metaverse1 project [16]. In [40], the authors present a framework for a streaming service based on MPEG-V.

We have developed the Virtual Video Server (VVS), a subsystem for streaming, displaying and recording video flows from both real world cameras and virtual world live streams. Architecture is shown in Fig. 20
                     .

The VVS does not deal directly with video capture and coding, instead, it gathers video flows from real world sensors as well as from the virtual clients output. Those flows can then be streamed to any video client, such as the one embedded in a TTC, displayed in real time in the VVS interface or stored for post operation analysis. SIMACOP incorporates the possibility of watching video associated with units or from cameras located in the operation theater. Moreover, since SIMACOP's architecture includes wireless sensor networks, these can be used to implement video streaming and if resources were to be allocated for each sensor, this could increase flexibility, sturdiness, and impact positively in performance, as exposed in [41].

A key contribution of the VVS is that it decouples consumers from producers, as there is just one video flow from producer to VVS, regardless of the number of clients that request that particular flow. Moreover, the system is designed to support the connection of several VVS serving video flows among them and their respective clients.

One of the main challenges is guaranteeing scalability. To achieve this, the VVS acts as a gateway of the different existing video flows; it receives each one of them and forwards them, using RTP, to each potential client that may request it. Since multicast and other associated techniques are used, for each video flow there will be only two bandwidth consumer flows, instead of N+1 as it would be in the case of N clients.

Another key feature for de architecture design is modularity and adaptability. The VVS is an independent application from the gateway RW–VW. Its role is to manage the video flows generated by TTC and independent sensors, all in MPEG4 format, and to distribute them to the clients that request them.

When a TTC starts, it checks its associated VVS and begins sending video directly to the VVS's IP address (the IP is on its mission file). The VVS can determine which of the available cameras will be enabled for streaming and which will be enabled for recording. Streaming cameras can be visualized in the VVS main display and also in all those client nodes which access it. In the case of recording cameras, it can be determined which video flows are recorded in the VVS repository for later visualization. Recordings are fragmented in ten cuts with ten minutes length for greater management efficiency and convenience in subsequent finding and viewing. This way, the VVS operator has the capacity to determine which video flows will be available for all other nodes and which will be recorded. Once enabled, the flows can be watched in the VVS, as shown in Fig. 21
                     .

@&#PERFORMANCE EVALUATION@&#

For developing and testing the system, a platform was implemented to serve as test bench. It includes one TTS, one VVS, four TTCs and a command and control system from the real world. Fig. 22
                      shows the network structure of the platform, including sensors, and Table 2
                      summarizes the characteristics of the equipment used to set up the test bench.

The use of resources is a primary concern. Thus, both CPU and bandwidth consumption were measured for each instant of time (an instant being 1s) within an interval of 300s and different testing conditions for the TTS, TTC and VVS. Results are discussed throughout Sections 7.1 to 7.3.

The percentage of CPU consumption was studied as a function of the number of clients from one (Fig. 23a) to five (Fig. 23b). It can be seen that, as expected, as the number of clients increases, the percentage of used CPU increases as well. By applying a linear fit using least-squares to the experimental results, we determined that CPU consumption will increase proportionally to the number of clients until reaching a value of 49.89% for 25 clients. Fig. 23c shows the experimental results along with the predicted values.

A study on the consumption of bandwidth as a function of the number of clients in the gateway (GW) was made for two cases: data exchange between real and virtual worlds, and traffic information related to node state management when data is exchanged exclusively between virtual worlds.

First, we studied the use of bandwidth in the exchange of information between the gateway and client nodes for 1 to 6 client nodes. All traffic is TCP.

In the case of 3 clients (Fig. 24a) consumption follows a cycle with two evident states, one of higher replication activity and the other of lower. For five clients (Fig. 24b) although less evident, there are also two states of replication periods and the average bandwidth consumption is higher.

As expected, it was determined that bandwidth consumption increases depending on the number of nodes, from an initial value of 0.5Kbps for one client to 3.32Kbps for 6 clients. Based on these results, and to predict the behavior of the system for a larger number of nodes, we set an operative range of 25 clients in which the system does not collapse and therefore we can assume a linear relation within it. Thus, measurements are extrapolated and by applying a linear fit using least-squares we obtain the expected behavior, as shown in Fig. 24c and which can be expected for a larger number of clients.

Next, we analyzed the traffic exclusively on the virtual side of the system for 1, 3 (Fig. 25a) and 5 clients (Fig. 25b). It can be seen that, in contrast to the bandwidth consumption in the gateway where two states could be easily distinguished, the traffic does not follow a cycle and multiple consumption peaks occur. From the obtained results, we determined that bandwidth consumption increases with an approximate factor of 1.2 as the number of clients rises. Fig. 25c shows the predicted evolution (using the linear fit) for up to 25 clients.

Finally, if we compare the bandwidth consumption for the two cases studied, results show that bandwidth consumption in the gateway increases very mildly and with a much lower slope than the bandwidth consumption in the VW. This is reflected in Fig. 26
                           ; a logarithmic scale was used to better appreciate the differences between both cases.

Three cases were studied for a single client: (a) the TTC is not connected to the gateway, nor sending video; (b) the TTC is connected to the gateway without sending video; and (c) the TTC is connected to the gateway and streaming video.

In the first case (Fig. 27a) average CPU consumption was 17.12%, whereas when the TTC is connected to the gateway but not sending video (Fig. 27b) the average CPU consumption increases to 19.2%. Lastly, when the client is connected to the gateway and sending its own video (Fig. 27c) the impact on CPU consumption increases significantly, to an average value of 25.8%.

Two cases were studied: (a) when the client is not connected to the gateway (information replica is carried out only in the RW) and (b) the TTC is connected to the gateway.

The first case was studied for three (Fig. 28a) and five (Fig. 28b) nodes in the system. It can be seen how the real world replica introduces peaks of bandwidth consumption in different periods, as a function of the period of the information flows and bandwidth increments that go from 20 to 180Kbps according to how the periods of each replicated information flow are overlapped. The evolution of bandwidth consumption in the TTC as the number of client increases was predicted as shown in Fig. 28c.

For case b, bandwidth consumption for VW management in the TTC has been evaluated for a single client (unlike RW replica, that implies an indeterminate number of nodes) and, similarly to the CPU consumption study, three cases were analyzed: (a) no connection to the gateway and no video sent (Fig. 29a), (b) connection to the gateway without sending video (Fig. 29b), and (c) connection to the gateway and video streaming (Fig. 29c).

The performance of the VVS was evaluated by testing CPU and bandwidth consumption for a varying number of video flows mainly processed in scenarios where only streaming or recordings were considered.

CPU use was analyzed using 1 to 4 incoming video flows and for two cases: (a) streaming without recording and (b) streaming and recording. It was observed that for all tests CPU consumption was higher in case b.

The results for 4 video flows for both cases are shown below. Respectively for case a (Fig. 30a) and case b (Fig. 30b), the average values are 6.786 and 8.028; the maximum values are 29.760 and 34.869; and the variance 24.357 and 28.987.

Following a similar process as for the TTS and TTCs, we applied a linear fit to the experimental results with the goal of predicting the behavior of CPU consumption for up to 25 video flows for both cases. Results are shown in Fig. 30c; the blue dashed line represents streaming and recording, while streaming without recording is the continuous red line.

Measurements of bandwidth consumption were made for 1 (Fig. 31a) to 4 (Fig. 31b) incoming video flows. It can be seen that with a single flow the consumption varies greatly, while it remains more stable as with the larger number of flows.

Also, and as could be expected, results showed that the higher the number of video flows, the higher the bandwidth consumption. Fig. 31c shows the experimental results and the predicted evolution.

From all the obtained results, we can assert that the system offers an adequate performance and the use of resources is correct. An interesting future task would be to compare such results to competing systems, however to the extent of our knowledge none exists yet. Also, studies as [42–44] raise interesting questions about evaluation of actuation procedures, reduction of number of casualties, gathering of feedback from users and distributed constraint optimization.

@&#CONCLUSIONS AND FUTURE WORK@&#

Analysis and preliminary implementations have been performed using the virtual world engine and development environment OLIVE. Currently, there are tests being conducted using the open source platform OpenSim [45].

The basic consideration of command and control architectures that reflect two main dimensions has been taken into account to develop the proposed architecture: the organizational structure and the communication network structure. On the other hand, the distributed nature of the system allows simulating in real time the same crisis in different computers geographically separated, although the simulation server is centralized. The interactions of each participant (whether they are virtual or real entities) are seamless in order to maintain coherence during the whole simulation.

The interconnection gateway developed by the research team provides two significant contributions to the state of the art: first, it acts as a middleware connecting a C4ISR for emergency management with a virtual world that simulates accurately the real world where the C4ISR is operating. On the other hand, it implements a data model built as an extension to the MPEG-V standard, which contributes to guaranteeing interoperability.

The Virtual Video Server represents a separate subsystem to efficiently manage video flows from real and virtual sources. It does not deal directly with video coding, although this could be a line for future research, as well as video optimization and MPEG-V annotation.

We can assert that the proposed architecture for joint real/virtual training in emergency management offers an innovative, interoperable, flexible and scalable solution that helps improving the knowledge in the area of emergency management and, as a direct consequence, improve the ways of actuation and response in the event of crisis. Besides, results obtained with the test bench demonstrate that the system has a satisfactory performance and that the use of resources is adequate. Future plans include evaluating the quality of experience (QoE).

Future plans are to continue exploring different platforms for virtual world management to develop virtual environments representing different types of real environments, and to integrate stationary sensors and UAV (Unmanned Aerial Vehicle). Also, current work is being developed to add augmented reality into the training system, in order to measure and improve the quality of experience.

Also, we intend to perform tests related to network capabilities and service availability, which should provide a valuable set of results for analysis and validation of the system.

On the other hand, a continuous and key task that will be continued in the future is keeping track of new studies covering a gamut of applications from areas that are out of the scope of this paper, which will contribute to verify the validity of our system and raise new and interesting research challenges.

Finally, synchronization in distributed environments, such as in the case of VW, RW and the coupling between them, is a line of research in which the research team plans to also keep on deepening in the near future.

@&#ACKNOWLEDGMENTS@&#

This work has been partially funded by the Spanish Ministry of Science and Innovation (Ministerio de Ciencia e Innovación, MICINN) through the project “Sistema de Entrenamiento C4ISR Multimedia para Gestión de Emergencias, basado en la Interconexión del Mundo Real y Mundos Virtuales” (Ref. TIN2010-18372).

@&#REFERENCES@&#

