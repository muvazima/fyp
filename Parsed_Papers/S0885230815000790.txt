@&#MAIN-TITLE@&#Systematic review of virtual speech therapists for speech disorders

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A systematic review on virtual speech therapists (VSTs) was presented.


                        
                        
                           
                           The comparison of VSTs and traditional speech therapy was discussed.


                        
                        
                           
                           We analyzed intervention methods used in VSTs such as articulation therapy.


                        
                        
                           
                           Hearing impairments was observed as the most frequent disorder targeted by VSTs.


                        
                        
                           
                           We explored 3D virtual heads and games as efficient therapy delivery approaches.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Virtual speech therapist

Computer-based speech therapy

Speech and language disorders

Computer-based intervention

@&#ABSTRACT@&#


               
               
                  In this paper, a systematic review of relevant published studies on computer-based speech therapy systems or virtual speech therapists (VSTs) for people with speech disorders is presented. We structured this work based on the PRISMA framework. The advancements in speech technology and the increased number of successful real-world projects in this area point to a thriving market for VSTs in the near future; however, there is no standard roadmap to pinpoint how these systems should be designed, implemented, customized, and evaluated with respect to the various speech disorders. The focus of this systematic review is on articulation and phonological impairments. This systematic review addresses three research questions: what types of articulation and phonological disorders do VSTs address, how effective are virtual speech therapists, and what technological elements have been utilized in VST projects. The reviewed papers were sourced from comprehensive digital libraries, and were published in English between 2004 and 2014. All the selected studies involve computer-based intervention in the form of a VST regarding articulation or phonological impairments, followed by qualitative and/or quantitative assessments. To generate this review, we encountered several challenges. Studies were heterogeneous in terms of disorders, type and frequency of therapy, sample size, level of functionality, etc. Thus, overall conclusions were difficult to draw. Commonly, publications with rigorous study designs did not describe the technical elements used in their VST, and publications that did describe technical elements had poor study designs. Despite this heterogeneity, the selected studies reported the effectiveness of computers as a more engaging type of intervention with more tools to enrich the intervention programs, particularly when it comes to children; however, it was emphasized that virtual therapists should not drive the intervention but must be used as a medium to deliver the intervention planned by speech-language pathologists. Based on the reviewed papers, VSTs are significantly effective in training people with a variety of speech disorders; however, it cannot be claimed that a consensus exists in the superiority of VSTs over speech-language pathologists regarding rehabilitation outcomes. Our review shows that hearing-impaired cases were the most frequently addressed disorder in the reviewed studies. Automatic speech recognition, speech corpus, and speech synthesizers were the most popular technologies used in the VSTs.
               
            

@&#INTRODUCTION@&#

Humans are social creatures and communication via speech enables humans to interact and share thoughts in a way which is not possible for any other species. Speech impairment has been shown to have an adverse impact on learning, literacy, applying knowledge, developing and maintaining relationships with friends and family, and securing and keeping a job (McCormack et al., 2009). Any disorder in speech will degrade a person's role in society, dissuading them from interacting in social activities in a way that exploits their potential. This may lead to other social anxiety disorders and avoidance behavior (Beilby et al., 2012; Hawley et al., 2013; McLeod et al., 2013). Considering the wide range of speech impairments, the prevalence of people with such disorders, and the related undesirable consequences on society, the importance of appropriate and comprehensive rehabilitation programs is evident (Bhattacharyya, 2014). An inquiry published by the Senate Standing Committees on Community Affairs of Australia in 2014 (Report, 2014), for instance, reported an estimation of more than 1.1 million Australians with a communication disorder which is around 5% of the Australia population. Taking advantage of the advances in both software and hardware in computer systems and also the micro-miniaturization and mobility, computer-based speech therapy (CBST) environments, referred to as virtual speech therapists (VSTs), are becoming increasingly popular (Kagohara et al., 2013). Compared with traditional speech therapy, these environments are gaining increasing acceptance because of their versatility, availability, portability, and controllability (Abad et al., 2013; van Vuuren and Cherney, 2014). They can provide a solution to the shortage of speech-language pathologists (SLPs) in schools or in regional areas by reducing the number of face-to-face therapy sessions, resulting in more affordable services as well. They are also capable of delivering impartial judgements as feedback and provide useful automatic profiling materials (Henshaw and Ferguson, 2013; van Vuuren and Cherney, 2014).

To the best of our knowledge, this is the first systematic review on virtual speech therapists (VSTs). It contains studies that state the intention to test the effects of VST programs on articulation and phonological disorders due to conditions such as dyslalia, aphasia, dysarthria, childhood speech-sound disorder, residual articulation errors (e.g. lisping), or hearing impairment; however, we did not consider other disorders that may affect people's voice and speech such as Alzheimer's disease (Mesulam et al., 2014), autism (Khowaja and Salim, 2013) and Down's syndrome (Laws and Hall, 2014) to exclude individuals with concomitant cognitive impairment that can affect the success rate of articulation and/or phonological therapies and potential ability to engage with VSTs versus face-to-face therapy. To select the related studies, extract the information and present the results, we followed the PRISMA Statement (Liberati et al., 2009) focusing on three research questions: (1) what types of articulation and phonological disorders have VSTs addressed, (2) how effective was the VSTs in the therapy, and (3) what technological elements have been utilized in VST projects.

In the next section, we propose a description for VST and several features that a VST should support. This definition will be our reference for the term VST throughout the paper. Section 3 deals with previous reviews on VST. Section 4 describes our method to prepare this systematic review. We study the extracted results in detail in Section 5. An overall discussion on the results is presented in Section 6, and finally Section 7 concludes the paper.

Considering different intervention frameworks, an overall regime for the majority of speech-communication therapy starts with the assessment of the patient's strengths and weaknesses. The SLP designs an appropriate therapy program based on the profile of the person under therapy, the disorder, and short and long-term targets. The outcomes of the therapy sessions are evaluated using specific materials and tables (Chien et al., 2014). To the best of our knowledge, no computer-assisted speech therapy system has been developed to deliver the whole cycle of the therapy. The cognitive pattern of SLPs driving and managing the therapy, based on their knowledge and experience, is too complicated to be completely coded into the algorithmic nature of software applications (Wren et al., 2010). However, none of the studied VSTs claim to be an option to replace SLPs in the whole therapy.

We did not find a complete and comprehensive definition for VST in the literature on speech pathology and computer science. On one hand, there were many studies on speech pathology using computer-based intervention in which computers were not involved to the degree where they could be referred to as VSTs, and on the other hand, we found many published studies on disordered speech processing, where no formal intervention frameworks were considered. To specify a domain for this systematic review and define a discriminative concept for selecting publications from the existing huge number of studies, we defined VST as:
                        Definition 1
                        An interactive computer program that targets a specific speech deficit
                              1
                           
                           
                              1
                              For this systematic review, the speech disorders are limited to articulation and phonological disorders and speech-language problems in hearing-impaired people.
                            based on a predefined therapy program, but not a program designed to facilitate improvement in speech or language skills in learning a non-native language, in the absence of a diagnosed speech or language deficit, or to tutor individuals whose abilities are already within the normal range.

In the above definition, a computer refers to any electronic device which uses a processor to run a stored program in its memory; therefore, the platform can be a personal computer (PC) in the form of desktops or laptops, personal digital assistants (PDAs), tablets, or mobile phones. All 20 studies reported PCs as the platform, while two studies reported VSTs that could be run on PDAs as well (Danubianu et al., 2009; Schipor et al., 2010).

We carried out a search to find any types of reviews on virtual speech therapy and to the best of our knowledge, this paper is the first systematic review on VSTs. We found several reviews in other related fields which address computer-assisted language learning (Golonka et al., 2012; Henshaw and Ferguson, 2013; Lidström and Hemmingsson, 2014) which may be useful for the reader; however, we do not include them in this paper since the focus is on articulatory and phonological skills in speech pathology.

@&#METHOD@&#

To carry out a standard and transparent systematic review, a process based on the PRISMA Statement was followed (Liberati et al., 2009). Studies were eligible if a computer-based therapy was utilized in a speech therapy for articulation and phonological disorders. They were also required to present quantitative or qualitative assessments and have the potential to cover the three research questions:
                        
                           1.
                           What types of articulation and phonological disorders have VSTs addressed?

How effective were the VSTs in the therapy?

What technological elements have been utilized in VST projects?

The eligibility criteria are detailed in the following subsection.

We considered various types of studies, such as meta-analysis, systematic reviews (Liberati et al., 2009), control trial and randomized control trial studies (Schulz et al., 2010), cohort studies (Furberg and Friedman, 2012) and case series and case reports (Berg and Lune, 2011) which study the efficiency of VSTs in the rehabilitation of articulation and phonological disorders. A definition of the study types can be found in the references provided. Papers were restricted to articles written in English and published between 2004 and 2014 (research carried out over the past ten years) as computer-based technology is a fast-developing field, and earlier studies become obsolete or superseded. The last search was run on July 28th 2014.

Participants of any age and any sex with articulation and phonological impairments due to the conditions such as dyslalia, aphasia, dysarthria, childhood speech-sound disorder, residual articulation errors (e.g. lisping), specific language impairment (SLI), or hearing impairment were taken into account. Studies including disorders such as autism (Khowaja and Salim, 2013), Alzheimer‘s disease (Mesulam et al., 2014), and Down's syndrome (Laws and Hall, 2014) were excluded since the concomitant cognitive impairment in individuals with these impairments can affect the success rate of the VST therapy because they may not be able to potentially engage with VSTs compared to the face-to-face therapy method.

This systematic review was limited to studies which analyze the effects of using VSTs in therapy sessions to improve articulation and phonological skills underlying speech production and/or speech comprehension. The adopted VST should match the VST definition presented in section 2.

This systematic review covers studies on both speech-language deficits and hearing-impaired cases; therefore, in quantitative studies, both speech production and speech comprehension measures are taken into account, such as the Goldman-Fristoe Test of Articulation (GFTA) (Goldman and Fristoe, 2000), percentage of constants correct (PCC) (Shriberg et al., 1997), correctness of pronunciation, task completion performance, word discrimination test (WDT) phonological assessment battery (PhAB) (Frederickson et al., 1997), phonological awareness (Gillon, 2004), hearing in noise test, sound pressure level, word recognition accuracy (WRA), BKB sentence test (Bench et al., 1979), average sentence level word accuracy, word naming score (WNS), and the word verification rate (WVR). In qualitative studies, the outcome measures are the interviews and/or questionnaires which were designed to address the research questions.

The studies were identified by searching electronic databases, scanning reference lists of articles and engaging in consultation with experts in the field of information technology and speech therapy. No limits were applied to the languages the proposed VSTs were designed for. This search was applied to Medline, PubMed,
                           2
                        
                        
                           2
                           
                              http://www.ncbi.nlm.nih.gov/pubmed.
                         ProQuest Central,
                           3
                        
                        
                           3
                           
                              http://www.proquest.com.
                         Web of Science,
                           4
                        
                        
                           4
                           
                              http://wokinfo.com.
                         Allied and Contemporary Medicine (AMED),
                           5
                        
                        
                           5
                           
                              https://www.ebscohost.com/academic/AMED-The-Allied-and-Complementary-Medicine-Database.
                         Informa Healthcare,
                           6
                        
                        
                           6
                           
                              https://www.informahealthcarestore.com/journals.html.
                         Wiley Digital Library,
                           7
                        
                        
                           7
                           
                              http://onlinelibrary.wiley.com.
                         Taylor & Francis,
                           8
                        
                        
                           8
                           
                              http://www.taylorandfrancis.com/.
                         Springer,
                           9
                        
                        
                           9
                           
                              http://www.springer.com/gp/.
                         ScienceDirect,
                           10
                        
                        
                           10
                           
                              http://www.sciencedirect.com.
                         IEEEXplore,
                           11
                        
                        
                           11
                           
                              http://ieeexplore.ieee.org/Xplore/home.jsp.
                         and ACM Digital Library
                           12
                        
                        
                           12
                           
                              http://dl.acm.org/.
                         electronic databases. The SpeechBite
                           13
                        
                        
                           13
                           
                              http://speechbite.com/.
                         database was also searched. Finally, we tried GoogleScholar
                           14
                        
                        
                           14
                           
                              https://scholar.google.com.
                         as an integrated and comprehensive academic search engine.

The following search terms were used to search all the databases: speech disorder*; language disorder*; dysarthr*; dyslalia*, articulat* disorder*; phonological disorder*; stutter*; apraxia; hearing impair*; hearing disorder*; deaf*; hard of hearing; aphasi*; voice disorder*; childhood apraxia of speech; virtual speech therap*; computer assisted therap*; computer assisted instruction; computer-based intervention; computer aided therapy; multimedia and; e-learning. Boolean operators were used to combine the terms as: compu* and speech, compu* and phonology*, compu* and articula*, compu* and virtual therap*, speech disorder* and treatm*, compu* and (speech or voice or language) and (disorder* or deficit or impair*).

919 shortlisted research studies were retrieved from individual databases through searches. A hand search of reference lists retrieved another 62 studies. After duplicates were removed, there were 782 studies. These studies were then filtered in three stages to address our research questions and meet the eligibility criteria. A review of the titles and keywords resulted in 219 studies. Titles which referred to disorders other than articulation and phonological disorders, and titles referring to second language learning and/or science tutors were omitted from the list. A review of the abstracts by the two review authors resulted in 46 studies that met the inclusion criteria. In this stage, studies with titles or keywords relating to articulation and phonological impairments but referring to the effects of other disorders such as depression, Alzheimer's disease, etc. on speech were omitted. We also removed studies without an assessment of the adopted VST and studies which used VSTs regardless of therapeutic objectives. The full texts of the 46 articles were then reviewed. These papers were filtered based on the VST definition mentioned in section 2, the information presented on the conducted experiments, and the reported technical description of their VSTs. There were disagreements between the two authors in this stage, which were resolved by discussion with the co-authors. The disagreements were mostly over matching the developed VST with Definition 1 (see Section 2), and the amount of technical information the studies presented on their VSTs. Fig. 1
                         shows all the phases of the identification and selection of the materials finally included in this systematic review. As shown in Fig. 1, 20 publications were finally selected.

We extracted the required information on: (1) study type, (2) characteristics of the participants (age, sex, the type of impairment(s) the participants had), (3) specifications of the therapy sessions (number of sessions, length of each session, how often the sessions were conducted), (4) intervention characteristics, (5) type of outcome measures and (6) outcomes. We organized this information in a single table (see Table 1
                        ) for the selected studies. In relation to the technical aspects of VSTs, the name of the adopted system, the environmental description and technological details were extracted as CBST name, CBST details and CBST technology, respectively. This technical information was included in another table (see Table 2
                        ).

@&#RESULTS@&#

As mentioned in the previous section, a total number of 20 articles on using VSTs in speech-language therapy between 2004 and 2014 were shortlisted. Fig. 2
                      details the publication trend of the shortlisted papers. In relation to the digital libraries, four of the selected publications were retrieved from the PubMed digital library (Massaro and Light, 2004; Segers and Verhoeven, 2004; Silva et al., 2012; Thompson et al., 2010), followed by three publications from ScienceDirect (Abad et al., 2013; Moore et al., 2005; Saz et al., 2009). Two publications from Springer (George and Gnanayutham, 2010; van Vuuren and Cherney, 2014), IEEEXplore (Chaisanit et al., 2010; Hawley et al., 2013), Taylor & Francis (Engwall et al., 2006; Eriksson et al., 2005), Informa (Stacey et al., 2010; Wren and Roulstone, 2008), and Wiley (Cole et al., 2007; Wild, 2009), and finally one from the ACM digital library (Fagel and Madany, 2008). One study was selected from the Computing and Informatics Journal
                     
                        15
                     
                     
                        15
                        
                           http://www.cai.sk/ojs/index.php/cai.
                      (Schipor et al., 2010) and one from International Journal on Advances in Life Sciences
                     
                        16
                     
                     
                        16
                        
                           http://unitedlifejournals.com/ijals/.
                      (Danubianu et al., 2009).

Details of the selected publications are presented in Table 1, ordered by publication date, including related references, study types, description of interventions, participant sample size, characteristics of the participants, outcome measures, and descriptions of the outcomes of the study. Table 2 also presents details about the twenty selected publications, but in a technical way, focusing on the developed VST. It refers to the name of the developed system, its application details, and its technological aspects. To generate both tables, one review author extracted data from the included studies, and the second review author double-checked and updated the data, based on the source publications. The disagreements, which were mostly about how to describe the interventions and outcomes in the tables, were resolved by discussion with the co-authors.

As shown by the box plot presented in Fig. 3
                        , sample sizes ranged from n
                        =0 (Cole et al., 2007) to n
                        =127 (Wild, 2009). The median for the measure of sample size among the reviewed studies was 13 (SD
                        =26.992).


                        Fig. 4
                         presents a box plot for the participants’ age ranges in the selected studies. The median is calculated as 11 (SD
                        =21.69). In relation to gender, based on the available information from the selected studies, 173 males and 130 females participated in the experiments. Three papers did not present any information on the gender of the participants (Danubianu et al., 2009; Fagel and Madany, 2008; van Vuuren and Cherney, 2014).

To generate this systematic review, we attempted to gather the most reliable studies on VST; however, the authors concluded that it was not a good idea to only include articles involving randomized controlled trials, for instance, hence it was decided to include both qualitative and quantitative studies in our shortlisted publications. Qualitative studies were based on user interviews as important client input during the analysis stage of the application development (Huttunen et al., 2014). Of the selected papers, two studies were exclusively based on interview sessions (Engwall et al., 2006; Eriksson et al., 2005) reporting improvements in the development of VSTs brought about by the results of the interviews. Two studies conducted interview sessions to qualitatively analyze the usability of their program (Cole et al., 2007; Saz et al., 2009), while providing quantitative test results as well. In all the four studies containing interviews, the purpose of the interviews was to scrutinize the developed systems, not the user.

Eighteen papers presented quantitative studies. Of these, sixteen studies measured the effects of the VST therapy on people with disorders; while two papers studied the efficiency of the VST in recognizing disordered speech (Hawley et al., 2013) and evaluating the similarities between the VST results with the results generated by SLPs (Abad et al., 2013). Control trial studies were reported in eight papers (Abad et al., 2013; Chaisanit et al., 2010; Danubianu et al., 2009; Hawley et al., 2013; Moore et al., 2005; Schipor et al., 2010; Segers and Verhoeven, 2004; Thompson et al., 2010), cohort studies reported in five papers (Fagel and Madany, 2008; George and Gnanayutham, 2010; Massaro and Light, 2004; Silva et al., 2012; Stacey et al., 2010), randomized control studies reported in four papers (Saz et al., 2009; van Vuuren and Cherney, 2014; Wild, 2009; Wren and Roulstone, 2008), and one study reported a single case design (Cole et al., 2007). Fig. 5
                         shows the distribution of study types among the selected papers. The control trial study is the most commonly used, being reported in eight papers.


                        Fig. 6
                         analyses the VSTs described in the selected studies based on the type of disorders they were developed to target. As the diagram shows, hearing impairment was the disorder most frequently addressed, as discussed in five publications (Chaisanit et al., 2010; Eriksson et al., 2005; Massaro and Light, 2004; Silva et al., 2012; Stacey et al., 2010). Of these five papers, four referred to children and young students. This reveals the importance of timely and persistent rehabilitation programs for hearing impairments in children which, if untreated for too long, can have a severe effect on children's language learning (Buran et al., 2014; May-Mederake, 2012). VSTs have a remarkable potential to create an interesting and engaging environment for these children for their therapy. Other disorders that were addressed were dysarthria in three studies (Cole et al., 2007; Hawley et al., 2013; Saz et al., 2009), aphasia in three studies (Abad et al., 2013; Thompson et al., 2010; van Vuuren and Cherney, 2014), dyslalia in two studies (Danubianu et al., 2009; Schipor et al., 2010), specific language impairment (SLI) in two studies (Engwall et al., 2006; Segers and Verhoeven, 2004), phonological impairments in one study (Wren and Roulstone, 2008), lisping in one study (Fagel and Madany, 2008), and finally samples with a few unclear words which was addressed in one study (George and Gnanayutham, 2010).

Various samples of VSTs were introduced and adopted in the selected studies. All the studies introduced their own developed tools and used them for intervention except one study, in which previously developed software was used (Wild, 2009). Interventions varied based on the technical abilities of the VST used, the disorder targeted, and the intervention framework the SLPs had selected. Fig. 7
                         presents a high-level analysis of the intervention categories for which VSTs were developed. Nine publications focused on articulation (Chaisanit et al., 2010; Engwall et al., 2006; Eriksson et al., 2005; Fagel and Madany, 2008; George and Gnanayutham, 2010; Hawley et al., 2013; Massaro and Light, 2004; Schipor et al., 2010; van Vuuren and Cherney, 2014) and eight publications considered phonological awareness intervention (Abad et al., 2013; Moore et al., 2005; Segers and Verhoeven, 2004; Silva et al., 2012; Stacey et al., 2010; Thompson et al., 2010; Wild, 2009; Wren and Roulstone, 2008). One study dealt with the acoustic features of pitch, vocal loudness, and duration, which was categorized in phonation class (Cole et al., 2007). Two studies had the features to deal with all three categories of articulation, phonological awareness, and phonation, which were classified in the general class in the diagram (Danubianu et al., 2009; Saz et al., 2009).

Based on the impairment, participants were trained and tested on different aspects of speech production and speech comprehension skills. Of the selected studies, only six focused on small parts of speech for phonological awareness and perception, such as phoneme synthesis, rhyming, syllabic awareness, word naming, etc. (Abad et al., 2013; Moore et al., 2005; Segers and Verhoeven, 2004; Silva et al., 2012; Wren and Roulstone, 2008). In seven studies, sound and words were considered for articulation assessment and training (Chaisanit et al., 2010; Engwall et al., 2006; Eriksson et al., 2005; Fagel and Madany, 2008; George and Gnanayutham, 2010; Hawley et al., 2013; Schipor et al., 2010). In three studies, longer parts of speech such as phrases and sentences were taken into account (Stacey et al., 2010; Thompson et al., 2010; van Vuuren and Cherney, 2014). The authors in Massaro and Light (2004) claimed that their word-level VST can be applied for both speech perception and speech production training. In Cole et al. (2007), the authors developed a VST for vowel pronunciation; however, the implemented virtual tutor used various sentences as audio feedback on the user's sound level and to make the session more engaging. One study contained all levels of speech, from isolated sounds and vowels to sentences (Saz et al., 2009). This product was developed as a package of different games for both phonological and articulation training.

The studies were heterogeneous in the sense of reporting the details of the conducted therapy sessions. Of the 20 selected studies, information on the therapy sessions was completely detailed in eleven studies, namely the number of sessions, the length of each session, and details on how often the sessions were conducted (Engwall et al., 2006; Eriksson et al., 2005; Hawley et al., 2013; Moore et al., 2005; Segers and Verhoeven, 2004; Silva et al., 2012; Stacey et al., 2010; Thompson et al., 2010; van Vuuren and Cherney, 2014; Wild, 2009; Wren and Roulstone, 2008). Only 7 studies mentioned the number of sessions and/or the length of the sessions (Abad et al., 2013; Cole et al., 2007; Danubianu et al., 2009; Fagel and Madany, 2008; George and Gnanayutham, 2010; Massaro and Light, 2004; Schipor et al., 2010). There was no information on the therapy sessions in two of the studies (Chaisanit et al., 2010; Saz et al., 2009).

The length of the therapy sessions varied from 25min (a 10-min test session and a 15-min interview) as reported in (Engwall et al., 2006), to 27h reported in (van Vuuren and Cherney, 2014). Fig. 8
                            (left) shows the statistical behavior for the length of the overall therapy in hours for twelve studies reporting this measure (see Table 1). The length of each therapy session also varied from 10min (Engwall et al., 2006) to 1h (Hawley et al., 2013; Stacey et al., 2010; Thompson et al., 2010). 30min sessions were popular, being reported in four studies (Moore et al., 2005; Silva et al., 2012; van Vuuren and Cherney, 2014; Wren and Roulstone, 2008). The statistical behavior of the length of each therapy session is presented in Fig. 8 (right). For the studies in which the therapy took more than one week, the frequency of sessions varied from one session per week reported in three studies (Segers and Verhoeven, 2004; Wild, 2009; Wren and Roulstone, 2008) to seven sessions per week reported in Hawley et al. (2013).

According to the type of intervention, various outcome measures were extracted from the selected studies. Four studies presented qualitative results (Cole et al., 2007; Engwall et al., 2006; Eriksson et al., 2005; Saz et al., 2009), three of these studies using interview sessions (Engwall et al., 2006; Eriksson et al., 2005; Saz et al., 2009), while the other one used both an interview session and questionnaires (Cole et al., 2007). All of these four papers studied the usability of VSTs.

Eighteen studies presented quantitative results based on quantitative measures. Of these 18 studies, in 16 studies SLPs were in charge of generating the outcome measures (Chaisanit et al., 2010; Cole et al., 2007; Danubianu et al., 2009; Fagel and Madany, 2008; George and Gnanayutham, 2010; Massaro and Light, 2004; Moore et al., 2005; Segers and Verhoeven, 2004; Silva et al., 2012; Stacey et al., 2010; Thompson et al., 2010; van Vuuren and Cherney, 2014; Wild, 2009; Wren and Roulstone, 2008). In one study, the same measures were generated by both the VST and the SLP (Abad et al., 2013). The goal of this study was to analyze the similarity between the generated measures. One study considered two measures of word recognition accuracy (WRA) generated by the VST and the measure of task completion performance as determined by the SLP.

We extracted various quantitative measures considering both speech production and speech comprehension skills. In relation to speech production, we extracted various measures such as the proportion of correct tries to pronounce words as mentioned in three studies (Massaro and Light, 2004; Schipor et al., 2010; van Vuuren and Cherney, 2014), sound pressure level (Cole et al., 2007), percentage of consonants correct (PCC) for the Goldman Fristoe Test of Articulation (GFTA) (Wren and Roulstone, 2008), degree of lisping (Fagel and Madany, 2008), the percentage of correct performance in sentence production (Thompson et al., 2010), correctness of vowel production (Chaisanit et al., 2010), correctness of sound production (George and Gnanayutham, 2010), and task completion performance (Hawley et al., 2013). The studies which considered speech comprehension focused on measures such as phonological awareness task scores (Segers and Verhoeven, 2004), word discrimination test score and phonological assessment battery (PhAB) (Moore et al., 2005; Wild, 2009), percentage of the word and sentence identification accuracy (Stacey et al., 2010), hearing in noise test (HINT) score (Silva et al., 2012), word naming score (WNS), word recognition accuracy (WRA) and word verification rate (WVR) (Abad et al., 2013).

To design a successful VST, an appropriate therapy program should be simulated and delivered by the integration of different algorithms and technologies. Here, we mention some of these technologies that were used in the introduced VSTs in the selected studies. We categorized five major building blocks namely automatic speech recognition (ASR), speech corpus, speech synthesizer, expert systems, and facial tracking systems. Fig. 9
                         shows the frequency of each of these blocks in the shortlisted publications in which a speech synthesizer is the most frequently used technology mentioned in 10 studies, followed by ASR reported in nine studies and speech corpus used in eight studies. Facial tracking and expert systems were each used in two studies. Fig. 10
                         shows the frequency of utilizing different technological building blocks among VSTs targeting different disorders. As shown in this figure ASR, speech synthesis and speech corpus were very popular in the studies to enrich VSTs.

Automatic speech recognition (ASR) technologies are required when the VST needs to make decisions based on the user's received utterance. Fig. 11
                            presents the frequency of using ASR technology in the shortlisted papers according to the type of intervention. Nine studies used ASR technology (Abad et al., 2013; Cole et al., 2007; Danubianu et al., 2009; Engwall et al., 2006; Eriksson et al., 2005; Hawley et al., 2013; Massaro and Light, 2004; Saz et al., 2009; Schipor et al., 2010). Of these nine studies, five used ASR in articulation intervention (Engwall et al., 2006; Eriksson et al., 2005; Hawley et al., 2013; Massaro and Light, 2004; Schipor et al., 2010), one in phonological awareness (Abad et al., 2013) and one in phonation therapy (Cole et al., 2007).

ASR has different technological layers, and an expert in speech processing is required to contribute if a VST needs to have this capability. There are many toolkits and frameworks that deliver ASR services which are available as open source tools or commercial products (Duarte et al., 2014). Appropriate ASR tools with suitable feature selection strategies and an acceptable level of efficiency should be selected (Rong et al., 2009), as the authors in (Massaro and Light, 2004) were not satisfied with the accuracy of their selected ASR toolkit. These challenges can lead to devising ASR-free VST architecture, in which it is preferred to record input speech for further reference, as stated in four studies (Fagel and Madany, 2008; Massaro and Light, 2004; Thompson et al., 2010; van Vuuren and Cherney, 2014). Three studies preferred to conduct human rating sessions (Fagel and Madany, 2008; Thompson et al., 2010; Wren and Roulstone, 2008) and seven studies developed special user interfaces through which a mouse and keyboard were used as the input medium
                              17
                           
                           
                              17
                              Note that some intervention strategies generally do not need the voice of the person under therapy in either table-top or computer-based forms.
                            (George and Gnanayutham, 2010; Moore et al., 2005; Segers and Verhoeven, 2004; Stacey et al., 2010; Thompson et al., 2010; Wild, 2009; Wren and Roulstone, 2008). An appropriate ASR technology could have been used to remove such inconsistencies, leading to more accurate results.

Two studies reported the use of facial feature tracking technology (Danubianu et al., 2009; Engwall et al., 2006). Tracking the facial features of the person under therapy can provide very good feedback for the virtual speech therapist to detect the user's emotional state and articulation deficits. This information helps the VST to select the next operational phase which can be delivering a new practice, providing new feedback, etc. To accomplish this goal, the face of the person under therapy is constantly tracked by cameras, and the emotional and clinical models are built by the program to infer the current status of the user (Chuang et al., 2014; Tjondronegoro et al., 2008). Facial feature tracking can be merged with speech features to detect speech impairments, since the correlation between jaw and lip position, for instance, can be detected for the corresponding speech acoustics (Danubianu et al., 2009; Engwall et al., 2006).

Ten studies used speech synthesis in their VST design (Abad et al., 2013; Cole et al., 2007; Engwall et al., 2006; Eriksson et al., 2005; Fagel and Madany, 2008; Hawley et al., 2013; Massaro and Light, 2004; Moore et al., 2005; Thompson et al., 2010; van Vuuren and Cherney, 2014). Of these ten studies, six studies used speech synthesis for articulation intervention (Engwall et al., 2006; Eriksson et al., 2005; Fagel and Madany, 2008; Hawley et al., 2013; Massaro and Light, 2004; van Vuuren and Cherney, 2014), three studies focused on phonological awareness (Abad et al., 2013; Moore et al., 2005; Thompson et al., 2010) and one study focused on phonation (Cole et al., 2007).

Speech synthesis is an important part of VSTs in the sense of human–computer interaction, since audio commands and feedback are automatically generated through speech synthesizers. The authors in Massaro and Light (2004) used this technology for their 3D head's articulation by adopting Festival text-to-speech synthesizer. In Fagel and Madany (2008), a modular audio-visual speech synthesizer (MASSY) was used to generate the audio-visual speech utterances. Phonetic transcription was realized by embedding the external program txt2pho from the HADIFIX speech synthesizer. Audio synthesis was used to generate the signal from the phonetic data and is rendered by the MBROLA speech synthesis engine. In this study, visual articulation was also generated with a simplified dominance model. Fig. 12
                            shows the frequency of using speech synthesizers in the shortlisted publications, showing the importance of this technology in articulation therapy.

Two studies reported the use of expert systems in their VSTs (Schipor et al., 2010; Ward et al., 2011). Expert system strategies may be used in different parts of VSTs to achieve a variety of goals. An expert system algorithm tries to emulate human decision-making by reasoning through a knowledge base in a particular subject area (Tyler, 2007). In Schipor et al. (2010), the authors designed an expert system to personalize the process of therapy, assist therapists with exercise selection, and to accomplish a self-correcting strategy for the system's knowledge base when differences between system's decisions and therapist's decisions were revealed. They also used triangular fuzzy sets to create an efficient model of the speech therapist's decisions. This system was found to be equally as effective when measuring pronunciation improvement as when a speech therapist made the exercise selection. However, in Eriksson et al. (2005), the authors concluded that the speech therapist should be involved at all stages of therapy and be able to select exercises based on patient's progress and motivation. Another study described the development of an expert system that involved specifying and codifying the rules that govern a clinician's response to the user's vocalizations (Cole et al., 2007).

Speech has a variable and nonlinear nature which makes speech recognition a challenging task, particularly when the system deals with disordered speech. All the reviewed studies that took advantage of ASR technology used learning algorithms as well to create a speech model upon which the user utterance is processed. The training data for these learning strategies are structured in a speech corpus containing both normal and disordered speech. Of the selected publications, all the studies that used ASR technology reported developing their own speech corpora except one, in which the type of intervention did not need a speech corpus since the algorithm was not based on a machine learning strategy (Cole et al., 2007).

There were other technologies used to complete VSTs in specific cases. Electropalatography is such a technology used in Massaro and Light (2004) along with ultrasound data from natural speech, in order to virtualize internal articulatory movements.

The therapy delivery approach is the human–computer interaction aspect of the VST. Various technologies and ideas were adopted to deliver the required information to the user. The two most popular technologies used in the reported VSTs were 3D virtual heads or animated heads and game-like user interfaces.

Nine studies used animated heads as a therapy delivery method (Abad et al., 2013; Chaisanit et al., 2010; Engwall et al., 2006; Eriksson et al., 2005; Fagel and Madany, 2008; George and Gnanayutham, 2010; Schipor et al., 2010; Thompson et al., 2010; van Vuuren and Cherney, 2014). For articulation therapy, virtual talking 3D heads was the most popular intervention technology where a real therapy session was simulated and virtualized with the virtual talking 3D head acting as a guide for users in animated form to deliver instructions and feedback. These animated guides describe how to use articulators, how to progress through the application and also how to generate useful audio-visual feedback and make the experience more personal, as the user is guided by a character rather than the computer (Abad et al., 2013; Cole et al., 2007; Thompson et al., 2010; van Vuuren and Cherney, 2014). In six designed VSTs, 3D heads were implemented in a multilayer way so the user can switch to transparent skin mode or sagittal view to see inside the articulators and the exact features that they may need to correct (Engwall et al., 2006; Eriksson et al., 2005; Fagel and Madany, 2008; George and Gnanayutham, 2010; Massaro and Light, 2004; Schipor et al., 2010). One study provided transparent skin mode with 2D animations within the user interface (Chaisanit et al., 2010). As shown in Fig. 13
                            (left), judging from the selected papers, this kind of intervention seems to be popular for people with hearing impairment and aphasia.

In relation to different disorders, six studies used games to deliver the therapy. Game-like features were one of the design recommendations put forward in a qualitative study with users of the ARTUR system (Eriksson et al., 2005). This is a popular and useful type of intervention to indirectly deliver exercises when the target population is children. Games, if well developed, make the therapy session more engaging and interesting. In the reviewed studies, games are used to improve phonological awareness in four studies (Moore et al., 2005; Segers and Verhoeven, 2004; Silva et al., 2012; Wren and Roulstone, 2008), vowel training in one study (Chaisanit et al., 2010), and articulation skills in one study (Saz et al., 2009). The authors in Moore et al. (2005) adopted an arcade-style reward selection to motivate users to follow the training stage. They also concluded that the difficulty of levels in game-like training was an important part of therapy. Fig. 13 (right) shows the disorders targeted by computer games. As shown in the figure, similar to the 3D heads, the category of hearing impairments was the disorder most frequently addressed using games as a therapy environments.

Whilst visual and audio cues given by 3D virtual heads were the most commonly used cues, a number of alternative and additional cues were described in computer-based therapy interventions. In a study on users of cochlear implants, visual feedback on accuracy was given by a green tick next to word in a sentence or red if it was not in the sentence. The sentence was repeated and presented acoustically and orthographically to maximize lexical feedback (Stacey et al., 2010). The authors in (Engwall et al., 2006) described the use of a screen color change and an optional written word as extra prompts. An extra animation to assist the user to connect with a sound was also described in (George and Gnanayutham, 2010). Similarly, in another intervention, each utterance was accompanied by multimedia displays (e.g. sustained/ah/causes a car to travel forward) (Cole et al., 2007). Baldi as a 3D in (Massaro and Light, 2004) could perform additional cues – e.g. voiced and voiceless segments for target sounds.

It was emphasized that negative feedback should not outweigh positive feedback (Eriksson et al., 2005). Several programs ensured that the child/user did not repeatedly fail an exercise and intervened to allow the user to progress (Segers and Verhoeven, 2004). One study concluded that the amount and detail of feedback should adapt online to performance and should only be given on a particular sound (other sound errors are logged for subsequent sessions). However, the ten feedback options used in this CBST were too crude and missed smaller errors.

An interface that can be personalized for color, sound and animation was found to be advantageous (George and Gnanayutham, 2010). The storage of sounds, pictures and texts for individual users was also recommended in the interview research by (Eriksson et al., 2005).

@&#DISCUSSIONS@&#

We encountered a great deal of heterogeneity when generating this systematic review. Focusing on our research questions, we attempted to manage the extracted information to draw an organized and structured conclusion. Here we discuss the limitations of this systematic review and present our findings according to our research questions. The quality of some of the studies was generally low, sample sizes were small and procedures were not outlined in sufficient detail. Interventions, outcome measures and type of computer-based intervention varied remarkably; therefore, general conclusions are difficult to draw. In the following, we discuss the extracted information in a way that answers our research questions.

Within the scope of the types of the disorders addressed by VSTs, we categorized various disorders and as mentioned in the result section (Section 5.4), hearing impairments were the most frequent disorder addressed by the shortlisted studies, with five studies dedicated to them. Combining these five studies with the results extracted on the technological building blocks showed that three of these five studies focused on articulation intervention (Chaisanit et al., 2010; Eriksson et al., 2005; Massaro and Light, 2004); two used ASR, speech corpus and speech synthesis technologies together with animated heads and games. The sample size of these five studies was less than 20 (median
                        =11, SD
                        =4.45). Among them, three studies showed a significant difference in VST improvement. All five studies mentioned using small parts of speech for the training stimuli; however, one of them considered both word and sentence level materials (Stacey et al., 2010). It is also important to mention that four of these five studies focused on children for therapy. Based on these data, we can state that VSTs developed for people with hearing impairment focus mostly on articulation, analysing small parts of speech, using technologies like ASR, speech corpus and speech synthesis. They use game-like features and animated heads to deliver the therapy.

Dysarthria and aphasia were the second most frequently addressed disorders, each being considered in three studies. In relation to dysarthria, one study focused on phonation (Cole et al., 2007), one study on articulation (Hawley et al., 2013), and one study considered both articulation and phonology (Saz et al., 2009). All three studies used ASR and speech corpus and two used speech synthesis. In (Cole et al., 2007), an animated head was used to deliver the therapy while in (Saz et al., 2009) games were used. Sample sizes were small (median
                        =9, SD
                        =6.55) and they used small parts of speech (sounds and words) in their studies. In the case of aphasia, two of the three studies were based on phonological awareness intervention (Abad et al., 2013; Beskow et al., 2008; Thompson et al., 2010) and one was on articulation (van Vuuren and Cherney, 2014). One of the three studies used small parts of speech (Abad et al., 2013), while the other two used longer parts of speech in their studies. Sample sizes were not large (median
                        =12, SD
                        =4). All three studies used speech synthesis. Only one used ASR and speech corpus (Abad et al., 2013), while the other two used animated heads to deliver the therapy. Based on these data, we can state that VSTs developed for people with dysarthria have ASR and speech corpus in their technological building blocks; however, in the case of aphasia, speech synthesizers and animated heads are more common. Five studies out of these six studies on dysarthria and aphasia reported significant differences.

In relation to other disorders the results are heterogeneous but the common specification is the small sizes of the participants recruited for the experiments.

In relation to experimental assessment, although almost all the studies agreed on the efficiency of VSTs in speech therapy (compared with the no-therapy groups using the pre- and post- tests), only six studies compared the performance of VSTs with traditional SLPs in speech-language therapy (Abad et al., 2013; Chaisanit et al., 2010; Danubianu et al., 2009; Schipor et al., 2010; Thompson et al., 2010; Wild, 2009; Wren and Roulstone, 2008). The authors in (Abad et al., 2013) compared the word naming score (WNS) generated by their VST with the SLP scores, finding that there was no significant difference between them. The authors in (Schipor et al., 2010) compared their VST with SLPs in the sense of training session management to determine the number, length and the content of therapy sessions. They came to the conclusion that there was no significant difference between the group managed by the VST and the group managed by the SLP. Two studies showed significant differences in the improvement of the participants as a result of their designed VSTs (Chaisanit et al., 2010; Wild, 2009); however, two other studies reported no significant difference in the performance of their VSTs compared with traditional therapy (Thompson et al., 2010; Wren and Roulstone, 2008).

Based on our extracted technical data, the speech synthesizer, speech corpus and ASR are the most frequently used technologies in the reported VSTs. ASR was used to analyze the incoming speech, while speech synthesis was used to generate natural speech. The popularity of these two technologies reveals the importance of human–computer interaction in the area of virtual speech therapist systems. Other technologies, such as face tracking and expert system modules, were also used.

@&#CONCLUSION@&#

This systematic review was written based on the PRISMA Statement to study papers published on virtual speech therapy environments developed for speech disorders. Data extraction was undertaken based on the following three research questions: What types of articulation and phonological disorders do VSTs address? How effective are virtual speech therapists? and What technological elements have been utilized in VST projects? The studies were very heterogeneous in terms of targeted disorders, sample size, outcome measures, assessment strategies, and technological aspects. The studies were not even homogeneous in the way they described and assessed their VSTs. Despite the inconsistencies, as we adopted an organized and structured way to review the studies, it is possible to reach some useful conclusions to answer the research questions and help future work on VST. A study of the disorders that were examined in the shortlisted papers (research question 1) showed that hearing impairments, aphasia, and dysarthria were the most frequently studied disorders, being addressed in five, three, and three studies, respectively. Lisping, SLI and phonological impairments were the other disorders examined in the studies. Hearing impairments were the most common disorder to be addressed by sophisticated VSTs involving complicated human–computer interaction technologies, such as 3D virtual heads and games. There were many recommendations for customizing and individualizing VSTs, according to the patients’ clinical condition. In relation to effectiveness (research question 2), all the studies agreed on the effectiveness of the VSTs and there were no recommendations to not use VSTs; however, there was no consensus on the superiority of VSTs over human speech pathologists. In relation to the technical issues (research question 3), adopting ASR, speech synthesis and speech corpus was very popular in the studies to enrich VSTs. Based on the publications we reviewed, the technical challenges of integrating cutting-edge technologies to develop a united functional VSTs dwarfs the clinical challenges. This complexity affected many studies in which the authors had to impose human interference in their computer-based therapy because of the lack of speech recognition systems or speech synthesizers embedded within their VSTs. Fortunately, these complex technologies are becoming more user-friendly and reusable, making it possible to expect more standard and efficient VSTs in the future.

@&#REFERENCES@&#

