@&#MAIN-TITLE@&#Bi-modal biometric authentication on mobile phones in challenging conditions

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We examine bi-modal (face/speaker) authentication in challenging mobile environment.


                        
                        
                           
                           We release new protocols (and data) with significant mismatch conditions (MOBIO).


                        
                        
                           
                           We study bi-modal and multi-algorithm fusion using generative modelling techniques.


                        
                        
                           
                           Multi-algorithm and multi-modal fusion provides a consistent performance improvement.


                        
                        
                           
                           The proposed bi-modal system significantly outperforms the state-of-the-art.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Face authentication

Speaker authentication

Bi-modal authentication

Gaussian mixture model

Session variability

Inter-session variability

Total variability

I-vector

Fusion

@&#ABSTRACT@&#


               
               
                  This paper examines the issue of face, speaker and bi-modal authentication in mobile environments when there is significant condition mismatch. We introduce this mismatch by enrolling client models on high quality biometric samples obtained on a laptop computer and authenticating them on lower quality biometric samples acquired with a mobile phone. To perform these experiments we develop three novel authentication protocols for the large publicly available MOBIO database. We evaluate state-of-the-art face, speaker and bi-modal authentication techniques and show that inter-session variability modelling using Gaussian mixture models provides a consistently robust system for face, speaker and bi-modal authentication. It is also shown that multi-algorithm fusion provides a consistent performance improvement for face, speaker and bi-modal authentication. Using this bi-modal multi-algorithm system we derive a state-of-the-art authentication system that obtains a half total error rate of 6.3% and 1.9% for Female and Male trials, respectively.
               
            

@&#INTRODUCTION@&#

Mobile phones have become an integral part of many people's daily life. They are used not just for telephonic communication, but also to send and receive emails, take photos or even have video conversations. This has led to the mobile phone being an inherently multimedia device, which often has a front-facing camera in addition to the standard microphone. Hence, it forms an exciting new device that allows researchers to explore the applicability of bi-modal (face and speaker) authentication in challenging mobile phone environments.

This exciting challenge of bi-modal authentication in the mobile phone environment has begun to receive more attention. An international competition was organised in 2010 [1], where researchers evaluated state-of-the-art algorithms for face and speaker authentication using Phase I of the MOBIO database [2]. In this evaluation, enrolment was exclusively performed with mobile phone data. It was shown that a combination of these systems produced an impressive bi-modal authentication system. Since then other researchers have examined methods to perform face [3,4], speaker [5,6] and bi-modal [7,8] authentication in the challenging mobile phone environment.

A theme common to some of the prior work on biometric authentication in a mobile environment is the idea of session variability modelling [5,4], which achieves state-of-the-art results for bi-modal authentication [8]. Session variability modelling aims to estimate and suppress any variability such as audio or image noise that may cause confusion between different observations of the same biometric identity. In [5] session variability modelling was used to cope with audio channel variability, while [4] introduced this concept to face authentication, where its application was supposed to reduce the impact of pose and illumination variation. Finally, in [8] state-of-the-art face and speaker authentication systems that used inter-session variability (ISV) modelling were combined to derive a state-of-the-art bi-modal authentication system. However, this prior work applied ISV modelling in the case of matched acquisition conditions, i. e., where biometric test samples are acquired using the same device as employed for client model enrolment. Furthermore, they did not use the most recent advances such as total variability (TV) modelling, which has been applied to speaker [9] and face [10] authentication.

In this paper we explore three issues of applying bi-modal authentication to the challenging mobile phone environment. First, we examine the issue of mismatched conditions between enrolment and testing. In particular, we examine the effect of enrolling users on high quality biometric samples acquired with a laptop computer and then authenticating them using lower quality biometric samples acquired with a mobile phone. As a significant contribution, we develop three new protocols for the MOBIO database [2] with respect to prior work [1,2,4,8] that was exclusively using mobile phone data both for enrolment and testing.

Second, we extend the work of [8] by examining the effectiveness of TV modelling for bi-modal authentication. Third, we show the effectiveness of multi-algorithm fusion to further improve the results for face, speaker and bi-modal authentication in the mobile phone environment. The final outcome of this work is the development of a state-of-the-art bi-modal (face and speaker) authentication system that improves upon the previous state-of-the-art [8] with a relative performance gain of 35% for Female and 27% for Male trials on the MOBIO database.

The remainder of this paper is structured as follows: In Section 2 we outline the employed face and speaker authentication systems, while Section 3 combines these into bi-modal and multi-algorithm authentication systems. Section 4 presents the new protocols for the MOBIO database that are used in our experiments, which we discuss in Section 5. Finally, Section 6 concludes the paper.

We examine the effectiveness of state-of-the-art Gaussian mixture model (GMM) based approaches for face, speaker and bi-modal authentication. GMMs have formed the basis of state-of-the-art speaker authentication systems for over a decade [11,9] and it was recently shown that incorporating session variability modelling into a GMM system produces state-of-the-art results for face authentication [12]. Also, the combination of GMM-based systems that use session variability modelling produces a state-of-the-art bi-modal (face and speaker) authentication system [8].

When using GMMs and session variability for speaker and face authentication, the same underlying approach is taken. The main difference is how the feature vectors are extracted from the image (face) and audio (speech) samples. Below we describe the feature extraction process for both face and speaker authentication followed by a description of the GMM and the associated session variability modelling approaches that we examine.

Two separate feature extraction processes are used for image (face) and audio (speech) data. For both modalities, a biometric sample 
                           O
                         (image or audio) is decomposed into a set O of K feature vectors (O
                        ={o
                        1,o
                        2,…,oK
                        }), where each feature vector is of dimensionality M. This decomposition is performed in the spatial domain for the image data, and in the time domain for the audio data.

For the image data, we rely on parts-based features that were proposed for the task of face authentication in [13]. These features have since been successfully employed by several researchers [14,15]. The key idea is to decompose the face image into a set of overlapping blocks before extracting a feature vector from each of them. The feature vectors extracted from these blocks are then considered as observations of the same signal (the same face), and can be modelled in a generative way.

The feature extraction process is similar to the approach described in [16]. First, each image is rotated, scaled and cropped to 64×80 pixels such that the eyes are 16 pixels from the top and separated by 33 pixels. Second, to reduce the impact of illumination, each cropped image is preprocessed with the multi-stage algorithm of Tan & Triggs [17], using their default parameterisation. Third, 12×12 blocks of pixel values are extracted from the preprocessed image using an exhaustive overlap, leading to K
                           =3657 blocks per image. Fourth, pixel values of each block are normalised to zero mean and unit variance, prior to extracting the M
                           +1 lowest frequency 2D discrete cosine transform (2D-DCT) coefficients [13] and removing the zero frequency coefficient as it is redundant. Fifth, the resulting 2D-DCT feature vectors are normalised to zero mean and unit variance in each dimension with respect to the other feature vectors of the image. As in previous work [16,8], M was set equal to 44.

For the audio data, observations are extracted at equally-spaced time instants using a sliding window approach. First, audio segments are denoised using the Qualcomm-ICSI-OGI front end [18]. Second, voice activity detection (VAD) is performed jointly using the normalised log energy and the 4Hz modulation energy [19]. The aim of the 4Hz modulation energy is to discriminate speech from other audio sources such as noise and music. An adaptive threshold is applied on both the 4Hz modulation energy and the normalised log energy. In our experiments, this approach provided a relative improvement of up to 16% compared to the common energy-based VAD. Third, 19 mel frequency cepstral coefficient (MFCC) and log energy features together with their first- and second-order derivatives are obtained by computing 24 filter bank coefficients over 20ms Hamming windowed frames every 10ms. This results in acoustic feature vectors of dimensionality M
                           =60. Finally, feature normalisation based on cepstral mean and variance normalisation (CMVN) is applied on the remaining speech. The number of feature vectors K extracted from each audio sample depends on the duration of the sample and the number of segments that the VAD classifies to be speech.

We use the same generative probabilistic framework that models the observed feature vectors using Gaussian mixture models (GMMs) for both image (face) and audio (speech) modalities. GMMs have been successfully applied first to speaker authentication [20,11] and then to face authentication [13,21,14–16]. One of the main challenges with GMMs is to reliably estimate a client model with limited enrolment data. This enrolment process is sensitive to the conditions, in which the data was captured. To address this issue, several session variability modelling techniques built on the GMM baseline have been proposed that constrain client models to be in a restricted subspace. In this work, we consider two approaches to session variability modelling, inter-session variability (ISV) modelling [22] and total variability (TV) modelling [9]. Both methods were initially proposed for speaker authentication [9,22] before being applied to face authentication [10,12]. In the remainder of this section, we first describe the GMM baseline system, followed by the more advanced ISV and TV techniques.

The distribution of the observed feature vectors (face or speech) is modelled using a GMM. A GMM is the weighted sum of C multi-variate Gaussian components 
                              N
                           :
                              
                                 (1)
                                 
                                    
                                       p
                                       
                                          
                                             o
                                             
                                                
                                                   Θ
                                                   gmm
                                                
                                             
                                          
                                       
                                       =
                                       
                                          
                                             ∑
                                             
                                                c
                                                =
                                                1
                                             
                                             C
                                          
                                          
                                             
                                                ω
                                                c
                                             
                                             N
                                             
                                                
                                                   o
                                                   ;
                                                   
                                                      μ
                                                      
                                                         c
                                                         ,
                                                      
                                                   
                                                   
                                                      ∑
                                                      c
                                                   
                                                
                                             
                                             ,
                                          
                                       
                                    
                                 
                              
                           where Θ
                           gmm
                           ={ω
                           
                              c
                           ,μ
                           
                              c
                           ,∑
                              c
                           }
                              c
                              ={1,…,C} are the parameters of this distribution: the weights, the means and the covariance matrices, respectively.

To use GMMs for authentication we need to learn a GMM 
                              
                                 S
                                 i
                              
                            for each subject i from a set of enrolment samples. One of the main challenges is that the number of enrolment images or audio recordings per client is usually limited, possibly to a single sample. In practise, it has been shown that for both speaker [11] and face authentication [14,15] an efficient enrolment method is to use a subject-independent prior GMM 
                              M
                           , called the universal background model (UBM), and to adapt this prior to the enrolment samples of the subject i to generate the client model 
                              
                                 S
                                 i
                              
                           . The UBM 
                              M
                            is learnt beforehand by maximising the likelihood of observations extracted from a large independent training set of several identities using the iterative expectation–maximisation (EM) algorithm [23]. Afterwards, adaptation is achieved by using maximum a posteriori (MAP) estimation [20], where only the means of the UBM are updated, as this has been shown to be efficient for both modalities [14,15,20]. As in previous work [11,14–16] GMMs are assumed to have diagonal covariance matrices.

A convenient and compact representation of mean-only MAP adaptation and other session variability modelling techniques is the GMM super-vector notation. This notation consists of grouping the parameters of the various Gaussian components of a GMM (weights, means or covariance matrices) into single large vectors or matrices. For instance, the mean super-vector m of the UBM 
                              M
                            is obtained by concatenating the means μc
                            of all its components: m
                           =[μ
                           1
                           
                              T
                           ,μ
                           
                              c
                           
                           
                              T
                           ,…,μ
                           
                              c
                           
                           
                              T
                           ]
                              T
                           . In [22] it was shown that mean-only MAP adaptation can then be written as:
                              
                                 (2)
                                 
                                    
                                       si
                                       =
                                       m
                                       +
                                       di
                                       ,
                                    
                                 
                              
                           where si
                            is the mean super-vector of the GMM 
                              
                                 S
                                 i
                              
                            and di
                            is a client-specific offset for subject i. This offset di
                            is given by:
                              
                                 (3)
                                 
                                    
                                       
                                          d
                                          i
                                       
                                       =
                                       D
                                       
                                          z
                                          i
                                       
                                       
                                       with
                                       
                                       D
                                       =
                                       
                                          
                                             
                                                Σ
                                                T
                                             
                                             ,
                                          
                                       
                                    
                                 
                              
                           where Σ is the variance super-vector of the UBM (recalling that covariance matrices are assumed to be diagonal), 
                              T
                            is a relevance factor that provides a weight for the prior UBM when performing MAP adaptation [20] and zi
                            is a latent variable, which is assumed to be normally distributed 
                              
                                 N
                                 
                                    0
                                    I
                                 
                              
                           . In the following, since only the means of the UBM are adapted, we will use si
                            and m to describe the client model 
                              
                                 S
                                 i
                              
                            and the UBM 
                              M
                           , respectively, by abusing the notation.

Once a client model is enrolled, a test sample 
                                 
                                    O
                                    t
                                 
                               (also called a probe sample) is authenticated against the model si
                               by calculating a log-likelihood ratio (LLR) score with respect to the UBM m 
                              [11]:
                                 
                                    (4)
                                    
                                       
                                          
                                             h
                                             gmm
                                          
                                          
                                             
                                                O
                                                t
                                             
                                             
                                                s
                                                i
                                             
                                          
                                          =
                                          
                                             
                                                ∑
                                                
                                                   k
                                                   =
                                                   1
                                                
                                                
                                                   K
                                                   t
                                                
                                             
                                             
                                                
                                                   
                                                      log
                                                      
                                                         
                                                            p
                                                            
                                                               
                                                                  
                                                                     o
                                                                     t
                                                                     k
                                                                  
                                                                  
                                                                     
                                                                        s
                                                                        i
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                      −
                                                      log
                                                      
                                                         
                                                            p
                                                            
                                                               
                                                                  
                                                                     o
                                                                     t
                                                                     k
                                                                  
                                                                  
                                                                     m
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                                .
                                             
                                          
                                       
                                    
                                 
                              With higher 
                                 
                                    
                                       h
                                       gmm
                                    
                                    
                                       
                                          O
                                          t
                                       
                                       
                                          s
                                          i
                                       
                                    
                                 
                               values, the probability increases that the observations Ot
                               extracted from the sample 
                                 
                                    O
                                    t
                                 
                               were produced by the client model si
                              .

Recently, a linear approximation of Eq. (4), known as linear scoring 
                              [24], has been adopted in the speaker authentication literature and has also been applied to face authentication [16]. It relies on the mean centralised first order sufficient statistics of the UBM given the observations Ot
                              , as given in Eq. (9) of [12]. This approximation was shown [24] to be orders of magnitude more efficient with no significant degradation in performance.

As a final step, we also perform zt-score normalisation 
                              [25] due to the consistent performance improvements that this gives for both face [16] and speaker authentication [26].

One problem of the GMM mean-only MAP adaptation approach is that the client model si
                            can be difficult to estimate reliably with limited enrolment data as it is sensitive to the conditions in which the data was captured. Part of the reason for this is that there is no explicit model to capture and suppress detrimental variations such as audio or image noise. Session variability modelling aims to estimate and suppress the effects of within-client variations in order to create more discriminant client models. For the face modality, within-client variations include variations of pose, illumination or expression of samples of a given subject, whereas for the speaker modality variations are, amongst others, caused by the sensor (microphone) or the environment (background noise or acoustic conditions).


                           Inter-session variability (ISV) modelling [22] and joint factor analysis (JFA) [27] are two session variability modelling techniques, used in the context of a GMM-based system, that have been successfully applied to both speaker [22,26,28] and face authentication [4,12]. Both techniques aim to estimate session variation like audio or image noise in order to compensate for it. This compensation for the estimated session variation is the key difference between ISV and the classic mean-only MAP adaptation. Note that for these experiments we have not examined JFA as it was shown empirically that ISV outperforms JFA for both speaker [8] and face [8,12] authentication.

As in [22] it is assumed that session variability results in an additive offset to the mean super-vector si
                            of the client model. This offset can be added directly to the normal mean-only MAP adaptation representation. Given the j-th biometric sample 
                              
                                 O
                                 
                                    i
                                    ,
                                    j
                                 
                              
                            of subject i the mean super-vector μ
                           
                              i,j
                            of the GMM that best represents this biometric sample is:
                              
                                 (5)
                                 
                                    
                                       
                                          μ
                                          
                                             i
                                             ,
                                             j
                                          
                                       
                                       =
                                       m
                                       +
                                       U
                                       
                                          x
                                          
                                             i
                                             ,
                                             j
                                          
                                       
                                       +
                                       D
                                       
                                          z
                                          i
                                       
                                    
                                 
                              
                           where U is a subspace that constrains the possible session effects, x
                           
                              i,j
                            is its associated latent session variable (
                              
                                 
                                    x
                                    
                                       i
                                       ,
                                       j
                                    
                                 
                                 ∼
                                 N
                                 
                                    0
                                    I
                                 
                              
                           ), while D and zi
                            represent the client-specific offset in the same manner as for mean-only MAP adaptation, which is given in Eqs. (2) and (3).

The model enrolment of a client is performed in the following manner. Given a session subspace U, which is learnt by maximising the likelihood of the training data, the latent variables x
                           
                              i,j
                            and zi
                            are jointly estimated using MAP. Afterwards, the session varying part is suppressed by retaining only the client-specific information:
                              
                                 (6)
                                 
                                    
                                       
                                          s
                                          i
                                       
                                       
                                          
                                          
                                             isv
                                          
                                       
                                       =
                                       m
                                       +
                                       D
                                       
                                          z
                                          i
                                       
                                       
                                          
                                          
                                             isv
                                          
                                       
                                       .
                                    
                                 
                              
                           For details on how to jointly estimate the latent variables and how to train the subspace U, readers are referred to [12].

ISV relies on a LLR score similar to Eq. (4). The main differences are that the session offsets of the enrolment samples have been compensated while generating the client model, and that session offsets of the probe sample 
                                 
                                    O
                                    t
                                 
                               are estimated prior to scoring. This means that the latent session variables x
                              
                                 i,j
                               and x
                              ubm,j
                               of the observed feature vectors 
                                 
                                    
                                       O
                                       t
                                    
                                    =
                                    
                                       
                                          o
                                          t
                                          1
                                       
                                       
                                          o
                                          t
                                          2
                                       
                                       …
                                       
                                          o
                                          t
                                          
                                             K
                                             t
                                          
                                       
                                    
                                 
                               extracted from a biometric sample 
                                 
                                    O
                                    t
                                 
                               are first estimated, before computing a LLR score:
                                 
                                    (7)
                                    
                                       
                                          
                                             h
                                             isv
                                          
                                          
                                             
                                                O
                                                t
                                             
                                             
                                                s
                                                i
                                             
                                          
                                          =
                                          
                                             
                                                ∑
                                                
                                                   k
                                                   =
                                                   1
                                                
                                                
                                                   K
                                                   t
                                                
                                             
                                             
                                                
                                                   
                                                      log
                                                      
                                                         
                                                            p
                                                            
                                                               
                                                                  
                                                                     o
                                                                     t
                                                                     k
                                                                  
                                                                  
                                                                     
                                                                        
                                                                           s
                                                                           i
                                                                        
                                                                        +
                                                                        U
                                                                        
                                                                           x
                                                                           
                                                                              i
                                                                              ,
                                                                              t
                                                                           
                                                                        
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                      −
                                                      log
                                                      
                                                         
                                                            p
                                                            
                                                               
                                                                  
                                                                     o
                                                                     t
                                                                     k
                                                                  
                                                                  
                                                                     
                                                                        m
                                                                        +
                                                                        U
                                                                        
                                                                           x
                                                                           
                                                                              ubm
                                                                              ,
                                                                              t
                                                                           
                                                                        
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          .
                                       
                                    
                                 
                              
                           

In practise, simplifications have been proposed to speed up the process in [24], which consists of first approximating the session offset Ux
                              
                                 i,t
                               of the client model, with the session offset Ux
                              ubm,t
                              , and then using the linear scoring approximation as for the GMM-baseline.

Finally, as with the GMM baseline, we perform zt-score normalisation.

In [29] it was shown that JFA can fail to separate between-client and within-client variations into two different subspaces. This is potentially caused by the high dimensionality of the GMM mean super-vector space.

To address this issue, an alternative technique called total variability (TV) modelling was developed for speaker authentication [30,31] and later applied to face authentication [10]. This framework is built on the GMM approach and relies on the definition of a single subspace that contains both identity and session variabilities. In particular, it aims to extract low-dimensional factors w
                           
                              i,j
                           , so-called i-vectors, from biometric samples 
                              
                                 O
                                 
                                    i
                                    ,
                                    j
                                 
                              
                           . More formally, the TV approach can be described in the GMM mean super-vector space by:
                              
                                 (8)
                                 
                                    
                                       
                                          μ
                                          
                                             i
                                             ,
                                             j
                                          
                                       
                                       =
                                       m
                                       +
                                       T
                                       
                                          w
                                          
                                             i
                                             ,
                                             j
                                          
                                       
                                    
                                 
                              
                           where T is the low-dimensional total variability subspace and w
                           
                              i,j
                            the low-dimensional i-vector, which is assumed to follow a normal distribution 
                              
                                 N
                                 
                                    0
                                    I
                                 
                              
                           .

The TV subspace T is learnt by maximising the likelihood over a large training set. This algorithm is similar to the one used to estimate the identity (between-class) subspace in JFA [32], with one major difference: while JFA jointly consider the samples coming from a given subject, TV treats them as if they have been produced by different identities, which is an advantage when large unlabelled training datasets are used. In addition, the extraction of i-vectors requires the estimation of a covariance matrix Σ
                              T
                            to model the residual variability that is not captured by the subspace T.

In contrast to ISV, TV does not explicitly perform session compensation. TV is just a front-end that extracts a low dimensional i-vector w
                           
                              i,j
                            from each sample 
                              
                                 O
                                 
                                    i
                                    ,
                                    j
                                 
                              
                            based on the total variability of the training set. As such, it is likely to capture both client-specific and session-specific information. Hence, TV requires to use separate session compensation and scoring techniques after the extraction of i-vectors. Additionally, a set of preprocessing algorithms have been proposed to map i-vectors into a more adequate space [33,34,10]. Possible variants of preprocessing, session compensation and scoring methods have been employed and combined in different manners [30,31,10]. Some of them are summarised in Fig. 1
                            and described in the remainder of this section.

First, i-vector whitening was proposed in [33,10] and shown to boost classification performance. Whitening consists of normalising the i-vector space such that the covariance matrix of the i-vectors, of a training set, is turned into the identity matrix. This is performed by applying:
                              
                                 (9)
                                 
                                    
                                       
                                          w
                                          
                                             
                                             
                                                i
                                                ,
                                                j
                                             
                                          
                                          
                                             whitened
                                          
                                       
                                       =
                                       
                                          W
                                          T
                                       
                                       
                                          
                                             
                                                w
                                                
                                                   i
                                                   ,
                                                   j
                                                
                                             
                                             −
                                             
                                                w
                                                ¯
                                             
                                          
                                       
                                       ,
                                    
                                 
                              
                           where 
                              
                                 w
                                 ¯
                              
                            is the mean of a training set of i-vectors, w
                           
                              i,j
                           
                           (whitened) the whitened i-vector, and W the whitening transform. This transform W is computed as the Cholesky decomposition of 
                              
                                 
                                    
                                       
                                          Σ
                                          ¯
                                       
                                       
                                          −
                                          1
                                       
                                    
                                 
                                 =
                                 W
                                 
                                    W
                                    T
                                 
                              
                           , where 
                              
                                 Σ
                                 ¯
                              
                            is the covariance matrix of a training set of i-vectors.

Another efficient preprocessing technique is i-vector length normalisation [34,10], which aims at reducing the impact of a mismatch between training and test i-vectors. It consists of mapping the i-vectors into a unit hypersphere:
                              
                                 (10)
                                 
                                    
                                       
                                          w
                                          
                                             i
                                             ,
                                             j
                                          
                                          
                                             1
                                             −
                                             norm
                                          
                                       
                                       =
                                       
                                          
                                             w
                                             
                                                i
                                                ,
                                                j
                                             
                                          
                                          
                                             
                                                w
                                                
                                                   i
                                                   ,
                                                   j
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           which is very effective when using session compensation or scoring methods that assume Gaussian-like distributions.

A set of session compensation techniques have been proposed for both speaker [31] and face [10] authentication. Linear discriminant analysis (LDA) [35] is a popular algorithm that aims at learning a linear projection maximising between-class variations while minimising within-class variations. The projection matrix A is learnt from a training set of i-vectors extracted from samples coming from several identities, by first computing the between-class and within-class scatter matrices:
                              
                                 (11)
                                 
                                    
                                       
                                          S
                                          W
                                       
                                       =
                                       
                                          
                                             ∑
                                             i
                                          
                                          
                                             
                                                
                                                   ∑
                                                   
                                                      j
                                                      =
                                                      1
                                                   
                                                   
                                                      J
                                                      i
                                                   
                                                
                                                
                                             
                                             
                                                
                                                   
                                                      w
                                                      
                                                         i
                                                         ,
                                                         j
                                                      
                                                   
                                                   −
                                                   
                                                      
                                                         w
                                                         ¯
                                                      
                                                      i
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            w
                                                            
                                                               i
                                                               ,
                                                               j
                                                            
                                                         
                                                         −
                                                         
                                                            
                                                               w
                                                               ¯
                                                            
                                                            i
                                                         
                                                      
                                                   
                                                   T
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                           
                              
                                 (12)
                                 
                                    
                                       
                                          S
                                          B
                                       
                                       =
                                       
                                          
                                             ∑
                                             i
                                          
                                          
                                             
                                                J
                                                i
                                             
                                             
                                                
                                                   
                                                      
                                                         w
                                                         ¯
                                                      
                                                      i
                                                   
                                                   −
                                                   
                                                      w
                                                      ¯
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               w
                                                               ¯
                                                            
                                                            i
                                                         
                                                         −
                                                         
                                                            w
                                                            ¯
                                                         
                                                      
                                                   
                                                   T
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where Ji
                            is the number of i-vectors from client i, 
                              
                                 
                                    w
                                    ¯
                                 
                                 i
                              
                            is the mean of this client-specific i-vectors, and 
                              
                                 w
                                 ¯
                              
                            the means of all i-vectors in the training set. Next, LDA maximises the ratio of the determinants of these two scatter matrices. The solution is found by solving the generalised eigenvalue decomposition S
                           
                              B
                           
                           v
                           =
                           λS
                           
                              W
                           
                           v. We then retain the n
                           1da eigenvectors with the greatest eigenvalues to build the projection matrix A. An i-vector w
                           
                              i,j
                            is projected into the LDA space by:
                              
                                 (13)
                                 
                                    
                                       
                                          w
                                          
                                             i
                                             ,
                                             j
                                          
                                          
                                             
                                                1
                                                da
                                             
                                          
                                       
                                       =
                                       
                                          A
                                          T
                                       
                                       
                                          w
                                          
                                             i
                                             ,
                                             j
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        


                           Within-class covariance normalisation (WCCN) is a technique initially introduced for SVM-based speaker authentication [36]. It has since been successfully applied to i-vectors for both speaker [30] and face authentication [10]. It aims to normalise the within-class covariance matrix of a training set of i-vectors. Given the within-class scatter matrix from Eq. (11) and the number of identities N in the training set, the WCCN linear transform B can be computed using the Cholesky decomposition of:
                              
                                 (14)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      1
                                                      N
                                                   
                                                   
                                                      S
                                                      W
                                                   
                                                
                                             
                                             
                                                −
                                                1
                                             
                                          
                                       
                                       =
                                       B
                                       
                                          B
                                          T
                                       
                                       .
                                    
                                 
                              
                           
                        

An i-vector w
                           
                              i,j
                            is projected into the corresponding WCCN space by:
                              
                                 (15)
                                 
                                    
                                       
                                          w
                                          
                                             i
                                             ,
                                             j
                                          
                                          
                                             wccn
                                          
                                       
                                       =
                                       
                                          B
                                          T
                                       
                                       
                                          w
                                          
                                             i
                                             ,
                                             j
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        

Once session compensation has been performed, any scoring technique might be employed for authentication purposes. Cosine similarity scoring 
                              [9,10] is a simple and efficient method used to estimate how close a (normalised) i-vector wt
                               extracted from a probe sample 
                                 
                                    O
                                    t
                                 
                               is to the i-vector wi
                               representing a client i:
                                 
                                    (16)
                                    
                                       
                                          
                                             h
                                             cosine
                                          
                                          
                                             
                                                w
                                                t
                                             
                                             
                                                w
                                                i
                                             
                                          
                                          =
                                          
                                             
                                                
                                                   w
                                                   t
                                                
                                                ×
                                                
                                                   w
                                                   i
                                                
                                             
                                             
                                                ∥
                                                
                                                   w
                                                   t
                                                
                                                ∥
                                                ∥
                                                
                                                   w
                                                   i
                                                
                                                ∥
                                             
                                          
                                          .
                                       
                                    
                                 
                              
                           

Another technique commonly applied in the i-vector space is probabilistic linear discriminant analysis (PLDA) [37–39]. PLDA is a probabilistic framework that incorporates both between-class and within-class information and, therefore, performs session compensation. In addition, considering the authentication problem, this probabilistic approach allows the generation of LLR scores.

More formally, PLDA assumes that the j-th i-vector of client i is generated by:
                                 
                                    (17)
                                    
                                       
                                          
                                             w
                                             
                                                i
                                                ,
                                                j
                                             
                                          
                                          =
                                          F
                                          
                                             h
                                             i
                                          
                                          +
                                          G
                                          
                                             k
                                             
                                                i
                                                ,
                                                j
                                             
                                          
                                          +
                                          
                                             ∈
                                             
                                                i
                                                ,
                                                j
                                             
                                          
                                          ,
                                       
                                    
                                 
                              where F and G are the subspaces describing the between-class and within-class variations, respectively, hi
                               and k
                              
                                 i,j
                               are the associated latent variables, which are assumed to be normally distributed 
                                 
                                    N
                                    
                                       0
                                       I
                                    
                                 
                              , and ∈
                              
                                 i,j
                               represents the residual noise, which is supposed to follow a Gaussian distribution 
                                 
                                    N
                                    
                                       0
                                       
                                          Σ
                                          ϵ
                                       
                                    
                                 
                              .

The parameters Θ
                              plda
                              ={F,G,Σ
                              ϵ} of this model are learnt using an EM algorithm over a training set of i-vectors. Once the model has been trained, given an i-vector wt
                               extracted from a probe sample 
                                 
                                    O
                                    t
                                 
                               and an i-vector wi
                               representing a client i, authentication can be achieved by computing the LLR score:
                                 
                                    (18)
                                    
                                       
                                          
                                             h
                                             plda
                                          
                                          
                                             
                                                w
                                                t
                                             
                                             
                                                w
                                                i
                                             
                                          
                                          =
                                          
                                             
                                                p
                                                
                                                   
                                                      
                                                         w
                                                         t
                                                      
                                                      ,
                                                      
                                                         w
                                                         i
                                                      
                                                      |
                                                      Θ
                                                   
                                                
                                             
                                             
                                                p
                                                
                                                   
                                                      
                                                         w
                                                         t
                                                      
                                                      
                                                         Θ
                                                      
                                                   
                                                
                                                p
                                                
                                                   
                                                      
                                                         w
                                                         i
                                                      
                                                      
                                                         Θ
                                                      
                                                   
                                                
                                             
                                          
                                          .
                                       
                                    
                                 
                              Here, p(w
                              
                                 t
                              ,
                              w
                              
                                 i
                              |Θ) is the log-likelihood that the i-vectors wt
                               and wi
                               share the same latent identity variable hi
                               and, hence, are coming from the same client, whereas p(w
                              
                                 i
                              |Θ)p(w
                              
                                 i
                              |Θ) is the log-likelihood that the i-vectors wt
                               and wi
                               have different latent identity variables ht
                               and hi
                               and, therefore, are from different clients. For details on how to estimate these likelihoods and how to train the parameters Θ
                              plda, readers are referred to [37–39].

Finally, recent work [40] on speaker recognition at NIST SRE 2012
                                 1
                              
                              
                                 1
                                 
                                    http://www.nist.gov/itl/iad/mig/sre12.cfm.
                                 
                               has shown that the duration mismatch between enrolment and test speech segments can tremendously affect the accuracy of the system. To cope with this variability, [40] proposed to truncate the speech signal into shorter segments. Then the i-vectors of the truncated versions together with the i-vectors of the original signals are used to train the PLDA. We also evaluate this technique on the MOBIO database (see Section 5.6).

Several fusion strategies are known in the literature [41]. They can be classified into three main categories:
                        
                           Low-level fusion which is also known as data fusion, combines multiple sources of raw data to produce new raw data. The major problem of this fusion method comes with non-balanced dimensionalities of data from the multiple sources.

Intermediate-level fusion or feature level fusion combines various features that might come from several raw data sources or even from the same raw data. The drawback of feature-level fusion is that synchronisation between modalities [42] is required, which is not provided in the MOBIO database.

High-level fusion which is also called decision level fusion, late fusion or score fusion, combines decisions from several systems. This fusion strategy is very flexible and can be used for multi-modal (face and speaker) or multi-algorithm (for instance combining GMM and ISV) fusion. High-level fusion methods include majority voting methods, fuzzy logic based methods [43], and statistical methods.

In this work we choose the high-level fusion approach due to its ease of use for both multi-modal [8] and multi-algorithm [44–46] fusion.

We take the well-known statistical linear logistic regression approach, which has been successfully employed for combining heterogeneous speaker and face authentication classifiers [44–46] and for bi-modal (face and speaker) authentication [8].

Linear logistic regression combines a set of Q classifiers using the sum rule. Let the probe 
                           
                              O
                              t
                           
                         be processed by Q classifiers, each of which produces an output score 
                           
                              
                                 h
                                 q
                              
                              
                                 
                                    O
                                    t
                                 
                                 
                                    s
                                    i
                                 
                              
                           
                        . These scores are fused using a linear combination:
                           
                              (19)
                              
                                 
                                    
                                       h
                                       fusion
                                    
                                    
                                       
                                          
                                             O
                                             t
                                          
                                          
                                             s
                                             i
                                          
                                          ,
                                          β
                                       
                                    
                                    =
                                    
                                       β
                                       0
                                    
                                    +
                                    
                                       
                                          ∑
                                          
                                             q
                                             =
                                             1
                                          
                                          Q
                                       
                                       
                                          
                                             β
                                             q
                                          
                                          
                                             h
                                             q
                                          
                                          
                                             
                                                O
                                                t
                                             
                                             
                                                s
                                                i
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        where β
                        =[β
                        0,β
                        1,…,β
                        
                           Q
                        ] are the fusion weights (also known as regression coefficients).

The coefficients β are computed by estimating the maximum likelihood of the logistic regression model on the scores of the development set. Let 
                           
                              X
                              true
                           
                         be the set of true client access trials, i. e., the set of pairs 
                           
                              x
                              =
                              
                                 
                                    O
                                    t
                                 
                                 
                                    s
                                    i
                                 
                              
                           
                        , where the identity of the test sample 
                           
                              O
                              t
                           
                         and of the client si
                         is the same. Let furthermore 
                           
                              X
                              imp
                           
                         be the set of impostor trials, i. e., the set of pairs 
                           
                              x
                              =
                              
                                 
                                    O
                                    t
                                 
                                 
                                    s
                                    i
                                 
                              
                           
                        , where the identities of the test sample 
                           
                              O
                              t
                           
                         and of the client si
                         are different. Let 
                           
                              X
                              =
                              
                                 X
                                 true
                              
                              ∪
                              
                                 X
                                 imp
                              
                           
                        . The objective function to maximise is:
                           
                              (20)
                              
                                 
                                    L
                                    
                                       β
                                    
                                    =
                                    −
                                    
                                       
                                          ∑
                                          
                                             x
                                             ∈
                                             X
                                          
                                       
                                       
                                          log
                                          
                                             
                                                1
                                                +
                                                exp
                                                
                                                   
                                                      −
                                                      
                                                         y
                                                         x
                                                      
                                                      
                                                         h
                                                         fusion
                                                      
                                                      
                                                         x
                                                         β
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        where:
                           
                              (21)
                              
                                 
                                    
                                       y
                                       x
                                    
                                    =
                                    
                                       
                                          
                                             
                                                +
                                                1
                                                ,
                                             
                                             
                                                if
                                                
                                                x
                                                ∈
                                                
                                                   X
                                                   true
                                                
                                             
                                          
                                          
                                             
                                                −
                                                1
                                                ,
                                             
                                             
                                                if
                                                
                                                x
                                                ∈
                                                
                                                   X
                                                   imp
                                                
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        The maximum likelihood estimation procedure converges to a global minimum. In our work, this optimisation is done using the conjugate-gradient algorithm [47].

This approach performs best when the scores of the classifiers are statistically independent of each other. For this reason we measure the independence and, therewith, the complementary nature of our classifiers. We use the scatter plots (see Fig. 8) and the relative common error (RCE):
                           
                              (22)
                              
                                 
                                    RCE
                                    =
                                    CE
                                    ×
                                    max
                                    
                                       
                                          
                                             1
                                             
                                                T
                                                
                                                   E
                                                   1
                                                
                                             
                                          
                                          ,
                                          
                                             1
                                             
                                                T
                                                
                                                   E
                                                   2
                                                
                                             
                                          
                                          ,
                                          …
                                          
                                             1
                                             
                                                T
                                                
                                                   E
                                                   Q
                                                
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        where CE is the number of common errors between the Q classifiers and TE
                           q
                         is the total number of errors of the qth
                         subsystem. The lower RCE is, the more independent the classifiers are.

In this work we evaluate the effectiveness of both bi-modal and multi-algorithm fusion. This leads to a number of different system combinations, which we outline in Fig. 2
                        . The top row of Fig. 2 displays the three different bi-modal fusion systems, while the bottom row shows the two different multi-algorithm fusion systems and the bi-modal multi-algorithm fusion approach that we examine.

The MOBIO database [2] is a unique bi-modal, face and speaker, database as it was captured almost exclusively using mobile phones. It consists of over 61h of audio-visual data of 150 people captured within twelve distinct sessions that are usually separated by several weeks. The users answered a set of questions, which varied in type, including:
                        
                           1
                           short response questions (p) such as “what is your address”,

free speech questions, where the user speaks about any subject for approximately 10s (f) or about 5s (r), and

pre-defined text (l) that the user read out.

All of this data was captured on a mobile phone, except for the first session, where data was obtained using both a mobile phone and a laptop computer. One of the unique attributes of this database is that the acquisition device was held by the user, rather than being in a fixed position. As such, the microphone and camera are not fixed and used in an interactive and uncontrolled manner. This presents several challenges such as high variability of pose and illumination conditions, high variations in the quality of speech, and variability in terms of acoustics as well as illumination and background. Exemplary images of one identity are given in Fig. 3
                     .

This challenging mobile phone database has been used to evaluate several face and speaker authentication systems [1] as well as bi-modal authentication systems [7,8]. The database provides a well defined protocol, which was initially described for the full database in [4]. This protocol separates the clients of the database into three non-overlapping partitions for training, development (DEV) and evaluation (EVAL). The performance is measured in a gender-dependent manner (Female and Male, respectively). An overview of this initial protocol, which we refer to as mobile-0, is provided in Table 1
                     . A limitation of this previously defined protocol is that only the lower quality biometric data acquired from the mobile phone was used, while the higher quality laptop data were ignored.

In this work we extend the MOBIO protocol [2] and define three novel protocols that explore mismatched conditions by making use of the laptop data.
                           2
                        
                        
                           2
                           The MOBIO database (videos, still images, eye locations and the four evaluation protocols) are available for free at http://www.idiap.ch/dataset/mobio.
                           
                         The mismatched conditions that we wish to investigate are the specific cases of enrolling a user with high quality biometric samples (for instance acquired from a laptop computer) and then compared, or tested, using lower quality biometric samples obtained using a mobile phone.
                           
                              mobile-1 is identical to a mobile-0, except that it includes the laptop data in the training set. This ensures that the same training data is being used for mobile and laptop evaluation (the next protocol that we present). It provides an additional 1050 training samples compared to mobile-0. Enrolment and testing is conducted using only mobile phone data. Please see Table 2
                                  for more details.

laptop-1 contains the same training data as mobile-1, but enrolment is performed exclusively using laptop data, while testing is conducted exclusively with mobile phone data. See Table 3
                                  for details on the kind of data used in this protocol.

laptop-mobile-1 also consists of the same training data as mobile-1. Here, enrolment is performed using both mobile and laptop data, while testing is still conducted exclusively on mobile phone data, see Table 4
                                  for details.

To measure the accuracy of the presented authentication systems, we use two different evaluation criteria that were previously defined in [2]. These measures are the half total error rate (HTER) and detection error trade-off (DET) plots.

The HTER is used to represent the performance of an authentication system on the unbiased evaluation partition as a single number. To compute the HTER, a threshold θ is defined on the development partition at the intersection point of the false acceptance rate (FAR) and the false rejection rate (FRR). The corresponding FAR (or FRR) value of the development partition at this threshold θ is known as the equal error rate (EER). The threshold is applied to the evaluation partition (D
                        EVAL) to obtain the HTER:
                           
                              (23)
                              
                                 
                                    HTER
                                    =
                                    
                                       
                                          FAR
                                          
                                             θ
                                             
                                                D
                                                EVAL
                                             
                                          
                                          +
                                          FRR
                                          
                                             θ
                                             
                                                D
                                                EVAL
                                             
                                          
                                       
                                       2
                                    
                                    ,
                                 
                              
                           
                        which is the average of the FAR and the FRR at θ. Finally, we provide a complete overview of the system performances using a DET plot [48], which outlines the miss probability (FRR) versus the probability of false acceptance (FAR) on the evaluation set.

@&#EXPERIMENTAL RESULTS@&#

In this section, we evaluate the accuracy of the uni-modal and bi-modal authentication systems described in Sections 2 and 3 across the four protocols defined in Section 4. Global observations are first highlighted in Section 1. A detailed comparison between GMM, ISV and TV systems is presented in Section 5.2. Bi-modal and multi-algorithm experiments are examined in Sections 5.3 and 5.4, respectively, and the results are compared with other state-of-the-art face and speaker authentication fusion systems. The results obtained on the four protocols are presented and summarised in Section 5.5. Section 5.6 presents the results of gender-dependent systems and the use of extended training set. We present both the EER on the DEV set and the HTER on the EVAL set. Results for the best systems for each modality are highlighted in bold. We also distinguish the best uni-modal single algorithm systems by highlighting them in bold italics.

To make the comparison simple and the results reproducible we used the same parameters throughout the experiments and conducted all experiments with the open-source Bob toolbox
                        3
                     
                     
                        3
                        Bob is a free signal processing and machine learning toolbox originally developed at Idiap Research Institute. The total variability system was incorporated especially for this paper. You can download Bob from: http://www.idiap.ch/software/bob.
                        
                      
                     [49] to implement all of our systems. GMMs are composed of 512 Gaussian components and the UBMs are trained in the following manner: 25 iterations of k-means clustering are performed to initialise the UBM and then 100 iterations of maximum likelihood estimation are executed. For ISV, the rank nU
                      of the subspace U is set to 50 for speaker authentication system (S-ISV) and 160 for face authentication system (F-ISV), 10 iterations are performed to train the subspace U. For TV, the rank nT
                      of the subspace T is set to 400, and 25 iterations are done for the subspace training. When using LDA with i-vectors the projection matrix A is limited to n
                     1da
                     =200 dimensions. For PLDA, the ranks nF
                      and nG
                      of the subspaces F and G are both set to 50. In addition, the cohort set for zt-normalisation is selected from the training data: two thirds are used for t-norm and the remaining third is used for z-norm. For t-models, we used one model per session (instead of one model per client), as in [4]. This copes with the limited number of clients in the cohort.

Looking at the results provided in Tables 5, 6, 7, and 8
                        
                        
                        
                        , two general trends are emerging throughout the results. First, error rates on female clients are higher than on male clients. This might be due to the fact that the training set contains more men than women. Second, comparing the results of face authentication (Face) and speaker authentication (Speaker) systems, it is obvious that error rates of Face systems are lower than error rates of Speaker systems. This is possibly caused by the fact that speech segments are relatively short. Indeed, the average duration of the MOBIO probes after VAD is 7.9s (see their distribution in Fig. 4(a)), whereas the average duration of the probes after VAD in NIST SRE 2012 is 91.5s (see Fig. 4(b)).

To be comparable with previous work [12,8], our analysis focuses on the results of the mobile-0 experiments, which are summarised in Table 5. However, similar conclusions might be drawn from the experiments on the other protocols that are given in Tables 6, 7 and 8.

It can be seen that F-ISV and S-ISV (rows 2 and 7) outperform F-GMM and S-GMM (rows 1 and 6). This is also shown in [8]. Apparently, except for a few cases TV (rows 3, 4, 8, 9) provides superior performance to the standard GMM approach.

However, the comparison between ISV and TV is not so simple, although usually ISV provides similar or better performance. For Speaker, S-TV and S-ISV are comparable, even though S-ISV is slightly better. For Face, F-ISV is significantly better than F-TV. The DET curves plotted in Figs. 5 and 6
                        
                         support this observation. In these curves we can see some intersection between the ISV and TV curves and between the GMM and TV curves. These curves also show that the systems are well calibrated (close to straight lines with angles close to 45°) especially the ISV and TV systems.

The observation that ISV is at least as good as TV for speaker authentication is strange because TV is considered to be a state-of-the-art speaker authentication approach [9]. We believe that TV is limited by a lack of data needed to train the several steps such as: learning the TV matrix, whitening, LDA, WCCN and PLDA. For this reason we explore the use of additional training data in Section 5.6.

The bi-modal ISV system (B-ISV) outperforms both the B-GMM and B-TV systems. In Table 5 (rows 11, 12, and 13) and in the DET curves in Fig. 7
                         it can be seen that B-ISV clearly outperforms B-TV. The error rates drop significantly for all bi-modal systems. For example, on the Female mobile-0 protocol the HTER of the ISV system drops from 10.6% (F-ISV) and 16.2% (S-ISV) to 7.2% (B-ISV), a relative performance gain of 33% compared to the best uni-modal system. The results on the Male mobile-0 protocol are even more impressive with the HTER of the ISV system dropping from 6.5% (F-ISV) and 10.4% (S-ISV) to 2.4% (B-ISV), a relative performance gain of 63% compared to the best uni-modal system. This improvement can be explained by the fact that image and audio modalities are complementary: when Face fails to take the right decision because of image variability (illumination, head pose, etc.), Speaker is available to rescue, and vice versa.

The fusion of multiple algorithms (GMM, ISV and TV) consistently outperforms single systems, as shown in Table 5 (rows 5, 10 and 14). For example, the HTER of the Speaker system on the Male mobile-0 protocol drops from 10.4% (for ISV) to 7.9%, which corresponds to a relative improvement of 24%. The impact of the multi-algorithm fusion is higher for Speaker than Face because Speaker obtains a relative improvement of on average 19% compared to 3% for Face (the average is taken across all four protocols). We attribute this larger gain in performance for Speaker to the fact that TV has comparable results with ISV for Speaker but not for Face; TV performs much worse than ISV for Face. Finally, we note that the best bi-modal multi-algorithm fusion (B-ALL) outperforms the best uni-modal Face (F-ALL) and Speaker (S-ALL) systems with a relative improvement of up to 69% and 76%, respectively (for Male trials).

To explore the reason for the performance gains from multi-algorithm fusion we examine scatter plots of the scores for the ISV, GMM and TV systems. Fig. 8
                         shows scatter plots that relate the ISV scores to GMM scores and TV scores to ISV scores for the Speaker system on the Male mobile-0 protocol. The scatter plots indicate that fusing TV and ISV scores is the best strategy since the overlap between impostor and real client access classes is lower than for ISV and GMM. The small overlap can be explained by the fact that the scoring methods used for TV are significantly different from the ones used for ISV and GMM. This is supported by the observation that ISV and GMM scores are more correlated (linear distribution of the points) than TV and ISV scores (more wide-spread distribution).

In Table 9
                         we present the relative common error (RCE) of all of the multi-algorithm fusion systems. Apparently, ISV and TV have the lowest percentage of common errors: RCE=31.6%, followed by TV and GMM with: RCE=35.4%, while ISV and GMM have the highest common error: RCE=54.7%. This result confirms that TV is the most helpful system for multi-algorithm fusion. Another finding of this table is that the fusion of the three systems is better (RCE=21.7%) than the fusion of any two systems. It can also be seen that having a low percentage of relative common errors leads to an improved HTER, see row 3 of Table 9. Similar finding are obtained for Face and the bi-modal fusion systems as given in Table 9, though the improvement is not as significant as for Speaker since TV for Face is not as good as for Speaker.

To better understand why multi-algorithm fusion significantly improves the results for Speaker, we group the audio probe files into three clusters depending on their duration as seen in Table 9(b).Fig. 9(a) displays the HTER of each of the groups. Although S-ISV is the best average system, Fig. 9(a) shows that S-TV is better for short duration (<5s) segments. Interestingly, S-GMM is performing better on relatively long duration (>10s) segments (this might be due to threshold tuning on DEV).Hence, S-TV performs better on the 22.6% audio samples that are less than 5s, while S-GMM leads on the 26.6% of audio samples longer 10s. We believe that these two observations are the major reasons for multi-algorithm fusion providing a significant boost in performance for Speaker.

By performing bi-modal multi-algorithm fusion we are able to develop a state-of-the-art bi-modal, face and speaker, authentication system. In Fig. 10
                            we compare our system against the results obtained in [2,8] on the same mobile-0 protocol. This figure shows that we obtain a relative improvement of 35% and 27% on Female and Male, respectively, compared to the results of [8].


                        Fig. 11
                         displays the impact on enrolment condition mismatch on speaker, face and bi-modal authentication. It shows that GMM and ISV are significantly affected by changing the enrolment conditions (between mobile-1 and laptop-1), whereas TV seems to be more robust to these changes. Indeed, for Male clients the F-GMM, S-GMM and B-GMM systems have a relative performance degradation of 54%, 33% and 67%, respectively, while F-ISV, S-ISV and B-ISV loose 47%, 30% and 86%, respectively. By contrast the F-TV, S-TV and B-TV systems have a relative performance decrease of only 9%, 21% and 35%, respectively. Another interesting result is that the degradation of Face systems is higher than on Speaker systems. This shows that Face is more affected by condition mismatch of high versus low image quality (see Fig. 3) and is an issue that deserves further investigation.

On the other hand, adding enrolment data as done in the laptop-mobile-1 protocol improves authentication in all systems of Speaker and Face, even though the additional laptop data is quite different to the mobile phone data. In Fig. 11(a) and (b) the laptop-mobile-1 protocol outperforms the other protocols throughout. Interestingly, the bi-modal systems are reaching a performance plateau, since the results on the laptop-mobile-1 are comparable to the results on mobile-0 and mobile-1 protocols (see Fig. 11(c)).

In this section we examine two issues that relate to the limited amount of training data available with MOBIO. These issues are: i) the use of gender-independent models versus gender-dependent models and ii) the performance difference between ISV and TV. For speaker authentication, gender-dependent models are usually derived as they provide improved performance [31,26]. However, for MOBIO we found that with limited data, gender-independent models provided similar or better performance, see Table 10
                        . In addition, current state-of-the-art speaker authentication systems use TV [9], but in our experiments TV does not provide better results than ISV. We attribute this lack of performance to the limited amount of data available with MOBIO. To explore both of these issues we use an external database to train gender-dependent models including: UBM, subspaces (subspace U for ISV and subspace T for TV), whitening, LDA and WCCN.

We conducted an experiment with additional audio data to train a gender-dependent Female model. The external data were collected from the Voxforge speech dataset.
                           4
                        
                        
                           4
                           
                              http://www.voxforge.org This dataset was selected because of its similarity with MOBIO (short duration segments, different sessions of the same client, etc.). However, since the dataset is mainly dedicated to speech recognition (ASR) and is updated on the fly, it tolerates errors especially in the client identities, which could limit its usability for speaker authentication.
                         The new audio training set contains 78 female clients, 65 of which belong to Voxforge.

The impact of extending the training data is examined in Table 11
                         for TV modelling. It can be seen that using more data reduces the EER and HTER from 20.3% and 21.1% to 14.8% and 18.7% respectively. It also shows that the main improvement is obtained on PLDA (row 1) with a relative performance gain of 18% (The HTER drops from 41.5% to 33.8%). Table 11 also shows that the duration variability [40] described in Section 2.2.3 is also helpful in the case of additional training data. In contrast to the performance gains of S-TV, S-ISV is not improved by adding external training data as can be seen in Fig. 12
                        .

@&#CONCLUSIONS@&#

In this paper, we studied the problem of face, speaker and bi-modal authentication in the challenging mobile environment. The study was carried out on the MOBIO database, for which we proposed three new protocols. One of these new protocols, laptop-1, presents a significant challenge for both speaker and face authentication as there is a significant mismatch between enrolment and test conditions. Empirically, we found that both face and speaker authentication are adversely affected by this condition mismatch with the relative performance of the best uni-modal and uni-algorithm systems F-ISV and S-ISV degrading by 47% and 37%,respectively, for Male trials. The impact of this condition mismatch was extended to the bi-modal system, whose relative performance degraded by as much as 80% for Male trials.

We also examined several aspects of bi-modal and multi-algorithm fusion in the challenging mobile environment. We developed a state-of-the-art bi-modal multi-algorithm fusion system (B-ALL) that outperformed the state-of-the-art system of [8] obtaining a relative performance improvement of 35% and 27% on Female and Male trials, respectively. We found that multi-algorithm fusion provides a consistent performance improvement, particularly for the audio modality with average performance improvements of 3% for face authentication and 19% for speaker authentication across Male and Female trials for all of the protocols. In addition to this we showed empirically that ISV consistently outperforms not only GMM, but also TV with a limited amount of training data for both face and speaker authentication. We further explored this performance difference and found that TV provided improved performance for short utterances (less than 5s) and ISV provided better performance for medium length utterances (between 5s and 10s).

@&#ACKNOWLEDGEMENT@&#

The research leading to these results has received funding from the Swiss National Science Foundation under the LOBI project, from the European Community's Seventh Framework Programme (FP7) under grant agreements 238803 (BBfor2) and 284989 (BEAT), and from NICTA. NICTA is funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Centre of Excellence Program.

@&#REFERENCES@&#

