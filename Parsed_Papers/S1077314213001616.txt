@&#MAIN-TITLE@&#Tracking hand rotation and various grasping gestures from an IR camera using extended cylindrical manifold embedding

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A new cylindrical manifold embedding is proposed for tracking view and posture variation of hand shapes.


                        
                        
                           
                           The cylindrical manifold is extended to cover shape variations in different grasping style gestures.


                        
                        
                           
                           Tracking of hand shape variations with hand rotations in different type of grasping gestures from IR camera image is presented.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Tracking

Manifold embedding

Inferred image

Hand gesture recognition

Style decomposition

Conceptual manifold embedding

@&#ABSTRACT@&#


               
               
                  This paper presents a new approach for tracking hand rotation and various grasping gestures through an infrared camera. For the complexity and ambiguity of an observed hand shape, it is difficult to simultaneously estimate hand configuration and orientation from a silhouette image of a grasping hand gesture. This paper proposes a dynamic shape model for hand grasping gestures using cylindrical manifold embedding to analyze variations of hand shape in different hand configurations between two key hand poses and in simultaneous circular view change by hand rotation. An arbitrary hand shape between two key hand poses from any view can be generated using a cylindrical manifold embedding point after learning nonlinear generative models from the embedding space to the corresponding hand shape observed. The cylindrical manifold embedding model is extended to various grasping gestures by decomposing multiple cylindrical manifold embeddings through grasping style analysis. Grasping hand gestures with simultaneous hand rotation are tracked using particle filters on the manifold space with grasping style estimation. Experimental results for synthetic and real data indicate that the proposed model can accurately track various grasping gestures with hand rotation. The proposed approach may be applied to advanced user interfaces in dark environments by using images beyond the visible spectrum.
               
            

@&#INTRODUCTION@&#

Many studies have examined natural human–computer interactions in virtual environments, augmented reality, smart devices, and games [1,2]. The hand is one of the most effective interaction tools because of its dexterous functionality in communication and manipulation [3,4]. For useful human–computer interactions using hand gestures, it is important to accurately track hand pose without the use of gloves or other devices. A hand configuration is a kinematic hand model represented by joint angles independent of global transformations or rotations. A hand shape is defined by a combination of hand configuration and orientation. In addition to hand configuration, its rotation and global translation provide useful information for understanding hand gestures. In a manipulative hand gesture, a hand rotation parameter may provide additional useful information about object manipulation.

@&#RELATED WORKS@&#

A number of studies have addressed vision-based estimation and tracking of hand poses [4]. There are two main approaches to estimating a hand pose: model-based and shape/appearance-based. In a model-based approach, complex three-dimensional (3D) models [5] or simple 2D cardboard models [6] are used to generate possible hand shape samples. A hand shape can be estimated by searching for the best matching sample from the given model. An articulated hand model requires more than 20 degrees of freedom, although actual fingers are interdependent and constrained by their applications [7]. Many algorithms have been developed for efficiently searching in the high-dimensional configuration space based on optimization-based methods [8,5,9,6], physical constraints [10], multiple hypotheses, and multiple cue integration [11], among others. In addition to the 3D hand pose, hand texture and shading information have been modeled and dynamically estimated to enhance the accuracy of hand pose estimation [12]. Self-occlusion has been modeled [8,10,12] for more accurate modeling of articulated hand poses. Searching for a high dimensional space is challenging, and real-time tracking is difficult.

In a shape/appearance-based approach, the mapping of a hand pose sample and its corresponding shape and/or appearance is learned through various machine learning techniques [13–15]. Because of self-occlusion and similarities between different viewpoints, the relationship between the hand pose and the observed shape can be represented through multi-modal and many-to-one mapping. For the simplification of the segmentation of finger joints and the extraction of feature points, a precisely designed color glove was used for the real-time tracking of hand pose [16].

If the viewpoint for a given shape can be identified, then it can be much easier to estimate hand configuration for a given observed shape. The representation of a hand pose in a low-dimensional space preserving its intrinsic similarity to viewpoint variations may facilitate the estimation of hand configuration and view for a given observed shape.

Modeling viewpoints has received research attention to achieve view invariant object recognition from early appearance-based 3D object recognition. From a collection of 2D images of a 3D object, a low dimensional pose space from view variations can be modeled [17] for each object after a linear dimensionality reduction through principle component analysis (PCA). Bilinear model [18] has been used for decomposing discrete face poses (views) and identities.

Nonlinear manifold learning techniques, such as Locally Linear Embedding (LLE) [19] and Isomap [20], have been applied to thousands of unarranged images captured from the view sphere to map view sphere parameters [21]. Manifold-constrained mapping from the view hemisphere to observed 2D images has been used to estimate object poses [22]. The view sphere can be directly used to track viewpoint variations from an observed sequence of image contours based on the submanifold learning of object contours [23]. In addition to the conceptual view manifold, circular identity manifolds have been used to model variations in vehicle type for automatic target tracking and recognition [24]. In general, target objects in view modeling are rigid 3D objects, and these objects and their poses are detected and recognized without the use of 3D models. However, the hand is very different from a rigid 3D object because it is an articulated part of the body with a high degree of freedom, self-occlusion, and motion (temporal variations). Therefore, view models that are presented for a rigid 3D object cannot be directly applied to dynamic hand gestures with view variation.

Alternatively, nonlinear manifold learning techniques can be used to reduce dimensionality in state representation and computational complexity of high-dimensional human body configuration for tracking the articulated human body motion [25,26]. Topological constraints are used to model style variations in multiple motions in the latent spaces [27]. View models for articulated human body models have also been proposed for cyclic human locomotion [28] and complex motions such as dancing [29]. Multiview learning using a conditional entropy criterion in the presence of view disagreement has been presented [30].

Approaches based on nonlinear manifold learning have been used to estimate hand posture. The Isometric Self-Organizing Map (ISOSOM) is used for effective 3D hand posture estimation based on low dimensional nonlinear manifolds [31]. However, this approach is for retrieving the hand pose, and it is difficult to use for tracking hand pose because the model provides no continuous pose space when both hand pose and view change simultaneously.

This paper proposes a low-dimensional representation of hand configurations and viewpoint variations for hand pose tracking in arbitrary views. The initial findings have already been presented [32], and the present paper extends the original two key hand pose based tracking with view variations into multiple key poses using style analysis.

The contributions of this paper can be summarized as follows:
                           
                              •
                              
                                 Modeling non-cyclic articulated motions with view variations: This paper presents a new motion model for non-cyclic articulated hand motions with view variations. Continuous hand pose variations can be modeled through a one-dimensional manifold connecting initial and final pose embedding points in a high-dimensional space. This paper demonstrates that a combination of continuous view and pose manifolds can be represented by a cylindrical manifold.


                                 Modeling grasping gesture variations: Grasping can be achieved not only by using all five fingers but also by applying two or three fingers depending on the size of the object to be grasped and its weight. This paper shows how to learn multiple grasping gestures and track them through a single model by applying style analysis to different hand gesture graphing models.


                                 Tracking hand gestures through infrared images: Few studies have employed invisible or infrared (IR) images to track and recognize hand motions, although this can be useful when the natural interactive environment involves varying lighting conditions.

Example applications based on hand gestures include a gesture-based remote control for a home theater or smart TV, and human–machine interface for driving when lighting conditions vary or when it is dark. For this purpose, an IR camera system is used to capture hand motions in a dark environment.

The rest of this paper is organized as follows. Section 2 presents cylindrical manifold embedding for hand configuration variation and hand rotation in grasping gestures. Section 3 extends the cylindrical manifold to model various grasping gestures. Section 4 presents experimental results, and conclusions follow in Section 5.

This section considers data-driven manifold embedding for hand shape data with view variations and compares reconstruction accuracy between the proposed cylindrical manifold embedding model and other data-driven embedding models to evaluate their accuracy in modeling hand configurations and view space and observation shape space.

Synthetic hand shape data on variations in hand configurations and viewpoints are collected to estimate the embedding manifold for grasping motions with hand rotation. Poser®, a character animation tool, is used to generate synthetic data on a sequence of hand grasps with rotation along the view circle. Fig. 1
                         shows samples of hand shape silhouette images. Here the column shows viewpoint variations and the row, hand configuration variations from a closed hand to an open one.

Several manifold learning techniques are applied to model the characteristics of hand view and configuration manifolds from the collected data on variations in viewpoints and hand configurations. Linear manifold learning based on PCA shows smooth changes in the representation of the neighbor shape for the low dimensional manifold. Fig. 2
                        (a) shows manifold embedding based on PCA for three dominant principle components. Partially, its embedding is circular because of variations in the circular view. However, it is difficult to characterize variations in viewpoints and hand configurations through a specific embedding coordinate in this PCA space. Isomap [20], a well-known nonlinear manifold learning technique, is applied to the same data set. Fig. 2(b) shows the distorted characteristics of the circular view for some coordinates from Isomap manifold embedding. Fig. 2 (c) shows similar embedding results for Laplician eigenmaps [33]. For hand posture variations, however, data-driven embedding results are distorted, stretched, and lose characteristics of hand pose variations, indicating a need for an alternative embedding model that can effectively model variations in hand views and configurations.

As discussed earlier (Section 1.1), some studies have modeled view circles or view sphere manifolds. This section models motion manifolds from configuration variations, as well as circular view manifolds from hand rotation, by using the collected data on variations in hand viewpoints and configurations. A torus manifold has been proposed for cyclic human motion manifolds with view variations [28]. However, the proposed manifold may not be appropriate in this grasping hand gesture modeling because the motion is non-cyclic. Therefore, a new manifold embedding model for non-cyclic motion with view variations along the view circle is required. A style analysis of human motions may be applicable if each viewpoint sequence is modeled through a separate style. In this case, however, the style vector is very high dimensional, which makes its estimation difficult, as demonstrated by the experiment in Section 4.2.

In this paper, cylindrical manifold embedding is proposed for noncyclic motions with circular view variations. A cylindrical manifold can be described by two independent variables: one for circular view variations (μ) and the other for noncyclic configuration variations (ν). The noncyclic configuration variation was caused by grasping hand motion. These two axes are orthogonal, and any point on the cylindrical manifold can be described by two variables, μ and ν, as follows:
                           
                              (1)
                              
                                 
                                    
                                       
                                       
                                          
                                             x
                                             =
                                             
                                                
                                                   R
                                                
                                                
                                                   a
                                                
                                             
                                             cos
                                             2
                                             π
                                             μ
                                             ,
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             y
                                             =
                                             
                                                
                                                   R
                                                
                                                
                                                   a
                                                
                                             
                                             sin
                                             2
                                             π
                                             μ
                                             ,
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             z
                                             =
                                             
                                                
                                                   R
                                                
                                                
                                                   b
                                                
                                             
                                             ν
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        where μ, ν
                        ∈[01], R
                        
                           a
                         is the radius of the cylinder, and R
                        
                           b
                         is the height of the cylinder. In the experiment, R
                        
                           a
                         and R
                        
                           b
                         are set to 1.0. Fig. 3
                         shows cylindrical manifold embedding for training hand data. The z value represents the configuration of the grasping hand gesture. Here the closed hand is 0.0, whereas the open hand is R
                        
                           b
                         (1.0 in our experiment). Intermediate configuration variations are modeled using equal interval samples. Any intermediate grasping configurations can be represented by a z value between 0 and R
                        
                           b
                        . In Section 3, the model is extended to variations in the grasping style by modeling the configuration of grasping gestures from a closed hand to a specific grasping open hand, which allows for consistent modeling of the configuration order between 0 and R
                        
                           b
                        .

Nonlinear mapping is learned between embedding points and their corresponding hand shape images, which are normalized. Nonlinear mapping from embedding points x
                        
                           i
                        
                        =(x
                        
                           i
                        ,
                        y
                        
                           i
                        ,
                        z
                        
                           i
                        ) to the corresponding hand shape y
                        
                           i
                        
                        ∈
                        R
                        
                           n
                         can be learned using a generalized radial basis function (GRBF) similar to that of Elgammal and Lee [25], where n is the shape dimension represented by the signed distance function along the silhouette boundary. Cylindrical manifold embedding points and corresponding shape pairs are used to learn nonlinear mapping through the GRBF. Nonlinear mapping can be represented by the kernel function ψ(·), where a thin-plate spline kernel is used in the experiment, and the linear projection B from the kernel space as follows:
                           
                              (2)
                              
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 B
                                 ψ
                                 (
                                 
                                    
                                       e
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 =
                                 B
                                 ψ
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       z
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 ,
                              
                           
                        
                        
                           
                              (3)
                              
                                 
                                    
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                          
                                             2
                                          
                                       
                                       +
                                       
                                          
                                             y
                                          
                                          
                                             i
                                          
                                          
                                             2
                                          
                                       
                                       )
                                    
                                 
                                 =
                                 
                                    
                                       R
                                    
                                    
                                       a
                                    
                                 
                                 ,
                              
                           
                        where e
                        
                           i
                         is an ith embedding point, and cylindrical embedding has the constraints in Eq. (3). As in the case of cylindrical manifold embedding, other nonlinear embedding e can be used to learn mapping between the embedding space and the observation space. For example, in Isomap embedding, nonlinear mapping between points on three-dimensional Isomap embedding and the observation shapes is learned using an equation similar to Eq. (2) without other constraints.

For an evaluation of modeling accuracy between the embedding space and the observation space, the reconstruction accuracy of intermediate view and configuration shape data is evaluated through the following two steps. In the first step, the embedding space is estimated from intermediate shape data by using direct inverse mapping from the learned GRBF using polynomial approximation similar to that of Elgammal and Lee [25], which shows modeling from the observation shape space to the embedding space. In the second step, new sequences of shape data are reconstructed from the estimated embedding space to the observation space, which allows for an evaluation of modeling accuracy from the embedding space to the observation shape space. Reconstruction accuracy is evaluated by measuring the similarities between the reconstructed and the original test shapes. Reconstruction error is measured by the mismatch in pixels between the original binary image and its reconstructed binary image. For PCA embedding, reconstruction accuracy is evaluated using a linear projection matrix directly without GRBF mapping. Table 1
                         shows the average reconstruction error per pixel according to different embedding models. Figs. 3 and 4
                         show the reconstruction of embedding points from the inverse mapping of embedding points for each given intermediate view and configuration observation in each embedding space. The estimated embedding points (blue
                           1
                           For interpretation of color in Figs. 3, 4 and 9, the reader is referred to the web version of this article.
                        
                        
                           1
                        ) for the intermediate test data show intermediate embedding points for each of the training sample embedding points (red). The evaluation results indicate that cylindrical embedding provides the most accurate reconstruction for the intermediate test data.

This section extends cylindrical manifold embedding to model various grasping hand motions. In the actual grasping gesture, two fingers (e.g., the thumb and the index finger) or three fingers (e.g., the thumb, the index and the middle finger) can be used to grasp small objects. The problem here is how to learn shape variations of dynamic grasping sequences as well as shape variations according to view change.

Conventional shape style analysis of dynamic human motion can be applied, similar to that of Elgammal and Lee [25]. In this case, each sequence of motion in the different views of each grasping gesture is considered separately by each different style of motion. For example, if M different grasping gestures are modeled with N different views, M
                     ×
                     N projection matrices from a kernel space are required, as shown in Fig. 5
                     (a), and consequently M
                     ×
                     N style vectors will be required to model the sequence of gestures. In this case, the shape variations by view and the shape variations by different grasping style are not distinguished; All variations of dynamic shape in the sample sequences are counted by style variations. To reduce the number of style vectors, multi-linear analysis of dynamic motion can be applied similar to Lee and Elgammal [29] after kinematic embedding of the sequence. In this approach, N view vectors will be decomposed in addition to M style vectors to model the sequence of grasping gestures with view variations.

When the proposed cylindrical manifold is applied, the grasping style variations can be modeled using more intuitive and straight-forward ways; each grasping style sequence is modeled by a style vector. The topological characteristics of the view circle are also consistently well modeled by circular components of cylindrical coordinates. Just M style vectors are required to model grasping shape variations of the same data set with M different grasping style gestures and N different view point sequences along view circle. Fig. 5(b) shows cylindrical manifold embedding for M grasping sequences of gestures with view variations and corresponding M projection matrices B
                     1, B
                     2,…,
                     B
                     
                        M
                     , which can be decomposed into M style vectors [25,34].

Additional grasping hand gestures are collected using Poser®, as was done in Section 2. For each sequence of gestures, nonlinear mapping is learned using GRBF as described in Section 2.2. Using the proposed cylindrical manifold embedding, we have M nonlinear mappings for each sequence as follows:
                        
                           (4)
                           
                              
                                 
                                    y
                                 
                                 
                                    t
                                 
                                 
                                    1
                                 
                              
                              =
                              
                                 
                                    B
                                 
                                 
                                    1
                                 
                              
                              ψ
                              (
                              
                                 
                                    e
                                 
                                 
                                    t
                                 
                              
                              )
                           
                        
                     
                     
                        
                           (5)
                           
                              
                                 
                                    y
                                 
                                 
                                    t
                                 
                                 
                                    2
                                 
                              
                              =
                              
                                 
                                    B
                                 
                                 
                                    2
                                 
                              
                              ψ
                              (
                              
                                 
                                    e
                                 
                                 
                                    t
                                 
                              
                              )
                           
                        
                     
                     
                        
                           (6)
                           
                              
                                 
                                    
                                    
                                       
                                          ⋯
                                       
                                    
                                 
                                 
                                    
                                    
                                       
                                          
                                             
                                                y
                                             
                                             
                                                t
                                             
                                             
                                                M
                                             
                                          
                                          =
                                          
                                             
                                                B
                                             
                                             
                                                M
                                             
                                          
                                          ψ
                                          (
                                          
                                             
                                                e
                                             
                                             
                                                t
                                             
                                          
                                          )
                                          ,
                                       
                                    
                                 
                              
                           
                        
                     where M is the number of the collected sequences of different grasping gestures.

Various grasping gestures can be represented using different projection coefficients B
                     
                        i
                     . Given these matrices for grasping styles 1,…,
                     M, the style parameters are factorized by fitting bilinear models to the projection coefficients such that
                        
                           (7)
                           
                              
                                 
                                    
                                       
                                          
                                             b
                                          
                                          
                                             1
                                          
                                       
                                       ,
                                       ⋯
                                       ,
                                       
                                          
                                             b
                                          
                                          
                                             M
                                          
                                       
                                    
                                 
                              
                              =
                              AS
                              ,
                           
                        
                     where each b
                     
                        i
                      is the vector representation of B based on column stacking. The style matrix S
                     =[s
                     1,
                     s
                     2,…,
                     s
                     
                        M
                     ]
                        T
                      is an orthogonal matrix of grasping styles. Different styles of the shape model can be tracked by estimating the weight of the grasping styles [34].

After learning M grasping gestures, a new style vector can be generated by combining style bases with the linear weight of different grasping gestures. New grasping gestures can be generated from new style vectors, which can be modeled by a combination of existing grasping gesture styles as shown in Eq. (8).
                        
                           (8)
                           
                              
                                 
                                    s
                                 
                                 
                                    new
                                 
                              
                              =
                              
                                 
                                    w
                                 
                                 
                                    1
                                 
                              
                              
                                 
                                    s
                                 
                                 
                                    1
                                 
                              
                              +
                              
                                 
                                    w
                                 
                                 
                                    2
                                 
                              
                              
                                 
                                    s
                                 
                                 
                                    2
                                 
                              
                              +
                              ⋯
                              +
                              
                                 
                                    w
                                 
                                 
                                    M
                                 
                              
                              
                                 
                                    s
                                 
                                 
                                    M
                                 
                              
                              ,
                           
                        
                     where w
                     1
                     +
                     w
                     2
                     +…+
                     w
                     
                        M
                     
                     =1. The transition in different grasping hand gestures can be controlled by the weighting vector in Eq. (8).

Each style vector is used in the weight estimation from a given hand shape during tracking. With a Gaussian distribution assumption, the probability of belonging to a specific gesture can be determined from the measurement of the distance from the center of the representative style to the observation, which is exponentially proportional to the error value of the observed data. In addition, the configuration parameter, z value, can be separately controlled in the embedding space, which can control the configuration transition. Therefore, the transition from one grasping gesture to another can be controlled in an arbitrary intermediate configuration.

Two additional grasping gestures are selected to learn cylindrical manifold embedding separately, and style factors are decomposed using the singular value decomposition method. Fig. 6
                     (a)–(c) show three collected grasping gestures, and Fig. 7
                      provides an example of style vectors decomposed from three different grasping sequences, where M
                     =3. These style vectors are eigenvectors of the matrix of the collection of projection vectors after column stacking. These style vectors are used to track multiple grasping gestures for synthetic (Section 4.2) and real (Section 4.3) data. Fig. 6(d) shows the transition of the grasping model from a full-finger grasping (s
                     3) to a three-finger grasping (s
                     2) gesture.

The style weight indicates the similarity of the selected basis for the representation of the original data for given particle states of views and configurations. The error value for each style basis for a given particle state can be easily measured by using the distance between observation sample and synthesized particle shape. The actual implementation of this estimation method can be achieved through two approaches.

In the proposed cylindrical embedding based approach, the amount of error with particles on the cylindrical embedding is measured for each style basis. From this error estimation, a new style is estimated based on the distance to the Gaussian distribution of each basis style, which allows for the estimation of the combined weights of each grasping style and the estimation of new styles through the combined weight of the existing style basis. The error of each particle is re-evaluated to estimate the likelihood of cylindrical embedding more accurately.

In the conventional style adaptive tracking based approach [34], view variations cannot be modeled separately. One way to apply this tracking method in different views is to model each view sample as a separate style. In this case, there are 18 styles for each grasping gesture from 18 discrete view sequences in the training data. Here 54 (18×3) dimensional styles are modeled using style adaptive tracking without view models, and 54 dimensional style vectors are estimated in addition to the circular embedding configuration. Circular embedding is used to represent configuration embedding space. The evaluation method is similar, although the style dimension is much higher, and circular embedding is used instead of cylindrical manifold embedding.

@&#EXPERIMENTAL RESULTS@&#

The proposed system for tracking the hand pose is applied as a human gesture interface for controlling a smart TV, which has numerous functions and is difficult to control by using a conventional remote control. Simple and intuitive control of such devices can be implemented using hand gestures. Tracking grasping hand gestures from an open hand to a closed one with hand rotation can be useful for smart TV control. For example, smart TV volume can be controlled based on a grasping gesture and its hand rotation. Similarly, the open hand and its rotation can be used to turn channels forward or backward according to the direction of the rotation, and video play can be stop using closed hand shape and its pushing gesture.

Based on the proposed cylindrical manifold embedding and its generative model, the hand shape with view and configuration variations can be synthesized. Hand rotation in a fixed hand configuration can be modeled by view variations in the given hand configuration. A Bayesian framework is employed to track hand pose variations. Hand state X
                     
                        t
                      is estimated from a given observation Y
                     
                        t
                      by using a particle filter as follows:
                        
                           
                              P
                              (
                              
                                 
                                    X
                                 
                                 
                                    t
                                 
                              
                              |
                              
                                 
                                    Y
                                 
                                 
                                    t
                                 
                              
                              )
                              ∞
                              P
                              (
                              
                                 
                                    Y
                                 
                                 
                                    t
                                 
                              
                              |
                              
                                 
                                    X
                                 
                                 
                                    t
                                 
                              
                              )
                              
                                 ∫
                                 
                                    
                                       
                                          X
                                       
                                       
                                          t
                                          -
                                          1
                                       
                                    
                                 
                              
                              P
                              (
                              
                                 
                                    X
                                 
                                 
                                    t
                                 
                              
                              |
                              
                                 
                                    X
                                 
                                 
                                    t
                                    -
                                    1
                                 
                              
                              )
                              P
                              (
                              
                                 
                                    X
                                 
                                 
                                    t
                                    -
                                    1
                                 
                              
                              |
                              
                                 
                                    Y
                                 
                                 
                                    t
                                    -
                                    1
                                 
                              
                              )
                              d
                              
                                 
                                    X
                                 
                                 
                                    t
                                    -
                                    1
                                 
                              
                              ,
                           
                        
                     where Y
                     
                        t
                      is the cumulative collection of observations Y
                     1,
                     Y
                     2,…,
                     Y
                     
                        t
                     .

Tracking performance is tested using simulated data first. For modeling the grasping hand gesture along the view circle, 18 discrete view samples along the view circle are obtained at each 20° interval. Posture variations from an open hand to a closed one are sampled for 12 different hand configurations. A total 216 (12×18) sample shapes are used to learn the proposed cylindrical model between open hand shape and closed.

For the performance evaluation of the synthetic data, a sequence of hand pose variations is obtained, from open to closed, with simultaneous view rotation twice along the view circle. Test hand shapes can easily be normalized, just as with the training data. The hand state X
                        
                           t
                         can be directly represented by the embedding point x
                        
                           t
                         from the given observation y
                        
                           t
                        . Therefore, hand pose and viewpoints can be estimated directly from the estimated embedding point. Embedding points can be estimated in two ways. First, they are estimated by inverse mapping from GRBF mapping [25]. This can be achieved using polynomial terms in the GRBF for inverse mapping. Second, particle filtering is used to estimate the embedding space for a given observation. Fig. 8
                        (a) shows the samples, and Fig. 8(b) presents the tracking results using a particle filter.

For the particle filter, 30 particles for the view state μ and 30 particles for the hand configuration state ν are used. Tracking particles on the cylindrical manifold are represented by the product of these two embedding parameters in the embedding space μ
                        ×
                        ν. Embedding points are converted to three-dimensional (x,
                        y,
                        z) points. Fig. 8(c) shows the embedding points obtained using the particle filter. Here the spiral trajectory with double circular path represents a twice-rotating hand with continuous hand pose variations from the open hand to the closed one. The approach based on the particle filter shows 25% performance improvement over that based on direct inverse-mapping in terms of reconstruction accuracy for estimated embedding points.

For the evaluation of the performance of the style-based cylindrical manifold extension for various grasping gestures, two different grasping sequences with view variations are considered. The test sequence starts from the closed hand at frame 1 and changes to open (s
                        3) with 45° rotation of the hand at frame 45, and then the hand changes back to the closed posture at frame 90 with constant change of configuration and rotation parameters, followed by three-fingers open hand (s
                        2) and back to the closed hand in the following 90 frames.

For a comparison of the performance of the proposed method with that of a conventional style of adaptive human motion tracking [34], each sample view sequence is modeled as a separate style. Because three different grasping gestures are used to model various grasping gestures with the extension of the cylindrical embedding, a three-dimensional style vector is sufficient for modeling the grasping style. This three-dimensional style vector needs to be estimated in addition to the cylindrical manifold embedding. The style of a grasping gesture can be modeled through a weighted combination of style vectors, as in Eq. (8). A typical way to estimate the new style during particle filtering is to model the weight of each style basis as a separate state of particles [34]. However, each new style includes a new mapping coefficient, and needs to model a new nonlinear mapping to the observation, which entails heavy computational costs.

Experimental results show that cylindrical embedding allows for more accurate tracking of hand shapes under view variations. Tracking accuracy is measured by the average pixel error of the reconstructed hand from the estimated configuration and view parameters. Table 2
                         shows the estimation results for the average accuracy per pixel. Fig. 9
                        (a) shows a test sample sequence. Fig. 9(b) shows the MAP (maximum a posteriori) estimated hand shape using a particle filter based tracking. Fig. 9(c) shows the estimated style weight of cylindrical manifold embedding. The range of style weight is constrained from 0 to 1 to estimate new style vectors by linear combination of existing style vectors. Estimated style around frame number 45 and 135 shows that s
                        3 and s
                        2 styles are dominant, as expected. However, when the configuration is near a closed hand, the style weight is biased towards s
                        1 style because s
                        1 is more similar to a closed hand than others. Fig. 9(d) shows the MAP tracking results for circular embedding and view style parameters. Fig. 9(e) shows average pixel errors in each frame from two different approaches: the proposed cylindrical embedding approach and the circular embedding approach with style parameters for view variations. The cylindrical embedding (blue) shows more accurate tracking in most of the frames than the style analysis without view modeling [34]. The blue line, which is based on circular embedding with view style parameters, does not show dips in the tracking error due to averaging effect of weight estimation from a large number of style vectors. This characteristic can be changed by modifying the assumed variance of Gaussian distribution of each style vector when weight is estimated. The number of particles used for view state μ is 30, and the number for pose states ν is also 30, which is consistent with the conditions of the previous experiments. In this experiment, the computation time required is 49.98s to track 181 frames under Windows 7 using i7 laptop without optimization in Matlab. On average, for each frame, 0.27s is required to estimate hand style, configuration, and view by particle filter on cylindrical manifold embedding. Real-time processing will be possible when it is converted to C++ code with optimization.

A sequence of hand motions is captured using an IR camera for real data experiments. Fig. 10
                        (a) shows the system setup, which includes an IR camera and infrared-emitting lighting equipment. A sequence of hand gestures is collected in a dark laboratory. Even for a simple background, a fixed threshold may not accurately segment the foreground hand from other objects. From collected IR images, a median filter is applied to extract foreground images with hole filling. If the filter is large, the smoothing effect can cause other artifacts and reduce tracking accuracy. Therefore, there is a need for a median filter that can minimize the smoothing effect of the image by appropriately selecting the size of the median filter. Fig. 10(b) shows the original raw data and the extracted foreground image before and after application of the median filter.

Hand views and configurations are estimated using a particle filter. In the first experiment with real data, the global transformation as well as the hand pose is estimated for the real data because hand detector is not used and there is no location information for the hand model. The two-dimensional translation (R
                        
                           x
                        ,
                        R
                        
                           y
                        ), rotation (θ), and scaling (s) are used to represent global transformation states in the particle filter. The scaling factor is represented by its log value in the particle sampling state space and the exponential value is applied to the sample value for the robustness of estimation results of the scale value. Fig. 11
                         shows tracking results of a grasping hand gesture with rotation. The red line indicates the estimated shape based on cylindrical manifold embedding. Image intensity is shown for a comparison of the original capture data and the estimated shape. In the actual experiment, a binary foreground hand shape is used to estimate hand configuration and view parameters. Experiment results show inaccurate tracking results in Fig. 11. In particular, the right two frames show a large change of hand pose even though there is a small frame difference. This is due to the large variance value selected for the experiments.

In the experiments, dynamics of the particle are based on random propagation of particle along one dimensional configuration parameter since the embedding configuration space characterizes very well the neighborhood shape of the hand during view change and configuration change. In addition, the hand configuration can vary from open hand to closed and from closed to open. A large variance value provides tracking during fast grasping and hand rotation motion. A small variance value may cause loss of tracking for fast motion even though it prevents abrupt changes of pose or views during tracking. In the original experiments, we used relatively large variance parameters, which causes abrupt change of estimated hand poses.

In the second experiment, normalized binary foreground images are directly used to estimate hand configuration and view parameters. From the given image intensity, a simple threshold value is used to extract foreground images. The scaling and cropping operations are applied as they were with the synthetic data. A fixed scale value and a fixed threshold value for foreground extraction are used; the foreground hand shape is not very good, and the size of hand varies with the distance to the camera. Fig. 12
                        (a) shows a raw test image sequence captured using an IR camera and Fig. 12(b) shows cropped and normalized shape sequences, which are used for evaluation of view and configuration. Fig. 12(c) shows a reconstructed shape sequence generated from estimated hand configuration and view parameters using learned nonlinear generative models. Estimated hand configuration shows a gradual change of configuration over time, although there are abrupt changes of parameter when there is confusion in the view parameter estimation as shown in Fig. 12(d). There are ambiguities in the hand view estimation, especially when the configuration is near the closed hand because the shape in the different view is similar when the pose is a closed hand.

Additional experiments were performed for adaptive tracking of grasping hand gestures from real data sequences; 1100 frames were collected, and foreground hand shapes were extracted by threshold from the captured IR images. Palm area holes were removed with a masking operation applied to the extracted binary images. At the beginning, the hand grasping is performed starting from a closed hand using three fingers (style S
                        2) with hand rotation, but the grasp later changes to all fingers (style S
                        3). In the closed hand (middle of the sequence), the grasping style using two fingers (S
                        1) has the highest weight because the gesture style is similar to other styles. Fig. 13
                        (a) and (b) show the original intensity sequence from the IR camera and the extracted foreground images. Reconstruction results based on estimated view, configuration, and style parameter are shown in Fig. 13(c). Estimated style parameters are shown in Fig. 13(d).

@&#CONCLUSION@&#

This paper presents a new approach to the simultaneous tracking of hand pose variations with hand rotation based on cylindrical manifold embedding. Hand configuration variations and rotation can be estimated simultaneously through a sequence of shape images by representing variations in hand configurations and viewpoints simultaneously in a low-dimensional cylindrical space. The cylindrical model is extended to wide variations in grasping hand poses though style analysis of different grasping gestures. The proposed model shows more accurate tracking results compared with style adaptive tracking without view variations. Compared with the previous approach [34], computationally, the proposed approach shows the different way to calculate the weights for different styles. However, it comes from a different way to model the embedding space. The presentation of different manifold embedding space is novel and can be applicable to many other applications, which is either not possible or is difficult with the conventional approaches.

The proposed model can be applied to many applications for human dynamic motion analysis with view modeling. In particular, the motion is non-cyclic with some additional style variations in person and in the ways of making gestures. Kicking, golf swings, and punching are example gestures in addition to grasping gestures.

@&#ACKNOWLEDGMENT@&#

This research was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education, Science and Technology (2010-0006178, 2012R1A1B4003830)

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.cviu.2013.08.006.


                     
                        
                           Supplementary Video 1
                           
                        
                     
                  


                     
                        
                           Supplementary Video 2
                           
                        
                     
                  

@&#REFERENCES@&#

