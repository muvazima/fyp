@&#MAIN-TITLE@&#A new vision of evaluating gene expression signatures

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Uniqueness, parsimony, and best performance of gene signatures should not be the only issues of concern.


                        
                        
                           
                           Multiple near-optimal gene expression signatures are shown to provide valuable computational and biological information.


                        
                        
                           
                           Bead-chain plot that can be generated by using any suitable meta-heuristics is presented for near-optimal signature evaluation.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Bead-chain plot

Gene signature evaluation

Near-optimal signature

Phenotype distinction

Swarm intelligence

@&#ABSTRACT@&#


               
               
                  Gene expression profiles based on high-throughput technologies contribute to molecular classifications of different cell lines and consequently to clinical diagnostic tests for cancer types and other diseases. Statistical techniques and dimension reduction methods have been devised for identifying minimal gene subset with maximal discriminative power. For sets of in silico candidate genes, assuming a unique gene signature or performing a parsimonious signature evaluation seems to be too restrictive in the context of in vitro signature validation. This is mainly due to the high complexity of largely correlated expression measurements and the existence of various oncogenic pathways. Consequently, it might be more advantageous to identify and evaluate multiple gene signatures with a similar good predictive power, which are referred to as near-optimal signatures, to be made available for biological validation. For this purpose we propose the bead-chain-plot approach originating from swarm intelligence techniques, and a small scale computational experiment is conducted in order to convey our vision. We simulate the acquisition of candidate genes by using a small pool of differentially expressed genes derived from microarray-based CNS tumour data. The application of the bead-chain-plot provides experimental evidence for improved classifications by using near-optimal signatures in validation procedures.
               
            

@&#BACKGROUND@&#

The analysis of gene expression data plays an important role in biomedical research. High-throughput screening (HTS), such as microarray data, is still the method of choice when it comes to obtaining gene expression profiles for cells of interest. The analysis of the obtained expression profiling data has shown enormous potential for the discovery of biomarkers in carcinogenesis studies and in the diagnoses of diseases in general (Nevins and Potti, 2007). Different types of tumour cells can be marked by discriminating genes at expression level. Thus, biomarkers for distinct tumourigenesis stages and cancer classification can be identified by selecting discriminating genes, a so-called gene signature. Such a gene signature is usually a subset of genes contributing to the predictive power (Kim, 2009). Due to abundance of transcripts in a tissue, genes are differently expressed and a tremendous amount of mRNAs can be regarded as noise. Out of more than thousands of genes, the aim of an analysis is, therefore, to select a small subset of candidates such that experimental validation can be performed effectively in order to identify prognostic signatures such as 70- (Vanâ€™t Veer et al., 2002; Van De Vijver et al., 2002; Buyse et al., 2006) and 76-gene signatures (Wang et al., 2005; Foekens et al., 2006; Desmedt et al., 2007), which are two well-known gene-expression signatures in early stage breast cancer. Most recently, another custom mini-array of 74 genes has been used for gastric cancer prognosis extracted form a microarray signature (Yin et al., 2013), exemplifying the clinical application of dimension reduction by a deterministic approach.

With further advances in HTS technologies, the number of oligonucleotide probes that can be interrogated simultaneously has risen to many thousands and the complexity of analysing the classification performance of the exponentially growing number of gene signatures requires the development of new computational approaches. So far, a number of methods from areas such as data mining, machine learning and pattern recognition have been employed in the analysis of gene expression data derived from high-throughput platforms (Saeys et al., 2007). Here, objects are represented by a vector of features. New techniques have been developed for selecting feature subsets that not only reduce the dimension of the vector without changing the descriptive power for each object, but also to find the minimal subset that maximises the classification performance. In terms of knowledge discovery, this is based on the parsimony principle (Bell and Wang, 2000), i.e., seeking a model that has as few as possible variables and fits the data sufficiently. Unfortunately, a typical microarray-based cancer experiment consists only of tens to hundreds clinical samples, but each sample has thousands of genes to be considered (Ein-Dor et al., 2006). Therefore, the challenge is to find a model within a tremendous dimensionality by using only relatively few data points. Another challenge is experimental noise. The noise resulting from the experimental design of microarrays, such as the stages of sample preparation, and the hybridisation processes is unavoidable (Tu et al., 2002). Consequently, a number of statistical models have been designed that take into account the quantitative characterization of noise at probe level in order to generate more accurate expression measurements (Irizarry et al., 2003; Wu et al., 2004; McCall et al., 2010). Based on these models, a software tool has been designed for finding single feature polymorphisms at DNA or RNA level by using cross species arrays (Lai et al., 2014).

The general problem of minimal feature subset selection is known to be NP-complete (Davies and Russell, 1994), and a number of heuristics have been proposed for this particular problem. In general, there are three types of feature selection methods for high dimensional gene selection. The three types are filter, wrapper, and embedded techniques. Filter-based techniques measure the discriminative power of genes only according to the intrinsic expression data and statistical evaluation models. Wrapper-based techniques measure the predictive power of gene sets by using machine learning methods and heuristic search strategies. If searching for features is embedded within the update of classification functions or penalty functions during the process of model training, the technique is called an embedded-based method. For an overview on these methods, we refer the reader to (Saeys et al., 2007; Lazar et al., 2012).

Depending on the level of mutually related genes and the degree of mis-representation of physiological and genetic changes caused by the sample set, many different prognostic signatures may exist with the same or varying cardinality. While having a similar classification performance, these distinct signatures may also produce a considerable overlap in patients that are, for example, at high risk of breast cancer recurrence (Fan et al., 2006). This demonstrates that uniqueness of signatures may not be the major concern in the process of identifying gene-expression signatures. Consequently we follow the idea that when calculating prognostic gene signatures it might be advantageous to focus on near-optimal solutions of comparable classification performance, which is in contrast to conventional gene selection methods that aim at the minimal subset of genes having the maximal predictive power (or the minimal misclassification rate).

In this paper, we would like to draw attention to the importance of gene signature evaluation as a subsequent step of gene selection that can be performed by any statistical models or dimension reduction techniques. Along this line, we propose a new approach for establishing gene signature profiles with regard to the number of selected genes, the number of gene-expression signatures and their performance with respect to the generalisation error rate. The new approach not only allows us to compare different gene signatures, but it also provides knowledge about the number and components of signatures having the same cardinality. When analysing biological networks, lists of candidate genes are usually selected according to exploratory data analysis and knowledge about the domain of interest, which is mainly based upon various statistical methods (like t-test, ANOVA, SAM (Tusher et al., 2001), LIMMA (Smyth, 2004), PCA, etc.). Subsequently, the selected candidate genes are experimentally validated and a gene signature related to a certain disease is identified.

In our approach, we use t-statistics in order to simulate the acquisition of candidate genes. Given a set of candidate gene expression measurements, we employ local search methods based upon swarm intelligence, namely, particle swarm optimization (PSO) (Kennedy and Eberhart, 1995, 1997; Kennedy et al., 2001) and the artificial bee colony (ABC) concept (Karaboga, 2005; Karaboga and Ozturk, 2009, 2011; Karaboga et al., 2014) in order to demonstrate the new approach for multiple gene signature evaluation over unrestricted signature cardinalities. For both PSO and ABC, the classification algorithms are designed as randomised swarm wrappers, while the sequential forward selection (SFS) wrapper is deterministic for the baseline construction. We employ the microarray data recording from (Pomeroy et al., 2002) for embryonic tumours in the central nervous system (CNS) in order to demonstrate our approach. The dataset consists of 60 samples with 7129 reference genes. Among the samples 21 are survivors (patients who are alive after treatment) and 39 are failures (patients who succumbed to their disease). As part of our approach, we offer the bead-chain plot for the visual analysis of gene signature profiles, which covers signature sizes, the number of signatures, and the signature performance explored by swarm wrappers.

@&#METHODS@&#

Our approach aims at the exploration of gene signature evaluations by taking into consideration the signature cardinality, the performance of signatures, and the number signatures. In order to achieve the objective of multiple gene-expression signature analysis, where the particular signatures exhibit a comparable performance, we utilise stochastic optimization methods. Local search procedures based upon swarm intelligence are the method of choice for the evaluation of feature subsets from the search space of all features. By combining randomised search with a classifier, we are able to build a swarm wrapper for finding the most likely solution for a signature at each particular search stage, where multiple search agents are working in parallel. Hiring multiple search agents is at the core of swarm intelligence, which is one of the typical population-based optimisation methods. There are many subtypes of meta-heuristic algorithms being part of the concept of swarm intelligence. In our empirical study, one deterministic wrapper and two meta-heuristics are employed: deterministic sequential forward selection (SFS), particle swarm optimization (PSO), and the artificial bee colony (ABC) concept. While SFS is used for demonstrating that uniqueness of signatures has its limitations, swarm wrappers are designed in order to show the power of multiple putative signatures of comparable performance.

As we intend to develop a method to effectively evaluate signature profiles without restrictions on the signature cardinality, it is important to keep the same size of the population of putative solutions in order to highlight the effect signature cardinalities. If, however, the signature size is constraint, we refer to the problem setting as the constraint swarm approach. On the other hand, swarm intelligence methods can innately capture the most feasible solutions without the constraint of having the signature size fixed, which is â€“ we regard this as referred to as the unconstraint swarm approach.

Sequential forward selection (SFS) is a stepwise forward procedure. Given a list of candidate genes, a SFS wrapper selects the first gene whose classification performance (evaluated by a given fitness function F) is the best. With the inclusion of the first selected gene, F evaluates the combined performance of all subsets of two genes and the second gene is selected where their evaluation performance is the best. Iteratively, the third gene is selected with the inclusion of the first two selected genes. The important manner of SFS is that once a gene is selected it cannot be removed from the previously selected subset. The process is terminated, after a pre-determined signature size s has been reached or the misclassification rate returned by F is sufficiently small.

Particle swarm optimization (PSO), proposed by Kennedy and Eberhart (1995), simulates the social behaviour of natural swarms such as a flock of birds and a school of fish. In PSO, a population, called a swarm, consists of a set of m particles. At time step t each particle P
                        
                           i
                        , 1â‰¤
                        i
                        â‰¤
                        m, has a position and a velocity in an n-dimensional search space, denoted by 
                           
                              X
                              i
                              t
                           
                           =
                           (
                           
                              x
                              
                                 i
                                 1
                              
                              t
                           
                           ,
                           
                              x
                              
                                 i
                                 2
                              
                              t
                           
                           ,
                           â€¦
                           ,
                           
                              x
                              in
                              t
                           
                           )
                         and 
                           
                              V
                              i
                              t
                           
                           =
                           (
                           
                              v
                              
                                 i
                                 1
                              
                              t
                           
                           ,
                           
                              v
                              
                                 i
                                 2
                              
                              t
                           
                           ,
                           â€¦
                           ,
                           
                              v
                              in
                              t
                           
                           )
                        , respectively. A potential solution is represented by the position of a particle. Each particle P
                        
                           i
                         moves to the next position according to its current position and its velocity, where first the velocity at time step t
                        +1 is updated according to
                           
                              (1)
                              
                                 
                                    v
                                    id
                                    
                                       t
                                       +
                                       1
                                    
                                 
                                 =
                                 
                                    wv
                                    id
                                    t
                                 
                                 +
                                 Î±
                                 
                                    r
                                    1
                                 
                                 (
                                 
                                    pbest
                                    id
                                 
                                 âˆ’
                                 
                                    x
                                    id
                                    t
                                 
                                 )
                                 +
                                 Î²
                                 
                                    r
                                    2
                                 
                                 (
                                 
                                    gbest
                                    d
                                 
                                 âˆ’
                                 
                                    x
                                    id
                                    t
                                 
                                 )
                                 ,
                              
                           
                        where 1â‰¤
                        d
                        â‰¤
                        n. Here, 
                           w
                         is the inertia weight, Î± and Î² are the learning factors (both typically set to 1 or 2), and r
                        1 and r
                        2 are uniformly distributed random numbers between 0 and 1. While the local best position pbest
                           i
                         is the individual best solution of particle P
                        
                           i
                         for a given fitness function F found so far until step t, the global best position gbest is the best recorded solution found so far with respect to F by all m particles. The position of P
                        
                           i
                         is then updated by
                           
                              (2)
                              
                                 
                                    x
                                    id
                                    
                                       t
                                       +
                                       1
                                    
                                 
                                 =
                                 
                                    x
                                    id
                                    t
                                 
                                 +
                                 
                                    v
                                    id
                                    
                                       t
                                       +
                                       1
                                    
                                 
                                 ,
                                 d
                                 =
                                 1
                                 ,
                                 2
                                 ,
                                 â€¦
                                 ,
                                 n
                                 .
                              
                           
                        
                     

PSO is executed iteratively in three phases. The first phase is to evaluate the solutions according to F of all m particles. During the second phase the solutions are compared individually for each P
                        
                           i
                         and globally for all m particles, and the best solutions are recorded. At the last phase, the velocities and positions are updated according to Eqs. (1) and (2). The phases are iterated until the maximal number of iterations is exhausted or a stopping criterion applies.

PSO was originally developed for solving real-valued optimization problems, and later Kennedy and Eberhart introduced the binary PSO approach for discrete problems (Kennedy and Eberhart, 1997; Kennedy et al., 2001). The standard binary PSO for feature selection can briefly be described as follows: The position of each particle is encoded by a binary string, that is, given m particles and n features, x
                        
                           id
                        
                        âˆˆ{0, 1}, i
                        =1, 2, â€¦, m and d
                        =1, 2, â€¦, n. The binary value denotes that the dth feature is selected or not selected, respectively. By using a classification algorithm, a generalisation error rate is computed based upon a fitness function F that measures the quality of a solution. The velocity of particle P
                        
                           i
                         is then calculated as before by Eq. (1). Finally, the new position of particle P
                        
                           i
                         is determined by its velocity component as shown in Eq. (3), which determines the binary value.


                        
                           
                              (3)
                              
                                 
                                    x
                                    id
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   1
                                                
                                                
                                                   if
                                                   
                                                   rand
                                                   <
                                                   S
                                                   (
                                                   
                                                      v
                                                      id
                                                   
                                                   )
                                                   ,
                                                
                                             
                                             
                                                
                                                   0
                                                
                                                
                                                   
                                                      otherwise
                                                      ;
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (4)
                              
                                 S
                                 (
                                 
                                    v
                                    id
                                 
                                 )
                                 =
                                 
                                    1
                                    
                                       1
                                       +
                                       
                                          e
                                          
                                             âˆ’
                                             
                                                v
                                                id
                                             
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        where rand is a uniformly distributed random number from [0, 1] and 
                           S
                           (
                           
                              v
                              id
                           
                           )
                         is the sigmoid function as shown in Eq. (4). Thus, the velocity is now no longer the rate of change of a particle's position, but the probability of the binary value assignment. Consequently, the lower the velocity, the more likely the binary value is equal to 0; the higher the velocity, the more likely the dth feature is selected.

In the present application, the dimensionality of particles is n
                        =30 (= number of top-ranked genes). Each particle position x
                        
                           id
                         is equal either to 0 or 1, i
                        =1, â€¦, m and d
                        =1, â€¦, n, indicating whether or not the dth gene is part of the solution (putative signature). In the unconstraint approach, the initial distribution of 0 and 1 in each of the m particles is chosen uniformly at random, ensuring though that all members of the population are different. The objective function F is defined by the misclassification rate, as explained in Section 3 for the k-NN classifier. In the constraint approach, the signature size is fixed to s, and therefore each particle initially and subsequently has exactly s positions x
                        
                           id
                         equal to 1, which might require modifications after Eq. (3) is applied: If 
                           
                              x
                              id
                              
                                 t
                                 +
                                 1
                              
                           
                         has less than s positions equal to one, then the difference to s is randomly switched to 1, if it has more than s positions equal to one, then the corresponding difference is randomly switched to 0.

Artificial bee colony algorithms (ABC), a swarm-intelligence based optimization method proposed by D. Karaboga in 2005 for solving multivariate numerical problems, models the foraging behaviour of honey bees (Karaboga, 2005). The basic model of artificial bee colony algorithms is built around the so-called employed, onlooker and scout bees. Scout bees search new sources randomly, employed bees scrutinise the neighbourhood of food sources, and onlooker bees are waiting within the hive for information provided by employed bees and are searching for new food sources based on the nectar amount of known food sources (Karaboga and Ozturk, 2009).

In the ABC model, a food source denotes a possible solution of an optimization problem and the nectar amount of a food source represents the quality of a solution. Each food source is associated with an employed bee in the colony. Thus, the number of food sources is equal to the number of employed bees, which is also equal to the number of onlooker bees (Karaboga and Ozturk, 2011).

Due to the discrete character of the feature selection problem, the original ABC is per se not suited to searching in such spaces. A simple binary version can be modelled by using rounded positions to be mapped onto [0, 1] when generating and updating solutions. The (initial) solutions are generated by using Eq. (5) where the ith food source is represented by X
                        
                           i
                        
                        ={x
                        
                           i1, x
                        
                           i2, â€¦, x
                        
                           in
                        } in the population and U
                        âˆˆ[0, 1] is a uniformly generated value.


                        
                           
                              (5)
                              
                                 
                                    x
                                    ij
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   0
                                                
                                                
                                                   if
                                                   
                                                   U
                                                   â‰¤
                                                   0.5
                                                   ,
                                                
                                             
                                             
                                                
                                                   1
                                                
                                                
                                                   if
                                                   
                                                   U
                                                   >
                                                   0.5
                                                   .
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Each food source X
                        
                           i
                         is associated with a single employed bee, and a new food source is produced depending on the position of the source and a randomly chosen neighbouring positions according to
                           
                              (6)
                              
                                 
                                    x
                                    ij
                                    â€²
                                 
                                 =
                                 
                                    x
                                    ij
                                 
                                 +
                                 
                                    Ï•
                                    ij
                                 
                                 (
                                 
                                    x
                                    ij
                                 
                                 âˆ’
                                 
                                    x
                                    kj
                                 
                                 )
                                 ,
                              
                           
                        where Ï•
                        
                           ij
                         is a random number between âˆ’1 and 1, k
                        âˆˆ{1, 2, â€¦, m} and j
                        =1, 2, â€¦, n, with m being the number of food sources and n denotes the dimension of the feature space. The X
                        
                           k
                         are selected randomly from the population for a small number of k. If the nectar quality, expressed by the fitness function F, of the new candidate food source 
                           
                              X
                              i
                              â€²
                           
                         is better than the known source, i.e. 
                        
                           F
                           (
                           
                              X
                              i
                              â€²
                           
                           )
                           >
                           F
                           (
                           
                              X
                              i
                           
                           )
                        , then the candidate 
                           
                              X
                              i
                              â€²
                           
                         is kept and X
                        
                           i
                         is deleted from the population of solutions. As before, the new 
                           
                              X
                              i
                              â€²
                           
                         is then mapped onto [0, 1].

In the subsequent phase, onlooker bees assign a normalised scoring function p
                        
                           i
                         to food sources according to
                           
                              (7)
                              
                                 
                                    p
                                    i
                                 
                                 =
                                 
                                    
                                       
                                          F
                                          i
                                       
                                    
                                    
                                       
                                          âˆ‘
                                          j
                                          m
                                       
                                       
                                          F
                                          j
                                       
                                    
                                 
                                 ,
                              
                           
                        where F is the fitness value of the solution represented by a food source.

If the quality of a food source cannot be improved during a predetermined number of iterations (limit parameter), the employed bee associated with the food source will become a scout bee that replaces the abandoned source with a new randomly produced food source according to Eq. (5). When the stopping criterion applies, a (the) solution (food source) with the best fitness value F(X
                        
                           i
                        ) is finally selected.

@&#RESULTS AND DISCUSSION@&#

The central nervous system (CNS) dataset (Pomeroy et al., 2002), one of the microarray-based cancer classification benchmarks, is used to demonstrate the effectiveness of the proposed approach for evaluating near-optimal gene signatures. The CNS embryonal tumour experiment is derived from 60 patients, of which 21 are medulloblastoma survivors and 39 are treatment failures. There are 7129 genes to be investigated. First of all, we carry out the acquisition of candidate genes. In this experimental study, the Student's t-test is applied to the CNS expression matrix in order to filter out an ordered set of statistically significant genes (p-values <0.01), ranked by their t-statistics. Out of 112 filtered genes, n
                     =30 at the top of the gene list are selected to be evaluated for optimal signatures. In order to be able to execute a performance assessment, a classification algorithm is required in the process of signature evaluation. For this purpose, we employ the k-Nearest Neighbours classifier (k-NN) with leave-one-out cross validation, along with the generalisation error rate (Err) and various search methods.

The value of Err is calculated in the usual way for k-NN classifiers: Assuming â„“ (= 112 or 30) genes and h (= 60) samples, where the samples are divided into two classes (here, of size 21 and 39, respectively), we are given a matrix M of h rows and â„“+1 columns, where column No. â„“+1 indicates the class to which each sample belongs. The entries into the h
                     Ã—(â„“+1) matrix â€“ except column â„“+1 â€“ are the expression values E
                     
                        ij
                     , i
                     =1, â€¦, h and j
                     =1, â€¦, â„“, from microarray data. A given signature S of s genes creates a submatrix M(S) defined by s gene columns plus the class label column (h
                     Ã—(s
                     +1). Then the usual k-NN procedure (k is odd number) is applied: For a given sample R at row r we consider the values 
                        (
                        
                           E
                           
                              
                                 ru
                                 1
                              
                           
                        
                        ,
                        
                           E
                           
                              
                                 ru
                                 2
                              
                           
                        
                        ,
                        â€¦
                        ,
                        
                           E
                           
                              
                                 ru
                                 s
                              
                           
                        
                        )
                     , where (u
                     1, â€¦, u
                     
                        s
                     ) represent the numbers of the columns of genes from signature S. Row number r is deleted from M(S), and for each of the remaining h
                     âˆ’1 samples, the Euclidean distance to R is calculated within M(S). The k samples with the smallest distance are selected, and therefore k class labels are available. Over the k class labels, a majority decision is made, and the resulting label is assigned to R. If the label is different from the label of R in M, then R has been misclassified. Thus, we set Err
                     =# misclassifications/h.

In terms of search strategies, sequential forward selection (SFS), one of the typical deterministic wrappers, is referred to as the baseline, while particle swarm optimization (PSO) and the artificial bee colony (ABC) concept represent randomised wrappers. Since both PSO and ABC are swarm intelligence-based methods, several parameter settings need to be specified and adjusted to the same scale for a sound analysis of their strengths and weaknesses. In the present empirical study, the population size (the amount of particles â€“ PSO â€“ and the number of food sources â€“ ABC) is m
                     =30 (= n) and the maximum number of iterations is 100. Furthermore, the limit value required for ABC is chosen equal to 100, and the specific PSO values are set by 
                        w
                        =
                        0.5
                      and Î±
                     =
                     Î²
                     =1.

In addition to the computational aspect, visualisation is also an important issue when designing a user-friendly tool. We propose a fairly straightforward two-dimensional plot, called bead-chain plot, for the analysis and assessment of discriminative gene signatures. In Fig. 1
                     , the misclassification rate is plotted against the size of a signature for a given wrapper. For any point in the plot, different sizes of beads are used to represent the number of gene signatures, each of them having the same performance (= error rate Err) and the same number of differentially expressed genes (= size s of signature). Given the evaluation profile from three different selection methods, there are three bead-chains in the plot to disclose the identity of the preferred signature. We see the bead-chain plot as a useful tool for an efficient interpretation of evaluation profiles supporting the identification of molecular signatures.


                     Fig. 1 shows the evaluation profile of the top 30 genes selected according to statistical significance from the CNS embryonal tumour gene expression profiles. There are three bead-chains in the plot, representing the three selection methods SFS (black dash line), PSO (red solid line), and ABC (blue dot line). Since two randomised approaches (constraint and unconstraint) are carried out in this experimental study for swarm wrappers, the star-chain denotes the results of the unconstraint approach, while the bead-chain presents the constraint approach. Similar to the representation of beads, the bigger the star the larger is the number of signatures. Unsurprisingly, SFS poorly performs with regard to optimal gene signatures, and the method identifies as best solutions two gene sets (of sizes 13 and 14) at the level Err
                     =0.1167. On the other hand, both PSO and ABC are able to find signatures having a much better classification performance as SFS, and, moreover, both methods identify multiple signatures at the same Err level. Specifically for PSO and ABC, we observe that PSO exhibits a slightly higher fluctuation rate in misclassifications than ABC. PSO identifies three signatures at the level of Err
                     =0.0833, where one signature is of size 8 and two are of size 9. Although ABC does not find any signatures reaching the level of Err
                     =0.0833, its performance is coherent between Err
                     =0.1 and Err
                     =0.1167 with the signature size ranging from 4 up until 24. In particular, ABC displays a very good performance at Err
                     =0.1 for a signature size 4, 6, 7, 13, and 14, while PSO is able to achieve the same Err value only for a signature size of 5 and 12. In general, from the viewpoint of multiple signature discoveries, one can easily observe that PSO overwhelmingly dominates the profile â€“ the red beads are far bigger than the blue ones at the same Err level and for the same signature size. The information can be advantageous when deciding about signatures based on a priori knowledge and experiences, in particular, if the best possible performance rate is not the deciding issue. For instance, the error rates 0.0833 and 0.1167 differ only by two misclassified samples. Therefore, it could be of interest to look at varying combinations of genes having almost best performance before experimental validations are executed.

In terms of the unconstraint approach, PSO and ABC are both getting stuck around Err
                     =0.12 with nine (PSO: 8â€“15 and 17) and four (ABC: 12â€“13 and 15â€“16) different signature sizes. The red stars again are larger than the blue ones, which means that unconstraint PSO identifies more signatures than unconstraint ABC does. A possible explanation is that for PSO each particle moves relying on the local best and the global best positions, and this may decrease its diversity. When comparing the results of the unconstraint approach to the constraint one, we notice that Err
                     =0.1167 is above the best solutions found for signature sizes of 12, 13, and 14 (the best Err value for the three signature sizes would be 0.1). Finally, we note that the bead-chain profiles indicate that the signatures discovered by swarm wrappers return the best Err values for signature sizes of 25â€“30.

The details of all signatures explored in our empirical study are presented in Table S1 until Table S5 in the Appendix. Table S1 summarises the full information about the thirty differentially expressed genes filtered out by t-statistics, which comprises of the rank of the genes (Rank), their position in the original CNS dataset of 7129 genes (Feature), and the gene name (Name). Table S2 displays the detailed results for three bead-chains (SFS, PSO, and ABC) presented in Fig. 1. Gene signatures found by SFS and the unconstraint swarm wrappers are presented in Table S3, while the complete signature information obtained by constraint ABC and constraint PSO are shown in Table S4 and Table S5, respectively.

Since our main objective is to demonstrate the advantages of having available multiple gene signatures with near-optimal performance rates along with varying signature sizes, we analyse in more detail selected levels of the misclassification rate Err. The results are summarised in Table 1
                      (see also Fig. 1), where we focus on the three error rates 0.1167, 0.1, and 0.0833, along with the number of misclassified samples equal to 7, 6, and 5, respectively. In Table 1, the row â€˜Sizeâ€™ indicates the signature cardinality, the row â€˜Numberâ€™ records the number of signatures discovered by all the swarm wrappers (constraint PSO/ABC and unconstraint PSO/ABC). A signature size labelled by â€˜*â€™ denotes that the misclassification rate (first two columns) presented in the same row is the best Err value obtained in our computational experiments for the particular signature size. For the experimental study we carried out, it is evident that swarm wrappers are able to identify near-optimal gene signatures at similar levels of discrimination over varying cardinality. For 7 misclassifications, the swarm wrappers return 168 best signatures for the *-labelled signature sizes, where most of them are reported by constraint PSO. For 6 misclassified instances, 11 best signatures are discovered across the signature sizes of 4â€“7 and 12â€“14 (sum of row â€˜Numberâ€™ for these columns), and most of the signatures are found by constraint ABC. There are 3 gene sets for the signature size of 8 and 9 that misclassify only 5 instances out of 60, where the underlying method is PSO.

Due to the relatively large number of signatures discovered for Err
                     =0.1167, we further analyse this crowded plateau by using a bar chart for zooming into this level as shown in Fig. 2
                     . The basic types of bars in the chart are black, red, and blue and represent SFS, PSO and ABC, respectively. While the filled red/blue bars indicate the constraint approach, the unfilled red/blue bars denote the unconstraint approach. As in Table 1, the *-labels shown in the x axis denote signature sizes where Err
                     =0.1167 is the best value for the particular signature size. Interestingly, SFS has identified two sets of best signatures for the size of 13 and 14 at Err
                     =0.1167. However, 0.1167 is not the best value obtained by all methods for signature size 13 and 14. Within the framework of our experimental study, SFS is not able to find any best gene signature for any of the signature sizes we analysed. When constraint ABC is compared to constraint PSO, then PSO can identify by far the most number of signatures, ranging from four to eighteen gene sets. Constraint ABC returns from one to three signatures for a specific size; the method does find the optimal signatures for the size of 23 and 24, where PSO does not return any signature. Although the unconstraint approach of swarm search could not find any best solutions, it is still of interest for identifying best signatures for a user-preferred signature size.

Diseases like cancer are correlating with various oncogenic pathways, and the initiation and progression are usually driven by a cluster of factors and several molecular characteristics. To evaluate putative gene signatures for further experimental validation and clinical application, it is essential to understand whether the genetic and molecular information are involved in a certain biological mechanism and therefore could provide biomedical meaning. Within the framework of our experimental study, we have conducted an analysis of Gene Ontology (Ashburner et al., 2000; The Gene Ontology Consortium, 2008) in connection with the output of the bead-chain plot, where 14 near-optimal signatures gained from the analysis of Table 1 and 1 from SFS wrapper were selected. Each of the selected signatures has a similar discriminative power (0.0833, 0.1, 0.1167), and each original probeset within a signature is mapped onto a unique Entrez gene symbol. GO analysis is then applied to these near-optimal signatures based upon the mapped genes, and statistically functional enrichment is subsequently retrieved by a hypergeometric test and Benjaminiâ€“Hochberg adjusted p-values at the level of 0.05. The results are shown in Table 2
                     , and the components of the verified gene signatures are summarised in Table S6. As one can see from Table 2, there is no statistical enrichment at all for signatures H and I and only a small one for signature J, where all the three signatures exhibit the best discriminative power of 0.0833. In terms of parsimony, it is apparent that significant molecular association is not present in parsimonious models such as signatures A, B, or E, no matter how good its performance is. On the other hand, the discriminative ability of signature K is not the best compared to other signatures, but it is actively involved in many biological processes and cellular components. In particular, the entire signature K is statistically enriched in GO:0044699, the entity of a single organism process, and the majority of the genes represented by signature K are also represented in the response to stimulus, cell periphery, cellular component organisation, biogenesis, and cell assembly. For the best signature found by SFS (signature O), it relates to 10 biological processes given 11 mapped genes even though it has the same misclassification rate (0.1167) as that of signature K and could have three more genes. Consequently, it seems to be justified to assume that the 10-gene signature K can be regarded as a suitable choice for in vitro validation. Thus, our experimental study demonstrates that aiming at the discovery of unique, parsimonious, and best predictive signatures might not be always the ideal strategy in practice. Similarly, our results underscore the advantages of including as much biological information related to molecular changes as possible when deciding about gene signatures.

The advent of high-throughput technologies creates a whole new range of clinical application. Efficient and effective biological validation is therefore vital when using biotechnology in clinical practice. While gene selection attracts much scientific attention, the impact of how an evaluation strategy for gene signature selection affects the suitability of the signature is rarely discussed. In the present study, we try to address the problem of gene signature evaluation by methods other than differentially expressed gene selection. Given a moderate pool of candidate genes selected by state-of-the-art statistical models or dimension reduction methods, having at hand a number of putative gene signatures can avoid the in vitro validation of each gene that is present in (reduced) microarray data sets. In terms of feature selection, a randomised wrapper approach seems to be particularly suitable for signature evaluation. In the present paper, we have demonstrated how the new concept of a bead-chain plot can efficiently represent the results produced by swarm intelligence techniques. We note that any appropriate meta-heuristic search method can be employed within the framework of our proposed strategy.

In present research, uniqueness and parsimony of gene signatures are particularly targeted, with the aim of finding a minimal subset of genes with maximal discrimination. However, our experimental study provides evidence for the importance of taking into account the highly complex nature of transcriptomic measurements of organisms regarding a certain disease phenotype classification. Genes are usually highly co-regulated in the transcriptome, and therefore, we think, the composition of a signature is at least equally important as its cardinal number. Consequently, we propose to focus on near-optimal gene signatures, as demonstrated in the present paper for the CNS embryonic tumour expression dataset. Our approach employs local search-based methods based upon swarm intelligence, combined with a machine learning algorithm. Our empirical results show that the proposed swarm wrappers have successfully explored multiple gene signatures over varying cardinality with similar discriminative power. The pool of explored gene sets can then be further investigated with the commonly co-regulated genes and the information about pathways of interest, which eventually leads to candidate signatures.

Only a small portion of statistically significant genes are investigated in the present empirical study. In future research, we intend to expand our analysis to gene lists that are typically selected in clinical trials. Furthermore, we plan to optimise and to extend our approach with regard to local search-based methods utilised in signature generation, to varying parameter settings, model selection, and an to in-depth biological and statistical analysis. Based on the proposed prototype, we are planning to design a bioinformatics tool for prognostic molecular signatures.

@&#ACKNOWLEDGEMENTS@&#

Celal Ã–zturk would like to acknowledge the financial support provided by the Scientific and Technical Research Council of Turkey. The authors also appreciate the comments received from the reviewers.

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.compbiolchem.2015.02.004.

The following are the supplementary data to this article: 
                        
                           
                        
                     
                  

@&#REFERENCES@&#

