@&#MAIN-TITLE@&#Multiobjective GRASP with Path Relinking

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We classify previous GRASP and Path Relink. in multiobjective optimization.


                        
                        
                           
                           We propose the different ways to apply GRASP and PR to multiobjective optimization.


                        
                        
                           
                           We propose and test 30 different methods on 2 NP-hard problems.


                        
                        
                           
                           Some of our methods outperform the best previous methods for these 2 problems.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Multiobjective optimization

Biobjective optimization

GRASP

Path Relinking

@&#ABSTRACT@&#


               
               
                  In this paper we review and propose different adaptations of the GRASP metaheuristic to solve multiobjective combinatorial optimization problems. In particular, we describe several alternatives to specialize the construction and improvement components of GRASP when two or more objectives are considered. GRASP has been successfully coupled with Path Relinking for single-objective optimization. Moreover, we propose different hybridizations of GRASP and Path Relinking for multiobjective optimization. We apply the proposed GRASP with Path Relinking variants to two combinatorial optimization problems, the biobjective orienteering problem and the biobjective path dissimilarity problem. We report on empirical tests with 70 instances and 30 algorithms, that show that the proposed heuristics are competitive with the state-of-the-art methods for these problems.
               
            

@&#INTRODUCTION@&#

The GRASP metaheuristic was developed in the late 1980s (Feo & Resende, 1989, 1995). The acronym was coined in Feo, Resende, and Smith (1994). Since then, GRASP has experienced continued development and has been applied in a wide range of problem areas (Duarte, Martí, Resende, & Silva, 2014; Sevaux, Rossi, Soto, Duarte, & Martí, 2014). We refer the reader to Resende and Ribeiro (2003), Resende and Ribeiro (2010) for recent surveys of this metaheuristic. In short, each GRASP iteration consists in constructing a trial solution with some greedy randomized procedure and then applying local search from the constructed solution. This two-phase process is repeated until some stopping condition is satisfied. The best local optimum found over all local searches is returned as the solution of the heuristic.

Path Relinking (PR) is an approach that generates new solutions by exploring trajectories that connect high-quality solutions (Glover & Laguna, 1997). Laguna and Martí (1999) adapted PR in the context of GRASP as a form of intensification, which consists in finding a path between a solution found with GRASP and a chosen elite solution. See Campos, Martí, Duarte, and Sanchez-Oro (2013), Duarte, Martí, Resende, and Silva (2011), Resende, Martí, Gallego, and Duarte (2010), for successful hybridizations of GRASP with PR. Resende and Ribeiro (2003) and Ribeiro and Resende (2012) present numerous examples of GRASP with PR.

The algorithm in Fig. 1
                      shows pseudo-code for a generic GRASP with Path Relinking for minimization. The greedy randomized construction seeks to produce a diverse set of good-quality starting solutions from which to start the local search phase. Let x be the partial solution under construction in a given iteration and let C be the candidate set with all the remaining elements that can be added to x. The GRASP construction uses a greedy function 
                        
                           g
                           (
                           c
                           )
                        
                      to measure the contribution of each candidate element 
                        
                           c
                           ∈
                           C
                        
                      to the partial solution x. A restricted candidate list 
                        
                           RCL
                        
                      is the subset of candidate elements from C with good evaluations according to g. In particular, if 
                        
                           
                              
                                 g
                              
                              
                                 min
                              
                           
                        
                      and 
                        
                           
                              
                                 g
                              
                              
                                 max
                              
                           
                        
                      are the minimum and maximum evaluations of g in C respectively, then 
                        
                           RCL
                           =
                           {
                           c
                           ∈
                           C
                           
                           |
                           
                           g
                           (
                           c
                           )
                           ⩽
                           
                              
                                 g
                              
                              
                                 min
                              
                           
                           +
                           α
                           (
                           
                              
                                 g
                              
                              
                                 max
                              
                           
                           -
                           
                              
                                 g
                              
                              
                                 min
                              
                           
                           )
                        
                     }, where 
                        
                           α
                        
                      is a number in 
                        
                           [
                           0
                           ,
                           1
                           ]
                        
                     . At each step, the method randomly selects an element 
                        
                           
                              
                                 c
                              
                              
                                 ∗
                              
                           
                        
                      from the restricted candidate list and adds this element to the partial solution. The construction is repeated in the inner while loop (steps 4 to 10) until there are no further candidates. If 
                        
                           C
                           =
                           ∅
                        
                      and x is infeasible, then a repair procedure needs to be applied to make x feasible (steps 12 to 14). Once a feasible solution x is on hand, a local search improvement is applied. The resulting solution is a local minimum. If it improves the best solution found so far (step 16), we update it (step 17), and store it in the elite set 
                        
                           ES
                        
                      (step 18). The GRASP algorithm terminates when a stopping criterion is met (typically a maximum number of iterations, time limit, or a target solution quality). At this stage, we apply the PR method to all pairs of solutions 
                        
                           (
                           x
                           ,
                           y
                           )
                        
                      stored in 
                        
                           ES
                        
                     . At each iteration, 
                        
                           PR
                           (
                           x
                           ,
                           y
                           )
                        
                      starts with the first solution x, and gradually transforms it into the second solution y, by swapping out elements in x and replacing them with elements in y. The elements in both solutions x and y remain in the intermediate solutions, 
                        
                           p
                           (
                           x
                           ,
                           y
                           )
                        
                     , generated in the path between them. If the best intermediate solution, 
                        
                           
                              
                                 p
                              
                              
                                 ∗
                              
                           
                           (
                           x
                           ,
                           y
                           )
                        
                     , improves the incumbent solution 
                        
                           
                              
                                 x
                              
                              
                                 ∗
                              
                           
                        
                     , it is updated (step 24). Some advanced designs also include the exploration of the path from y to x. Path Relinking finishes when all the pairs in the elite set have been explored. The best overall solution 
                        
                           
                              
                                 x
                              
                              
                                 ∗
                              
                           
                        
                      is returned as the output of the heuristic.

In this paper, we deal with multiobjective optimization problems, in which, without loss of generality, we want to minimize k objective functions: 
                        
                           
                              
                                 f
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 f
                              
                              
                                 2
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 f
                              
                              
                                 k
                              
                           
                        
                     . Specifically, we want to determine the set of efficient points (usually called the efficient Pareto front). A point or solution 
                        
                           
                              
                                 x
                              
                              
                                 ∗
                              
                           
                        
                      is said to be efficient if there is no other solution x such that 
                        
                           
                              
                                 f
                              
                              
                                 i
                              
                           
                           (
                           x
                           )
                           ⩽
                           
                              
                                 f
                              
                              
                                 i
                              
                           
                           (
                           
                              
                                 x
                              
                              
                                 ∗
                              
                           
                           )
                        
                      for all 
                        
                           i
                           =
                           1
                           ,
                           …
                           ,
                           k
                        
                      and 
                        
                           
                              
                                 f
                              
                              
                                 j
                              
                           
                           (
                           x
                           )
                           <
                           
                              
                                 f
                              
                              
                                 j
                              
                           
                           (
                           
                              
                                 x
                              
                              
                                 ∗
                              
                           
                           )
                        
                      for at least one 
                        
                           j
                           ∈
                           {
                           1
                           ,
                           …
                           ,
                           k
                           }
                        
                     . Essentially, efficiency means that a solution to a multiobjective function is such that no single objective can be improved without deteriorating another objective. Since we apply heuristic procedures, we usually obtain an approximation to the set of efficient points. In this context, we say that a solution 
                        
                           
                              
                                 x
                              
                              
                                 ∗
                              
                           
                        
                      
                     dominates another solution x if the former is not worse than the latter in all the objectives, and it is better in at least one objective. Similarly, we say that 
                        
                           
                              
                                 x
                              
                              
                                 ∗
                              
                           
                        
                      
                     weakly dominates x if it is not worse than x in all the objectives.

Many books and key references have appeared in the last years with the main concepts, applications, and metrics of assessment in this field. We refer the reader to Coello, Veldhuizen, and Lamont (2002), Knowles (2002), Zitzler, Thiele, Laumanns, Fonseca, and Grunert da Fonseca (2003), and Gandibleux, Sevaux, Sorensen, and T’kindt (2004) for some tutorials on multiobjective optimization.

In this paper we propose different adaptations of the single-objective GRASP outlined in Fig. 1 to the multiobjective case. Specifically, we consider the extension of the construction and improvement phases. Moreover, we also include a post-processing phase based on Path Relinking for multiobjective optimization.

As far as we know, there are just a few previous works on multiobjective GRASP (around 10 papers) and they do not follow a common terminology or framework. On the contrary, each one adapts GRASP in a different way. The main motivation of this paper is to standardize and classify the different ways in which GRASP can be adapted to a multiobjective combinatorial optimization problem. The description and classification of these different designs in general terms, i.e., as a metaheuristic, is our main objective. Additionally, we consider their empirical comparison on two problems, the orienteering and the path dissimilarity, to test if any of these designs is consistently better than the others.

In line with our objective, a noteworthy previous work is due to Paquete and Stützle (2007), who distinguish between two types of stochastic local search algorithms for multiobjective optimization. The first type is based on the component-wise ordering of the objective function vector, and methods in this category basically accept a neighbor solution if it is non-dominated. The second type is the classical approach of aggregating the objectives in a single parameterized objective. We will review and extend this classification when we describe in Section 3 the possible adaptations of the GRASP local search to a multiobjective problem.

In this paper, we first propose in Section 2 several designs of the GRASP construction phase for multiobjective optimization. Similarly, Sections 3 and 4, respectively, describe the local search phase of GRASP and the Path Relinking post-processing phase in the context of multiobjective optimization. Once our general proposals and classifications have been introduced, we review in Section 5 the previous work according to them. Section 6 describes the two problems used for testing, the path dissimilarity problem (PDP) and the bi-orienteering problem (BOP), and our adaptations of the methods proposed in the previous sections to these two problems, totaling 26 procedures. Computational experiments with PDP and BOP to test these 26 methods and the four previous methods identified to be the best for these two problems, are reported in Section 7. Concluding remarks are made in Section 8.

In single-objective GRASP, each construction is guided by a greedy function 
                        
                           g
                           (
                           c
                           )
                        
                      which measures the contribution of each candidate element 
                        
                           c
                           ∈
                           C
                        
                      to the partial solution x under construction. In multiobjective GRASP, we have k greedy functions, 
                        
                           
                              
                                 g
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 g
                              
                              
                                 2
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 g
                              
                              
                                 k
                              
                           
                        
                     , where 
                        
                           
                              
                                 g
                              
                              
                                 i
                              
                           
                           (
                           c
                           )
                        
                      evaluates candidate element c with respect to objective 
                        
                           
                              
                                 f
                              
                              
                                 i
                              
                           
                        
                     . We distinguish two types of constructions which we call pure and combined.

In pure construction a single objective is considered during a single construction, while in combined construction different objectives guide a single construction. Within the pure construction category we differentiate between two types of methods: those in which the objective to be considered in a construction is randomly selected (pure-random) and those in which the objective is selected in an ordered fashion (pure-ordered).

In pure-random construction, we randomly select a greedy function 
                        
                           
                              
                                 g
                              
                              
                                 i
                              
                           
                        
                      for each construction. In each step of a construction we evaluate 
                        
                           
                              
                                 g
                              
                              
                                 i
                              
                           
                           (
                           c
                           )
                        
                      for each candidate element 
                        
                           c
                           ∈
                           C
                        
                      to compute the restricted candidate list 
                        
                           RCL
                        
                     . It should be noted that we only select a greedy function once in each construction and use it in all the steps of the construction. In other words, each entire construction is guided by a single objective function (the one associated with the greedy function selected).

In pure-ordered construction, each construction is guided by a different objective, selected one at a time in an ordered fashion. Specifically, in the first construction we evaluate the candidate elements with 
                        
                           
                              
                                 g
                              
                              
                                 1
                              
                           
                        
                     , in the second construction with 
                        
                           
                              
                                 g
                              
                              
                                 2
                              
                           
                        
                     , and so on, until we reach the 
                        
                           k
                           +
                           1
                        
                     -th construction, in which we resort again to 
                        
                           
                              
                                 g
                              
                              
                                 1
                              
                           
                        
                     . In short, we follow the order of the objectives across different constructions. In this way, it is expected that each construction will produce a solution of good quality with respect to the objective that is evaluated.

The combined construction considers more than one objective in each construction. We can distinguish between two types of methods. We call sequential those in which each construction step is guided by a different objective, and weighted those in which each step is guided by a weighted function of the evaluations.

In the sequential combined methods, a greedy function is selected at each step of a given construction. Let x be the partial solution under construction in a given iteration, and let C be the candidate set with all the remaining elements that can be added to x. We evaluate the elements in C with a function 
                        
                           
                              
                                 g
                              
                              
                                 j
                              
                           
                        
                     , for some 
                        
                           j
                           =
                           1
                           ,
                           …
                           ,
                           k
                        
                     , thus computing 
                        
                           RCL
                           =
                           {
                           c
                           ∈
                           C
                           
                           |
                           
                           
                              
                                 g
                              
                              
                                 j
                              
                           
                           (
                           c
                           )
                           ⩽
                           
                              
                                 g
                              
                              
                                 j
                              
                              
                                 min
                              
                           
                           +
                           α
                           (
                           
                              
                                 g
                              
                              
                                 j
                              
                              
                                 max
                              
                           
                           -
                           
                              
                                 g
                              
                              
                                 j
                              
                              
                                 min
                              
                           
                           )
                        
                     }, where 
                        
                           α
                        
                      is a number in 
                        
                           [
                           0
                           ,
                           1
                           ]
                        
                      and 
                        
                           
                              
                                 g
                              
                              
                                 j
                              
                              
                                 min
                              
                           
                        
                      and 
                        
                           
                              
                                 g
                              
                              
                                 j
                              
                              
                                 max
                              
                           
                        
                      are respectively the minimum and maximum evaluations of 
                        
                           
                              
                                 g
                              
                              
                                 j
                              
                           
                        
                      in C. If the function 
                        
                           
                              
                                 g
                              
                              
                                 j
                              
                           
                        
                      is randomly selected in each iteration we call the combined method random-sequential. Alternatively, when the function 
                        
                           
                              
                                 g
                              
                              
                                 j
                              
                           
                        
                      is selected in order, where we use 
                        
                           
                              
                                 g
                              
                              
                                 1
                              
                           
                        
                      in iteration 1, 
                        
                           
                              
                                 g
                              
                              
                                 2
                              
                           
                        
                      in iteration 2, and so on, we call the combined method ordered-sequential.

Finally, in the weighted combined methods, we consider a weighted combination
                        
                           (1)
                           
                              g
                              (
                              c
                              )
                              =
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       j
                                       =
                                       1
                                    
                                    
                                       k
                                    
                                 
                              
                              
                                 
                                    w
                                 
                                 
                                    j
                                 
                              
                              
                                 
                                    g
                                 
                                 
                                    j
                                 
                              
                              (
                              c
                              )
                           
                        
                     of the evaluation functions in step i of the construction, where 
                        
                           
                              
                                 w
                              
                              
                                 j
                              
                           
                        
                      is the weight of the evaluation function 
                        
                           
                              
                                 g
                              
                              
                                 j
                              
                           
                        
                     . We then compute the restricted candidate list, 
                        
                           RCL
                        
                     , with this weighted function. We can either keep the same weights across different constructions or change them in each construction step. Note that the evaluations (objectives) can have magnitudes that vary significantly and in this case the weights help us scale them into similar (and comparable) magnitudes. Moreover, in multiobjective optimization, some objectives can be minimized while others maximized and therefore the weights can take positive and negative values to reflect this fact.


                     Fig. 2
                      summarizes this classification which, as will be shown, also applies to local search and Path Relinking methods. In short, we have identified four different schemes: pure-random, pure-ordered, sequential combined, and weighted combined. In the case of constructive methods, the sequential combined is broken down into two sub-categories: random-sequential combined and ordered-sequential combined.

Local search, also known as neighborhood search, proceeds iteratively from one solution to another until no further improvement is possible. Each solution x has an associated neighborhood 
                        
                           N
                           (
                           x
                           )
                        
                     , and each solution 
                        
                           y
                           ∈
                           N
                           (
                           x
                           )
                        
                      is reached from x by an operation called move.

In the local search we can define the same two main strategies as in the construction methods, pure and combined, according to the way in which we select the objective functions.

If we obtain a solution x with a pure constructive method guided by objective function 
                        
                           
                              
                                 f
                              
                              
                                 i
                              
                           
                        
                     , we attempt to improve it with a local search also guided by the same function. Since x was constructed only considering 
                        
                           
                              
                                 f
                              
                              
                                 i
                              
                           
                        
                      and ignoring the rest of the objectives, we permit the deterioration of these other objectives in the pure local search while improving 
                        
                           
                              
                                 f
                              
                              
                                 i
                              
                           
                        
                     . The method stops when objective 
                        
                           
                              
                                 f
                              
                              
                                 i
                              
                           
                        
                      cannot be further improved.

If we obtain a solution x with a combined constructive method, we distinguish whether it is a sequential or a weighted combined method. In the sequential construction, different objectives are applied. We therefore do not allow the objective functions to deteriorate during the local search phase. At each step of the sequential-combined local search, we consider a different objective function when selecting the best solution in the neighborhood. The method stops when no objective can be further improved (without deteriorating any of the others).

Finally, if we obtain a solution x with a weighted-combined construction method using (1), we consider in the weighted-combined local search, the weighted objective function
                        
                           (2)
                           
                              f
                              (
                              x
                              )
                              =
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       j
                                       =
                                       1
                                    
                                    
                                       k
                                    
                                 
                              
                              
                                 
                                    w
                                 
                                 
                                    j
                                 
                              
                              
                                 
                                    f
                                 
                                 
                                    j
                                 
                              
                              (
                              x
                              )
                              .
                           
                        
                     
                  

In this way, this local search is guided by the same objective function as its associated construction method. We can either keep the same weights across different moves or change them in each one. The local search stops when 
                        
                           f
                           (
                           x
                           )
                        
                      cannot be further improved.

We apply a post-processing phase within the improvement method to certify the local optimality with respect to all the objectives. In particular, before terminating the local search we attempt to improve, one-by-one, each of the objectives without deteriorating any of the others. We select them in order, from 1 to k, performing at each iteration the best associated move with respect to the corresponding objective. This post-processing finishes when no objective can be improved. Note that in the sequential-combined local search this process is not necessary because it already applies it by definition.

Note that an important difference between single-objective local search and multiobjective local search is that in the former we only need to check if the final solution obtained (the local optima) improves upon the best known solution. On the contrary, in multiobjective local search, every solution visited has to be checked for its possible inclusion in the set of non-dominated solutions.

It is worth mentioning the previous work by Paquete and Stützle (2007), who reviewed stochastic local search algorithms and distinguished between two types of methods; those based on the component-wise ordering of the objective function vector, and those aggregating the objectives in a single parameterized objective. In the first category, the authors considered two sub-categories, called respectively direct and indirect use of the component-wise ordering. Among the first approaches are Pareto local search (Paquete & Stützle, 2006) and bi-criteria local search (Angel, Bampis, & Gourvés, 2002). They basically select solutions in the set of non-dominated solutions identified so far, called archive, and examine their neighborhoods in search for a non-dominated solution to be added to the archive. More complex acceptance criteria to add new solutions to the archive are proposed in Knowles and Corne (1999). On the other hand, many population-based local search algorithms rely on a mapping of the objective function value vector of each candidate solution in the archive into a single value, i.e., a rank. These multiobjective evolutionary algorithms consider an archive of bounded size and propose different strategies to rank the solutions. It is interesting to observe that in our classification we put the emphasis in the way that the local search is guided, while these previous studies analyzed and proposed rules to manage the evolution of the set of non-dominated solutions (archive) identified during the local search.

The second category considered in Paquete and Stützle (2007) corresponds to the weighted-combined local search described above. The authors divided the approaches in this class between non-proprietary and proprietary methods. In the former, the local search is embedded into a general framework that mainly says how it is applied, as it is the case of the GRASP metaheuristic here. (The authors reviewed simulated annealing, tabu search, and memetic algorithms in this class.) In the latter class, the local search is adapted to tackle multiobjective problems depending on the problem being solved.


                     Path Relinking (PR) was suggested as an approach to integrate intensification and diversification strategies in the context of tabu search (Glover & Laguna, 1997). This approach generates new solutions by exploring trajectories that connect high-quality solutions by starting from one of these solutions, called an initiating solution, and generating a path in the neighborhood space that leads toward the other solutions, called guiding solutions. This is accomplished by selecting moves that introduce attributes contained in the guiding solutions, and incorporating them in an intermediate solution initially originated in the initiating solution.


                     Laguna and Martí (1999) adapted PR in the context of GRASP as a form of intensification. The relinking in this context consists in finding a path between a solution found with GRASP and a chosen elite solution. Therefore, the relinking concept has a different interpretation within GRASP since the solutions found in one GRASP iteration to the next are not linked by a sequence of moves (as in the case of tabu search). Resende and Ribeiro (2003) and Ribeiro and Resende (2012) present numerous examples of GRASP with PR.

Let x and y be two solutions of the multiobjective problem. The Path Relinking procedure PR (
                        
                           x
                           ,
                           y
                        
                     ) starts with the first solution x, and gradually transforms it into the second solution y, by swapping out elements in x and replacing them with elements in y. The elements in both solutions x and y remain in the intermediate solutions generated in the path between them. Note that an element can be a node in a graph, a value of a variable, an edge or a path in a network, or any other attribute depending on the particular problem being solved.

Let 
                        
                           
                              
                                 El
                              
                              
                                 x
                              
                           
                        
                      be the set of elements in x. Let 
                        
                           
                              
                                 El
                              
                              
                                 x
                                 -
                                 y
                              
                           
                        
                      be the set of elements in x not present in y and symmetrically, let 
                        
                           
                              
                                 El
                              
                              
                                 y
                                 -
                                 x
                              
                           
                        
                      be the set of elements in y not present in x. Let 
                        
                           
                              
                                 p
                              
                              
                                 0
                              
                           
                           (
                           x
                           ,
                           y
                           )
                           =
                           x
                        
                      be the initiating solution in the path P (
                        
                           x
                           ,
                           y
                        
                     ) from x to y. To obtain the next solution in P (
                        
                           x
                           ,
                           y
                        
                     ), 
                        
                           
                              
                                 p
                              
                              
                                 1
                              
                           
                           (
                           x
                           ,
                           y
                           )
                        
                     , we can remove from x a single element 
                        
                           i
                           ∈
                           
                              
                                 El
                              
                              
                                 x
                                 -
                                 y
                              
                           
                        
                     , or add an element 
                        
                           j
                           ∈
                           
                              
                                 El
                              
                              
                                 y
                                 -
                                 x
                              
                           
                        
                     , or both (add i and remove j) thus obtaining
                        
                           
                              
                                 
                                    El
                                 
                                 
                                    
                                       
                                          p
                                       
                                       
                                          1
                                       
                                    
                                    (
                                    x
                                    ,
                                    y
                                    )
                                 
                              
                              =
                              
                                 
                                    El
                                 
                                 
                                    
                                       
                                          p
                                       
                                       
                                          0
                                       
                                    
                                    (
                                    x
                                    ,
                                    y
                                    )
                                 
                              
                              ⧹
                              {
                              i
                              }
                              ,
                           
                        
                     or
                        
                           
                              
                                 
                                    El
                                 
                                 
                                    
                                       
                                          p
                                       
                                       
                                          1
                                       
                                    
                                    (
                                    x
                                    ,
                                    y
                                    )
                                 
                              
                              =
                              
                                 
                                    El
                                 
                                 
                                    
                                       
                                          p
                                       
                                       
                                          0
                                       
                                    
                                    (
                                    x
                                    ,
                                    y
                                    )
                                 
                              
                              ∪
                              {
                              j
                              }
                              ,
                           
                        
                     or
                        
                           
                              
                                 
                                    El
                                 
                                 
                                    
                                       
                                          p
                                       
                                       
                                          1
                                       
                                    
                                    (
                                    x
                                    ,
                                    y
                                    )
                                 
                              
                              =
                              
                                 
                                    El
                                 
                                 
                                    
                                       
                                          p
                                       
                                       
                                          0
                                       
                                    
                                    (
                                    x
                                    ,
                                    y
                                    )
                                 
                              
                              ⧹
                              {
                              i
                              }
                              ∪
                              {
                              j
                              }
                              .
                           
                        
                     
                  

Following the classification proposed in the previous sections, we consider here three implementations of multiobjective Path Relinking. In the pure Path Relinking algorithm, the selection of the elements i and j is made according to one objective function. To obtain 
                        
                           
                              
                                 p
                              
                              
                                 k
                                 +
                                 1
                              
                           
                           (
                           x
                           ,
                           y
                           )
                        
                      from 
                        
                           
                              
                                 p
                              
                              
                                 k
                              
                           
                           (
                           x
                           ,
                           y
                           )
                        
                     , we evaluate all the possibilities for 
                        
                           i
                           ∈
                           
                              
                                 El
                              
                              
                                 
                                    
                                       p
                                    
                                    
                                       k
                                    
                                 
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 -
                                 y
                              
                           
                        
                      to be removed and 
                        
                           j
                           ∈
                           
                              
                                 El
                              
                              
                                 y
                                 -
                                 
                                    
                                       p
                                    
                                    
                                       k
                                    
                                 
                                 (
                                 x
                                 ,
                                 y
                                 )
                              
                           
                        
                      to be added, and perform the best swap in terms of one objective function. In each application of PR, we consider one objective function, say 
                        
                           
                              
                                 f
                              
                              
                                 i
                              
                           
                        
                      and use it to select the intermediate solutions in the entire path P (
                        
                           x
                           ,
                           y
                        
                     ). On the other hand, in the sequential Path Relinking we alternate the objective function used to select the intermediate solutions. In this way, if we use 
                        
                           
                              
                                 f
                              
                              
                                 i
                              
                           
                        
                      to select 
                        
                           
                              
                                 p
                              
                              
                                 k
                              
                           
                           (
                           x
                           ,
                           y
                           )
                        
                     , we then use 
                        
                           
                              
                                 f
                              
                              
                                 i
                                 +
                                 1
                              
                           
                        
                      to select 
                        
                           
                              
                                 p
                              
                              
                                 k
                                 +
                                 1
                              
                           
                           (
                           x
                           ,
                           y
                           )
                        
                     . Finally in the weighted Path Relinking the weighted objective function
                        
                           
                              f
                              (
                              x
                              )
                              =
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       j
                                       =
                                       1
                                    
                                    
                                       k
                                    
                                 
                              
                              
                                 
                                    w
                                 
                                 
                                    j
                                 
                              
                              
                                 
                                    f
                                 
                                 
                                    j
                                 
                              
                              (
                              x
                              )
                           
                        
                     is used to select all the intermediate solutions in every application of PR (
                        
                           x
                           ,
                           y
                        
                     ).

The PR algorithm operates on a set of solutions, called elite set(ES), constructed with the application of a previous method. In this paper, we apply GRASP to build the elite set. In a multiobjective problem we can identify this set with the set of non-dominated solutions. Initially ES is empty, and we apply GRASP for a certain number of iterations to populate it with the non-dominated solutions obtained. Then, in the following iterations, we apply PR to all pairs of solutions in ES. Specifically, for each pair x and y we apply PR (
                        
                           x
                           ,
                           y
                        
                     ) and PR (
                        
                           y
                           ,
                           x
                        
                     ). The GRASP with PR algorithm terminates when all the pairs in ES have been submitted to the PR method. As previously documented (Laguna & Martí, 1999) we can apply a local search to some of the intermediate solutions in every PR path to obtain improved outcomes.

All the intermediate solutions from x and y, found in the P (
                        
                           x
                           ,
                           y
                        
                     ) path (i.e., 
                        
                           
                              
                                 p
                              
                              
                                 1
                              
                           
                           (
                           x
                           ,
                           y
                           )
                           ,
                           
                              
                                 p
                              
                              
                                 2
                              
                           
                           (
                           x
                           ,
                           y
                           )
                           ,
                           …
                           ,
                           
                              
                                 p
                              
                              
                                 r
                                 -
                                 1
                              
                           
                           (
                           x
                           ,
                           y
                           )
                        
                      where 
                        
                           r
                           =
                           |
                           
                              
                                 El
                              
                              
                                 x
                                 -
                                 y
                              
                           
                           |
                           =
                           |
                           
                              
                                 El
                              
                              
                                 y
                                 -
                                 x
                              
                           
                           |
                        
                     ) have to be checked for their possible inclusion in the set of non-dominated solutions found by the algorithm. We store these intermediate solutions in a pool, Intermediate Pool (IP), and do not check whether they are non-dominated until the Path Relinking method finishes. At that point, we merge the elite set, ES, which initially contained the non-dominated solutions, with IP and return the non-dominated solutions, considering both sets, as the output of the method.

It is worth mentioning the work on multiobjective ant colony optimization (ACO) described in López-Ibáñez and Stützle (2012). Although this methodology is not the target of this paper, it shares a common principle with Path Relinking: the use of memory. The authors studied the different ways in which ACO can be implemented to solve a multiobjective problem. Their empirical study on the biobjective traveling salesman problem reveals how particular design choices affect the quality and the shape of the Pareto front generated with the method.

In previous sections we have identified and classified the different ways in which GRASP with Path Relinking can be applied to solve a multiobjective problem. In this section we review previous work. They are listed in Fig. 3
                     .

It is worth mentioning that we have only found one previous approach of a Path Relinking in the context of multiobjective optimization. In particular, Ishida, Pozo, Goldbarg, and Goldbarg (2009) hybridized it with GRASP for the learning classification problem. It is interesting to observe that although GRASP with Path Relinking is nowadays considered as a standard metaheuristic (see for example Resende & Ribeiro (2003) and Ribeiro & Resende (2012)), it is still ignored in multiobjective optimization.

The most popular approach in previous multiobjective GRASP methods is the weighted combination of the objectives. The first multiobjective GRASP is probably the one in Vianna and Arroyo (2004) for the multiobjective knapsack problem and implements a weighted combined procedure in which the weights are uniformly changed in each construction. In particular, at each iteration, the construction and the local search are guided by a weighted combination of the objectives,
                           
                              (3)
                              
                                 f
                                 (
                                 x
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          j
                                          =
                                          1
                                       
                                       
                                          k
                                       
                                    
                                 
                                 
                                    
                                       λ
                                    
                                    
                                       j
                                    
                                 
                                 
                                    
                                       f
                                    
                                    
                                       j
                                    
                                 
                                 (
                                 x
                                 )
                                 ,
                              
                           
                        where the preference coefficient 
                           
                              
                                 
                                    λ
                                 
                                 
                                    j
                                 
                              
                           
                         is computed in a particular way to obtain a variety of solutions uniformly distributed in the Pareto front. Specifically, for a given value of s, they create all the possible integer vectors 
                           
                              (
                              
                                 
                                    w
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    w
                                 
                                 
                                    2
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    w
                                 
                                 
                                    k
                                 
                              
                              )
                           
                         where 
                           
                              
                                 
                                    ∑
                                 
                                 
                                    j
                                    =
                                    1
                                 
                                 
                                    k
                                 
                              
                              
                                 
                                    w
                                 
                                 
                                    j
                                 
                              
                              =
                              s
                           
                        . Then, each GRASP construction and local search is performed with the preference vector set to 
                           
                              
                                 
                                    λ
                                 
                                 
                                    j
                                 
                              
                              =
                              
                                 
                                    w
                                 
                                 
                                    j
                                 
                              
                              /
                              s
                           
                         for 
                           
                              j
                              =
                              1
                              ,
                              …
                              ,
                              k
                           
                        . In this problem, the candidate list of each construction consists of all the non-assigned items. The evaluation of these items is given by the ratio between the sum of the corresponding coefficients in each objective function, weighted with the 
                           
                              λ
                           
                         vector, and the sum of the corresponding coefficients in each constraint (extending in this way the classical ratio of single-objective knapsack problems to several objectives). The same ratio between objective and constraint coefficients is used in the local search, which is based on removing some of the items in the knapsacks, those with the smallest ratios, and then applying the same constructive procedure described above to select some other items. The GRASP method is applied for 1000 iterations, each one with a 
                           
                              λ
                           
                         vector.

Similarly, the method in Higgins, Hajkowicz, and Bui (2008) for environmental investment decision making is guided by a weighted function, but with weights randomly selected, as it is the case of Reynolds and Iglesia (2009). In particular, the environmental investment decision making considered in Higgins et al. (2008) maximizes multiple environmental benefits within a budget constraint. Specifically, the authors consider travel time of water maximization, biodiversity, and carbon sequestration. The environmental investment problem is an extension of the bi-criteria knapsack problem and a multi-start GRASP algorithm is proposed to approximate its efficient front. The construction phase is guided by a weighed summation objective with random weights in each of the three objective functions. Producing solutions using different sets of weights allows them to be distributed along the Pareto front. The method maintains a population of solutions both dominated and non-dominated for the sake of diversity. This set evolves by applying a local search to randomly selected solutions until the time limit is reached. Note that the local search is not applied to all the constructed solutions as it is customary in GRASP; on the contrary, it is only applied to a fraction of them. The local search is also guided by a weighted sum of the three objectives with randomly generated weights as in the construction step.

The multicriteria minimum spanning tree is studied in Arroyo, Vieira, and Vianna (2008) and a GRASP is proposed for its solution. The construction phase uses Kruskal’s algorithm and the local search is based on a drop-and-add neighborhood. The construction is guided by a weighted combination of the objectives where the preference coefficient 
                           
                              
                                 
                                    λ
                                 
                                 
                                    j
                                 
                              
                           
                         is computed as in Vianna and Arroyo (2004). Given a constructed solution, the local search generates a new spanning tree by dropping and adding edges. The method is compared with a multiobjective version of the Kruskal algorithm.

More sophisticated is the weighted function in Li and Landa-Silva (2009), where the weights are modified from one iteration to the next, according to the location of the generated solution. Their GRASP targets the multiobjective quadratic assignment problem (QAP). In this version of the well-known QAP, multiple types of flows are considered between any two facilities. The proposed algorithm, called mGRASP, is characterized by three features: elite greedy randomized construction, adaptation of search directions, and cooperation between solutions. To find a diverse set of Pareto optimal solutions, mGRASP uses multiple distinct weight vectors evenly spread in the construction and local search phases. Specifically, mGRASP uses up to one thousand weight vectors, where once a solution is reached, the weight vector is modified on the basis of its location with respect to the closest non-dominated neighbor. The method first generates a large set (called 
                           
                              Θ
                           
                        ) with all the normalized weight vectors (uniformly distributed) with components chosen from the set: 
                           
                              {
                              0
                              ,
                              1
                              /
                              H
                              ,
                              …
                              ,
                              (
                              H
                              -
                              1
                              )
                              /
                              H
                              ,
                              1
                              }
                           
                        , where H is a positive integer number. Then, N initial weight vectors evenly spread are selected from the candidate weight set 
                           
                              Θ
                           
                         by first generating the extreme weight vectors, each corresponding to the optimization of a single objective. Then, it identifies the set A of all weight vectors in 
                           
                              Θ
                           
                         with the same maximal distance to these extreme weights, and iteratively selects the weight vector in A with the maximal distance to the recently selected. In this way, all weight vectors selected so far are well-distributed. Finally, the value of each weight is modified by adding or subtracting a relatively small amount. Unlike the classical GRASP algorithm, their method constructs each solution by adding some elements from the previous local optima found. In this sense it can be considered a GRASP implementation with memory structures.


                        Reynolds and Iglesia (2009) proposed a GRASP for the partial classification of a database. This problem basically consists in finding simple classification rules that represent strong descriptions for a particular class of database. Association rules can be evaluated according to a number of conflicting criteria, which has lead to the application of multiobjective metaheuristics. In their GRASP implementation, the authors first apply a construction phase based on a random weighted combination of the objectives and create a set of non-dominated solutions. The local search phase is then applied to the non-dominated solutions. A comparison with previous evolutionary methods for multiobjective optimization favors the proposed GRASP. A refinement of this method is proposed in Reynolds, Corne, and de la Iglesia (2009) to select a small subset of the rules previously identified.

We have also found multiobjective GRASP applications in the context of flow shop scheduling problems. In particular, Davoudpour and Ashrafi (2009) considered the hybrid flow shop scheduling problem with sequence dependent setup times. The measure of performance of a given solution is computed as a function of the assigned due date of each job, in terms of the earliness, tardiness and completion time. They are combined in a single objective and the proposed GRASP solves the associated single-objective problem.

In Delorme, Gandibleux, and Degoutin (2010) we can find a GRASP for the biobjective set packing problem related with a real railway problem. The authors embed GRASP in a parametric procedure so that it can run successively and independently for a wide set of search directions obtained using a scalarization function based on objective weights. In particular they applied a single-objective GRASP for the classic set packing problem, previously published by the same authors, with each scalarized weighted function. In that function, the weights are systematically changed to determine the search directions. The GRASP method is finally hybridized with SPEA for improved outcomes.

Finally in weighted combined methods, we found an interesting approach in which a weighted construction is combined with a pure local search. Salazar-Aguilar, Ríos-Mercado, and González-Velarde (2012) proposed several GRASP algorithms for the biobjective commercial territory design problem. In their approach, the authors look for a partition of a set of city blocks into a fixed number of territories in such a way that certain requirements are satisfied. Two of them are treated as objectives and the rest as constraints. In particular, the first objective is to minimize the dispersion since blocks in each territory must be close to each other. The second one is to obtain balanced territories, achieved by minimizing the deviation with respect to the target territory size. The construction phase in their GRASP methods is guided by a weighted combination of the two objectives. In one of the variants, they relax the connectivity constraint during the construction process, while in the other it is considered as a hard constraint. The improvement method consists in a successive application of single-objective local search procedures (taking one objective at a time). When a constraint is relaxed in the construction process, it is added as an objective in the improvement method to obtain feasibility.

Three papers address sequential implementations of GRASP for multiobjective problems.

In Ishida et al. (2009) a GRASP with Path Relinking for learning classification rules is proposed, where the goal is to create rules that together have good performance for classification. A frequent measure used to evaluate the performance of a classifier is the AUC, the area under the ROC curve (the curve that relates the false and the true positive proportions). The authors propose an alternative approach based on two criteria: sensitivity and specificity. In this way, they face a biobjective problem where the rules (solutions of this problem) are stored as a set of pairs of attribute–value. The constructive method computes both objectives for each possible pair and chooses a fraction of the best in terms of any of both. It randomly selects a pair from the 
                           
                              RCL
                           
                         and accepts it if improves any of the objectives. Otherwise it is discarded. The construction method terminates when the 
                           
                              RCL
                           
                         is empty. Each solution is improved with a local search procedure. Specifically, the algorithm randomly selects one attribute or value to replace in the current solution if it improves either one objective or the other. The second phase applies the Path Relinking between the elite solutions generated so far. For each two elite solutions, the method creates a path with intermediate solutions replacing one attribute–value pair of the initial rule with a pair of the terminal rule, saving it if it qualifies, in the pool of non-dominated solutions according to sensitivity and specificity. The authors show that their GRASP with Path Relinking algorithm obtains an approximation of the Pareto front that gives a good AUC value. We classified this method as a sequential combined method since it adds to the partial solution under construction the elements (pairs of attribute–value) that improve either one objective or the other.


                        Martí, González-Velarde, and Duarte (2009) proposed a GRASP for the path dissimilarity problem, in which a solution consists of a set of p different paths, where two conflicting objectives are optimized: the average length of the paths (minimized), and the dissimilarity among the paths in the set (maximized). The construction phase applies a previous method called RDE to obtain different solutions by combining a k-shortest path algorithm with a mechanism to remove similar paths. The local search performs exchanges (removing a path in the solution and inserting a new path in it) to improve the current solution. Specifically, the method alternates two stages; in the first it tries to improve (to reduce) the value of the first objective (the length) regardless the value of the second objective (dissimilarity). In the second stage it tries to improve (to increase) the second objective regardless the value of the first one. If the method is unable to improve the corresponding objective at either stage it stops and resorts to the other stage. Both stages are alternated until no further improvement is possible or the maximum number of global iterations is reached.


                        Chica, Cordón, Damas, and Bautista (2010) extended the classical line balancing problem by considering the joint minimization of the number and the area of stations, given a cycle time limit. The authors proposed a GRASP algorithm to solve the real problem of a Nissan factory. The constructive method is guided by a greedy function that computes, for each candidate task, the preference of assigning it to the current opened station. This measure is proportional to the processing time and area ratio. The local search, on the other hand, optimizes a weighted sum of the two problem objectives, where weights are randomly selected. However, once this local search is applied, an additional local search is then applied to the greedy solution, focusing now only on one objective (according to the values of the weights).

From the descriptions and classification above, we conclude that most of the previous methods rely on a weighted combination evaluation of the objectives to construct a solution and we can distinguish between those methods in which the weights are generated at random and those in which they are obtained in a systematic fashion. A small fraction of the previous methods implement a sequential construction and none of them can be considered a pure method. It is clear that pure methods are more limited in their original design than the others. However, it must be noted that within the GRASP template, in which randomization has an important role, we could expect an improved performance of this original design. In particular, the greedy evaluation of a single method is applied to construct the 
                           
                              RCL
                           
                         from which we select at random the next element to be added to the solution. It is therefore difficult to draw a priori conclusions about the performance of the different evaluators within GRASP.

We consider two biobjective combinatorial optimization problems to test the different GRASP and Path Relinking variants proposed in the previous sections: the path dissimilarity problem and the bi-orienteering problem.

We target these problems because they are well-known, they are different in nature, and high-quality solutions to several problem instances are available. We now provide a brief description of each problem class.

The path dissimilarity problem (PDP) is a biobjective routing problem in which a set of p paths from an origin to a destination must be generated with minimum length and maximum dissimilarity. Finding different paths in a graph is a classical optimization problem. The best known is the k-shortest path problem in which the shortest, second shortest, 
                           
                              …
                              ,
                              k
                           
                        -th shortest paths from an origin o to a destination d are obtained in a graph. However, many of these alternative paths are likely to share a large number of edges. This is why in some applications we need to consider an alternative approach. For example, in the context of hazardous material (hazmat) transportation we want to obtain spatially dissimilar paths that minimize the risk (distributing the risk over all regional zones to be crossed uniformly).

Given an undirected graph 
                           
                              G
                              =
                              (
                              V
                              ,
                              E
                              )
                           
                         with V the set of vertices and E the set of edges with associated cost 
                           
                              
                                 
                                    c
                                 
                                 
                                    ij
                                 
                              
                           
                         for 
                           
                              (
                              i
                              ,
                              j
                              )
                              ∈
                              E
                           
                        , and a pair of origin–destination vertices, 
                           
                              o
                              -
                              d
                           
                        , we define 
                           
                              P
                              (
                              o
                              ,
                              d
                              )
                           
                         as the set of all paths in G from o to d. Note that in most applications the cost 
                           
                              
                                 
                                    c
                                 
                                 
                                    ij
                                 
                              
                           
                         of edge 
                           
                              (
                              i
                              ,
                              j
                              )
                           
                         is its Euclidean distance. Given an integer number 
                           
                              p
                              >
                              1
                           
                        , a feasible solution to the path dissimilarity problem, PDP, is a set 
                           
                              S
                              ⊆
                              P
                              (
                              o
                              ,
                              d
                              )
                           
                         such that 
                           
                              |
                              S
                              |
                              =
                              p
                           
                        . Given a solution 
                           
                              S
                              =
                              {
                              
                                 
                                    P
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    P
                                 
                                 
                                    2
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    P
                                 
                                 
                                    p
                                 
                              
                              }
                           
                        , we define its value 
                           
                              
                                 
                                    f
                                 
                                 
                                    1
                                 
                              
                              (
                              S
                              )
                           
                         as the average of the costs of the paths in S:
                           
                              
                                 
                                    
                                       f
                                    
                                    
                                       1
                                    
                                 
                                 (
                                 S
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             t
                                             =
                                             1
                                          
                                          
                                             p
                                          
                                       
                                       c
                                       (
                                       
                                          
                                             P
                                          
                                          
                                             t
                                          
                                       
                                       )
                                    
                                    
                                       p
                                    
                                 
                                 
                                 where
                                 
                                 c
                                 (
                                 
                                    
                                       P
                                    
                                    
                                       t
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          (
                                          i
                                          ,
                                          j
                                          )
                                          ∈
                                          
                                             
                                                P
                                             
                                             
                                                t
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       c
                                    
                                    
                                       ij
                                    
                                 
                                 .
                              
                           
                        
                     

We also define its dissimilarity value 
                           
                              
                                 
                                    f
                                 
                                 
                                    2
                                 
                              
                              (
                              S
                              )
                           
                         as the average of the dissimilarity between the 
                           
                              
                                 
                                    
                                       
                                          
                                             
                                                p
                                             
                                          
                                          
                                             
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           
                         distinct pairs of paths in S:
                           
                              
                                 
                                    
                                       f
                                    
                                    
                                       2
                                    
                                 
                                 (
                                 S
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             p
                                             -
                                             1
                                          
                                       
                                       
                                          
                                             ∑
                                          
                                          
                                             j
                                             =
                                             i
                                             +
                                             1
                                          
                                          
                                             p
                                          
                                       
                                       dis
                                       (
                                       
                                          
                                             P
                                          
                                          
                                             i
                                          
                                       
                                       ,
                                       
                                          
                                             P
                                          
                                          
                                             j
                                          
                                       
                                       )
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         p
                                                      
                                                   
                                                   
                                                      
                                                         2
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where, as applied in Martí et al. (2009) the dissimilarity 
                           
                              dis
                              (
                              
                                 
                                    P
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    P
                                 
                                 
                                    j
                                 
                              
                              )
                           
                         between two paths 
                           
                              
                                 
                                    P
                                 
                                 
                                    i
                                 
                              
                           
                         and 
                           
                              
                                 
                                    P
                                 
                                 
                                    j
                                 
                              
                           
                         is computed as the average of the distances between each vertex in 
                           
                              
                                 
                                    P
                                 
                                 
                                    i
                                 
                              
                           
                         to the path 
                           
                              
                                 
                                    P
                                 
                                 
                                    j
                                 
                              
                           
                         plus the average of the distances between each vertex in 
                           
                              
                                 
                                    P
                                 
                                 
                                    j
                                 
                              
                           
                         to the path 
                           
                              
                                 
                                    P
                                 
                                 
                                    i
                                 
                              
                           
                        .

With these elements, we can formulate the PDP as:
                           
                              
                                 
                                    
                                       
                                          
                                             (
                                             PDP
                                             )
                                             
                                                min
                                             
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   f
                                                
                                                
                                                   1
                                                
                                             
                                             (
                                             S
                                             )
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                max
                                             
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   f
                                                
                                                
                                                   2
                                                
                                             
                                             (
                                             S
                                             )
                                          
                                       
                                    
                                    
                                       
                                          
                                             subject to
                                             
                                          
                                       
                                       
                                          
                                             S
                                             ⊆
                                             P
                                             (
                                             o
                                             ,
                                             d
                                             )
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             |
                                             S
                                             |
                                             =
                                             p
                                             .
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The constructive method for the PDP in Martí et al. (2009) starts by creating a set S with 2000 paths (the first 1000 with the Yen’s implementation of the k-shortest path method described in Carotenuto, Giordani, & Ricciardelli (2007) and the other 1000 with the Iterative Penalty Method described in Johnson, Joy, & Clarke (1992)). Then, in each iteration, they remove from S a path with low contribution until it only contains p paths. The contribution is measured in terms of the objective function 
                           
                              
                                 
                                    f
                                 
                                 
                                    2
                                 
                              
                           
                        , the dissimilarity among paths. In our GRASP constructions for the PDP we consider the same set of 2000 paths but both objectives functions, 
                           
                              
                                 
                                    f
                                 
                                 
                                    1
                                 
                              
                           
                         and 
                           
                              
                                 
                                    f
                                 
                                 
                                    2
                                 
                              
                           
                        . Therefore, for each candidate path 
                           
                              c
                              ∈
                              S
                           
                         to be removed, its evaluations are 
                           
                              
                                 
                                    g
                                 
                                 
                                    1
                                 
                              
                           
                         and 
                           
                              
                                 
                                    g
                                 
                                 
                                    2
                                 
                              
                           
                        , which measure respectively its contribution to 
                           
                              
                                 
                                    f
                                 
                                 
                                    1
                                 
                              
                           
                         and 
                           
                              
                                 
                                    f
                                 
                                 
                                    2
                                 
                              
                           
                        . In particular, following the algorithms in Section 2, we propose:
                           
                              •
                              a pure-random (PRA),

pure-ordered (PO),

random-sequential combined (RSC),

ordered-sequential combined (OSC),

three weighted combined methods (WC-R, WC-S and WC-0.5).

Note that PRA and PO can be considered as an extension of the constructive method in Martí et al. (2009) since they are guided by a single objective, 
                           
                              
                                 
                                    f
                                 
                                 
                                    1
                                 
                              
                           
                         or 
                           
                              
                                 
                                    f
                                 
                                 
                                    2
                                 
                              
                           
                        , in each construction. On the other hand, in weighted combined (WC-R), weights are randomly generated in each construction, as Higgins et al. (2008) and Reynolds and Iglesia (2009) do for other problems. Alternatively, WC-S generates the weights in a systematic way, as Vianna and Arroyo (2004) and Arroyo et al. (2008) do on other optimization problems. Finally, we consider a simpler version, called WC-0.5, in which the weights are constant and equal to (0.5, 0.5) to test if the randomization in GRASP is enough to generate different good solutions without changing the weights.

The local search consists in exchanging a path in the solution with another path, not included on it, taken from the set S defined above with the 2000 initial paths. Therefore, the two associated move values are respectively the change in each objective function. In particular, they are the average of the costs of the paths from the origin to the destination, 
                           
                              
                                 
                                    f
                                 
                                 
                                    1
                                 
                              
                           
                        , and the average of the dissimilarities among paths, 
                           
                              
                                 
                                    f
                                 
                                 
                                    2
                                 
                              
                           
                        . Note that to speed up the process the paths in S are ordered according to their value (
                           
                              
                                 
                                    f
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    f
                                 
                                 
                                    2
                                 
                              
                           
                        , or a weighted sum). This can be performed off-line as a preprocessing of the method. We therefore propose three local search methods, pure, sequential, and combined, based on the description given in Section 3. In the computational experiments in Section 7, we evaluate the seven constructive methods above and match the best one in each category, pure, sequential, and weighted, with the corresponding local search; thus obtaining three GRASP methods for the PDP.

The GRASP in Martí et al. (2009) for the PDP has a pure constructive method based on 
                           
                              
                                 
                                    f
                                 
                                 
                                    2
                                 
                              
                           
                        . However, their local search implements a sequential-combined method since it alternates two stages; in the first it tries to improve the value of 
                           
                              
                                 
                                    f
                                 
                                 
                                    1
                                 
                              
                           
                         regardless the value of 
                           
                              
                                 
                                    f
                                 
                                 
                                    2
                                 
                              
                           
                        , and symmetrically, in the second stage it tries to improve 
                           
                              
                                 
                                    f
                                 
                                 
                                    2
                                 
                              
                           
                         regardless the value of 
                           
                              
                                 
                                    f
                                 
                                 
                                    1
                                 
                              
                           
                        , for a maximum number of iterations. So their method significantly differs from our methods since we do not couple pure construction with combined local search and, more importantly, in our sequential improvements we do not allow the deterioration of one objective while improving the other.

Given two solutions x and y, we defined in Section 4 
                        
                           
                              
                                 
                                    El
                                 
                                 
                                    x
                                    -
                                    y
                                 
                              
                           
                         as the set of elements in x not present in y and 
                           
                              
                                 
                                    El
                                 
                                 
                                    y
                                    -
                                    x
                                 
                              
                           
                         as the set of elements in y not present in x. In the PDP, solutions x and y are sets of p paths connecting the origin o with the destination d and the elements, present or not in these two solutions, are the paths. At each iteration of the PR from x to y, the paths in 
                           
                              
                                 
                                    El
                                 
                                 
                                    y
                                    -
                                    x
                                 
                              
                           
                         are considered to be added to the current intermediate solution (set of p paths), replacing one of the paths in 
                           
                              
                                 
                                    El
                                 
                                 
                                    x
                                    -
                                    y
                                 
                              
                           
                         (to keep the number of paths constant, and equal to p). The method performs the best exchange in terms of the average of the costs of the paths from the origin to the destination (
                           
                              
                                 
                                    f
                                 
                                 
                                    1
                                 
                              
                           
                        ), and the average of the dissimilarities among paths (
                           
                              
                                 
                                    f
                                 
                                 
                                    2
                                 
                              
                           
                        ), depending on the PR variant that we are implementing (pure, sequential, or weighted as described in Section 4).

The bi-orienteering problem (BOP) is a generalization of its single-objective version, also known as the selective traveling salesman problem, introduced by Tsiligirides (1984). In the orienteering problem, each vertex of a given directed graph 
                           
                              G
                              =
                              (
                              V
                              ,
                              A
                              )
                           
                         has a profit. The aim of this problem is to select a subset of vertices in order to maximize the sum of profits of the selected vertices. Moreover, the tour visiting the selected vertices cannot exceed a maximum length or time (see Campos et al. (2013) for a recent solving method). The motivation of the biobjective version of this problem was the planning of a set of tourist routes in a large city. Each point of interest has different profits associated with different activities (for instance, culture and leisure). Since the maximization of the profits associated with one activity does not imply the maximization of the profits of another activity, this problem is multiobjective in nature.

There are several problems related with the orienteering problem. For instance, in the Prize-Collecting TSP, see Balas (1988), each vertex has a given prize and penalty, and the goal is to minimize the length of the tour plus the total of the penalties of the vertices not in the tour, while collecting a given quota of the prizes. Feillet, Dejax, and Gendreau (2005) classified these problem types as TSP with profits. Archetti, Hertz, and Speranza (2007) extended the TSP with profits to several tours naming this version the Vehicle Routing Problem (VRP) with profits. Note that all of them are single-objective approaches to similar problems. Recently however, Schilde, Doerner, Hartl, and Kiechle (2009) proposed two metaheuristic procedures for solving the biobjective orienteering problem. The first is based on Ant Colony Optimization (ACO), introduced by Dorigo and Gambardella (1997), and the second is based on the Variable Neighborhood Search (VNS) metaheuristic of Mladenović and Hansen (1997). Both algorithms were combined with a Path Relinking procedure.

The biobjective OP, called BOP, can be stated on a directed graph 
                           
                              G
                              =
                              (
                              V
                              ,
                              A
                              )
                           
                         with 
                           
                              V
                              =
                              {
                              0
                              ,
                              1
                              ,
                              2
                              ,
                              …
                              ,
                              n
                              +
                              1
                              }
                           
                         the set of vertices and 
                           
                              A
                              =
                              {
                              (
                              i
                              ,
                              j
                              )
                              :
                              i
                              ,
                              j
                              ∈
                              V
                              ,
                              i
                              
                              ≠
                              
                              j
                              ,
                              i
                              
                              ≠
                              
                              n
                              +
                              1
                              ,
                              j
                              
                              ≠
                              
                              0
                              }
                           
                         the set of arcs. Without loss of generality we suppose that G is a complete graph with associated cost 
                           
                              
                                 
                                    c
                                 
                                 
                                    ij
                                 
                              
                           
                         for 
                           
                              (
                              i
                              ,
                              j
                              )
                              ∈
                              A
                           
                        . We have two profits 
                           
                              
                                 
                                    f
                                 
                                 
                                    i
                                    1
                                 
                              
                              ,
                              
                                 
                                    f
                                 
                                 
                                    i
                                    2
                                 
                              
                           
                         associated with each vertex 
                           
                              i
                              ∈
                              V
                              ⧹
                              {
                              0
                              ,
                              n
                              +
                              1
                              }
                           
                        . Both vertices 0 and 
                           
                              n
                              +
                              1
                           
                         have no profit and represent the starting and ending vertices, respectively. Sometimes vertices 0 and 
                           
                              n
                              +
                              1
                           
                         denote the same physical point. A tour in the original problem is represented by a directed path in G from vertex 0 to vertex 
                           
                              n
                              +
                              1
                           
                        . Let L be the maximum length allowed to tour 
                           
                              τ
                           
                         and consider the set of all feasible tours 
                           
                              Θ
                              =
                              {
                              τ
                              |
                              c
                              (
                              τ
                              )
                              ⩽
                              L
                              }
                           
                        . The BOP can then be formulated as:
                           
                              
                                 
                                    
                                       
                                          
                                             (
                                             BOP
                                             )
                                             
                                                max
                                             
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   f
                                                
                                                
                                                   1
                                                
                                             
                                             (
                                             τ
                                             )
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                max
                                             
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   f
                                                
                                                
                                                   2
                                                
                                             
                                             (
                                             τ
                                             )
                                          
                                       
                                    
                                    
                                       
                                          
                                             subject to
                                             
                                          
                                       
                                       
                                          
                                             τ
                                             ∈
                                             Θ
                                             .
                                          
                                       
                                    
                                 
                              
                           
                        
                     

A detailed formulation of the general form of BOP when considering multiobjective functions can be found in Schilde et al. (2009). Since both problems, PDP and BOP, have two objective functions, this provides the opportunity of visualizing the pairs of objective values of non-dominated solutions in the objective space and then comparing the quality of the procedures. It is possible to construct an approximation to the Pareto-efficient front which is formed by the points with the values of the objectives of all the non-dominated solutions.

A solution is a directed tour 
                           
                              τ
                           
                         visiting some of the vertices from an origin to a destination, where each vertex has two profits and the two objective values of the tour, 
                           
                              
                                 
                                    f
                                 
                                 
                                    1
                                 
                              
                              (
                              τ
                              )
                           
                         and 
                           
                              
                                 
                                    f
                                 
                                 
                                    2
                                 
                              
                              (
                              τ
                              )
                           
                        , are computed by adding their corresponding profits. Therefore, in our constructive methods for the BOP, for each candidate vertex c to be included in the partial tour, 
                           
                              
                                 
                                    g
                                 
                                 
                                    1
                                 
                              
                              (
                              c
                              )
                           
                         and 
                           
                              
                                 
                                    g
                                 
                                 
                                    2
                                 
                              
                              (
                              c
                              )
                           
                         are the profits of c. Following the GRASP construction process, the selected vertex in the restricted candidate list (
                           
                              RCL
                           
                        ), is inserted in the best position in the partial tour. To create the 
                           
                              RCL
                           
                        , we apply 
                           
                              
                                 
                                    g
                                 
                                 
                                    1
                                 
                              
                              (
                              c
                              )
                           
                         and/or 
                           
                              
                                 
                                    g
                                 
                                 
                                    2
                                 
                              
                              (
                              c
                              )
                           
                         in the different ways described in Section 2, obtaining the following seven methods:
                           
                              •
                              a pure-random (PRA),

pure-ordered (PO),

random-sequential combined (RSC),

ordered-sequential combined (OSC),

three weighted combined methods (WC-R, WC-S and WC-0.5).

As in the PDP, in the weighted combined WC-R, weights are randomly generated in each construction, in WC-S they are generated in a deterministic and systematic way (Vianna & Arroyo, 2004), while in the WC-0.5 they are fixed to 0.5.

In the BOP the neighborhood is based on a exchange between a vertex in the tour and another vertex not in the tour (without exceeding the maximum length L). The difference between the profits of both vertices provides the respective move values. When a one-to-one exchange is performed, we try an insertion move in which a vertex not present in the current tour is considered to be added to the tour. Note that in this problem the insertion of a new point into the tour could not necessarily increment its length (some points are in the same location). It could even reduce the tour length because the distances do not satisfy the triangular inequality in the instances tested, and therefore when adding a vertex into the tour, we try the addition of more vertices. This is why after an exchange we consider insertions as long as we can add vertices in the tour without exceeding the maximum length L. The added vertices are inserted in the best position.

As in the PDP, we propose three improvement methods: pure, sequential, and combined (Section 3). We evaluate in Section 7 the seven constructive methods above and match the best one in each category, pure, sequential and weighted, with the corresponding local search; thus obtaining three GRASP methods for the BOP.

The Path Relinking (PR) described in Section 4 can be adapted to the BOP, in which solutions x and y are two tours in the graph, by defining the elements in 
                           
                              
                                 
                                    El
                                 
                                 
                                    x
                                    -
                                    y
                                 
                              
                           
                         as the vertices present in tour x and not present in tour y (and conversely 
                           
                              
                                 
                                    El
                                 
                                 
                                    y
                                    -
                                    x
                                 
                              
                           
                        ). At each iteration of the PR from x to y we add a vertex in 
                           
                              
                                 
                                    El
                                 
                                 
                                    y
                                    -
                                    x
                                 
                              
                           
                         to the current intermediate solution according to its profit values (
                           
                              
                                 
                                    f
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    f
                                 
                                 
                                    2
                                 
                              
                           
                        , or a weighted sum depending on the PR variant). If the resulting solution is feasible, we consider it as the next intermediate solution in the path; otherwise, we remove from the current intermediate solution the worst vertex in 
                           
                              
                                 
                                    El
                                 
                                 
                                    x
                                    -
                                    y
                                 
                              
                           
                         in terms of profit values. We keep removing vertices until the current solution becomes feasible. At this point we consider it as the current intermediate solution and resort to the next PR step. For each of the three GRASP methods described above for the BOP, we consider the associated PR implementations, pure, sequential and weighted, to create the three PR versions for this problem: Pure-PR, Seq-PR, and Weight-PR.

This section describes the computational experiments that we performed to test the efficiency of our GRASP with Path Relinking procedures as well as to compare them with the previous methods identified to be the state of the art for both the path dissimilarity problem (PDP) and the bi-orienteering problem (BOP).

The 30 PDP test instances with approximately 500 vertices in our experimentation are taken from Martí et al. (2009). These instances were generated removing most of the edges in the following 10 original instances from the well known TSP library: ali535, att532, d493, d657, fl417, gr666, gr431, rat575, u574, and pcb442.
                           1
                           Available at http://www.iwr.uni-heidelberg.de/groups/comopt/software/TSPLIB95/.
                        
                        
                           1
                         In particular, the authors only included those edges with a cost (distance value) smaller than 10% of the maximum distance value in each instance, resulting in a sparse and connected graph. The farthest points are taken as the origin and destination in the PDP. For each of these 10 TSP instances, we consider the number of paths 
                           
                              p
                              =
                           
                        5, 10 and 15; thus obtaining 30 PDP instances. We consider the 10 instances with 
                           
                              p
                              =
                              15
                           
                         to calibrate the methods (i.e., to perform the first and the second experiment).

The 30 BOP test instances in our experiment, with number of vertices ranging from 21 to 2143 are taken from Schilde et al. (2009). The instances were collected from a variety of sources. Specifically, they are: 2-p21, 2-p32, 2-p33, 2-p64, 2-p66, 2-p97, 2-p292, 2-p559, 2-p562, and 2-p2143. For each of them, we consider three maximum lengths, the smallest, medium, and maximum of those considered by these authors, obtaining 30 instances. Additionally, we generated 10 medium-size instances to calibrate the methods. All the instances and results of our experiments are available at http://www.optsicom.es. Experiments for the PDP were performed on an Intel Core 2 Quad CPU and 6GB RAM, while those for the BOP were run on an Intel I5 at 3.2GHz.

The comparison of the performance of heuristic methods is always a difficult task since it involves solution quality, computational effort, robustness, and other factors. In the case of multiobjective optimization, performance assessment is even more complicated because we compare sets of points (approximations to the efficient front). We apply the indicators and guidelines proposed in Knowles, Thiele, and Zitzler (2006) and implemented in the PISA project (http://www.tik.ee.ethz.ch/pisa). In particular, they propose the following four evaluators as the most discriminant indicators to compare multiobjective methods:
                           
                              •
                              
                                 Hyper-volume: This metric suggested by Zitzler and Thiele (1999) measures the size of the space covered. In other words, it computes the hypervolume of the portion of the objective space that is weakly dominated by an approximation set. This measure is usually computed as the hypervolume difference to a reference set where the smaller values correspond to higher quality (in contrast with the original hypervolume).


                                 Unary epsilon: The epsilon indicator family, introduced in Zitzler et al. (2003), has additive, multiplicative, unary and binary versions. In our comparison we consider the unary multiplicative version given by default in PISA: Given a frontier A and a reference frontier 
                                    
                                       R
                                       ,
                                       ∊
                                       (
                                       A
                                       ,
                                       R
                                       )
                                    
                                  gives the minimum factor 
                                    
                                       ∊
                                    
                                  by which each point of R can by multiplied such that the resulting transformed approximation frontier is weakly dominated by A. Therefore, as with the hypervolume difference, the smaller the measure, the higher the quality of the method.


                                 R: Hansen and Jaszkiewicz (1998) proposed three different indicators R1, R2, and R3, based on using a set of utility functions, which can be defined as a mapping from the objective vectors to the set of real numbers. These utility functions represent the preferences of the decision maker. Different functions have been proposed, from the simple weighted combination of objectives to the complex Tchebychev function. Similarly to the 
                                    
                                       ∊
                                    
                                  indicator, the R indicator can be defined as unary or binary. In this section we consider the R2 unary indicator since it is a standard in the PISA project.


                                 C: This is known as the coverage measure of two sets (Zitzler & Thiele, 1999). 
                                    
                                       C
                                       (
                                       A
                                       ,
                                       B
                                       )
                                    
                                  represents the proportion of points in the estimated efficient frontier B that are dominated by the efficient points in the estimated frontier A. The value 
                                    
                                       C
                                       (
                                       A
                                       ,
                                       B
                                       )
                                       =
                                       1
                                    
                                  means that all decision vectors in B are weakly dominated by A. The opposite, 
                                    
                                       C
                                       (
                                       A
                                       ,
                                       B
                                       )
                                       =
                                       0
                                    
                                 , represents the situation where none of the points in B are weakly dominated by A. Note that always both directions have to be considered, since 
                                    
                                       C
                                       (
                                       A
                                       ,
                                       B
                                       )
                                    
                                  is not necessarily equal to 
                                    
                                       1
                                       -
                                       C
                                       (
                                       B
                                       ,
                                       A
                                       )
                                    
                                 .

As mentioned in Knowles et al. (2006), if two reliable indicators, as the four above, contradict one another on the preference ordering of two approximation sets, then it implies that the two sets are incomparable.

In our first experiment we compare the seven different constructive methods for both the PDP and BOP described in Section 6:
                           
                              •
                              
                                 pure-ordered (PO),


                                 pure-random (PRA),


                                 ordered-sequential combined (OSC),


                                 random-sequential combined (RSC),


                                 three weighted combined methods (WC-R, WC-S and WC-0.5).


                        Table 1
                         shows the hyper-volume (Hyp.), the unary epsilon (eps.), and the R2 indicator for the 10 PDP instances with 
                           
                              p
                              =
                              15
                           
                        . The reference set R in the first two indicators is the set of non-dominated points obtained when we merge the approximations of the efficient frontier obtained with the seven methods. These results were obtained by executing each algorithm one time over each instance. In fact, we follow this approach in the entire experimentation except in the last experiment, where we investigate the robustness of each algorithm by performing several executions over individual instances.


                        Table 2
                         shows the coverage between all pairs of these seven methods. Similarly, Tables 3 and 4
                        
                         show respectively these metrics for the 10 medium-size biorienteering instances.

Considering the PDP instances, Tables 1 and 2 show that pure methods provide slightly better approximations to the efficient frontiers than the sequential combined ones; although the best seem to be the weighted combined methods, being the WC-S the best overall. The hypervolume, epsilon, and R2 metrics support this point since weighted methods present lower values than the pure methods, and pure methods have lower values than the sequential ones. The coverage in Table 2 is in line with this observation, with the following inequalities 
                           
                              C
                              (
                              WC
                              -
                              S
                              ,
                              M
                              )
                              >
                              C
                              (
                              M
                              ,
                              WC
                              -
                              S
                              )
                           
                         for any other method M reported in this table. We identify PO, OSC, and WC-S, as the best method in each category, pure, sequential, and weighted, based on the four metrics considered. In particular, PO and PRA exhibit the same hypervolume, epsilon, and R2 values but 
                           
                              C
                              (
                              PO
                              ,
                              PRA
                              )
                              =
                              0.37
                              >
                              0.26
                              =
                              C
                              (
                              PRA
                              ,
                              PO
                              )
                           
                        ; so we conclude that PO is slightly better than PRA. A similar situation is observed with the sequential methods in which OSC and RSC have very similar values in Table 1 with a tiny improvement of OSC over RSC. However, the coverage differences are larger and indicate the superiority of OSC (
                           
                              C
                              (
                              OSC
                              ,
                              RSC
                              )
                              =
                              0.46
                              >
                              0.32
                              =
                              C
                              (
                              RSC
                              ,
                              OSC
                              )
                           
                        ). Finally, the hypervolume, epsilon, and R2 exhibit differences among the three weighted methods with WC-S as the clear winner. This superiority is confirmed by the coverage values. We therefore will consider the PO, OSC, and WC-S methods to be coupled with the local search to design a complete GRASP in each category of methods.


                        Table 3 indicates that on the Bi-orienteering problem the differences among methods are smaller than in the PDP. The lowest values (i.e., the best evaluations) are obtained with the random sequential combined (RSC), closely followed by the random weighted combined (WC-R). If we want to select the best method in each category, pure, sequential, and weighted, we can see that PO performs slightly better than PRA. In particular, the epsilon value of PO is 0.18 which compares favorably with the 0.19 of PRA and, as shown in Table 4, 
                           
                              C
                              (
                              PO
                              ,
                              PRA
                              )
                              =
                              0.14
                              >
                           
                         
                        
                           
                              0.10
                              =
                              C
                              (
                              PRA
                              ,
                              PO
                              )
                           
                        . Regarding the sequential methods, it is clear that RSC performs better than OSC since all three metrics (Hyp., eps., and R2) present lower values in RSC. The coverage confirms this point since 
                           
                              +
                              C
                              (
                              RSC
                              ,
                              OSC
                              )
                              =
                              0.33
                              >
                              0.09
                              =
                              C
                              (
                              OSC
                              ,
                              RSC
                              )
                           
                        . Finally, among the three weighted methods, the random method (WC-R) seems to perform better than both WC-S and WC-0.5 according to the three metrics in Table 3 and the coverage in Table 4. Therefore we select PO, RSC, and WC-R as the best in each category; to be coupled with the local search in the next experiment.

We performed the non-parametric Friedman test for multiple related samples to the individual values (for each instance) of the three evaluators averaged in Tables 1–3 (hypervolume, epsilon, and R2). This test computes, for each instance, the rank value of each method according to solution quality (an evaluator in our case). Then, it calculates the average rank value for each method across all instances. If the averages differ greatly, the associated p-value or level of significance is small. The resulting p-value of 0.24 obtained with the data in Table 1 indicates that there are no statistically significant differences among the variants tested. In other words, the differences among the methods are small. On the contrary, we obtained a p-value lower than 0.01 with the data in Table 3, indicating that in this case there are statistically significant differences among the variants tested. Additionally, in both cases the rank of this test is in line with the comments above.

It is interesting to observe that in both PDP and BOP, the weighted constructive methods perform better than the pure or sequential methods. However, we cannot draw a general conclusion regarding the best weighted method. The weighted systematic variant, in which the weights are uniformly chosen in each construction, performs better than the other two in the PDP; but the weighted random variant, in which weights are randomly generated, performs slightly better than the other two in the BOP.

To complement this information we consider a representation of the non-dominated points generated with the three best constructive methods in each problem. Specifically, Fig. 4
                         shows the non-dominated points obtained with PO, OSC, and WC-S on a medium-size PDP instance, while Fig. 5
                         shows the non-dominated points obtained with PO, RSC, and WC-R on a medium-size BOP instance. Both figures confirm the superiority of the weighted approaches over the other two considered.

In our second experiment we compare three different GRASP variants for both PDP and BOP:
                           
                              •
                              
                                 Pure-GRASP in which the construction is a pure method coupled with the pure local search (as described in Section 3),
                              


                                 Seq-GRASP where the construction implements a sequential combined method and is coupled with the sequential-combined local search,


                                 Weight-GRASP where the construction and the local search consist in the combined weighted methods, respectively.

According to the first experiment we consider for the PDP the PO method as the pure construction, the OSC as the sequential combined, and WC-S as the weighted method. For the BOP we consider the PO, RSC, and WC-R constructions, respectively. Table 5
                         shows the hyper-volume (Hyp.), the unary epsilon (eps.), and the R2 indicator for the 10 PDP instances with 
                           
                              p
                              =
                              15
                           
                        . Table 6
                         shows the coverage between all pairs of these three methods. Similarly, Tables 7 and 8
                        
                         show these statistics for the three GRASP methods on the 10 medium-size bi-orienteering instances.


                        Table 5 shows that the best GRASP variant for the PDP is the Pure method (PO-GRASP), since it obtains lower values in the three measures reported. Table 6 confirms this point because 
                           
                              C
                              (
                              PO
                              -
                              GRASP
                              ,
                              M
                              )
                              >
                              C
                              (
                              M
                              ,
                              PO
                              -
                              GRASP
                              )
                           
                         for any other GRASP method M considered here. However, it is worth mentioning that the weighted variant (WCS-GRASP) also obtains good results (close to those achieved by the pure variant). It seems that the random component of GRASP benefits to the pure method in this problem, and it is able to create better approximations to the efficient frontier than its competing methods.

A different picture is shown in Table 7 for the BOP. Here the weighted method (WCR-GRASP) is the winner, closely followed by the sequential variant (RSC-GRASP). Both present lower values of the Hyp., eps., and R2 statistics than the pure method. Results in Table 8 are in line with them since the coverage values are 
                           
                              C
                              (
                              WCR
                              -
                              GRASP
                              ,
                              PO
                              -
                              GRASP
                              )
                              =
                              0.27
                              >
                              C
                              (
                              RSC
                              -
                              GRASP
                              ,
                              PO
                              -
                              GRASP
                              )
                              =
                              0.25
                           
                        , and both values are larger than 
                           
                              C
                              (
                              PO
                              -
                              GRASP
                              ,
                              RSC
                              -
                              GRASP
                              )
                              =
                              0.03
                           
                        , which is larger than 
                           
                              C
                              (
                              PO
                              -
                              GRASP
                              ,
                              WCR
                              -
                              GRASP
                              )
                              =
                              0.02
                           
                        .

As in the previous experiment, we perform the non-parametric Friedman test with the three evaluators (hypervolume, epsilon, and R2) averaged in Tables 5–7. In both tests we obtain a p-value lower than 0.01 indicating the significant differences among the methods. Additionally, the rank values confirm that the PO-GRASP is the best method for the PDP, and the WCR-GRASP the best one for the BOP.

In our next experiment we study the hybridization of the Path Relinking (PR) methodology described in Section 4 with the three GRASP variants tested above, pure, sequential and weighted, in both the PDP and the BOP. Moreover, we compare the resulting hybrid methods with the best algorithms known for both problems. Table 9
                         shows, for the 30 PDP instances, the hyper-volume (Hyp.), the unary epsilon (eps.), the R2 indicator, and the CPU time in seconds, with the three PR variants and the two previous methods identified to be the best:
                           
                              •
                              
                                 Pure-PR,


                                 Seq-PR,


                                 Weight-PR,


                                 GP (Martí et al., 2009),


                                 MSPA (Dell’Ollmo, Gentilli, & Scozzari, 2005).

Note that each PR variant is applied to the elite set of solutions obtained with the application of the corresponding GRASP method (pure, sequential or weighted respectively). Table 10
                         shows the coverage between all pairs of these five methods. They have been run for a similar CPU time in the same computer (close to 2min).

Results in Table 9 clearly indicate that the Pure-PR is the best method overall in the PDP, closely followed by the weighted variant. The pure method obtains the lowest values in the three measures reported in these tables (0.00 for the Hyp., 0.02 for the eps., and 0.00 for R2), which compares favorably with the values of the other four methods. Moreover, Pure-PR and Weight-PR improve upon the two previous methods identified to be the best for this problem: MSPA and GP, obtaining lower values in the three metrics. The coverage values shown in Table 10 confirm the analysis above. In particular, for any method M considered, 
                           
                              C
                              (
                              Pure
                              -
                              PR
                              ,
                              M
                              )
                              >
                              C
                              (
                              M
                              ,
                              Pure
                              -
                              PR
                              )
                           
                        . Moreover, according to these coverage values, the weighted variant, Weight-PR, also improves the two previous methods given that 
                           
                              C
                              (
                              Weight
                              -
                              PR
                              ,
                              MSPA
                              )
                              =
                              0.98
                              >
                              0.04
                              =
                              C
                              (
                              MSPA
                              ,
                              Weight
                              -
                              PR
                              )
                           
                         and similarly, 
                           
                              C
                              (
                              Weight
                              -
                              PR
                              ,
                              GP
                              )
                              =
                              0.73
                              >
                              0.02
                              =
                              C
                              (
                              GP
                              ,
                              Weight
                              -
                              PR
                              )
                           
                        . To complement this information, we depict in Fig. 6
                         the approximation of the efficient frontier obtained with Pure-PR and the two previous methods, MSPA and GP on one of the largest instances (d657 with p
                        =10). This figure illustrates the superiority of the Pure-PR w.r.t the previous methods in terms of obtaining a good approximation of the efficient frontier.

We applied the Friedman test for paired samples to the data used to generate Table 9. The resulting p-value is lower than 0.01, which clearly indicates that there are statistically significant differences among the five methods tested. A typical post-test analysis consists in ranking the methods under comparison according to the average rank values computed with this test. According to this, we obtain that the Pure-PR method is the best overall with an average rank of 1.22, followed by Weight-PR with an average rank of 1.88. Finally, we obtain the three methods with larger rank values (as compared with the previous methods): GP (3.03), Seq-PR (3.86), and MSPA (5.00).

We compare now the results of Pure-PR and Weight-PR shown in Table 9 with two well-known non-parametric tests for pairwise comparisons: the Wilcoxon test and the Sign test. On the one hand, the Wilcoxon test answers the question: Do the two samples (solutions obtained with both methods in our case) represent two different populations? The resulting probability value of 0.001 indicates that the values compared come from different methods. On the other hand, the Sign test computes the number of instances on which an algorithm supersedes another one. The resulting probability is lower than 0.001, which indicates that Pure-PR is significantly better than Weight-PR.

We now compare the Path Relinking and best methods for the BOP. From the 30 instances in our benchmark, the 10 with the smallest length allowed, L, only have one point in the efficient frontier and the five methods in this comparison are able to match them (note that they are the most restricted instances with a very few feasible solutions). We therefore remove these 10 instances from the comparison, specially because the metrics that we are using need a larger number of efficient points. Table 11
                         shows the hyper-volume (Hyp.), the unary epsilon (eps.), the R2 indicator, and CPU time in seconds, on the 20 BOP instances with large L values previously reported, for the three PR variants (Pure-PR, Seq-PR, and Weight-PR) and the two previous methods identified to be the best:
                           
                              •
                              
                                 Pure-PR,


                                 Seq-PR,


                                 Weight-PR,


                                 ACO (Schilde et al., 2009),


                                 VNS (Schilde et al., 2009).


                        Table 12
                         shows the coverage between all pairs of these five methods.

Results in Table 11 show that the Weight-PR and Seq-PR are the best methods in the BOP. Specifically, they obtain the lowest values of the three metrics considered (Hyp., eps., and R2). This is specially relevant in the case of the Weight-PR since it exhibits a much shorter average running time than the other methods. The running times of the previous methods (ACO and VNS) correspond to an Intel Pentium 4D at 3.2GHz (Schilde et al., 2009) while the PR variants were run on an Intel I5 at 3.2GHz, which is considered 1.21 times faster than the Pentium 4D. Note that the Seq-PR exhibits a longer average CPU time than the other two PR variants. This is due to one instance, 2-p2143 with 
                           
                              L
                              =
                              80
                           
                         and 
                           
                              L
                              =
                              150
                           
                        , in which it takes 3926s. The coverage values shown in Table 12 confirm the superiority of the Weight-PR variant. In particular, for any method M considered in this table, 
                           
                              C
                              (
                              Weight
                              -
                              PR
                              ,
                              M
                              )
                              >
                              C
                              (
                              M
                              ,
                              Weight
                              -
                              PR
                              )
                           
                        . As in the PDP described above, we complement this information with a scatter-plot of an instance (2-p559 with 
                           
                              L
                              =
                              150
                           
                        ) shown in Fig. 7
                        . This figure illustrates the superiority of the Weight-PR with respect to VNS and ACO in terms of obtaining a better approximation of the efficient frontier in this instance.

We applied the Friedman test for paired samples to the data used to generate Table 11. The resulting probability value is lower than 0.01, which clearly indicates that there are statistically significant differences among the five methods tested. The associated rank shows that the Weight-PR method is the best overall with an average rank of 1.66, followed by Seq-PR (2.04), ACO (3.48), Pure-PR (3.88), and VNS (3.94). We finally compare Weight-PR and Seq-PR with the Wilcoxon test and the Sign test. The resulting probability values of 0.01 and 0.007 respectively, indicate that Weight-PR is the clear winner between both methods.

From the analysis above, we can conclude that the Pure GRASP with Path Relinking, Pure-PR, is the best method for the PDP while in the BOP the best approximation of the efficient frontier is obtained with the Weight-PR. On the other hand, the proposed hybrid algorithm have exhibit a better performance than the previous methods for both problems. As a general comment, from the different achievement of the methods in these two problems, it is clear that when we face a new problem, we cannot discarda prioriany of these designs and we should test and compare them.

In the previous experiments, we run all the methods a single run over a set of instances and reported the average results. In this final experiment, we run the methods several times (independent runs) on a single instance, to evaluate their robustness. We report our results by means of box plots, which show how these executions are distributed. Each box contains 50% of the data, where the upper and lower ends of the box correspond to the first and third quartile, respectively. The line inside the box establishes the median of the considered data. Finally, the whiskers represent the maximum and minimum value of the data. For each algorithm, we conduct 10 independent executions. In order to obtain more robust conclusions, we consider 9 different instances.


                        Fig. 8
                         shows the box plots associated to the hyper-volume obtained by the five methods described for the PDP. This figure shows that the Pure-PR method clearly obtains the best results, while the MSPA obtains the worst ones. GP and Weight-PR obtain similar results when considering the median of the data. However, GP presents low diversification, i.e., each execution over each instance has a similar value. Therefore, when considering an average behavior, Weight-PR emerges as the second best procedure. Notice that these results are in line with those obtained in the previous experiments.


                        Figs. 9 and 10
                        
                         show a similar experiment but considering the epsilon and the R2 indicators, respectively. This experiment confirms again the superiority of the Pure-PR method. As it was previously described, the performance of Weight-PR and GP is similar. In particular, Weight-PR obtains better median of the epsilon indicator than GP in 6 out of 9 instances, while GP obtains better R2 median than the Weight-PR in 5 instances.


                        Fig. 11
                         shows the associated box plot of hyper-volume for 9 instances of the BOP. A first significant difference with respect to the PDP is that, in general, the upper whisker is larger in BOP (close to 1 in several instances) than in PDP (with a maximum value of 0.6). Another interesting observation is that the larger instances (p550t150, p562t80, and p2143t150) present a narrower distribution. This fact could indicate that these instances have a large basin of attraction in terms of the objective function values. Considering the median value, Weight-PR emerges as the best method (outperforming its competitors in 5 out of 9 instances). The remaining methods present a similar behavior, and in our opinion, it is not possible to say that a method improves the others.


                        Fig. 12
                         shows the box plot of the epsilon indicator. In this case, the maximum whisker is not as large as the one presented in the previous experiment. On the other hand, as in the case of the hyper-volume, the largest values and the largest variation in the distribution of the epsilon indicator appear in the smallest instances (p21t40, p32t50, and p33t60). In line with the previous results, the median value of the Weight-PR method slightly outperforms the median value of the other multiobjective algorithms.

Finally, Fig. 13
                         shows the box plot associated to the R2 indicator for the BOP. These results are less discriminant than the others since all methods obtain good values (small upper whisker and median). Therefore, this metric does not help us to rank the methods.

@&#CONCLUSIONS@&#

The objective of this study has been to advance the current state of knowledge about implementations of GRASP and Path Relinking procedures in the context of multiobjective optimization. We first reviewed previous applications of both metaheuristics to then establish a classification of the different ways in which they can be applied. Specifically, we considered three basic ways to implement them: pure, where each objective is optimized in isolation, sequential, where each objective alternates to guide the search, and weighted, where all the objectives are combined into a single master objective.

We considered two hard biobjective combinatorial problems, the path dissimilarity (PDP) and the bi-orienteering problems (BOP), to test the 26 different GRASP and PR variants proposed in the paper. We compare these variants with the best methods previously reported on 70 instances and the comparison favors some of our GRASP with PR implementations. An interesting conclusion of our study is that in each problem the best results are obtained with a different GRASP with PR variant. Specifically, in the PDP the pure variant achieves the best results, while in the BOP the weighted variant is the winner.

@&#ACKNOWLEDGMENTS@&#

This research has been partially supported by the 
                     Ministerio de Economía y Competitividad of Spain (Grant Refs. TIN2009-07516 and TIN2012-35632-C02). The authors thank Profs. Schilde, Doerner, Hartl, and Kiechle for making their results on the BOP available to them. The authors also thank Profs. Knowles, Caballero, Molina, and the three anonymous referees for their insightful indications to improve the paper.

@&#REFERENCES@&#

