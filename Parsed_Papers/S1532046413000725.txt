@&#MAIN-TITLE@&#Developing an expert panel process to refine health outcome definitions in observational data

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Expert panelists reviewed claims data to classify real health outcome cases.


                        
                        
                           
                           Challenges: data availability, extraction and presentation; case determination.


                        
                        
                           
                           Success factors: filtering and sorting data; consensus process to reach agreement.


                        
                        
                           
                           Future directions: additional outcomes and datasets; larger panelist groups.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Administrative claims

Expert panel

Observational

Dashboard

Health outcome of interest

@&#ABSTRACT@&#


               
               
                  Objectives
                  Drug safety surveillance using observational data requires valid adverse event, or health outcome of interest (HOI) measurement. The objectives of this study were to develop a method to review HOI definitions in claims databases using (1) web-based digital tools to present de-identified patient data, (2) a systematic expert panel review process, and (3) a data collection process enabling analysis of concepts-of-interest that influence panelists’ determination of HOI.
               
               
                  Methods
                  De-identified patient data were presented via an interactive web-based dashboard to enable case review and determine if specific HOIs were present or absent. Criteria for determining HOIs and their severity were provided to each panelist. Using a modified Delphi method, six panelist pairs independently reviewed approximately 200 cases across each of three HOIs (acute liver injury, acute kidney injury, and acute myocardial infarction) such that panelist pairs independently reviewed the same cases. Panelists completed an assessment within the dashboard for each case that included their assessment of the presence or absence of the HOI, HOI severity (if present), and data contributing to their decision. Discrepancies within panelist pairs were resolved during a consensus process.
               
               
                  Results
                  Dashboard development was iterative, focusing on data presentation and recording panelists’ assessments. Panelists reported quickly learning how to use the dashboard. The assessment module was used consistently. The dashboard was reliable, enabling an efficient review process for panelists. Modifications were made to the dashboard and review process when necessary to facilitate case review. Our methods should be applied to other health outcomes of interest to further refine the dashboard and case review process.
               
               
                  Conclusion
                  The expert review process was effective and was supported by the web-based dashboard. Our methods for case review and classification can be applied to future methods for case identification in observational data sources.
               
            

@&#INTRODUCTION@&#

Adverse drug events (ADEs) continue to be a common problem leading to significant morbidity, mortality, and financial costs [1–7]. Serious limitations exist for two frequently used ADE detection methods: clinical trials and post-marketing surveillance. Clinical trials use narrowly defined populations that, while relevant to determining medication efficacy, are usually too small to identify ADEs to a meaningful extent. Post-marketing surveillance mechanisms rely on spontaneous ADE reporting by clinical researchers, health care professionals, and the public. Both methods provide limited value in identifying ADEs. Accordingly, the Institute of Medicine (IOM) called for systematic use of automated health care databases from a variety of settings to actively monitor drug safety and efficacy [8]. Congress subsequently mandated that the FDA collaborate with a variety of groups to implement the IOM’s recommendation [9].

The FDA Sentinel Initiative was borne out of Congress’ mandate, and is focused on monitoring medical products throughout their entire life cycle by using data from large, disparate electronic data sources [10]. To this end, researchers are exploring approaches and methods for the use of large observational data sources for active safety surveillance, including the Mini-Sentinel project [11] and the Observational Medical Outcomes Partnership (OMOP) [12]. These initiatives are founded on the belief that active surveillance can be performed using administrative claims and electronic health record (EHR) data. These observational data sources offer several advantages: (1) the ability to analyze data from multiple sources covering a large number of subjects, (2) the data are collected as a routine part of the provision of care and do not rely on the additional time and effort required to submit a spontaneous report, (3) the sheer volume of data provides access to millions of records, better reflecting actual medication-related usage than demonstrated in a clinical trial, and (4) the current national focus on EHR adoption [13] suggests the amount of available data will only increase.

While observational data offer new opportunities, their use is not without limitations. One particularly concerning challenge is the ability to accurately identify health outcomes of interest (HOIs). Administrative claims data, for example, are designed for reimbursement, not clinical care documentation. When conducting research with claims data, the lack of standards has resulted in outcome definitions that rely on billing codes such as ICD-9-CM diagnosis codes or CPT procedure codes which may not accurately reflect a patient’s clinical status or care delivery because of problems including sloppy coding, up-coding (e.g., assigning procedural billing codes that commands higher reimbursement), or coding that reflects clinical work-up to rule out a diagnosis [14]. As is evident from the literature, when these codes are used in research, additional problems are introduced by the multitude of possible combinations of codes that are included to define an HOI [15]. For example, one study might define acute myocardial infarction as any ICD-9-CM diagnosis code beginning with 410, while another study might include only 410.2 and 410.4, and a third might require one of these eligible diagnosis codes plus a relevant coronary artery bypass graft procedures (e.g., CPT codes 33510-33536). Variability in which codes comprise definitions will impact measurement.

If observational data are to be used reliably for active drug safety surveillance, approaches to identifying HOIs must be improved. To first determine the variability across definitions, OMOP funded two independent systematic literature reviews to detail how studies of observational data defined 10 example HOIs [15]. They concluded that large variability exists in the literature, and no single definition of the HOIs they reviewed demonstrated clear superiority. As a result, a library of competing, and in most cases hierarchical, definitions for HOI measurement was developed [16]. Further research is needed to identify best practices for measuring HOIs in observational data.

We conducted methodological work to better understand how HOI definitions in the OMOP library compare, and to explore how these definitions might be refined. To do this, we developed a web-based dashboard to facilitate expert panel review of patient cases with competing HOI definitions. To ensure patient confidentiality, data needed to be under our control and securely accessed. Additionally, panelists needed to review the data from disparate locations with dynamic filtering and sorting capabilities. We also needed an efficient mechanism to collect panelists’ assessments and opinions for subsequent analysis, as well as the ability to conduct mediated disagreement resolution sessions that allowed simultaneous review of panelists’ evaluations and patient data. After considering our needs and available options, existing methods such as manual chart review and surveys were not deemed suitable.

Expert panelists were presented observational data from a sample of patient cases identified by competing HOI definitions, and were asked to provide opinions on whether they believed these patient data were consistent with having the HOI. Through dual, independent panelist review and a mediated consensus process for cases of disagreement, we were able to classify cases and create a modeling dataset for studying HOI measurement.

In this paper, we present our review process and the Web dashboard we created to efficiently present large volumes of data to panelists, capture panelists’ assessments, and resolve disagreements. Specifically, we describe the development and implementation of: (1) a systematic case review and consensus-building process; (2) a Web-based dashboard to present de-identified claims and laboratory data; and (3) a process to collect case review data in a manner enabling identification and analysis of concepts of interest that influenced panelists’ determination of presence/absence of specific HOIs.

@&#MATERIAL AND METHODS@&#

The Truven MarketScan Lab Database (MSLR), licensed by OMOP within their research lab on the Amazon EC2 cluster, provided the observational data source for this study. These data represent inpatient, outpatient, and pharmacy claims from approximately 1.5million privately insured patients from 2003 to 2007. The claims data are supplemented with laboratory results.

The MSLR data were transformed to the OMOP Common Data Model (CDM), which is a data transformation that allows consistent coding to be used across various types of observational data sources [17]. Data transformed to the CDM are amenable to use with publicly available OMOP tools for selecting cohorts (e.g., RICO [16]) and producing standardized summaries (e.g., OSCAR [18] and NATHAN [19]). Details on these tools can be found in the references cited. The CDM provides a mechanism for combining multiple occurrences of diagnosis codes and prescription drugs to define eras of conditions or drug treatments which simplifies the complex data presentation. For our data presentation, we relied on the following data tables from the CDM: condition eras (e.g., combining conditions with start and stop dates into eras), procedure occurrences, laboratory observations, visit occurrences, and drug eras (e.g., continuous periods of drug filling with start and stop dates).

Patients were sampled for competing HOI definitions using the HOIs of acute liver injury (4 of 7 definitions), acute kidney injury (4 of 4 definitions), and myocardial infarction (3 of 4 definitions) [16]. For each HOI, we sampled cohorts of patients using a modified version of OMOP’s Person-level Exploratory Data Review of Outcomes (PEDRO). The PEDRO algorithm identified patients that met one or more of our HOI definitions by assigning an identifier (Concept ID) to the patient record. The HOI definition-specific Concept ID was attached to the patient’s record with the first calendar date when all of the conditions for group inclusion for an HOI definition were met. This first occurrence date when a patient became part of the HOI cohort was recorded as the patient’s index date (time zero). For purposes of strict data de-identification, all elements of data were anchored to this index date, with time preceding the index date representing negative values and time following the index date representing positive values.

Due to the hierarchical HOI definition relationships, the patient cohort extraction process yielded patients meeting more than one HOI definition option. For example, acute liver injury used a hierarchical definition where the broadest definition only includes diagnosis codes, a secondary definition includes both diagnosis and procedure codes, and a third definition includes diagnosis codes, procedures codes, and laboratory values. Patients meeting the definition that includes diagnosis, procedures, and laboratory values would also meet the previous two definitions. Because sample sizes were smaller for the most restrictive definitions and we did not want to inadvertently exclude patients meeting the most restrictive HOI definitions, we randomly sampled (without replacement) from the more restrictive HOI definitions first, and then iteratively through definitions that were less restrictive. We sampled a total of 612 patients: 208 for acute liver injury, 200 for acute kidney injury, and 204 for myocardial infarction. Power calculations were not conducted, but rather the number of patients was based on feasibility.

Four panelists were recruited for each HOI. Using a modified Delphi method, these four panelists were divided into two panels to allow for independent, dual review of each patient case within a single HOI. For example, panelists A and B independently reviewed half of the 200 cases for acute kidney injury while panelists C and D independently reviewed the other 100 cases for this HOI. Each panel consisted of a specialist for the HOI being studied (i.e., a researcher with experience in observational studies of the HOI with or without a clinical degree) and a generalist. The generalists were required to have experience as a practicing physician and applied experience working with or interpreting administrative claims data.

The panelist dashboard was designed to allow efficient data presentation so panelists could determine the patient’s HOI status, as well as capture factors that influenced panelists’ HOI determination. This also required several administrative functional elements: (1) management of panelists, (2) assignment of patients to panelists, (3) viewing panelists’ assessment data, (4) exporting panelists’ assessment data, and (5) management of the disagreement resolution process. Study investigators managed these administrative functions. Additional functional elements of the dashboard included technical (i.e., “backend”) and user experience considerations. Technical functional elements supported the management of case review data that was presented to panelists. The user experience was largely dictated by the overall design of the dashboard, including navigation, layout, and access.

After reviewing patient data, panelists responded to a series of questions: Question 1 Do you believe this case represents the health outcome of interest? (yes/no); Question 2 To what extent do you agree or disagree that this is a case of the health outcome of interest? (1=Strongly Disagree and 10=Strongly Agree; Question 3 To what extent do you rate the severity of the patient’s health outcome of interest? (N/A, minor, moderate, severe, or fatal); and Question 4 Which of the following contributed to your decision?. Options included diagnoses, procedures, laboratory values, or drugs. Panelists indicated whether the presence, absence, or number of each option influenced their classification of the patient. The last question provided a grid illustrating combinations of types of events and their timing, and panelists were asked to check all that apply. Visits were added to this grid for the myocardial infarction panel since whether patients received inpatient versus outpatient care was identified by panelists as an important data element in their review. Question 4 also included a text box that allowed panelists to enter notes for the specific case. Kappa statistics assessed inter-rater reliability for initial case review.

Once initial case reviews were complete, answers from panelist pairs reviewing the same patient were compared with respect to their determination of the presence or absence of the HOI. We used a consensus process to resolve disagreement on this question. Disagreement resolution occurred during video conference meetings, with a shared desktop image of the dashboard so patient data could be reviewed as a group. Panelist pairs were simultaneously presented their answers to questions 1 (case classification), 2 (10-point likelihood scale), and 3 (severity scale), as well as any comments entered on question 4. The resolution process was led by 2 members of the research team. One member of the team controlled the shared desktop, initially presenting the panelists’ evaluations. Panelists discussed the factors influencing their decisions, with the researcher navigating through the patient’s data as dictated by the panelists’ prompts. The other research team member took notes from the discussion. After discussing each patient and the available data, the panelists identified a consensus answer for these three questions. The researcher recorded the consensus responses in the dashboard and moved to the next patient.

All work with patient data was conducted on the Amazon Elastic Compute Cloud (EC2), a web-based service that provides scalable computing capacity. For research staff or panelists to access the de-identified patient data presentation portion of the dashboard, a user ID and Web server password needed to be authenticated. This information was passed directly from a Web server on the Auburn University campus to the Amazon Web server. No users of the system had both the user ID and the Amazon server password. The Amazon EC2 was utilized for storage.

@&#RESULTS@&#

All project work occurred between June, 2011, and June, 2012. Web dashboard development took place during June through September, 2011, except for specific requests identified by panelists during the review process. The Dashboard development was an iterative process, whereby technical and clinical team members regularly met and reviewed prototype functionality. This process repeated until the live version of the dashboard was completed and made available to panelists. During October, 2011, acute liver injury patient samples were loaded in the dashboard and mock reviews were conducted by research staff to test dashboard functionality. Also, prior to initiating their full case review, each panelist pair reviewed 5 cases for understanding of the review process and dashboard functionality. Questions originating from this initial review were addressed prior to full case review. The first expert panel was launched in November, 2011.

The presentation included a number of data attributes that would facilitate the expert case review process. Available demographic information included age, sex, and eligible periods of observation with respect to the HOI index date. Clinical data were grouped as diagnoses, visits, procedures, laboratory values, and drugs. Data were presented with descriptive labels rather than corresponding claims-based code values. Data were presented with a “Start” number, indicating their occurrence date in relation to the patient’s qualification for the HOI of interest, such that positive values indicate the event (i.e., diagnosis, visit, procedure, laboratory value, and drug) occurred the corresponding number of days after meeting HOI criteria, and negative values indicating the event occurred prior to meeting HOI criteria.


                        Fig. 1
                         depicts the first screen that a panelist saw after selecting a patient for review. The screen’s layout was modeled after eBay Motors™, utilizing multiple filters and sorting options. eBay Motors™ is a division of the online auction site, eBay, that focuses specifically on the sale/purchase of automobile-related products. It was selected as the template for the dashboard because it provides efficient filtering and sorting functions that were deemed similar to the needs for this project. The actual patient data, in table form, subsume the majority of the window. Along the left side, starting at the top, the panelists had access to patient demographic data (age and gender), a “Days-to-index” slider, as well as filters. The Days-to-index slider allowed panelists to narrow or widen the date range from which data are displayed, anchored by the date of the first qualifying HOI-related observation. The “Eligible observation period” was defined as the range of days before and after the index date for which we had any available observational data. Panelists are also able to sort the data by date, as well as alphabetically, by clicking the appropriate header in the patient data table.

To make it as simple as possible for panelists to review data that would be most relevant to their HOI classification, we created filters for priority concepts. Priority concept indicators were added to all of the patient level data, which essentially allowed panelists to view custom reports based on data most relevant to the HOI. Available filters allowed panelists to filter data based on diagnoses, visits, procedures, laboratory values, and drugs. The priority concept filters were created through a multi-step process. First, we reviewed the literature and created a summary document for diagnoses, procedures, labs, and treatments relevant to each HOI. These documents were shared with the panelists and revised based on their feedback. Second, we used these documents to create a key word list for each HOI. These key words were then run against the descriptors in the claims, and observations matching a text field of the key word were retained as a priority concept. For example, if a descriptive text field for a procedure code contained one or more of the predetermined clinically meaningful text string items or key words, the concept id for that procedure code would be flagged as a priority concept. For drugs, we ran these key words against text strings for indications, contraindications, and off-label uses listed in the OMOP Standard Vocabulary, which were derived from the First DataBank NDDF Plus Source Vocabulary. For lab values, we ran the key word list against the Logical Observation Identifiers Names and Codes [20] (LOINC) database to identify the LOINC terms that would constitute the laboratory priority concepts.


                        Fig. 2
                         depicts several of the available sorting and filtering features. Specifically, this patient’s Days-to-index sliders have been adjusted to include the year before and the year after their first qualifying HOI-related observation. The figure shows the patient’s “Lab Values of Interest”, sorted by “Start” days-to-index. For lab values, panelists were presented the reported value (when available), the reference range, and the upper limit of normal. For Diagnoses and Procedures, panelists were able to further narrow the data to the previously defined concepts of interest. Visits could be narrowed to show any combination of “Inpatient Hospital”, “Outpatient Hospital”, and “Office” visits. Medications could be narrowed to “Indicated Drugs”, “Drugs Used Off-Label”, and “Contraindicated Drugs”. Filters were applied by first clicking the radio button to the left of the primary filter (e.g., “All Diagnoses”) and then selecting the checkbox for the appropriate secondary filter (e.g., “Diagnoses of Interest”).


                        Fig. 2 also depicts a highlighting feature within the dashboard, which allowed panelists to mark data of interest by clicking on the line of interest. A single click evokes a yellow highlight, signifying that the panelist finds the observation generally interesting. Clicking the same line of data a second time turns the row green, signifying that the panelist identifies the observation as a positive indicator for the HOI. A third click turns the row red, signifying that the panelist identifies the observation as a negative indicator of the HOI. A fourth click removes all highlighting. We incorporated this feature for two reasons: (1) to assist panelists in their final case determination, and (2) to facilitate capture of those factors believed by the panelist to influence their assessment of the likelihood that this is a ‘true’ case of the HOI.

Panelists then clicked “View selected rows” above the demographic data to view only the highlighted observations, as depicted in Fig. 3
                        . The colored highlights, as determined by the panelist, were visible, along with the other data elements for each observation that had been highlighted. This intermediate step provided panelists with a quick, focused view of the factors that they believed were important to case determination prior to entering their assessment.

Panelists entered their assessment (described above) on the screen shown in Fig. 4
                        . Prior to arriving at this page, the panelists have already viewed the patient data, filtered, sorted, and highlighted as necessary, and are now ready to assess the likelihood that this patient is a case of the HOI.

Several functional abilities were incorporated into the dashboard to support project administration, including: (1) management of panelists’ accounts, (2) assignment of panelists to patients, (3) viewing panelists’ assessment data, (4) management of the consensus building process, and (5) exporting panelists’ assessment data. Once panelists began case review, administrators needed to be able to monitor panelists’ progress and identify any potential problems with the case review process. Project administrators were able to view all highlighted rows by HOI. The dashboard also identified individual patient cases in which panelist pairs disagreed on the presence of the HOI. After consensus was reached, the dashboard exported panelists’ assessment data to Microsoft Excel for analysis.


                        Fig. 5
                         depicts the administrator view of patient cases that have been assessed by at least one panelist. In the figure, the HOI is liver injury (top left corner of the screen). In the blue row, “userID” is the panelist who evaluated the case, and “Patient” signifies the particular patient case. When checked, the box below “ALI” (Acute Liver Injury) indicates the panelists answered “yes” to Question 1 of the assessment (Fig. 4). “Certainty” corresponds with Question 2. “Severity” corresponds with Question 3. Columns “Q4A” thru “Q4F” correspond, in order, to the various parts of Question 4. For example, “Q4A” corresponds to “No occurrences BEFORE index date”. The four-lettered combinations of Ns and Ys under “Q4A” - “Q4F” correspond, in order, with panelists’ responses to the “Diagnosis,” “Procedures,” “Laboratory Values,” and “Drugs” portion of question 4. Here, “N” corresponds to an unchecked box, and “Y” corresponds to a checked box. “Time” signifies when the panelist completed the assessment, and “Description” corresponds to comments entered in the Question 4 textbox.


                        Fig. 6
                         depicts the consensus building screen. “liverGroupB” is the consensus user account. Panelists’ independent case determinations (Fig. 4) are depicted in the first two rows below the patient’s data. The consensus panel moderator entered final determinations in the third row, and this became the final data for disagreement cases. The two acute liver injury panels had 16 and 18 cases requiring consensus discussion. The two acute kidney injury panels had 52 and 26 consensus cases, and the two myocardial infarction panels had 20 and 21 cases. Consensus discussions for liver and MI lasted approximately 1h each; acute kidney injury discussions lasted approximately 2h and 1h 30min, respectively.

@&#DISCUSSION@&#

The primary focus of this methods research was to design, develop, and implement a systematic case review and consensus-building process for expert panelists’ review of de-identified observational data using a Web-based dashboard. Existing OMOP tools such as RICO and PEDRO allowed for efficient identification of patients and the CDM structure could be intuitively presented to panelists. The expert panel review could be conducted remotely from anywhere with high speed internet connectivity allowing connection to the Amazon cloud. The review process provided modeling data sets that can be used to create better case definition methods. While previous research has explored numerous methods to identify and validate medication-related health outcomes of interest in a variety of data sources, we are aware of no studies utilizing a distributed research network of expert panelists to conduct Web-based case reviews from a distance with a shared dashboard for case evaluation and disagreement resolution [12,21–27].

Panelists’ anecdotal comments indicated that the dashboard was efficient and easy to use, including the initial learning curve. This was supported by receipt of few questions beyond basic navigation functions after panelists’ initial review of 5 cases to become familiar with the process and only one request for assistance via the built-in Help feature during the entire course of the project. Panelists indicated that the time required to complete a case review was reasonable once becoming familiar with the dashboard, suggesting a usual review time of 1–5min. Panelists’ comments to the research team highlighted a primary challenge of working with observational data: case determination was based solely on the available data. During consensus discussions, panelists often suggested more data would help guide their final decision. For example, the data source sometimes lacked actual laboratory results corresponding to a laboratory order. This is a limitation of the data source, not of our review process or dashboard.

The method by which we implemented the severity question on the panelist’s assessment screen is a limitation in our approach. We added this question based on the recommendation of a reviewer after their initial review of 5 patients to become familiar with the dashboard and process. Although all panelists were presented this question for all patients, we did not explicitly review criteria for each level of severity with each panelist. As a result, panelists’ perceptions of severity may not be consistent. However, discussions during the disagreement resolution process suggested that panelists’ internal criteria for levels of severity were similar.

We implemented a robust, Web-based dashboard that facilitated case review by panelists from disparate locations. Panelist and patient case management within the dashboard was a core functionality that performed consistently. The dashboard performed reliably for data presentation, panelist assessment, and assessment data capture functions. Discussions with panelists suggested that panelists’ first step in narrowing the available data included sorting the labs of interest alphabetically, and then examining the lab results in relation to the index date. While the specific HOI sometimes influenced whether labs or diagnoses were examined first, we observed this behavior consistently from panelists. These observations underscore the importance of filtering and sorting to enable panelists’ review, validating the multi-step process of creating priority concepts. For some HOIs, we found that case review could be expedited by modifying the default range for the days-to-index slider to reflect clinical practice. For example, the default range was changed to the 60days before and 60days after the index date for myocardial infarction to assist panelists’ assessment of any observations that could inform their case determination.

The highlight feature allowed panelists to create a color-coded clipboard of important data for making their assessments. This helped with efficiency in their review process. The highlighted data also provided our research team with indicators of what panelists were focusing on. Our future research will explore and model these highlighted data as one means of trying to refine case definitions for HOIs.

The consensus-building process worked well. Although we anticipated a consensus-building process would be necessary, we did not initially build this function into the dashboard. Once constructed and tested, the function supported a total of 6 consensus discussions. Panelists valued seeing their paired reviewer’s assessment and comments. During consensus discussions, it was not uncommon for one panelist to identify an important observation that the other overlooked. It was also common for panelist pairs to agree that a specific laboratory test, diagnosis, or other observation would substantially impact their ability to determine the status of the HOI.

We believe that our methods research produced a reliable, scalable systematic case review and consensus-building process for expert panelists’ review of de-identified observational data using a Web-based dashboard. We believe the ability to perform reviews anywhere (with Internet access) at any time is an important feature of our methods. We refined our processes for data management and panelist training, case-review, and consensus-building through each subsequent HOI. Our work is limited by the total number of HOIs and panelists who participated in the project. Future work should explore other HOIs and include larger numbers of panelists to identify additional modifications that are necessary in the dashboard and review process. The process we describe provides tools to efficiently collate, filter, and sort electronic data. However, additional work is needed to directly compare our methods to existing methods.

@&#CONCLUSIONS@&#

An expert panel review of patient data was effectively facilitated by our Web-based dashboard. Panelists were able to review patient cases and classify whether identified cases were true or false positives. Data generated from this process can be used to refine case identification methods, which ultimately will improve the ability to identify drug-related adverse events in observational data. This approach may be useful in other areas of research and quality assurance that require detailed record review. Its utility may improve substantially in those data sources populated with more extensive standardized information.

This research is funded by a grant from the Foundation for the National Institutes of Health (HANSE11OMOP).

This research has been approved by the Auburn University IRB (protocol 11-161 EP 1105).


                     
                        
                           Conception and design: BIF, JCH, RAH

Analysis and interpretation of data: RAH, BIF, MDG, JCH, JG

Drafting the article: BIF and JCH

Critical revision of the article for important intellectual content: BIF, JCH, RAH, MDG, MLH

Final approval of the article: BIF, JCH, RAH, MDG, MLH, JG

Obtaining funding: RAH, MDG, BIF

Administrative, technical, or logistical support: MLH, JG, MDG, BIF

@&#ACKNOWLEDGMENTS@&#

We express our sincere thanks to the OMOP leadership team, especially Emily Welebob, Marc Overhage, MD, PhD, and Paul Stang, PhD, for their support and guidance throughout this project. We greatly appreciate the time and effort devoted to this project by the following expert panelists: Themistocles Assimes, MD, PhD (cardiologist, Stanford University); Andrew Bomback, MD, MPH (nephrologist, Columbia University); T. Craig Cheetham, PharmD, MS (liver expert, Kaiser Permanente); Jon Duke, MD, MS (internist and drug safety expert, Regenstrief Institute); Mei Sheng Duh, MPH, SCD (acute liver injury and methods expert, Analysis Group); Gregory Hess, MD, MBA, MSc (emergency medicine and methods expert, University of Pennsylvania; Ben Powers, MD, MHS (internist and methods expert, St. Luke’s Health System); Scott Sanoff, MD, MPH (nephrologist, University of Virginia).

@&#REFERENCES@&#

