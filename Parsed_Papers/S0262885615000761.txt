@&#MAIN-TITLE@&#Developing a contactless palmprint authentication system by introducing a novel ROI extraction method

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A novel unrestricted, contactless palmprint image acquisition device is developed.


                        
                        
                           
                           A novel model-based ROI extraction method, robust to the small errors is proposed.


                        
                        
                           
                           A contactless palmprint DB is constituted by collecting 1752 images from 145 subjects.


                        
                        
                           
                           The images in DB have pose, rotation, position variations and cluttered backgrounds.


                        
                        
                           
                           The results achieved by the proposed system are encouraging: 0.277% EER and 99.488% RR.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Contactless palmprint authentication system

Model-based ROI extraction

Palm direction estimation by nonlinear regression

Contactless palm database

AAM-based palm segmentation

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

The recent research attention focused on the improvement of palmprint recognition systems can be ascribed to its commercialization potentials in real-world biometric applications. As a result, most of the end-users of this technology seek to deploy secure, accurate, hygienic biometric identification solutions. A contactless palmprint acquisition device can be the ultimate solution to alleviate such hygiene concerns. For this reason, the trend has been toward the contactless acquisition devices and developing robust approaches for contactless hand-based biometrics [1–4].

This paper presents a novel contactless palmprint recognition system with a novel region of interest (ROI) extraction method. The users present their hands without touching any guidance peripheral. Furthermore, there is no limitation about the scene behind the users' hand.

@&#RELATED WORKS@&#

In the literature, the first contactless palmprint image acquisition and recognition system was developed by Han et al. in 2007. This system [5] consists of a standard PC and an image capturing device that captures two images for each hand by two cameras. One of them is a NIR (near infrared) camera that captures NIR images with NIR lighting to segment hand regions under unconstrained scenes by hardware. The other one is a CMOS web camera used for capturing color hand images. In this system, users have to open their hands and place them toward the camera at a distance from 35cm to 50cm.

Michael et al. developed three different contactless palmprint recognition systems [6,3,7]. Their first developed system [6] involves a web camera, a fluorescent light and an enclosure. The segmentation of the hands was achieved by a simple algorithm (skin-color thresholding). In Ref. [3] where a multi-modal palmprint and knuckle print system was presented, the lighting source was replaced by a bulb emitted yellowish light. They also included a dimmer to adjust the brightness of the light bulb. The third system [7] uses additionally two biometric models: hand vein and hand geometry. To obtain the vein images the system involves a second camera, infrared (IR) filter, a number of infrared LEDs, and a diffuser paper.

In the recent work [8], Sato et al. have been proposed to normalize the deformation arising from the different hand postures, for reliable palm image matching. Their contactless system consists of a diffraction grating laser and a high-speed camera. The geometric correction of the image has been achieved by the measured 3D points with the use of diffraction grating laser and grid pattern.

Other recent studies also include the mobile contactless biometrics [1] and the simultaneous use of 3D and 2D hand information in the contactless systems [2]. Although the mobile contactless palmprint systems allow ease of use, their recognition performances are not satisfactory at present. On the contrary, the proposed system in Ref. [2] gives superior recognition performance in unrestricted schemes as compared to the mobile contactless systems, but the cost and computation complexity of the system are very high, and the 3D digitizer can be affected by the ambient noise.

One of the important considerations in the contactless setting is how the palm can be segmented from hands that have pose and orientation variations within unrestricted background. Some of the recently proposed contactless palmprint systems [1,6] use monotone backgrounds for palm capturing to eliminate the complex segmentation problems. The works performed in Refs. [3,6], for instance, use skin color based segmentation and variations. But, this approach gives poor segmentation results for the images that have skin-colored backgrounds.

Because the use of pixel-based techniques leads to errors and in some scenarios they are not adequate for correct segmentation, Doublet et al. [9] proposed to improve the skin color modeling approach with active shape model (ASM). In our earlier work [10], we have proposed to use Active Appearance Model for palm segmentation within unrestricted backgrounds which is also used in this study. This method makes use of a combined statistical model of shape and texture together to segment the palm images.

ROI extraction is one of the most important and influential steps for the palmprint recognition/verification which is not emphasized as required in the papers although it can directly affect the results. Almost all the methods presented in the literature have extracted the ROIs with point wise manner by using the valley points of the gaps between the fingers [11,12]. The major steps of the most common method proposed by Zhang et al. [11] are as follows: (i) Convert the grayscale image to a binary image. (ii) Obtain the boundaries of the gaps between the fingers. (iii) Calculate the tangent of the upper and lower gaps, which represents the y axis. (iv) Draw a line (x axis) passing through the midpoint of two valley points which is perpendicular to these points. (v) Extract a fixed size region based on the coordinate system.

In Ref. [12], Connie et al. proposed to utilize from the gap between middle and ring finger. They drew two lines passing through the valley points of the gaps and assigned the two corner points of the square region as the midpoint of the index finger and little finger on these lines. Similarly, Badrinath and Gupta [13] developed a new method inspired by the works of Connie et al. and Zhang et al. This method uses the tangent of the two gaps (reference line) between the fingers. And the two corner points of the square region are assigned as the midpoints of the fingers on the lines drawn at specific angle with respect to the reference line.

The researchers in Ref. [6] have introduced an enhancement to ROI extraction which utilizes from the pixel neighborhoods. According to this approach, a pixel can be considered as a valley point if some of the neighboring points lie in the non-hand region while the most of the neighbors are the points of the hand region. In the work presented by Leng et al. [14], the regions of valleys were determined after some morphological operations. And centroids of the regions of the candidate valley points that satisfy all conditions (4, 8 and 16 point check) were chosen as the final valley points. In another paper [15], the valley points of the gaps between fingers were selected by the modified Harris Corner Detection algorithm.

Unlike the others, Li et al. [16] presented a ROI alignment refinement method based on the principal lines to improve the verification accuracy. They applied the iterative closest point method to estimate the transformation parameters of the images. The transformation was performed and the new aligned ROI was then produced.

In the literature, there are many types of palmprint recognition algorithms: Local feature-based [11], holistic-based [17], and hybrid [18]. Local feature-based approaches extract local features from palmprint images and use a matcher to compare them [11,4,19]. Holistic-based approaches extract the features by regarding a palmprint image as a whole image, or a high dimensional vector. Appearance-based methods also called subspace methods [17,20] are involved in this group. In many subspace methods, the feature extraction methods (such as PCA, LDA or LPP) are combined with the spectral representations. The Gabor feature based kernel Fisher Discriminant Analysis which gives superior results than other feature extraction methods has been proposed in our recent study [21]. Hybrid methods [18], which utilize from more than one feature extraction approaches were proposed to improve the recognition accuracies [22].

We have endeavored to develop a novel contactless palmprint recognition system. Our contactless system consist of a CCD camera, a DC auto iris lens, LED light sources, a standard PC, a touch screen and an enclosure to acquire palmprint images, as shown in Fig. 1
                        . The main contributions are as follows:
                           
                              •
                              A novel contactless device for high quality palmprint image acquisition in real environment is developed. The trade-off between quality and flexibility (free hand placement in unrestricted environment) is optimized for high recognition results.

As a novelty, the usage of fitted palm model and nonlinear regression (Least Squares Support Vector Regression) to extract the region of interests (ROI) of the palms is proposed. This model-based approach provides reliable ROI extraction that is robust to the small segmentation errors. Whereas point wise approaches are substantially affected from these errors.

One of the most important stages in developing a contactless palmprint recognition system is to accurately segment freely positioned hands under unconstrained scenes. For that consideration, it is proposed to use Active Appearance Model [23] method for palm segmentation. In this study, the AAM method is configured specifically to achieve palm segmentation accurately for the contactless palmprint authentication system.

A novel database for contactless system is constituted by collecting 1752 images from 145 different subjects who present their hands in various directions, positions under unrestricted environment.

In this work, our earlier restricted palmprint identity verification system [21] has been further developed to achieve an unrestricted contactless palmprint verification system. External view of the system is shown in Fig. 1. This system involves a standard PC with Intel Core2 Quad CPU, 4GB RAM and 500GB HD to execute authentication program written in C++ programming language.

The capturing device is a low-cost CCD camera with DC auto iris lens which has 28mm focal length, maximum aperture of f/2.8 and wide angle of vision for viewing hand region completely. Shutter speed, focal aperture and focus adjustments of the camera have been made for the best image quality. Although the images taken with a low f-number gives focused hand images at a distance and out of focused background (leads to better segmentation), it is not suitable for the contactless biometric systems. The reason is that the distance between the hand and camera can be varied in different sessions which results in unfocused palm images. Furthermore, faster shutter speed can provide sharpened images for shaken hands, but it needs more illumination which removes the details of the palm texture.

To reveal the details in the palm image and to reduce the negative effect of the ambient lighting variations, LED-based light sources were located at a specific location and with a certain angle (as shown in Fig. 1). Furthermore, the intensity of the LED light was adjusted to an optimal value. The bottom light source was located to remove the side effects of the 3D posture variations, and the upper light source was located at the right side of the enclosure with 45 degrees to make the wrinkles and ridges visible.

The subjects interact with the system by using a touch screen. In this way, the verification errors caused by the erroneous usage of the system are decreased. The camera and the light sources are controlled by an electronic interface which communicates with PC via a serial port interface. The upper surface shape of the enclosure is formed as a trapezoid hole due to the concordance with the hand shape. Furthermore, wooden plates on two sides of the enclosure were added for ensuring the proper presentation of the hands to the system. Users' hands do not touch these wooden plates. They are included to the system in order to guide users to present their palms correctly with appropriate hand-camera distances. Thus, the probable presentation errors which lead to unfocused palmprint images can be prevented as much as possible.

The acquisition of the palm image is realized in such a way that the user presents his/her hand about 20cm above the CCD camera. There is no other restriction to the users and to the background scenes.

In our study, the segmentation of the hand was realized by an advanced model based segmentation method named as AAM method. The construction of the appearance model was done by using our earlier proposed palm model with 25 landmark points [10].

The AAM method [23] consists of two stages; model construction and model search. In both stages the shape vector, s, is represented by the coordinates of the ℓ landmark points, s
                           =(x
                           1,
                           y
                           1,...,
                           x
                           
                              ℓ
                           ,
                           y
                           
                              ℓ
                           )
                              T
                           . Then, a shape variation, modeled based on PCA, is defined by a linear combination of a mean shape, 
                              
                                 s
                                 ¯
                              
                           , and n shape base vectors, P
                           
                              s
                           :
                              
                                 (1)
                                 
                                    s
                                    =
                                    
                                       s
                                       ¯
                                    
                                    +
                                    
                                       P
                                       s
                                    
                                    
                                       b
                                       s
                                    
                                 
                              
                           where b
                           
                              s
                            is the shape parameter vector that controls the synthesized shape. After the mean shape is calculated, the training textures are warped onto the mean shape, 
                              
                                 s
                                 ¯
                              
                           , in order to obtain shape free textures. Then, the texture model is given by:
                              
                                 (2)
                                 
                                    g
                                    =
                                    
                                       g
                                       ¯
                                    
                                    +
                                    
                                       P
                                       g
                                    
                                    
                                       b
                                       g
                                    
                                 
                              
                           where g is the synthesized shape free texture, 
                              
                                 g
                                 ¯
                              
                            is the mean texture vector, P
                           
                              g
                            is the eigenvector of texture and b
                           
                              g
                            is the texture parameter vector. Combined appearance model parameters are obtained by concatenating weighted shape parameters and texture parameters.

In the model search, the texture error (E) between the model texture (g
                           
                              m
                           ) and the image texture (g
                           
                              s
                           ) is evaluated with the Euclidean distance and minimized by tuning the model and pose parameters. The optimization of the model is realized by assuming a linear relationship between parameter changes, δ
                           c, and pixel differences, δ
                           g, for facilitation of the computation.
                              
                                 (3)
                                 
                                    δ
                                    c
                                    =
                                    A
                                    δ
                                    g
                                 
                              
                           where the matrix A is estimated by the regression at the training stage. For large models, regression has been replaced by estimating the Jacobian, ∂(δ
                           g)/∂c, over the training set. This gives better results and far less computation overhead. More details on the AAM method can be found in Ref. [23].

In the palm modeling, 25 anatomical landmark points (as shown in Fig. 2
                           ), corresponding to the borders of the shape, are manually selected, as in Ref. [10]. Ten of them describe the end points of the palm model and can be located by sliding their positions forward or backward along the border of the hand shape. This flexibility makes the presentation of the whole fingers or upper/lower palm arcs unnecessary (presenting approximately 50% of the upper and lower palm arcs is sufficient).

The rest 15 landmark points are also used to model the gaps between the fingers, the upper and lower palm arcs. Each gap in the model is represented by three points (valley, left and right corner points).

In our studies, to implement the AAM method, we used the open source AAM API library [24], coded in C++ programming language by M. Stegmann. Use of this library, allows the users to change many parameters of the improved version of the AAM method. The applied parameter adjustments determined as optimum are listed below:
                              
                                 •
                                 In the model construction stage, the eigenvectors which are associated with the smallest eigenvalues were truncated (98% of the sum of the eigenvalues were involved).

The optimization of the model was realized by estimating the Jacobian over the training set.

The initialization of the model search was performed by scanning a 5×5mesh on the test image for all model parameters. Likewise, orientation and scale of the models were set to the optimal values (five scalings (0.75, 0.85, 1.0, 1.15, 1.25) and five orientations (−20, −10, 0, 10, 20)).

The model was fitted in a two-stage procedure. In the first stage, the optimum 15 models which have the minimum model reconstruction errors were determined and saved. In the second stage, fine grain search was performed in these 15 models only. The fine grain search was achieved by pattern search approximation. The numbers of the iterations were selected as 15 and 25 for the first and second stages, respectively. The convergence level was set to 0.00001.

Damping factors [25] were used to improve the fitting performance and determined as 1.0, 0.5, 0.25, 0.12, 1.25, and 1.5.

After palm image has been segmented, palmprint region, called region of interest (ROI), is extracted for subsequent processes. This region generally has a squared shape and involves sufficient information to represent the palm.

A novel regression-based approach is proposed for the extraction of ROI which utilizes the hand shape model. Almost all the other approaches in the literature are pixel-based, and hence a possible inaccurate selection of the hand valley points will lead to vast erroneous extraction of ROI. The model-based approach proposed in this study prevents the weakness of the pixel-based approaches by using the palm model as a whole in the regression. The major performance difference between the model and pixel-based approaches arises from the accurate determination of the hand orientation.

Although the proposed approach is suitable for model-based methods, it can be easily adapted to the traditional segmentation methods by determining landmark points.

The four major steps in the extraction of the ROI are as follows:
                           
                              
                                 Step 1. In our method, the orientation of each palm is characterized by the slope (m) of a line (y=mx+n) which shows the main direction of the palm. As a first step, a training set that contains hand images is constituted from different shaped hands. Then the slope of each palm in this set is manually determined by a supervisor and enrolled into a palm slope database. These slopes are used to estimate the orientation of each palm as a function of landmark points in the following steps.


                                 Step 2. The orientation of each palm is associated with the positions of the landmark points which constitute the palm model, and trained with a nonlinear regression method. Thus, the small errors at the determination of the landmark points do not affect the palm orientation. In our study, we have used Least Squares Support Vector Regression (LS-SVR) [26] method with RBF kernel. This method is selected due to the following reasons: low computational cost, performance, and enabling non-linearity for complex structures. The method is given by the following optimization problem:
                                    
                                       (4)
                                       
                                          m
                                          i
                                          
                                             n
                                             
                                                w
                                                ,
                                                b
                                                ,
                                                e
                                             
                                          
                                          
                                             J
                                             P
                                          
                                          
                                             w
                                             e
                                          
                                          =
                                          
                                             1
                                             2
                                          
                                          
                                             w
                                             T
                                          
                                          w
                                          +
                                          
                                             γ
                                             2
                                          
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                N
                                             
                                             
                                                
                                                   e
                                                   i
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       (5)
                                       
                                          subject
                                          
                                          t
                                          o
                                          
                                          
                                             y
                                             i
                                          
                                          =
                                          
                                             w
                                             T
                                          
                                          φ
                                          
                                             
                                                s
                                                i
                                             
                                          
                                          +
                                          b
                                          +
                                          
                                             e
                                             i
                                          
                                          ,
                                          i
                                          =
                                          1
                                          ,
                                          ...,
                                          N
                                       
                                    
                                 where y
                                 
                                    i
                                  and s
                                 
                                    i
                                  are considered as the orientation of a palm (m), and a vector formed by the positions of the landmark points (s
                                 
                                    i
                                 
                                 =[P1
                                    x
                                 , P1
                                    y
                                 , P2
                                    x
                                 , P2
                                    y
                                 ,…,P25
                                    x
                                 , P25
                                    y
                                 ], respectively. 
                                    φ
                                    
                                       ⋅
                                    
                                    :
                                    
                                       ℝ
                                       n
                                    
                                    →
                                    
                                       ℝ
                                       
                                          n
                                          h
                                       
                                    
                                  is a mapping to a high dimensional feature space. e
                                 
                                    k
                                  and b are the error and bias terms in the formulation. N is the number of samples and γ represents the error tolerance. This optimization problem means that the distances between the separating hyperplane and the training samples are precisely “1” unit (with the allowed error which is adjusted by the squared loss function).

The kernel form of the above problem is obtained by constructing the Lagrangian, eliminating w, e, and applying kernel trick. The final formula is represented by the linear Karush–Kuhn–Tucker system:
                                    
                                       (6)
                                       
                                          
                                             
                                                
                                                   
                                                      0
                                                   
                                                   
                                                      
                                                         1
                                                         v
                                                         t
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         1
                                                         v
                                                      
                                                   
                                                   
                                                      K
                                                      +
                                                      
                                                         γ
                                                         
                                                            −
                                                            1
                                                         
                                                      
                                                      
                                                         I
                                                         N
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      b
                                                   
                                                
                                                
                                                   
                                                      α
                                                   
                                                
                                             
                                          
                                          =
                                          
                                             
                                                
                                                   
                                                      0
                                                   
                                                
                                                
                                                   
                                                      y
                                                   
                                                
                                             
                                          
                                       
                                    
                                 where I
                                 
                                    N
                                 
                                 ∈ℝ
                                    N
                                    ×
                                    N
                                  is an identity matrix, K is the kernel matrix, and y
                                 =[y
                                 1;…;
                                 y
                                 
                                    N
                                 ], 1
                                 
                                    v
                                 
                                 =[1;…;1], α
                                 =[α
                                 1;…;
                                 α
                                 
                                    N
                                 ].


                                 Step 3. For the test sample, a palm direction (see Fig. 2) which is searched for the ROI extraction is then estimated by using the palm model at the regression. This is achieved by the following formula in LS-SVR:
                                    
                                       (7)
                                       
                                          y
                                          
                                             s
                                          
                                          =
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                N
                                             
                                             
                                          
                                          
                                             α
                                             i
                                          
                                          k
                                          
                                             s
                                             
                                                s
                                                i
                                             
                                          
                                          +
                                          b
                                       
                                    
                                 where k is the kernel function, and s is the test sample.


                                 Step 4. The upper and lower lines are then obtained as parallel lines to the palm direction and passing tangentially to the curves of the upper and lower palm arcs (see Fig. 2). The width of the palm can be determined by measuring the distance between these two lines. A line which is perpendicular to the upper and lower lines and tangent to one of the gaps between the fingers that does not intersect with the other gaps at two points is defined as a reference line. This line can be visualized as the innermost gap between the fingers. The distance between the upper and lower lines, L, is then used to extract the ROI of the palm, and is shown in Fig. 2. To form the ROI, the upper, lower and reference lines have to be shifted toward the center of palm by 0.07L, 0.07L and 0.02L, respectively. The resultant square shaped ROI with edge length 0.86L is formed between the intersections of these lines. The shifting amount and the size of the ROI are determined empirically with the aim of reducing the negative effects caused by the various hand postures. This approach also ascertains to retain the maximum palmprint information. The position of the lines and the extracted ROI are shown in Fig. 2.

Finally, since the ROI sizes can be varied due to the distance between the hand and the camera, the extracted ROI images are resized to a 128×128pixel resolution. Furthermore, the hands can be also in different sizes.

In our earlier work [21], the Gabor-based kernel Fisher discriminant analysis method for online palmprint verification system has been employed to control the entries of the PC laboratory of our department since October 2009 [27] with a great success. In this paper, this method is implemented to achieve contactless palmprint recognition/verification experiments. The method consists of two stages: Gabor wavelet feature representation of the palmprint images, and kernel Fisher discriminants for meaningful feature extraction (which has more discriminative power).

In Gabor wavelets, basis function is Gabor kernel which is Gaussian modulated by a complex sinusoidal:
                              
                                 (8)
                                 
                                    g
                                    
                                       x
                                       y
                                    
                                    =
                                    
                                       
                                          1
                                          
                                             2
                                             π
                                             
                                                σ
                                                x
                                             
                                             
                                                σ
                                                y
                                             
                                          
                                       
                                    
                                    exp
                                    
                                       
                                          −
                                          
                                             1
                                             2
                                          
                                          
                                             
                                                
                                                   
                                                      x
                                                      2
                                                   
                                                   
                                                      σ
                                                      x
                                                      2
                                                   
                                                
                                                +
                                                
                                                   
                                                      y
                                                      2
                                                   
                                                   
                                                      σ
                                                      y
                                                      2
                                                   
                                                
                                             
                                          
                                          +
                                          2
                                          πjWx
                                       
                                    
                                 
                              
                           where σ
                           
                              x
                            and σ
                           
                              y
                            are the horizontal and vertical standard deviations of Gaussian envelope, and W is central frequency. Gabor wavelets can be obtained with dilations and rotations of the Gabor kernel.

Because Gabor functions form a non-orthogonal basis set, filtered images has redundant information. Manjunath and Ma [28] developed a strategy to reduce this redundancy. In our experiments, to reduce the computational cost, this scheme was used with five different scales and four orientations of Gabor wavelets. Parameters of the Gabor function were selected as in Ref. [21].

Fisher linear discriminants (FLD) aim at finding the best separation hyperplane via maximizing the between-class variances (S
                           
                              B
                           ) and minimizing the within-class variances (S
                           
                              W
                           ). Nonlinear version of FLD, called KFD, can be implemented in a similar manner by adding the kernel trick [29]. Alternatively, Mika et al. [30] proposed to use convex quadratic programming (optimization) problem for large number of samples:
                              
                                 (9)
                                 
                                    
                                       
                                          min
                                          
                                             α
                                             ,
                                             b
                                             ,
                                             ξ
                                          
                                       
                                    
                                    
                                       
                                          ξ
                                       
                                       2
                                    
                                    +
                                    C
                                    P
                                    
                                       α
                                    
                                 
                              
                           subject to:
                              
                                 (10)
                                 
                                    K
                                    α
                                    +
                                    1
                                    b
                                    =
                                    y
                                    +
                                    ξ
                                 
                              
                           
                           
                              
                                 (11)
                                 
                                    
                                       1
                                       i
                                       T
                                    
                                    ξ
                                    =
                                    0
                                    ,
                                    i
                                    =
                                    1
                                    ,
                                    2
                                 
                              
                           for α,
                           ξ
                           ∈ℝ
                              ℓ
                           , and b,
                           C
                           ∈ℝ, C
                           ≥0. Besides, define 1
                           ∈ℝ
                              ℓ
                            as the vector of all ones. Here, K is the kernel matrix, b is the bias term, C and P are regularization function and the regularization parameter. The term ‖ξ‖2 minimizes the variance of the error. The first constraint pulls the outputs of the each sample to its class and the second ensures that the average outputs for each class is the label. Then, the KFD features for the test sample x is computed by:
                              
                                 (12)
                                 
                                    
                                       
                                          w
                                          ⋅
                                          Φ
                                          
                                             x
                                          
                                       
                                    
                                    =
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          l
                                       
                                       
                                    
                                    
                                       α
                                       i
                                    
                                    k
                                    
                                       
                                          x
                                          i
                                       
                                       x
                                    
                                 
                              
                           where k() is the kernel function and ℓ is the number of training samples.

The KFD explained above can be easily extended to multi-class problem with the output coding techniques. In our experiments we used RBF kernel and one-versus-all (1vsA) output coding techniques due to the achieved superior performance compared to other approaches.

In this study, weighted Euclidean distance (WED) [31] has been selected for the similarity measurement which gives superior results than other metrics (such as Euclidean, Manhattan and cosine). It is defined as follows:
                           
                              (13)
                              
                                 
                                    d
                                    k
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                d
                                             
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      f
                                                      
                                                         i
                                                      
                                                      −
                                                      
                                                         f
                                                         k
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                
                                                2
                                             
                                             
                                                
                                                   
                                                      s
                                                      i
                                                   
                                                
                                                2
                                             
                                          
                                       
                                    
                                    
                                       1
                                       /
                                       2
                                    
                                 
                              
                           
                        where f and f
                        
                           k
                         are the feature vectors of the test sample and k-th training sample, respectively; s
                        
                           i
                         is the standard deviation of the i-th feature; and d denotes the feature length of the samples.

Nearest neighbor (NN) classifier, which provides high performance with less computation overhead, was used to perform the classification of the test patterns.

@&#EXPERIMENTS@&#

The performance of the proposed approach is evaluated on two different contactless palmprint databases. One of them (KTU-Contactless Palm DB) is a new and realistic database which is constituted by the images captured from the proposed contactless palmprint authentication system. Another one is the most publicly used contactless palm DB: IIT Delhi Palmprint Image Database version 1.0 (briefly IITD database) [32].

In this work, a contactless palm database
                              1
                           
                           
                              1
                              The details can be found in http://ceng2.ktu.edu.tr/~cvpr/contactlessPalmDB.htm.
                            has been constituted for the performance measurement of the proposed system. By the novel palm image acquisition system a total of 1752 images of size 768×576 were collected for the palm database. These images belong to the right hand of the 145 individuals, of whom 105 are male and the rest are female. The minimum, maximum and average numbers of images per palm are 6, 30 and 12, respectively.

To evaluate the performance of the proposed method more realistic, and to simulate the various difficulties that can be encountered, the hand images in the database were collected by performing various scenarios. They are as follows: (1) The environment brightness was changed by decreasing/increasing the light intensity in the room to reveal the invariability on the external brightness. Some hand image samples with the estimated palm model for this type of scenario are shown in Fig. 3.i,k. (2) The users are asked to present their hands toward the capturing device by changing the hand-camera distance to construct a database with different shaped hand images. They are also asked to present their hands in various pose and translations (strongly bending their hands downward, changing the space between the fingers, excessively moving their hands in the forward/backward, and the left/right directions, different hand rotations to right and left sides). Some of the hand images in our database and their segmentation results (palm models) are also given in Fig. 3.a, c, e–g, i, k–l. (3) The objects that have similar textures and colors such as faces, other hands and complex textured objects were also added to the background to evaluate the robustness of the algorithms on the complex backgrounds. The sample images and their palm models are shown in Fig. 3.b, d, j. In the meantime, we captured the images at different times during the day so as to acquire natural differences on the external lighting conditions. Furthermore, the users did not remove the rings or other ornaments from their fingers while the hand images were taken (see Fig. 3.j). The KTU-Contactless Palm DB also includes hand images which have partly viewed palms or occlusions to the palmprint regions.

This publicly available contactless palm image database [32] consists of the hand images collected from the students and staff at IIT Delhi, India. The images in the database (with pixel resolutions of 800×600pixels) have been acquired in the Biometrics Research Laboratory during January 2006 to July 2007 using a very simple contactless imaging setup. This database divided into two sub-data sets: left and right hand image data sets. The left hand image data set consists of 1393 palm images from 234 individuals, while the right hand image data set consists of 1376 palm images from 233 individuals. The minimum number of images for each hand is 5. The images are grabbed in a semi-closed enclosure and so there is no background complexity as in our database. Additionally, the images of this database are not taken in different environment brightnesses, and the spaces between the fingers are not changed excessively. However, the hand images have different pose variations arisen from the contact-free structure of the system.

In the experiments, for the model construction in AAM, a training set with 73 images was constituted. The images in the training set were selected due to the representation ability of the variations on the shape and texture of the palm images. Furthermore, the number of the samples in the training set is determined according to the trade-off between the computational complexity arising from large data sets and incompatibility occurred from small data sets [25]. The ROI selection results of the proposed method are given visually with the sample images in a comparative manner. In addition to these results, recognition rate, ROC graph and equal error rate (EER) values were computed to quantify the performance of the recognition and verification systems.

In this section, the ROI images extracted by the proposed method were analyzed in comparison with the images of a point-based method [12] introduced by Connie et al. The most commonly used ROI extraction method [11], described by Zhang et al., was not taken into account in the comparisons. Because, there is an uncertainty in the determination of the ROI size (the extracted ROIs have a fixed size which are suitable for their palm images only), and the location of a ROI is determined with the visible part of the palm (extracting the central part as ROI) which can lead to misleading results. Therefore, another commonly used method, which is similar to the Zhang et al.'s method, was selected for the comparisons. In Ref. [12], the midpoints on the index and little fingers are used to locate the ROI of the palm. The midpoints are also estimated by using the gaps between the fingers and the contour of the hand shape. Henceforth, we called this technique as classical ROI. To obtain the ROIs with this approach, the gaps between the fingers and the contour of the hand shape were extracted from the fitted palm model.

Generally, it has been observed from the visual comparisons that the proposed method provides more accurate selection of the ROIs as compared to the classical method. Sample palmprint images extracted by these methods are given in Fig. 4
                           .

In Fig. 4, for each item, the images in the left and right are extracted with the classical and proposed ROI extraction methods, respectively. The samples which are linked with the arrows are obtained from the same user with different hand postures. Although the selection of the ROIs are translated and scaled with the use of the classical method, the proposed method results are not affected with different postures. Besides, the valley point near the thumb finger can be seen and the ROI can be misplaced substantially in some images for the classical ROI approach.

There are two main reasons for the general performance improvement obtained by the proposed ROI extraction approach. First of them is the removal of the occurred errors due to the faulty detection of the valley points of the gaps between the fingers. These errors are the common problems for the point-based approaches which are very sensitive to valley point selection. Incorrect valley point selection may cause such methods to obtain rotated, translated and scaled ROI images. In the proposed method, orientation of the hand and ROI are evaluated with regressing all landmark points in the training set, which are annotated by a supervisor. Furthermore, the major changes occurred from small errors can be tolerated by a regularization coefficient in LS-SVR. Therefore, the results are not greatly affected by the errors originated from faulty landmark point selection in our method. The second reason that affects the performance negatively is the existence of the wrinkles in the palmprint region which are caused by decreasing the distance between the thumb and the index finger. This negative effect is observed in a more pronounced manner in classical ROI methods as compared to the proposed method since the ROI is extracted more closely to the thumb finger. Besides, in some hand shapes, the valley point between the thumb and index finger appears on the ROIs extracted by the classical approach. In our approach, the position of the ROI (the distances to the edges of the palm) is empirically determined to prevent such errors. These problems are illustrated in Fig. 4.

To draw attention on the importance of the ROI extraction techniques with quantitative results, the recognition and verification experiments were performed in a comparative manner. In the experiments, the first four samples for each palm belonging to an individual are selected for training set, and the rest are selected for the test set. The results, extracted by using both ROI approaches, are summarized in Table 1
                           .

In the recognition experiments the proposed ROI extraction method gives better results as compared to the classical ROI extraction method. We have observed a performance improvement of 0.854% as compared to the method given in Ref. [12] and we have achieved an overall recognition rate of 99.488%. The achieved 99.488% recognition rate means that only 6 images were classified incorrectly when the palmprints were extracted by using the proposed ROI method.

Verification results of the system were obtained by matching each test sample with all of the training samples. Correct and incorrect matchings are defined by whether two samples are from the same palm or not, respectively. The total number of matchings is 679,760. The correct matching number is 4688, and the incorrect matching number is 675,072. The false acceptance rate (FAR) and false reject rate (FRR) curves are estimated by correct and incorrect matchings and illustrated for the proposed ROI extraction method in Fig. 5
                           .

From the FAR-FRR graph, it can be seen that, FAR and FRR curves are separated from each other sufficiently and EER of the unrestricted contactless palmprint verification system is considerably low (only 0.277%) with the proposed method. The classical ROI extraction method leads to more than twice verification errors.

The performance of a biometric system is mostly illustrated by the ROC curve, which is frequently used in the assessment of genuine acceptance rate against false acceptance rate. The ROC curves for the proposed ROI and classical ROI extraction methods are shown comparatively in Fig. 6
                           .

It is clear from the ROC curves that our ROI extraction approach gives superior GAR results than the point-based approach in all of the FAR values. For the false acceptance rate of 1E-4%, 97.056% genuine acceptance rate is achieved with the proposed method, while with the use of classical ROI method only 92.726% GAR can be achieved.

As another experiment, to evaluate the developed contactless system's general performance and reveal the effect of the increase in the number of the training samples on the recognition/verification performance, we used the first 2, 3, 4 and 5 samples for each subject as training samples. The recognition rates and equal error rates of the system are listed in Table 2
                           .


                           Table 2 shows that, for the increasing number of the training samples, the recognition rate (RR) and EER are logarithmically increased and decreased, respectively. Besides, the performance of the five training samples per palm is higher than others, in which 99.708% RR and 0.117% EER are achieved. However, it is known from the practical experience that, the users are mostly unwilling to present their hands repeatedly. Therefore, the use of a few number of training samples can be preferred in a real life application. If the system was trained with only two samples per palm, the results were obtained as: 97.606% RR and 0.855% EER. In our studies given above, four training samples scheme was preferred for the optimality and practicability. This scheme also used in the performance comparisons with the literature.

The performance of the proposed ROI extraction method is also evaluated on the public IIT Delhi Palmprint Image Database version 1.0 [32] through the recognition and verification experiments. For the training of AAM model, 73 sample hand images are selected. These samples are also used for training the direction of the palm. In the experiments three samples for each palm were selected for the training set, and the rest were chosen for the test set due to the limitations on the samples per individual. The verification and recognition results are summarized in Table 3
                            for the left and right hand data sets separately.

The recognition results of the left and right hand data sets are very close to each other and better for the proposed ROI extraction method than the classical method. While the recognition rates of the proposed method are approximately 99.4% for both data sets, the recognition rate of the classical method for the left and right hand data sets are 98.8% and 98.5%, respectively. This means that the proposed method gives a higher recognition rate of 0.579% than the classical method in the worst case.

Verification results also support the superiority of the proposed method. In the experiments, for the left and right hand data sets, the correct matching numbers are 2073 and 2031; and the incorrect matching numbers are 483,009 and 471,192, respectively. The obtained EERs by the proposed method are much lower than the EERs of the classical method. The proposed methods reduce the errors by 75% as compared to the classical method for the left hand data set and by about 50% for the right hand data set. The ROC curves for the proposed ROI and classical ROI extraction methods on IITD DB are shown comparatively in Fig. 7
                           .

The ROC curves show that the proposed ROI extraction method gives superior ROC curves than the classical point-based method. For the left hand data set, with the false acceptance rate of 1E-3%, 95.900% genuine acceptance rate is achieved by the proposed method, while with the use of classical ROI method only 91.581% GAR can be achieved. This distinction can be more visible for the right hand data set (92.708% to 85.112%). All of the recognition/verification results achieved on the IITD DBs are consistent with the results achieved on the KTU-Contactless Palm DB: the proposed method gives higher GAR values for the all FAR values in the ROC curves, it reduces the errors (EERs) by 50% minimally, and it has 99.4% recognition rates for both databases. The EER values of the proposed method are 0.277% and 0.149% for the KTU and IITD DBs, respectively. And it gives 0.854% and 0.579% higher recognition rates for the KTU and IITD DBs, respectively. Hereby, it is confirmed that the proposed method gives better recognition and verification results than the classical point-based approaches.

In this section, a comparison on the recognition/verification accuracies has been given for our contactless palmprint system and the other recently proposed contactless palmprint systems. Quantitative performance results of the contactless recognition/verification systems were given in Table 4
                         with the number of images, a number of training samples per subjects and a number of individuals which the experiments performed on. Although some of the contactless systems include other biometrics or multi-spectral structures
                           2
                        
                        
                           2
                           For example in Ref. [33] the 3-D and 2-D palmprint features, in Ref. [3] palmprint and knuckle-print biometrics were combined.
                         we only consider 2-D palmprint parts of them. In the comparisons, the papers utilized CASIA palmprint database were not taken into account despite they have lower performances (for example in Ref. [34] the EER is 0.9%). Because, CASIA database consists of images acquired from a peg-free but not a contact-free device.

Most of the papers in the literature have usually presented either verification or recognition results of their systems. However, in our work both experiments were performed. Hence, we have selected five high quality studies for the verification and three studies for the recognition comparisons, which were recently published.

From the comparisons in Table 4, it can be seen that our contactless palm database is one of the biggest database in the literature. Most of the contactless palm databases (five of eight analyzed databases) have a smaller number of samples and individuals than our database has (e.g. in Ref. [38] there are only 49 individuals and 490 samples). Furthermore, in Ref. [33] the database involves right and left hand images, and so it provides a significant advantage for the verification of the palmprints. From the database comparisons we can say that, the results of the experiments in this paper are more reliable and realistic than the others. Furthermore, the complexities of the images in the databases are satisfactory from the point of representing the real-world conditions. Some of the studies emphasized on the cluttered backgrounds (e.g. in Refs. [3,38]) and the others emphasized on the pose, translation and scale variations in semi-closed background (e.g. in Refs. [2,35]) to simulate the difficulties on the real-world conditions. Our database also includes images which were collected with pose, translation and rotation variations on different cluttered backgrounds.

In the proposed work, Gabor wavelets based kernel Fisher discriminant analysis was performed to extract the palmprint features. Most of the palmrint verification studies such as Refs. [2,3,33,36] used the coding approaches for the same purpose. In Ref. [38], a novel robust decomposition method was suggested. Poinsot et al. [37] utilized the Gabor filter and binarization to extract the features. Unlike the others, in Refs. [4,35], SIFT-based local descriptors were used as the palmprint features. For matching and classification, Weighted Euclidean Distance and Nearest Neighbor methods were used in our work, respectively. On the other hand, all of the papers, mentioned above, used the Hamming distance for matching (in Refs. [4,35] also Euclidean distance were used for SIFT feature matching). The classifications of the palms in the recognition experiments were achieved by Support Vector Machines in Ref. [38], Rank-Level Fusion in Ref. [36], and Nearest Neighbor in Ref. [37].

The number of training samples per subject is an important criterion for the subspace methods in which the features are extracted from these training samples. However, for the other methods (e.g. coding-based methods, SIFT-based methods), the training set/test set separation is not essential. In these methods, since it does not influence the extracted features directly, each sample can be matched with all the other samples in the data set, and this leads to greater number of matchings. In other words, a sample is matched to all the remaining samples of that subject repeatedly for all of the test samples to generate genuine match scores. So, we refer to this approach as “leave-one-out (LOO)”. The works in Refs. [2–4,35,36] used LOO, and cannot be compared with the proposed method in this respect. Although, Kanhangad et al. used competitive coding in Ref. [33], they separate the samples to training set/test set and used five samples for training set. The other works [37,38] which are selected for recognition comparisons used three and seven samples for training, respectively. When compared with the results of the proposed method for three training samples per subject it can be seen that, Poinsot et al. [37] gives a lower performance.


                        Table 4 also shows that, the performance of the proposed contactless palmprint authentication system is encouraging and superior to the others (0.277% EER and 99.488% RR). The number of verification errors occurred in Ref. [3] is higher by a factor of seven as compared to the proposed system. Besides, the system which has the highest verification performance in the literature [35] gives 50% more errors as compared to the proposed method. Furthermore, in some papers it is stated that the matching was performed in a leave-one-out manner, and the images in their databases were not separated as training and test sets. This selection scheme on the database can affect their results positively. Similarly, the highest recognition rate given in the papers referenced above is 0.568% lower than the proposed system. In other words, the number of errors given in those papers is higher than the errors occurred in this study at least by a factor of two for the recognition experiments.

Another comparison can be made by discussing the results for the IITD touchless palmprint database. In Ref. [35], 0.43% and 0.49% EER, in Ref. [4] 0.20% and 0.21% EER were achieved for the left and right palm data sets, respectively. However, with the proposed method superior results can be achieved as highlighted by the EER results of 0.033% and 0.149% for the left and right palm data sets. Also, the recognition rate given in Ref. [36] (97.55%) for the right palm data set is lower than the recognition rate of the proposed method for this data set (99.41%).

Consequently, it has been revealed that the robustness of the evaluated system is very competitive and promising as a whole with all of its parts including image acquisition device, segmentation, ROI extraction and recognition. This observation is confirmed by extensive evaluation studies carried on an unrestricted contactless palmprint database.

@&#CONCLUSION@&#

This paper presents a novel contactless palmprint image capturing and recognition system. In this system, the users were asked to introduce their hands above the CCD camera with various hand pose and orientations under unconstrained environment. Due to the presence of these challenges where the pixel-based segmentation methods cannot give acceptable results, we utilized AAM-based palm modeling to accurately segment the palms.

For the extraction of a central area from a palm image (called ROI), a novel ROI extraction approach that utilizes the palm model and nonlinear regression has been proposed. Because it removes weaknesses of the other approaches, a great deal of improvement has been achieved by the usage of this method. The performance of the method was evaluated with recognition rate, equal error rate, and ROC graph. The possible reasons of the success are also illustrated with ROI image samples.

In our contactless palmprint database that contains 1752 images collected from 145 subjects, we have achieved high genuine acceptance rate (97.056% GAR) at low false acceptance (0.0001% FAR) rate, 0.277% EER, and 99.488% recognition rate, which are clearly encouraging for the contactless palmprint system under unconstrained scenes. This high performance is also supported by the results of the recognition/verification tests on the public IIT Delhi Palmprint Image Database. In future works, the proposed system should be transformed to an online contactless palmprint identification system and more users should be included into the database to test its feasibility in large organizations.

@&#ACKNOWLEDGMENT@&#

The work described in this paper has been supported by National Science Foundation under Project No. 107E212.

Portions of the work were tested on the IITD Touchless Palmprint Database version 1.0.

The authors would like to thank the associative editor and the anonymous reviewers for their valuable comments that resulted in an improved manuscript, and Dr. Sedat Gormus from the Karadeniz Technical University, for his helpful discussions on the technical sections of the paper.

@&#REFERENCES@&#

