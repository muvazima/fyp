@&#MAIN-TITLE@&#Emotion transplantation through adaptation in HMM-based speech synthesis

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           We propose an emotion transplantation method based on adaptation techniques.


                        
                        
                           
                           Emotions can be imbued into neutral synthetic speech models regardless of gender.


                        
                        
                           
                           Five perceptual evaluations, including one with a robot, were carried out.


                        
                        
                           
                           Emotion transplantation clearly improves emotional performance over neutral voices.


                        
                        
                           
                           High quality source models provide high quality transplanted models.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Statistical parametric speech synthesis

Expressive speech synthesis

Cascade adaptation

Emotion transplantation

@&#ABSTRACT@&#


               
               
                  This paper proposes an emotion transplantation method capable of modifying a synthetic speech model through the use of CSMAPLR adaptation in order to incorporate emotional information learned from a different speaker model while maintaining the identity of the original speaker as much as possible. The proposed method relies on learning both emotional and speaker identity information by means of their adaptation function from an average voice model, and combining them into a single cascade transform capable of imbuing the desired emotion into the target speaker. This method is then applied to the task of transplanting four emotions (anger, happiness, sadness and surprise) into 3 male speakers and 3 female speakers and evaluated in a number of perceptual tests. The results of the evaluations show how the perceived naturalness for emotional text significantly favors the use of the proposed transplanted emotional speech synthesis when compared to traditional neutral speech synthesis, evidenced by a big increase in the perceived emotional strength of the synthesized utterances at a slight cost in speech quality. A final evaluation with a robotic laboratory assistant application shows how by using emotional speech we can significantly increase the students’ satisfaction with the dialog system, proving how the proposed emotion transplantation system provides benefits in real applications.
               
            

@&#INTRODUCTION@&#

ARABOT and INAPRA (and previously URBANO and ROBONAUTA) are coordinated Spanish research projects on interactive mobile robotics for real environments such as museums or universities. The robots integrate autonomous navigation, a distributed object-oriented architecture, automatic speech recognition, affective speech synthesis, a mechatronic emotional face and robotic arms (Rodriguez-Losada et al., 2008). In order to adapt to an ever-changing domain of application, the robot has a domain-independent emotional model of behavior (Lutfi et al., 2013) which is able to automatically estimate the degree of satisfaction of the users the robot is interacting with, and is able to adapt the emotional state and the spoken dialog to the context of use. By means of this adaptive empathetic strategy, the artificial agent significantly increases users’ satisfaction and minimizes users’ frustration, even when the performance of the speech recognizer or the dialog manager cannot be improved.

Current speech synthesis systems, whether we are talking about unit selection or HMM-based systems, can provide very good naturalness and intelligibility when synthesizing read speech regardless of the technology (Barra-Chicote et al., 2010; Barra-Chicote, 2011) which is ideal for neutral speech interfaces that do not need to engage in a direct conversation with the user. On the other hand, applications such as dialog systems (Lutfi et al., 2013), robots or virtual characters, where simulating a more human-like behavior is necessary, a neutral speech synthesis does not live up to the task. Imbuing the synthetic speech with expressive features (e.g. emotions, speaking styles, etc.) is the role of expressive speech synthesis.

Due to the sheer amount of possible expressiveness, recording complete databases that cover all of them is unthinkable, making unit selection based systems fall behind in terms of scalability, although they are definitely capable of producing expressive speech (Adell et al., 2010, 2012; Andersson et al., 2010; Erro et al., 2010). On the other hand, HMM-based systems, because of their parametric nature, can be easily adapted through speaker adaptation techniques and can be successfully used for this task, and have been proven to provide significant improvements in perceived speech quality (Yamagishi et al., 2005).

One of the biggest problems of expressive speech synthesis is data acquisition. As human expressiveness is not a discrete space but a continuous one, the expressive strength and nuances vary greatly not only from person to person but from utterance to utterance for the same person. This problem can be focused on from different approaches: lexical analysis (Andersson et al., 2012) for correctly classifying the available data and training more precise systems or acoustic analysis. For acoustic analysis several aspects have been considered such as expressiveness detection (El Ayadi et al., 2011; Lorenzo-Trueba et al., 2012; Schuller et al., 2010), expressiveness production (Obin et al., 2011; Raitio et al., 2013), expressive intensity control (Nose et al., 2013; Picart et al., 2011) or expressiveness transplantation (Chen et al., 2012; Latorre et al., 2012).

The work present in this paper is enclosed mainly under the field of expressive speech synthesis, and aims to fix one of its main shortcomings: scalability. Human communication is so rich and so deep that it is impossible to imagine obtaining data for every combination of speaker and expressiveness, and that is why we want to propose a method capable of learning the paralinguistic information of emotional speech, control its emotional strength and transplant it to different speakers for whom we do not have any expressive information. We decided to focus on emotional speech as a particularization of expressive speech (fitting the aim of creating different emotional voices for several affective robots in a museum such as the Principe Felipe Science Museum in Valencia we are collaborating with), but we can expect the transplantation method to be able to support different expressive domains.

A successful transplantation method that has been introduced lately (Chen et al., 2012; Latorre et al., 2012) is based on Cluster Adaptive Training (CAT) (Gales, 2000), a projective adaptation technique. As such it is only capable of producing speaker models based on linear combinations of the original training speaker models. The main advantage of this approach is that as the produced model is always a combination of pre-existing training models, the process is extremely robust, outputting very high quality speech (Yanagisawa et al., 2013). On the other hand, the level of expressive strength or speaker similarity cannot be guaranteed as the transplantation reach is very constrained. This is also the case for model interpolation techniques (Hsu et al., 2012), capable of achieving better expressiveness than traditional adaptation techniques at a cost in speaker similarity.

Another approach to emotion transplantation is the use of rules to directly modify the synthesis models. This approach is theoretically capable of imbuing an emotion on any target speaker as long as we know the correct rules. In reality these approaches, while usually capable of providing emotional strength controllability and reasonably good recognition rates (Zovato et al., 2004; Takeda et al., 2013), speech quality and speaker similarity degradation tend to be a problem.

The proposed emotion transplantation method considers the best of both previously mentioned approaches: using adaptation to lessen speech quality degradation while using the adaptation functions as pseudo-rules for modifying the speaker models. As a result we present a method capable of controlling expressive strength while reasonably maintaining speech quality and speaker identifiability when compared to non-transplanted expressive synthetic speech (Lorenzo-Trueba et al., 2013a,b).

The paper is organized as follows. In Section 2 we introduce the neutral and emotional speech corpora we have used for training and evaluation purposes during the development of the proposed method. Section 3 introduces the transplantation method, where Section 3.1 introduces the mathematical aspects of the used CSMAPLR adaptation and how it was expanded for our purposes, and Section 3.2 explains in detail the procedure through which the emotion transplantation is applied, together with an emotion transplantation baseline. Section 4 describes how the perceptual evaluations were carried out and analyzes the results. In Section 5 we describe and present the results of our evaluation with a robotic agent. Finally in Section 6 we discuss the results obtained in our evaluations and in Section 7 we present the conclusions to be drawn from this paper together with a brief summary of the main proposals.

For the development and evaluation of the proposed emotional speech transplantation method we employed both neutral and emotional databases. The emotional database (SEV, Barra-Chicote et al., 2008) was evaluated in the Albayzin 2012 speech synthesis challenge. The neutral data is a combination of databases from previous Albayzin challenges (Mendez Pazo et al., 2010) (UVIGO-ESDA Database, Banga, 2010 and UPC-ESMA Database, Bonafonte and Moreno, 2008) and a number of male and female speakers recorded in our laboratory environment.
                        SEV Database
                        
                           Emotional database consisting of a male and female speaker. Out of the available emotions only 4 of them were considered: anger, happiness, sadness and surprise also including the neutral voice as the reference. All the emotions were recorded for the same utterances favoring the learning of expressiveness cues. There is approximately 30min of training speech for each emotion and speaker.

A database consisting of a single male Spanish speaker (UVD) in a neutral situation totaling 2h of speech recorded in studio.

A database consisting of a single professional female speaker (UEM) totaling 1.75h of neutral style speech, recorded in a noise reduced room.

A number of male and female speakers were recorded in our acoustically-treated room, providing high quality and stable speech. Two male (JLC and JEC) and two female speakers (NAS and EMA) were used as the transplantation targets. Data durations vary from 6min for EMA, 7min for JEC to 30min for JLC.

The corpora can be seen in Table 1
                     , where we can see the acronyms that identify the speakers, their gender, emotions and amount of data.

Emotion transplantation methodologies can be defined as the procedures that allow the modification of a synthetic speech model to incorporate emotional information learned from other speaker models while maintaining the identity of the original speaker. By this definition it follows that transplantation is a field of study that aims to solve one of the biggest problems in expressive speech synthesis: scalability.

Adaptation is a powerful tool when considering emotional speech synthesis and more concretely emotion transplantation, as it allows us to exploit the versatility of HMM-based speech synthesis. In the task at hand we consider the adaptation task of generating a speaker model from an average voice model (AVM) and adaptation data for the desired target speaker (Yamagishi et al., 2003).

Focusing on emotional speech adaptation, it has been proved that it is very important to consider not only the means of the HMM Gaussian Distributions but also the variances. This means that it is necessary for the adaptation algorithms to be more complex, or “constrained” as it is called. Ultimately, constrained structural maximum a posteriori linear regression (CSMAPLR) has been proposed and has been proven to be extremely successful for speaker adaptation, particularly when adapting from average voice models (Yamagishi et al., 2009).

CSMAPLR consists in applying the structural MAP criterion (SMAP) (Shinoda and Lee, 1997) to the CMLLR adaptation algorithm (Gales, 1998) and using the recursive MAP criterion (Chesta et al., 1999) to estimate the transforms for simultaneously transforming the mean vectors and covariance matrices of the state output and duration distributions of the speaker model.

There are three main reasons for using CSMAPLR as the adaptation technique. First of all is the aforementioned capability of not only adapting the mean vectors but also the covariance matrices. The second reason that differentiates CSMAPLR from the more traditional CMLLR adaptation, is that CSMAPLR makes use of the linguistic information of the regression tree by doing recursive MAP-based estimation of the transformation matrices from the root of the context decision tree to the lower nodes, combining the advantages of SMAP and CMLLR. Finally, the fact that CSMAPLR relies on MAP-based estimations means that it is robust when using sparse adaptation data, which is frequently the case in the emotional speech synthesis task.

The concept of cascade transforms has been used previously in automatic speech recognition to adapt the background models both to the target speaker and noise at the same time (Seltzer et al., 2012). The transplantation method we present is based on the same concept, but in this case we propose chaining transformations that model both the emotional source speaker and the target speaker to produce emotional speech synthesis models, as introduced in the preliminary version of the system (Lorenzo-Trueba et al., 2013a).

In Fig. 1
                            we can see the block diagram representation of the proposed emotion transplantation method. If we define the CSMPALR adaptation functions in terms of their rotation matrix ζ and bias vector ϵ:


                           
                              
                                 (1)
                                 
                                    
                                       
                                          μ
                                          ¯
                                       
                                       emo
                                    
                                    =
                                    
                                       ζ
                                       emo
                                    
                                    
                                       μ
                                       N
                                    
                                    +
                                    
                                       ϵ
                                       emo
                                    
                                 
                              
                           
                           
                              
                                 (2)
                                 
                                    
                                       
                                          Σ
                                          ¯
                                       
                                       emo
                                    
                                    =
                                    
                                       ζ
                                       emo
                                    
                                    
                                       Σ
                                       N
                                    
                                    
                                       ζ
                                       emo
                                       T
                                    
                                 
                              
                           
                           
                              
                                 (3)
                                 
                                    
                                       
                                          μ
                                          ¯
                                       
                                       spk
                                    
                                    =
                                    
                                       ζ
                                       spk
                                    
                                    
                                       μ
                                       N
                                    
                                    +
                                    
                                       ϵ
                                       spk
                                    
                                 
                              
                           
                           
                              
                                 (4)
                                 
                                    
                                       
                                          Σ
                                          ¯
                                       
                                       spk
                                    
                                    =
                                    
                                       ζ
                                       spk
                                    
                                    
                                       Σ
                                       N
                                    
                                    
                                       ζ
                                       spk
                                       T
                                    
                                 
                              
                           where 
                              
                                 
                                    μ
                                    ¯
                                 
                                 
                                    emo
                                    /
                                    spk
                                 
                              
                            and 
                              
                                 
                                    Σ
                                    ¯
                                 
                                 
                                    emo
                                    /
                                    spk
                                 
                              
                            are the mean vectors and covariance matrices of the emotional source models and target speaker model respectively. Then, the model resulting from applying in cascade the emotion transforms and then the speaker transforms becomes:


                           
                              
                                 (5)
                                 
                                    
                                       
                                          μ
                                          ¯
                                       
                                       tar
                                    
                                    =
                                    
                                       ζ
                                       spk
                                    
                                    
                                       ζ
                                       emo
                                    
                                    
                                       μ
                                       N
                                    
                                    +
                                    
                                       ζ
                                       spk
                                    
                                    
                                       ϵ
                                       emo
                                    
                                    +
                                    
                                       ϵ
                                       spk
                                    
                                 
                              
                           
                           
                              
                                 (6)
                                 
                                    
                                       
                                          Σ
                                          ¯
                                       
                                       tar
                                    
                                    =
                                    
                                       ζ
                                       spk
                                    
                                    
                                       ζ
                                       emo
                                    
                                    
                                       Σ
                                       N
                                    
                                    
                                       ζ
                                       emo
                                       T
                                    
                                    
                                       ζ
                                       spk
                                       T
                                    
                                 
                              
                           
                        

The resulting emotional target model will be able to produce emotional synthetic speech for the target speaker even if emotional training data for the target speaker is not available.

The proposed transplantation method can be summed up in three steps:
                           
                              1.
                              Adapt a neutral average voice from the average voice model to work as a reference emotion (Fig. 2
                                 (1)).

Adapt the neutral target model and the emotional source models from the neutral average voice (Fig. 2(2)).

Apply in cascade the emotion and speaker transforms to the neutral average voice. The results are the emotional target models (Fig. 2(3)).

The average voice model is obtained by applying speaker adaptive training (SAT) (Anastasakos et al., 1997) with as much training data as possible, which allows us to obtain a very context-rich background model to work with. A robust and complete AVM will be capable of producing better speech quality at synthesis time even with sparse emotional adaptation data. Also, sharing a background model for all the adaptation functions makes the cascade adaptation easier, because the context decision trees will be shared, making the adaptation functions immediately compatible between adapted models.

Adapting the neutral average voice (NAV) from the AVM is necessary because in the second step we have two objectives: on one hand we want to be able to learn the differences between the emotional source model and the NAV, effectively learning the nuances of the desired emotional speech. On the other hand we want to learn the difference between the target speaker speaking in a typical, neutral style and the NAV, thus learning the nuances of the target speaker identity. If both adaptation functions are not obtained from a common reference emotion, neutral speech in the present case, they will not learn the desired characteristics and the transplantation process would not be successful. Ideally, we want to have both data for the target emotion and reference emotion for the same speaker so the emotion adaptation function defines purely the emotional source model, but if that is not available we can assume that using an average of different speakers will show the relevant information of the emotions while lessening the identities of the speakers.

Finally, we apply in cascade the emotion and speaker transforms as defined previously (Fig. 2(3)), obtaining the desired emotional target models. The produced emotional strength can be easily controlled by means of a transplantation control ratio to linearly scale the adaptation function. Neither the target speaker nor the target emotion data had to be present in the AVM, and in the presented evaluation (Section 4) we prove it to be successful with as little as 5min of target speaker speech data or 30min of emotional speech, which is why the proposed transplantation method is a good way of providing scalability in expressive speech synthesis. Nonetheless, in order to provide a basis for comparison, we also propose an emotion transplantation baseline.

The proposed emotion transplantation baseline joins all our emotional data into an average emotion model (Qin et al., 2006) to be used as the emotional source. This average, when transplanted into the neutral target model, can be expected to imbue an undefined emotion that removes the typical monotony in read speech models. This alternative can also be expected to provide higher quality speech when compared to transplanting a single emotion as the adaptation process for the average emotion can make use of much more data, thus giving more stable adaptation functions. This approach could be very useful when the task does not require us to synthesize any particular emotion or if we do not have enough emotional data to obtain good emotion transplantation quality.

Naturally, this alternative also presents numerous shortcomings: if there is a significant bias toward positive or negative data in the average emotion model, transplanting the average emotion could be the same as transplanting an emotion, resulting on unnatural synthesized utterances for opposite emotions such as producing happy speech for a sad text. Another expected problem is that the naturalness should be lower than transplanting the correct emotion for the text to be synthesized.

The goal of the perceptual evaluation was to verify if the expressiveness was transplanted successfully in terms of naturalness, speech quality and emotional strength. Naturalness measure was done by means of forced preference tests, as they are very useful when we want to compare systems that are similar between them but with variation in some conditions (King et al., 2008). Three different evaluations were carried out: a first evaluation that compared the average emotional model transplantation against the traditional neutral synthetic voices in order to establish an emotion transplantation baseline. A second evaluation to compare the proposed emotion transplantation system against neutral voices so as to validate the usefulness of the presented methodology. A third evaluation comparing the average emotion transplantation with the proposed emotion transplantation system, aiming to rate the performance of the proposed system against the baseline.

Four emotions (anger, happiness, sadness and surprise) learned from the SEV corpus were transplanted into 3 male speakers and 3 female speakers, so for each test the total number of systems was 24. Following the Latin-square (Gao, 2005) approach this meant that we needed 24 different utterances to be synthesized for all the systems to be presented to the listeners in a random order without repetitions. In the end we decided for considering 24 utterances per evaluation and not multiples of it so as to keep the evaluation itself from becoming too long, lasting around 30min in this implementation. In the test, two audio samples were presented to the listener by means of a web interface: the transcription of the synthesized texts and the intended emotion to be transmitted. The samples could be played as many times as desired by the listener. The synthesized texts, not present in the training data, were carefully designed by ourselves to present clear emotional context. A sample utterance for each emotion can be seen in Table 3
                        . The listener was asked their preference for which of the samples was more adequate to transmit the desired emotion, being forced to pick one or the other. Then they were asked to rate the utterances in the traditional 5 point MOS evaluation for both speech quality (from very bad to very good) and emotional strength (very low to very high). The evaluation for speech quality and emotional strength had to be answered for both samples regardless of the selected preference. In the case of the multiple choice secondary evaluation (Section 4.3.1), the listeners were presented the transcribed synthesized text, the target emotion and a set of 5 utterances and were asked to select one sample out of the five that they believed better conveyed the target emotion. The 5 utterances corresponded to the 5 emotions (anger, happiness, neutral, sadness, and surprise) and were randomized in order for each sample.

The carried out evaluations did not include an analysis of similarity and emotional strength control despite them being advantages of the proposed systems because they have been thoroughly analyzed in previous tests. Our initial emotion transplantation evaluation (Lorenzo-Trueba et al., 2013a) measured the similarity, emotion recognition rates, speech quality and emotional strength of the proposed systems with different emotion transplantation control ratios, using neutral synthetic speech as baseline and natural voice as a top-line. Only the male source and target speakers of the introduced corpora (Section 2) were used for this preliminary evaluation. The results for the emotion identification rates can be seen in Table 2
                        . It must be noted that because the synthesized text in the preliminary evaluation was designed to be completely neutral, the emotion identification task was significantly harder than for the task in the present evaluation, where not only the text was carefully designed by us to be clearly emotional but also we provided the evaluators with the intended emotion. The previous test showed how that the attainable similarity is comparable to natural speech, and that values of the transplantation control ratio that keep a good balance between speech quality and emotional strength range between 0.75 and 1.00. This was because higher ratios degraded the speech quality too much and lower ratios limited too much the emotion identification rates and the perceived emotional strength. Consequently, for the present evaluation we did not measure similarity any further, and the applied emotion transplantation control ratio was 1.00. Regarding the statistical significance of the results, for the preference test we applied the Chi-squared criterion and for the speech quality and emotional strength MOS tests we applied the Wilkoxon Signed-Rank Test for a 95% confidence ratio. A minimum of 24 subjects per evaluation was fixed in order to guarantee the full coverage of the Latin square matrix, totaling at least 24 samples per speaker, emotion and evaluation.

Regarding the speaker models, the three considered systems (neutral, average and proposed) both share a common AVM trained with three feature streams with their Δ and Δ
                        2 coefficients: logarithm of the fundamental frequency (1 coefficient), mel-cepstral analysis coefficients (MCEP, 60 coefficients) and aperiodicity bands (25 coefficients). For the synthesis part we used the STRAIGHT vocoder (Kawahara et al., 2001) for all the systems. The data used to train each system can be seen in Table 4
                        .

The first test aims to establish the evaluation baseline by comparing the average emotion transplantation system and the traditional neutral synthesis system, and was carried out by 28 listeners. The first aspect to note is that the preference test greatly favors the average system as we can see in Table 5
                        , with an average preference rate of 75%. The results for emotional strength also show that the average system is perceived generally as more expressive, with an average of 0.7 points more perceived strength than in the neutral system, especially in the angry and surprised cases. Finally, speech quality suffers a slight degradation of 0.2 points in average, being this category the only one with non significant differences (Fig. 3
                        ). All the boxplots shown in this paper follow the same structure: the boxes capture the ranges between the first and third quartile, with the median represented by the thick line and the whiskers showing the highest and lowest datum within 1.5 the inter-quartile range.

The second test looks to show the advantages of the proposed emotional target model when compared against the neutral target model, and it was carried out by 27 listeners. The results (Table 6
                        ) show a clear preference for the transplanted system, in this case in an average of 88% preference rate. This rate is considerably higher than in the first evaluation, and reaches preference rates as high as the 96% or 95% for the happy and surprised systems respectively. This result is further proven by the emotional strength results, where there is an average of 1.2 points in the perceived emotional strength, especially once again in the happy and surprised systems, clearly shown in the boxplots (Fig. 4
                        ). Speech quality, on the other hand, suffers a slightly stronger degradation (an average of 0.4 points in this case).

In order to measure the capabilities of the proposed emotion transplantation system to convey the desired target emotion we carried out a follow up to the second test, where we asked the evaluators to pick the utterance they felt that conveyed more clearly the target emotion out of a pool of 5 (anger, happiness, neutral, sadness and surprise). This test was carried out by 25 listeners and the confusion matrix resulting can be seen in Table 7
                           .

The results show how the evaluators significantly prefer the utterances that were transplanted with the desired emotion when compared to the rest of the utterances with an average preference of around 70% for the intended one, which proves that the method is capable of adequately transplanting emotions. In particular we can see how there was almost no preference for the neutral utterances, even in the sad system where a neutral way of speaking could sometimes be acceptable. Also, the confusion between happiness and surprise makes sense, with both of them being positive emotions with high arousal that tend to produce higher pitched voices. The confusion of anger with happiness and surprise can be explained by the fact that sometimes anger is conveyed by a high arousal variation called hot anger. Sadness on the other hand is slightly confused with both cold anger and neutral, both low arousal states with slower speaking rhythm and lower pitch.

The third and final evaluation aims to measure the advantages of the proposed transplantation system against the baseline average emotion transplantation system, which means measuring the advantages of transplanting the emotion associated to the text to be synthesized against using an average standard emotion for all texts. In this case, the results obtained from 31 listeners, clearly show a preference for the proposed transplantation system, with an average preference rate of 75% (Table 8
                        ). Also, the perceived emotional strength results show an increase of 0.5 points when selecting the adequate emotion, stronger for the happy and surprised systems as shown in the boxplot (Fig. 5
                        ), but also present in the angry and sad systems. Finally, for speech quality we see extremely close results, less than 0.1 points in average.

In order to evaluate the improvements attainable by the proposed emotion transplantation method, we also designed an evaluation in a more realistic application: synthesizing the voice of a robotic laboratory assistant (Fig. 6
                     ). This robotic laboratory assistant, in its finished form, would be capable of moving around a laboratory interacting with the students and answering their questions or doing quizzes to evaluate their progress. For this particular evaluation, as we wanted to focus on how we could improve the robot's interaction capabilities by adapting its way of speaking to the context, we only used the robot's head in a ‘Wizard of Oz’ dialog scenario. Our role as the ‘Wizard of Oz’ was to replace the speech recognition system, starting the robot's turns in the scripted dialog.

We have developed a face with 5 degrees of freedom, 2 to control the mouth, one for each eyebrow and another one for closing both eyes, similar to (Rodriguez-Losada et al., 2008). The eyes were completely static, the mouth could not be opened and both eyelids had to be moved together. The actuators are based on Futaba servomotors and a simple servo-controller board allows sending sequences of control bytes through a serial port from a portable PC on-board the robot. As shown in Fig. 7
                        , it is perfectly capable of showing basic emotions such as happiness, sadness, anger or surprise.

So as to emulate a realistic scenario, we developed a total of 36 interaction scenarios (12 for happiness, 12 for sadness, 6 for anger and 6 for surprise) of which 6 would be carried on by each evaluator (2 for happiness, 2 for sadness, 1 for anger and 1 for surprise). A sample dialog translated to English can be seen in Table 9
                        . This dialog distribution was chosen because we believe that it is more common in a student-robotic assistant dialog to show happiness and sadness than anger or surprise. Regardless, all of them had to be present to fully test the proposed system. This evaluation was also designed with the Latin square principle in mind, so that the order of the emotions and dialogs were fully randomized. In each scenario a short dialog between 1 and 3 interaction turns would be played out between the robot and the evaluator, and it would be played twice (once for the neutral system and once for the proposed emotional transplantation system, in a random order). The robotic face would then adjust its expression to match that of the context regardless of the system that was being used, so that the only variation between the iteration of the dialogs was the synthetic speech. Also, to prevent the evaluation from becoming too long, we only considered one of our target speakers, UVD.

The evaluators, which were real students from UPM University, were explained that they were going to evaluate a robotic laboratory assistant called Groucho that wanted to be capable of adapting its way of speaking according to the context. Then they were given the scripts for the dialogs and were told that each dialog would be played twice, once for each emotional system. After carrying out each scenario twice, the evaluator was asked to choose between both versions which one they felt was better according to the dialog that was played out in a forced preference test. If they were undecided, the scenarios could be carried out multiple times, but only the complete scenario.

The results for the 24 students that took part on a first evaluation can be seen in Table 10
                        . There we see how the preference is heavily in favor of the proposed emotion transplantation system, which clearly shows that being able to provide a speech synthesis system capable of adapting its speaking style (emotions in this particular application) to the context can provide significant increases in the satisfaction of the users of the dialog system. It is only sadness that did not provide significant improvements, and informal dialog with the evaluators that disliked it showed that the problem was that they felt that the sad system was out of place in the laboratory assistant. Careful inspection of the source sadness data showed that it presents a tearful tone, which adds an aspect to the sad emotion that makes it inadequate for this particular domain.

In an attempt to fix the problems with sadness we prepared a second evaluation in which sadness was transplanted with only a 0.5 transplantation control ratio so that the synthesized emotional strength was lessened, while the rest of the emotions remained at 1.0. In this second iteration of the system 12 students participated, giving results very similar to the previous iteration as we can see in Table 11
                        . Informal conversation once again showed that while less recognizable, the effects of tearfulness still made the transplanted sadness to feel out of place, showing that reducing the emotional strength is not enough to attenuated the tearfulness aspect from the sad models as they are conveyed by complex spectral cues that cannot be eliminated without removing also sadness itself.

@&#DISCUSSION@&#

To sum up, the baseline average emotion transplantation system provides an average preference rate with an increase of 0.7 points in emotional strength at the cost of an average of 0.2 points in speech quality. In comparison, the proposed emotion transplantation system provides an average 87% preference rate increasing the perceived emotional strength in an average of 1.2 points at the cost of 0.4 points in speech quality. The final test, comparing the baseline and proposed systems reinforces the idea that transplanting the correct emotion is a better approach, as there is a preference for the proposed system of 74%, with a perceived increase in 0.5 points in emotional strength, with an only partially statistically significant decrease in speech quality smaller than 0.1 points. All in all, we can say that the proposed emotion transplantation method is clearly capable of imbuing the emotional information learned from a source speaker into different target speakers regardless of gender with significant increases in perceived naturalness and emotional strength when compared to traditional systems at a slight cost in speech quality. In total we had 86 evaluators, 28 for the first, 27 for the second and 31 for the third and last one. This meant a grand total of 2046 evaluated utterances and 516 per emotion, providing statistically significant results in the presented evaluations.

In order to quantify the robustness of the proposed transplantation system to different speaker conditions we have carried out a number of ANOVA and correlation analyses of the effect that speaker gender, speaker identity and neutral synthetic model speech quality have on the speech quality and preference of the proposed system. The results of the ANOVA analysis show that neither speaker gender nor speaker identity have any relevant effect on the transplanted speech quality or preference. On the other hand, the speech quality of the neutral model clearly impacts the transplanted speech quality and preference. This impact is further confirmed by the Pearson's product-moment correlation between neutral and transplanted speech quality, taking a value of 0.38. Based on these results we can conclude that the proposed transplantation system is significantly robust against speaker variability, with the attainable speech quality strongly depending on the quality of the quality of the source model. This means we can provide high quality emotional speech synthesis for any speaker as long as we have a high quality neutral speech of the target speaker.

Finally, we carried out a pair of evaluations which considered the interaction between an end user and a robot. These evaluations show that the students clearly prefer when the robot is capable of adapting its way of speaking to the situation, and an informal questionnaire after the evaluations proved that the overall satisfaction with the emotional system was much higher. In the end this shows that there is a lot to gain from implementing expressive and adaptive versions of human–machine interaction systems. This evaluation has also proven how when dealing with specific application domains one has to consider not only the quality of the emotion but also the appropriateness. In our particular case the listeners felt that sadness did not fit the laboratory assistant task, so it would have been better to replace it with a neutral voice.

@&#CONCLUSIONS@&#

We have proposed an emotion transplantation method capable of learning the paralinguistic nuances of any particular emotion in order to transplant them into a new target speaker for whom only neutral read speech recordings are available. This is done by means of chaining a pair of CSMAPLR adaptation functions, one that characterizes the target speaker identity and another that captures the paralinguistic characteristics of the desired emotion. Finally a triplet of perceptual evaluations were carried out.

For the perceptual evaluations, four emotions (anger, happiness, sadness and surprise) from a Spanish emotional database and six target speakers (three male and three female) were considered. A first evaluation compared in terms of naturalness, speech quality and emotional strength a baseline average emotion transplantation with traditional neutral read speech synthesis. The second evaluation compared the proposed emotion transplantation method with neutral speech synthesis, and the final evaluation compared the baseline with the proposed method. The results clearly proved how emotion transplantation greatly increases the perceived emotional strength of the synthesized utterances (up to 1.2 points for the proposed method), also showing a very significant increase in preference (an average of 87% for the proposed system) at a cost of only 0.4 points in speech quality. This results are enforced by the average 70% preference rates in the multiple-choice evaluation. The comparisons also showed how transplanting the correct emotion is a much better alternative than transplanting an average emotion, with significant increases of 0.5 points in perceived emotional strength and a 74% preference rate at a cost of less than 0.1 points in speech quality. We have also seen that there is no correlation between speaker gender or identity and the obtained results, while there is a strong correlation between the source neutral system speech quality and the transplanted quality, meaning that the proposed transplantation system is robust against speaker variability, and is capable of providing high quality emotional models if the source neutral speaker model is of high quality. The transplanted voices provided us with multiple emotional voices for the robots developed in the ARABOT and INAPRA projects, but by imbuing emotions on previously available speakers recorded in just a neutral speaking style.

Future work includes increasing the speech quality obtained through the transplantation process, verifying the method with different expressiveness such as speaking styles, fine-tuning and evaluating the transplanted emotions (and the whole emotional robotic system) in real scenarios such as the Principe Felipe Science Museum, and considering the possibility of applying it across languages to transplant effects known to be language independent such as Lombard speech.

@&#ACKNOWLEDGMENTS@&#

The work leading to these results has received funding from the European Union under grant agreement 287678. It has also been supported by TIMPANO (TIN2011-28169-C05-03), INAPRA (MICINN, DPI2010-21247-C02-02), and MA2VICMR (Comunidad Autonoma de Madrid, S2009/TIC-1542) projects. Jaime Lorenzo has been funded by Universidad Politécnica de Madrid under grant SBUPM-QTKTZHB. The authors want to thank the other members of the Speech Technology Group, ARABOT and Simple4All projects for the continuous and fruitful discussion on these topics. The authors also want to thank the Multimedia Technology Group from Universidad de Vigo and the Speech Processing Group (VEU) from Universidad Politécnica de Cataluña for sharing some of the databases used in this research.

@&#REFERENCES@&#

