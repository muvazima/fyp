@&#MAIN-TITLE@&#Audiovisual temporal integration in reverberant environments

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Explores perceived audiovisual synchrony in reverberant environments.


                        
                        
                           
                           The temporal smear caused by reverberation can alter a signal’s acoustic signature.


                        
                        
                           
                           Reverberating acoustics is a common problem in teleconferencing systems.


                        
                        
                           
                           Reverberation may not have adverse effects on the perceived synchrony for continuous audiovisual speech.


                        
                        
                           
                           The temporal integration of speech syllables and isolated events is affected by the acoustic phenomenon.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Audiovisual speech

Audiovisual asynchrony

Temporal integration

Reverberation

@&#ABSTRACT@&#


               
               
                  With teleconferencing becoming more accessible as a communication platform, researchers are working to understand the consequences of the interaction between human perception and this unfamiliar environment. Given the enclosed space of a teleconference room, along with the physical separation between the user, microphone and speakers, the transmitted audio often becomes mixed with the reverberating auditory components from the room. As a result, the audio can be perceived as smeared in time, and this can affect the user experience and perceived quality. Moreover, other challenges remain to be solved. For instance, during encoding, compression and transmission, the audio and video streams are typically treated separately. Consequently, the signals are rarely perfectly aligned and synchronous. In effect, timing affects both reverberation and audiovisual synchrony, and the two challenges may well be inter-dependent. This study explores the temporal integration of audiovisual continuous speech and speech syllables, along with a non-speech event, across a range of asynchrony levels for different reverberation conditions. Non-reverberant stimuli are compared to stimuli with added reverberation recordings. Findings reveal that reverberation does not affect the temporal integration of continuous speech. However, reverberation influences the temporal integration of the isolated speech syllables and the action-oriented event, with perceived subjective synchrony skewed towards audio lead asynchrony and away from the more common audio lag direction. Furthermore, less time is spent on simultaneity judgements for the longer sequences when the temporal offsets get longer and when reverberation is introduced, suggesting that both asynchrony and reverberation add to the demands of the task.
               
            

@&#INTRODUCTION@&#

Teleconference systems have evolved from being a direct communication platform between two individuals to becoming an extended meeting arena for larger groups of people. With larger groups and larger meeting rooms come larger challenges to tackle, such as reverberating sound components that tend to extend in time as the room size and source distance increase (de Lima et al., 2009). Reverberation is the consequence of the acoustic response from an enclosure (ITU, 2009), characterised by the temporal smearing of an auditory signal. Unlike an echo, which returns one distinct acoustical response, reverberation arises as a mix of acoustical responses from the multiple surfaces of the enclosed space (de Lima et al., 2009). Thus, the sound that finally reaches the ear is a combination of the acoustic waves that have been conveyed directly, and the reflected ones that have been delayed in time (Assmann and Summerfield, 2004). Both the strength and the length of reverberations contribute to influence the experience of audiovisual (AV) quality (Jumisko-Pyykkö et al., 2007). In speech, the resulting effect not only disturbs the experienced quality (de Lima et al., 2008), but also alters the signature and intelligibility of the spoken sounds (Cox et al., 1987).

Specifically, reverberation may transform dynamic speech phonemes into more static elements, thereby flattening formants and blurring the onset and offset of certain consonants and vowels, while extending others (Assmann and Summerfield, 2004). Compared to quiet conditions, reverberation makes it difficult to discriminate pitch (Sayles and Winter, 2008) and it can create confusion among vowels (Cox et al., 1987). For example, the perception of reverberant speech will typically merge the two-vowel sound of a diphthong into a single-vowel monophthong (Nábĕlek, 1988). Furthermore, confusion related to consonant place of articulation and voicing has also been established (Cox et al., 1987), especially for consonants that follow a vowel at the end of a word (Gelfand and Silman, 1979). In line with Kurtovic’s model (1975, described in Gelfand and Silman, 1979), the energy reflected from the preceding vowel is believed to mask the subsequent consonant and thereby make the articulation features less intelligible. This masking would be far less detrimental for a consonant in a word-initial position.

In addition to altering speech sound intelligibility, reverberation leads to confusion in the arrival of an auditory signal, hampering the perceptual capacity to discern the precedence to one ear before the other (Hartmann, 1983). Because this precedence, or interaural time difference, is an important cue for localising sound sources, reverberation contributes to difficulties in establishing the origin of a sound and even retaining attention to it (Culling et al., 1994; Darwin and Hukin, 2000). Relatedly, when tone series are presented in simulated reverberation, as opposed to quiet conditions, it is harder to keep in synchrony with the presented tempo (Naylor, 1992). According to Naylor, the tone envelopes become smoothed to the extent that the tail of one could overlap the onset of the next. This implies that reverberation not only alters the acoustical properties of speech sounds, but acts also on the auditory perception of less complex signals. Moreover, a reverberant environment can hinder sound localisation processes. In a natural environment with several people engaged in a conversation, binaural cues would normally assist in locating the speaker; however, in a teleconference, the reverberation that could arise from the transmission would be detrimental to this process (Nunes et al., 2011). Moreover, the potential disturbance from background noises and voices may serve to enhance the problem of reverberation in teleconferences.

The current study considers simulated reverberation and reverberation recorded from two distinct teleconference rooms. However, instead of looking into the established effect on auditory speech intelligibility, we here explore the potential influence of auditory smearing on temporal perception. Whereas many earlier works have been restricted to auditory perception (Culling et al., 1994; Darwin and Hukin, 2000; de Lima et al., 2008), the current investigation extends this line of research to include not only auditory perception, but also the visual modality. While background noise typically will increase perceptual dependence on visual input (Alm et al., 2009; Sumby and Pollack, 1954), less is known about the perceived correspondence between vision and hearing in the presence of reverberation. One study used reverberant depth cues to demonstrate perceptual alignment to simulated source distances, where greater distances required auditory signals to lag further behind the visual signals for perceived subjective synchrony (Alais and Carlile, 2005). In other words, when the auditory and visual signals happened at the exact same moment in time, participants would not perceive that the two happened simultaneously. Another study found less accurate temporal order judgements for spatially and temporally separated AV signals in reverberant conditions, compared to anechoic conditions (Sankaran et al., 2013). Combined, these findings point to a decreased sensitivity to temporal offsets in the presence of reverberation. Despite the relevance to teleconference systems, as far as we know, no study has been carried out to directly explore the impact of reverberation on the perceived synchrony between auditory and visual speech signals.

Synchrony remains a highly relevant challenge in teleconferencing. Due to the encoding, compression and transmission of audio and video, a temporal misalignment can take place and the two streams will be separated in time (Bang et al., 2009). Short temporal offsets are rarely noticeable, but once they exceed certain durations, they can be detrimental to both the subjective experience of quality (Steinmetz, 1996) and the intelligibility of spoken sounds (Grant and Greenberg, 2001). Nevertheless, no fundamental thresholds mark the transition from perceived synchrony to perceived asynchrony (Roseboom et al., 2009); instead, they vary with the measure and the nature of the AV event (van Eijk et al., 2008). For instance, perceptual tolerance to asynchrony is typically greater for spoken words and sentences than for more action-oriented events, such as a hitting hammer (Conrey and Pisoni, 2006; Dixon and Spitz, 1980). Moreover, the perceptual tolerance to temporal offsets is inherently asymmetric (Maier et al., 2011). Thus, the points of detection tend to reflect a lesser tolerance to asynchrony where the auditory signal precedes the visual signal (audio lead) than to asynchrony where the visual signal arrives first (audio lag) (Dixon and Spitz, 1980). These points are typically represented as thresholds, and are defined by the temporal offset required for synchrony to be perceived at a given rate, for instance 50% of the time (Conrey and Pisoni, 2006). The thresholds also define the window of temporal integration (Keetels and Vroomen, 2012), within which sensory inputs from two modalities are considered to be aligned in time.

Perceived synchrony in speech varies depending on the sound, with asynchrony noticed at shorter offsets for bilabial stops than for the less visibly articulated velar and alveolar stops (Vatakis et al., 2012). Although the two sounds are distinct in their manner of articulation, the temporal integration of labiodental fricatives is fairly similar to that of bilabial stops (Vatakis et al., 2012). However, the more gradual frequency increase in the voicing of fricatives (McMurray et al., 2008) and the more noise-like nature of their articulation (Schwartz et al., 2012) could make fricatives more vulnerable to acoustic disturbances such as reverberation. For continuous speech stimuli, the window of temporal integration tends to be fairly extensive. For example, in a temporal order experiment where stimuli included a spoken sentence, the audio lead and lag thresholds were established at 118ms and 190ms, respectively (Vatakis and Spence, 2006). Another study assessing the perception of simultaneity for spoken sentences found thresholds at 131ms for audio lead and 225ms for audio lag (Conrey and Pisoni, 2006). Yet another experiment used a discrimination task where participants had to indicate which of two spoken sentences was out of synchrony, for the unfiltered speech stimuli the audio lead threshold was approximately 60ms and the audio lag threshold was around 230ms (Grant et al., 2003). These results illustrate how diverse windows of temporal integration can be, but they still have two things in common. One is a characteristic asymmetry, which portrays how the perceptual system is more sensitive to asynchrony when the auditory signal arrives before the visual, than vice versa. The other relates to the extent of the reported thresholds for continuous speech stimuli, the audio track can lag behind the video with more than 200ms and still go unnoticed. Thus, the perceptual tolerance to asynchrony in continuous speech is asymmetric, but fairly robust.

This study addresses two technical challenges relevant to teleconference systems, reverberation and asynchrony, and explores the temporal integration of visual signals with reverberant auditory signals in two experiments. Knowing that reverberation alters the temporal signatures of certain speech characteristics (Cox et al., 1987), and may even mask subsequent speech sounds (Gelfand and Silman, 1979), we assume that increased reverberation will lead to increased temporal distortion. By creating a temporal ambiguity, we expect reverberation to contribute to a greater perceptual tolerance to AV asynchrony by widening the temporal window of integration. Furthermore, due to the extended masking that may result when the audio precedes the video, we believe that the increased tolerance will be most prominent for auditory lead asynchrony. To explore whether the temporal distortion expected from reverberation may differ between speech sounds and less complex auditory signals, the first experiment looks at two speech scenarios, as well as an action-oriented scene. This experiment also addresses the time spent by participants when evaluating the simultaneity of AV presentations at different levels of asynchrony and reverberation. Because uncertainty and cognitive load may vary across conditions, the evaluation times could contribute with insights on the effort required to make simultaneity judgements. The second experiment follows up the first findings by focusing on isolated speech events to investigate the possibility that the continuity of spoken sentences can serve to mask the reverberating elements that follow the speech signals. The inclusion of two syllables with dissimilar manners of articulation allows for an exploration of the potential difference in temporal integration, along with the influence from reverberation.

The first experiment explores the perception of synchrony for continuous speech in reverberant conditions that simulate two different teleconference rooms. We aimed to make the setting realistic and to use stimuli with natural speech production and a familiar action event.

@&#METHOD@&#

Because sound conditions are unique to any enclosure, we used simulated reverberation to keep the experiment conditions consistent and comparable. Hence, auditory signals were filtered using impulse responses recorded from two teleconference rooms that differed in size and reverberation time. The experiment was set up in a university meeting room, with video presented on a computer screen and audio through loudspeakers. In order to explore temporal integration across the different conditions, we chose to use simultaneity judgements (SJ) due to the relative ease of the task. The SJ task requires participants to attend to an AV presentation and decide whether they perceive the audio and video to be synchronous or asynchronous, or simultaneous versus successive (van Wassenhove et al., 2007). This measure is used to derive the audio lead and audio lag thresholds, as well as the temporal offset where participants are most likely to perceive synchrony, referred to as the point of subjective simultaneity (PSS) (van Eijk et al., 2008).

To explore the effect of auditory reverberations on the temporal integration of audio and video, we selected three AV sequences: News, P.M., and Chess. With teleconference systems as the applied scenario, two of the sequences contained television excerpts that represented well-known speech scenarios. The Chess content, which contained no speech, served as a predictable object-action sequence to compare with the dynamic speech contents. Further details on the three sequences are provided in Table 1
                           . We made sure that the duration was kept the same for all video sequences by selecting an excerpt from the News content that was logical and semantically representative in isolation, and where the anchor paused in silence both before and after the presented news. From this basis, we established a common duration of 13s and found an equally logical excerpt from the P.M. content that fit within these constraints. For the Chess content, the 13s included five full moves on the board. All editing was performed with Audacity (2.0.1) (Audacity Team, 2012), Praat (Boersma and Weenink, 2012), and Final Cut Pro (10.0.8). Audio files were recorded and edited in stereo, Chess at 44.1kHz and the others at 48kHz. They were also normalised to keep the average audio intensity at 70dB. Similarly, all videos were exported with their resolution set to 1024×576pixels.

Before manipulating the asynchrony of the video sequences, we ensured that their synchrony was maintained throughout. Admittedly, temporal alignment cannot be guaranteed when audio and video tracks are edited and encoded separately in this manner (Bang et al., 2009), but note that researchers have recently introduced techniques for the accurate alignment of audio and video streams (Maier et al., 2011).

We controlled the synchrony of the speech sequences by comparing the visual lip movements for voiced initial syllables and the corresponding spectrograms. Similarly, for the Chess sequence we verified that the moment the chess piece made contact with the board coincided with the auditory event on the spectrogram. The experimental asynchrony levels we applied to the AV presentations were established over several steps in an earlier study (Eg et al., in press). Knowing that the perceptual system is more sensitive to audio lead asynchrony and that the temporal offsets need not exceed 300ms audio lead and 500ms lag (Conrey and Pisoni, 2006; Grant et al., 2003; van Wassenhove et al., 2007), we tried out this range in a pilot study. Because of the higher sensitivity, we applied smaller temporal steps for audio lead asynchrony, and in the end we narrowed down the range to 200ms audio lead and 400ms audio lag.

When introducing asynchrony, we kept the video tracks consistent so that the visual onsets and offsets remained the same across all temporal misalignments, thereby eliminating visual cues. In other words, only the audio tracks were displaced according to applied levels of asynchrony. Our selection criteria ensured that the temporal displacements did not exceed the duration of silence at the start or end of any of the sequences, so that the videos were always playing and no parts of the auditory signals were lost. By editing out the duration of the temporal offset at the start of the audio track, audio lead asynchrony was applied at 50ms, 100ms, 150ms, and 200ms. Meanwhile, including a small part of the audio track preceding the selected sequence served to apply audio lag asynchrony at 100ms, 200ms, 300ms, and 400ms.

To simulate reverberation, audio tracks were manipulated using impulse responses recorded in two of Cisco’s teleconference rooms in San Jose, California. Impulse responses used to filter audio tracks for the 0.64s long reverberation condition, Reverb1, came from recordings from a small conference room designed for 4 people, which measured ≈3.5 (w)×4 (l)×3 (h)m. Recordings for the 0.54ms long reverberation condition, Reverb2, were carried out in a large conference room that could seat up to 16 people and measured ≈6.5 (w)×8.5 (l)×3 (h)m. The non-reverberant condition is referred to as Quiet. Fig. 1
                            includes examples of audio from the three contents presented in quiet and reverberation.

@&#LIMITATIONS@&#

In our first experiment, we aimed for ecological validity. Considering that reverberation is dependent on acoustic responses from an enclosure, we surmised that the most natural listening conditions would come from audio presented through loudspeakers. However, this approach introduces added reverberation from the test room used in the experiment. Furthermore, any audio recording performed outside an anechoic environment is bound to have reverberating acoustical elements mixed into the signal. With these precautions in mind, we cannot establish a quantification of the reverberation that our participants were exposed to. However, we provide a summary of all associated parameters we can derive from measurements or estimations in Table 2
                           . The reverberation of the test room used in the experiment is reported as the time it took our recorded sound signals (balloon bursts) to decay by 60dB, referred to as T
                           60. We also performed blind estimations of the T
                           60 of our experimental audio files, and the same files recorded while playing in the test room, applying a method developed by Löllmann et al. (2010) and evaluated by Gaubitch et al. (2012).

A total of 9 female and 11 male participants, aged 20–38years (M
                           =25.60, SD
                           =4.35), were recruited from the University of Oslo. All were fluent in the Norwegian language. Participants received information about the experiment and the relevant ethical considerations, and gave their consent, prior to the experimental task.

@&#PROCEDURE@&#

We tested participants individually in a meeting room at the University of Oslo, with videos presented using the Superlab software running on a 2.53GHz MacBook Pro with a 15″ monitor (1440×900pixel resolution). They sat approximately 70cm from the monitor, with two Logitech Z4i satellite speakers (8.5W each, >92dB S/N, 35Hz–20kHz frequency response) placed immediately next to the monitor, on each side, and the subwoofer turned off. Participants were asked to attend to the audio and the video and to decide whether they perceived them to be in synchrony or not. Responses could be given at any time during the presentation by pressing one of two labelled buttons on the keyboard. The labels read “sync” and “async”. Given the long duration of the presentations, the time taken to make a response is not equivalent to reaction time. Instead, the time measures are considered as representative of a conscious decision-making process and are included and analysed as evaluation times.

Evaluation times varied between individuals, some responded only at the end of the 13-second presentation, whereas others responded sooner; as a consequence, participants progressed through the stimulus blocks at variable paces. We decided that all participants needed to complete a minimum of six blocks, corresponding to six repetitions of every stimulus condition. Further on, the progressed duration at the completion of the first six blocks determined whether participants would have to complete one or two additional blocks, or none at all. Thus, the number of repetitions varies between participants, from 6 to 8. All stimulus conditions, 81 trials, were randomised separately for every participant and for every block, making the total number of trials 486, 567, or 648, depending on the number of repetitions. We set the maximum duration of the full experiment to 90min, and we included breaks at the completion of every second stimulus block.

@&#RESULTS AND DISCUSSION@&#

Because of the unequal number of repetitions completed by participants, we converted the simultaneity judgement scores to ratios and averaged them across repetitions. Individual Gaussian curves, distributed across asynchrony levels, were then fitted for each content and sound condition. The ideal temporal offset for the subjective perception of synchrony is rarely zero, but is instead defined by the means of the fitted curves and is here represented as PSS. The average of all curve means thus determined the PSS for each stimulus condition, as illustrated in Fig. 2
                           . The full-width at half-maximum of each curve established both the temporal window of integration and the temporal thresholds for lag and lead asynchrony (Conrey and Pisoni, 2006). The temporal thresholds correspond to the points where synchrony and asynchrony are reported at equal rates, at chance level. We assessed potential outliers from the temporal thresholds by excluding any scores that exceeded the value of the maximum temporal offset. With this criterion, scores from one female participant were excluded from the analyses for the Chess content. Fig. 3
                            shows the 50% thresholds for audio lead and audio lag asynchrony, highlighting the different impact of reverberation on the speech and action stimuli. Repeated-measures ANOVAs (Howell, 2002) explored the effect of asynchrony and reverberation, along with their interaction, on the subjective perception of synchrony. Based on the differences between distributions, we ran separate analyses for each content; all results are presented in Table 3
                           . Effect sizes are reported with the partial-eta square statistic (ηp
                           
                           2).

Analyses uncovered main effects of asynchrony for all contents, as seen in Table 3, but the influence from reverberation was only significant for the Chess sequence. The interaction between reverberation and asynchrony reveals a prominent shift from audio lag to audio lead asynchrony, which is reflected in the PSS, Fig. 2, and the temporal thresholds, Fig. 3a. However, Figs. 2 & 3b and c illustrate the consistency in the temporal perception of the speech sequences, in quiet or in any of the two reverberating scenarios. The windows of temporal integration for News and P.M. correspond well to earlier works (Conrey and Pisoni, 2006; Dixon and Spitz, 1980), with the characteristic asymmetry that points to greater perceptual tolerance when visual signals precede, rather than succeed, auditory signals (Grant et al., 2003). With the missing impact of reverberation on the temporal integration of AV speech, it is clear that participants were equally able to judge AV synchrony in quiet and reverberant environments. Considering the severely audible reverberation, the lack of an effect on the temporal perception of AV speech is surprising. Although the stimuli preparations and the experimental set-up have likely interacted with the applied reverberation, hindering a possible quantification of the effect, we still observe an impact on the perceived synchrony of the chess event. Keeping in mind that we used the same procedure for all stimuli, we deem it unlikely that the lack of an effect on temporal speech perception could solely be attributed to the outlined limitations. Seemingly, reverberant conditions can contribute to misidentifications of vowels and consonants (Cox et al., 1987), but will still not affect the perceived temporality of speech. Although the acoustical signal is likely distorted from the reverberation (Culling et al., 1994), the temporal distortion may affect nothing more than the intelligibility of the speech sounds. Moreover, as several studies have demonstrated, the perception of synchrony for continuous speech is very resilient to temporal offsets (Conrey and Pisoni, 2006; Dixon and Spitz, 1980; Grant et al., 2003; Vatakis and Spence, 2006), and this finding certainly appears to extend to reverberant environments.

On the other hand, the results for the Chess segment tell a different story. Judging from the PSS, longer audio lead times are required for synchrony to be perceived in reverberation, regardless of room size. Related to this result are the audio lead and lag thresholds that become significantly skewed with reverberation, to the point where participants were unable to perceive audio lead asynchrony for the temporal offsets presented in the experiment. However, the room size showed no significance on the influence of the reverberant environment. With the absent effect of reverberation upon the temporal integration of speech, the severe impact observed for the Chess sequence was also surprising. Still, when taking into account the contrast found when comparing the perception of synchrony for AV speech and more action-oriented events (Dixon and Spitz, 1980; Vatakis and Spence, 2006), it is plausible that these results can be attributed to the isolation of the AV event. Unlike the continuous speech of the news anchor and the prime minister, the acoustical signal of the chess piece touching the board was brief and not masked by preceding and succeeding events. In effect, the temporal smearing of the auditory signal was allowed to stretch further in time without interruptions, and the signal itself could have been modulated. The extension of the tail end of the signal would likely have caused ambiguity around the conclusion of the sound, and the modulation may have made it less audible. In effect, the visual capture facilitated by the altered sounds appears to make audio lead asynchrony close to impossible to perceive for these isolated events.

Analyses of evaluation times are included to investigate whether participants find it more challenging to make judgements of simultaneity in reverberant than in undisturbed conditions, in which case they would presumably require longer time to evaluate the two options. Any evaluation time that exceeded the duration of the video sequences was excluded as an outlier, which was the case for 246 of the 12,380 presentations (<2%). The remaining evaluation times were averaged across stimulus repetitions for each participant. Three repeated-measures ANOVAs, one for each content, yielded the results presented in Table 4
                            and Fig. 4
                           .

As seen from the analyses presented in Table 4, participants’ evaluation times reflect an effect of asynchrony for all contents, as well as main effects of reverberation for Chess and News. The distributions presented in Fig. 4 illustrate how evaluation times are, for the most part, shorter with long temporal offsets. For the two speech sequences, participants required more time to make judgements on synchrony when the temporal offsets were close to objective synchrony. Arguably, these asynchrony levels should be harder to discern and would therefore require more consideration. In line with this result are the shorter evaluation times observed for speech sequences presented with audio and video in synchrony. Reductions in evaluation times also seem to follow the ease of synchrony perception for the Chess sequence. Considering the PSS estimated for Chess, presented in Fig. 2 with and without reverberation, these are less skewed towards the audio lag direction, compared to News and P.M. It follows that simultaneity judgements should be less demanding with audio lag than audio lead asynchrony, consistent with the observed evaluation times.

Oddly, the main effects for reverberation show different trends for Chess and News. With average evaluation times ranging from 6.73s and 6.75s for Reverb1 and Reverb2, respectively, to 7.11s in quiet, simultaneity judgements for the Chess content were made quicker in reverberation. Conversely, for the News content, the evaluation times increased from 5.58s in quiet to 5.64s for Reverb1 and 5.83s for Reverb2. Thus, the time spent on simultaneity judgements for News is longer with reverberation than in quiet, but all over they are shorter than for Chess. However, the interaction with asynchrony was only significant for the Chess sequence. As seen from Figs. 2 and 3, audio lead asynchrony was difficult to discern for the Chess content presented in reverberation and this result appears to be reflected in the judgement-making process. The figures convey a tendency for Reverb2 to increase evaluation times when temporal offsets were short and simultaneity judgements were likely harder to make. In the audio lag direction, an opposite tendency can be found, with shorter time spent on reverberating sequences. As mentioned, this is likely connected to the relative ease of discerning audio lag asynchrony for the Chess sequence, and the related difficulties in discerning audio lead asynchrony. Accordingly, the shorter evaluation times seem to correspond to the less demanding simultaneity judgements. The overall decrease in time spent on evaluating the Chess sequence as audio lag asynchrony increases seems consistent with the trend seen for the speech sequences as audio lead and lag asynchrony becomes greater. In general, more time was spent on simultaneity judgements for AV sequences when asynchrony and/or reverberation conditions should implicate higher cognitive loads.

To further explore whether the lack of impact from reverberation on temporal integration could be attributed to continuously changing speech sounds, we ran a second experiment with isolated speech syllables.

@&#METHOD@&#

Again we used simulated reverberation, this time manipulating the sound signals directly with parameters from the Reverb1 condition.

In this experiment, we used syllables spoken by a young adult Norwegian female, visualised in Fig. 5
                           . The recordings (25fps, 48kHz) were carried out in a sound-isolated and damped Voice-Over-Booth studio, where the floor measured 2.1×3.6m and the ceiling height was 2.15m. According to the manufacturer’s specifications, the studio’s T
                           30 decay time for wideband speech is 0.15s.

We selected two AV syllables to investigate how reverberation affects isolated speech sounds. We used a voiced, bilabial stop /ba/ because of the easily discernable articulation and the clear correspondence between the auditory and the visual consonant burst. For a less visible articulation, we used a voiced, labiodental fricative /va/. This consonant is characterised by the manner of articulation, where air is forced through a narrow channel, leading to soft, but noise-like, sound that could potentially be disturbed by reverberating sound components. The audio files were edited and manipulated as mono channels and then duplicated and exported in stereo, normalised to an average intensity of 70dB. We used the GVerb
                              1
                              Developed by Juhana Sadeharju.
                           
                           
                              1
                            plug-in for Audacity to manipulate the reverberation for the speech audio files, setting the T
                           60 decay time to 0.64s and the room measurements to 14m2, corresponding to the specifications for Reverb1 in Experiment 1. The acoustic spectra of the syllables, with and without the reverberation manipulation, are presented in Fig. 6
                           .

Before video editing, we made sure that the durations of the articulations were similar for both syllables (≈400ms), and that the silence before and after the articulations were consistent across both (≈900ms before and ≈700ms after). The 2-second long speech videos were thereafter imported to Final Cut Pro together with the related audio files. We confirmed that the recorded audio and video were in synchrony by comparing the voice onsets and bursts of the acoustic spectra and the visual articulations. With respect to asynchrony, we used a similar technique as for Experiment 1, keeping the video track constant and shifting the audio track to adjust the temporal misalignment. As before, we ensured that the temporal shifts did not extend beyond the video onsets or offsets. We also extended the asynchrony levels of the first experiment by increasing the temporal resolution, thus the applied audio lead asynchronies were set to 40ms, 80ms, 120ms, 160ms, 200ms, and 240ms, while audio lag asynchronies included 80ms, 160ms, 240ms, 320ms, 400ms, and 480ms.

We recruited 5 female and 10 male participants between the ages of 24 and 58years (M
                           =32.33, SD
                           =8.49) from Simula Research Laboratory, all fluent in Norwegian. Participants were provided with information on the experiment and relevant ethical considerations, and gave their consent before commencing the experiment.

@&#PROCEDURE@&#

Participants did the experiment individually, seated in a small private office where the lights were off and curtains were shading the natural daylight. We used the Superlab software to run the experiment and presented the videos at a resolution of 1440×900pixels on a 21.5″ iMac (2.7GHz). Audio was conveyed through SoundBlaster Tactic 3D headphones. We asked participants to pay attention to the correspondence between the visual articulation and the speech sound and to determine whether the two were in synchrony or not. They responded by pressing the “s” key on the keyboard if they perceived the presentation to be synchronous and “a” if they perceived it to be asynchronous. The experiment included six repetitions of every stimulus condition, divided into six blocks and randomised separately for every participant and for every block. With 52 trials per block, the total number comes to 312 trials. Participants had the opportunity to take two short breaks; therefore, the full duration varied between 20 and 30min.

@&#RESULTS AND DISCUSSION@&#

Due to the short duration of the speech syllables, we do not consider evaluation times in Experiment 2. Hence, the focus is on participants’ simultaneity judgements and the derived measures for temporal integration.

We analysed the simultaneity judgement responses as described in Section 2.2.1, deriving individual averages for perceived synchrony for every stimulus condition, as well as PSS scores and temporal integration thresholds. We ran a repeated-measures ANOVA on the perceived synchrony means to investigate the effects of reverberation and asynchrony on the temporal integration for the /ba/ and /va/ syllables. The ANOVA results are summarised in Table 5
                           , while the overall PSS averages are plotted in Fig. 7
                           , and the temporal thresholds are presented in Fig. 8
                           .

In contrast to Experiment 1, reverberation did have an influence on the temporal integration of the AV syllables. The significant interaction between asynchrony and reverberation, listed in Table 5, reveals that the perception of synchrony changes depending on the sound condition. This is also reflected in the PSS values seen in Fig. 7, where subjective simultaneity is closer to zero in the reverberant condition, compared to the undistorted condition. In fact, the PSS changes by 30ms for /ba/ and 43ms for /va/, indicating a temporal shift for perceived synchrony. The temporal thresholds, visualised in Fig. 8, shed further light on the interaction. In reverberation, the window of temporal integration increases relative to the quiet condition, but only in the audio lead direction. With the thresholds differing by 63ms and 87ms for /ba/ and /va/, respectively, the temporal smearing is having a marked influence on the perceptual tolerance to asynchrony. This increase is consistent with the effect observed for the Chess sequence in Experiment 1. However, the entire window of temporal integration shifted when reverberation was introduced in the Chess sequence; for the speech syllables, the audio lag thresholds remain fairly constant. In other words, the perceptual tolerance to audio lead asynchrony appears to be extended in reverberation, whereas the tolerance to audio lag asynchrony remains unaffected. The expansion of the audio lead thresholds corresponds well to our prediction that reverberation contributes to an ambiguous sound onset and extends the tail of the auditory signal, thereby providing a greater temporal window to compensate for an audio signal that arrives before the visual signal. When speech sounds are presented in isolation, this effect is not masked by the onset of new speech sounds, which could explain why the effect of reverberation was absent for the continuous speech stimuli in Experiment 1.

Although our results revealed no significant interaction between reverberation and syllable, nor a main effect of syllable, the observed windows of temporal integration are larger for the /va/ syllable. Furthermore, we did find a significant interaction between asynchrony and syllable. This interaction signifies a difference in synchrony perception between the bilabial stop and the labiodental fricative. Indeed, the distribution seen in Fig. 8 illustrate the wider window of temporal integration for the /va/ compared to the /ba/. The PSS measures in Fig. 7 also show that the temporal integration for /ba/ is centred closer to objective synchrony than /va/. From this, we deduce that the less visible manner of articulation of /va/ makes the perception of synchrony more tolerant to temporal offsets. Furthermore, although non-significant, the window of temporal integration did increase more in reverberation for the fricative than for the stop, possibly due to the turbulent, noise-like articulation of the /v/.

@&#CONCLUSIONS@&#

Following the premise that reverberation leads to an alteration of the temporal signature of an acoustic signal, this study has investigated the influence of a reverberant environment on AV temporal perception. Considering the technical challenges related to teleconference platforms, and the associated perceptual consequences, the study has specifically addressed how the co-occurrence of asynchrony and reverberation affects the temporal integration of both continuous and isolated AV speech. In addition, an action-oriented sequence was included to shed light on the differences in temporal integration that are often found between speech and culminating events.

With respect to the two long speech sequences, the temporal smearing resulting from reverberation had no impact on the temporal integration of the auditory and visual signals. Similar to previous findings (Conrey and Pisoni, 2006; Dixon and Spitz, 1980; Grant et al., 2003; Vatakis and Spence, 2006), the current thresholds for perceived synchrony fell in favour of a temporal robustness when it comes to continuous speech. Moreover, if these results are applicable to a broader speech context, they imply that the temporal ambiguity that arises from reverberations could be masked by subsequent speech events. In natural and dynamic speech, one syllable tends to follow the other, and this could possibly cover up or work against the temporal smearing. All in all, the established windows of temporal integration, with and without reverberation, indicate that the perceptual system operates with a temporal buffer when integrating sensory signals across modalities and this buffer is particularly resilient to auditory signals lagging behind. The implications of such a theoretical buffer offer good news to providers of teleconference systems, suggesting that perfect objective synchrony is not required for these services.

On the other hand, when speech is presented as isolated syllables, with no preceding or succeeding speech sounds to mask the acoustic tail, the temporal smearing from reverberation becomes influential on the perceptual integration process. Under reverberant conditions, the windows of temporal integration extended further in the audio lead direction for both the bilabial stop and the labiodental fricative. This finding adds support to our assumption that the continuity of everyday speech may contribute to a masking of the acoustical tail that arises from reverberation. Although this assumption could alleviate concerns with respect to the applied teleconference scenario, we would rather emphasise the implications of the uncovered effect. Possibly, the impact of reverberation is exclusive to isolated events, speech or otherwise, but even when absent, the effect may merely be masked. If so, the presence of reverberation is still affecting the perceptual system and may even add to the cognitive load, which in turn would make a conversation in reverberant environments more demanding for the listener. Despite all the efforts put into making teleconferences as natural as possible, people still tend to find these systems more exhausting in the long run than face-to-face communication. As a consequence, reverberation may decrease the endurance of those engaged in a teleconference.

As surmised from related studies (Dixon and Spitz, 1980; Vatakis and Spence, 2006), the temporal integration does indeed differ between the AV action-oriented event and the continuous speech sequences. In the absence of reverberation, the temporal thresholds for the Chess sequence show a more symmetrical distribution between audio lead and lag asynchrony. Likely, the distinctiveness and relative slow movement of the chess piece touching the board, as compared to the dynamics of continuous speech, create an uncertainty regarding the specific moment of impact. This ambiguity may in turn extend the window of temporal integration. Further uncertainty arises from the reverberant environments, with the extension of the isolated signals’ acoustical tails. Combined with the visual ambiguity of the chess move, the extended tail of the chess sound could facilitate an auditory capture of the delayed visual event. In fact, with reverberation smearing the sound of the Chess sequence, the magnitude of the window of temporal integration does not change much. However, it is shifted even further in the audio lead direction. Although audio lead asynchrony could go almost unnoticed for an event with an ambiguous moment of impact, the reverberant environment has a clear detrimental effect on AV temporal integration.

Judging from the windows of temporal integration, particularly without reverberation, we assume that audiovisual synchrony is harder to perceive for isolated syllables than for continuous speech. Most noticeable are the audio lead thresholds for the /ba/ and /va/ syllables, that are approximately 100ms longer compared to the two long speech sequences. This greater temporal tolerance is likely linked to the single audiovisual event contained within the syllable stimuli. In contrast, the 13s duration of the long speech stimuli provide numerous audiovisual articulations that serve as reference points to the temporal alignment of the two modalities. With regards to the syllables, we also observed a difference in the temporal integration, where the significant interaction with asynchrony points to greater temporal sensitivity for the voiced, bilabial stop /ba/ compared to the voiced, labiodental fricative /va/. Previous studies on the temporal integration of isolated speech syllables have not found the same difference between similar speech sounds (Vatakis et al., 2012). We surmise that our uncovered effect could be the result of the experimental task. We asked our participants to judge the audiovisual synchrony, whereas Vatakis and colleagues used temporal order judgements. Some have speculated that the SJ task could be more sensitive to temporal variations between audiovisual stimuli (Vatakis et al., 2008), and this could explain how we found a difference between the bilabial and labiodental syllables.

With evaluation times as an added measure, we also assessed potential variations in cognitive load that could be attributed to asynchrony and reverberation. Our analyses show that the time spent on judging simultaneity did differ according to perceived synchrony. With the exception of the Chess sequence presented with audio leading the video, time spent on judging conditions with extreme temporal offsets was shorter than for presentations closer to objective synchrony. Fully synchronous presentations also tended to be evaluated more quickly than presentations with asynchrony levels close to the temporal thresholds, which are harder to discern. For the Chess sequence, where reverberation led to an extremely tolerant temporal perception of audio lag asynchrony, evaluation times were shorter when compared to the quiet condition. Again the presumed cognitive load of the simultaneity judgement task was associated with time spent on the task. Consequently, these results demonstrated perceptual consequences that go beyond temporal integration. AV presentations that were perceived as synchronous more than half of the time were still affecting the cognitive processing, adding load to an already hardworking system. As mentioned, the potential added cognitive load caused by reverberant environments might have adverse consequences for teleconference participants.

The increased tolerance to audio lead asynchrony observed for the syllables and the Chess sequence is bound to affect perceptual processes beyond temporal integration. In the best case, reverberation may only affect the perceived quality; in the worst case, it may also affect the intelligibility of the AV content. Indeed, past research has demonstrated the detrimental effect of reverberation on many auditory perceptual processes, among them vowel (Cox et al., 1987; Nábĕlek, 1988) and consonant (Cox et al., 1987; Gelfand and Silman, 1979) comprehension, along with sound localisation (Culling et al., 1994; Darwin and Hukin, 2000). Accordingly, semantic and phonetic AV processes could very well suffer in the presence of reverberation. While teleconferencing was the main focus of the study, few enclosed environments are free from this acoustical disturbance. With our findings we have demonstrated the potential implications on the perception of AV synchrony, but we assume that the temporal integration process does not take place in isolation. Other perceptual processes are likely to be affected, and the cognitive demands are equally likely to escalate.

@&#ACKNOWLEDGEMENTS@&#

The authors would like to thank the team at Cisco Norway for the collaboration and their assistance with the stimuli material. They also extend their gratitude for the helpful feedback provided by the anonymous reviewers.

@&#REFERENCES@&#

