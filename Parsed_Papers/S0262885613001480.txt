@&#MAIN-TITLE@&#Mixtures of Gaussian process models for human pose estimation

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Novel algorithm for large scale human pose estimation problems.


                        
                        
                           
                           Uses multiple Gaussian processes in a mixture of expert framework.


                        
                        
                           
                           Allows the accurate regression of Gaussian processes to be scaled to large data.


                        
                        
                           
                           Algorithm gives state of the art performance on 3 pose estimation data sets.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Computer vision

Gaussian processes

Human pose estimation

Mixture of experts

@&#ABSTRACT@&#


               
               
                  Discriminative human pose estimation is the problem of inferring the 3D articulated pose of a human directly from an image feature. This is a challenging problem due to the highly non-linear and multi-modal mapping from the image feature space to the pose space. To address this problem, we propose a model employing a mixture of Gaussian processes where each Gaussian process models a local region of the pose space. By employing the models in this way we are able to overcome the limitations of Gaussian processes applied to human pose estimation — their O(N
                     3) time complexity and their uni-modal predictive distribution. Our model is able to give a multi-modal predictive distribution where each mode is represented by a different Gaussian process prediction. A logistic regression model is used to give a prior over each expert prediction in a similar fashion to previous mixture of expert models. We show that this technique outperforms existing state of the art regression techniques on human pose estimation data sets for ballet dancing, sign language and the HumanEva data set.
               
            

@&#INTRODUCTION@&#

Gaussian processes are a powerful technique for accurately modelling noisy regression problems. Discriminative human pose estimation, where we wish to estimate the articulated pose of a human directly from an image feature, can be solved using regression techniques to gain a fast and accurate pose estimate. However the mapping from image features to pose is multi-modal due to the ambiguities inherent in estimating a 3D pose from a single 2D image. In this paper we evaluate a mixture of Gaussian processes model, where each model represents a local part of the data set. This allows us to overcome the primary limitations of Gaussian processes, their O(N
                     3) training complexity, and their uni-modal predictive distribution.

Mixture of expert models [13,3] have received a lot of attention in discriminative pose estimation [17,23,6,25,14]. These models employ multiple linear regression models, each acting as an expert for a local region of the data set. The predictive distribution is a mixture of Gaussian distributions where each Gaussian component is given by one of the experts and the priors are given by a logistic regression model. This distribution is able to model the multi-modal pose distribution, giving a predictive mean and variance for each mode. These models are learnt using an expectation maximisation algorithm in a maximum likelihood formulation. Although each of the experts is a linear model, a mixture of experts model is able to model non-linear functions by approximating the function using piecewise linear chunks.

Alternatively, non-linear feature transforms such as Gaussian kernels can be used to allow each expert to model non-linear regions. However, obtaining good kernel hyper parameters can be challenging, often requiring an expensive cross validation procedure. These models also suffer from a degenerate variance formulation where the uncertainty of a prediction starts to decrease as the test point moves away from the training data, resulting in confident but incorrect predictions.

In this paper we use a model consisting of multiple Gaussian process experts, each expert modelling a local region of the data set. We demonstrate how to adapt the mixture of expert framework such that it can be applied to Gaussian processes. This allows us to take advantage of the attractive properties of Gaussian processes, namely their ability to infer accurate kernel parameters from training data and their accurate uncertainty formulation. Our proposed model also allows their computational limitations to be overcome allowing our model to scale to large data sets. We show that this model outperforms other state of the art regression techniques for discriminative human pose estimation.


                     Section 2 discusses the relevant work, Section 3 details our mixture of Gaussian process model, and Section 4 evaluates the model and compares it to other state of the art regression techniques. Finally, Section 5 concludes and suggests directions for future work.

@&#RELATED WORK@&#

Mixture of experts models [13,3] have been used extensively for discriminative human pose estimation [23,14,24,17,6]. These models allow for fast prediction of test data due to their simple inference procedure. For linear experts, the computational complexity of predicting test data is constant with respect to the training data size [6]. However, when using a kernel to allow each expert to model a non-linear function test inference scales linearly with the training data. Agarwal and Triggs [1] compare using a relevance vector machine (RVM) and support vector machine 
                     (svm) to a standard kernel linear models for human pose estimation. These models are able to find a minimal set of training examples required for accurate test prediction. This sparse representation allows the model to generalise better to test data, improving both test accuracy and computation time.

Thayananthan et al. [25] use an expectation maximisation algorithm for training a mixture of rvms. However instead of using a logistic regression model to select the prediction experts, they use a bank of Kalman filters and a generative model to select the correct pose. Relevance vector machines are a special case of a Gaussian process where the kernel function has an explicit dependence on the training data [18]. The kernel formulation of an rvm suffers from the property that a test point that lies sufficiently far from the training data will have a predictive variance of zero — resulting in an incorrect but confident prediction.

Gaussian processes (GPs) have been used in a variety of different ways for human pose estimation for learning discriminative mappings [30,27,10,5,29] as well as dynamical models [28,12,29]. Zhao et al. [30] show that a single gp gives comparable performance to a mixture of experts' model on 3D human pose estimation. However a single gp can only be applied to small data sets of a few hundred points due to its O(N
                     3) training complexity. Urtasun and Darrell [27] employ online local Gaussian processes where each gp models a local region of the data set. They train sets of gp hyper parameters offline on different regions of the training data. For a test image, they construct multiple online gps, each centred around a different nearest neighbour of the test input. A pose prediction is then made by a weighted combination of the online gp predictions where the weights are given as a function of the inverse predictive variance for each model. The motivation for this approach is that the different modes of the predictive distribution should be represented in the selection of training points found using nearest neighbours. This has two significant drawbacks, firstly the computation cost of constructing each online gp makes test inference slow, and secondly, weighting the predictions using the inverse predictive variance causes the model to bias modes with less signal noise. Bo and Sminchisescu [5] introduce twin Gaussian processes for human pose estimation where they learn a structured output. Rather than learning an independent model for each pose joint, they learn a model that represents the global pose of the subject, enforcing structure between the output variables. They show that this improves tracking accuracy compared to a normal gp with no structure imposed on the outputs.

Ek et al. [10] use a Gaussian process latent variable model (GPLVM) to learn a relational mapping between the image and the pose space. This learns a shared latent space between the image feature and the pose space, using a gp to map from the latent points to the features and pose. Test inference for a single image is performed by finding the latent point which maximises the likelihood of the mapping to the feature space. The pose mapping is then used to obtain a predictive pose from the optimal latent point. By learning the GPLVM with a dynamics constraint, they are able to jointly optimise an entire sequence of poses resulting in more accurate tracking. They demonstrate that their single frame method gives similar performance to an rvm, but by optimising over an entire sequence they are able to obtain significantly better results. Gupta et al. [11] use a similar approach but learn a discriminative mapping from the image feature to the latent space. This allows for fast initialisation of the latent points, and a generative model is used to optimise the predicted pose from the initial latent points.

The above methods still rely on learning a single GP to model the entire data set. Chen et al. [9] extend the above technique by manually partitioning their data set and using a shared space GPLVM to model each partition. A Markov model is used to learn the sequences of partitions allowing the pose to be inferred from test sequences. Memisevic et al. [16] use shared kernel information embedding to learn a similar model to [10]. This model uses Gaussian kernels instead of Gaussian processes to learn the latent space, allowing training to be performed in O(N
                     2). However, for larger data sets they are required to learn local online models centred around each test point which slows inference considerably. They demonstrate that this method outperforms [10] on the poser data set [1].

Previous models employing mixtures of Gaussian processes [26,19] are not applicable in a discriminative human pose estimation context which consist of large data sets with high-dimensional features. The model proposed by Tresp [26] uses a set of Gaussian processes for performing expert selection which require training on the entire dataset, limiting the model to small data sets. Rasmussen and Ghahramani's model [19] learns a kernel classifier to estimate the probability of a test point to belonging to each expert. This requires estimating a length-scale parameter for each dimension of the input feature. Learning such a model is infeasible for large dimensionality input features due to the large number of Monte-Carlo evaluations required.

In this paper we introduce a novel technique that is similar in spirit to [27] except we approach it from a mixture of expert's perspective, learning a logistic regression model to give a prior over the experts. Our proposed method allows more robustness to noisy inputs by automatically learning the relevant input features for expert selection. We demonstrate a mixture of Gaussian processes model that can be directly applied to human pose estimation problems and overcome the limitations of previous methods [27]. We show that our model is able to give start of the art performance compared to other regression techniques for human pose estimation.

A Gaussian process models the joint distribution of a set of input variables X
                        ={x
                        
                           n
                        }
                           n
                           =1
                        
                           N
                         to provide a functional mapping to an output Y
                        ={y
                        
                           n
                        }
                           n
                           =1
                        
                           N
                         
                        [18].
                           
                              (1)
                              
                                 
                                    
                                       y
                                       n
                                    
                                    =
                                    f
                                    
                                       
                                          x
                                          n
                                       
                                    
                                    +
                                    
                                       ϵ
                                       n
                                    
                                 
                              
                           
                        
                     

In our case, the input x
                        
                           n
                         is an image feature and the output yn
                         is a joint position or angle of an articulated skeleton. A Gaussian process prior is placed on f as a Gaussian distribution with zero mean and covariance given by a kernel function
                           
                              (2)
                              
                                 
                                    p
                                    
                                       
                                          f
                                          
                                             X
                                          
                                       
                                    
                                    =
                                    N
                                    
                                       0
                                       K
                                    
                                    ,
                                 
                              
                           
                        where f
                        =[f
                        1,…,f
                        
                           N
                        ]
                           T
                         is a vector of function output values f
                        
                           n
                        
                        =
                        f(x
                        
                           n
                        ) and K is the covariance matrix given by a kernel function K
                        
                           i,j
                        
                        =
                        k(x
                        
                           i
                        ,x
                        
                           j
                        ). The kernel function represents the pairwise distance between training examples and can take on many forms. In this paper we use kernels based on a squared exponential function
                           
                              (3)
                              
                                 
                                    k
                                    
                                       
                                          x
                                          i
                                       
                                       
                                          x
                                          j
                                       
                                    
                                    =
                                    
                                       σ
                                       signal
                                       2
                                    
                                    
                                    exp
                                    
                                       
                                          −
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            x
                                                            i
                                                         
                                                         −
                                                         
                                                            x
                                                            j
                                                         
                                                      
                                                   
                                                   T
                                                
                                                
                                                   P
                                                   
                                                      −
                                                      1
                                                   
                                                
                                                
                                                   
                                                      
                                                         x
                                                         i
                                                      
                                                      −
                                                      
                                                         x
                                                         j
                                                      
                                                   
                                                
                                             
                                             2
                                          
                                       
                                    
                                    +
                                    b
                                    +
                                    
                                       σ
                                       noise
                                       2
                                    
                                    
                                       δ
                                       ij
                                    
                                    ,
                                 
                              
                           
                        where the model hyper-parameters are θ
                        ={P,σ
                        signal
                        2,b,σ
                        noise
                        2}, σ
                        signal
                        2 is the signal noise, b is a constant bias and σ
                        noise
                        2 is the noise term ϵ in (1). P represents the bandwidth of the kernel, this parameter adjusts how quickly the output variable y varies with respect to the input x. We evaluate two formulations, an isotropic kernel P
                        =
                        I
                        p where p is the bandwidth applied to all input dimensions, and a kernel with automatic relevance detection P
                        =diag([p
                        1,…,p
                        
                           D
                        ]) where a bandwidth is learnt for each input dimension.

A Gaussian process gives a Gaussian predictive distribution over the pose given by a mean and covariance function as functions of the input
                           
                              (4)
                              
                                 
                                    p
                                    
                                       
                                          y
                                          
                                             x
                                             X
                                             Y
                                             θ
                                          
                                       
                                    
                                    =
                                    N
                                    
                                       
                                          y
                                          
                                             
                                                μ
                                                
                                                   x
                                                
                                                ,
                                                σ
                                                
                                                   x
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where
                           
                              (5)
                              
                                 
                                    μ
                                    
                                       
                                          x
                                          ∗
                                       
                                    
                                    =
                                    k
                                    
                                       
                                          x
                                          ∗
                                       
                                       X
                                    
                                    
                                       K
                                       
                                          −
                                          1
                                       
                                    
                                    Y
                                    ,
                                 
                              
                           
                        
                        
                           
                              (6)
                              
                                 
                                    σ
                                    
                                       
                                          x
                                          ∗
                                       
                                    
                                    =
                                    k
                                    
                                       
                                          x
                                          ∗
                                       
                                       
                                          x
                                          ∗
                                       
                                    
                                    −
                                    k
                                    
                                       
                                          
                                             x
                                             ∗
                                          
                                          X
                                       
                                       T
                                    
                                    
                                       K
                                       
                                          −
                                          1
                                       
                                    
                                    k
                                    
                                       
                                          x
                                          ∗
                                       
                                       X
                                    
                                    .
                                 
                              
                           
                        
                     

The predictive mean can be thought of as a weighted average of the training outputs where the weights are given by the kernel distance between the training data and the test point.

The predictive variance can be interpreted such that the first term k(x
                        ∗,x
                        ∗) captures the variance inherent in the data (σ
                        noise
                        2, (3)) and the second term incorporates the information from the neighbouring training data. When there is closely neighbouring training data, the second term is larger resulting in a more certain prediction. If there is no neighbouring training data, this term will fall to 0 and the predictive variance is given by k(x
                        ∗,x
                        ∗). This offers a major advantage compared to a kernel model such as an RVM which has a variance formulation such that as the test input x
                        * is sufficiently far from the training data, the predictive variance collapses to 0, giving confident but inaccurate predictions.

Gaussian process learning consists of optimising the hyper-parameters θ with respect to the training data y,
                        X. This is performed by maximising the log marginal likelihood
                           
                              (7)
                              
                                 
                                    log
                                    
                                    p
                                    
                                       
                                          Y
                                          
                                             X
                                             θ
                                          
                                       
                                    
                                    =
                                    −
                                    
                                       1
                                       2
                                    
                                    
                                       Y
                                       T
                                    
                                    
                                       K
                                       
                                          −
                                          1
                                       
                                    
                                    Y
                                    −
                                    
                                       1
                                       2
                                    
                                    log
                                    
                                       K
                                    
                                    −
                                    
                                       n
                                       2
                                    
                                    log
                                    2
                                    π
                                    .
                                 
                              
                           
                        
                     

This computation is dominated by computing the inverse of the kernel matrix, K
                        −1, which is O(N
                        3). Partial derivatives 
                           
                              
                                 
                                    δ
                                    
                                       δ
                                       
                                          θ
                                          j
                                       
                                    
                                 
                              
                              p
                              
                                 
                                    y
                                    
                                       X
                                       θ
                                    
                                 
                              
                           
                         can be computed allowing each of the kernel parameters to be optimised using gradient descent. This allows kernel parameters to be accurately learnt from the entire training set, without having to rely on cross validation as with other kernel regression models.

To model the multi-modal mappings, we introduce multiple Gaussian processes in a mixture of expert style model. We do this by training each expert on a subset of the training data that is local in the pose space. Thus, each expert represents one mode of the mapping, allowing the gating network to give a probability over each mode at test time. The predictive distribution is given by
                           
                              (8)
                              
                                 
                                    p
                                    
                                       
                                          
                                             y
                                             ∗
                                          
                                          
                                             
                                                x
                                                ∗
                                             
                                             X
                                             Y
                                             Θ
                                          
                                       
                                    
                                    =
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          K
                                       
                                       
                                          
                                             π
                                             i
                                          
                                          p
                                          
                                             
                                                
                                                   y
                                                   ∗
                                                
                                                
                                                   
                                                      x
                                                      ∗
                                                   
                                                   
                                                      X
                                                      
                                                         ϑ
                                                         i
                                                      
                                                   
                                                   
                                                      Y
                                                      
                                                         ϑ
                                                         i
                                                      
                                                   
                                                   
                                                      θ
                                                      i
                                                   
                                                
                                             
                                          
                                       
                                    
                                    =
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          K
                                       
                                       
                                          
                                             π
                                             i
                                          
                                          N
                                          
                                             
                                                
                                                   μ
                                                   i
                                                
                                                
                                                   
                                                      x
                                                      ∗
                                                   
                                                
                                                ,
                                                
                                                   σ
                                                   i
                                                
                                                
                                                   
                                                      x
                                                      ∗
                                                   
                                                
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        where Θ
                        ={θ
                        
                           i
                        }
                           i
                           =1
                        
                           K
                        , ϑi
                         is a set of indices representing the training points used for each GP and θi
                         are its learnt model parameters. Each expert contribution is weighted by the output of a logistical regression model given by πi
                        . To obtain the training indices ϑi
                         we follow [27] and use an approach based on selecting nearest neighbours in the pose space. First we cluster the pose space using the 
                           k
                        -means algorithm to obtain a set of K expert centres. For each centre, we take the nearest S training points in the pose space to form the indices ϑi
                         for expert i.

To obtain a weight for each expert for a test point x
                        * we use a logistic regression model
                           
                              (9)
                              
                                 
                                    p
                                    
                                       
                                          z
                                          
                                             
                                                x
                                                ∗
                                             
                                          
                                       
                                    
                                    =
                                    
                                       
                                          e
                                          
                                             
                                                w
                                                i
                                                T
                                             
                                             
                                                x
                                                ∗
                                             
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             K
                                          
                                          
                                             
                                                e
                                                
                                                   
                                                      w
                                                      j
                                                      T
                                                   
                                                   
                                                      x
                                                      ∗
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        which gives the probability of the test input feature x
                        * belonging to expert i. The weights W
                        ={w
                        
                           i
                        }
                           i
                           =1
                        
                           K
                         are learnt by maximising the penalised log likelihood
                           
                              (10)
                              
                                 
                                    
                                       W
                                       ^
                                    
                                    =
                                    
                                       argmax
                                       W
                                    
                                    
                                       
                                          log
                                          
                                          p
                                          
                                             
                                                C
                                                
                                                   W
                                                   X
                                                
                                             
                                          
                                          +
                                          log
                                          
                                          p
                                          
                                             W
                                          
                                       
                                    
                                 
                              
                           
                        where C is an N
                        ×
                        K matrix representing which expert each training point is assigned to. The first term p(C|W,X) measures how well the predicted probabilities given in (9) match the correct expert assignments C. The second term p(W) is a regularisation prior to avoid over-fitting. We set p(W)=exp(λ|W|1) where |W|1
                        =∑
                        
                           i
                           =1
                        
                           K
                        
                        ∑
                        
                           j
                           =1
                        
                           D
                        |w
                        
                           i,j
                        | is the l
                        1 norm. This prior has the effect of pushing irrelevant weights to zero allowing the model to select the relevant features for each class. The influence of the prior is controlled using the parameter λ which is set using cross validation.

Typically, the matrix C indicating the target classes is encoded as a 1-of-K binary matrix, where each training point is assigned to exactly one expert. However, in the context of this model, a single training point can potentially belong to zero or more experts, as such we evaluate a number of methods for assigning the expert assignments C in Section 4.3.

Previous work by Urtasun and Darrell [27] inferred test poses by constructing local Gaussian processes centred around the nearest neighbours of each test point. Each component of the predictive distribution corresponded to a gp centred around a different neighbouring test point, and the priors were set using the inverse predictive variance of each gp. Setting the priors in this manner causes the model to bias output modes that have a lower signal noise. Further, the online construction of each GP is computationally expensive, significantly slowing test inference. In Section 4 we compare our method to Urtasun and Darrell's method and show a consistent performance improvement.

Another benefit is the interpretability of the predictive distribution. Fig. 1
                         demonstrates the predictive distribution on a synthetic data set. In our model, each component of the predictive distribution (8) corresponds to a separate mode of the output. The priors have the effect of dividing the input space to activate each expert when the test input lies close to its training data.

However the Urtasun and Darrell model constructs online experts centred around each training point resulting in many experts modelling a single output mode. The priors are set using the inverse predictive variance of each expert. This predictive distribution can have unpredictable results depending on the experts constructed. In this example it misses the multi-modal region where x
                        ∈[45,55] because all the experts are centred on the top mode, and gives a overly high predictive variance for x
                        ∈[60,75].

An individual Gaussian process has cubic training complexity O(N
                        3) due to the inversion of the covariance matrix required during the likelihood computation as shown in (7). In our model, we train K experts of a fixed size, S, resulting in a computational complexity for training the experts of O(KS
                        3). As the number of training examples increases, the expert size remains fixed and the number of experts grows. As such, the computational complexity of our model grows linearly as you increase the number of experts to accommodate more training examples. The likelihood function for the logistic regression model given in (10) also scales linearly with respect to the number of training samples. For a model with N training samples and K experts the complexity for evaluating the likelihood is O(NK).

Making predictions of test data with an individual Gaussian process is O(N
                        2). In a similar fashion to the training process, our model reduces this to O(KS
                        2) by making predictions from multiple small Gaussian processes.

@&#EVALUATION@&#

We evaluate our model on a ballet dataset [12], a sign language dataset [8] and HumanEva [22]. We evaluate the model using two types of image features, bag-of-words 
                     [1,17,16,6] and HMAX [14,27,21]. The bag-of-words features are constructed following [16] using a codebook of 300 cluster centres. For ballet and HumanEva data sets, silhouettes are extracted and we use shape context descriptors [2] extracted from the contour of the silhouette. Standard background subtraction techniques can't be used to obtain silhouettes on the sign language data set so we extract SIFT descriptors instead [15].

The ballet data set consists of a complex ballet choreography with 3D joint position annotations. The choreography is performed 5 times and we use 4 of the sequences for training and the final sequence for testing — resulting in 1601 training samples and 253 test samples.

The sign language data set is extracted from BBC television and consists of a continuous 6000 frame sequence. This is a very difficult sequence due to the moving background and image blur caused by fast movements. We break the sequences into 400 frame chunks and then randomly select chunks for the training and test sets to give 4400 training samples and 1200 test samples. We use 5 different random partitions of training and test data to evaluate the models.

The HumanEva data set consists of 3 subjects recorded from multiple cameras performing a range of actions. We evaluate our model by training a combined model for subjects S1, S2 and S3 using the images from a single camera (C3) for the Walking and Jog sequences.

We obtain a predicted pose from a test image by computing the expected value of the predictive distribution
                        
                           (11)
                           
                              
                                 y
                                 =
                                 E
                                 
                                    
                                       p
                                       
                                          
                                             y
                                             
                                                x
                                             
                                          
                                       
                                    
                                 
                                 =
                                 
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       K
                                    
                                    
                                       
                                          π
                                          i
                                       
                                       E
                                       
                                          
                                             N
                                             
                                                
                                                   
                                                      μ
                                                      i
                                                   
                                                   
                                                      x
                                                   
                                                   ,
                                                   
                                                      σ
                                                      i
                                                   
                                                   
                                                      x
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     which gives the weighted mean of the individual expert predictions. We report errors as the mean absolute error (MAE) between the predicted pose and the ground truth. For a test prediction y and ground truth annotation ŷ we compute the error as
                        
                           (12)
                           
                              
                                 E
                                 
                                    y
                                    
                                       y
                                       ^
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             D
                                          
                                          
                                             
                                                
                                                   
                                                      y
                                                      i
                                                   
                                                   −
                                                   
                                                      
                                                         
                                                            y
                                                            ^
                                                         
                                                      
                                                      i
                                                   
                                                
                                             
                                          
                                       
                                    
                                    D
                                 
                                 ,
                              
                           
                        
                     where D is the dimensionality of the pose vector. These errors are then averaged over all frames in a sequence to obtain the final results. For the ballet and HumanEva data sets these errors are given in millimetres for the sign language data set the errors are given in pixels.

The main parameters of this algorithm are the number of experts K and the size of each expert S. A practical guideline for configuring the model is to choose a value for the expert size, typically S
                        =100, and then set the number of experts as K
                        =
                        N
                        /
                        S where N is the number of training points. This way, each training point will belong to one expert on average.


                        Fig. 3
                        
                         demonstrates the effect of changing the number of experts K with a fixed expert size of 100. We set the number of experts using a multiplier α such that the number of experts is given by K
                        =
                        αN
                        /
                        S. Thus setting α to 1 results in each training point belonging to one expert on average. Increasing α to higher values persuades a greater overlap between experts — giving greater certainty at boundary regions but increasing training time. In general, the performance tends to increase for more experts however saturates when α is set to 2.5. The performance starts to decrease slightly as lots of experts are added, this may because it makes learning the gating network more challenging.


                        Fig. 2 demonstrates the effect of varying the expert size while holding the number of experts fixed. We train two models for each expert size, using a different number of experts for each. The number of experts is calculated by setting α to 1 for expert sizes 50 and 100.

The general pattern is that performance tends to increase with expert size up to a saturation point. While this varies with each sequence, 100–200 tends to be a suitable expert size. The ballet data set shows high sensitivity to the expert size, favouring each expert to have 100 points. Larger expert sizes cause a significant drop in performance, this may be because the ballet sequence is relatively short, approximately 300 frames, thus large expert sizes cause each expert to model a very wide range of poses in this short but complex sequence. The poor performance of models with 16 experts of size less than 75 could be attributed to the sparse coverage of the training data. As such, increasing the number of experts at this size leads to a significant performance increase.

Gaussian processes optimise the kernel hyper parameters in order to gain a good fit of the training data. In this section we compare using an isotropic (ISO) kernel and a kernel with automatic relevance detection (ARD). Isotropic kernels have a single parameter p which is used to set the length-scale of the kernel. Automatic relevance detection kernels learn an individual length-scale for each input dimensionality [18]. This allows a Gaussian process with an ARD kernel to learn which dimensions of the input features are more relevant than the others.


                        Fig. 4
                         shows a comparison between both kernels on each sequence with different features. As can be seen the ISO kernels outperform the ARD kernels in all experiments. This is a surprising result as one would expect that ARD kernels would identify the relevant features and give better test generalisation. We suspect that isotropic kernels perform better due to the fewer number of parameters that must be fitted. With an isotropic kernel there are 4kernel parameters, as opposed to 304 parameters with an ARD kernel (see Section 1). As each expert is learnt using a relatively small number of training examples, it may be the case that there are not enough examples for the optimisation algorithm to accurately identify the features that correspond to noise. In this case the simpler optimisation with an isotropic kernel with only a single length-scale parameter may be more effective.

In this section we evaluate the most effective way of training the logistic regression model. This provides a weight over each expert's prediction and is important for ensuring that the model accurately represents the predictive distribution of the pose space.

We experiment with a number of ways for setting the target N
                        ×
                        K probability matrix C from (10)
                        
                           
                              
                                 
                                    
                                       
                                          Expert
                                          
                                          Assignment
                                       
                                       
                                          
                                             C
                                             
                                                n
                                                ,
                                                i
                                             
                                          
                                          =
                                          
                                             
                                                
                                                   
                                                      1
                                                   
                                                   
                                                      if
                                                      
                                                      n
                                                      ∈
                                                      
                                                         ϑ
                                                         i
                                                      
                                                      ,
                                                   
                                                
                                                
                                                   
                                                      0
                                                   
                                                   
                                                      otherwise
                                                      .
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    
                                       
                                          
                                             Nearest
                                             
                                             Expert
                                             
                                             Centre
                                          
                                          
                                             
                                                C
                                                n
                                             
                                             =
                                             
                                                argmin
                                                i
                                             
                                             
                                                
                                                   
                                                      y
                                                      n
                                                   
                                                   −
                                                   
                                                      ϕ
                                                      i
                                                   
                                                
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    
                                       
                                          
                                             Expert
                                             
                                             Density
                                          
                                          
                                             
                                                C
                                                
                                                   n
                                                   ,
                                                   i
                                                
                                             
                                             =
                                             p
                                             
                                                
                                                   
                                                      y
                                                      n
                                                   
                                                   
                                                      
                                                         Y
                                                         
                                                            ϑ
                                                            i
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

Assignments of the form C
                        
                           n
                        
                        =
                        i, where i is the expert index, are converted to a 1-of-K encoding. We also normalise the columns of C such that they sum to one and represent probabilities. The first approach directly uses the expert assignments to set C. If a single training point has been assigned to two experts, then the normalisation step ensures that the target probabilities will assign it equal likelihood for each expert.

Setting C to the nearest expert centre assigns each training point to exactly one expert. We compute the distance of each training point to all the expert centres ϕ
                        ={ϕ
                        
                           i
                        }, and assign each point to its nearest centre. Finally, we fit a density model to the training poses to give a probability of each point belonging to each expert. The density models we experiment are a single Gaussian distribution and a Gaussian mixture distribution learnt using variational Bayesian inference [4].


                        Tables 1 and 2
                        
                         compare the above methods on each data set. We train a fixed set of experts and compare how the different methods of setting the target probability matrix C affect the tracking accuracy. The expert assignment technique gives good performance all-round, giving a significant improvement on the ballet data set with HMAX features. On the sign language data set all methods give similar performance, with the nearest expert centre technique taking a small lead. The nearest expert centre technique performs poorly on the ballet data set without background subtraction. This could be because the background noise causes ambiguities that need to be reflected in the priors. By using the nearest expert centre technique each training point in C is assigned to exactly one expert, thus C does not represent the ambiguity in the data.

In the HumanEva data set, the expert assignment technique often falls behind the others and nearest centre or the expert density techniques make better choices. The differences between these techniques are mostly minor, but the optimal technique should be chosen for each data set.

We evaluate our mixture of Gaussian processes model (MGPR) against a selection of state of the art techniques. We compare to Bayesian mixture of experts BME [6], local shared kernel information embedding (SKIE) [16], Urtasun and Darrell's [27] local online Gaussian processes, random forests 
                        [7,20] and kernel regression [16]. Quantitative results are shown in Tables 3 and 4
                        
                         and visual results are shown in Fig. 5
                        .

Our model offers a significant improvement of previous models based on using multiple Gaussian processes for regression [27]. We outperform this technique on all experiments, offering a particularly large improvement on those experiments where there is no background segmentation — leading to noisier image features. In such cases, the logistic regression model used for expert selection allows greater invariance to noisy features leading to a more robust model. This effect can also be seen with SKIE [16], where its isotropic input kernel directly encodes noise into the regression inference. Their model is able to give strong performance on the experiments with features based on silhouettes, but is unable to obtain accurate results on the sign language data set with its complex moving background.

It should be noted that our technique is also able to perform fast prediction due to its offline learnt models. Techniques such as SKIE [16] and Urtasun and Darrell's local Gaussian processes [27] build models online for each test point resulting in slow test inference. For the 1600 frames of the sign language data set, our model took 5min to make all test predictions, while [27] took 38min.

Our model is often marginally outperformed by random forest regression. Both techniques give similar performance and both support fast learning and prediction. The advantage of using our model is the compact mixture of Gaussian predictive distribution. This allows it to be directly incorporated into a principled dynamics framework such as [23]. Such a framework can then be used to resolve the multi-modalities modelled by the predictive distribution and provide a smooth tracking signal.

@&#CONCLUSION@&#

In this paper we have evaluated a mixture of Gaussian process model for discriminative pose estimation showing that it is a flexible regression technique that gives state of the art performance. It is relatively insensitive to parameter choices allowing it to be used confidently on a variety of data sets. Its Gaussian mixture predictive distribution allows for predictions to be easily interpreted as a set of weighted predictions over different output modes. This makes it ideal for incorporating into larger models such as existing dynamics frameworks [23].

We have shown that it is able to overcome issues with previous work which uses local Gaussian processes for human pose estimation [27]. Our formulation gives increased robustness to noisy image features, faster test prediction and easily interpretable predictive distribution, where each expert represents a distinct mode.

@&#REFERENCES@&#

