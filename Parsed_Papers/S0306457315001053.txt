@&#MAIN-TITLE@&#DeASCIIfication approach to handle diacritics in Turkish information retrieval

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Risk-sensitive evaluation of approaches for handling diacritics in Turkish information retrieval.


                        
                        
                           
                           Application of diacritics restoration to Turkish information retrieval.


                        
                        
                           
                           Investigation of the diacritics sensitivity of stemming algorithms.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Accents

DeASCIIfier

Diacritics restoration

Risk-sensitive evaluation

Stemming

Turkish information retrieval

@&#ABSTRACT@&#


               
               
                  The absence of diacritics in text documents or search queries is a serious problem for Turkish information retrieval because it creates homographic ambiguity. Thus, the inappropriate handling of diacritics reduces the retrieval performance in search engines. A straightforward solution to this problem is to normalize tokens by replacing diacritic characters with their American Standard Code for Information Interchange (ASCII) counterparts. However, this so-called ASCIIfication produces either synthetic words that are not legitimate Turkish words or legitimate words with meanings that are completely different from those of the original words. These non-valid synthetic words cannot be processed by morphological analysis components (such as stemmers or lemmatizers), which expect the input to be valid Turkish words. By contrast, synthetic words are not a problem when no stemmer or a simple first-n-characters-stemmer is used in the text analysis pipeline. This difference emphasizes the notion of the diacritic sensitivity of stemmers. In this study, we propose and evaluate an alternative solution based on the application of deASCIIfication, which restores accented letters in query terms or text documents. Our risk-sensitive evaluation results showed that the diacritics restoration approach yielded more effective and robust results compared with normalizing tokens to remove diacritics.
               
            

@&#INTRODUCTION@&#

American Standard Code for Information Interchange (ASCII)
                        1
                     
                     
                        1
                        
                           http://tools.ietf.org/search/rfc4949
                        
                      is a scheme that encodes 128 specified characters into 7-bit binary integers. ASCII contains English alphabet letters (a–z and A–Z), numbers from 0 to 9, and some other special characters. However, a number of languages use characters outside the ASCII range and they have letters with diacritics in their alphabet, such as Czech, Danish, Finnish, French, Greek, Hungarian, Icelandic, Latvian, Lithuanian, Norwegian, Polish, Romanian, Swedish, Spanish, and Turkish. The absence of diacritics in digitally stored text is a severe obstacle to natural language processing (NLP) and information retrieval (IR) for languages with alphabets not covered by the standard ASCII range.

Turkish is an agglutinative and morphologically complex language, where a relatively small set of distinct stems is expanded by rich combinations of derivational and inflectional suffixes to create new meanings.

The Turkish alphabet is a Latin alphabet that comprises 29 letters. It has all the letters of the English alphabet, except “q,” “w,” and “x.” In addition, it has the local characters “ç,” “ğ,” “ı,” “ö,” “ş,” and “ü” with diacritic symbols, which have been modified from their Latin originals to meet the phonetic requirements (to distinguish different sounds) of the language.

Diacritics are also referred to as accent marks, which are defined as: “A mark placed above, below, or to the side of a character to alter its phonetic value.”
                        2
                     
                     
                        2
                        
                           http://www.unicode.org/glossary/#accent_mark
                        
                     
                     Table 1
                      shows Turkish accented letters and their ASCII equivalents. Due to these non-ASCII letters, Turkish users experience many IR problems on the Internet (Aytaç, 2005).

With regard to accents and diacritics, Manning et al. stated that: “Nevertheless, the important question is usually not prescriptive or linguistic but is a question of how users are likely to write queries for these words. In many cases, users will enter queries for words without diacritics, whether for reasons of speed, laziness, limited software, or habits born of the days when it was hard to use non-ASCII text on many computer systems” (Manning et al., 2008).

However, Turkish texts written in the English alphabet may have different meanings that cannot be distinguished even by a human. For example, an interesting news story by Diaz (2008) described how a Turkish SMS written in English letters resulted in a completely twisted meaning that resulted in homicides.

Turkish users have a tendency to write Turkish search queries without using accented Turkish letters due to the reasons explained above. Therefore, there is a need for an ability to search with and without accents in Turkish IR. For instance, in diacritic insensitive IR, the words resume and résumé should be treated as if they are the same word.

ASCIIfication, also referred to as latinization, is the normalization of tokens to reduce all accented letters to their base character. ASCIIfication is a common practice for achieving accent-insensitive IR; however, this may result in a change in meaning because certain words are distinguished only by their accents. Table 2
                      shows a list of examples that would retrieve false matches.

Furthermore, the ASCIIfication process yields synthetic words, which can negatively affect downstream morphological analysis components (such as stemmers or lemmatizers) in the processing pipeline. The transformation of the word hastalığının into hastaliginin is an example. In this case, hastaliginin is not a legitimate Turkish word, so it is not recognized by morphological analyzers. In this study, we propose and evaluate an alternative solution based on the application of deASCIIfication for restoring diacritics in query terms or text documents.

The remainder of this paper is organized as follows. Section 2 describes related research. In Section 3, we explain the two deASCIIfication algorithms used in this study. In Section 4, we provide intrinsic evaluation results obtained using the two different deASCIIfication systems. Section 5 describes the experimental setup. In Section 6, we present the experimental results and a discussion. In Section 7, we give robustness results obtained from a comparative risk-sensitive evaluation of different approaches for handling diacritics in Turkish IR. In Section 8, we give our conclusions and suggestions for future research.

@&#RELATED WORK@&#

Diacritics restoration (DR) can be defined as the automatic insertion of diacritics into text when they are absent. Due to the continuously increasing volume of user-generated textual content (blogs, forums, wikis, etc.) on the Web, DR tools have become essential components in many important applications, such as IR, machine translation, named entity recognition, corpora acquisition, and the construction of machine-readable dictionaries (Mihalcea, 2002).

DR studies have been performed for many languages, including Arabic (Azmi & Almajed, 2015), Croatian (Šantić et al., 2009), Vietnamese (Do et al., 2013), Romanian (Grozea, 2012), and Turkish; however, the studies in this category are not related to IR.

The first published study of Turkish DR was carried out by Tür (2000). He constructed a character-based language model using an 18 million-word corpus of Turkish and built a hidden Markov model (HMM) whose states denoted Turkish characters-grams and whose state transition probabilities were obtained from the language model. The performance of the deASCIIfier was evaluated using a test data of 8511 words, among which 5864 words needed to be corrected. Tür tested the system using different character context sizes and found that a 4-gram language model yielded a significant error rate reduction compared with a 3-gram model.


                        Okur et al. (2013) defined the task of DR as: “Rewriting texts in English characters using Turkish characters,” from an NLP perspective. In their study, word sense disambiguation techniques were used to choose the right Turkish word among different alternatives.

A recent study by Adalı and Eryiğit (2014) implemented a diacritic restorer for Turkish language, where conditional random fields (CRFs) were used to choose the most probable candidate within the current context. CRFs allow neighboring words to be used as features, so this method is a discriminative equivalent of the character-based HMM approach proposed by Tür (2000).

These two systems (Akın & Akın, 2007; Yüret & de la Maza, 2006) are used in the present study and they are described in detail in Section 3. The term “deASCIIfication” is employed by the Turkish community to describe DR, and thus this term is used in the present study.

A limited number of studies have investigated the effects of diacritics on IR, because IR studies have focused mostly on English. Moreover, English is the only language without diacritics among the European languages that use Latin-based alphabets.

In many previous studies (Asubiaro, 2014; Bar-Ilan & Gutman, 2005; Bitirim et al., 2002; Choroś, 2005), a number of search queries were first submitted to commercial search engines (Bing, Google, Yahoo!, etc,) with or without diacritics. The numbers of results returned were then compared and evaluated. When the phrase query string http://scholar.google.com/scholar?q=%22How+do+search+engines+handle+%2A+queries%22 “How do search engines handle *queries” was submitted to Google Scholar, studies were retrieved in various languages such as Arabic, Chinese, and Greek. The algorithms employed by commercial Web search engines are trade secrets and they change over time; therefore, these studies (tests of the effectiveness of commercial search engines) did not propose methods for handling diacritics.


                        Bar-Ilan and Gutman (2005) explored the capabilities of search engines (AltaVista, FAST, and Google) in non-English languages (Russian, French, Hungarian, and Hebrew). In most cases, their results indicated that general search engines ignore the special characteristics of non-English languages and they sometimes handle diacritics poorly.


                        Choroś (2005) tested the effectiveness of retrieval using Polish queries with diacritics. The Polish language has several local characters with diacritic symbols, such as: ą ć ę ł ń ó ś ź ż. Choroś submitted a number of queries to major and local search engines, and found that the search engines retrieved different results depending on the use or non-use of diacritics. It was reported that users were not consistent in the use of diacritics in their queries or in pages. The question: “Why do Polish words without diacritics also appear on the Polish Internet?” was asked and answered. Free style private information pages, blogs, chats, forums, discussions, and commentaries were given as examples.


                        Bitirim et al. (2002) examined how Turkish search engines respond to search queries that contain Turkish accented characters. Two forms of the following query with different spellings were searched for separately: MP3 songs of “Barış Manço,” and MP3 songs of “Baris Manco.” The authors compared the two lists of result returned by each search engine and reported that “the use of Turkish characters such as ‘ç,’ ‘ı,’ and ‘ş’ in queries still create problems for Turkish search engines as retrieval results differed for such queries” (Bitirim et al., 2002).


                        Asubiaro (2014) analyzed the effects of diacritics in Yorùbá search queries on the performance of major search engines when retrieving Yorùbá documents.


                        Alpkoçak and Ceylan (2012) were the first to investigate the effects of diacritics on Turkish IR using a TREC-style evaluation methodology (referred to as the Cranfield paradigm) based on a test collection, a set of information needs, and a set of relevance judgments. This method differs from previous studies that aimed to determine how commercial search engines handle diacritics. Alpkoçak and Ceylan defined the problem of the performance loss caused by the inappropriate use of accented letters in documents and queries, and then proposed three different approaches to this problem. They investigated the normalization of tokens to remove diacritics (referred to as ASCIIfication or latinization in this study) during the indexing and retrieval process. They also employed an expansion method that simply added the same terms with Latin letters to documents and queries. To measure the performance loss, they indexed the original tokens with Turkish letters. Hence, they created three versions of the documents and queries in the test dataset.

                           
                              1.
                              T: A set where the terms were intact with the original Turkish letters.

L: All of the Turkish letters were replaced with standard Latin letters.

TL: This set included the original form with Turkish letters as well as equivalent forms with standard Latin letters.

Their empirical results showed that applying ASCIIfication: “to obtain synthetic types for both queries and documents gives better results than all other techniques applied in [their] paper, except if the documents are expanded (TL) and queries are typed in Turkish letters (T). This assumption is valid for all stemming approaches and weighting methods applied in [their] study.” They concluded that: “simply using standard Latin letters with both the queries and documents is almost as good as using the original documents and queries” (Alpkoçak & Ceylan, 2012).

However, the stemmers used in their study, i.e., NS and F5, were not negatively affected by the absence of diacritics. We refer to this property as diacritic sensitivity. As shown in the present study, latinization significantly degrades the effectiveness of retrieval when combined with a widely used Turkish stemmer. Furthermore, we propose and evaluate an application of DR to Turkish IR as an alternative solution to the problem.

In the present study, we assume that we do not have the original queries and documents with Turkish letters. Therefore, according to the naming convention employed by Alpkoçak and Ceylan (2012), Queries (T), Queries (TL), Documents (T), and Documents (TL) are not defined in our study. We used the same dataset, with the same two stemmers (NS for no stemming and F5 for truncating the terms after the first five characters), same evaluation metric (bpref) (Buckley & Voorhees, 2004), and TF × IDF term-weighting model. We replicated certain runs from the previous study (Alpkoçak & Ceylan, 2012) for comparison purposes.

To the best of our knowledge, this is the first study to integrate DR into Turkish IR and to compare DR with latinization in a risk-sensitive IR evaluation setting. Therefore, our study complements and extends the research reported by Alpkoçak and Ceylan (2012).

Our proposed approach restores diacritics rather than removing them. Thus, deASCIIfication is the reverse process of removing diacritics. In this process, a Turkish deASCIIfier outputs the correct Turkish text with specific Turkish letters.

We hypothesize that if we can incorporate a deASCIIfier that works at a reasonable level of accuracy, this will eliminate the retrieval loss caused by diacritic-less queries. We used the following two publicly available open source deASCIIfier implementations for Turkish language in our experiments.

We translated Deniz Yüret’s Emacs Turkish Mode
                           3
                        
                        
                           3
                           
                              http://www.denizyuret.com/2006/11/emacs-turkish-mode.html
                           
                        , which was inspired by Tür (2000), from Lisp into Java. The original program utilizes decision lists, which were created based on a one million-word corpus of Turkish news articles using the Greedy Prepend Algorithm (Yüret & de la Maza, 2006).

We implemented a Lucene/Solr plugin named TurkishDeASCIIfyFilter (TDF) that is optimized for Lucene/Solr. TDF produces a single output for a given word. We added a Boolean option to preserve the original token and to emit it in the same position as the deASCIIfied token only if the actual token changes.

Zemberek
                           4
                        
                        
                           4
                           
                              http://code.google.com/p/zemberek
                           
                         (version 2.x) is an open source, platform-independent, general purpose NLP library and toolset designed for Turkic languages, especially Turkish. Zemberek provides fundamental NLP operations including spell checking, morphological analysis, stemming, DR (deASCIIfier), and extracting syllables (Akın & Akın, 2007). Zemberek is fast becoming the most popular tool used in Turkish text processing-related studies, such as text retrieval (Haddad & Bechikh Ali, 2014; Öztürkmenoğlu & Alpkoçak, 2012) and clustering (Tunalı & Bilgin, 2012).

It has been announced that Zemberek 2 development will not be continued. Future releases of the project (a.k.a. Zemberek 3) will be hosted by the zemberek-nlp
                           5
                        
                        
                           5
                           
                              http://github.com/ahmetaa/zemberek-nlp
                           
                         project in GitHub. In this study, Zemberek 2’s deASCIIfier and stemmer pieces are used because Zemberek 3 does not have a deASCIIfier implementation. For each piece, we implemented two Lucene/Solr plugins called Zemberek2DeASCIIfyFilter (ZDF) and Zemberek2StemFilter (ZSF), respectively.

ZSF is based on Zemberek 2’s morphological analysis, which does not perform any contextual disambiguation during the morphological interpretation of the words. Instead, it processes one word at a time without using any other context information and it produces multiple parses/analyses. ZSF chooses one of the multiple stem forms heuristically by selecting either: (i) the most frequent one, (ii) that with the longest stem, (iii) that with the minimum number of morphemes, or (iv) the first one. These options are configurable in our framework and we selected that with the longest root in this study.

Zemberek 3 has a morphological disambiguation implementation, but the developers of the project reported that it is not usable at present and that it works less accurately than expected. In the context of IR, it is not clear whether morphological disambiguation during stemming would be beneficial or not. Even if it is possible to disambiguate words correctly in documents, short queries may lack sufficient contextual information for this task. Therefore, ambiguity is less of a problem for the stemming algorithms used in IR.

ZDF injects the original text and all of the words generated by Zemberek DeASCIIfier at the same position. In contrast to the Turkish DeASCIIfier, Zemberek DeASCIIfier can output multiple alternatives for a given word. Thus, for a given ASCIIfied word, it is capable of producing all of the possible diacritized versions that are all legitimate Turkish words.

In this section, we consider each deASCIIfier as a stand-alone system and we analyze the accuracy of the results obtained. In later sections, we investigate the impact of the deASCIIfiers within the context of IR. Intrinsic evaluation intends to measure how good each deASCIIfier (Yüret’s or Zemberek) is with respect to the DR task independently of the downstream application.

The evaluations were performed using the freely available dataset called Turkish Wikipedia
                        6
                     
                     
                        6
                        
                           http://dumps.wikimedia.org/trwiki/
                        
                      dump (trwiki-20150121-pages-meta-current) from January 2015. The dataset comprised 454.3MB when compressed in size (bzip2) with 129,288,786 words, among which 53,375,687 contained at least one of the Turkish accented characters, i.e., accented words accounted for about 41% of the collection.

For deASCIIfication tasks, creating a gold standard is a straightforward process because the reverse operation (ASCIIfication) is trivial and it can be performed fully automatically. We removed accents from the accented words and fed the ASCIIfied words into deASCIIfiers. In the evaluation, we determined the error rate, which was calculated over all of the accented words as follows.

                        
                           (1)
                           
                              
                                 E
                                 r
                                 r
                                 o
                                 r
                                 R
                                 a
                                 t
                                 e
                                 =
                                 
                                    
                                       #
                                       
                                       o
                                       f
                                       
                                       w
                                       r
                                       o
                                       n
                                       g
                                       l
                                       y
                                       
                                       d
                                       i
                                       a
                                       c
                                       r
                                       i
                                       t
                                       i
                                       z
                                       e
                                       d
                                       
                                       w
                                       o
                                       r
                                       d
                                       s
                                    
                                    
                                       #
                                       
                                       o
                                       f
                                       
                                       a
                                       c
                                       c
                                       e
                                       n
                                       t
                                       e
                                       d
                                       
                                       w
                                       o
                                       r
                                       d
                                       s
                                    
                                 
                              
                           
                        
                     
                  

It should be noted that the 
                        
                           E
                           r
                           r
                           o
                           r
                           R
                           a
                           t
                           e
                           =
                           
                              
                                 #
                                 
                                 o
                                 f
                                 
                                 w
                                 r
                                 o
                                 n
                                 g
                                 
                                 c
                                 h
                                 a
                                 r
                                 a
                                 c
                                 t
                                 e
                                 r
                                 s
                              
                              
                                 #
                                 
                                 o
                                 f
                                 
                                 a
                                 m
                                 b
                                 i
                                 g
                                 u
                                 o
                                 u
                                 s
                                 
                                 c
                                 h
                                 a
                                 r
                                 a
                                 c
                                 t
                                 e
                                 r
                                 s
                              
                           
                        
                      metric used by Tür (2000) only considered the ambiguous characters, which made it slightly more lenient than the word-level error rate defined by Eq. (1). Thus, if the input word “ışık” were corrected as “işık,” then the error rate would be 
                        
                           
                              1
                              3
                           
                           =
                           33.33
                           %
                        
                     . Therefore, the results reported in this section are not directly comparable with the error rates reported by Tür (2000).

In Table 3
                     , we present the intrinsic evaluation results obtained for the two different deASCIIfication systems used in this study. Zemberek DeASCIIfier’s first output was used in the evaluation.

During the evaluation, we inserted the accented words into a map as values and we used ASCIIfied versions as keys. This allowed us to group words together that had the same normalized version when the diacritics were removed. After the content of the map was examined, we observed that the dataset contained many inconsistent diactrization errors, although their frequencies were quite low in the collection.

For example, the word 
                        ilkö
                        ğretim (primary education) contains four accented letters (underlined), and thus it has a total of 
                        
                           
                              4
                              2
                           
                           =
                           16
                        
                      possible diactritized combinations. Intuitively, we would expect to only see two forms (fully ASCIIfied and properly deASCIIfied), but 13 variations out of 16 possible combinations were found in the dataset. These 13 words and their collection frequencies are as follows:

[ilköğretim × 31892, ilkögretim × 81, ılkögretim × 10, ılköğretim × 8, ilkoğretim × 7, ılkogretim × 7, ilkogretım × 3, ilkögretım × 2, ılkogretım, ilköğretım, ilkoğretım, ılköğretım, ılkoğretım].

Note that only the first word is a legitimate Turkish word. This interesting example is given to demonstrate that partially diacritized versions of words appear quite often in electronic documents.

In all of the experiments, we used Apache Solr
                           7
                        
                        
                           7
                           
                              http://lucene.apache.org/solr/
                           
                         (version 4.8.0) as the retrieval system. Apache Solr is an open source enterprise search platform, which is written in Java and it runs as a standalone full-text search server. Apache Solr uses Apache Lucene (Białecki et al., 2012) under the hood.

In the Lucene/Solr ecosystem, text analysis comprises a process where plain text is converted into a stream of tokens by tokenization, lowercasing, ASCII-folding, stemming, synonym expansion, stopword removal, etc. Solr’s field type definition, which encapsulates text analysis, comprise three parts: (i) zero or more char filters, (ii) a single tokenizer, and (iii) zero or more token filters. For each part, Lucene/Solr provides a selection with several built-in implementations. For instance, ASCIIFoldingFilter (ASCIIFF) and ICUFoldingFilter can be used to reduce accented letters to their base character.

The text analysis components can be chained together to create complex analysis pipes, which apply a series of transformations to each token. However, the order of the components is of crucial importance. For example, all of the terms must already be in lowercase for the SnowballFilter (which is responsible for stemming) to work correctly. In other words, the input must be lowercased by an upstream component such as LowerCaseFilter or LowerCaseTokenizer.

Solr provides a simple form (called the analysis page), which is used to test the text analysis configuration on a sample text where every analysis step is visualized. Figs. 1
                         and 2
                         show two example screenshots of the analysis page to illustrate the working of ASCIIFF and ZDF, respectively.


                        Fig. 1 shows the steps required to analyze the title parts of two sample topics (QueryID: 235 and 241) selected from a query set for indexing using the ascii_ns field type. It should be noted that the sample text is first parsed with the StandardTokenizer (ST) and then each token passes through ApostropheFilter (AF), TurkishLowerCaseFilter (TLCF), and finally ASCIIFF, which removes diacritical marks at the end.


                        Fig. 2 shows how ZDF restores the ASCIIfied text from Fig. 1 according to the steps required to analyze the sample text for querying using the zemberek2_deascii_ns field type. Note that the word “kus” is converted into multiple tokens: “kuş” and “küs,” so including the original token itself, three tokens (at the same position) are indexed.

In addition to NS and F5, two other stemming options (Zemberek and Snowball) are used. Solr comes with the Snowball stemmer implementation for the Turkish language. Snowball is a rule-based stemmer that does not use a dictionary. For F5 stemming, Lucene/Solr use the TruncateTokenFilter to truncate tokens at a configurable maximum length. In our experiments, we used Solr’s default weighting scheme (TF × IDF).

In our experiments, we created two different indices (called cores in the Solr terminology) for categories A and B. A maximum of 1000 documents per query (sorted by relevancy scores) are fetched from each index, in a similar manner to standard TREC-type ad hoc runs. We found that certain short queries returned different documents with identical score values in their results list. The documents in the results list were sorted by relevancy scores, so documents with equal scores were tied. In Lucene/Solr, when the default sort criterion (which sorts in descending order by relevancy scores) or explicitly supplied sort criteria yield a tie, the internal Lucene document identifier is used to break it as a last resort. However, internal Lucene document identifiers can change non-deterministically when the index changes. Thus, for different indices, the same document could be assigned to a different Lucene document identifier. Short queries attained slightly different effectiveness measures from different indices because of the sorting behavior, which breaks ties in a non-deterministic manner. To ensure the stability of runs across indices, we supplied the DOCNO field (which is unique across the dataset) as a secondary sort parameter to serve as a tiebreaker. Thus, documents with the same score were sorted in ascending order by DOCNO within themselves.

The experiments were conducted using the Bilkent Milliyet Collection (Can et al., 2008), which is the only large-scale IR test collection in the Turkish language. This collection comprises 408305 documents (news stories), 72 information needs (referred to as topics in the TREC terminology), and binary (either relevant or non-relevant) relevance judgments (right answers), where the collection is roughly 800MB in size.

Each document in the Milliyet Collection comprises eight fields: {author, date, DOCNO, headline, source, text, time, URL}. Among these eight fields, only the headline and text fields contain searchable textual information. Therefore, we combined the text and headline fields in a single field and performed searches on this field.

In the Milliyet Collection, information needs have three parts (title, description, and narrative), which are similar to a typical TREC query. In the aforementioned study (Alpkoçak & Ceylan, 2012), title-only queries are named as short queries (Qs) and description-only queries are named as middle-length queries (Qm). Thus, for comparative reasons, we used the same notation.

Some salient statistics related to the queries are listed in Table 4
                        . Qs queries have three terms on average, where 52% of the query terms contain accented Turkish letters. These terms are subject to change by the (de)ASCIIfication process. Among 72 topics, nine (QueryID: 243, 270, 298, 300, 306, 311, 426, 433, and 474) have no accented letters in their titles. Thus, these nine topics were not suitable for comparisons of title-only queries in this study due to a lack of discriminative power. Qm queries have 13 terms on average, where 47% of the query terms contain Turkish accented letters. All 72 topics contain at least one accented word in their descriptions.

Queries are lowercased. Apostrophes and following suffixes were stripped from query terms. This reflects real-world settings because users do not employ capitalization when writing their queries. We stripped question marks (?) from query terms because in the default Lucene/Solr query parser syntax, ? is a wildcard search operator.

We investigate the problem caused by the absence of diacritics in documents and queries for Turkish IR in two different settings that reflected real-world scenarios. Queries were always expected to be entered without diacritics.


                        Category A. In this setting, it was assumed that documents were written well in terms of spelling rules. This is usually the case when documents are generated by trusted sources such as newspapers, government, authors, and academics. News articles, legal documents, books, and publications would have few or no diacritic mistakes.


                        Category B. The present study focused mainly on search queries. However, as noted by Choroś (2005), documents are generated by users in some search systems, e.g., online auction websites, forums, and micro-blogs. Diacritics are also often omitted from comments and reviews. This category reflected a setting where documents are user-generated, and thus diacritics are often omitted.

In order to evaluate how well the proposed deASCIIfier approach works, description-only (Qm) and title-only (Qs) queries were executed over two Solr cores (catA & catB) with 16 different fields in our experiments. Each Solr field was named with a notation, which was a combination of the two parameters: the strategy and stemming method.


                        Table 5
                         shows all of the possible parameters, their abbreviations, and descriptions. For example, the field name ascii_f5 shows that the ASCIIfication approach was used and that the F5 stemming method was applied. In another example, turkish_deascii_zemberek2 indicates that the Turkish DeASCIIfier was used and the Zemberek stemming method was applied. To distinguish different query lengths, QM or QS suffixes were added at the ends of field names to generate run names. For category A and category B runs, two different Solr cores were created. We did not append category information to run names for simplicity because it would have created long names; instead, we present the results obtained for each category separately.

@&#RESULTS@&#

In order to measure the retrieval performance (based on a maximum of 1000 retrieved documents), we used the binary preference (bpref) calculated by trec_eval
                        
                           8
                        
                        
                           8
                           
                              http://trec.nist.gov/trec_eval/
                           
                         utility (version 8.1), which is the standard tool used by the TREC community for evaluating an ad hoc retrieval run. This performance measure was introduced by Buckley and Voorhees (2004) and it was designed for comparing systems over test collections with incomplete relevance judgments.

The bpref measure calculates a preference relation regarding whether the relevant documents are retrieved ahead of the documents that are judged irrelevant in the ranked result list. Thus, this metric uses relative ranks of judged documents and ignores un-judged documents.


                        Tables 6
                         and 7
                         show the results for category A, where the highest score in each row is highlighted in bold. The first column presents the results obtained when all of the documents and queries were the originals with Turkish letters. This column is the baseline and it was used to calculate the performance loss percentages for different strategies. The second column in each table presents the results obtained when all of the documents and queries were ASCIIfied, which is the traditional approach. The last two columns present the results obtained when all of the documents were indexed with Turkish accented letters and different deASCIIfiers were used during querying.

To determine whether the performance decrease caused by a specific strategy was statistically significant, we conducted a paired t-test at a 95% significance level. For a given row in each table, the results of strategies with significantly different mean effectiveness values compared with the baseline columns in the tables are denoted by †  for p <  0.05. Thus, the strategies with results highlighted by †  symbols exhibited significantly worse performance than the baseline for a particular stemming algorithm.

For all of the stemming methods, Turkish DeASCIIfier yielded better results than the ASCIIfaction method. However, the differences were not significant for the NS and F5 stemmers, whereas they were significant for the Snowball and Zemberek stemming options.

The performance loss caused by ASCIIfication was not substantial or statistically significant for the NS and F5 stemming approaches. By contrast, ASCIIfication method performed significantly worse than the baseline with the Snowball and Zemberek stemming options. This is because the Zemberek and Snowball stemmers cannot operate well on synthetic words. Indeed, Zemberek cannot process them at all. However, F5 is a simple truncating method and it is not affected by whether its input is valid Turkish words or not. Thus, F5 is a diacritic-insensitive stemmer, whereas Zemberek is a diacritic-sensitive stemmer.

For instance, in Table 6, when switching from the Turkish (baseline) column to the ASCII column, the performance of Zemberek stemmer dropped by 13.84% and the performance of Snowball stemmer dropped by 4.27%. However, NS and F5 were affected little (less than 1%).

Among the deASCIIfiers, Zemberek did not perform very well and it was the only strategy that performed significantly worse than the baseline (for all category A runs). Zemberek has the drawback that it expects proper nouns to be capitalized in order to produce correct results. For example, Zemberek cannot deASCIIfy the words turk, turkiye, and kibris, whereas it can deASCIIfy Turk, Turkiye, and Kibris. Furthermore, the query set contained many capitalized proper nouns and this limitation explains why Zemberek DeASCIIfier performed poorly. It should be noted that we lowercased the query terms in order to reflect real-world settings. In order to address this limitation, if no result was obtained when processing a term, we capitalized the first letter in the term and tried again. This simple technique improved the performance of the Zemberek DeASCIIfier, but it was not sufficient to change its rank order. We concluded that the multi-output nature of the Zemberek DeASCIIfier, which lacks a mechanism to select one of its outputs, degraded the retrieval performance in category A runs.

The Turkish DeASCIIfier could correctly recover ASCIIfied proper names when they were written in lowercase; thus, it was not case-sensitive. In contrast to Zemberek DeASCIIfier, it produced a single output. We observed that enabling the preservation of the original term slightly degraded its performance. For short queries, Turkish DeASCIIfier recovered ASCIIfied query terms with a success rate of 100%. Therefore, the first and last columns are exactly the same in Table 7.

In addition, some query words could be recovered by the Turkish DeASCIIfier but not by the Zemberek DeASCIIfier, such as dogalgaz, tatlises, akgunduz, cernobil, and sakip.

Among the stemmers, Zemberek performed better than F5 and Snowball. Given that the first column is the baseline, the Zemberek stemmer combined with the Turkish DeASCIIfier yielded the best overall results in this study.


                        Tables 8
                         and 9
                         show the category B results obtained when both the documents and queries were ASCIIfied. In each table, the best runs per row are highlighted in bold. It should be noted that the first two columns (Turkish and ASCII) are the same as the category A results. As mentioned earlier, the (†) symbol is used to denote a significant difference between a specific strategy and the baseline.

Similar to the category A results, the deASCIIfier approach performed better than the ASCIIfication approach for both query lengths. For medium-length queries, the Turkish DeASCIIfier produced the best results in Table 8. However, as shown in Table 9, the Zemberek DeASCIIfier performed slightly better than the Turkish DeASCIIfier with short queries. It is also interesting that when no stemming was used, the ASCIIfication method yielded exactly the same results as the Turkish DeASCIIfier for both Qs and Qm.

Surprisingly, the Zemberek DeASCIIfier obtained slightly better results than the baseline, although not significantly better. To understand these results, we performed a query-by-query analysis by sorting queries according to the bpref differences from their baseline levels. Thus, we identified the improved queries where Zemberek DeASCIIfier performed better (gains) and the degraded queries where Zemberek DeASCIIfier performed worse (losses) than the baseline. Lucene/Solr has a useful explain feature, which displays the details of the score calculation for each document that matches the query. After testing some of these queries with the explain feature, we found that expanded terms contributed more than once to the final score.

For example, in the query (QueryID: 349) Global ısınma, which achieved the highest gain (0.7428
                           −
                        0.3043=0.4385) in terms of bpref, ZDF expanded the ASCIIfied version of the second query term into three terms: isinma, ışınma, and ısınma. By design, ZDF retains the original text (isinma) as well as the words (ışınma, ısınma) generated by the Zemberek DeASCIIfier. However, it should be noted that the Zemberek DeASCIIfier produced ışınma, although it is not a correct Turkish word. In this example, the score contribution of the second term was counted/weighted three times more heavily. In other words, the multi-output nature of Zemberek DeASCIIfier boosted the accented terms in a query. For category B, the same expansion was also performed during indexing. This behavior caused gains if the accented word was a content-bearing word in a query. By contrast, it caused losses if the accented word was a function word in a query. However, there was no actual correlation between accents and the importance of terms. For example, in the query (QueryID: 278) Stresle Başa Çıkma Yolları, the most important term is Stresle, which does not contain accents. Boosting the remaining terms in this particular query caused one of the largest losses with respect to the baseline. By coincidence, the cumulative effect caused by the boosting mechanism was positive for short queries.

In this section, we summarize the experimental inferences that hold for all of the category (A & B) and query length (Qs & Qm) combinations.

The ASCIIfication strategy performed significantly worse than the baseline with the Snowball and Zemberek stemming options. However, with the F5 stemming option, the observed difference in the effectiveness between the Turkish (baseline) and ASCII strategies was both practically and statistically insignificant, which highlights the notion of “accent sensitivity” in stemmers.

In addition, for all of the stemming options, the Turkish DeASCIIfier performed as well as the baseline because there was no statistically significant difference between them.

In the previous section, we concentrated on the arithmetic mean of our effectiveness measure calculated over 72 queries. Thus, we did not investigate the performance losses for individual queries with respect to the baseline run.

In 2013, the TREC forum introduced a new risk-sensitive retrieval task, which rewards algorithms that achieve improvements in terms of their average effectiveness across topics (good performance on average), but that also maintain good robustness. The main goal of the risk-sensitive task is defined as: “to encourage research on algorithms that go beyond just optimizing average effectiveness in order to effectively optimize both effectiveness and robustness, and achieve effective tradeoffs between these two competing goals” (Collins-Thompson et al., 2014).


                     Wang et al. (2012) defined the robustness of a system as its ability to not perform poorly for any individual query with respect to a given baseline and also to achieve good average effectiveness over all topics. The risk-sensitive evaluation was motivated by the empirical fact that the retrieval strategies usually improve the effectiveness for specific queries while degrading it for others compared with a baseline system that does not use such strategies. End users tend to remember their bad search experiences when interacting with an information search system. Therefore, a robust system is desirable that does not disappoint its users often by minimizing the risk of significant failures relative to a particular baseline.

For each query q in the set Q of all N queries, the risk-reward tradeoff between the system under evaluation (run) and the baseline system was defined by Wang et al. (2012) as follows:

                        
                           (2)
                           
                              
                                 
                                    F
                                    
                                       R
                                       I
                                       S
                                       K
                                    
                                 
                                 =
                                 
                                    1
                                    N
                                 
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    N
                                 
                                 m
                                 a
                                 x
                                 
                                    [
                                    0
                                    ,
                                    b
                                    a
                                    s
                                    e
                                    l
                                    i
                                    n
                                    e
                                    
                                       (
                                       
                                          q
                                          i
                                       
                                       )
                                    
                                    −
                                    r
                                    u
                                    n
                                    
                                       (
                                       
                                          q
                                          i
                                       
                                       )
                                    
                                    ]
                                 
                                 ,
                              
                           
                        
                     where FRISK in Eq. (2) captures the absolute retrieval effectiveness loss for degraded queries relative to the baseline.

                        
                           (3)
                           
                              
                                 
                                    F
                                    
                                       R
                                       E
                                       W
                                       A
                                       R
                                       D
                                    
                                 
                                 =
                                 
                                    1
                                    N
                                 
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    N
                                 
                                 m
                                 a
                                 x
                                 
                                    [
                                    0
                                    ,
                                    r
                                    u
                                    n
                                    
                                       (
                                       
                                          q
                                          i
                                       
                                       )
                                    
                                    −
                                    b
                                    a
                                    s
                                    e
                                    l
                                    i
                                    n
                                    e
                                    
                                       (
                                       
                                          q
                                          i
                                       
                                       )
                                    
                                    ]
                                 
                              
                           
                        
                     
                  


                     FREWARD
                      in Eq. (3) captures the absolute retrieval effectiveness gain for improved queries relative to the baseline.

                        
                           (4)
                           
                              
                                 
                                    U
                                    
                                       G
                                       A
                                       I
                                       N
                                    
                                 
                                 =
                                 
                                    F
                                    
                                       R
                                       E
                                       W
                                       A
                                       R
                                       D
                                    
                                 
                                 −
                                 
                                    F
                                    
                                       R
                                       I
                                       S
                                       K
                                    
                                 
                              
                           
                        
                     
                  

The risk-sensitive utility measure URISK
                      of a system over the set of queries Q was introduced and defined by Wang, Bennett, and Collins-Thompson (2012) as:

                        
                           (5)
                           
                              
                                 
                                    U
                                    
                                       R
                                       I
                                       S
                                       K
                                    
                                 
                                 =
                                 
                                    U
                                    
                                       G
                                       A
                                       I
                                       N
                                    
                                 
                                 −
                                 α
                                 ·
                                 
                                    F
                                    
                                       R
                                       I
                                       S
                                       K
                                    
                                 
                                 =
                                 
                                    F
                                    
                                       R
                                       E
                                       W
                                       A
                                       R
                                       D
                                    
                                 
                                 −
                                 
                                    (
                                    1
                                    +
                                    α
                                    )
                                 
                                 
                                    F
                                    
                                       R
                                       I
                                       S
                                       K
                                    
                                 
                                 ,
                              
                           
                        
                     where α ≥ 0 is a risk-aversion parameter that controls the tradeoff between risk and reward. As explained in the TREC Web 2013 Guidelines
                        9
                     
                     
                        9
                        
                           http://research.microsoft.com/en-us/projects/trec-web-2013
                        
                     , for large α values, URISK will become more sensitive to large losses relative to the baseline. When α is small, URISK will tend to omit the baseline. When α is zero, URISK
                      is equal to the average effectiveness across queries.


                     Wang et al. (2012) noted that a risk-sensitive utility measure can be calculated for any commonly used retrieval effectiveness measure (e.g., NDCG@k, ERR@k, recall, and average precision). The TREC community updated their evaluation tools to compute risk-sensitive versions of retrieval effectiveness measures.

The risk-sensitive evaluation methodology is applicable to our experimental setup. In the present study, tr_* runs were the baselines where the documents and queries had their original forms with accented Turkish letters. Thus, we investigated the robustness by performing a comparative risk-sensitive evaluation of different approaches for handling diacritics in Turkish IR.

We used the gdeval.pl TREC evaluation tool (downloaded from trec-web-2013
                        10
                     
                     
                        10
                        
                           http://github.com/trec-web/trec-web-2013
                        
                      GitHub repository) to calculate the normalized discounted cumulative gain (NDCG@20) (Järvelin & Kekäläinen, 2002) and the expected reciprocal rank (ERR@20) (Chapelle et al., 2009) (the standard retrieval effectiveness metrics used by the TREC Web Track) (Collins-Thompson et al., 2014), as well as their risk-sensitive versions for α=1 and α=5. For example, to perform a risk-sensitive evaluation relative to the baseline, gdeval.pl was used as in
                        11
                     
                     
                        11
                        
                           gdeval.pl -riskAlpha 5 -baseline tr_ns_QM.txt qrels.txt ascii_ns_QM.txt
                        
                      to obtain the last two data items in the second row of Table 10
                     .


                        Table 10 shows the retrieval effectiveness measures and their risk-sensitive versions for category A runs where the deASCIIfier processes occurred only during the search time. As shown in Table 10, the Zemberek DeASCIIfier obtained systematically worse results than the other methods, in terms of both ERR@20 and NDCG@20. Based on these results, we can conclude that expanding a query term to all of its possible valid diacritized forms, but without performing the same expansion at index time, has deleterious effects on the effectiveness of retrieval.

Furthermore, URISK α=5 levels with the Zemberek DeASCIIfier deviated from the other results. This was because Zemberek DeASCIIfier led to large losses (significant failures) with respect to the baseline for certain queries. In addition, the risk measure became more conservative when large α (e.g., 5) value was used. Based on the analysis of the category A runs, we make the following observations for different query length options.


                        Medium-length queries. All of the risk-sensitive metrics were negative numbers, so they all performed worse than the baseline. For NS and F5, there was no clear winner between ascii and turkish_deascii in terms of risk averseness. According to the ERR@20 measure, ascii was better, whereas according to the NDCG@20 measure, turkish_deascii was better. For the Snowball and Zemberek stemming options, turkish_deascii was the most risk-averse run in terms of all the metrics and α values.


                        Short-length queries. For all of the stemming options and metrics, the turkish_deascii runs had zero risk, and thus they performed as well as the baseline run.


                        Table 11
                         shows the retrieval effectiveness measures and their risk-sensitive versions for the category B runs. As shown in Table 11, the ranking of a strategy could change when risk-based metrics were used instead of ad hoc metrics. For example, ascii_f5_QM was demoted from rank 1 to rank 2, while turkish_deascii_f5_QM was promoted from rank 2 to rank 1. In other examples, the Zemberek DeASCIIfier was sometimes the best performing method according to the overall gain across queries (average effectiveness), whereas the Turkish DeASCIIfier was the best performing method for the same runs when URISK α=5 robustness measures were used. This is because the risk-sensitive evaluation method punished Zemberek DeASCIIfier by weighting losses α+1 times as heavily as gains. Based on the analysis of the category B runs, the following observations are true for all metrics and α values.


                        Medium-length queries. For NS, the ascii and turkish_deascii strategies were equally robust. For the F5, Snowball, and Zemberek stemming options, turkish_deascii was the most risk-averse run.


                        Short-length queries. For NS, the ascii and turkish_deascii strategies were equally robust. For the Snowball and Zemberek stemming options, turkish_deascii was the most risk-averse run.

For F5, zemberek2_deascii was the most risk-averse run for ad hoc metrics and risk equivalents when α=1. However, when α was increased to 5, turkish_deascii was more robust. Therefore, turkish_deascii performed better in avoiding large losses (with respect to the baseline) than zemberek2_deascii, because when α was large, the system was less forgiving of large losses.

@&#CONCLUSIONS@&#

In this study, we proposed the use of a DR (deASCIIfication) technique in IR and we compared its effectiveness and robustness with a latinization (ASCIIfication) technique, which is the traditional method for addressing the problems caused by search queries and documents written without diacritics. Based on our results, we reached the following conclusions.

                        
                           •
                           ASCIIfication is preferable (especially if documents have diacritic mistakes) when no stemmer or an accent-insensitive stemmer (simple truncate stemming) is employed during text analysis.

The ASCIIfication approach should be avoided if we want to take advantage of accent-sensitive stemmers.

DeASCIIfication is preferable if we employ an NLP-based stemmer that expects to operate on valid Turkish words.

DeASCIIfication is more flexible in category A settings (documents are written well in terms of diacritics) because it does not require a change in the index. This method is applied during the retrieval time only, so it can be enabled or disabled on the fly.

In our practical experiments, we demonstrated how open source solutions can be used to set up a diacritic-insensitive Turkish retrieval system. To facilitate repeatability, reproducibility, and open-source sharing, the custom token filter implementations, Solr field type definitions (used in this study), and the script file used to obtain the evaluation metrics have been made publicly available to other researchers (and anyone else who is interested) under Apache License, Version 2.0. They are available on GitHub in the public repository: http://github.com/iorixxx/lucene-solr-analysis-turkish
                  

The evaluation methodology used in the present study measures the success and robustness of search systems when handling diacritic-less query terms and content. The experimental methodology employed for evaluating the diacritics sensitivity of retrieval systems could also be applied to other languages with accented letters in their alphabets (e.g., Czech, Danish, Finnish, French, Greek, Hungarian, Icelandic, Latvian, Lithuanian, Norwegian, Polish, Romanian, Swedish, and Spanish).

In future research, we will combine the outputs of the Zemberek DeASCIIfier and Turkish DeASCIIfier to overcome the shortcomings of each. It should be noted that the Turkish DeASCIIfier produces a single output for a given word, whereas the Zemberek DeASCIIfier can produce multiple outputs. In addition, we will try to develop a system that selects the best candidate from the united output of combined deASCIIfiers to determine whether this approach significantly enhances the effectiveness of IR, or if it simply adds redundant computations and run-time complexity overheads.

In our future research, we also plan to investigate the accent sensitivity of successor variety stemming (Stein & Potthast, 2007), which is a special form of truncate stemming.

Supplementary material associated with this article can be found, in the online version, at 10.1016/j.ipm.2015.08.004
                  


                     
                        
                           
                        
                     
                  

@&#REFERENCES@&#

