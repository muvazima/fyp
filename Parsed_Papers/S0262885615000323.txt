@&#MAIN-TITLE@&#Local color transformation analysis for sudden illumination change detection

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Illumination change detection is carried out without assuming a specific color transformation.


                        
                        
                           
                           The method is designed to improve existing standard foreground detection algorithms.


                        
                        
                           
                           Backup procedures are designed to recover from detection failures.


                        
                        
                           
                           The method is applicable to a wide range of varying illumination conditions.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Background modeling

Foreground detection

Illumination invariance

Color transformation

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

The segmentation of the foreground objects in a scene is a fundamental task which lays at the foundation of many computer vision systems. Most approaches to this task create a model of the background that is updated progressively as time passes. Consequently sudden illumination changes are challenging problems, since the appearance of the background no longer matches the past observations. This issue has been studied for many years [6,9,10,20] due to its relevance to applications. Practical computer vision systems are placed in environments where the illumination conditions vary through time. For example, indoor scenes frequently exhibit light switching, while passing clouds affect outdoor cameras.

There are different ways to address this problem. On one side there are pixel-level algorithms that work by analyzing the scene on a pixel by pixel basis, so that an independent decision is made for each pixel. On the other hand, there are other approaches which work at a higher level, namely block-level (where the decision for one pixel depends on the information from several nearby pixels) or frame-level (where all the pixels of the frame might be taken into account to decide whether a pixel belongs to the foreground). It is often the case that methods use multiple levels simultaneously to analyze the video. A representative case of this approach is the Wallflower system 28, which uses the three levels already mentioned. One of the downsides attributed to this method is that it uses a non flexible criterion in real situations because it selects a set of background scene models representing different situations and each frame has to be assigned to the model which produces the fewest foreground pixels. This implies that each possible situation must be predicted in advance, and that a representative background model of each situation must be found. Hence, the approach is less adequate for scenes where unpredictable events occur which affect the background, such as a left object or a parked car which starts moving.

There are other methods using region-level analysis as in the case of [22]. This approach analyzes the image at block and pixel levels. The main idea is that each pixel belongs to several overlapping blocks, so that it is determined whether it belongs to the background or not depending on how it has been classified in each of these blocks.

A more recent algorithm is [14], which uses pixel-level and image-level elements. The proposal consists in using a two-layer architecture based on a Gaussian Mixture Model to represent the background. Then the result is optimized using a Markov random field decision framework.

On the other hand, there are many algorithms that work at the pixel level. For example [27] is based on applying a homomorphic filter, while [12] performed an analysis with stereo vision and employs a disparity model created offline to mitigate the consumption of extra CPU time required for this type of processing. Also, [8] uses discriminative texture features to capture background statistics, by means of texture operators named local binary patterns (LBP). Finally, [6] presents an adaptive algorithm that uses multiple feature subspaces and Principal Component Analysis to capture and learn different lighting conditions.

Our aim here is to develop an illumination change detection system which works along an existing foreground detection algorithm. A previous example of an add-on illumination change detection algorithm can be found in [32]. Unlike our proposal, they assume that the order of pixel values is preserved in local neighborhoods when illumination changes occur, and they root their proposal on the analysis of physical properties, namely the radiance. In contrast to this, we are not interested in the particular form of the color transformation, but in its smoothness properties. This way, we consider that any color transformation which is not smooth can not correspond to a lighting change, i.e. it is due to a foreground object. Hence, we are able to detect variations in illumination irrespective of the particular features of the color transformation at hand. Moreover, the procedure is largely independent from the baseline background model, so it can be used to improve a number of well known methods which are not specifically designed to work under illumination changes.

This paper is organized as follows. Section 2 presents the illumination change detection method. Section 3 presents some experimental results to demonstrate the ability of our approach to manage complex scenes. The main features and properties of our proposal are discussed in Section 4. Finally, Section 5 is devoted to conclusions.

The illumination change management procedure that we present here has two parts. The first one classifies the pixels of the current frame according to its current state with respect to illumination changes (Subsection 2.1). The second part uses this information to decide which pixels must undergo a reset because an illumination change has rendered their background models outdated (Subsection 2.2).

Here we must estimate the illumination state of each pixel of the current video frame. The illumination state of pixel i is formed by three fuzzy variables Rough, Difference, and Baseline; their values (membership degrees) will be noted α
                        
                           i
                        ,
                        β
                        
                           i
                        ,
                        γ
                        
                           i
                        
                        ∈[0,1], respectively. The interpretation of the variables is as follows:
                           
                              •
                              
                                 Rough indicates whether the transformation of the colors in the previous frame to the colors in the current frame is not smooth in the vicinity of pixel i. If αi
                                  is high, then it is unlikely that an illumination change is happening, since illumination changes produce smooth changes in the colors of the background and the foreground objects.


                                 Difference indicates whether the color of pixel i in the current frame is very different from that stored in the background model for pixel i. If βi
                                  is high, then either an illumination change is happening or a foreground object is present.


                                 Baseline indicates whether the baseline background model has detected a foreground object. Please note that γ
                                 
                                    i
                                 
                                 ∈{0,1} for background models that do not output a degree of confidence for the foreground detection.

Next we describe how to compute the fuzzy membership values αi
                         and βi
                        ; please note that γi
                         is simply the output of the baseline background model.

Let us center our attention in a small neighborhood Wi
                         of pixel i. In our experiments we have considered square windows of size 5×5pixels, which offer a good tradeoff between efficiency and accuracy. Now we define a vector field which represents the transformation of the colors of the neighborhood Wi
                         in the background model to the colors of Wi
                         in the current frame:
                           
                              (1)
                              
                                 
                                    f
                                    i
                                 
                                 :
                                 
                                    ℝ
                                    3
                                 
                                 →
                                 
                                    ℝ
                                    3
                                 
                              
                           
                        where tristimulus color values are assumed, but any color space could be used. Now, our task is to detect those vector fields fi
                         which are not smooth. A simple and fast way of measuring the smoothness of fi
                         is based on the computation of the following quotients:
                           
                              (2)
                              
                                 
                                    q
                                    i
                                 
                                 
                                    j
                                    k
                                 
                                 =
                                 
                                    
                                       
                                          a
                                          i
                                       
                                       
                                          j
                                          k
                                       
                                    
                                    
                                       
                                          b
                                          i
                                       
                                       
                                          j
                                          k
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (3)
                              
                                 
                                    a
                                    i
                                 
                                 
                                    j
                                    k
                                 
                                 =
                                 
                                    
                                       
                                          
                                             f
                                             i
                                          
                                          
                                             
                                                x
                                                j
                                             
                                          
                                          −
                                          
                                             f
                                             i
                                          
                                          
                                             
                                                x
                                                k
                                             
                                          
                                       
                                    
                                    2
                                 
                              
                           
                        
                        
                           
                              (4)
                              
                                 
                                    b
                                    i
                                 
                                 
                                    j
                                    k
                                 
                                 =
                                 
                                    
                                       
                                          
                                             x
                                             j
                                          
                                          −
                                          
                                             x
                                             k
                                          
                                       
                                    
                                    2
                                 
                              
                           
                        where x
                        
                           j
                         and x
                        
                           k
                         are the colors in the background model of two pixels j,
                        k
                        ∈
                        W
                        
                           i
                        , and fi
                        (x
                        
                           j
                        ) and fi
                        (x
                        
                           k
                        ) are the colors of those pixels in the current frame. The vector field fi
                         is non smooth whenever qi
                         attains high values for some pairs j,
                        k
                        ∈
                        W
                        
                           i
                        . As seen in Fig. 1
                        , a smooth transformation is one that maps similar colors in the background model to similar colors in the current frame, even if the colors change considerably from the background model to the current frame. That is, the distances ‖x
                        
                           j
                        
                        −
                        f
                        
                           i
                        (x
                        
                           j
                        )‖ are irrelevant to the smoothness of fi
                        . This way we can manage the switching of lights of any color, for example yellow or blue lights. It must be pointed out that low values of qi
                         are not interesting because they are usually associated with homogeneous foreground objects passing in front of textured backgrounds, which are adequately managed by standard foreground detection algorithms.

In practice pixel noise can lead to large errors in the estimation of qi
                        , in particular if the denominator bi
                         contains noise. We alleviate this by considering a filtered quantity ϕ
                        
                           i
                        :
                           
                              (5)
                              
                                 
                                    ϕ
                                    i
                                 
                                 
                                    j
                                    k
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      a
                                                      i
                                                   
                                                   
                                                      j
                                                      k
                                                   
                                                
                                                
                                                   B
                                                   low
                                                
                                             
                                          
                                          
                                             if
                                             
                                             
                                                b
                                                i
                                             
                                             
                                                j
                                                k
                                             
                                          
                                          
                                             <
                                             
                                                B
                                                low
                                             
                                          
                                       
                                       
                                          
                                             0
                                          
                                          
                                             if
                                             
                                             
                                                b
                                                i
                                             
                                             
                                                j
                                                k
                                             
                                          
                                          
                                             >
                                             
                                                B
                                                high
                                             
                                          
                                       
                                       
                                          
                                             
                                                q
                                                i
                                             
                                             
                                                j
                                                k
                                             
                                          
                                          
                                             otherwise
                                          
                                          
                                       
                                    
                                 
                              
                           
                        where Blow
                         and Bhigh
                         are suitable lower and upper thresholds for the denominator value bi
                        , with Blow
                        
                        <
                        Bhigh
                        . Please note that Blow
                         manages low lighting conditions, where pixel values have little precision. On the other hand, Bhigh
                         ensures that highly textured backgrounds (which are associated to high values of ai
                         and bi
                        ) are not mistaken as non smooth color transformations. Finally we use the highest ϕ
                        
                           i
                        (j,
                        k) as a measure of the roughness of fi
                        :
                           
                              (6)
                              
                                 
                                    α
                                    i
                                 
                                 =
                                 min
                                 
                                    
                                       1
                                       ,
                                       
                                          1
                                          
                                             K
                                             α
                                          
                                       
                                       
                                          
                                             
                                                max
                                                
                                             
                                             
                                                j
                                                ,
                                                k
                                                ∈
                                                
                                                   W
                                                   i
                                                
                                             
                                          
                                       
                                       
                                          ϕ
                                          i
                                       
                                       
                                          j
                                          k
                                       
                                    
                                 
                              
                           
                        where the min function ensures that α
                        
                           i
                        
                        ∈[0,1], and Kα
                         are a suitable scaling parameter.

On the other hand, the second membership value βi
                         is obtained from the squared Euclidean distance between the color of pixel i in the current frame and the color stored in the background model for pixel i:
                           
                              (7)
                              
                                 
                                    β
                                    i
                                 
                                 =
                                 
                                    1
                                    
                                       K
                                       β
                                    
                                 
                                 
                                    
                                       
                                          
                                             x
                                             0
                                          
                                          −
                                          
                                             f
                                             i
                                          
                                          
                                             
                                                x
                                                0
                                             
                                          
                                       
                                    
                                    2
                                 
                              
                           
                        where x
                        1 and f (x
                        1) are the colors of the pixel at hand in the background model and in the current frame, respectively; and Kβ
                         is another scaling parameter.

The possible values of the three fuzzy membership variables are mapped to eight possible illumination states for pixel i according to Table 1
                        . The standard fuzzy set operations are used to compute the fuzzy memberships of the eight states from α
                        
                           i
                        ,
                        β
                        
                           i
                        ,
                        γ
                        
                           i
                        . Then a defuzzification is carried out by declaring pixel i to be in the state with the maximum membership.

Pixels in state ‘dubious’ must be corrected to either ‘Foreground object’ or ‘Background with illumination change’. That is, given a pixel with a smooth color transformation and a large difference with respect to the background model color, which is classified as foreground by the baseline algorithm, we cannot tell whether it is a real foreground object or a background pixel subject to an illumination change. At the pixel level this ambiguity cannot be solved. Hence we propose a frame level procedure for this task. From each dubious pixel i we trace lines to the four directions up, down, left, and right, and to the four diagonals. If any of these lines hits a pixel in the ‘Foreground object’ or ‘Background with illumination change’ states, then we count a vote for that state. If a line exits the frame without having hit a pixel in any of these two states, then that line does not cast any vote. Finally, pixel i is changed to the state with the most votes. If there is a tie, then no correction is made. The rationale behind this procedure is that dubious pixels belonging to foreground objects are usually located in the interior of the objects. Hence, for these pixels most of the eight lines hit some pixels in the ‘Foreground object’ state which lie in the borders of the object. It must be noted that dubious pixels are more frequent in objects with a homogeneous interior. On the other hand, dubious pixels belonging to the background are usually surrounded by some pixels in the ‘Background with illumination change’ state.


                        Fig. 2
                         exemplifies the illumination state estimation procedure described in this subsection. Please note that the states are represented in the pseudocolors specified in Table 1. As shown, the dubious pixels inside the person are corrected to the ‘Foreground object’ state, while those which lie in his shadow are corrected to the ‘Background with illumination change’ state. Additionally, Fig. 3
                         shows the flowchart of our proposal for the k-th frame.

Once the illumination state of every pixel has been computed by the procedure explained in the previous subsection, a decision must be made about which pixels must be reset. A reset should be carried out whenever the system has failed to detect an illumination change in the background, so that the background model is not updated and pixels stay wrongly in the foreground state during many frames. We keep a counter ci
                         for each pixel i which counts how many consecutive frames the pixel has been declared as foreground by the baseline background model. If ci
                         surpasses a prespecified limit Climit
                        , then we assume that the baseline algorithm has failed, so that the pixel must be reset. A pixel is reset by application of the initialization procedure of the baseline algorithm to it.

The illumination state of the pixels is used to perform an early reset of some pixels by increasing their counters ci
                         when an illumination change has been detected on them. Two rules are applied on each video frame. The first one (‘soft reset’) says that pixels i which are in the ‘Background with illumination change’ state in the current frame modify their counters as follows:
                           
                              (8)
                              
                                 
                                    c
                                    i
                                 
                                 =
                                 max
                                 
                                    
                                       c
                                       i
                                    
                                    
                                       C
                                       soft
                                    
                                 
                                 .
                              
                           
                        
                     

The second rule (‘hard reset’) says that pixels i which have been in the ‘Background with illumination change’ state continuously for the last Khard
                         frames modify their counters as follows:
                           
                              (9)
                              
                                 
                                    c
                                    i
                                 
                                 =
                                 max
                                 
                                    
                                       c
                                       i
                                    
                                    
                                       C
                                       hard
                                    
                                 
                              
                           
                        where C
                        
                           hard
                        
                        >0 and
                           
                              (10)
                              
                                 0
                                 <
                                 
                                    C
                                    soft
                                 
                                 ≤
                                 
                                    C
                                    hard
                                 
                                 ≤
                                 
                                    C
                                    limit
                                 
                                 .
                              
                           
                        
                     

The soft and hard reset rules model situations where the likelihood of an illumination change is low and high, respectively. The longer a pixel stays in the ‘Background with illumination change’ state, the most likely that the illumination change is real.

@&#EXPERIMENTAL RESULTS@&#

This section analyzes the performance of our proposal
                        1
                     
                     
                        1
                        The source code and some demo videos of our proposal are available at http://www.lcc.uma.es/%7Eezeqlr/lighting/lighting.html.
                     . To this end we compare the original versions of several methods against these same methods modified according to our proposal. For this study various sequences and a wide set of parameters have been employed in order to obtain a clear idea of the merits of the considered alternatives. The methods to be tested are described in Subsection 3.1, and the video sequences are presented in Subsection 3.2. Our choice for the parameters of the competing models is explained in Subsection 3.4, and the quantitative performance measures are outlined in Subsection 3.3. The effect of the values of the parameters on the performance of the methods is studied in Subsection 3.4. Finally, the qualitative and quantitative results are shown in Subsections 3.5 and 3.6, respectively.

@&#METHODS@&#

Our proposal analyzes the scene at a low level pixel by pixel, as is the case of the so-called pixel-level methods. This is why we use a set of five methods of this type for the experiments. They have been chosen on the basis of the availability of a public and reasonably well tested implementation, and their popularity in the computer vision community. In order to fulfill these requirements, we have restricted our attention to version 1.3.0 of the BGS library. It is freely accessible from its website
                           2
                        
                        
                           2
                           
                              https://github.com/andrewssobral/bgslibrary.
                        . The first studied method is Pfinder [30], noted WrenGA, which uses only a single Gaussian per pixel. Also, three methods have been selected as representatives of the methods based on mixtures of Gaussians: GrimsonGMM [26], GMMV1 [13] and GMMV2 [34]. A fifth method has been chosen which is based on a fuzzy approach to self-organizing background subtraction (SOBS) [18]; it will be named FASOM. On the other hand, we have tested three background subtraction methods which are specifically designed for this kind of problems and therefore have not been modified with our proposal: Agrawal [1], Horprasert [9] and Reddy [21]. We have used the implementation available in the author's website
                           3
                        
                        
                           3
                           
                              http://www.umiacs.umd.edu/aagrawal/cvpr06/EdgeSuppression.html.
                         for Agrawal, a freely available implementation
                           4
                        
                        
                           4
                           
                              http://www.markyd13.com/code/background_subtraction/.
                         to test Horprasert, and finally the authors of the Reddy method have provided us the source code.

Except Agrawal and Horprasert which use Matlab, all the code is written in C++ language. Version 2.4.3 of the OpenCV Computer Vision library
                           5
                        
                        
                           5
                           
                              http://sourceforge.net/projects/opencvlibrary.html.
                         has also been employed. Please note that GMMV1 and GMMV2 are the current standard foreground detection methods of the OpenCV library.

In order to reduce the CPU time of our proposal, the Threading Building Blocks library version 4.1 has been used. It is also freely available in its website
                           6
                        
                        
                           6
                           
                              http://threadingbuildingblocks.org/.
                        . All the experiments have been carried out on a desktop PC with a quad core 3.10GHz CPU, 6GB RAM and standard hardware. No GPU acceleration has been done.

To make a comparison as clear and fair as possible, no additional post-processing has been done either in the original algorithms or their modified versions.

With the aim of analyzing the behavior of the methods in real situations, we consider changes in spot lights and also to ambient light, testing a total of eight real sequences: five outdoor and three indoor. The objects to detect are persons or vehicles. The chosen sequences are available on the Internet. They represent different conditions like abrupt lighting changes, such as switching a light; or soft such as the passage of clouds in an outdoor scene. Moreover, although most of the selected sequences present midway objects, there are sequences with distant objects and other with close objects.

The first indoor sequence is Lobby (LB), available on this website
                           7
                        
                        
                           7
                           
                              http://perception.i2r.a-star.edu.sg/bk_model/bk_index.html.
                        , which has abrupt lighting changes and is also characterized by having background areas particularly vulnerable to camouflage effects. This situation occurs when the algorithm erroneously segments as background some regions that in fact belong to the foreground because they have very similar colors. The next indoor sequence is RecCenter (RC) which exhibits smooth lighting changes and a large number of people crossing the scene, available at GTILT Dataset
                           8
                        
                        
                           8
                           
                              http://www.ece.gatech.edu/research/pica/GTILT.html.
                        . The opposite situation happens in LightSwitch (LS), where illumination changes are abrupt and there is only one person who traverses the scene several times during the sequence.

On the other hand, outdoor scenes are represented by the following videos: Cars1 (C1), Cars3 (C3), Roadside (RS), PED1 (P1) and Bank (BK), which have been obtained also from GTILT Dataset. C1 and C2 are urban sequences characterized by having most objects relatively close to the camera. RS is a rural sequence that presents lighting changes affecting the image progressively as the clouds move over the scene. P1 is very interesting because it has significant changes in lighting and there are foreground objects near and far from the camera. Finally BK is a sequence with large illumination changes, in which both people and vehicles appear at close range.

To be as fair as possible, a constant sampling rate was used to determine which frames would be used as ground truth. Since the frequencies at which foreground objects appear in the scene are different in each video, the sampling frequency is also different in each case so that all benchmarks have a similar number of ground truth frames. For Lobby, half of the ground truth frames available in the repository only consider the last 200 frames in a sequence of 1546 frames, so we chose a wider sample representing 1446 frames (the first hundred frames are reserved for training), including frames with different amounts of lighting, and we performed a manual segmentation of these frames. Furthermore the GTILT Dataset provides one or no ground truth frames depending on the case, so that the ground truth frames of these sequences have also been segmented manually.

We have selected six quantitative performance measures to compare the segmentation methods we have considered. First of all, the accuracy of a method is obtained as follows (higher is better):
                           
                              (11)
                              
                                 AC
                                 =
                                 
                                    
                                       card
                                       
                                          
                                             A
                                             ∩
                                             B
                                          
                                       
                                    
                                    
                                       card
                                       
                                          
                                             A
                                             ∪
                                             B
                                          
                                       
                                    
                                 
                              
                           
                        where ‘card’ stands for the number of elements of a set, A is the set of all pixels which belong to the foreground, and B is the set of all pixels which are classified as foreground by the analyzed method:
                           
                              (12)
                              
                                 A
                                 =
                                 
                                    
                                       t
                                       |
                                       χ
                                       
                                          t
                                       
                                       =
                                       1
                                    
                                 
                              
                           
                        
                        
                           
                              (13)
                              
                                 B
                                 =
                                 
                                    
                                       t
                                       |
                                       
                                          χ
                                          ˜
                                       
                                       
                                          t
                                       
                                       =
                                       1
                                    
                                 
                                 .
                              
                           
                        
                     

On the other hand, the proportions of false negatives and false positives (lower is better) are given by:
                           
                              (14)
                              
                                 F
                                 N
                                 =
                                 
                                    
                                       card
                                       
                                          
                                             A
                                             ∩
                                             
                                                B
                                                ¯
                                             
                                          
                                       
                                    
                                    
                                       card
                                       
                                          
                                             A
                                             ∪
                                             B
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (15)
                              
                                 F
                                 P
                                 
                                 =
                                 
                                 
                                    
                                       card
                                       
                                          
                                             
                                                A
                                                ¯
                                             
                                             ∩
                                             B
                                          
                                       
                                    
                                    
                                       card
                                       
                                          
                                             A
                                             ∪
                                             B
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

From set theory we know that:
                           
                              (16)
                              
                                 AC
                                 ,
                                 
                                 F
                                 N
                                 ,
                                 
                                 F
                                 P
                                 ∈
                                 
                                    01
                                 
                              
                           
                        
                        
                           
                              (17)
                              
                                 AC
                                 +
                                 F
                                 N
                                 +
                                 F
                                 P
                                 =
                                 1
                                 .
                              
                           
                        
                     

The optimal performance would be achieved for AC =1,
                        FN
                        =0,
                        FP
                        =0. On the other hand, the worst possible performance corresponds to AC =0,
                        FN +
                        FP
                        =1.

The precision and recall measures can be computed as follows (higher is better):
                           
                              (18)
                              
                                 P
                                 R
                                 
                                 =
                                 
                                 
                                    
                                       card
                                       
                                          
                                             A
                                             ∩
                                             B
                                          
                                       
                                    
                                    
                                       card
                                       
                                          B
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (19)
                              
                                 R
                                 C
                                 
                                 =
                                 
                                 
                                    
                                       card
                                       
                                          
                                             A
                                             ∩
                                             B
                                          
                                       
                                    
                                    
                                       card
                                       
                                          A
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (20)
                              
                                 P
                                 R
                                 ,
                                 
                                 R
                                 C
                                 ∈
                                 
                                    01
                                 
                                 .
                              
                           
                        
                     

Often there is a tradeoff between precision and recall, i.e. we can make one of them grow as desired at the expense of diminishing the other. Finally, the F-measure combines both of them into an overall performance measure:
                           
                              (21)
                              
                                 F
                                 M
                                 
                                 =
                                 
                                 
                                    
                                       2
                                       ⋅
                                       P
                                       R
                                       ⋅
                                       R
                                       C
                                    
                                    
                                       P
                                       R
                                       +
                                       R
                                       C
                                    
                                 
                                 .
                              
                           
                        
                     

Here the parameter selections for the competing methods are presented. Table 2
                         shows the parameters tested in the simulations. All the values that have been found to yield good results for at least some of the sequences have been included. Moreover, we have also included the values recommended by the BGS library and those from the original implementations. An exhaustive parameter search would be unfeasible due to the number of parameters to be tuned for each method and sequence. It was found that our proposal only requires tuning parameter Climit
                        , which determines when the pixels are reset, as seen in Sub-section 2.2 because the other parameters do not appreciably affect the performance. Consequently the following fixed values were used: β
                        
                           low
                        
                        =10, β
                        
                           high
                        
                        =50, K
                        
                           α
                        
                        =100, K
                        
                           β
                        
                        =20, C
                        
                           soft
                        
                        =25 and C
                        
                           hard
                        
                        =50. The combination of all considered values for each parameter yields the set of tested configurations for each algorithm.


                        Fig. 4
                         shows the results of each algorithm in terms of various performance measures. In the case of FN versus FP, the closer the points are to the origin, the better the segmentation. For the other performance measures, the farther from the origin, the better the achieved results. Therefore, this figure indicates that our proposal improves the original algorithm for all the considered methods, since for every configuration of the original algorithm there is a configuration of our proposal that outperforms it. It is also worth mentioning that Agrawal, Horprasert and Reddy are unable to obtain good results in these sequences.

In this subsection the quantitative performance of the methods is assessed. Figs. 4 to 7
                        
                        
                         display the results obtained by the original and modified algorithms in some frames of the studied sequences. In general it can be observed that our proposal is capable of removing a significant amount of false positives and, to a lesser extent eliminates some false negatives.

It is interesting to note that, in the sequences of Fig. 5, WrenGA and FASOM suffer from false positives due to changes in the light coming from the spotlights, but our proposal is able to correct this behavior. Moreover, virtually all original algorithms suffer from cast shadows in these sequences and again our proposal is able to attenuate the problem in several cases. This kind of artifact occurs when foreground objects cast shadows and they are segmented erroneously as part of the foreground.

Camouflage is another common problem for the tested segmentation methods. For example, in the first column (Lobby), and particularly in the third (WrenGA) and fifth (GrimsonGMM) rows, the person is not completely segmented. In contrast to this, the second column shows how our proposal is able to alleviate this problem significantly.

For the last indoor sequence (LightSwitch) in Fig. 6, the chosen frame is a clear example in which background objects are segmented as foreground due to changes in lighting. On the left side of the scene there is an object which is segmented as part of the foreground. Our proposal detects the occurrence of a change of lighting in this position and therefore succeeds in segmenting it. In the same frame FASOM segments the scene poorly, and it produces a large number of false positives; this behavior also occurs in other sequences. As seen, our modification can correct this problem to the point of achieving results which are similar to those of the remaining algorithms.

On the other hand, the remaining sequences (Figs. 7 and 8
                        ) contain outdoor frames only. There are a significant number of false positives in the background due to lighting changes and our proposal is able to overcome this problem. These sequences also exhibit an important number of false negatives; e.g. the frames corresponding to C3, C1 and Bank show that the vehicle is not fully segmented. However, our proposal is able to fill in the gaps in a satisfactory way.

As we see in Fig. 5 our proposal produces some isolated foreground pixels for LB sequence with FASOM. In order to correct these effects we could use a two step post-processing, namely first erode the output image with a structuring element operator and then dilate the result using the same element. Two structuring elements have been considered; a square element E
                        
                           square
                         and a diamond shaped element E
                        
                           diamond
                        :
                           
                              (22)
                              
                                 
                                    E
                                    square
                                 
                                 =
                                 
                                    
                                       
                                          
                                             1
                                          
                                          
                                             1
                                          
                                       
                                       
                                          
                                             1
                                          
                                          
                                             1
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (23)
                              
                                 
                                    E
                                    diamond
                                 
                                 =
                                 
                                    
                                       
                                          
                                             0
                                          
                                          
                                             1
                                          
                                          
                                             0
                                          
                                       
                                       
                                          
                                             1
                                          
                                          
                                             1
                                          
                                          
                                             1
                                          
                                       
                                       
                                          
                                             0
                                          
                                          
                                             1
                                          
                                          
                                             0
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     


                        Fig. 9
                         exemplifies the results of such post-processing. It shows the results of the original and the modified version of FASOM in a LB frame. If we do not use post-processing our proposal is able to get 0.79 F-measure. When we use E
                        
                           square
                         as the structuring element, it achieves 0.88 (fourth column). Finally, by employing E
                        
                           diamond
                         and the structuring element, we increase the score to 0.89 (fifth column). It must be highlighted that no post-processing has been used in any experiments other than those reported in Fig. 9.

The above subsection has focused on the qualitative results. A clearer view of the differences among our proposal and the original algorithms can be obtained from Fig. 10
                        . It shows the difference between the performance achieved by our proposal and that of the original version in terms of F-measure. The configuration that performs best in terms of F-measure is employed for this figure.

The improvement is different in each method. The FASOM method and, to a lesser extent WrenGA are cases where an improvement over the original algorithm can be seen more clearly, since our approach obtains better results in most situations.


                        Fig. 11
                         shows the performance in terms of F-measure of the Agrawal, Horprasert and Reddy methods compared with our modified versions of the rest of methods. All plots show poor results for these three methods and only in some frames they are able to outperform our proposal.

In order to get a more comprehensible view of the differences it is better to use Table 3
                        , which displays the mean values corresponding to Figs. 10 and 11. If we consider F-measure our proposal overcomes the original versions in most cases, only original GMMV1 is able to get better results compared to its modified version in LB, C1 and BK. We have to mention that original WrenGA and GrimsonGMM also get better results in RC and BK, but the difference with our proposal is smaller.

Complementary to F-measure, the performance using the accuracy measure is also analyzed in Table 3. It can be observed that our proposal improves the original algorithm even more than with F-measure. There are only three videos where there is a method that our algorithm cannot improve the original version: LB, RC and BK. Again GMMV1 is the clearest example of this, obtaining better results in LB and BK. WrenGA also overcomes our proposal in RC, but the difference is small.

It is interesting to note that Reddy gets better results than our proposal in two sequences according to accuracy. However this is not the case for the performance in terms of F-measure. Furthermore, Reddy is the only method that is able to get a higher score on accuracy than on F-measure. This behavior occurs because accuracy benefits algorithms with few FN, being less sensitive to FP than F-measure, which penalizes both situations [25]. This is the case of Reddy as we see in Fig. 4, where most configurations get more FP than FN.

A statistical significance study has been carried out for all the quantitative performance measures. The nonparametric Friedman test with the corresponding post-hoc Dunn test have been used to determine whether the difference of the best performing approach with respect to the second best performing one is statistically significant. These tests are robust for comparisons over multiple datasets [4]. A 95% confidence level has been chosen in all cases. For each sequence, a configuration marked with * means that it is better than the second best configuration with a 95% confidence level. Our proposal achieves significantly better results in four of the eight sequences in terms of F-measure and in three sequences in the case of accuracy. None of the original algorithms is significantly better than the modified version, while Reddy is significantly better in two sequences and according to accuracy only.

Finally, Table 4
                         shows the FPS performance of each algorithm for all sequences. Since real-time requirements are satisfied when it exceeds the threshold of 15fps, it is observed that all algorithms verify this condition except Agrawal and Reddy. As we see, GrimsonGMM and FASOM are the methods improved with our algorithm with the worst results regarding to computational time. On the other hand the modified version of GMMV2 is the fastest modified method and, as we have previously shown, it is one of the best performing options. Our proposal has a higher computational load, but it must be taken into account that we do not use hardware acceleration, so it is possible to improve this feature.

@&#DISCUSSION@&#

In this section several key features of our proposal are discussed:
                        
                           •
                           The main difference of our approach with respect to other algorithms for illumination change detection is that there is no specific form of the color change to be expected when an illumination change takes place. This means that our proposal does not depend on the physical properties of the lights that are present in the environment. Therefore, it can be applied to a wide range of situations where the lighting conditions vary in an unpredictable way. It must be highlighted that most current approaches assume some model of pixel color variation under illumination changes either explicitly or implicitly [7,11,31,29]. A particularly interesting case is the approach in [32], which only assumes that the sign of the difference between two pixel luminance measurements is maintained across illumination changes in a local neighborhood of the frame. However, this assumption does not hold in practice for textured regions, where some pixels can increase their luminance while others decrease. In contrast to this, our approach handles textured regions correctly because it checks whether pixels in a local neighborhood that had a similar color before the change continue to have a similar color after the change, so that pixels corresponding to different materials of the local texture are never compared unlike [32].

As reported in the experiments, the approaches in [1,9,21] are outperformed by our proposal. This is because these methods consider an excessively restrictive model of illumination changes. In particular, a linear transformation in the RGB values is considered in [9], where the observed color is projected onto a line in the RGB color cube which is assumed to model all possible illumination states of the pixel. This is unrealistic, since the response of the materials with respect to light may not be linear, in particular if the color content of the incident light is different for the three RGB channels. The illumination change management in [21] considers that the angle subtended between the observed color and the estimated mean background color is small under varying illumination. Again this amounts to assuming that the response of the materials to light is similar for the three RGB channels, which is not realistic for the reasons outlined before. On the other hand, the approach in [1] focuses on detecting foreground objects as those with edges which are not present in the background model. This strategy will fail on homogeneous regions or noisy videos where the local edge information is not reliable. Also, the different response of the materials with respect to light means that local edges may have a different appearance when illumination changes.

Given the previous considerations, a qualitative assessment of the challenges faced by the competing illumination change detection algorithms is carried out next. As we see in Fig. 12
                               Agrawal performs well in situations with clearly defined edges (first row), but fails to segment those frames with flat regions or camouflage effects (second row). Please note that these failures are due to unreliable edge information, as explained before. Horprasert is highly dependent on the training phase because the initial model is not updated. The first row in Fig. 13
                               shows a situation where this method correctly segments the image, but the second row shows that it fails to adapt to lighting changes that occur on the left side of the image, which exemplifies the limitations of its illumination change model. One of the parameters needed to configure Reddy is the block advancement. High values yield poorly defined edges on foreground objects and therefore high values of FP, see the last image of Fig. 8 which corresponds to a block advancement of 8. If we use a lower value (see first row of Fig. 14
                               or the last image of Fig. 5), the problem is partially corrected and the edges are smoother, but the computational performance drops down dramatically in terms of FPS. Another common problem is that Reddy does not perform well with variations in ambient light due to its poor illumination change model, see the second row of Fig. 14.

The decision that a pixel has undergone an illumination change is not driven by binary variables (yes/no), but by fuzzy variables which are able to manage the inherent uncertainty in the determination of the illumination state of a pixel. This way the proposal is able to use as much information as possible from the evaluation of the lighting conditions. The validity of fuzzy approaches to foreground detection under varying lighting conditions has been demonstrated in literature [33,24,5].

A procedure to combine the illumination state information from nearby pixels is established. This is of paramount importance, since at the pixel level it is not possible to distinguish a sudden illumination change from an incoming object with a homogeneous color. This situation, which is common to all pixel level approaches [23,17,15], is modeled in our proposal by means of the ‘dubious’ pixel state (Table 1).

The experimental results show that our procedure is able to enhance the performance of several state of the art foreground detection methods, so that the difference is statistically significant with a high confidence level. It is also observed that the main part of the enhancement comes from the removal of false positives, which means that illumination changes are no longer mistaken as foreground objects as expected from this kind of algorithms [29,19,16,3,2].

@&#CONCLUSIONS@&#

We have presented a new approach to illumination change detection. Our proposal is designed to be attached to an existing foreground detection algorithm. It is based on the study of the local color transformation which the pixels undergo when a lighting change occurs. One of the advantages is that we only need to tune one parameter and, using a few different values, good results can be obtained. Therefore it is easily usable and does not require a vast expertise.

In order to demonstrate its performance, we have considered a set of eight real sequences with different complex environments, representing indoors and outdoors situations. All of them can be found in public repositories. Five state-of-the-art methods have been modified with our proposal. This amounts to a total of 40 benchmarks, and our proposal overcomes the original algorithm in 35 of them, while it achieves competitive results in the other five. There are also three background subtraction and shadow detection methods that we have tested; they achieve poor results compared to our proposal.

From the above, it can inferred that our proposal increases the performance of existing background segmentation methods in a wide range of illumination change conditions, which demonstrates its potential to enhance computer vision systems which include a foreground detection subsystem.

@&#ACKNOWLEDGMENTS@&#

This work is partially supported by the Ministry of Economy and Competitiveness of Spain under grant TIN2011-24141, project name Detection of anomalous activities in video sequences by self-organizing neural systems. It is also partially supported by the Autonomous Government of Andalusia (Spain) under projects TIC-6213, project name Development of Self-Organizing Neural Networks for Information Technologies; and TIC-657, project name Self-organizing systems and robust estimators for video surveillance. All of them include funds from the European Regional Development Fund (ERDF). The authors thankfully acknowledge the computer resources, technical expertise and assistance provided by the SCBI (Supercomputing and Bioinformatics) center of the University of Málaga.

@&#REFERENCES@&#

