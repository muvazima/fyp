@&#MAIN-TITLE@&#Prediction of hot regions in protein–protein interaction by combining density-based incremental clustering with feature-based classification

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Proposed method significantly improves the prediction performance for hot regions.


                        
                        
                           
                           We combine density-based incremental clustering with feature-based classification.


                        
                        
                           
                           Feature selection is used to get the best features for classification.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Hot region

Clustering

Feature-based classification

Protein–protein interaction

Density-based

@&#ABSTRACT@&#


               
               
                  Discovering hot regions in protein–protein interaction is important for drug and protein design, while experimental identification of hot regions is a time-consuming and labor-intensive effort; thus, the development of predictive models can be very helpful. In hot region prediction research, some models are based on structure information, and others are based on a protein interaction network. However, the prediction accuracy of these methods can still be improved. In this paper, a new method is proposed for hot region prediction, which combines density-based incremental clustering with feature-based classification. The method uses density-based incremental clustering to obtain rough hot regions, and uses feature-based classification to remove the non-hot spot residues from the rough hot regions. Experimental results show that the proposed method significantly improves the prediction performance of hot regions.
               
            

@&#INTRODUCTION@&#

Protein functions can be expressed by protein–protein interactions which are very useful to understand the origination of diseases, but the principles that govern the interaction of two proteins and the general properties of their interaction interfaces remain unknown, resulting in difficulties when predicting interface regions. Hot spots [1–6] of protein–protein interactions play important roles in the functions and stability of protein complexes. Instead of being distributed along the protein interfaces homogeneously, hot spot residues are clustered within tightly packed regions [3,5,7,8], which are called hot regions. These are more important than hot spots in maintaining the stability of protein complexes and exerting the molecular mechanism of biological functions.

In the past, many attempts have been made to predict hot regions. The research group [3,6,8–14] in Koc University, Turkey, made contributions to the prediction of hot regions. Keskin developed an algorithm [3] to cluster hot spots into hot regions after studying the organization and contribution of structurally conserved hot spot residues. Tuncbag proposed a method [13] which combined the conservation of residues, accessible surface area and pair potential for prediction of hot regions. In [12,14] they predicted hot regions by the rule in [3] and the method of predicting hot spots in [8], then built a database called Hot Region [11]. But this method requires the structure of the protein, and is therefore limited by the available protein structures. In 2007, Hsu [15] presented a pattern-mining approach for the identification of hot regions in protein–protein interactions. The proposed method aimed to demonstrate that the important residues associated with the interface of protein–protein interactions may be discovered by sequential pattern-mining automatically. In [16], Pons studied a network-based method and used small-world residue networks to predict protein-binding areas. Although the proposed method has potential applications for protein docking as a complement to energy-based approaches, it shows limitations in many cases with certain topological features, like spherical or very large proteins. In [17], Nan proposed a method to predict hot regions based on complex network and community detection. By revising false positive and false negative during the detection process, the proposed method can improve the reliability in the recognition of hot regions. However, the prediction accuracy needs further improvement.

In this paper, we propose a method called Density-based Incremental Clustering with Feature-based Classification (DICFC), which can predict hot regions in protein–protein interactions by combining density-based incremental clustering with feature-based classification. DICFC first forms the primary clusters by applying the density-based incremental clustering method to remove outliers, and then forms final hot regions, where a feature-based classification method is presented to remove the non-hot spot residues in the clustering results. In order to get the best features for classification, a feature selection method is studied. Experimental results show that the proposed method significantly improves the prediction performance for hot regions.

@&#METHOD@&#

In the proposed method, firstly, standard hot regions can be constructed for comparison using hot spots with the experimental data from the alanine mutation energy database [27]; then we will make some hot region predictions of both the hot spots and non-hot spots using the proposed method which combines density-based incremental clustering and feature-based classification; finally the prediction accuracy will be compared to the standard hot regions constructed above, from which the superiority of the proposed method can be drawn.

In this paper, we adopted the standard definition of hot regions from Ozlem Keskin [3]. A hot region is defined as follows: every hot region contains at least three hot spots, and each hot spot is assumed to be within a hot region if it has at least two hot spot neighbors, and each hot spot residue is assumed to be a perfect sphere with a specific volume. The Cα-atoms of the hot spot residues are the centers of these spheres. The radii of the spheres are extracted from their sphere volumes. If the distance between the centers of two spheres (two Cα-atoms of two hot spots) is less than the sum of the radii of the two spheres plus a tolerance distance (2Å), the two hot spot residues are flagged to be clustered and to form a network in the hot region.

The coordinates of a Cα atom are obtained from the Protein Data Bank (PDB) [18], and the volume of a hot spot is as described in Appendix 1.

Based on the above definitions, the 65 hot spots in the data set (see Table 6 of Section 3.1) are organized into 10 hot regions, which contain the 49 hot spots shown in 
                        Table 1. Eight complexes out of 16 (see Table 7) have formed hot regions while the other eight complexes are excluded. The hot spots outside the hot regions are unable to form standard hot regions since they are not physically close enough to other hot spots. 
                        Table 2 lists all the hot spots of the eight complexes in standard hot regions and the hot spots outside standard hot regions are signified in bold.

Similar to density-based clusters, hot spot residues are packed tightly within local regions rather than distributed along the protein interfaces homogeneously. Thus the hot spot residues can be clustered using some clustering methods. Clustering is a process to group data into multiple sub-groups or clusters so that objects within a cluster may have strong similarities [19]. The density of a residue O in the space can be measured by the number of residues close to it. Thus clustering is used to find the core residues, which are defined as the residues that have dense neighborhoods [19]. The proposed algorithm connects core residues and their neighborhoods to form dense regions as clusters. In order to use the clustering method to cluster the hot spot residues, we need to adopt several concepts from [19] (all distances in this paper are Euclidean distance):
                           
                              •
                              
                                 Neighborhood: a user-specified parameter ε>0 is used to specify the radius of a neighborhood for every residue. The ε-neighborhood of a residue O is the space within radius ε centered at O.


                                 Density of neighborhood: due to the neighborhood size determined by ε-neighborhood, the density of any neighborhood can be measured simply by the number of residues in the corresponding neighborhood.


                                 Dense region: to determine whether a neighborhood is dense or not, another user-specified parameter “Min” is used to specify the density threshold of dense regions. “Min” is a variable that can be specified by the user.


                                 Core residue: a residue is a core residue if the ε-neighborhood of that residue contains at least “Min” residues.

For a dataset D composed of residues, we will identify all core residues in it with respect to the given parameters “ε” and “Min” by checking the number of residues in the neighborhood of a residue. Thus, the clustering task is reduced to using core residues and their neighborhoods to form dense regions, which are the clusters we need.

The process of density-based incremental clustering is described as follows: Initially, all residues in D are marked as “unvisited”. Then an unvisited residue p is selected randomly and marked as “visited”. After that, the ε-neighborhood of p is checked to see whether it contains at least “Min” residues. If not, p is marked as a noise point.

Otherwise, a new cluster C is created for p, and all the residues in the ε-neighborhood of p are added to a candidate set, N. The algorithm iteratively adds to C those residues in N that do not belong to any cluster. In this process, for a residue p in set N that is labeled “unvisited”, we mark it as “visited” and check its ε-neighborhood. If the ε-neighborhood of p׳ has at least “Min” residues, those residues in the ε-neighborhood of p׳ are added to N. Clustering continues adding residues to C until C can no longer be expanded, that is, until N is empty. At this time, cluster C is completed, and then we randomly select an unvisited residue from the remaining ones. The clustering process continues until all residues in the set D are visited.

After clustering, several clusters are generated, which contain both hot spots and non-hot spots. However, the hot region is just composed of hot spots [3], thus the non-hot spots should be removed from the obtained clusters. In this paper, a feature-based classification method [20] is introduced to identify the non-hot spots, which can then be removed from the density-based incremental clusters

SVM has been widely applied in the field of bioinformatics. Based on the research of Xia et al. [20], SVM, Bayes NET, Naive Bayes, RBF Network, decision tree and decision table are all involved in hot-spots classification but SVM outperforms the others, as can also be validated by the results from many other studies [20,21]. In this paper, an extended version of SVM called LIBSVM [22] is adopted. So the clustered residues are applied for classification using LIBSVM, in which Leave-one-out cross-validation is followed. First we divide all the clustered residues into 10 subsets, and then we randomly select a subset as the test set, and the other 9 subsets as the training set. After repeating the process ten times, the parameters c and g can be optimized.

For classification, feature selection is essential. There are many physical and chemical features for protein complexes and we cannot use all of them because some redundant and irrelevant features need to be removed to improve the classification performance. In our work, in order to explore the optimal features, first all the features by SVM-RFE with NMIFS filter (SRN) [23] are sorted and then the F-score is introduced to find the optimal combination of those features.

When sorting the features, the SRN [23] feature selection is a combination of SVM-RFE [24] and NMIFS [25]. Features are selected by embedding a SVM classifier, where normalized mutual information is used to balance the relevance between a feature and class label and the redundancy among different features.

In a linear SVM, the final classification function for a pattern 
                           x
                           ∈
                           
                              
                                 R
                              
                              D
                           
                         is 
                           f
                           
                              (
                              x
                              )
                           
                           =
                           
                              
                                 ∑
                              
                              
                                 i
                                 =
                                 1
                              
                              
                                 D
                              
                           
                           
                              
                                 w
                              
                              
                                 i
                              
                           
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           +
                           b
                        , and the component 
                           
                              
                                 w
                              
                              
                                 i
                              
                           
                         in the weight vector 
                           w
                           ∈
                           
                              
                                 R
                              
                              D
                           
                         is used to measure the effectiveness of the i-th feature on the final classifier.

In the SRN method, the rank of the i-th feature is calculated as follows [23]:
                           
                              (1)
                              
                                 
                                    
                                       r
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 β
                                 
                                    |
                                    
                                       
                                          
                                             w
                                          
                                          
                                             i
                                          
                                       
                                    
                                    |
                                 
                                 +
                                 
                                    (
                                    
                                       1
                                       −
                                       β
                                    
                                    )
                                 
                                 
                                    [
                                    
                                       
                                          
                                             R
                                          
                                          
                                             s
                                             ,
                                             
                                                
                                                   f
                                                
                                                
                                                   i
                                                
                                             
                                          
                                       
                                       −
                                       
                                          
                                             Q
                                          
                                          
                                             s
                                             ,
                                             
                                                
                                                   f
                                                
                                                
                                                   i
                                                
                                             
                                          
                                       
                                    
                                    ]
                                 
                                 =
                                 β
                                 
                                    |
                                    
                                       
                                          
                                             w
                                          
                                          
                                             i
                                          
                                       
                                    
                                    |
                                 
                                 +
                                 
                                    (
                                    
                                       1
                                       −
                                       β
                                    
                                    )
                                 
                                 [
                                 I
                                 
                                    (
                                    
                                       c
                                       ;
                                       
                                          
                                             f
                                          
                                          
                                             i
                                          
                                       
                                    
                                    )
                                 
                                 −
                                 
                                    1
                                    
                                       |
                                       s
                                       |
                                    
                                 
                                 
                                    ∑
                                    
                                       f
                                       ∈
                                       s
                                    
                                 
                                 N
                                 I
                                 (
                                 
                                    
                                       f
                                    
                                    
                                       i
                                    
                                 
                                 ;
                                 
                                    
                                       f
                                    
                                    
                                       s
                                    
                                 
                                 )
                                 ]
                              
                           
                        where the parameter 
                           β
                           ∈
                           
                              |
                              
                                 0
                                 ,
                                 1
                              
                              |
                           
                         is a user-tuned parameter, 
                           I
                           (
                           c
                           ;
                           
                              
                                 f
                              
                              
                                 i
                              
                           
                           )
                         is the relevance of feature 
                           
                              
                                 f
                              
                              
                                 i
                              
                           
                         and the class label 
                           c
                           ∈
                           C
                           =
                           
                              {
                              
                                 +
                                 1
                                 ,
                                 −
                                 1
                              
                              }
                           
                        , 
                           N
                           I
                           
                              (
                              
                                 
                                    
                                       f
                                    
                                    
                                       i
                                       ;
                                    
                                 
                                 
                                    
                                       f
                                    
                                    
                                       s
                                    
                                 
                              
                              )
                           
                         is the normalized mutual information between features 
                           
                              
                                 f
                              
                              
                                 i
                              
                           
                         and 
                           
                              
                                 f
                              
                              
                                 j
                              
                           
                        , where 
                           |
                           S
                           |
                         is the number of features in the currently selected feature set 
                           S
                        . Eq. (1) is used to rank features by combining the weight in the SVM classifier, relevance and redundancy.

The SRN works in an iterative way. The method uses a backward feature elimination strategy to obtain the important features and remove insignificant features (i.e., the feature having the smallest impact on the object function is excluded) at each step. It starts with all the features selected, then at each step the features are ranked with the above rank function, and those with the smallest rank value will be deleted. Finally, the rest will be features that are expected.

We apply the SRN [23] to select 19 physical and chemistry features [20,21,26], which are displayed in Table 3.

Then, features can be selected using the F-score [20], which assesses the discriminatory power of each individual feature. The larger the F-score is, the more discriminative ability it has for the feature. The F-score was calculated as
                           
                              (2)
                              
                                 F
                                 -
                                 s
                                 c
                                 o
                                 r
                                 e
                                 =
                                 
                                    
                                       x
                                       n
                                       i
                                       −
                                       x
                                       h
                                       i
                                    
                                    
                                       σ
                                       n
                                       i
                                       +
                                       σ
                                       h
                                       i
                                    
                                 
                              
                           
                        where x
                        
                           ni
                         and x
                        
                           hi
                         denote the averages of both non-hot spots and hot spots, and σ
                        
                           ni
                         and σ
                        
                           hi
                         are the corresponding standard deviations. The F-score can measure the separation of the means for the two populations (hot spots and non-hot spots) in terms of their variances, which is closely related to the F-statistics and commonly used to evaluate the separation of the means for two random variables.

To discover the highest F-score combination, we list the 19 features in descending order in 
                        
                        Table 4. First we combine the feature ranked the top with its next one to obtain the F-score; the same process is continued by grouping the last combined features with its next one until all the features are combined. Finally, the corresponding F-scores can be calculated, as shown in 
                        Table 5. It is obvious that the F-score of “combination 1–5” is the highest, and these five features are RctASA, RcsASA, BsASA, UsASA and RctmPI.

The proposed algorithm is a hybrid algorithm which combines density-based incremental clustering with feature-based classification (DICFC) for hot region prediction. First, the algorithm visits all the residues in the data set D to find the core residues set C, and then LIBSVM is used to remove the non-hot residues from set C to obtain predicted hot regions. The algorithm can be described as follows:


                        Input:
                        
                           
                              ●
                              D: a data set containing 155 residues and their three-dimensional coordinates,


                                 ε: the radius parameter,

Min: the neighborhood density threshold.


                        Output: predicted hot regions from DICFC.

Method:
                           
                              
                                 
                                 
                                    
                                       (1)mark all residues as unvisited;
                                    
                                    
                                       (2)do
                                    
                                    
                                       (3)
                                          randomly select an unvisited object p;
                                    
                                    
                                       (4)
                                          mark p as visited;
                                    
                                    
                                       (5)
                                          if the ε-neighborhood of p has at least “Min” residues
                                    
                                    
                                       (6)
                                          
                                          create a new cluster C, and add p to C;
                                    
                                    
                                       (7)
                                          
                                          let N be the set of residues in the ε-neighborhood of p;
                                    
                                    
                                       (8)
                                          
                                          for each point p׳ in N
                                    
                                    
                                       (9)
                                          
                                          
                                          
                                          if p׳ is unvisited
                                    
                                    
                                       (10)
                                          
                                          
                                          
                                          
                                          mark p׳ as visited;
                                    
                                    
                                       (11)
                                          
                                          
                                          
                                          if the ε-neighborhood of p׳ has at least “Min” residues, add those residues to N;
                                    
                                    
                                       (12)
                                          
                                          
                                          
                                          if p׳ is not yet a member of any cluster, add p׳ to C;
                                    
                                    
                                       (13)
                                          
                                          end for
                                    
                                    
                                       (14)
                                          output clusters C;
                                    
                                    
                                       (15)else mark p as outlier;
                                    
                                    
                                       (16)until no residue is unvisited;
                                    
                                    
                                       (17)run PSAIA with all residues in clusters C;
                                    
                                    
                                       (18)calculate RctASA, RcsASA, BsASA, UsASA and RctmPI of residues in clusters C;
                                    
                                    
                                       (19)run LIBSVM with the above five structure feature values;
                                    
                                    
                                       (20)mark h as hot spot residue, n as non-hot residue;
                                    
                                    
                                       (21)remove residues of marked n in clusters C,
                                    
                                    
                                       (22)mark formed new cluster as hot region R;
                                    
                                    
                                       (23)output hot region R.
                                    
                                 
                              
                           
                        
                     


                        Phase I: Clustering
                     

(1)–(6): Create the core residues set C.

(7)–(13): Visit residues in the ε-neighborhood of set C, and add all residues that match the condition into set C.

(14)–(16): The clustering process continues until all residues in set D are visited, then output set C and mark all residues outside set C as outliers.


                        Phase II: Classification
                     

(17)–(18): Calculate the five structure feature values of the residues in C using PSAIA.

(19)–(21): Classify all residues in set C using LIBSVM to remove non-hot spot residues.

(22)–(23): Mark newly formed clusters from (21) and output as hot regions.

In the experiments, we used the 16 protein complexes listed in Table 7. These complexes are from the database called ASEdb [27]. In the experiments, each complex is composed of a bunch of interface residues. These interface residues [3] are the residues that the decrement of the accessible surface area are more than 1Å during the process of forming the complex. The energy values binding for each residue were obtained by alanine mutation experiments.

We removed the protein chains which are repetitive using the CATH [28] query system with the sequence identity less than 35% and the SSAP [29] score less than or equal to 80. Moreover, we also removed the protein chains that do not interact with each other from the original data set. After that, we had 255 interface residues. An interface residue [3,20] will be defined as a hot spot if its corresponding binding free energy is higher than or equal to 2.0kcal/mol, while an interface residue with binding free energy less than or equal to 0.4kcal/mol is considered as a non-hot spot, according to which there are 65 hot spots and 90 non-hot spots in the dataset. The other 100 unlabeled residues with binding free energy between 0.4kcal/mol and 2.0kcal/mol are excluded from the training set because their hot spot or non-hot spot features are not very clear [20]. 
                        
                        Table 6 summarizes the interface residue labeling for each complex, and illustrates the number of interface residues, hot spots, non-hot spots, and other unlabeled residues separately. In the experiments, we got 155 interface residues as the training data set, whose structure features are obtained by PSAIA [30].

@&#EXPERIMENTAL RESULTS@&#

We first conducted the clustering experiments, where 3-D coordinates of the residues were used for clustering. In the experiments, we used the coordinates of the Cαatom of a protein residue to represent the residue׳s coordinates. All the values of coordinates of the residues were from the protein data bank (PDB) [18]. In the clustering experiments, the density threshold “Min” was set to three when performing the density-based incremental clustering because every hot region contains at least three hot spots [3]. In order to determine the value of the neighborhood radius ε, we studied the relationship between neighborhood radius ε and the number of clusters. The relationship is shown in 
                        Fig. 1, from which we can find that different ε values will result in different cluster numbers. When ε=9, the cluster number reaches the maximum value, which is 13. When ε≥61, the number of clusters is 1. And the experiment result indicates that when ε<9 or ε>9, the hot spot residues are not enough to form any hot region. Thus we set ε=9.


                        
                        Table 8 provides the clustering results in each complex when ε=9 and Min=3. 
                        Fig. 2 shows the residues highlighted on the structure and their IDs are stated in Table 8. After clustering, there are 104 residues left when 51 outliers from the dense region are eliminated. There are 13 clusters in 10 proteins of the 16 complexes, of which there are 3 clusters in 3HFM, 2 clusters in 1A22, and 1 cluster each in 1BRS, 1BXI, 1CBW, 1DAN, 1DVF, 1F47, 1GC1 and 1JRH, but no clusters in 1A4Y, 1AHW, 1FC2, 1FCC, 1VFB and 2PTC. 
                        Table 9 shows the final results after using feature-based classification to remove non-hot spot residues from the clusters obtained in Table 8. 
                        Fig. 3 shows the residues highlighted on the structure and their IDs are stated in Table 9. There are 9 hot regions compounded from 43 residues. The numbers of the hot regions in each protein complex are given in 
                        Table 10.

@&#EVALUATION@&#

In order to evaluate the performance of the proposed method, three criteria are used for predicting both hot spots and hot regions [17,20]:
                           
                              (8)
                              
                                 R
                                 e
                                 c
                                 a
                                 l
                                 l
                                 =
                                 
                                    
                                       TP
                                    
                                    
                                       TP
                                       +
                                       FN
                                    
                                 
                              
                           
                        
                        
                           
                              (9)
                              
                                 P
                                 r
                                 e
                                 c
                                 i
                                 s
                                 i
                                 o
                                 n
                                 =
                                 
                                    
                                       TP
                                    
                                    
                                       TP
                                       +
                                       FP
                                    
                                 
                              
                           
                        
                        
                           
                              (10)
                              
                                 F
                                 -
                                 m
                                 e
                                 a
                                 s
                                 u
                                 r
                                 e
                                 =
                                 
                                    
                                       2
                                       ⁎
                                       R
                                       e
                                       c
                                       a
                                       l
                                       l
                                       ⁎
                                       Pr
                                       e
                                       c
                                       i
                                       s
                                       i
                                       o
                                       n
                                    
                                    
                                       R
                                       e
                                       c
                                       a
                                       l
                                       l
                                       +
                                       Pr
                                       e
                                       c
                                       i
                                       s
                                       i
                                       o
                                       n
                                    
                                 
                              
                           
                        
                     

When predicting hot spots, the following notations are used:

True Positive (TP): the number of hot spots in predicted hot regions and also in standard hot regions.

False Negative (FN): the number of hot spots that are not in predicted hot regions but in standard hot regions.

False Positive (FP): the number of hot spots in predicted hot regions but not in standard hot regions.

Precision represents the accuracy of the hot spot prediction, and Recall represents the coverage of predicted hot spots in standard hot regions. With a good balance between Precision and Recall, the F-measure offers a better overall accuracy of hot spot prediction.

However, for prediction of hot regions, the above notations assume different meanings, as follows:

True Positive (TP): the number of hot regions in predicted hot regions and also in standard hot regions.

False Negative (FN): The number of hot regions that are not in predicted hot regions but in standard hot regions.

False Positive (FP): The number of hot regions in predicted hot regions but not in standard hot regions.

Similarly, Precision represents the accuracy of the hot region prediction, and Recall represents the coverage of predicted hot regions in standard hot regions. With a good balance between Precision and Recall, the F-measure offers a better overall accuracy in predicting hot regions than solely using either Precision or Recall.

In this study, we compared the proposed method with the previous methods including those of Tuncbag [14] and Nan [17].


                           
                           Table 11 summarizes the performance of the different methods on the same data set. Among these approaches, Tuncbag׳s [13,14] method is physicochemical and structure feature-based, while Nan׳s method [17] is based on complex network and community detection. Regarding the number of hot regions, DICFC is able to correctly predict 7 (recall=0.700) hot regions from 10 standard hot regions while Nan and Tuncbag only predict 4 (recall=0.400) and 2 (recall=0.200). On the hot spot coverage of predicted hot regions in standard ones, the hot spot recall for DICFC, Nan׳s method, and Tuncbag׳s method are 0.571, 0.285 and 0.122, respectively. Therefore, the proposed method predicts the largest proportion of hot regions, which reaches 57.1%, while the other two methods achieve no more than 30%. DICFC is able to predict hot regions correctly from the data set with 0.651 and 0.571. This means that DICFC is able to correctly predict 57.1% of the true hot regions in this data set, and 65.1% of the predicted hot regions are identified as true hot regions. By contrast, Tuncbag׳s method has high accuracy, but the coverage of predicted hot regions in standard hot regions is very low, which means that Tuncbag׳s method is able to predict only 12.2% of hot regions; although the predicted hot regions are correct, the accuracy is too low. The prediction performance of Nan׳s method is located between that of our method and that of Tuncbag׳s. In addition, the comprehensive score F-Measure of DICFC is 0.608, which is higher than Nan׳s and Tuncbag׳s. From the above comparison, we can conclude that DICFC provides a remarkably better prediction performance than the previous prediction methods.


                           
                           Fig. 4 describes the coverage status of predicted hot regions in standard ones. Our method is able to predict 7 hot regions from 10 standard hot regions, while Nan is able to predict 4 (Appendix 2) and Tuncbag is able to predict 2 (Appendix 3).

For a more comprehensive study, the results of applying hot spot prediction before the density clustering were also conducted as a comparison with the current one. The prediction results using the same data set are shown in Appendix 4, where the recall, precision and F-measure of hot spots are 0.408, 0.645 and 0.5, while the recall, precision and F-measure of hot regions are 0.6, 0.857 and 0.706. According to these results, applying density clustering before hot spot prediction (DICFC in this paper) gives better prediction performance than applying hot spot prediction before density clustering. Considering that these two methods are of the same type, we select the one with the better performance as the proposed method.

In order to show the effectiveness of the proposed method, we provided a visualization of the prediction results obtained by the different methods. The visualization tool we adopted is the molecular visualization software Pymol [31]. 
                           Fig. 5 shows the visualization results for 8 complexes.

To further illustrate the effectiveness of the proposed approach (DICFC) for predicting hot regions, we randomly selected two examples for detailed analysis. The two complexes are 1BRS and 1JRH.

The first example is the interaction between chain A and chain D of complex 1BRS, which is shown in 
                           Fig. 6. In this complex, there are seven hot spot residues in the standard hot regions and they are (A GLU 73), (A ARG 87), (A HIS 102), (D TYR 29), (D ASP 35), (A ARG 59) and (D ASP 39). DICFC is able to correctly predict six of these seven hot spots, which are (A HIS 102), (A ARG 87), (D TYR 29), (D ASP 35), (D ASP 39) and (A ARG 59). Tuncbag׳s method is only able to predict three of the seven hot spots correctly, i.e. (A HIS 102), (D ASP 35) and (D ASP 39). Nan׳s method failed to predict any residues in this hot region. However, hot spots (A GLU 73) and (D TYR 29) cannot be predicted correctly by any of these three methods, which suggests that the mutations of these two residues might contribute to protein destabilization.

The second example is the interaction between chain H, chain I and chain D of complex 1JRH, which is shown in Fig. 6. There are eight hot spot residues in the standard hot regions, which are (L TRP 92), (I TYR 49), (I LYS 52), (I ASN 53), (I LYS 47), (I TRP 82), (H TRP 52) and (H TRP 53). DICFC is able to correctly predict six of these eight hot spots, which are (L TRP 92), (I TYR 49), (I ASN 53), (I LYS 52), (I LYS 47) and (H TRP 52). The unpredicted residues are (I TRP 82) and (H TRP 53), and the incorrectly predicted hot spot is (I LYS 98). The five residues (L TRP 92), (I LYS 47), (I ASN 53), (I TYR 49) and (I LYS 52) are correctly predicted by both DICFC and Nan’ methods, which means that the region formed by these five residues plays an important role in the function and the stability of protein complexes. Tuncbag was unable to predict any of the residues in this hot region. Residue (H TRP 53) is not predicted by any of the three methods, suggesting that the mutations of this residue might contribute to protein destabilization.

These prediction results demonstrated that DICFC is able to predict more hot regions than the others in all the hot regions formed by the 16 protein complexes. Moreover, DICFC predicts more hot spots in a single hot region of a single protein complex.

@&#DISCUSSION@&#

A deeper understanding of the functions of proteins and the interactions between proteins is the key to designing drugs and proteins. And discovering hot regions in protein–protein interaction is important for understanding the interactions between proteins. Because of the complexity of experimental methods, the computational prediction method is greatly improving the efficiency and accuracy of experimental identification of hot regions. The major contribution of this study is to propose a new model combining clustering with classification that has several advantages as it significantly improves the hot region prediction performance; firstly it imports density-based incremental clustering into hot region prediction; it simplifies the hot region prediction requirements from specific shape and detailed structure information to the residue coordinates only; the SRN feature selection combined with SVM-RFE and NMIFS is applied to efficiently find the best features for classification.

Meanwhile, the limitation of DICFC is that the clustering results probably depend on the choice of the values of the neighborhood density threshold and neighborhood radius. In the future, we will conduct research on better principles or criteria for choosing these two parameters, and we will also continue to collect more experimental and published data for wider tests in order to optimize our prediction model in the future.

Although the proposed model in this paper is density-based incremental clustering combined with feature-based SVM, a better cluster method and novel feature selection methods can easily be incorporated to improve the hot regions prediction performance.

@&#CONCLUSIONS@&#

In this paper, we propose a method called DICFC, which predicts hot regions by combining density-based incremental clustering with feature-based classification. The first step uses the density-based incremental clustering method to obtain rough hot regions and the second step uses feature-based classification to remove the non-hot spot residues from the clusters obtained by the first step. By removing outliers when clustering, the experimental results show that the proposed method significantly improves the performance of hot regions prediction.

Feature-based classification, as an important and efficient way to predict hot spots, may not be the most efficient method to use for hot region prediction. Our research shows that we have already completed most of the predictions by discovering a large part of the hot region clusters using the density-based incremental cluster before feature classification. Although we go on to use feature classification to further improve the prediction accuracy, compared with feature classification, the density-based incremental clustering plays a more important role in hot region prediction. Importing density-based incremental clustering will greatly improve the prediction accuracy of hot regions.

We declare that we have no financial and personal relationships with other people or organizations that could inappropriately influence our work, there is no professional or other personal interest of any nature or kind in any product, service and/or company that could be construed as influencing the position presented in, or the review of, the manuscript entitled “Prediction of hot regions in protein–protein interaction by combining density-based incremental clustering with feature-based classification”.

@&#ACKNOWLEDGMENT@&#

This work is supported by the National Natural Science Foundation of China (Nos. 61273225 and 61201423), the Young Science and Technology Outstanding Training Plan of Wuhan University of Science and Technology (No. 2013xz014). Thanks to Dongfang Nan, Juhong Qi and Jing Ye in our lab, and two college students Maosen Chen and Qi Mo for their meaningful discussion.

Supplementary data associated with this article can be found in the online version at doi:10.1016/j.compbiomed.2015.03.022.


                     
                        
                           
                              Supplementary Material
                           
                           
                        
                     
                  

@&#REFERENCES@&#

