@&#MAIN-TITLE@&#Automatic negation detection in narrative pathology reports

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Three methods were proposed to detect negation in narrative pathology reports.


                        
                        
                           
                           A machine learning-based approach shows potential advantages.


                        
                        
                           
                           A lexicon-based approach benefits most by customizations to the corpus.


                        
                        
                           
                           Negation rules and patterns were designed in a syntax-based approach.


                        
                        
                           
                           Different approaches for each section may improve the overall performance.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Negation detection

Lexicon-based approach

Syntax-based approach

Machine-learning-based approach

Pathology reports

@&#ABSTRACT@&#


               
               
                  Objective
                  To detect negations of medical entities in free-text pathology reports with different approaches, and evaluate their performances.
               
               
                  Methods and material
                  Three different approaches were applied for negation detection: the lexicon-based approach was a rule-based method, relying on trigger terms and termination clues; the syntax-based approach was also a rule-based method, where the rules and negation patterns were designed using the dependency output from the Stanford parser; the machine-learning-based approach used a support vector machine as a classifier to build models with a number of features. A total of 284 English pathology reports of lymphoma were used for the study.
               
               
                  Results
                  The machine-learning-based approach had the best overall performance on the test set with micro-averaged F-score of 82.56%, while the syntax-based approach performed worst with 78.62% F-score. The lexicon-based approach attained an overall average precision of 89.74% and recall of 76.09%, which were significantly better than the results achieved by Negation Tagger with a similar approach.
               
               
                  Discussion
                  The lexicon-based approach benefitted from being customized to the corpus more than the other two methods. The errors in negation detection with the syntax-based approach producing poorest performance were mainly due to the poor parsing results, and the errors with the other methods were probably because of the abnormal grammatical structures.
               
               
                  Conclusions
                  A machine-learning-based approach has potential advantages for negation detection, and may be preferable for the task. To improve the overall performance, one of the possible solutions is to apply different approaches to each section in the reports.
               
            

@&#INTRODUCTION@&#

A large portion of clinical findings mentioned in clinical reports (e.g., discharge summaries, radiology reports) are negated or discursive references about the disease in general. Accurately identifying whether these findings are present or absent for the patient's case is critical to extract pertinent information from the reports and index them. To detect whether particular findings are negated is of great significance to make a proper decision on diagnosis and prognosis. For example, constitutional symptoms such as fever, weight loss and night sweats are known to be of prognostic value in non-Hodgkin lymphoma (NHL) [1,2]. The presence or absence of these symptoms can be used to define two categories for each stage of NHL: A (if symptoms absent) and B (if symptoms present) [3].

In our structured pathology reporting project [4], we have established a pipeline system to extract medical entities and populate structured reports based on the entities. Negation detection, as one of the most important components of the system, definitely can affect the performance of the whole system.

Generally, negation detection includes the detection of negation cues (specific terms indicating negation) and scope (the text negated by the terms). In the following example, “no evidence of malignancy”, where “no evidence of” is the negation cue and “malignancy” is in the scope negated by the cue.

Previous works suggest that a small set of words cover a large portion of negation cues. It is evident that “no”, “denies/denied”, “without”, and “not” are the most frequently used terms to indicate the absence of clinical observations [5]. Several rule-based approaches that utilized lexical pattern matching have been widely applied to the clinical domain. For example, Schadow et al. developed a parser that separated observation statements into smaller phrases by some punctuation, conjunctions or prepositions, and the entire phrase is negated by the negation cue detected on it [6]. However, such a simple method would not work well when a negated concept was separated from the negation cue by one of the separation signals above.

Negfinder used a left-to-right, rightmost-derivation parser to detect negations in surgical notes and discharge summaries and achieved a sensitivity of 95.7% and a specificity of 91.8% without extracting syntactical structures of sentences and phrases [7]. However, it could not detect negated concepts correctly if the negation cue was far away from the intended target concepts, since it terminated a concept list or negation if there were more than three intervening words between concepts or between a negating phrase and a concept.

NegEx, a regular expression-based algorithm, which is simple to implement, has shown its success in detecting negations in discharge summaries with a recall of 77.8%, and precision of 84.5% [8]. It relied on three types of terms to determine whether a condition is negated, namely trigger terms, pseudo-trigger terms, and termination terms. Trigger terms like “no” and “not” indicate that the clinical conditions within the scope of the trigger term should be negated. Pseudo-trigger terms, such as “not rule out” and “gram-negative”, appear to indicate negation but identify double negatives or modify meanings instead. The default scope of the negation is a five-word window. Termination terms, e.g., “but” and “though”, can terminate the scope of the negation before the end of the window. Since NegEx did not take into account any syntactic clue to determine the negation scope, it had faced difficulty in determining the scope of the negation phrase in some difficult cases. Without any customization, its application to the pathology domain had lower performance, probably because the negation and pseudo-negation phrases that were used by NegEx may not adequately cover the spectrum of phrases in pathology reports [9]. It also revealed that failure to correctly map the text phrase to the unified medical language system (UMLS) [10] concept was one major source of errors, as the augmentation with human coded concepts boosted the average recall and precision of the negation tagger by 28% and 13%, respectively.

For complicated negation cases, defining negation scope is still a challenging task. The above approaches could perform reliably when a negated concept is close to a negation cue, but unsatisfactorily when they are separated with multiple words not mapped to a controlled terminology (e.g., UMLS, systematized nomenclature of medicine—clinical terms (SNOMED CT) [11]). To reduce some of NegEx's errors, Meystre et al. used a flexible window, which was extended to the end of the sentence, to the next negation or conjunctional phrase if closer [12]. Syntactic information is also useful to resolve this problem. A hybrid approach by combining regular expression matching with grammatical parsing has been purposed to detect negations [13]. The results showed that the structure grammar rules developed using linguistic principles were more powerful than detecting negated concepts at a fixed distance from negation cues. One of the limitations was the comprehensiveness of such a manually derived negation grammar. Another approach named DepNeg [14], used dependency parses which directly encode thematic roles like subject and object, performed better than the original clinical text analysis and knowledge extraction system (cTAKES) [15] in terms of F-score and accuracy (with about 1.6% and 1.2% increase, respectively) and was able to identify complicated negations that were wrong in cTAKES by a limited set of dependency rules compiled from a small data set.

There has also been some effort with statistical or machine learning methods to detect negation as well. For example, as part of the assertion task, negation detected by machine learning techniques has been the state of the art in the 2010 i2b2 challenge [16]. For example, Patrick et al. established a conditional random fields (CRF) model by adopting several features, and gained more than 92% F-score on the “absent” category in the task [17].

Inspired by the approaches mentioned above, we decided to use three different methods to identify negation in free-text pathology reports, which are lexicon-based approach, syntax-based approach and machine-learning-based approach. The lexicon-based approach highlighted the importance of the utilization of lexicons tailored to the domain; the syntax-based approach focused on the extraction of rules to cover the negation patterns based on the dependency output from the parser; the machine-learning-based approach emphasized on the extraction of useful features to build the statistic model.

The study protocol was approved by Royal Prince Alfred Hospital, Sydney, Australia.

In total, 284 pathology reports of lymphoma were collected for this study. They were scanned, optical character recognition (OCR)-ed and de-identified. We randomly selected 227 reports as a training set, and 57 reports as a test set.

The corpus was annotated manually following the annotation guidelines developed according to Tumours of Haematopoietic and Lymphoid Tissue Structured Reporting Protocol (1st edition 2010) [18]. Besides instances of medical entity types were annotated, we also designed a linguistic category named “Lexical Polarity Negative” (terms of lexically bound polarity that is to negate a medical entity, such as “no” and “not”) to annotate phrases that trigger negation. The boundaries of most entities could cross the boundary of a base noun phrase (e.g., an “Architecture” entity—“diffuse effacement of normal architecture”, a “Diagnosis” entity—“malignant lymphoma of CLL type”), except that a “Clinical Impression” entity should be either a noun phrase or an adjective phrase. And for machine learning purpose, we also assigned a relation type called “Negate” between an instance of “Lexical Polarity Negative” and a medical entity. The annotation process was carried out with a two-phase validation and gold standard development. One computational linguist was trained to annotate the corpus, and then a senior computational linguist reviewed each annotation, as a validator for the development of the gold standards. Although intra-annotator agreement cannot be evaluated in this method, it is assumed that high level of consistency has been attained, as the overall F-score on the self-validation (a 100% train and test strategy: applies the computed model with baseline feature set to validate the training data until no improvement of the performance can be made [17]) is about 99.9%. This is probably because computational linguists can reliably achieve higher consistency than pathologists in annotating a large corpus of pathology reports, which was indicated in Patrick et al.’s study [19]. For some complex cases, we also consulted pathologists for advice. In addition, our pilot study on ten reports shows that over 95% consistency was attained between the negations annotated by a pathologist and those by a linguist. This indicates that linguists were competent to accomplish the annotation task, which is consistent with Vincze's work, which shows a consistency between 95% and 98% [20].

In this study, only pertinent negations within a sentence were considered. Phrases that indicate uncertainty (e.g., “unlikely”) were not considered; normal findings and test results (e.g., “absence of CD15 expression”, “non-diagnostic changes”) were not considered either. Negative prefix or suffix was also not considered, because they are often semantically ambiguous or they are part of an entity (e.g., “non-Hodgkin malignant lymphoma”). Finally, 318 and 96 pertinent negations were identified in the training data and test set, respectively.

@&#METHODS@&#

This study applied three different methods to negation detection. Given a medical entity in a sentence, we seek to determine whether that entity is negated. The processing components for negation detection are shown in Fig. 1
                        , which consist of:
                           
                              (1)
                              Pre-processing includes detecting section contexts (e.g., “Clinical History”, “Macroscopic”, and “Microscopic”), sentence boundary detection, tokenization, part-of-speech (POS) tagging and dependency parsing. Preliminary experiments have indicated that the default POS tagger in Stanford parser cannot tag some biomedical terms like “eosinophils” correctly, hence the GENIA tagger [21] is adopted for POS tagging instead. The results from POS tagging are fed to the Stanford parser to generate the dependency output.

The medical entity recognizer annotates the medical entities and instances of “Lexical Polarity Negative” in the test set. The medical entity recognizer is built by a supervised machine-learning-based method trained with a CRF [22] learner (the toolkit used in this task is CRF++
                                    1
                                 
                                 
                                    1
                                    
                                       http://crfpp.googlecode.com/svn/trunk/doc/index.html.
                                 ). The medical entity types that we focus on this study are “Clinical Impression”, “Other Sites of Disease”, “Constitutional Symptoms”, “Predisposing Factors”, “Architecture”, “Cytomorphology”, “Tissue Reaction”, “WHO Grade”, “Coexistent Pathology”, “Diagnosis”, and “Diagnosis Subtype” (see Appendix 1 for the definitions and examples).

Detecting negated medical entities and the results are filtered by identification of pseudo-negations.

Similar to NegEx [8], through manual scanning, we divided 17 unique instances of “Lexical Polarity Negative” as trigger terms into three groups (Group 1: preceding a medical entity, Group 2: succeeding a medical entity, Group 3: preceding or succeeding a medical entity). We also defined some terms and particular punctuation (section context-sensitive) as termination clues. We have compiled these terms or clues and some examples in Table 1
                           .

This rule-based method can be summarized to three steps:
                              
                                 (1)
                                 Find out whether the trigger term and the medical entity are separated by a termination clue. If they are separated, filter out the instance.

Validate whether the order of appearance of the trigger term and the entity is the same as the associated group. If not, filter out the trigger term.

If the trigger term is not filtered out in the above steps, yield “absent” as the final output.

We defined several rules on the dependency path between the medical entity and the “Lexical Polarity Negative” instance based on the dependency output from the Stanford parser [23]. These rules were prepared referring to negation patterns in DepNeg [14], which were subsequently tuned for the corpus and represented with a set of grammatical relations that were drawn from the Stanford dependencies [24] (see Appendix 2 for more details).


                           Fig. 2
                            divides the rules into several categories based on the length of the dependency paths, grammatical relations, the role of the headwords and linkage words, and the prerequisite conditions.

According to the rules, we can obtain overall seven negation patterns (combinations of the rules):
                              
                                 •
                                 Pattern 1: Rule no. 1.

Pattern 2: Rule no. 3.

Pattern 3: Rule no. 5.

Pattern 4: Rule no. 2/Rule no. 4/Rule no. 6+Rule no. 7/Rule no. 9/Rule no. 11.

Pattern 5: Rule no. 2/Rule no. 4/Rule no. 6+Rule no. 8+Rule no. 13.

Pattern 6: Rule no. 2/Rule no. 4/Rule no. 6+Rule no. 10+Rule no. 14.

Pattern 7: Rule no. 2/Rule no. 4/Rule no. 6+Rule no. 12+Rule no. 15.

Note that for Pattern 6, grammatical relation “dep” cannot co-occur in Rule no. 10 and Rule no. 14.

Examples for each pattern are presented in graphical form in Fig. 3
                           .

This method is processed as follows:
                              
                                 (1)
                                 Find the headwords. For a medical entity, its headword is the head noun if it is a noun phrase; else, its headword is the last word of the entity (following the rightmost head rule [25]). For a “Lexical Polarity Negative” instance, the first word of the instance (e.g., “no” for “no evidence of”, “not” for “not”) is the headword.

Compute the path. We compute the dependency path between the headwords of the “Lexical Polarity Negative” instance and the medical entity. To reduce the complexity of the rules, we only consider the path with length not larger than two. The other words except for the headwords in the path are called linkage words.

Match the patterns. Evaluate whether the path matches the patterns described above. If the length of the path is zero and it matches one of patterns in Patterns 1–3, the entity is negated; if the length of the path is one and it matches Pattern 4, the entity is negated; if the length of the path is two and it matches one of patterns in Patterns 5–7, the entity is negated.

An instance of “Lexical Polarity Negative” (the first concept) and a medical entity (the second concept) was paired and then passed into a support vector machine (SVM) [26] classifier to classify the negation relationships between them. Positive pairs were created for each pair with negation in a sentence, and negative pairs were created for each pair without negation in a sentence. There were 318 positive pairs and 93 negative pairs in total.

Six feature sets were prepared for the SVM classifier, which are described below:
                              
                                 (1)
                                 Context features: (a) Four words before and after the first concept; (b) Four words before and after the second concept.

Semantic features: (a) Annotation types of each concept.

Lexical features: (a) Words inside each concept; (b) Lowercase orthography of words inside each concept; (c) Words between the two concepts; (d) POS tags of the words between the two concepts.

Grammatical features: (a) Headwords of each concept.

Headwords are determined as described above.

Position features: (a) Token distance between the two concepts; (b) The order of appearance for the two concepts.

For (a), as average token distance for all positive pairs is smaller than one, so assign the value as “C” if the token distance between the two concepts is not larger than one; else, assign the value as “F”.

For (b), if the second concept precedes the first concept, assign the value as “P”; else, assign the value as “S”.

Syntax features: (a) The grammatical relations of the shortest dependency path between the headwords of two concepts; (b) The length of the shortest dependency path between the headwords of two concepts.

Dependencies are acquired by the Stanford parser [23].

For (b), since most of the length of the shortest dependency path between the headwords of two concepts for the positive pairs is not larger than two, so assign the value as “C1” if it is not larger than one; “C2” if it is two; “F” if it is larger than two; “O” if the shortest dependency path cannot be found.

Examples of the above features are presented in Table 2
                           . It can be seen that most of them are adapted from the similar ideas in the two rule-based approaches. This is motivated by Patrick et al.’s work that converted a baseline rule-based method to a statistical approach based on the same idea for assertion classification, which produced better performance [17].

A special module was implemented with regular expressions to handle pseudo-negations in the sentences. These regular expressions are triggered by some pseudo-negation phrases (e.g., “not possible”, “not likely”, “to exclude”). If a match was found, this result would take precedence over the result from the negation module, and prevent the system generating the false positive output.

All approaches were evaluated using single train-test cycles. The toolkit used for applying SVM in the machine-learning-based approach is LIBSVM [27].

The performances of the three methods were measured by the precision, recall and F-score. Comparing our results to the work of Mitchell et al. [9], we adopted the same three metrics: strict, lenient and average metrics to compute the results [28]. Strict metrics only consider exact match of the system predictions and gold-standards when they have the same boundaries; lenient metrics also consider partial match when they have any overlap of the boundaries; average metrics are the mean of the two above metrics. They are computed by true positive (TP), false positive (FP), false negative (FN) and partial positive (PP) as follows:
                           
                              •
                              Strict precision (SP)=TP/(TP+FP+1/2PP).

Strict recall (SR)=TP/(TP+FN+1/2PP).

Strict F-score (SF)=2×SP×SR/(SP+SR).

Lenient precision (LP)=(TP+1/2PP)/(TP+FP+1/2PP).

Lenient recall (LR)=(TP+1/2PP)/(TP+FN+1/2PP).

Lenient F-score (LF)=2×LP×LR/(LP+LR).

Average precision (AP)=(SP+LP)/2.

Average recall (AR)=(SR+LR)/2.

Average F-score (AF)=2×AP×AR/(AP+AR).

@&#RESULTS@&#

We summarize the report sections in four categories (according to the protocol and advice from pathologists): Macroscopic, Microscopic, Summary (including “Summary” and “Supplementary Summary”), and Other (composed of “Clinical History”, “Specimen”, “Frozen Section Report”, and “Supplementary Report”).

The initial evaluation on the training set by applying the lexical-based approach and the syntax-based approach shows that rules presented in both methods were competent to capture most negation patterns in the training data. Specifically, the syntax-based approach achieved an overall F-score of 98.90%, while overall F-score for the lexical-based approach was 98.60%. Ten-fold cross-validation experiments were also carried out for machine-learning-based approach on the training set and an overall micro-averaged F-score of 96.74% was attained.


                     Table 3
                      shows the contribution of features to the machine-learning-based approach on the training set under strict condition. From Table 3, the baseline model achieved 86.98% F-score. Contextual feature set yielded the biggest gain, and improved the model by 3.98%. Meanwhile, moderate improvements are contributed by the positional feature token distance between the two concepts and the lexical feature tokens between the two concepts (with 2.62% and 1.33% gain, respectively). Three syntax features and the positional feature the order of appearance for the two concepts yielded some gains ranging from 0.39% to 0.45%, respectively. Minimal improvements were made by adding the lexical feature lowercase of words inside each concept and headwords of each concept with 0.11% and 0.02%.

The system performance across report sections for the three methods on the test set are presented in Table 4
                     . The overall micro-averaged F-scores decrease by 14.18–20.28%. The machine-learning-based approach performed well on the Microscopic section, with 84.85% F-score; the syntax-based approach performed well within the Summary and other sections, with 100% and 61.54% F-scores, respectively. The significant levels (P values) between the overall results of the machine-learning-based approach and the lexical-based approach, and that of the syntax-based approach are 0.35 and 0.37, respectively. Although there are no statistically significant differences between them at the 0.05 critical alpha level, we believe this is due to the small sample size and the trend in favour of the machine-learning-based approach will be validated with larger samples.

As shown in Table 5
                     , the majority of errors on the test set can be attributed to medical entity recognition from the CRF, especially the relatively low recall (during ten-fold cross-validation experiments of medical entity recognition on the training set, an overall recall of 82.93% was attained, while the overall precision was 87.52% and the micro-averaged F-score was 85.16%). Note that we categorize the errors to medical entity recognition in priority, hence it cannot be excluded that some errors were actually from both medical entity recognition and negation detection instead.

@&#DISCUSSION@&#

We have described the processing components for negation detection on pathology reports. Three different methods: lexicon-based approach, syntax-based approach and machine-learning-based approach have been implemented for detecting negated medical entities extracted from a supervised machine-learning-based medical entity recognizer. The ability of the three methods to accurately identify negations has been evaluated against a gold-standard corpus of negation annotations.

It can be seen from Table 4 that performance is best on the Summary section and worst on other sections. This is possibly because of the better grammatical structure in the Summary section. This section is present in almost every report, often in a well-structured and formalized format. This is also the main reason for the best performance in this section with the syntax-based approach (100% F-score), as the syntax-based approach relies on the output from the parser and the performance of the parser is hindered by the linguistic constructions of the input. Accurate parsing output can be generated due to the simple linguistic constructions in this section. The best performance for the Microscopic section is obtained by the machine-learning-based approach, which indicates one of the advantages of using this method when the amount of training samples are sufficient: the training examples in the Microscopic section have the largest proportion (about 76%), that allows the machine learner to learn more effectively, which leads to more accurate prediction on the test set with the lowest drop (about 12.3%) in this section.

It can also be seen from Table 4 that F-scores for the Microscopic section evaluated with lenient metric were higher than those under strict condition, as partial match performance on medical entity recognition was much better than extract match performance; the majority of medical entities were located in the Microscopic section, thus the system performance on this section was significantly affected by this.


                        Table 4 shows that errors from negation detection directly contribute to about 8.7% to 25.8% of total errors depending on the method. The reasons for those errors with syntax-based approach are mainly due to the poor parsing results from the parser, where the rules do not work as expected if parse trees contain errors. Analysis of parsing results indicates that the average accuracy was about 71.9%, wherein 100%, 86.7%, 75% and 33.3% on the Macroscopic, Summary, Microscopic and Other sections, respectively. The possible reason for this is the Stanford parser was not trained on clinical domain. For example, given the input “No vasculitis [“Coexistent Pathology”] with fibrinoid necrosis or leucocytoclastic debris, granulomas [“Tissue Reaction”] or necrosis [“Tissue Reaction”] are seen.”, the parser attached the noun phase “leucocytoclastic debris” at the wrong place in the parse tree (see Fig. 4
                        ). Consequently, the entities “granulomas” and “necrosis” could not be identified as negated concepts. This suggests that such errors can be amended by using a domain-specific parser or a parser trained with medical corpora to improve the parsing performance.

It is known that medical narratives often prefer to be presented in compact expressions, and therefore noun and prepositional phrases are more frequently used in a long sentence rather than complex verb structures or short sentences as in the general domain, and in some cases, they may be irregular grammatical structures. Errors from other methods are probably because of these abnormal structures written by the pathologists. Here is an example, NO EVIDENCE OF [“Lexical Polarity Negative”] METASTATIC MELANOMA IN 11 LYMPH NODES, SMALL LYMPHOCYTIC NON-HODGKINS LYMPHOMA [“Diagnosis”], Gene rearrangement studies pending, Please see report.

The instance of “SMALL LYMPHOCYTIC NON-HODGKINS LYMPHOMA” should not be tagged as negated in the above. However, due to the irregular grammatical structure of the sentence, it is too difficult for the negation detection module to generate a correct output. In fact, we think that generally, this sentence can be divided into four sentences: “NO EVIDENCE OF METASTATIC MELANOMA IN 11 LYMPH NODES.”, “SMALL LYMPHOCYTIC NON-HODGKINS LYMPHOMA.”, “Gene rearrangement studies pending.” and “Please see report.”; or some conjunctions like “and”, “but” should be supplemented in the sentence for grammatical correction.

A similar work has been done by Mitchell et al. on detecting and annotating UMLS concepts as well as annotating negation based on the NegEx algorithm [9]. They reported that the overall precision and recall under average conditions were about 64% and 55%, respectively, which are about 25.7% and 21.1% lower than the ones obtained from our lexicon-based approach derived from the NegEx algorithm. This is probably because our lexicon-based approach has been modified from NegEx according to the corpus. For example, the negation and pseudo-negation phrases were extracted from the training data; except for some terms, we also introduced particular punctuation for each section as termination clues to improve the precision of our algorithm. Nevertheless, there are also some notable differences between the Mitchell et al.’s work and our study:
                           
                              (1)
                              The materials for evaluation. The previous work used surgical pathology reports; our study selected pathology reports of a specific disease (lymphoma).

The development of gold-standards. The previous work used a modified Delphi technique to achieve consensus among the panel and four pathologists were participated in the manual annotations; our study only used a single pass panel and two linguists were involved in manual annotations.

The test sample size. The previous work had a larger test sample size with 311 entities; our study only had 96 entities for testing. Given this is a pilot study, the system was tested on a relatively small data set. We are planning to evaluate the system on a lager sample set in future work.

The semantic types. The semantic types chosen by the previous work were up to 35 from 5 semantic categories relevant to surgical pathology reports in UMLS; our study defined 11 specific semantic types based on the protocol (as required in the structured pathology reporting project as defined by the Royal College of Pathologists of Australasia).

The methods for concept recognition. The previous work performed look-ups to the NLM Knowledge Source Server, and matched phrases against the UMLS, therefore each extracted entity is an UMLS concept; our study used a supervised machine-learning-based approach to extract medical entities, hence the extracted entities are not UMLS concepts.

The results for Comment section. The results for most sections we reported are corresponding to the previous work, e.g., Macroscopic vs. Gross Description, Microscopic vs. Microscopic Description, and Summary vs. Final Diagnosis. However, we have not presented the results for the Comment section, since all the contents in the Comment section are required to be populated to the structured templates instead of each entity type in our structured pathology reporting project, and we did not annotate any entity in this section.

Comparisons with other works were not performed due to different genres of clinical documents used as research materials between this work and them.

@&#SUMMARY@&#

The Syntax-based approach has the best overall performance on the training set but the poorest on the test set, in contrast to the machine-learning-based approach. This indicates the potential advantages of using machine-learning-based approaches for negation detection. In the test set, there are unforeseen structures in the sentences which the rules or patterns designed for syntax-based approach cannot handle properly though they work well on the training set. The model generated by the machine-learning-based approach predicts the test data with features not only captured from the training set but also based on the similarity between them and those from the test set, which make it less vulnerable to the unforeseen structures.

@&#LIMITATIONS@&#

The first limitation is the sample size for testing may be too small to reveal the full view of the research problem in this study. More testing samples are needed for further validation. With a larger sample size, we can get a clearer picture of how these methods perform and what errors would be observed in practice.

Another limitation is that we have not engaged the pathologists fully in the study, as the gold-standard was created by linguists. Although their annotation performance is not as precise as pathologists, their work has faced validity and been shown to be internally consistent in our previous project [19].

The future work can address other issues, e.g., assessing the effect of the headword selection on the syntax-based approach, retraining the Stanford parser to obtain better parsing results.

@&#CONCLUSIONS@&#

We developed three different approaches, which were lexicon-based, syntax-based and machine-learning-based to detect negation in narrative pathology reports. The results show that the machine-learning-based approach is equal best (statistically) and with the potential to improve with more extensive training data. The performances on most document sections vary by approach, indicating that to improve the overall performance one of the possible solutions is to apply different approaches to each section. The study also reveals that using a supervised machine-learning-based approach to extract medical entities can overcome some limitations of dictionary look-up method for concept recognition, although the incorrect extracted entities still account for most errors in the evaluations of the test set. However, due to the small sample size and the simple process for gold-standard development, a more thorough and comprehensive validation should be carried out. We plan to implement the negation detection module presented here to establish how it performs in our project to automatically populate structured reports from narrative pathology reports.

@&#ACKNOWLEDGMENTS@&#

The authors would like to give special thanks to members of the Health Information Technologies Research Laboratory for their valuable contributions. This project is funded by the Quality Use of Pathology Program of the Department of Health and Ageing, Australian Government.


                     
                        
                           
                              Annotation types with definitions and some examples.
                           
                           
                              
                              
                              
                              
                                 
                                    Annotation type
                                    Definition
                                    Example
                                 
                              
                              
                                 
                                    Architecture
                                    The pattern of infiltration or architecture of abnormal cells
                                    “diffuse infiltrate”, “nodular areas”, “mantle zone”
                                 
                                 
                                    Cytomorphology
                                    Characteristic cytological features of individual tumour cells
                                    “centroblastic”, “R-S cells”, “centrocytes”
                                 
                                 
                                    Diagnosis
                                    The diagnosis of the lesion within the specimen(s)
                                    “large B-cell lymphoma”, “classical Hodgkin lymphoma”, “Chronic lymphocytic leukaemia”
                                 
                                 
                                    WHO grade
                                    The grade for follicular lymphoma
                                    “WHO GRADE 1”, “low-grade”, “high grade”
                                 
                                 
                                    Clinical impression
                                    The clinical diagnosis or differential diagnosis
                                    “lymphoma”, “SCC”, “CLL”
                                 
                                 
                                    Other sites of disease
                                    Involved sites or pattern of disease spread and whether disease is nodal or extra-nodal
                                    “extranodal spread”, “involving mediastinum”, “mets”
                                 
                                 
                                    Constitutional symptoms
                                    All relevant constitutional symptoms
                                    “5kg weight loss”, “fevers”, “Night sweats”
                                 
                                 
                                    Predisposing factors
                                    Immunocompromised states (immunodeficiency associated lymphoproliferative disorders) and autoimmune conditions, infective agents
                                    “post chemo”, “Hep C”, “HIV +ve”, “liver transplanted”
                                 
                                 
                                    Tissue reaction
                                    Host cells and tissue reactions
                                    “eosinophils”, “necrosis”, “granulomas”
                                 
                                 
                                    Diagnosis subtype
                                    The sub-classification of lymphoma
                                    “lymphocyte depleted type”, “syncytial variant of nodular sclerosing”, “MALT”
                                 
                                 
                                    Coexistent pathology
                                    Coexistent pathological abnormalities
                                    “ulceration”, “perforation”, “polyps”
                                 
                              
                           
                        
                     
                  

DepNeg used several types of negation patterns based on dependency paths, which were computed from the dependency parse. The negation patterns included:
                        
                           •
                           Negated verbs—if a particular verb is negated, the whole verb phrase is negated as well, including the objects or complements of the verb.

Negative verbs—particular verbs indicate exclusion of the direct object of the verbs.

Negative prepositions—particular prepositions negate the object of the prepositions.

Negated nouns—certain determiners negate the nouns they modify.

Negative adjectives—certain adjectives negate the nouns they modify.

Conjunction expansion—a general rule can be applied to every other pattern to allow conjunctions or lists of the targets above.

The medical entities were identified in a name entity recognition module in DepNeg, which resembled this study. Therefore, the syntax-based approach purposed in this study also used the dependency paths from the parser to extract the rules.

A set of grammatical relations was drawn from the Stanford dependencies, which are all binary relations that hold between a governor and a dependent.
                        
                           •
                           Adverbial modifier (advmod): an adverbial modifier of a word.

Adjectival modifier (amod): an adjectival modifier of a noun phrase.

Appositional modifier (appos): an appositional modifier of a noun phrase.

Conjunct (conj): the relation between two elements connected by a coordinating conjunction.

Dependent (dep): when the parser is unable to determine a more precise relation between two words, it assigns this label to the words.

Determiner (det): the relation between the head of a noun phrase and its determiner.

Direct object (dobj): a noun phrase which is the accusative object of a verb.

Infinitival modifier: an infinitive that serves to modify a noun phrase.

Negation modifier (neg): the relation between a negation word and the word it modifies.

Noun compound modifier (nn): any noun that serves to modify the head noun.

Nominal subject (nsubj): a noun phrase which is the syntactic subject of a clause.

Passive nominal subject (nsubjpass): a noun phrase which is the syntactic subject of a passive clause.

Participial modifier (partmod): a participial verb form that serves to modify a noun phrase or sentence.

Object of a preposition (pobj): the head of a noun phrase following the preposition or the adverbs “here” and “there”.

Prepositional modifier (prep): any prepositional phrase that serves to modify a verb, adjective, noun, or another preposition.

Relative clause modifier (rcmod): a relative clause modifying a noun phrase.

Open clausal complement (xcomp): a clausal complement without its own subject, and is determined by an external subject.

In the collapsed representation, dependencies involving prepositions and conjuncts are collapsed to get direct dependencies between content words. Conjuncts involve conjunctions “and” and “or” are collapsed as “conj_and” and “conj_or”. Several variant conjunctions for “and not”: “but not”, “instead of”, “rather than”, and “but rather” are collapsed as “conj_negcc”. Prepositional modifiers regarding prepositions “of”, “without “, and “such as” are collapsed as “prep_of”, “prep_without” and “prep_such_as”, respectively.

@&#REFERENCES@&#

