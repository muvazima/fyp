@&#MAIN-TITLE@&#Fast detection of high-order epistatic interactions in genome-wide association studies using information theoretic measure

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose a fast method for detection of high order epistatic interaction in GWAS.


                        
                        
                           
                           We employ mutual information as association measure and for SNP clustering.


                        
                        
                           
                           Our algorithm makes high order GWAS in a matter of hours on a PC.


                        
                        
                           
                           We report up to 5-way interactions on each of seven diseases in WTCCC data.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Genome-wide association studies

High-order epistatic interactions




                     K-means clustering

Mutual information

WTCCC

@&#ABSTRACT@&#


               
               
                  There are many algorithms for detecting epistatic interactions in GWAS. However, most of these algorithms are applicable only for detecting two-locus interactions. Some algorithms are designed to detect only two-locus interactions from the beginning. Others do not have limits to the order of interactions, but in practice take very long time to detect higher order interactions in real data of GWAS. Even the better ones take days to detect higher order interactions in WTCCC data.
                  We propose a fast algorithm for detection of high order epistatic interactions in GWAS. It runs k-means clustering algorithm on the set of all SNPs. Then candidates are selected from each cluster. These candidates are examined to find the causative SNPs of k-locus interactions. We use mutual information from information theory as the measure of association between genotypes and phenotypes.
                  We tested the power and speed of our method on extensive sets of simulated data. The results show that our method has more or equal power, and runs much faster than previously reported methods. We also applied our algorithm on each of seven diseases in WTCCC data to analyze up to 5-locus interactions. It takes only a few hours to analyze 5-locus interactions in one dataset. From the results we make some interesting and meaningful observations on each disease in WTCCC data.
                  In this study, a simple yet powerful two-step approach is proposed for fast detection of high order epistatic interaction. Our algorithm makes it possible to detect high order epistatic interactions in GWAS in a matter of hours on a PC.
               
            

@&#INTRODUCTION@&#

There have been many genome-wide association studies (GWAS) to identify associations between phenotypes and whole genome information. Some studies have been successful in finding associations between single SNPs and disease susceptibilities. However, large part of genetic effects to phenotypes remains unresolved. Main reason of this deficiency is that many diseases are associated with effects of gene–gene interactions (Cordell, 2009). These effects are called epistatic interactions (epistasis) (Moore and Williams, 2009). Many studies demonstrated existence of epistatic interactions in such diseases as breast cancer (Ritchie et al., 2001), coronary heart disease (Nelson et al., 2001) and Alzheimer's disease (Zubenko et al., 2001).

Many algorithms for detecting epistatic interactions have been proposed. These algorithms can be largely classified into four categories: exhaustive search methods, stochastic approaches, data mining/machine learning approaches and stepwise approaches.

Exhaustive search strategies test every possible combination of k-SNPs. They are guaranteed to find the most susceptible k-SNP, but take a long time to execute. CPM (Nelson et al., 2001) enumerates all possible genotype combinations, and its use is limited to candidate SNP association studies due to long execution time. RPM (Culverhouse, 2007) reduces execution time compared with CPM, but its use is still limited. MDR (Ritchie et al., 2001) is a representative of exhaustive search method, which has been applied to many clinical studies. MDR divides all genotypes into one of two categories instead of enumerating all genotypes, which makes it faster than CPM or RPM. However, it is still short of genome-wide association studies. TEAM (Zhang et al., 2010), BOOST (Wan et al., 2010a) and BiForce (Gyenesei et al., 2012) reduce execution time by devising clever data structures. They can be used in GWAS, but are limited to detecting only two-locus interactions.

Stochastic approaches use random sampling and BEAM (Zhang and Liu, 2007) is a representative of them. BEAM maximizes the probability of models by Markov Chain Monte Carlo (MCMC). These algorithms usually have many parameters whose values are set by users, and execution times depend on the values of these parameters. In most cases, long execution times are required to obtain good results.

There are many methods of data mining/machine learning approaches. Neural networks (Serretti and Smeraldi, 2004), decision trees (Xie et al., 2005), and random forests (Meng et al., 2009; Winham et al., 2012) have been applied to detect epistatic interactions among candidate SNPs. SNPRuler (Wan et al., 2010b) employs predictive rule learning to make it possible to detect epistasis in GWAS. Various methods of data mining/machine learning approaches have their own pros and cons. In general, they are faster than exhaustive approaches, but may find local optima instead of global ones.

Stepwise approaches consist of two stages: filtering stage and search stage. At the filtering stage, the SNPs that are not meaningful are filtered out; at the search stage, survived SNPs are tested for epistatic interactions. At the search stage, any of exhaustive, stochastic, data mining/machine learning approaches can be used. Feature selection methods Relief (Kenji Kira, 1992) and ReliefF (Kononenko, 1994) are adopted and modified to TuRF (Moore and White, 2007) and SURF (Greene et al., 2009) to be used as filtering methods for GWAS. SNPHarvester (Yang et al., 2009) uses local search algorithm to select the SNPs to be tested at the next stage. Screen and clean (Wu et al., 2010) uses lasso logistic regression to select the significant SNPs. BOOST (Wan et al., 2010a) uses likelihood ratio test to prune insignificant SNPs. EDCF (Xie et al., 2012) recursively detects d-locus interactions from the results of (d
                     −1)-locus interactions.

There are many algorithms for detecting epistatic interactions in GWAS. However, most of these algorithms are applicable only for detecting two-locus interactions. Some algorithms are designed to detect only two-locus interactions from the beginning. Such cases are TEAM, BOOST, and BiForce. Others do not have limits to the order of interactions, but in practice take very long time to detect higher order interactions in real data of GWAS. Even the better ones take days to detect higher order interactions in WTCCC data (Burton et al., 2007). SNPRuler reports that it took ∼2 days to analyze one WTCCC dataset. SNPHarvester reports that it took about 2 weeks to handle WTCCC data.

We propose a fast algorithm for detection of high order epistatic interactions in GWAS. Our algorithm can analyze 5-locus interactions in a WTCCC dataset in a few hours on a PC. It runs k-means clustering algorithm on the set of all SNPs. Then candidates are selected from each cluster. These candidates are examined to find the causative SNPs of k-locus interactions. We use mutual information from information theory as the measure of distance between two SNPs for clustering. The details of the method, including how candidates are selected and why mutual information is used, will be explained in Section 2.

We tested the power and speed of our method on extensive sets of simulated data. The results show that our method has equal or more power, and runs much faster than previously reported methods including SNPRuler and SNPHarvester. We also applied our algorithm on each of seven diseases in WTCCC data to analyze up to 5-locus interactions. It takes only a few hours to analyze 5-locus interactions in one dataset. From the results we make some interesting and meaningful observations on each disease in WTCCC data. Our algorithm makes it possible to detect high order epistatic interactions in GWAS in a matter of hours on a PC.

@&#METHODS@&#

Chi-square tests are often used as the association measure in GWAS. However, they are not appropriate to use for the cases where there are cells whose frequencies in contingency tables are less than 5. In the studies of detecting high order interactions, it is highly probable that contingency tables have cells of very low frequencies. Thus we used mutual information as the measure of association between genotypes and susceptibilities of diseases.

Mutual Information I(X;Y) represents the amount of the information shared by random variables X and Y. The definition of I(X;Y) and its relation to other information theoretic measures are given in Table 1
                        . In our study X represents the genotype value (minor/minor, major/minor, or major/major) of a particular SNP among samples, and Y represents whether a sample has the disease or not (case or control). Let us call Y disease state from this point on. Higher value of I(X;Y) implies the SNP has stronger association with the disease. Note that mutual information can be extended to accommodate multiple SNPs.

In this study, entropy is defined in terms of partitions. The set of samples is partitioned by the values of a SNP, and also partitioned into cases or controls. Let X
                        ={A
                        1, A
                        2, …, A
                        
                           n
                        } be a partition of S. In other words, S
                        =
                        A
                        1
                        ∪
                        A
                        2
                        ∪…∪
                        A
                        
                           n
                         and A
                        
                           i
                        ∩
                        A
                        
                           j
                        
                        =∅ for distinct i and j. The entropy H(X) of X is defined as follows:
                           
                              
                                 
                                    H
                                    (
                                    X
                                    )
                                    =
                                    −
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       n
                                    
                                    
                                       
                                          |
                                          
                                             A
                                             i
                                          
                                          |
                                       
                                       
                                          |
                                          S
                                          |
                                       
                                    
                                    log
                                    
                                       
                                          |
                                          
                                             A
                                             i
                                          
                                          |
                                       
                                       
                                          |
                                          S
                                          |
                                       
                                    
                                 
                              
                           
                        where |·| denotes the number of the elements of a set. We can extend the definition to any number of partitions. Let 
                           
                              
                                 X
                                 j
                              
                              =
                              {
                              
                                 A
                                 1
                                 
                                    (
                                    j
                                    )
                                 
                              
                              ,
                              
                                 A
                                 2
                                 
                                    (
                                    j
                                    )
                                 
                              
                              ,
                              …
                              ,
                              
                                 A
                                 
                                    
                                       n
                                       j
                                    
                                 
                                 
                                    (
                                    j
                                    )
                                 
                              
                              }
                           
                        , for j
                        =1, 2, …, k, be partitions for a set S. The joint entropy H(X
                        1, X
                        2, …, X
                        
                           k
                        ) of the partitions X
                        1, X
                        2, …, X
                        
                           k
                         is defined as
                           
                              
                                 
                                    −
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             i
                                             1
                                          
                                          =
                                          1
                                       
                                       
                                          
                                             n
                                             1
                                          
                                       
                                    
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             i
                                             2
                                          
                                          =
                                          1
                                       
                                       
                                          
                                             n
                                             2
                                          
                                       
                                    
                                    …
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             i
                                             k
                                          
                                          =
                                          1
                                       
                                       
                                          
                                             n
                                             k
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   A
                                                   
                                                      
                                                         i
                                                         1
                                                      
                                                   
                                                   
                                                      (
                                                      1
                                                      )
                                                   
                                                
                                                ∩
                                                
                                                   A
                                                   
                                                      
                                                         i
                                                         2
                                                      
                                                   
                                                   
                                                      (
                                                      2
                                                      )
                                                   
                                                
                                                ∩
                                                …
                                                ∩
                                                
                                                   A
                                                   
                                                      
                                                         i
                                                         k
                                                      
                                                   
                                                   
                                                      (
                                                      k
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                       
                                          |
                                          S
                                          |
                                       
                                    
                                    log
                                    
                                       
                                          
                                             
                                                
                                                   A
                                                   
                                                      
                                                         i
                                                         1
                                                      
                                                   
                                                   
                                                      (
                                                      1
                                                      )
                                                   
                                                
                                                ∩
                                                
                                                   A
                                                   
                                                      
                                                         i
                                                         2
                                                      
                                                   
                                                   
                                                      (
                                                      2
                                                      )
                                                   
                                                
                                                ∩
                                                …
                                                ∩
                                                
                                                   A
                                                   
                                                      
                                                         i
                                                         k
                                                      
                                                   
                                                   
                                                      (
                                                      k
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                       
                                          |
                                          S
                                          |
                                       
                                    
                                 
                              
                           
                        
                     

Now the mutual information between the joined partition of X
                        1, X
                        2, …
                        X
                        
                           k
                         and a partition Y can be defined as follows:
                           
                              
                                 
                                    I
                                    (
                                    
                                       X
                                       1
                                    
                                    ,
                                    
                                       X
                                       2
                                    
                                    ,
                                    …
                                    ,
                                    
                                       X
                                       k
                                    
                                    ;
                                    Y
                                    )
                                    =
                                    H
                                    (
                                    
                                       X
                                       1
                                    
                                    ,
                                    
                                       X
                                       2
                                    
                                    ,
                                    …
                                    ,
                                    
                                       X
                                       k
                                    
                                    )
                                    +
                                    H
                                    (
                                    Y
                                    )
                                    −
                                    H
                                    (
                                    
                                       X
                                       1
                                    
                                    ,
                                    
                                       X
                                       2
                                    
                                    ,
                                    …
                                    ,
                                    
                                       X
                                       k
                                    
                                    ,
                                    Y
                                    )
                                 
                              
                           
                        
                     

Let X
                        1, X2, …, X
                        
                           k
                         be partitions for the set of samples induced by the genotypes of the SNPs SNP
                        1, SNP
                        2, …, SNP
                        
                           k
                        , respectively, and Y be the partition by disease state (case or control). Then I(X
                        1, X
                        2, …, X
                        
                           k
                        
                        ;
                        Y) represents the degree of association between genotypes of SNP
                        1, SNP
                        2, …, SNP
                        
                           k
                         and the disease state.

In this study, we try to find the set of k SNPs that maximizes the value I(X
                        1,X
                        2, …, X
                        
                           k
                        
                        ;
                        Y).


                        Table 2
                         illustrates mutual information of a single SNP and the disease state. In contingency table T
                        1, distributions of genotypes are identical in cases and controls. Then the mutual information is 0, and this locus has nothing to do with whether a given sample is a case or a control. In contingency table T
                        3, the mutual information is 1, and we can make a complete prediction on the disease state by the genotype of the sample. Supplementary 1 Eq. (S1) shows the intermediate steps of calculating the mutual information value from contingency table T
                        2.

To show that the mutual information is appropriate to measure the association between a group of SNPs and a query disease, we compared mutual information values and chi-square values on randomly generated data (Supplementary 1 Fig. S1). We found that they are in direct proportion, but a bit perturbed when low frequency cells are present, in which case chi-square is known to be unsuitable to use.

We want to find the set of k SNPs X
                        1, X
                        2, …, X
                        
                           k
                         that maximizes the value I(X
                        1,X
                        2, …, X
                        
                           k
                        
                        ;
                        Y). But it takes too much time to examine every k-combinations of a half million SNPs in GWAS for k
                        ≥3. In order to reduce time complexity, we make use of k-means clustering. K-means clustering takes only linear time. As the result of clustering, strongly interacting SNPs are likely to be placed into different clusters. Then we can save time by investigating the interactions of only those SNPs that belong to different clusters with high chance.

To find k-locus interactions, first we perform k-means clustering on the set of given SNPs. Here we define the distance between two SNPs X
                        
                           i
                         and X
                        
                           j
                         as the mutual information I(X
                        
                           i
                        , X
                        
                           j
                        
                        ;
                        Y). In other words, dist(X
                        
                           i
                        , X
                        
                           j
                        )=
                        I(X
                        
                           i
                        , X
                        
                           j
                        
                        ;
                        Y), where Y is the disease state of the samples. The process of selecting the centroid of each cluster deserves some comments. Each SNP generates a contingency table consisting of genotype frequencies among the samples. We define the average contingency table T
                        avg of a cluster as follows: each entry of T
                        avg is the average of the corresponding entries of all the contingency tables generated by the SNPs belonging to the cluster. For example, in Table 2, T
                        avg is the average of T
                        1, T
                        2, and T
                        3. We define the centroid of a cluster as the SNP whose contingency table is closest to T
                        avg with respect to the sum of squared errors, i.e. the SNPq that minimizes 
                           
                              |
                              |
                              
                                 T
                                 q
                              
                              −
                              
                                 T
                                 
                                    a
                                    v
                                    g
                                 
                              
                              |
                              
                                 |
                                 2
                              
                           
                        . For example, in Table 2, T2 is the centroid of the cluster consisting of SNP1, SNP2, and SNP3. The detailed calculations are given in Supplementary 1 Eq. (2).

After clustering, we select m candidates from each cluster. A candidate in a cluster is an element that is far apart from the elements in the other clusters. In order to precisely describe the criteria of selecting candidates, first we define the score of an element X in the ith cluster as follows, where C
                        
                           j
                         is the centroid of the jth cluster.
                           
                              
                                 
                                    s
                                    c
                                    o
                                    r
                                    e
                                    (
                                    X
                                    )
                                    =
                                    
                                       
                                          ∑
                                       
                                       
                                          j
                                          ≠
                                          i
                                       
                                    
                                    d
                                    i
                                    s
                                    t
                                    (
                                    X
                                    ,
                                    
                                       C
                                       j
                                    
                                    )
                                    =
                                    
                                       
                                          ∑
                                       
                                       
                                          j
                                          ≠
                                          i
                                       
                                    
                                    I
                                    (
                                    X
                                    ,
                                    
                                       C
                                       j
                                    
                                    ;
                                    Y
                                    )
                                    =
                                    I
                                    (
                                    X
                                    ,
                                    
                                       C
                                       1
                                    
                                    ;
                                    Y
                                    )
                                    +
                                    …
                                    +
                                    I
                                    (
                                    X
                                    ,
                                    
                                       C
                                       
                                          i
                                          −
                                          1
                                       
                                    
                                    ;
                                    Y
                                    )
                                    +
                                    I
                                    (
                                    X
                                    ,
                                    
                                       C
                                       
                                          i
                                          +
                                          1
                                       
                                    
                                    ;
                                    Y
                                    )
                                    +
                                    …
                                    +
                                    I
                                    (
                                    X
                                    ,
                                    
                                       C
                                       k
                                    
                                    ;
                                    Y
                                    )
                                 
                              
                           
                        
                     

In other words, score(X) is the sum of the distances between X and the centroid of the other clusters. The elements of higher scores have stronger interactions with the elements in the other clusters. From each cluster we select top m elements by the score, and call these elements the candidates. Since we select m candidates from each cluster, we have km candidates in total. Among these km candidates, we exhaustively search the set of k SNPs that has the highest mutual information value I(X
                        1, X
                        2, …, X
                        
                           k
                        
                        ;
                        Y).

The summary of the algorithm is as follows:
                           
                              1.
                              Perform the k-means clustering on the set of SNPs (mutual information is used as the measure of distance between two SNPs).

From each cluster, select the top m SNPs by the score (a SNP gets a higher score if it is farther from the other clusters).

Find the set of k SNPs with the highest value of mutual information I(X
                                 1, X
                                 2, …, X
                                 
                                    k
                                 
                                 ;
                                 Y) among km candidates.

As the result of clustering, weakly interacting SNPs tend to be placed in the same cluster, and strongly interacting SNPs tend to be placed into different clusters. And the SNPs with higher scores have even stronger interactions.

In a word we use the following heuristic: In order to find the set of k SNPs X
                        1, X
                        2, …, X
                        
                           k
                         that maximize I(X
                        1, X
                        2, …, X
                        
                           k
                        
                        ;
                        Y), we select as candidates those X's that have higher value of score(X)=
                        I(X, C
                        1
                        ;
                        Y)+…+
                        I(X, C
                        
                           i−1
                        ;
                        Y)+
                        I(X, C
                        
                           i+1
                        ;
                        Y)+…+
                        I(X, C
                        
                           k
                        
                        ;
                        Y), and then perform search among those candidates.

In our experiments the value of m ranges from 10 to 20. Let us compare the numbers of tuples to be examined. With typical values of n
                        =500,000, m
                        =20, and k
                        =3, we get 60 candidates. Then the number of 3-combinations to be examined is only 
                           
                              
                                 
                                    
                                       
                                          
                                             
                                                k
                                                m
                                             
                                          
                                       
                                       
                                          
                                             k
                                          
                                       
                                    
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          
                                             
                                                60
                                             
                                          
                                       
                                       
                                          
                                             3
                                          
                                       
                                    
                                 
                              
                              ≈
                              36,000
                           
                        . If we examine every 3-combinations among the original SNPs, the number amounts to 
                           
                              
                                 
                                    
                                       
                                          
                                             n
                                          
                                       
                                       
                                          
                                             k
                                          
                                       
                                    
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          
                                             
                                                500,000
                                             
                                          
                                       
                                       
                                          
                                             3
                                          
                                       
                                    
                                 
                              
                              ≈
                              2
                              ⋅
                              
                                 
                                    10
                                 
                                 
                                    16
                                 
                              
                           
                        .

Our algorithm takes stepwise approach. At the filtering stage, only km SNPs survive through k-means clustering. At the next stage, all the k-SNP combinations are examined exhaustively among km candidates.

Candidates size m can be chosen by users. We tried different values of m and found that the power increases, as the value of m gets bigger. Between m
                        =10 and m
                        =20, the power increase was very slow, and after m
                        =20 we could not observe noticeable increase of the power. Therefore we used m
                        =10 for our experiments on simulated data and m
                        =20 on real data. We recommend m
                        =20 to analyze real data of the WTCCC scale.

Our algorithm consists of three parts: clustering (K-means procedure in Fig. 1
                        ), candidates selection (SelectCandidates procedure in Fig. 1), and finding the k-SNP combination of the highest mutual information value (GetBest procedure in Fig. 1).


                        K-means clustering takes O(kn) time, where k is the number of clusters (the order of interactions) and n is the number of elements (SNPs). For calculating the distance (mutual information) between each element and the representative, it takes O(l) time to construct the contingency table, where l is the number of samples. Hence the k-means clustering stage takes O(lkn) time.

In candidates selection stage, calculating the score of each SNP takes O(k) time. Hence calculating the scores of every element takes O(kn) time. Selecting top m SNPs by the score takes O(mn) time. Since k≪m, candidates selection stage takes O(mn) time.

The search stage examines every k-SNP combination among km SNPs to find the one with the highest mutual information value. Since computing mutual information of each k-SNP combination and disease state takes O(l) time, this stage takes O(l(km)
                           k
                        ) time.

In GWAS data, the value of n ranges from half million to one million and the value of l is up to a few thousands. The value of k, which is the order of interactions we are interested in, rarely exceeds 6. We found that the value of m
                        =20 is sufficient to detect interactions through experiments on simulated data. Hence we can see that the O(lkn) clustering stage and the O(l(km)
                           k
                        ) search stage dominate the running time of the algorithm. With k
                        <4 the clustering stage takes longer than the search stage, and with k
                        >4 the search stage takes longer.

In our experiments, it takes only a few seconds to run one set of simulated data consisting of 2000 SNPs and 4000 samples, and a few hours for one real data set of about 500,000 SNPs and 3500 samples.

@&#RESULTS@&#

In this study, we validate the proposed algorithm both on simulated datasets and on the real data from Welcome Trust Case Control Consortium (WTCCC). All tests are conducted on a 64 bit MS windows platform with 3.3GHz CPU and 8GB RAM.

We consider two categories of simulation scenarios: the interaction models without marginal effect and those with marginal effect. The models with marginal effects assume at least one SNP that can be discovered through a single locus test. In contrast, the experiments without marginal effect have no SNP that can be discovered through a single locus test. These without-marginal effect data is sometimes called epistasis data. The interaction between SNPs with marginal effects can be epistasis when SNPs have interactive associations to a disease.

The proposed algorithm denoted by MIC is compared with the state-of-the-art algorithms of SNPRuler (Wan et al., 2010b), SNPHarvester (Yang et al., 2009), TEAM (Zhang et al., 2010) and BOOST (Wan et al., 2010a). Since BOOST and TEAM are designed for two-locus interactions and not suitable for higher-order interactions, these two algorithms are compared only for two-locus interactions through previously reported results in Wang et al. (2011). Therefore, our focus is more on the comparison with SNPRuler and SNPHarvester in this study. We ran SNPRuler with chunksize 50,000, pruning ratio 0 and depth equal to the number of true causative SNPs. SNPHarvester was run with the number of iterations (successive run) of 20, and both min
                        k and max
                        k were equal to the number of true causative SNPs.

Power is used as an evaluation measure, which is defined as the proportion of 100 datasets on which the top prediction matches the ground-truth (Wang et al., 2011). The execution time of each algorithm is also compared.

The synthetic datasets without marginal effect are downloaded from the website http://discovery.dartmouth.edu/epistatic_data (Velez et al., 2007). The datasets are based on commonly used epistasis models under 70 different settings from five different two-locus models, two minor allele frequency values at 0.2 or 0.4, and seven heritability values at 0.01, 0.025, 0.05, 0.1, 0.2, 0.3 or 0.4 (Velez et al., 2007). For each parameter setting, 100 datasets are generated with each sample size of 200, 400, 800, and 1600. Each dataset contains 1000 SNPs. More details about the simulation models are described in Supplementary 1 Tables S1 and S2.

Overall, the proposed MIC consistently and significantly outperforms four benchmark algorithms of TEAM (T), BOOST (B), SNPHarvester (SH) and SNPRuler (SR). For example, the median powers of MIC at different sample sizes of 200, 400, 800, 1600 were 0.50, 0.98, 1.0 and 1.0, respectively, while those of the best performing benchmark algorithm reported in Wang et al. (2011) were about 0.48, 0.8, 0.9 and 1.0, respectively (in Supplementary 1 Fig. S3). To refine the performance analysis, we re-ran SNPRuler and SNPHarvester on the same dataset in our side with the parameter setting specified above. It confirmed the superior overall performance of MIC over these algorithms (Fig. 2
                           ).

In Fig. 3
                           , we compare the performance with respect to different heritability values and MAFs. For all methods, the performance degrades as the heritability or MAF becomes smaller. Across all the MAFs and heritability values, MIC shows the best performance and SNPHarvester comes the second. We notice that this re-run performance of both SNPRuler and SNPHarvester is better than that reported in Wang et al. (2011) even on the same dataset (Fig. 2 versus Supplementary 1 Fig. S2). We believe it is because we use a better parameter setting for SNPRuler and SNPHarvester.

We then validate the proposed method on datasets with marginal effect. We consider 8 different simulation models under varying orders of interactions: four different three-locus interaction models (Model 1–Model 4), one six-locus model (Model 5) and three different two-locus models (Model 6–Model 8). All datasets contain 2000 SNPs and 2000 cases/2000 controls. These are from widely adopted disease loci models in previous studies (Zhang and Liu, 2007) or expanded from the existing models as will be explained below. More details of the interactions models are also described in Supplementary 1 Table S3.

We first consider three-locus interaction scenarios. Model 1 corresponds to the model used in Zhang and Liu (2007) for BEAM. This data consists of 24 different settings, two different LDs at 0.7 or 1.0, three minor allele frequency (MAF) values at 0.1, 0.2 or 0.5, and four α’s at 0.5, 1.5, 4.0, or 10.0. The result is summarized in Fig. 4
                           . Overall, MIC shows the best performance. The performance of SNPHarvester is greater than SNPRuler except for a few cases such as LD of 0.7, α at 10.0 and MAF of 0.2. The performance tends to degrade as α increases, LD decreases, and MAF decreases.

For more validation on three-locus interaction cases, we expand the widely used two-locus models denoted by Model 6, Model 7, and Model 8(Wang et al., 2011) into three-locus models: Model 2, Model 3, and Model 4, respectively. Model 2 exhibits multiplicative effect between and within loci, Model 3 multiplicative effect between loci, and Model 4 threshold effect. Under each model, datasets are generated using different values of λ (lambda: marginal effect size) at 0.2, 0.3 or 0.5, and three minor allele frequency (MAF) values at 0.1, 0.2 or 0.5. The results on Models 2–4 are presented in Fig. 5
                           . Since the overall power of SNPRuler was almost zero or too low compared to that from MIC or SNPHarvester across all the datasets, we exclude the results from SNPRuler. MIC shows mostly superior or comparable performance to that of SNPHarvester.

Next, we apply MIC to datasets under six-locus interaction model, Model 5, presented in Zhang and Liu (2007). The data consists of six settings, two LD values at 0.7 or 1.0 and three MAF values at 0.1, 0.2, or 0.5. In addition to the original model in Zhang and Liu (2007) where θ is set to 50, we also test the datasets with the half-sized marginal effect (θ
                           =25). In this experiment, SNPHarvester and SNPRuler did not terminate after three days of run on a single dataset, so we report the result only from MIC in Fig. 6
                           . Our method produces a comparable result to the previous report (Zhang and Liu, 2007). Note that this result is obtained in a substantially less amount of time (240s per dataset on average) than those in Zhang and Liu (2007) since BEAM is known to run much slower than SNPHarvester or SNPRuler.

Finally, we consider the two-locus models, Model 6, Model 7, and Model 8, for validation in simple two-locus interaction detection. These models have been widely used for generating simulated data to demonstrate the powers of various algorithms for detecting epistatic interactions in GWAS (Zhang and Liu, 2007; Wan et al., 2010b; Yang et al., 2009; Wang et al., 2011; Marchini et al., 2005).

Again, SNPRuler is excluded because SNPRuler has substantially lower performance than SNPHarvester in overall cases. MIC and SNPHarvester show largely comparable performance overall (Fig. 7
                           ).

We compare the empirical running time of each algorithm. The execution time is measured as an average of one hundred executions per dataset.

As summarized in Tables 3 and 4
                           
                           , MIC is orders of magnitude faster than the other algorithms. Specifically, on datasets without marginal effect assuming two-locus interactions, it takes about 0.31s for MIC to run with 1600 samples and 1000 SNPs, while SNPHarvester and SNPRuler terminates after about 12 and 7s, respectively. On average, MIC is about 40 times faster than SNPHarvester and 30 times faster than SNPRuler in case of data without marginal effects (Table 3). For higher-order interactions, this gap becomes more significant. For example, the execution time measured on three-locus interactions with marginal effects (Models 1–4) is summarized in Table 4. The average execution time of MIC, SNPHarvester, and SNPRuler is 1.85, 147.25, 78.5s, respectively. The run of MIC for six-locus interaction (Model 5) takes about 240s on average. This result highlights the computational efficiency of the proposed MIC without sacrificing the detection power for high order interactions.

We applied the proposed MIC on the real data from Welcome Trust Case Control Consortium. This data consists of one control dataset and seven patient datasets from seven diseases of bipolar disorder (BD), coronary artery disease (CAD), Crohn's disease (CD), hypertension (HT), rheumatoid arthritis (RA), Type 1 and 2 Diabetes (T1D, T2D). Each data has about 500,000 SNP's genotypes. The control data has 1504 samples and each patient dataset contains around 2000 samples. We run MIC by using each of the disease data as a case data. The same control data is used for each test.

Each dataset is investigated to find k-locus interactions for k
                        =2–5. We choose the top five ranked k-locus interactions. The candidate set size m is set to 20 with which it takes less than a day for MIC to run all the two- to five-locus experiments for a single disease. We demonstrate the results on Bipolar Disorder (BD) and Type 1 Diabetes (T1D) in this section. The results on all the seven diseases are included in Supplementary 2 Tables S1–S7.

In Tables 5 and 6
                        
                        , we report the top-ranked SNP set identified from the five-locus interaction experiments for Bipolar Disorder and the three-locus interaction for T1D. The first three columns of the table correspond to each SNP's RS number, chromosome number, and the gene containing or close to the SNP. We find the related gene by batch query or manual search in dbSNP (Sherry et al., 2001). In addition, we also show each SNP's marginal effect by the rank and the corresponding percentage from a single locus test (column 5). Mutual information value in column 6 is the objective function of our method. We perform the permutation test on the detected SNP combinations in Tables 5 and 6 with 108 random shuffles. No SNP combination had mutual information higher than the reported ones. We additionally obtained the p-value of zero from chi-square test as a reference. Since chi-square test is not reliable in case of high-order interactions, a more efficient permutation test scheme would be needed to measure statistical significance of the detected SNPs.

In general, the identified interactions by MIC contain many SNPs that have previously shown to be in association with the corresponding or related diseases. Some evidence for the validity of the SNP set reported in Table 5 for five-locus interaction on BD include the followings:
                           
                              •
                              Association between FYN (rs4947143) gene and bipolar disorder is already reported (Szczepankiewicz et al., 2009a,b; Rybakowski et al., 2012). Association between C15orf53 (rs7168730) gene and bipolar disorder has also been established (Kranz et al., 2012).

Two SNPs (rs4947143 and rs7168730) are detected in all the two-locus to five-locus experimental results. Moreover, the two SNPs are in the same set except for the four-locus interaction test. This indicates strong interaction between these two SNPs. It is found that both of these SNPs are involved in T-cell receptor signaling pathway (in Supplementary 1 Fig. S3). This may imply indirect association between this pathway and Bipolar Disorder through these SNPs. We find pathway information at KEGG pathway (Kanehisa et al., 2000) through DAVID (Huang da et al., 2009a,b).

The SNP rs384497 about 300kbp away from SNP rs41419445 has function related to brain (Seshadri et al., 2007) and the SNP rs10858396 about 150kbp away from SNP rs41424148 has relation to attention deficit disorder with hyperactivity (Mick et al., 2011). We conjecture these SNPs may have indirect or weak association with Bipolar Disorder.

We observe that MIC identifies interacting SNPs with various levels of marginal effects. For example, the first SNP rs41419445 and the third SNP rs41488349 in Table 5 have the largest marginal effect (rank=1 and 2, respectively) while the second SNP rs4947143's percentage is close to 0.5, which means median strength and hence relatively weak marginal effect. In the two-locus interaction results (in Supplementary 2 Table S1), the first pair of SNPs again shows very strong marginal effect (with ranks 1 and 2). In contrast, both SNPs in the second pair have weak marginal effect, but the combination of them makes strong association, which implies epistatic interaction between them. Furthermore, in the two-locus interaction results (Supplementary 2 Table S7), SNPs of the top-ranked pair have weak marginal effects individually, but the combination of them shows the strongest interaction effect. These instances demonstrate that MIC works competently on identifying interacting SNPs with varying degrees of marginal effects.


                        Table 6 shows the top-ranked SNPs in the three-locus interaction results on Type 1 Diabetes. The first (rs7168730) and second (rs4947143) SNP also appeared in the result for BD in Table 5. The two SNPs are in T-cell signaling pathway and the association between T-cell signaling pathway and Type 1 Diabetes is reported in Stechova et al. (2012). The third SNP (rs9273363) is mapped to HLA-DRB1 gene that is one of MHC-II involved in type 1 diabetes mellitus pathway (in Supplementary 1 Fig. S4). In type 1 diabetes mellitus pathway, T-cell receptor signaling pathway is located in the upper right and this supports that the three SNPs have strong interaction and have strong association with type 1 diabetes.

In summary, our results include highly relevant SNPs confirmed by many previous reports. Another example from coronary artery disease experiment includes ATXN2 gene mapped to SNP rs41500449 (in Supplementary 2 Table S2). The association between ATXN2 gene and coronary artery disease is already reported in WTCCC data experiments (Ikram et al., 2010). The interactions and SNPs that have not previously reported would need further investigation for the biological relevance.

@&#DISCUSSION@&#

In this paper, we proposed an efficient two-step approach to fast detection of genetic interactions. In the first step, k-means clustering is applied to SNPs by using mutual information between a SNP pair and the disease state as a distance measure. Therefore, SNPs that are likely to interact for the disease are clustered into different clusters. Then, a SNP is selected as a candidate if it has high probability of interaction with (k
                     −1) centroid SNPs in the other clusters that the SNP does not belong to. By doing this, we can pre-select those SNPs that are more likely to interact with another (k
                     −1) SNPs in substantially reduced time.

In the second step, selected SNPs are tested for k-locus interaction by exhaustive search. In our initial study, we tried to test k-SNP combination in which each SNP belongs to a distinct cluster only. However, we found that some candidate SNPs belonging to the same cluster still happened to be involved in k-locus interaction. This discrepancy is partly due to the fact that in the clustering step, only the pairwise information between two SNPs is considered while we eventually need k SNPs’ mutual information with the disease state. Thus, we search the k-SNP interactions by exhaustive search among the selected candidates. This substantially increased the power of our algorithm. Note that the exhaustive search is done only in the second step dealing with a relatively small number of pre-selected candidate SNPs.

The clustering-based filtering step greatly improves the computational efficiency of the proposed method. The execution time of the filtering step is linear in the interaction order k, and not exponentially as in exhaustive search. This is one of the key contributing points of our algorithm over previous approaches. Through our simulation study, we also showed the effectiveness of the mutual information based filtering step and the mutual information based objective function for selecting causative SNPs to the disease state.

The proposed method does not build upon any assumption about the underlying SNP distribution or the epistatic model, and thus enjoys more flexibility. Moreover, many previous approaches such as SNPRuler rely on (k
                     −1)-way interaction results to identify k-locus interactions. This may ease the computational burden but at the cost of losing k-locus interaction that is not presented in (k
                     −1)-locus interaction. Our approach is different in that it directly handles information about k-locus interaction using mutual information between k-SNP combination and the disease state.

One limitation of our method is that it is not from a parametric assumption and hence does not return a p-value. One remedy is to check the significance level by chi-square test based on the identified SNPs and the resulting contingency table. Permutation test or existing method for computing p-values from mutual information (Wu et al., 2009) may be used as well. And mutual information is not a strict distance metric, and therefore it tends to limit the intuitive explanation of the clustering result. We may explore other variations of mutual information as an alternative distance measure in our future work.

@&#CONCLUSION@&#

In this study, a simple yet powerful two-step approach is proposed for fast detection of high order epistatic interaction. We thoroughly validated the method on both synthetic data and real data. Our analysis supports the utility of the proposed method for real GWAS in terms of speed as well as the accuracy. Our algorithm makes it possible to detect high order epistatic interactions in GWAS in a matter of hours on a PC.

SL, HJ, JL and KW designed the study. SL and HJ implemented the idea and performed the experiments. SL, HJ, KW and KS developed the idea and performed the analysis. SL, JL, KW and KS wrote the paper.

All authors declared that there is no conflict of interest in this research.

@&#ACKNOWLEDGEMENTS@&#

This research was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education 
                  NRF-2012R1A1A2042792.

This study makes use of data generated by the Wellcome Trust Case Control Consortium. A full list of the investigators who contributed to the generation of the data is available from www.wtccc.org.uk. Funding for the project was provided by the Wellcome Trust under award 076113.

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.compbiolchem.2014.01.005.

The following are the supplementary data to this article:
                        
                           
                        
                     
                     
                        
                           
                        
                     
                  

@&#REFERENCES@&#

