@&#MAIN-TITLE@&#RGB-D camera based wearable navigation system for the visually impaired

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A wearable RGB-D camera based indoor navigation system for the blind is proposed.


                        
                        
                           
                           Egomotion estimation is performed using both sparse features and dense point clouds.


                        
                        
                           
                           Probabilistic mapping and traversability analysis for path planning is presented.


                        
                        
                           
                           The system stores and reloads maps to expand coverage area of navigation.


                        
                        
                           
                           The system improves mobility performance complementing the white cane.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Wearable navigation system

Visual SLAM

Assistive technologies for the visually impaired

@&#ABSTRACT@&#


               
               
                  In this paper, a novel wearable RGB-D camera based indoor navigation system for the visually impaired is presented. The system guides the visually impaired user from one location to another location without a prior map or GPS information. Accurate real-time egomotion estimation, mapping, and path planning in the presence of obstacles are essential for such a system. We perform real-time 6-DOF egomotion estimation using sparse visual features, dense point clouds, and the ground plane to reduce drift from a head-mounted RGB-D camera. The system also builds 2D probabilistic occupancy grid map for efficient traversability analysis which is a basis for dynamic path planning and obstacle avoidance. The system can store and reload maps generated by the system while traveling and continually expand the coverage area of navigation. Next, the shortest path between the start location to destination is generated. The system generates a safe and efficient way point based on the traversability analysis result and the shortest path and updates the way point while a user is constantly moving. Appropriate cues are generated and delivered to a tactile feedback system to guide the visually impaired user to the way point. The proposed wearable system prototype is composed of multiple modules including a head-mounted RGB-D camera, standard laptop that runs a navigation software, smart phone user interface, and haptic feedback vest. The proposed system achieves real-time navigation performance at 28.6Hz in average on a laptop, and helps the visually impaired extends the range of their activities and improve the orientation and mobility performance in a cluttered environment. We have evaluated the performance of the proposed system in mapping and localization with blind-folded and the visually impaired subjects. The mobility experiment results show that navigation in indoor environments with the proposed system avoids collisions successfully and improves mobility performance of the user compared to conventional and state-of-the-art mobility aid devices.
               
            

@&#INTRODUCTION@&#

According to the 2014 World Health Organization, 285 million people reported experiencing vision loss. Visual perception plays a very important role in everyday life, and hence visual impairment adversely affects several daily activities such as ambulating familiar and unfamiliar environments. Researches have proved that vision loss severely lowers the mobility of the visually impaired (Golledge et al., 1997; Turano et al., 2004; West et al., 2002). As a result, more than 30% of the blind population do not ambulate autonomously outdoors (Clark-Carter et al., 1986). Visual impairment also increases the risk of unintentional injuries which often result in medical consequences (Legood et al., 2002; Manduchi and Kurniawan, 2011).

Long canes and the guide dogs have been the most popular navigation aids among the blind. An interview in Manduchi and Kurniawan (2011) also indicated that 12% and 55% of the visually impaired interviewees have used the guide dog and the long cane as a primary mobility aid, respectively. However, the guide dog has very limited availabilities as only about 1500 individuals are estimated to graduate from a dog-guide user program (news service, 1995) every year. The relatively small coverage range and a reactive nature of the long cane still accompany a high risk of collision, because the visually impaired can avoid obstacles only when they make contact with obstacles. Furthermore, both aids do not provide directional information toward a destination.

To overcome the limitations of the conventional navigation aids and improve the orientation and mobility performance of people with vision loss, researchers have proposed Electronic Travel Aids (ETA) utilizing various types of technologies. Over the past few decades, computer vision and mobile robotics technologies have advanced remarkably aided by improvements of vision sensors on measuring 3D structures in terms of an accuracy, price, power profile, and size. This noteworthy improvement in 3D sensing has driven various successful applications in autonomous navigation, manipulation, and human-robot interaction fields. Autonomous robotics navigation and human navigation are different in many respects including sensory systems, spatial representation, and behaviors. However, underlying problems in the two domains are quite similar (Werner et al., 1997). Therefore, we claim that it is very appropriate to transfer these advanced computer vision and autonomous robotics technologies into a wearable navigation system for the visually impaired to improve their mobility.

In this paper, we present a novel head-mounted wearable RGB-D camera based navigation system for the blind built on Lee’s work (Lee and Medioni, 2014). The proposed system provides a visually impaired user with navigation aids to reach a destination in an environment where static and dynamic obstacles are present with the minimum amount of help which is a one time travel from one location to another location located on the same floor of a building given no GPS signals and prior map are given. The proposed system allows a blind user to label places of interest, estimates the pose of the blind user, builds incremental maps of traversed environments, and plan a safe and efficient way point to help a blind user navigate from one location to another location in traversed areas.

The tactile interface device consisting of four micro-vibration motors is designed to help the blind user receive a cue generated by the algorithms at a reasonable cognitive load through a wireless network. Note is that the system is designed to complement the white cane, not to replace it. We believe the white cane is still good a safety mechanism for the blind and using the system in conjunction with the white cane will guide the blind more safely and naturally.

Contributions of this paper are twofold. The first contribution is to propose a wearable navigation system that allows a blind user to label places of interest, builds an incremental map of the traversed environments over time, and helps a blind user to navigate among the labeled places. To the best of authors’ knowledge, this is the first system that performs a SLAM with user interactions allowing a blind user to reuse results of the SLAM algorithm efficiently to improve mobility and orientation of the blind. The system has extended a scope of navigation tasks of existing system (Pradeep et al., 2010) from directing users towards the nearest open space to navigating to a specific destination in a more robust way as presented in Section 4. The second contribution is that we validated and demonstrated the proposed system with both blind and blind-folded subjects. We present experimental results showing that the system improves the mobility performance of the blind subjects and is able to guide the blind subject to a designated goal in unknown environments without colliding to obstacles in a reasonable completion time.

In the rest of this paper, we discuss challenges of wearable indoor navigation system for the visually impaired in Section 2, and related research in electronic travel aids devices to improve orientation and mobilities of the visually impaired in Section 3. Then overview of hardware and software components of the system are described in Section 4 followed by details of the navigation software between Section 5 and Section 7. In Section 8, experimental results are presented and we finish with conclusions and future work in Section 9.

For a successful navigation, localization is an essential step. Localization is a process to find a relative locations and orientation of a system by analyzing measurements from attached sensors with respect to a global reference coordinate or landmarks. Unlike outdoor environments, such a reference from a global positioning system (GPS) is not readily available in indoor environments. Therefore, a main challenge in indoor localization method is how to define such landmarks.

Visual SLAM in robotics and computer vision field has shown very successful performance recently. Visual SLAM approaches build an incremental map of traversed environments and localizes the system within the map simultaneously by detecting and matching visual features. One disadvantage of employing Visual SLAM approaches for real-time applications in large scale is that it is computationally very expensive as the size of features and map grows. Thus, a challenge to use the Visual SLAM approaches is to design an efficient data structure and process to meet the real-time requirement. The performance of Visual SLAM approaches is dependent on the quality of 2D / 3D inputs. It is well understood that a motion from a moving camera attached to a walking person’s head often causes jitter in images. Hence, maintaining accuracy and performing successful navigation tasks using limited hardware resources of wearable navigation system from a moving camera are also a very challenging problem.

Another challenge for the system is to find the most adequate interface to guide the blind subjects based on a signal generated by the system. Speech is one of the most popular way for communication for a higher accuracy. Speech feedbacks usually last more than a second which often causes delays in cue deliveries and constant audio feedback has proved to have very high cognitive loads.

@&#LITERATURE REVIEW@&#

Recently, numerous works have proposed Electronic Travel Aids (ETA) to improve the orientation and mobility of the visually impaired. Orientation and mobility are basic skills for the blind. To be clear, orientation refers to an ability to know where you are and where you want to go in space relative to landmarks in the surrounding environments. On the other hands, mobility regards an ability to coordinate actions to move from one place to another safely and efficiently while maintaining orientation to the destination. ETAs are elaborately designed for the visually impaired to enhance user confidence for independent travel, rather than to replace conventional aids such as the long cane and guide dog completely. Various types of ETAs including ultrasound, laser ranging, or imaging sensors have been proposed (Golledge et al., 1997; Guerrero et al., 2012; Helal et al., 2001; Laurent and Christian, 2007; Li et al., 2012; Martinez et al., 2008; Nicolau et al., 2009). Such devices mainly consist of three basic components: a sensor, a processing system, and an interface. A sensor captures data of surrounding environments in a specific type. Then a system processes this data to generate useful information for the visually impaired user to make a decision. Lastly, an interface delivers this processed information to the person using an appropriate sensory modality. Since target users of the applications are suffering from blindness, the user interface needs to employ sensory modality such as auditory or tactile to convey information, which is called sensory substitution. However, there is currently no standardized or complete system available on the market. In this section, we review previous ETAs for the people with the blindness.

Smart phones have become a popular platform for indoor localization problems with their drastically increasing capabilities in processing and various sensors. Many recent works propose to use an IMU sensor mounted on a smart phone. One advantage of the system is that it provides an inexpensive localization solution if the blind user already owns a smart phone. A recently proposed electronic mobility aids (Apostolopoulos et al., 2014) suggested to use an IMU sensor mounted on a smart phone. The system estimates the step length of a user and the number of steps at each frame and calculates the traveled location of the user using the particle filter which is similar to Li et al. (2012). However, the system assumes that it already has a metric map of accessible areas with some distinctive landmarks. Hence, the system neither operates in unknown environments nor help the visually impaired avoid obstacles.

Reflectance based devices emit a signal, either light or sound, and analyze the reflected signal to detect obstacles. These devices are mainly designed to improve mobility of the blind by detecting and alerting obstacles in the immediate path. Examples include the Mowat Sensor (Pressey, 1977), the Nottingham Obstacle Detector (NOD) (Bissitt and Heyes, 1980), Miniguide (Hill and Black, 2003), and the Binaural Sonic Aid (Sonicguide) (Jones et al., 2004). Researchers have proposed utilizing laser of sensors to replace the white cane (Borenstein and Ulrich, 1997; Yuan and Manduchi, 2005). Virtual white cane (Yuan and Manduchi, 2005) used laser sensor and a vision sensor to detect uneven surfaces and especially staircases. However, these devices only provide local obstacles avoidance functionalities. Therefore, they require significant training, as the lack of spatial context in local range data does not deliver rich information to interpret the environment directly. Such ETAs designed to replace or to be attached to the long cane do not prevent head-level (chest level or higher) accidents effectively. Sonic pathfinder (Dodds et al., 1984) and NavBelt (Shoval et al., 1998) used an array of ultra-sound sensors attached to a belt or mobile robots to provide enhanced obstacle avoidance algorithm. Shoval et al. (1998) used mobile robot obstacle avoidance system as a guidance device for blind and visually impaired people. Just as electronic signals are sent to a mobile robot’s motor controllers, they used auditory signals to guide the blind traveler around obstacles, or alternatively provide an “acoustic image” of the surroundings. In general, reflectance-based systems suffer from traversability and lack of complete user control issues that limit system’s effectiveness.

The main advantage of the computer vision based algorithms is that they are usually infrastructure free and have an ability to build an incremental map based on sensor inputs and and relocalize within the map. In Martinez et al. (2008); Sáez and Escolano (2011), the authors proposed a stereo vision based system that estimates a camera trajectory to predict user motion and build the vicinity 3D maps. The described system, however, detects only overhead obstacles. They are also shoulder- or chest-mounted which makes it difficult for the blind users to scan surroundings and maintain visuomotor coordination. In Pradeep et al. (2010), Pradeep et al. proposed real-time head-mounted stereo-vision based navigation system for the visually impaired. The system is known to be the first complete wearable navigation system that adopted head mounted vision system and tactile feedback prototype which brings many advantages in mobility aids applications for the blind. The head-mounted sensor design enables the visually impaired to stand and scan surroundings easier. The head-mounted device also matches the frame of reference of the person, allowing relative position commands. The tactile interface device helps the blind users receive a resulting cue generated by the algorithms at a reasonable cognitive load through a wireless network for guidance along the safe path.

The release of the Kinect has drawn enormous attentions among researchers and caused a surge in researches in 3D SLAM. RGB-D camera based mobility aid systems are described (Brock and Kristensson, 2013; Zllner et al., 2011). However, they were close to Proof-of-Concept platforms conducting obstacle avoidance tasks without egomotion estimation or building a global map of surroundings which is a key component of navigation system. In Lee and Medioni (2011); 2014), authors extended (Pradeep et al., 2010) and introduced a real-time wearable RGB-D camera based indoor navigation aid for the blind that uses a Primesense RGB-D camera as an input device to overcome some limitations of a passive stereo camera system. The navigation system extends the coverage range and is able to detect obstacles located at the blind sight of the long cane. The experiment results indicated that the tactile vest system combined with the white cane is more efficient at alerting blind users to the presence of obstacles and helping blind subjects steer away from obstacles than the white cane alone.

The proposed system is based on Lee and Medioni (2014). A main difference of the proposed system is that the proposed system enables the system to reuse the results of SLAM algorithms by introducing an efficient data structure and a mapping algorithm that utilizes user’s inputs. We take advantage of the fact that a blind user, unlike autonomous robotics system, is able to provide an approximation of location information for a navigation task using a smartphone application. The proposed system also introduces a keyframe based pose refinement using the ground plane and an efficient iterative closest points (ICP) algorithm for robust navigation performance.

@&#OVERVIEW@&#

The navigation system is composed of a smart phone user interface, head-mounted RGB-D camera, navigation software, and haptic feedback vest as shown in Fig. 1
                        . A head-mounted design has become a popular form factor for wearable device applications (Google, 2013; Microsoft, 2015). The head-mounted design is ideal for scanning around surrounding environments as the visually impaired user traverse to reach a goal. It matches the frame of reference of the person, allowing relative position commands and extends the range of obstacle detection by detecting obstacles located at the blind sight of the long cane. The system is also equipped with a small9DOF MEMS IMU sensor on top of the RGB-D sensor for an orientation initialization purpose only. The IMU sensor communicates with the system at 15 Hz using Bluetooth communication. Lastly, the haptic feedback system consisting of four micro-vibration motors is designed to guide the visually impaired user along the computed path and to minimize cognitive loads.

The navigation software is comprised of several modules, which are a hybrid pose estimation algorithm, mapping algorithm that builds a 2D probabilistic occupancy grid map and integrates multiple mapping results, and dynamic path planning algorithm as shown in Fig. 2
                        . The system first interacts with the blind user to initiate a navigation task. The blind user gives a command to the system with a minimal set of information such as a building name, floor number, the name of a starting location, and the name of a destination. The starting location can be an approximation of the current location of the user as a localization algorithm can correct a minor deviation. A smartphone application shown in Fig. 3
                         helps the visually impaired choose a place from a list of registered places or translates a name of places by speech recognition engines to the system.

If the proposed system finds all information required exist in the database, the system begins performing a real-time navigation module. We adapt the real-time navigation algorithm of Lee and Medioni (2014) for our navigation process. In another case, a blind user travels to the destination with the aid of another (sighted) person available. The system performs pose estimation and map building process while traveling. After the blind subject reaches the destination, the system associates the starting location and the destination with the map. Then the system stores collected information for reuse.

The system stores a 3D map consisting of a set of point clouds of traversed areas, RGB images, poses of keyframes, and adjacency list of each place visited. We believe that 3D information which is viewpoint invariant in addition to RGB images is required in our applications because of a physical limitation of the sensor such as the limited field of view. For example, a visually impaired user may travel an area from an opposite direction to where the system had collected RGB images. 2D image based loop detection algorithms often fail to localize under such drastic viewpoint changes. The system also needs to associate a new set of stored data to the existing map and integrate the information accordingly. In order for a navigation system to handle increasing amount of information at real-time, a scalable and efficient data structure is required. For that purpose, we propose to construct a multi level hash table data structure for a constant read, write, access time. The system also stores an adjacent list of each place labeled by the visually impaired user. This will allow the system to quickly find the connectivity between any two nodes in the system before initiating a real-time navigation task. In the following sections, we discuss details of each module, how we store information, and how we reuse the store information.

Visual odometry is a process of estimating the pose of vision system which has a very successful history in the fields of computer vision and robotics. In order to maintain a sense of egocentricity of the blind user, we continuously estimate the 6 degrees of freedom (DOF) camera pose in the coordinate frame obtained from the initialization step. The hybrid RGB-D pose estimation algorithm is consisted of a real-time FAST corner based real-time motion estimator (FOVIS) (Huang et al., 2011) that keeps track of the camera pose at every frame and ICP algorithm that refines motion at every keyframe using 3D point clouds and the ground plan as presented in Fig. 4
                        .

We begin with a few practical assumptions. First, we assume that the ground plane is observed most of the times. For this purpose, we trained visually impaired users to tilt their head slightly toward the ground plane in experiments. Second, we assume that when a visually impaired user travels to a physical location and labels the place for the first time, the subject is accompanied by someone who can guide the visually impaired user to a desired location.

Roll and Pitch angle of the camera orientation is initialized to be parallel to the ground plane of the world coordinate frame. Using IMU sensor readings as an orientation prior and normals of point clouds obtained using real-time normal estimation algorithm (Holzer et al., 2012), we first obtain the major point clouds that have normals parallel to the gravity vector. Note is that IMU is introduced to speed up the ground plane detection only at the current form. From the set of point of clouds, we find a the plane coefficients using a RANSAC-based least square method and find the major plane 
                              
                                 
                                    
                                       Π
                                    
                                    G
                                 
                                 =
                                 
                                    (
                                    n
                                    ,
                                    D
                                    )
                                 
                                 ,
                              
                            where 
                              
                                 n
                                 =
                                 
                                    
                                       (
                                       A
                                       ,
                                       B
                                       ,
                                       C
                                       )
                                    
                                    T
                                 
                              
                            is a normal vector of a plane. We then find a 
                              
                                 
                                    R
                                    0
                                 
                                 ∈
                                 S
                                 O
                                 
                                    (
                                    3
                                    )
                                 
                                 
                                 such
                                 
                                 that
                                 
                                 
                                    R
                                    0
                                 
                                 ⋅
                                 n
                                 =
                                 
                                    
                                       (
                                       0
                                       ,
                                       −
                                       1
                                       ,
                                       0
                                       )
                                    
                                    T
                                 
                              
                           . Yaw angle is initialized using the magnetometer reading in order to maintain an orientation with respect to one global reference frame. The pose of the blind user is represented by Tn
                            where n represents the frame number.

                              
                                 
                                    
                                       
                                          T
                                          0
                                       
                                       =
                                       
                                          [
                                          
                                             
                                                
                                                   
                                                      R
                                                      0
                                                   
                                                
                                                
                                                   
                                                      t
                                                      0
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      0
                                                      T
                                                   
                                                
                                                
                                                   1
                                                
                                             
                                          
                                          ]
                                       
                                       ,
                                       
                                       where
                                       
                                       
                                          t
                                          0
                                       
                                       =
                                       
                                          
                                             (
                                             0
                                             ,
                                             0
                                             ,
                                             0
                                             )
                                          
                                          T
                                       
                                    
                                 
                              
                           This initialization process is essential for real-time wearable navigation system for two reasons. The first reason is that the initialization step helps the visually impaired maintain consistent orientation with respect to surroundings because the system has the reference coordinate that takes a configuration of the physical world into consideration. Incremental SLAM methods build a map with respect to the camera pose of the first frame. Failures to maintain the orientation of the first frame with respect to the real world causes the navigation system to create an incorrect model, for example, converting the ground plane into obstacles. Another reason is that the ground plane model helps the navigation algorithm to maintain much more stable motion estimation results. By finding the ground plane, the system can estimate changes in roll and pitch angle of the visually impaired user faster and more robustly, which enables the motion estimation algorithm to run faster and more accurate.

We adapt the Fast Odometry from VISion (FOVIS) (Huang et al., 2011) to achieve a real-time frame by frame motion estimation. Every frame begins with an RGB and depth image capture from the camera. The RGB image is converted to grayscale and smoothed using a fixed size Gaussian kernel. Then a Gaussian pyramid is constructed to detect robust features at each Gaussian pyramid level. At each Gaussian pyramid level, FAST corners are extracted. In order to guarantee uniform distribution of features, images at each level is divided into 80 × 80 sub-images. The 25 strongest FAST corners are selected from each sub-image. Fast corners associated with invalid depth are discarded. Generally, the features motion in the image plane is caused by 3D rotation for continuous motion estimation applications. An algorithm proposed by Mei et al. (2009) to compute an initial rotation by directly minimizing the sum of squared pixel errors between downsampled versions of the current and previous frames is used. This initial rotation estimation helps to constrain the search area for feature matching. A 9 × 9 square patch around each FAST corner is used as a feature for matching. Features are matched across frames using sum-of-absolute differences (SAD) as a match score. A graph of consistent feature matches is constructed using Howards approach (Howard, 2008). The graph consists of vertices, a pair of feature matches, and edges that connects vertices. Rigid motions should preserve the Euclidean distance between two features matches over time in a static scene. The 3D Euclidean distance between features matches does not change drastically between consecutive frame. Hence, the set of inliers make up the maximal clique of consistent matches in a static scene. Then the maximal clique in the graph is found using a greedy algorithm (Hirschmuller et al., 2002; Howard, 2008). Initial estimation of 
                              
                                 6
                                 −
                                 −
                              
                           DOF motion is calculated using Horn’s absolute orientation method (Horn, 1987) given the inliers by minimizing the Euclidean distances between the inlier feature matches. The initial motion estimation is further refined by minimizing reprojection errors. Matches with a reprojection error above a threshold will be discarded from the inlier set and the motion estimate is refined once again to obtain the final motion estimate. Some of visual odometry results are shown in Fig. 6
                           
                           .

Frame-to-frame based motion estimation suffers from drifts and error accumulation resulted from . We refine the estimated pose by performing an ICP algorithm at every keyframe to prevent the drifts and error accumulation. Keyframes are extracted when there is a motion from the last keyframe above a threshold or the number of features tracked drops below a predefined threshold value. When a new keyframe is generated, the ground plane, Π
                           i, n
                        , is estimated, if available, in the same manner as the initialization step. We can easily align Π
                           i, n
                         to the ground plane extracted at the initialization step, ΠG
                         using their pl ane equations Fig. 5. It is known that the accuracy of ICP algorithms are affected by an initial pose estimation between two set of point clouds. This initial alignment step using the ground plane reduces matching errors as shown in Fig. 7
                        . This step also reduces the search space from 6D (X, Y, Z, ϕ, θ, ψ) to 3D (X, Z, θ) because two sets of point clouds are aligned on a plane. This initial alignment steps are illustrated in Fig. 7. Then the ICP algorithm finds the transformation between point clouds of the current keyframe, p, and the existing map, q, that minimizes the following plane-to-point error metric developed by Chen and Medioni (1991).

                           
                              (1)
                              
                                 
                                    E
                                    
                                       (
                                       p
                                       ,
                                       q
                                       )
                                    
                                    =
                                    
                                       ∑
                                       i
                                    
                                    
                                       
                                          ∥
                                          
                                             
                                                (
                                                R
                                                
                                                   p
                                                   i
                                                
                                                +
                                                T
                                                −
                                                
                                                   q
                                                   i
                                                
                                                )
                                             
                                             T
                                          
                                          ·
                                          
                                             n
                                             i
                                          
                                          ∥
                                       
                                       2
                                    
                                 
                              
                           
                        where R ∈ SO(3), t, pi, qi
                        , and 
                           
                              
                                 n
                                 i
                              
                              ∈
                              
                                 R
                                 3
                              
                           
                         represent a rotation matrix, a translation vector, the ith element of p and q, and a normal vector of a pi
                        . To further reduce the computation, we create a sliding window based buffer that maintains 3D pointclouds from the last n frames. Because the computation complexity of the ICP algorithm is O(N
                        2), and therefore reducing the number of pointclouds helps to reduce the computation time. Furthermore, we process sequential inputs and the current observation matches to the recent observations as opposed to the outdated observations. Finally, pointcloud p and q are downsampled to a predefined resolution, 5 cm in our case.

Note is that 3D map is a byproduct of the pose refinement using dense point clouds. For the real-time navigation algorithm to perform a probabilistic 2D occupancy grid map and chamfer distance array that stores a distance to the closest obstacles are also constructed. The detail of the 2D probabilistic occupancy grid map and chamfer distance array will be discussed in Section 6.

Even though path planning is performed on 2D space to reduce computation complexity in a subsequent stage, traversability is analyzed in 3D space. This allows the visually impaired to prevent collisions, since the system is able to also detect obstacles in 3D space that may be located in blind spots of the long cane. A 3D local voxel grid map (LVGM), a quantized representation of 3D point clouds, is generated with respect to the camera reference frame as represented in Fig. 8. Using the estimated camera orientation, we align the 3D LVGM in Fig. 8 with respect to the global reference frame at each frame as shown in Fig. 8. The aligned 3D LVGM is classified into occupied, free, and unknown states. Invisible voxels that are out of field of view of the camera or obstructed by another voxel are classified as unknown states. The state of the rest voxels, either occupied or free, is determined based on the number of point clouds in each voxel. Green and red voxels in a upper left corner of Fig. 8 represent free and occupied state voxels, respectively while light blue colored voxels are the voxel with unknown states. These voxels are further classified into vertical and horizontal patches which is a projection onto the local traversability map (LTM), local 2D local grid space parallel to the ground plane, centered at 
                           
                              O
                              c
                              ′
                           
                         as shown in a upper left corner of Fig. 8. If one or more occupied voxels projected onto a cell on the LTM, the cell is registered as a vertical patch. On the other hands, cells that free voxels fall into are registered as a horizontal patch. Green cells and red cells in the bottom of a upper left corner in Fig. 8 indicate horizontal patch and vertical patches on LTM, respectively.

Head-mounted cameras for the navigation often faces an spatial configuration where the most of point clouds from a RGB-D sensor are located beyond a certain distance as the goal of the system is to guide the visually impaired to steer the subject away from possible collision threats such as walls and obstacles which act as a reliable source when estimating motion of the camera. Hence, depth values from the head-mounted platform tend to be quite noisy since the noise characteristic of the RGB-D sensors is proportional to the quadratic of distance to a point (Nguyen et al., 2012) We have empirically found out that a single vertical patch without neighboring vertical patches or a isolated horizontal patch surrounded by neighboring vertical patches generally result from noisy sensor reading. Hence, we apply a simple filtering algorithm using 2D image processing techniques, erosion and dilation, for noise reductions on LTM. Fig. 9
                         shows a simple example how this erosion and dilation process fills a hole on a local grid map that is likely to be misinterpretation of the environment introduced by sensor noise.

Then the vertical and horizontal patches in the LTM are transformed to the world reference frame to update the occupancy probabilities of the 2D GTM as displayed in 8. The occupancy probability of each cell of GTM is updated by occupancy grid mapping rule suggested by Moravec and Elfes (1985).

                           
                              (2)
                              
                                 
                                    P
                                    
                                       (
                                       n
                                       |
                                       
                                          z
                                          
                                             1
                                             :
                                             t
                                          
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          [
                                          1
                                          +
                                          
                                             
                                                
                                                   1
                                                   −
                                                   P
                                                   (
                                                   n
                                                   |
                                                   
                                                      z
                                                      t
                                                   
                                                   )
                                                
                                             
                                             
                                                
                                                   P
                                                   (
                                                   n
                                                   |
                                                   
                                                      z
                                                      t
                                                   
                                                   )
                                                
                                             
                                          
                                          
                                             
                                                
                                                   1
                                                   −
                                                   P
                                                   (
                                                   n
                                                   |
                                                   
                                                      z
                                                      
                                                         1
                                                         :
                                                         t
                                                         −
                                                         1
                                                      
                                                   
                                                   )
                                                
                                             
                                             
                                                
                                                   P
                                                   (
                                                   n
                                                   |
                                                   
                                                      z
                                                      
                                                         1
                                                         :
                                                         t
                                                         −
                                                         1
                                                      
                                                   
                                                   )
                                                
                                             
                                          
                                          
                                             
                                                
                                                   P
                                                   (
                                                   n
                                                   )
                                                
                                             
                                             
                                                
                                                   1
                                                   −
                                                   P
                                                   (
                                                   n
                                                   )
                                                
                                             
                                          
                                          ]
                                       
                                       
                                          −
                                          1
                                       
                                    
                                 
                              
                           
                        For efficient computation in updating, we use logOdds() notation which can be directly converted into probabilistic values when needed. As stated in Hornung et al. (2013), this notation replaces multiplication with addition which is more efficient in computation.

                           
                              (3)
                              
                                 
                                    L
                                    
                                       (
                                       n
                                       |
                                       
                                          z
                                          
                                             1
                                             :
                                             t
                                          
                                       
                                       )
                                    
                                    =
                                    L
                                    
                                       (
                                       n
                                       |
                                       
                                          z
                                          
                                             1
                                             :
                                             t
                                             −
                                             1
                                          
                                       
                                       )
                                    
                                    +
                                    L
                                    
                                       (
                                       n
                                       |
                                       
                                          z
                                          t
                                       
                                       )
                                    
                                 
                              
                           
                        One disadvantage of the update policy represented in (3) is that it requires as many observation as had been integrated before to obtain the current state. In order for the system to respond to dynamic changes in the environment immediately and overcome the overconfidence in the map, the upper and lower bound, lmax
                         and lmin
                        , of the logodds values is enforced using clamping update policy proposed by Yguel et al. (2008).

                           
                              (4)
                              
                                 
                                    L
                                    
                                       (
                                       n
                                       |
                                       
                                          z
                                          
                                             1
                                             :
                                             t
                                          
                                       
                                       )
                                    
                                    =
                                    m
                                    a
                                    x
                                    
                                       (
                                       m
                                       i
                                       n
                                       
                                          (
                                          L
                                          
                                             (
                                             n
                                             |
                                             
                                                z
                                                
                                                   1
                                                   :
                                                   t
                                                   −
                                                   1
                                                
                                             
                                             )
                                          
                                          +
                                          L
                                          
                                             (
                                             n
                                             |
                                             
                                                z
                                                t
                                             
                                             )
                                          
                                          ,
                                          
                                             l
                                             
                                                m
                                                a
                                                x
                                             
                                          
                                          )
                                       
                                       ,
                                       
                                          l
                                          
                                             m
                                             i
                                             n
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        Cells on the probabilistic global traversability map whose occupancy probability is lower than P(lmin
                        ) are registered as traversable while cells with higher occupancy probability than P(lmax
                        ) are registered as non-traversable, where P(lmin
                        ) and P(lmax
                        ) are corresponding probability value of lmin
                         and lmax
                        , respectively. All cells are initialized to have unknown state. Once the state of a cell changed to either traversable or non-traversable from unknown, cells whose occupancy probability value falls between P(lmax
                        ) and P(lmin
                        ), are registered as unknown areas.

We also build and update a chamfer distance array that stores distance to the closest obstacle corresponding to the global 2D traversability map which is used to verify a risk of collision instantaneously. Eq. 5 defines the distance to the closest obstacle. i and j represent row and column index of a 2D grid map. xobs
                         and yobs
                         represent a column and row index of the closest obstacle of a cell at (i, j).

                           
                              (5)
                              
                                 
                                    
                                       d
                                       c
                                    
                                    
                                       (
                                       i
                                       ,
                                       j
                                       )
                                    
                                    =
                                    
                                       m
                                       i
                                       n
                                       (
                                       |
                                    
                                    
                                       x
                                       
                                          o
                                          b
                                          s
                                       
                                    
                                    −
                                    
                                       i
                                       |
                                       ,
                                       |
                                    
                                    
                                       y
                                       
                                          o
                                          b
                                          s
                                       
                                    
                                    −
                                    j
                                    
                                       |
                                       )
                                    
                                 
                              
                           
                        The Fig. 10
                         shows an example of the chamfer distance array given 3 obstacles which can be defined in the traversability analysis process.

After a navigation task is finished, 
                           
                              M
                              =
                              {
                              
                                 M
                                 
                                    3
                                    D
                                 
                              
                              ,
                              
                                 M
                                 
                                    2
                                    D
                                 
                              
                              ,
                              G
                              ,
                              C
                              ,
                              
                                 N
                                 s
                              
                              ,
                              
                                 N
                                 d
                              
                              }
                           
                         is stored where M
                        3D
                        , ~M
                        2D
                        , G, C, Ns
                        and ND
                         represent a 3D map, 2D probabilistic occupancy grid map, Ground plane equation, chamfer distance map, starting node, and destination node, respectively, as shown in Fig. 12
                        
                        . A node is represented as 
                           
                              N
                              =
                              {
                              l
                              a
                              b
                              e
                              l
                              ,
                              
                              L
                              ,
                              
                              x
                              ,
                              
                              y
                              ,
                              
                              I
                              }
                           
                         where L, ~x, ~y indicates an adjacent list of nodes on the same floor, x, y location of the node, and the map ID. There may exist multiple maps on the same floor if a blind user has traveled different places without making a loop as will be discussed in Section 6.4.2. Hence, map ID is required to find out which map the node belongs to. This information is essential to find connectivity between nodes to make a decision on whether to perform a single session SLAM or the real-time navigation process. Each node is identified by its label that the blind user assigns so that search from the stored information becomes easier for the blind user. Nodes are stored in a multi level structure using a building, floor, and a node’s name. For example, a conference room and kitchen on the second floor of PHE building are stored under the same building and floor as shown in Listing 1
                        .

The system needs to be able to integrate results created by multiple single session SLAM tasks as shown in Fig. 11. The idea of the mapping algorithm is similar to the Multi-session SLAM algorithm introduced in Labbé and Michaud (2014); McDonald et al. (2011). Multi-session SLAM addresses the problem of integrating results of multiple SLAM tasks carried out over time repeatedly in the environments with overlaps. The goal of the mapping algorithm in our application is to combine multiple maps that have partial overlaps under uncertainties and estimate a consistent map in a common metrical coordinate system in a robust and efficient manner. In order to tackle this problem, we take an advantage of the fact that a user provides an approximated locations by labels. Another goal is to achieve a real-time performance under increasing amount of information. The VI user shall visit different places over time, for example, home, workplaces, schools, grocery stores, and so on. It is obvious that the amount of information the system needs to maintain grows as the number of places a VI user has visited increases. One of requirements for such a system is an efficient and scalable data structure so that the system handles user’s requests in real time. In the following sections, we introduce an efficient multi level hash table data structure for a mapping for multiple SLAM results and describe how to merge the new information to an existing map.

Let us assume that the blind user requests a new navigation task and the system contains some maps in its data structure. The system classifies locations using a multi-level structure consisting of building, floor, and node levels of which elements are identified their name. The multi-level structure enables more efficient searches and alleviates labeling issues. For instance, a location name that repeatedly appears in multiple floors or buildings, such as elevator, can be reused without further manipulation. The system must firstly determine whether it is capable of guiding a blind user from the current location to a destination or needs to collect more information. Note is that nodes on the same floor in the same building are considered to be reachable in one navigation task in the current system. For that, the system queries if the requested building, floor, and nodes exist in the map by their label. If the system finds the current location and destination in the data structure, connectivity of two nodes are further examined using the adjacent list information of each node. Once the connectivity is established, the system initiates the real-time navigation algorithm presented. Or the system performs another single session SLAM to build a map of the unexplore area. At the end of each single session SLAM task, the system also associates new information to an existing map in the same manner using labels. The system merges the new information to an existing map found to have a common node. Detail of the map merge process will be discussed in the next section. Evaluations of the query time of a node, time to find a connection between two nodes in a map, and memory consumption of the data structure is illustrated in Fig. 14
                           
                           . A search operation in a sequential array has O(log (n)) complexity. A hash table performs a very efficient search, insertion, and deletion at O(1) complexity in average. However, it consumes more space in memory to reserve an enough bucket size to minimize collisions in hashing. The system avoids the problem by storing the number of items at each level in an xml file as shown in Listing. 1. Hence, when loading map data from an offline file when initiating the program, the number of buildings, floors, and places can be read first, and the system reserves the appropriate bucket size to avoid hash collisions and minimize the memory occupation at the same time. By implementing the efficient multi-level hash table, the system can access or modify a node in the map at the O(1) as opposed to O(log (nB
                           )log (nF
                           )log (nN
                           )), where nB, nF, nN
                            represent the number of buildings in an entire map, floors in the building of interest, and nodes on the floor of interest, respectively. A structure of the multi-level hash table is illustrated in Fig. 13. A connectivity between two nodes on the same floor can be checked in O(n) time in the worst case, where n represents the number of nodes on a floor, if a hash table structure is used. As you can see, time to take to find a query node and a connection between two nodes in a map remains almost constant when there is a large number of elements in the data structure. The query time has increased in some cased as the size of data structure increases, for example when 
                              
                                 B
                                 =
                                 10
                                 ,
                                 F
                                 =
                                 10
                              
                           . However, the increment in time is negligible compared to the scale changes of node size. Memory consumption increases depending on the size of buckets required for a stable hashing which is pre-allocated as described above so that no rehasing is required during online operations.

When a new map finds an associated node called an anchor node in an existing map, two maps can be merged. The system calculates a rigid transformation between two maps around an anchor node and verifies a geometric consistency. If an error, E(p, q), obtained from Eq. (1) is greater than a threshold value, θmerge
                            two maps are not merged and stored as separate maps. For example, the system associates two nodes, S
                           2 and G
                           1, named ‘printer’ on the same floor in a building after traveling from S
                           2 to G
                           2 in Fig. 11.

When two nodes are verified to be the same location, the system merges the new map into a n existing one Fig. 15. We use 3D point clouds instead of RGB images for place matching unlike (Labbé and Michaud, 2014; McDonald et al., 2011) to achieve robust matching results under severe view point changes. It is important in our application because the navigation system often encounters a scenario where RGB images have little or no overlaps between two maps due to limited field of view of the RGB-D sensor. A good example can be a case where a blind user travels from point A to B and wants to come back to point A from point B. However, 3D point clouds are invariant to the field of view and hence more suitable in our applications. As stated earlier, the computational complexity of an ICP is O(n
                           2) where n is the number of points. Therefore, we reduce the number of point to be used in ICP by extracting two set of k nearest neighbors p and q near the anchor node.

2D and 3D maps built in a single map building task accumulates errors caused by pose estimation and sensor noises which in turn causes deformation of maps. The current implementation merges two maps using a rigid transformation only when finding overlaps between two separate map. Deformation which is difficult to correct using rigid transformations only results in inconsistency in map as shown in Fig. 16
                        b. Future direction regarding the issue will be discussed in Chapter 9.

The goal of this process is to generate an efficient, stable and safe waypoint based on the shortest path and traversability analysis information and that can be easily delivered to the tactile feedback system to guide the blind user. Shortest path planning algorithms such as Koenig and Likhachev (2005) provide the most efficient path in distance to reach a destination. However, they do not take physical constraints such as a dimension of a body of a blind user into consideration. Therefore, the shortest path is perceived as unrealistic and cannot be directly used for our application. As stated in Pradeep et al. (2010), ideal trajectories that a navigation system for the blind generates will be trajectories created by the sighted subjects or by blind users guided by sighted guides. Hence, the proposed navigation system aims to generates trajectories close to those of sighted person based on the shortest path algorithm for efficiency and the chamfer distance map for safety. Furthermore, delivering the entire path to a blind user causes another difficult user interface issue. According to Nicolau et al. (2009), the blind prefers to build a very simple and precise set of instructions, with very few details or long descriptions about the place or route. Here, we solve this problem by generating a waypoint, an intermediate goal towards destination, to guide the blind subject by creating much sparse representation yet close to the sighted subjects’ trajectories.

The blind subject is assumed to travel on a ground plane. Therefore, the path planning is performed on the global 2D traversability map for computational efficiency. We also build and update a chamfer distance array that stores distance to the closest obstacle corresponding to the global 2D traversability map which is used to verify a direct measure of risk of collision instantaneously. The shortest path is produced using the D⋆ Lite Algorithm as suggested by Koenig and Likhachev (2005) from the current location to the destination. D⋆ algorithm can handle dynamic changes of the surrounding very efficiently. However, subtle changes in the map sometimes result in changes of the generated path in high frequency, which confuses the blind subject. The shortest path produced is a set of cells, DL, on the grid map which connects the current location to the destination without using untraversable cells. Instead of directly following DL, we generate a reliable waypoint Fig. 17, W, that are confirmed to be traversable and located at least some distance from obstacles. In order to generate a waypoint, we find the furthest point in DL which is directly visible and traversable from the current location, D
                        1, and another cell, D
                        2, located furthest in the set DL which is directly visible and traversable from D
                        1. Then search the neighborhood of D
                        1 limited by a predefined radius and find a cell that minimize the cost function, f, given in (7). C
                        
                           i, j
                         and CF
                        
                           i, j
                         represent a cell in ith row and jth column and Chamfer distance value of C
                        
                           i, j
                        , respectively. d(C
                        1, C
                        2) indicatesy distance between two cell, C
                        1 and C
                        2.

                           
                              (6)
                              
                                 
                                    W
                                    =
                                    
                                       argmin
                                       
                                          i
                                          ,
                                          j
                                       
                                    
                                    
                                       f
                                       
                                          i
                                          ,
                                          j
                                       
                                    
                                    
                                    ,
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    where
                                    
                                    
                                       f
                                       
                                          i
                                          ,
                                          j
                                       
                                    
                                    =
                                    
                                       w
                                       1
                                    
                                    ×
                                    
                                       [
                                       d
                                       
                                          (
                                          
                                             C
                                             
                                                i
                                                ,
                                                j
                                             
                                          
                                          ,
                                          
                                             D
                                             1
                                          
                                          )
                                       
                                       +
                                       d
                                       
                                          (
                                          
                                             C
                                             
                                                i
                                                ,
                                                j
                                             
                                          
                                          ,
                                          
                                             D
                                             2
                                          
                                          )
                                       
                                       ]
                                    
                                    +
                                    
                                       w
                                       2
                                    
                                    ×
                                    C
                                    
                                       F
                                       
                                          i
                                          ,
                                          j
                                       
                                    
                                 
                              
                           
                        There should be a case where D
                        2 is not available. In this case, we set D
                        1 to be an temporary waypoint, W. The temporary waypoint is updated when there is motion or rotation greater than a certain threshold , when the current waypoint is not visible from the current location, or the blind subject arrives the vicinity of the waypoint to avoid frequent updates of the generated path and redundant computation.

The tactile feedback instead of audio feedback was used in order to reduce the amount of cognitive loads from hearing, which most blind users rely on for various other tasks. This system has intuitive four different cues for a user as followings:

                           
                              •
                              Straight (no tactile sensors on)

Stop & Scan: Proximity Alert (all tactile sensors on)

Turn left (top–left sensor on)

Turn right (top–right sensor on)

The cue is generated simply based on a relative angle between a direction obtained by drawing a line from the current position to the waypoint and the current heading. In order to prevent frequent changes in cues which often causes confusions from the visually impaired subject, we utilize a hysteresis loop equation that takes the current status of a cue into account as well as a rotation angle required to reach the waypoint, W, as follows.


                        
                           
                              New
                              
                              Cue
                              =
                              
                                 {
                                 
                                    
                                       
                                          
                                             Turn
                                             
                                             Left
                                          
                                       
                                       
                                          
                                             if
                                             
                                          
                                       
                                       
                                          
                                             −
                                             
                                                180
                                                ∘
                                             
                                             <
                                             θ
                                             ≤
                                             −
                                             
                                                θ
                                                2
                                             
                                             ,
                                          
                                       
                                    
                                    
                                       
                                          
                                             Curr.
                                             
                                             Cue
                                          
                                       
                                       
                                          
                                             if
                                             
                                          
                                       
                                       
                                          
                                             −
                                             
                                                θ
                                                2
                                             
                                             <
                                             θ
                                             <
                                             −
                                             
                                                θ
                                                1
                                             
                                             ,
                                          
                                       
                                    
                                    
                                       
                                          Straight
                                       
                                       
                                          
                                             if
                                             
                                          
                                       
                                       
                                          
                                             −
                                             
                                                θ
                                                1
                                             
                                             ≤
                                             θ
                                             ≤
                                             
                                                θ
                                                1
                                             
                                             ,
                                          
                                       
                                    
                                    
                                       
                                          
                                             Curr.
                                             
                                             Cue
                                          
                                       
                                       
                                          
                                             if
                                             
                                          
                                       
                                       
                                          
                                             
                                                θ
                                                1
                                             
                                             <
                                             θ
                                             <
                                             
                                                θ
                                                2
                                             
                                             ,
                                          
                                       
                                    
                                    
                                       
                                          
                                             Turn
                                             
                                             Right
                                          
                                       
                                       
                                          
                                             if
                                             
                                          
                                       
                                       
                                          
                                             
                                                θ
                                                2
                                             
                                             ≤
                                             θ
                                             <
                                             
                                                180
                                                ∘
                                             
                                             .
                                          
                                       
                                    
                                 
                              
                           
                        
                     

For example, let us assume the rotation angle, θ, falls somewhere between θ
                        1 and θ
                        2. If the current cue is Straight, a new cue will still be Straight. On the other hands, if the current cue is Turn right, a new cue will be Turn right until a subject completes rotation so that θ < θ
                        1. In our application, θ
                        1 and θ
                        2 are defined as 5° and 15°, respectively. The generated cue is transmitted through the wireless transmitter to the vest interface that has four vibration motors as provided in Fig. 1. Each vibration motor delivers a specific cue such as turn left, turn right, and so on.

@&#EXPERIMENTS@&#

We present the experimental details and results of mobility experiments of the proposed system . Our navigation system runs at 31 Hz on average on the following configuration.

                        
                           •
                           CPU: Intel(R) Core(TM) i7-3630QM CPU @ 2.40GHz

RAM: 8.00 GB

OS: Windows 7 - 64 bit

The breakdown of processing timing is represented in Fig. 18
                     .

The first experiment estimates 6DOF poses of a blind-folded person navigating an indoor environment as shown in Fig. 19
                           
                           a guided by a sighted guide and compare the results against a state-of-the-art RGBD sparse feature based pose estimation algorithm.

In the experiment, the blind-folded subject makes a loop of a building. While the subject is traveling, the RGBD data has been captured. The captured data has been processed using sparse feature based pose estimation algorithm (FOVIS), and our algorithm firstly. Recently, graph optimization based SLAM algorithm such as Kaess et al. (2012) has shown that the pose estimation can be further optimized. Therefore, we also provide a result of the pose estimation results of the proposed algorithm after performing graph SLAM (Kaess et al., 2012). The error metric used is to measure the differences in distance (translation) after making a loop, which ideally is 0.

                              
                                 (7)
                                 
                                    
                                       E
                                       (
                                       S
                                       ,
                                       F
                                       )
                                       =
                                       ∥
                                       S
                                       −
                                       F
                                       ∥
                                    
                                 
                              
                           where 
                              
                                 S
                                 ,
                                 F
                                 ∈
                                 
                                    R
                                    3
                                 
                              
                            represent 3D location of the start and the finish point. Total traversed distance is 49m. As shown in Fig. 19b, the proposed algorithm reduces drift from 3.674 m to 2.661m by 27.6%.

The goal of the pose estimation algorithm is to capture accurate pose of the blind person while walking from point A to point B. We recruited 6 blind subjects as an additional experiments when the mobility test is performed by Aminat Adebiyi a Ph.D student in Biomedical Engineering department at University of Southern California. Six blind subjects provided a verbal consent that the motion capture data can be used for experiment purpose. Four datasets from subjects out of Six were used for evaluation as there were sensor obstructions by a sighted guidance and a wire of the sensor. Fig. 21
                           a shows the experimental environment and approximated ground truth of subjects. Four subjects were asked to move one corner of a room to an opposite corner in diagonal direction of the room, for example A to C or B to D. Each subjects collected two datasets on different starting points always guided by a sighted user by hand.

Performance of the pose estimation algorithm were evaluated using an absolute and relative error metric. The absolute error (E) was measured by displacement of the estimated location of a destination from the ground truth location, 
                              
                                 
                                    E
                                    =
                                    ∥
                                 
                                 
                                    d
                                    G
                                 
                                 −
                                 
                                    
                                       d
                                       E
                                    
                                    ^
                                 
                                 
                                    ∥
                                 
                              
                            where dG
                            and 
                              
                                 
                                    d
                                    E
                                 
                                 ^
                              
                            represent 3D location of a ground truth location of a destination and corresponding location estimated by the system Fig. 22
                           . The relative error is obtained by 
                              
                                 
                                    E
                                    
                                       d
                                       
                                          t
                                          r
                                          a
                                          j
                                       
                                    
                                 
                                 ,
                              
                            where dtraj
                            represents the total distance of a trajectory. The experiment results show that the system is capable of tracking the movement of the blind under oscillations caused by walking within 6.32% errors of the traveled distance. In the experiments, average traveled distance was 13.93 m and the E was 0.88 m which is close enough distance to reach the destination within a couple of steps.

As shown in Fig. 23
                           , the proposed algorithm outperforms one of the state-of-the-art algorithm, FOVIS (Huang et al., 2011), in most of the walking scenarios. The large errors of FOVIS algorithm results from unreliable 3D information of visual features that the algorithm relies upon for pose estimation Fig. 20. As the visual feature based algorithm uses only a few points that have visual distinctiveness and the depth measurement error of the sensor is known to be increasing quadratically to the depth value, the performance of the pose estimation algorithm start decreasing in case most of detected features are located beyond a distance. On the other hand, utilizing dense point clouds helps to mitigate the problem as a few features in the further distance are not dominant elements any longer, and the pose is refined using evenly distributed across all depth ranges.

We recorded RGBD data using the proposed system from two sighted subjects in eight different scenarios. Each scene has been selected to have different combination of a starting point and destination to test the robustness of the proposed waypoint generation algorithm in different spatial configuration. The system estimates the pose of the camera and builds 2D grid maps as the subject records the RGBD data. Then the system loads the map and the shortest path is generated as presented in the right side of the Fig. 24
                            as if they are performing the same new navigation task. The corresponding path is plotted in red in the left side. The shortest path is the most efficient. However, minimizing traveling distance is not optimal path for a human to follow as there are higher risks of collisions. The estimated trajectories of the sighted subjects are close to the shortest paths. However, they are generally smoother than the shortest path which is expected as the sighted subjects plans collision free paths prior to reach the adjacent area of the obstacles which is what the proposed waypoint generation algorithm aims to achieve.

As presented in Fig. 25
                           , the way point algorithm successfully creates much fewer number of candidates as intermediate goals that are close to the trajectories of the sighted subjects’ trajectories. Quality of waypoint generation is difficult to measure quantitatively. Therefore, we visually confirm similarities between trajectories of the control group and the proposed algorithm and compare the results from the shortest path algorithm as shown in Fig. 25 as presented in Pradeep et al. (2010) at the moment. Quantitative metric to measure the similarity between trajectories (curves) of the proposed algorithm and control group will be researched as a future work.

These experiments are designed to test an ability of the system to guide a blind user in a cluttered environment. In this environment, there exist many obstacles such as tables, chairs, cubicle walls, and a fridge that are not present on the floorplan, which is a primary reason that the system does not rely entirely on the floorplan for mapping and planning and performs a real-time probabilistic map update. Four subjects are blind-folded and asked to reach a destination in three different scenarios, three times per scenario. In the first scenario, the subjects are asked to travel to a destination whose location is known using the white cane and provided the initial orientation towards the destination. This scenario is very similar to what one experiences in an office or at home where you spend most of your time. In the second scenario, the subjects are asked to travel to a destination whose location is known using the white cane and does not know the initial orientation towards the destination. This scenario is designed to simulate a situation often happening in a daily basis even in a very familiar environment when a subject loses a sense of the orientation or gets confused by some changes in the spatial configuration. The last scenario has the same configuration to the second scenario except that the subjects are provided with the proposed navigation system in addition to the white cane. For the last scenario, a database that contains location of interests in the building is assumed to be available at the beginning for all experiments. The database only stores 2D location of interests of a building which is very concise and can be easily appended on top of existing maps, an indoor google map for example. We extract the location of rooms and some points of interests such as elevators, exits, printers, and a fridge from using floor plan as shown in Fig. 26
                         aligning them to a grid where the north points up and store them in a database. You provide the system with the destination and the current location, for example to a printer room from Room 101C, to start a navigation task.

It is obvious that you will get to a destination the fastest when you know the surrounding environments and orientation very well which is not always the case when you travel to different places. One thing to note is that losing a sense of orientation can decrease the mobility performance by about a half in terms of the completion time to reach a goal even in familiar environments which will get probably worse in unfamiliar environments. As you can see in Fig 27
                        , the proposed system improved the mobility performance by 57.77% on average, 79.0% and 36.5% for each destination. This results from an ability of the system that can initialize the orientation using magnetometer reading and help the subjects to align themselves to the existing map.

We have presented an integrated framework using RGB-D as an input to operate real-time SLAM and dynamic path planning in the presence of obstacles in the context of a mobility aids for the blind applications. The proposed pose estimation algorithm that uses sparse features and dense 3D point clouds guided by the ground plane outperforms one of the state-of-the-art sparse feature based pose estimation algorithm that has an absolute error of 3.674 m in a walking scenario by the absolute error of 2.661 m for a 49.0 m trajectory with the blind folded subject guided by a sighted person. The proposed pose estimation algorithm shows the real-time performance given QVGA (320 x 240) image and depth streams. Performance evaluation of the real-time pose estimation with the four blind subjects has been presented. The proposed pose estimation algorithm also provided much accurate result of average error of 0.88 m on 13.93 m trajectories in average. Dynamic path planning in the presence of obstacles is an essential step for a human navigation where the pose of the subject changes and the surrounding environments may also change. In order to overcome a unrealistic realization of the shortest path algorithm and make is easier to generate cues to the blind user, we propose to generate a waypoint based on the shortest path algorithm (Koenig and Likhachev, 2005) and chamfer distance map of occupancy grid. Pilot studies show that the waypoint generated by the system provides trajectories that are close to those of sighted people with a fewer intermediate locations to follow compared to the shortest path generated by D* Lite algorithm. Mobility experiments of the proposed system when used together with the white cane improved the mobility performance of the blind-folded subject by 57.77% in average over when the white cane is used alone. We are planning on adapting a real-time image based localization algorithm because in many cases people visit a few places multiple times and an ability to localize the camera location using an efficient image based algorithm would help to mitigate error accumulation problem and to optimize pose estimation when you revisit a place. We have encountered severe image blurs or poor feature matching led by rapid head rotations sometimes, which caused inaccurate visual odometry results. An image based localization algorithm will also help the system relocalize when the pose estimation algorithm encounters such failure cases. 2D and 3D maps built in a single map building task accumulates errors caused by pose estimation and sensor noises which in turn causes deformation of maps. The current implementation merges two maps using a rigid transformation only when finding overlaps between two separate map. Deformation which is difficult to correct using rigid transformations only results in inconsistency in map. Therefore, we would also like to do research to correct the deformation of the map using a floor plan as a reference. Visual-Inertia (VINS) sensor fusion has been proven to provide much more robust tracking results in dynamic environments due to complementary characteristics of an IMU sensor and cameras (Leung and Medioni, 2014). For this reason, we plan to extend the range of an IMU sensor integration that is utilized only at the initialization stage under the proposed system. There is a room for improvement in path planning algorithm to make the navigation system more practical. For that purpose, we are performing a research to find common walking patterns of the blind subjects that can be characterized in a cost function of the path planning algorithm. Finally, experiments with the real visually impaired subjects in various environments are planned in order to evaluate the effectiveness of the navigation system.

@&#ACKNOWLEDGEMENT@&#

This research was made possible by a cooperative agreement that was awarded and administered by the U.S. Army Medical Research & Materiel Command (USAMRMC) and the Telemedicine & Advanced Technology Research Center (TATRC), at Fort Detrick, MD under Contract Number: W81XWH-10-2-0076.

Supplementary material associated with this article can be found, in the online version, at 10.1016/j.cviu.2016.03.019
                  


                     
                        
                           Supplementary Data S1
                           
                              Supplementary Raw Research Data. This is open data under the CC BY license http://creativecommons.org/licenses/by/4.0/
                              
                           
                           Supplementary Data S1
                           
                        
                     
                  

@&#REFERENCES@&#

