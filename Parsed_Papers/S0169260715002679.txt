@&#MAIN-TITLE@&#Exploiting ensemble learning for automatic cataract detection and grading

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Fundus image analysis provides great potentials for automatic cataract diagnosis.


                        
                        
                           
                           Ensemble learning is exploited for combining multiple models to improve the performance.


                        
                        
                           
                           Three independent feature sets, i.e., wavelet, sketch, and texture based features, are utilized.


                        
                        
                           
                           Two base learning models, i.e., Support Vector Machine and Back Propagation Neural Network, are investigated in the ensemble approach.


                        
                        
                           
                           The experiments demonstrate that our approach outperforms single model significantly.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Cataract detection

Fundus image classification

Ensemble learning

Support vector machines

Neural network

@&#ABSTRACT@&#


               
               
                  Cataract is defined as a lenticular opacity presenting usually with poor visual acuity. It is one of the most common causes of visual impairment worldwide. Early diagnosis demands the expertise of trained healthcare professionals, which may present a barrier to early intervention due to underlying costs. To date, studies reported in the literature utilize a single learning model for retinal image classification in grading cataract severity. We present an ensemble learning based approach as a means to improving diagnostic accuracy. Three independent feature sets, i.e., wavelet-, sketch-, and texture-based features, are extracted from each fundus image. For each feature set, two base learning models, i.e., Support Vector Machine and Back Propagation Neural Network, are built. Then, the ensemble methods, majority voting and stacking, are investigated to combine the multiple base learning models for final fundus image classification. Empirical experiments are conducted for cataract detection (two-class task, i.e., cataract or non-cataractous) and cataract grading (four-class task, i.e., non-cataractous, mild, moderate or severe) tasks. The best performance of the ensemble classifier is 93.2% and 84.5% in terms of the correct classification rates for cataract detection and grading tasks, respectively. The results demonstrate that the ensemble classifier outperforms the single learning model significantly, which also illustrates the effectiveness of the proposed approach.
               
            

@&#INTRODUCTION@&#

Cataract is defined as a clouding of the lens inside the eye leading to impaired vision; it is considered the most common cause of blindness [1–4]. The longer a patient has an untreated cataract, the more severe is the vision impairment. According to a WHO report [2], the estimated number of people in the world who are visually impaired is 285 million: 39 million are blind, and 246 million have impaired vision; 33% of cases of visual impairment and 51% of cases of blindness are caused by cataracts. In low and middle income counties and regions, the prevalence of cataracts is even higher because of lower investment in health [2].

Although it is widely accepted that early detection and treatment can reduce the suffering of cataract patients and prevent visual impairment from turning into blindness, people in less developed areas still do not receive timely treatment because of poor eye care services and lack of professional ophthalmologists. On the other hand, common methods for cataract diagnosis require a slit lamp (e.g., the lens opacity classification system (LOCS III) [4] for clinical assessment or the Wisconsin cataract grading system (Wisconsin System) [5] for photographic grading), which is complicated and expensive for many patients. Therefore, reducing costs and simplifying the process for early cataract diagnosis is a crucial means of improving eye care service in less developed areas and bringing light to cataract patients.

We know that ophthalmologists can diagnose cataracts by checking the clear degree of fundus images [3,7,8]. Fig. 1
                      shows the gradings of cataract fundus images. Fig. 1(a) is an image with no cataract, where the optic disc and large and small blood vessels can be seen very clearly. There are fewer blood-vessel details in the Mild cataract image in Fig. 1(b), while only the large blood vessels and optic disc are visible in the moderate cataract image in Fig. 1(c). Furthermore, virtually nothing can be seen in Fig. 1(d), the severe cataract image. Thus, the task of fundus image classification is to build a classifier to simulate fundus image checking activities for automatic cataract detection and grading [1]. The fundus image classification task is based on the assumption that image blurring is not secondary to technical errors in image acquisition.

The research on fundus image analysis has been widespread [1,7], and several studies on automatic cataract detection and classification have been reported [9–18,42–46]. However, the proposed techniques use a single learning model for fundus image classification.

It is widely accepted that by combining multiple learning models, ensemble learning has greater potential to achieve more accurate classification than any of the constituent learning models [18]. This paper presents our experimental work on the ensemble learning framework for improved cataract detection and grading. To achieve classifier diversity (the key to high quality classification results in ensemble learning [19]), three independent feature sets, i.e., wavelet- [20], sketch- [21], and texture-based [22] features, are extracted from each fundus image. For each feature set, two base learning models, i.e., Support Vector Machine (SVM) [9,23] and Back Propagation Neural Network (BPNN) [24], are used to construct the base classifiers. Then, the ensemble methods [19], majority voting and stacking, are investigated to combine the multiple base classifiers for the final fundus image classification.

The contribution of this paper can be summarized as follows: (1) An ensemble learning-based approach is proposed for fundus image classification, where three independent feature extraction methods (i.e., wavelet-, sketch-, and texture-based methods) and two base learning models are investigated for base classifier building; then, two widely used ensemble approaches, i.e., stacking and majority voting, are employed to combine the multiple base classifier for improved cataract detection and grading. (2) The empirical experiments on the real-world datasets are presented; they illustrate that the best performance of the ensemble classifier can achieve 93.2% and 84.5% accuracy in terms of classification rates for cataract detection (two-class task) and cataract grading (four-class task), respectively. The experiments also demonstrate that the ensemble learning approach outperforms the single learning model significantly. We believe that our experimental study can serve as an important reference for eye disease diagnosis based on fundus image analysis.

This paper is organized as follows. Section 2 discusses the related work. Section 3 describes the details of the proposed ensemble learning approach of fundus image classification. Experiments and evaluation results are reported in Section 4. Section 5 concludes the paper.

@&#RELATED WORK@&#

Studies on fundus image analysis have been conducted for years. Segmentation and location of retinal structures, such as retinal lesions [26–28], vessels [20,28], optic discs [30–33], and aneurysms [32], have been widely studied. Based on these techniques, researchers are also trying to develop diagnostic systems for specific retina-related diseases, including microaneurysms [25], diabetic retinopathy [6, 29, 34–36], age-related macular degeneration [7], glaucoma [37–41], and cardiovascular diseases [41]. These studies are relevant but not closely related to the work in this paper.

From the point of view of automatic cataract detection and grading, the work in this paper is closely related to [8] and [10]. For example, Li et al. made an effort to classify and diagnose specific cataracts automatically by split images and retro-illumination images, including nuclear cataracts [42–45], cortical cataracts [45], and posterior sub-capsular cataracts [46]. Yang et al. [8] used an improved Top-bottom hat transformation to pre-process the retinal image and extracted the luminance and texture message as features; they then constructed the classifier with a two-layered back propagation neural network [24]. The work in [10] proposed a fundus image classification approach for cataract detection using the wavelet transform- and sketch-based methods for feature extraction and multi-class Fisher discriminant analysis [47] for classifier building. All of these studies built a single learning model for fundus image classification. In this paper, however, we adopt an ensemble learning framework for automatic cataract detection, where multiple heterogeneous learning models are combined for more accurate prediction of cataract classification.

The basic process of ensemble learning is to construct multiple base learning models and combine them to solve the same problem [18,38]. If each base learning model is viewed as an expert, multiple experts may be better than any single expert if their individual judgments are appropriately combined. Because the ensemble idea has great potential for reducing the learning bias of base learning models, it may demonstrate better performance in many classification tasks than any single constituent base model [18].

The study of ensemble learning [19] mainly focuses on two tasks: (1) how to construct the multiple base (weak) classifiers with diversity concerns (to ensure that individual classifiers in an ensemble system make different errors in different instances); and (2) how to combine multiple weak learning models to produce a stronger classifier. To apply the ensemble learning approach for high quality fundus image classification, three feature extraction methods, and two popular learning models are adopted in this paper to construct six independent base learning models. Then, the widely used majority voting and stacking methods are investigated to obtain accurate ensembles.

More concretely, the implementation of the proposed approach to ensemble learning-based fundus image classification involves essentially four steps:
                        
                           (1)
                           Fundus image preprocessing: This step resizes all fundus images and extracts the green channel images. The patients’ personal information on these images is erased for the protection of their privacy.

Feature extraction: Three independent feature sets, i.e., wavelet-, sketch-, and texture-based features, are extracted from each fundus image; this is important to create diversity in the base classifiers for ensemble learning.

Base learning model construction: Two base learning models, i.e., Support Vector Machine and Back Propagation Neural Network, are adopted to build the base classifiers. The different learning models themselves are the source of diversity.

Ensemble of multiple base models: Two widely used ensemble methods, majority voting and stacking, are investigated for the fundus image classification task.

The following provides a detailed description of each of the four steps.

Because fundus images from different fundus cameras might have different sizes, the preprocessing step first sizes the fundus images uniformly. Because the green component image can enhance the contrast between the foreground and blood vessel (while keeping most of the details of the original image [48]), the original fundus images are converted from RGB colour space to the green channel. Then, histogram equalization is conducted on the green channel image to further increase the global contrast. In addition, we erase the patients’ personal information to protect the patients’ privacy [11,49]. Examples of the preprocessing of a non-cataractous fundus image and a severe cataract fundus image are shown in Fig. 2
                        . A comparison of the original images with the images after preprocessing shows that, in both examples, the resulting images have greater contrast than the original images, which indicates the potential for enhanced quality of the extracted features for classifier building.

Diversity is a crucial condition for obtaining better performance with ensembles of multiple base learning models [18,51]. To create diversity in the base learning model for ensemble learning, three independent feature sets, i.e., wavelet-, sketch-, and texture-based features, are extracted from each fundus image.

The wavelet transform has been used in fundus image analysis to create contrast between blood vessels and the background [20]. The Haar transform is used in this paper to decompose the fundus image into 3 levels to distinguish the high frequency components in a fundus image, mainly related to the vessels, from the low frequency component representing the background. For example, Fig. 3
                            depicts the distribution of coefficients corresponding to the third level horizontal details of Haar wavelet transformations of the fundus image in different categories.

It can be observed from Fig. 3 that the fundus images from different cataract categories have significantly different amplitudes of coefficients derived from the Haar wavelet transform, which implies that the amplitude range of coefficients can be used as an important feature for cataract classification.

We also borrow from the sketch-based approach [21] to extract features from fundus images for the purpose of building cataract detection and grading classifiers. As shown in Fig. 4
                           , the intensity of the fundus image is sampled uniformly along the 18 radial lines and 5 concentric circles in different radii. When the vessel goes through the sampling lines or circles, it correspondingly forms a local peak. The clearer the details that an image has, the more wide are the fluctuations in the resulting sketch-based sampled data. Fig. 5
                            shows an example of the signals sampled on a sketch line.

By comparing the two sampled signals in Fig. 5(a), it is found that two fundus images from non-cataractous and severe cataract categories show different fluctuations. Fig. 5(b) illustrates that the behaviour of this phenomenon is more clear after the signals are processed using the discrete cosine transform. Therefore, the sketch-based method with the discrete cosine transform is accepted as a feature of fundus image classification.

Texture is a set of metrics reflecting the spatial organization of pixel values of an image [22]. Before texture feature extraction, the improved Top-bottom hat transformation is first employed to improve the contrast between objects and background. Then, the trilateral filter is used to remove noise [8]. Fig. 6
                            shows the resulting images of the trilateral filter operation on the fundus images in Fig. 1. Finally, the following three types of texture features are extracted to represent the fundus image:
                              
                                 1)
                                 The luminance feature refers to the clearness of a fundus image. By using image banalization with the threshold set to 0.6, the luminance feature is represented as the number of white pixels in the image.

The grey co-occurrence matrix uses the spatial relations of similar grey tones to capture numerical features. For a fundus image, we extract 24 features from the degree of 0, 45, 90, and 135 [52]: angular second moment, correlation, entropy, contrast, inverse difference moment, and sum of squares.

The grey-gradient co-occurrence matrix concerns the associated statistic distribution of the grey and edge gradient. The following 15 features can be computed [53]: little gradient dominance, large gradient dominance, grey heterogeneity, gradient heterogeneity, energy, grey average, gradient average, grey mean square error, gradient mean square error, correlation, grey entropy, hybrid entropy, inertia and inverse difference moment.

For each feature set, two base learning models, i.e., Support Vector Machine [23] and Back Propagation Neural Network [24], are built.

Based on the structural risk minimization principle from statistical learning theory, SVM was originally introduced to solve the two-class pattern recognition problem [23]. The main idea is to build a hyperplane that separates the positive and negative examples while maximizing the smallest margin (i.e., to achieve the largest margin from two classes of data). According to [23], SVM has two important advantages: (1) feature selection is often not needed, as it is fairly robust to overfitting and can scale up to high dimensionalities; (2) no effort in parameter tuning is needed, as the theoretically derived “default” choice of parameter setting has been shown to provide the greatest effectiveness.

Neural Network (NN) is a virtual intelligence tool that can be used to simulate the human brain to perform analysis and generate results [24]. This technology has excellent abilities in non-linear mapping, generalization, self-organization, and self-learning. BPNN is a multilayer hierarchical NN with upper neurons fully associated with lower neurons; it is not only the most widely used but also one of the most maturely developed NN technologies [8,24]. Properly trained BP networks tend to give reasonable answers when presented with inputs that have never been seen by the NN. This generalization property makes it possible to train a network on a representative set of input–output pairs and obtain good results without training the network on all possible input–output pairs.

Many studies have demonstrated that the combination of multiple classifiers gives significantly less error than any one of them individually [19]. For the fundus image classification task in this paper, two popular ensemble methods, majority voting and stacking, are investigated for the ensemble of multiple based learning models.

Two widely used voting methodologies for ensemble classification are majority voting and weighted voting [19]. For the former, the class label of a given unlabelled instance x will be the one that obtains the highest number of votes (i.e., the most frequent vote) given by these multiple base classifiers involved in the ensemble. In the latter, weights of voting are varied among the different base learning models. Different policies for tuning the appropriate weights of votes have been proposed in the literature [18], e.g., dynamic classifier selection and weighted linear combination.

As the most common ensemble method, majority voting does not require any parameter tuning once the base classifiers have been constructed [19]. Therefore, it has frequently been used as an ensemble scheme for the comparison study of newly proposed methods.

In this paper, we adopt it as one of the ensemble scheme for fundus image classification. Assuming d(y) is the domain of class label y, y
                           
                              k
                           (x) is the class label of fundus image x assigned by the kth base model, and 
                              
                                 v
                                 (
                                 
                                    y
                                    k
                                 
                                 (
                                 x
                                 )
                                 ,
                                 
                                    c
                                    i
                                 
                                 )
                              
                            is the indicator function:
                              
                                 
                                    
                                       v
                                       (
                                       
                                          y
                                          k
                                       
                                       (
                                       x
                                       )
                                       ,
                                       
                                          c
                                          i
                                       
                                       )
                                       =
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         1
                                                         ,
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            y
                                                            k
                                                         
                                                         (
                                                         x
                                                         )
                                                         =
                                                         
                                                            c
                                                            i
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         0
                                                         ,
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            y
                                                            k
                                                         
                                                         (
                                                         x
                                                         )
                                                         ≠
                                                         
                                                            c
                                                            i
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

The formula to compute c(x) assigned to an unlabelled fundus image x is given as below:
                              
                                 
                                    
                                       c
                                       (
                                       x
                                       )
                                       =
                                       arg
                                       
                                          
                                             max
                                          
                                          
                                             
                                                c
                                                i
                                             
                                             ∈
                                             d
                                             (
                                             y
                                             )
                                          
                                       
                                       
                                          
                                             
                                                ∑
                                                k
                                             
                                             
                                                v
                                                (
                                                
                                                   y
                                                   k
                                                
                                                (
                                                x
                                                )
                                                ,
                                                
                                                   c
                                                   i
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

Unlike majority voting, which assigns the class label by a majority of the multiple base classifiers, the stacking ensemble approach employs a meta classifier that consumes the classification results of multiple base classifiers to generate the final classification [18].

A typical implementation of stacking uses a 2-layer frame structure, where a number of base classifiers are generated from the training dataset in the first level-0 layer. These individual classifiers are then combined by the meta classifier in the second level-1 layer. The training processes of ensemble classifier based on stacking are usually as follows:

Step 1: Train each classifier in the level-0 layer with the method of leave-one-out cross validation and obtain the training dataset of second layer. Given a training set {(x
                           1, y
                           1)…(x
                           
                              m
                           , y
                           
                              m
                           )} and L learning models, in each training process, sample x
                           
                              i
                            is removed from the training dataset, and then, the remaining m
                           −1 training samples are used to train L learning models. By using the obtained L classifiers to classify sample x
                           
                              i
                           , we can obtain L classification results 
                              
                                 (
                                 
                                    y
                                    i
                                    1
                                 
                                 …
                                 
                                    y
                                    i
                                    L
                                 
                                 )
                              
                           . Together with y
                           
                              i
                           , a new training sample 
                              
                                 <
                                 (
                                 
                                    y
                                    i
                                    1
                                 
                                 …
                                 
                                    y
                                    i
                                    L
                                 
                                 )
                                 ,
                                 
                                    y
                                    i
                                 
                                 >
                              
                            is generated. Repeat the above process m times, and the dataset 
                              
                                 {
                                 <
                                 (
                                 
                                    y
                                    i
                                    1
                                 
                                 …
                                 
                                    y
                                    i
                                    L
                                 
                                 )
                                 ,
                                 
                                    y
                                    i
                                 
                                 >
                                 |
                                 1
                                 ≤
                                 i
                                 ≤
                                 m
                                 }
                              
                            is obtained. It will serve as the training data for the next layer classifier.

Step 2: Training level-1 classifier using the training dataset from Step 1, the resulting one will be adopted for generating the final classification result.

The leave-one-out cross validation method in Step 1 needs to train L classifiers m times. A very large m would make the computational cost very high. For such a case, we can divide the training dataset into k parts on average. Then, one part is used as the test data, and the other k−1 parts are used as the training data. In this way, we only need to train the L base models k times. Because k is much smaller than m, the computing cost can be reduced significantly.


                           Fig. 7
                            illustrates the two-layer structure of the stacking-based ensemble approach for fundus image classification. Through the combination of three independent feature sets together with the two base learning models, six base classifiers are constructed in the level-0 layer, where we set k
                           =3. In the second level-1 layer, the output classification results of the six base classifiers in the first layer are used to train the SVM to generate the final classification results.

This section reports the empirical experiments on the available fundus image datasets and evaluation results.

The dataset used in the experiments contains 1239 fundus images, which are collected from the database of a Picture Archiving and Communication System (PACS). Each fundus image is manually labelled by the ophthalmologist as non-cataractous, mild cataract, moderate cataract, or severe cataract. There are 767 (246, 128, and 98) non-cataractous (mild cataract, moderate cataract, and severe cataract) images. After preprocessing, these fundus images are resized as 3048*2432 before being used in the experiments.

Two tasks are derived from the real-world requirement for cataract diagnosis:
                              
                                 1)
                                 Cataract detection: This is a two-class classification task, i.e., to determine whether a fundus image is or is not a cataract. This task is mainly designed for large scale cataract screening.

Cataract grading: This is a four-class classification task that needs to classify a fundus image as non-cataractous, mild, moderate, or severe, the results of which will provide more detailed guidance or tentative suggestions to the eye doctor on the patient treatment (e.g., whether to conduct cataract surgery).

For both tasks, three widely used criteria, i.e., sensitivity (se for short, which refers to the proportion of one class's positive fundus images that are designated with correct labels), specificity (sp for short, which refers to the proportion of one class's negative fundus images that are designated with correct labels), and average correct classification rate (accr for short, which refers to the proportion of fundus images that are designated with correct labels), are adopted to measure the quality of image classification.

@&#IMPLEMENTATION@&#

The PC used for the experiments has an i7-4500 CPU, 8G memory, and Microsoft Windows 8 as the operational system. The fundus image preprocessing and feature extraction components are implemented with the image processing toolbox in Matlab. The pattern recognition toolbox of Matlab is employed to construct the SVM and BPNN classifiers.

For the SVM classifier, the Radial Basis Function (RBF) is chosen as the kernel function. Its implementation needs to determine the appropriate values of two parameters, i.e., the error cost coefficient C and RBF kernel function parameter γ. Actually, the setting of these parameters can be seen as an optimization problem, where the variables are C and γ, and the objective function is the SVM recognition rate on the test set. The grid search method in SVM toolbox is used for the parameter tuning.

For the BPNN classifier, we also need to choose an appropriate number m of neurons in the hidden layer for balancing the learning accuracy and training speed. If m is too small, the network cannot manage a complicated problem; if m is too large, the time for network learning sharply increases. Until now, there has been no perfect theory to guide the selection of m. For the fundus image classification tasks, the computing time of classifier building depends heavily on the size of the feature sets. Therefore, for balancing learning accuracy and computing cost, we use a compromised policy in the implementation of our experiments; i.e., the more features that are used, the lower the value of m that is set. Because there are 60 wavelet-based features, 23 sketch-based features, and 40 texture-based features, the number of hidden neurons in corresponding BPNN are set to 10, 15, and 15, respectively.

The performances of the base classifiers and their ensemble for cataract detection tasks (i.e., cataract detection classification) and cataract grading are reported in this section.

Because there are just 1239 labelled samples, the three-fold cross validation method is used to evaluate the performance of the classifier. All fundus images are equally divided into 3 subsets; thus, there are 413 images in one subset. In each fold, one subset is chosen as a testing set and another nine subsets are chosen as the training set, execute base classifiers and their majority voting and stacking-based ensembles; then, the classification accuracy of this fold is obtained.

Through a combination of three feature extraction methods (i.e., sketch, wavelet, and texture features) and two base learning models (i.e., SVM and BPNN), a total of six independent base classifiers are built. In addition, for comparison, the All-SVM and All-BPNN classifiers are implemented using all of the extracted wavelet-based, sketch-based, and texture features.

The experimental results of the six base classifiers for the cataract detection and cataract grading tasks are illustrated in Tables 1 and 2
                           
                           , respectively. From these two tables, it can be seen that Wavelet-BPNN (Wavelet-SVM) has the best performance in cataract detection (cataract grading) task, which achieves 91.9% (82.5%) in terms of the average correct classification rate. In addition, Sketch-BPNN has the worst performance in both the cataract detection and cataract grading tasks. When All-SVM and All-BPNN are considered, it is apparent that Wavelet-BPNN outperforms both of them in the cataract detection task. However, in the cataract grading task, All-SVM has the best performance, and All-SVM and All-BPNN outperform all six base classifiers.

The figures in Tables 1 and 2 show that Wavelet (Sketch)-based features have the best (worst) performance in terms of the average correct classification rate. To analyze the impact of three different feature sets on the final classification results, the box plot diagram is employed for the visualization of their numerical data to see if there are some discriminating values between particular groups. The scree plot is first used to find the important factors (i.e., the principle components) that should be employed for the visualization of each set of features. As shown in Fig. 8
                           , where the horizontal (vertical) axis denotes the factor (the corresponding variance value), five principal components (one principle component) are (is) selected for the wavelet-based and texture-based features (sketch-based features). Their Tukey box plots are illustrated in Figs. 9–11
                           
                           
                           , respectively, where the horizontal axis denotes the four classes of fundus images, and the vertical axis is the numerical values of the corresponding factors. A circle in these diagrams refers to the fundus image for which the numerical value in this factor is not within 1.5 IQR of the lower and upper quartiles. Comparing the figures in Tables 2 and 3
                            with the box plots in Figs. 9–11, it can be seen that although the dominant factor of a sketch-based feature has the best discriminative ability, its resulting classifier has the worst performance in terms of average correct classification rate for both sketch-SVM and sketch-BPNN. However, the five dominant factors of wavelet-based features do not show clear discrimination values between four groups; the resulting SVM and BPNN classifiers both achieve the best performance in terms of average correct classification rate among the corresponding classifiers from the three feature sets. It is postulated that this phenomenon is because the semantic interlinkages among multiple factors might make their combination have better discriminative ability than when they are used separately.

The diversity derived from multiple base classifiers plays a crucial role in the ensemble generating better classification quality. In other words, the base classifiers must differ in the errors that they make; this provides the necessary room for performance improvement. To verify this point, the classification quality of six base classifiers is analyzed. By considering whether the classification results are consistent and correct, the base classifiers’ results are divided into 5 cases: (1) All classification results are consistent and correct (Consistent-Correct); (2) The majority of the results are correct (Majority-Correct); (3) The votes distribute equally among a set of classes (Each involved class receives an equal number of votes) and contain the correct results (Equal-Correct); (4) The minority results are correct (Minority-Correct); (5) All of the classification results are wrong (All-Wrong). For case (3), the only mode of equally distributed voting in the cataract detection task is (3-3), which means that the relevant two classes received the same number of votes. In the cataract grading task, there are three modes of equally distributed voting, i.e., (3-3), (2-2-2) and (2-2-1-1). The statistics of the classification results are illustrated in Table 3, where the accumulative percentage in cataract detection (accumulative percentage in cataract grading) means that when the above mentioned case (x), 1≤
                           x
                           ≤5, is considered, the percentage of fundus images falls within cases (1)–(x). For example, in the column of the accumulative percentage in cataract detection, when the case (3): Equal-Correct is considered, 93.29%=81.21%+8.05%+4.03%.

From Table 3 it can be seen that, for the cataract detection (cataract grading) task, there is at least one base classifier that selects the correct class label for 98.66% (97.29%) of the fundus images in the test set. This means that regardless of the ensemble strategy that is used, the best performance of the ensemble of six base classifiers can achieve at most 98.66% (97.29%) in terms of the correct classification rate. In addition, with a majority voting policy, the ideal cataract detection and cataract grading performances of the six base classifiers’ ensemble are 93.29% and 91.88%, respectively.

The 3-fold cross validation approach is also used for the experiments on the performance evaluation of two ensemble approaches. For case (3) Equal-Correct mentioned above, because the votes are equally distributed among a set of classes, we randomly select one class label in the implementation of the majority voting approach. The experimental results of the cataract detection and cataract grading tasks are illustrated in Tables 4 and 5
                           
                           , respectively.

Comparing Stacking and Majority voting approaches for the ensemble of base classifiers, Majority voting outperforms Stacking in the cataract detection task. Its fundus image classification is 93.2% in terms of the correct classification rate. However, Stacking has better performance than that of Majority voting in the cataract grading task, whose correct classification rate is 84.5%. We postulate that this phenomenon is because for the cataract grading task, the modes of equally distributed voting are (3-3), (2-2-2), and (2-2-1-1), while there is only one model of equally distributed voting (3-3) in the cataract detection task.

Comparing the performances of base classifiers (as shown in Tables 1 and 2) with that of their ensembles (as shown in Tables 4 and 5), both Stacking and Majority Voting approaches outperform the best single learning model-based classifiers Wavelet-BPNN in the cataract detection task and All-SVM in the cataract grading task significantly (utilizing two-tailed t-test with p-value=0.01), which verifies the fact that these base classifiers are diversified and differ in the errors that they make.

As for computing complexity, because the ensemble learning approach is an integration of multiple base learners, in terms of the building time, the computing cost for the majority voting approach should be the sum of that of the involved base learners. For the stacking approach, the time for the level-1 classifier training should also be added. In the running time, the computing cost for the majority voting approach (stacking approach) is the sum of the decision making time of the involved base learning models plus the time of the majority voting process (decision making time of the level-1 classifier).

@&#CONCLUSION@&#

Unlike existing work on fundus image classification using a single learning model for cataract detection and grading, this paper proposes an ensemble learning approach that combines multiple heterogeneous learning models for more accurate prediction of cataract classification. Three independent feature extraction methods, i.e., wavelet-, sketch-, and texture-based methods, and two base learning models are investigated for the base classifier building. Then, stacking and majority voting are adopted to combine the multiple base classifiers for improved fundus image classification. The final two-class and four-class classification accuracy of our method is 93.2% and 84.5%, respectively, which represents significant improvement over the single learning models. We can see the great potential of our fundus image-based cataract classification method in early cataract screening and diagnosis, and we believe our method can be helpful in reducing the cost and simplifying the procedure of cataract diagnosis.

None declared.

@&#ACKNOWLEDGEMENTS@&#

This work is supported by Beijing Natural Science Foundation (4152007, 7142014) and China National Key Technology Research and Development Program project with no. 2013BAH19F01.

@&#REFERENCES@&#

