@&#MAIN-TITLE@&#Cross-domain polarity classification using a knowledge-enhanced meta-classifier

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose a new generic meta-learning-based approach to polarity categorization.


                        
                        
                           
                           Study impact of word sense disambiguation and vocabulary expansion-based features.


                        
                        
                           
                           State-of-the-art results on single and cross-domain polarity categorization.


                        
                        
                           
                           Our approach does not perform any domain adaptation, therefore it is generic.


                        
                        
                           
                           Our approach obtains the most stable results across the different tested domains.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Sentiment analysis

Cross-domain polarity classification

Meta-learning

Word sense disambiguation

Semantic network

@&#ABSTRACT@&#


               
               
                  Current approaches to single and cross-domain polarity classification usually use bag of words, n-grams or lexical resource-based classifiers. In this paper, we propose the use of meta-learning to combine and enrich those approaches by adding also other knowledge-based features. In addition to the aforementioned classical approaches, our system uses the BabelNet multilingual semantic network to generate features derived from word sense disambiguation and vocabulary expansion. Experimental results show state-of-the-art performance on single and cross-domain polarity classification. Contrary to other approaches, ours is generic. These results were obtained without any domain adaptation technique. Moreover, the use of meta-learning allows our approach to obtain the most stable results across domains. Finally, our empirical analysis provides interesting insights on the use of semantic network-based features.
               
            

@&#INTRODUCTION@&#

Text classification (also known as text categorization) is the task of assigning a category or categories to a text document from a set of predefined categories. Although at first this topic was approached from a knowledge engineering perspective (manually defining a set of rules encoding expert knowledge), in the 90s machine learning became the main approach, and so it stands today. A good survey on machine learning approaches to text classification can be found in Sebastiani [51].

The nature of the predefined categories in text classification can be very heterogeneous. The most common task is that of topic-based classification, attempting to classify documents according to their subject matter (e.g. Sports vs. Politics vs. Economics). More recently, in the context of the Web 2.0 and social media, it emerged the task of deciding whether a subjective text (typically, a textual review of some product or a cultural or political issue) is positive or negative, depending on the overall sentiment detected. This particular task is known as polarity classification or sentiment classification [54,42]. Although it can be defined in terms of text classification (being positive and negative the predefined categories) and tackled with similar approaches, polarity classification has been proved to be a more difficult task [42]: while topics are often identifiable by keywords alone, sentiment can be expressed in a more subtle manner, and even more when for instance irony is employed [48]. Therefore, solutions based only on bag-of-words representations of documents may not be enough.

In this work we are interested in single and cross-domain polarity classification. Since we are applying machine learning techniques, we start with a training set of documents to build some classifiers. In this context, single-domain classification is the aforementioned common text classification; it refers to training and testing classifiers on the same domain (e.g. movie reviews). Meanwhile, cross-domain classification refers to testing on a different domain (target domain) from that or those used in training (source domains), e.g. training on movie reviews and testing on books reviews. Because manually labeled documents are needed for training, the latter allows to work with domains where no labeled documents are available. The problem of cross-domain text classification was first tackled by Dai et al. [13], and the first results on cross-domain polarity classification were reported by Blitzer et al. [7].

In order to combine different approaches from the research literature and recent knowledge-based approaches, and also to measure the contributions of each one, we propose the use of a meta-learning scheme called Stacked Generalization [56]. The set of base classifiers to be combined using that scheme include solutions used in the past as a TF-IDF bag-of-words classifier, a TF-IDF word n-gram classifier, and a lexical resource for opinion mining-based classifier; but also two new proposals, a word sense disambiguation-based classifier and a vocabulary expansion-based classifier. The latter two classifiers are trained on the basis of knowledge graphs, a subset of a semantic network, i.e., BabelNet [38], focused on the concepts belonging to the text being classified.

The rest of the paper is structured as follows. In Section 2 we describe the related work on single and cross-domain polarity classification. In Section 3 we introduce our new knowledge-enhanced meta-classifier. In Section 4 we evaluate our approach in the tasks of single and cross-domain polarity classification, and compare it with other state-of-the-art approaches. In that section we evaluate also the performance of our different base classifiers. Finally, in Section 5 we draw the conclusions and mention directions for future work.

@&#RELATED WORK@&#

The first experiments on single-domain polarity classification using machine learning techniques were performed by Pang et al. [42]. They used a movie review dataset extracted from IMDb.
                        1
                        
                           http://www.cs.cornell.edu/people/pabo/movie-review-data/.
                     
                     
                        1
                      They concluded that polarity classification achieves worse results than other text classification tasks when applying the standard machine learning techniques. Another interesting conclusion was that using unigram presence instead of unigram frequency leads to better results, contrary to observations in other works on text classification [33].

Recent works on polarity classification use the Multi-Domain Sentiment Dataset [7] for evaluation. In its last version, the resource is composed by Amazon product reviews of 25 product types, though most works report results on only the four domains used by Blitzer et al. [7]: Books, Electronics, DVDs and Kitchen appliances. Focused on single-domain polarity classification, Dredze et al. [16] presented a new online learning method named confidence-weigthed learning. The method is based on measuring the confidence of each parameter of the classifier; less confident parameters are updated more aggressively than more confident ones. They performed experiments on standard datasets related to different text classification tasks, reporting very good results for the Multi-Domain Sentiment Dataset. Another approach, proposed by Li and Zong [30], use n-grams combined with Binormal Separation [22], an alternative to TF-IDF to select the optimal set of features. They reported interesting results in single domain classification.

Cross-domain polarity classification has gained popularity thanks to the advances in domain adaptation [14,6,4]. These techniques make use of labeled data from a source domain, and unlabeled data from source and target domains to train their classifiers. Using the different domains available in the Multi-Domain Sentiment Dataset, Blitzer et al. [7] was also the first to report results on cross-domain classification proposing two algorithms: structural correspondence learning (SCL), and its variant using mutual information (SCL-MI). The SCL model selects pivot (unigram and bigram) features frequently appearing in both source and target domains. Then it learns to predict those pivot features in the unlabeled data from both domains. Later, a singular value decomposition is performed to reduce dimensions, and a binary classifier is trained to determine the polarity. Similarly, interesting results on cross-domain polarity classification have been reported by spectral feature alignment (SFA) [41]. Using unigram and bigram features, the model exploits the mutual information between each feature and the domain label to differentiate domain-specific and domain-independent features. Next, a bipartite graph is constructed by dividing both types of features. An edge connects features from different types if there exists co-occurrence. Finally, a spectral clustering is performed to generate feature clusters and a binary classifier is built for the polarity classification. More recently, Bollegala et al. [8,9] used a cross-domain lexicon creation to generate a sentiment-sensitive thesaurus (SST) that groups different words expressing the same sentiment, using also unigram and bigram features as representation. This approach also obtained competitive results in single-domain polarity classification.

Note that all cross-domain approaches use domain adaptation techniques extracting relevant features from the source domains, in order to obtain important features to classify the target domain. In contrast, we do not use unlabeled data from the target domain. Our approach is focused on proposing new knowledge-based features which allows for training models using the source domains that are able to be directly applied to the target domain. In Section 4.4 we compare our approach in the task of single-domain polarity classification against SST and the state-of-the-art approaches proposed by Dredze et al. [16] and Li and Zong [30]. Next, in Section 4.5 we compare our approach in the task of cross-domain polarity classification against SCL-MI, SFA and SST models.

We propose the use of a meta-learning scheme for combining different classical approaches, i.e., bag of words, n-grams or lexical resource-based classifiers. Key to our approach is adding also other knowledge-based classifiers. By using a semantic network, we perform word sense disambiguation and generate new independent classifiers for the main part-of-speech tags: disambiguated adjectives, nouns, verbs and adverbs. Using the disambiguated terms, the semantic network allows us to obtain a vocabulary expansion-based classifier. In Section 3.1 we present the semantic network, and the word sense disambiguation and vocabulary expansion methods. Then, in Section 3.2 we describe the base classifiers that compose our system. Finally, in Section 3.3 we define the Stacked Generalization that we use to combine those classifiers.

A semantic network [53] is a (un)directed graph consisting of vertices, which represent concepts, and edges, which represent semantic relations between them. Concepts are usually organized into a taxonomic hierarchy. Fig. 1
                         shows a simple example of semantic network.

In this work we use the semantic network graph to: (i) perform word sense disambiguation, and (ii) perform a vocabulary expansion using the disambiguated words. Despite having the WordNet Semantic Network [21], which is an historical resource including 117,000 synsets
                           2
                           Set of word synonyms.
                        
                        
                           2
                         in English, in this work we are interested in employing a larger size wide-coverage lexical knowledge resource. Among those, we can find knowledge bases extracted automatically from Wikipedia such as DBPedia [5] or YAGO [27]. However, due to its WordNet-based internal structure combined with Wikipedia, the high amount of synsets included, and the lexicalizations of its concepts available in multiple languages,
                           3
                           While this work is exclusively evaluated on English, this multilinguality allows us to perform at multilingual level.
                        
                        
                           3
                         we chose the BabelNet Multilingual Semantic Network.

BabelNet
                              4
                              
                                 http://babelnet.org.
                           
                           
                              4 2.5 [38] is a multilingual semantic network whose concepts and relations are obtained from the automatic mapping onto Wordnet of Wikipedia,
                              5
                              
                                 http://wikipedia.org.
                           
                           
                              5
                            OmegaWiki,
                              6
                              
                                 http://omegawiki.org.
                           
                           
                              6
                            Wiktionary,
                              7
                              
                                 http://wiktionary.org.
                           
                           
                              7
                            Wikidata,
                              8
                              
                                 http://wikidata.org.
                           
                           
                              8
                            and Open Multilingual WordNet.
                              9
                              
                                 http://compling.hss.ntu.edu.sg/omw/.
                           
                           
                              9
                            BabelNet is therefore a multilingual “encyclopedic dictionary” that combines lexicographic information with wide-coverage encyclopedic knowledge. Concepts in BabelNet are represented similarly to WordNet, i.e., by grouping sets of synonyms in the different languages into multilingual synsets. Multilingual synsets contain lexicalizations from WordNet and Open Multilingual WordNet synsets, the corresponding Wikipedia pages, the OmegaWiki, Wiktionary and Wikidata entries, and additional translations by a statistical machine translation system. The relations between synsets are collected from WordNet, Open Multilingual WordNet, and from Wikipedia’s hyperlinks between pages. The current version of BabelNet includes 9,348,287 synsets, covers 50 languages, and has a WordNet-Wikipedia mapping correctness of 91% [36].

Word sense disambiguation (WSD) [35] is the process of identifying which sense (i.e., meaning) of a word is used in a sentence, when the word is polysemic. In general, the approaches for WSD can be classified into three types: (i) supervised, with a considerable effort for new languages and domains due to the huge amount of annotated data required [52,43]; (ii) unsupervised approaches, which have to deal with data sparsity and an intrinsic difficulty with their evaluation [2,15]; and (iii) knowledge-based approaches, which exploit the knowledge available in structured knowledge bases [44,37,1,34]. Vocabulary expansion benefits from the WSD performed using a knowledge base by exploiting the relations in its network.

BabelNet has been used for WSD in several works, including some of the aforementioned publications and also as part of the Multilingual Word Sense Disambiguation Task of the SemEval Workshop [36]. Similarly to Navigli and Ponzetto [38] and Franco-Salvador et al. [23,24], we followed Navigli and Lapata [37] to create knowledge graphs
                              10
                              A knowledge graph is a subset of the original semantic network focused on the concepts belonging to a text, and in the intermediate concepts and relations between them.
                           
                           
                              10
                            in order to perform the WSD and the vocabulary expansion. The five-step method we used to perform the WSD is the following:

Initially we process a document d with tokenization, multi-word extraction, part-of-speech (POS) tagging and lemmatization
                                 11
                                 For this purpose we used the Stanford Log-linear Part-Of-Speech Tagger: http://nlp.stanford.edu/software/tagger.shtml. For the multi-word extraction we implemented our own tool based on the matching of typical patterns.
                              
                              
                                 11
                               to obtain the list of tuples (lemma,tag) T. We are interested only in the POS tags available on BabelNet (adjectives, nouns, verbs and adverbs).

Next, we create an initially-empty knowledge graph 
                                 
                                    G
                                    =
                                    (
                                    V
                                    ,
                                    E
                                    )
                                 
                              , i.e., such that 
                                 
                                    V
                                    =
                                    E
                                    =
                                    ∅
                                 
                              . We populate the vertex set V with the set 
                                 
                                    
                                       
                                          S
                                       
                                       
                                          K
                                       
                                    
                                 
                               of all the synsets in BabelNet which contain any tuple (lemma,tag) in T in the document language L, that is:
                                 
                                    (1)
                                    
                                       
                                          
                                             S
                                          
                                          
                                             K
                                          
                                       
                                       =
                                       
                                          
                                             
                                                ⋃
                                             
                                             
                                                t
                                                ∈
                                                T
                                             
                                          
                                       
                                       
                                       
                                          
                                             Synsets
                                          
                                          
                                             L
                                          
                                       
                                       (
                                       t
                                       )
                                       ,
                                    
                                 
                              where Synsets 
                                 
                                    
                                       
                                       
                                          L
                                       
                                    
                                    (
                                    t
                                    )
                                 
                               is the set of synsets which contains a tuple (lemma,tag) t in the language of interest L.

We create the knowledge graph by searching on BabelNet to obtain the set of paths P connecting pairs of synsets in V. Formally, for each pair 
                                 
                                    {
                                    v
                                    ,
                                    
                                       
                                          v
                                       
                                       
                                          ′
                                       
                                    
                                    }
                                    ∈
                                    V
                                 
                               such that v and 
                                 
                                    
                                       
                                          v
                                       
                                       
                                          ′
                                       
                                    
                                 
                               do not share any lexicalization
                                 12
                                 This prevents different senses of the same term from being connected via a path in the resulting knowledge graph.
                              
                              
                                 12
                               in T, for each path in BabelNet 
                                 
                                    v
                                    →
                                    
                                       
                                          v
                                       
                                       
                                          1
                                       
                                    
                                    →
                                    …
                                    →
                                    
                                       
                                          v
                                       
                                       
                                          n
                                       
                                    
                                    →
                                    
                                       
                                          v
                                       
                                       
                                          ′
                                       
                                    
                                 
                              , we set: 
                                 
                                    V
                                    ≔
                                    V
                                    ∪
                                    {
                                    
                                       
                                          v
                                       
                                       
                                          1
                                       
                                    
                                    ,
                                    …
                                    ,
                                    
                                       
                                          v
                                       
                                       
                                          n
                                       
                                    
                                    }
                                 
                               and 
                                 
                                    E
                                    ≔
                                    E
                                    ∪
                                    {
                                    (
                                    v
                                    ,
                                    
                                       
                                          v
                                       
                                       
                                          1
                                       
                                    
                                    )
                                    ,
                                    …
                                    ,
                                    (
                                    
                                       
                                          v
                                       
                                       
                                          n
                                       
                                    
                                    ,
                                    
                                       
                                          v
                                       
                                       
                                          ′
                                       
                                    
                                    )
                                    }
                                 
                              . That is, we add all the path vertices and edges to G. Following Navigli and Ponzetto [38], the path length is limited to maximum length of 3, in order to avoid an excessive semantic drift.

As a result of populating the graph with intermediate edges and vertices, we obtain a knowledge graph which models the semantic context of document d.

The next step consists of weighting all the concepts and semantic relations of the knowledge graph G. For weighting relations we use the original weights from BabelNet, which provide the degree of relatedness between the synset end points of each edge.
                                 13
                                 At this point, we removed the edges below a certain threshold that represents a low semantic relationship.
                              
                              
                                 13
                               For weighting concepts different methods, including the PageRank [40] algorithm, have been tested in the past. In this work, we score each concept using its own outdegree, which has proved to obtain the best results [38].

Finally, for each tuple (lemma,tag) 
                                 
                                    t
                                    ∈
                                    T
                                 
                              , we collect from BabelNet the set of synsets 
                                 
                                    
                                       
                                          S
                                       
                                       
                                          t
                                       
                                    
                                 
                               containing t, and we select as proper disambiguation 
                                 
                                    
                                       
                                          t
                                       
                                       
                                          WSD
                                       
                                    
                                 
                               the synset with the highest score:
                                 
                                    (2)
                                    
                                       
                                          t
                                          WSD
                                       
                                       
                                          =
                                          
                                             s
                                             ∈
                                             
                                                S
                                                t
                                             
                                          
                                       
                                       argmax
                                       
                                       score
                                       (
                                       s
                                       )
                                       ,
                                    
                                 
                              
                           

Once we have disambiguated the words of a document d, to enrich and increase the available context, we perform an automatic vocabulary expansion [18,17] using the BabelNet graph topology. A simple vocabulary expansion can be done using directly any connected concept to a disambiguated one, up to a certain distance in the graph. However, to preserve as much context as possible and to avoid introducing noise, we include only intermediate concepts between pairs of disambiguated words. Formally, using the knowledge graph G created in Section 3.1.2, we obtain a vocabulary expansion as follows:

We first use the process described in the previous section to obtain the set 
                                 
                                    
                                       
                                          S
                                       
                                       
                                          WSD
                                       
                                    
                                 
                              . This set is composed by the disambiguation synsets of the original words of document d.

We create a path set 
                                 
                                    
                                       
                                          P
                                       
                                       
                                          ′
                                       
                                    
                                 
                               by removing from the path set P all the paths between synsets which are not in 
                                 
                                    
                                       
                                          S
                                       
                                       
                                          WSD
                                       
                                    
                                 
                              . This step removes noise by creating a knowledge graph focused on the disambiguated concepts.

We obtain the vocabulary expansion by creating a set 
                                 
                                    
                                       
                                          S
                                       
                                       
                                          exp
                                       
                                    
                                 
                               including the intermediate concepts in the paths of 
                                 
                                    
                                       
                                          P
                                       
                                       
                                          ′
                                       
                                    
                                 
                              . We remove the source and target concepts from paths to evaluate the performance of the vocabulary expansion without the original words (see Section 4.3).
                                 14
                                 This last part is optional, although it helps to focus on the vocabulary expanded concepts.
                              
                              
                                 14
                              
                           


                              Fig. 2
                               provides an example
                                 15
                                 Weights and nodes representing alternative senses or intermediate concepts are removed for simplicity.
                              
                              
                                 15
                               of disambiguation and vocabulary expansion using knowledge graphs.

We can now define the base classifiers that compose our system. We first include a TF-IDF bag-of-words classifier, a TF-IDF word n-gram classifier and a lexical resource for the opinion mining-based classifier. The choice of these components has been motivated by the good results that they achieved in the past. In addition, in this work we want to investigate the impact of knowledge-based classifiers; therefore we include an independent classifier to study the contribution of WSD for each POS tag employed (adjectives, nouns, verbs and adverbs). Finally, under the assumption that semantically-related concepts have a common near relative, we want to exploit this possible relatedness between concepts including a vocabulary expansion-based classifier. Next we explain in more detail our eight base classifiers:

This approach transforms a document d into a traditional vector representation. Following the literature, we selected the most widely used representation for real-valued feature vectors, commonly used as baseline: the Term Frequency-Inverse Document Frequency (TF-IDF) weighting [49,50].
                              
                                 (3)
                                 
                                    tf-df
                                    (
                                    w
                                    )
                                    =
                                    tf
                                    (
                                    w
                                    )
                                    N
                                    /
                                    n
                                    (
                                    w
                                    )
                                    .
                                 
                              
                           where 
                              
                                 tf
                                 (
                                 w
                                 )
                              
                            is the number of times a term w occurs in document 
                              
                                 d
                                 ,
                                 N
                              
                            is the total number of documents in the collection and 
                              
                                 n
                                 (
                                 w
                                 )
                              
                            is the number of documents that contain w. We removed stopwords from documents for all the base classifiers.

As classifier, we selected Support Vector Machines (SVM) [11], with a linear kernel function,
                              16
                              We use the linear kernel function for all the SVM base classifiers.
                           
                           
                              16
                            given its good performance for text classification [29] using TF-IDF weighting.

The use of word n-grams has been proposed several times [10,32,30] as a better alternative to single word vector representation due to the additional information that it provides. Using n-grams is a plus for a complex classification task like polarity classification: while topics are often identifiable by keywords alone, sentiment can be expressed in a more subtle manner [42]. For example, the keyword like may be correlated with positive sentences (e.g. “I like this paper a lot.”) or with negative sentences (e.g. “I do not like this paper at all.”). Using n-grams also allows us to learn frequent, opinion-bearing multiword expressions (e.g. “you will love (this story)”).

This n-gram representation is processed with a TF-IDF weighting and an SVM classifier. Since larger n-grams will not be frequent, we included only a combination [30] of 1, 2, and 3-grams.

The use of lexical resources for opinion mining was strongly popularized by the release of SentiWordNet [20,3]. This resource assigns to each synset of WordNet three sentiment scores: positivity, negativity, objectivity. It has been sucessfully applied to polarity classification in the past [39,26].

We selected as lexical resource ML-SentiCon [12], which proved to make several improvements with respect to the original SentiWordnet 3.0, with a significative better positivity, negativity and objectivity estimation, reflecting those results on their evaluation.

For this base classifier, we decided to use the tree-based C4.5 [46] model, which infers a hierarchy of rules as a function of different feature values to determine the final class, and provides good performance for polarity classification [28]. Its use is also motivated by the different types of features that we selected for this classifier (see Table 1
                           ): some of them are discrete and unbounded. In addition, considering that there are only 10 features, using SVM did not pose any additional advantage with regard to a simpler C4.5 tree-based classifier.

As we stated at the beginning of this section, to study the impact of WSD on polarity classification, we generate an independent classifier for each POS tag available on BabelNet (adjectives, nouns, verbs and adverbs) on the basis of the method explained in Section 3.1.2.

During the prototyping process, we realized that due to the use of independent classifiers for each POS tag, and the error introduced by wrong disambiguations, the TF-IDF weighting provided an imprecise representation of documents, and worse results than using only binary TF (presence or not of the word w in the document). Since the use of this technique has been studied in the past with good results [42], for the WSD-based models we decided to use binary TF as weighting and SVM as classifier.

The last base classifier uses the vocabulary expansion explained in Section 3.1.3 to represent each document d as a binary TF of synsets, which are related to the original disambiguated ones of d. The classification is performed using SVM. Since we are removing the original concepts of the documents from the vocabulary expansion, a document containing the concepts “Michael Jordan” and “NBA” will be represented by concepts as “Basketball” and “Sport”, but not by the original concepts. As previously stated, the original concept removal was performed because we are interested in evaluating the performance of the vocabulary expansion without the original words.


                           Table 2
                            provides a summary
                              17
                              Column “Avg. # feat.” shows the average number of potential features of the classifier across domains before applying their respective thresholds (see Section 4.2).
                           
                           
                              17
                            of all the base classifiers.

We combine the base classifiers with one of the most popular combination methods in meta-learning: stacking. It has been used successfully in Natural Language processing (NLP) tasks [55,19] in the past. This method follows the original Stacked Generalization method [56] to project documents onto a new dimensional space, which is composed by the annotations of a first-level base classifiers set. This combination is able to exploit additional information from a corpus by processing it with different classifiers. A second-level classifier uses all of the annotations of the first level to obtain a final decision, with the advantage of recognizing and classifying correctly patterns in which the correct class tag is in inferiority. In this work, instead of representing the results of the first level as a vector of class tags, we represent them as a vector of class probabilities, which proved to obtain better results using SVM [31].
                           Algorithm 1
                           Stacking generalization algorithm 
                                 
                                    
                                       
                                       
                                          
                                             
                                                Require: a tagged training corpus T and a untagged test corpus t.
                                          
                                          
                                             
                                                Ensure: a tagged test corpus 
                                                   
                                                      
                                                         
                                                            t
                                                         
                                                         
                                                            ″
                                                         
                                                      
                                                   
                                                .
                                          
                                          
                                             
                                                1: Split T into K parts to obtain 
                                                   
                                                      
                                                         
                                                            T
                                                         
                                                         
                                                            1
                                                            ,
                                                            …
                                                            ,
                                                            K
                                                         
                                                      
                                                   
                                                 partitions.
                                          
                                          
                                             
                                                2: Tag 
                                                   
                                                      
                                                         
                                                            T
                                                         
                                                         
                                                            1
                                                            ,
                                                            …
                                                            ,
                                                            K
                                                         
                                                      
                                                   
                                                 using cross-validation with the 
                                                   
                                                      
                                                         
                                                            C
                                                         
                                                         
                                                            1
                                                            ,
                                                            …
                                                            ,
                                                            N
                                                         
                                                      
                                                   
                                                 base classifiers to obtain 
                                                   
                                                      
                                                         
                                                            T
                                                         
                                                         
                                                            1
                                                            ,
                                                            …
                                                            ,
                                                            K
                                                         
                                                         
                                                            ′
                                                         
                                                      
                                                   
                                                 partitions containing the transformed samples of T.
                                          
                                          
                                             
                                                3: Using 
                                                   
                                                      
                                                         
                                                            T
                                                         
                                                         
                                                            1
                                                            ,
                                                            …
                                                            ,
                                                            K
                                                         
                                                      
                                                   
                                                 for training, classify t with 
                                                   
                                                      
                                                         
                                                            C
                                                         
                                                         
                                                            1
                                                            ,
                                                            …
                                                            ,
                                                            N
                                                         
                                                      
                                                   
                                                 to obtain the transformed corpus 
                                                   
                                                      
                                                         
                                                            t
                                                         
                                                         
                                                            ′
                                                         
                                                      
                                                   
                                                .
                                          
                                          
                                             
                                                4: Use 
                                                   
                                                      
                                                         
                                                            T
                                                         
                                                         
                                                            1
                                                            ,
                                                            …
                                                            ,
                                                            K
                                                         
                                                         
                                                            ′
                                                         
                                                      
                                                   
                                                 as a single partition to train the second-level classifier 
                                                   
                                                      
                                                         
                                                            C
                                                         
                                                         
                                                            comb
                                                            .
                                                         
                                                      
                                                   
                                                .
                                          
                                          
                                             
                                                5: Classify 
                                                   
                                                      
                                                         
                                                            t
                                                         
                                                         
                                                            ′
                                                         
                                                      
                                                   
                                                 with 
                                                   
                                                      
                                                         
                                                            C
                                                         
                                                         
                                                            comb
                                                            .
                                                         
                                                      
                                                   
                                                 to obtain the tagged test corpus 
                                                   
                                                      
                                                         
                                                            t
                                                         
                                                         
                                                            ″
                                                         
                                                      
                                                   
                                                .
                                          
                                       
                                    
                                 
                              
                           

We can see the Stacked Generalization method detailed in Algorithm 1. Lines 1–3 correspond to the first level of the classifier, which makes the transformation of the training corpus. The second level of the classifier is explained in Lines 4–5, which obtains the final classification of the test corpus. A complete scheme of the model is shown in Fig. 3
                        .

@&#EVALUATION@&#

In this section we evaluate the base classifiers of our Knowledge-enhanced Meta-classifier (KE-Meta), and compare our approach with state-of-the-art models on single and cross-domain polarity classification.

To evaluate our system we chose a classical state-of-the-art dataset, the Multi-Domain Sentiment Dataset (version 2.0)
                           18
                           
                              http://www.cs.jhu.edu/mdredze/datasets/sentiment/.
                        
                        
                           18
                         
                        [7], which has been used for the evaluation of several research works on sentiment analysis [16,30,7,9]. The dataset is composed by Amazon product reviews of 25 product types. Each review contains metadata including a rating of 0–5 stars, the reviewer name and location, the product name, the review date and title, and the review text. In addition, for research purposes, a subset of the reviews with rating <3 were originally labeled as negative, and with rating >3 as positive. Following the literature, in this work we use the Books, Electronics, DVDs, and Kitchen appliances reviews, with 1000 positive and 1000 negative documents per domain, having a total of 8000 reviews. With this setup, we can compare our results on single and cross-domain polarity classification directly with the state of the art.

@&#METHODOLOGY@&#

The evaluation of our approach in single-domain polarity classification is performed using a stratified 10-fold cross-validation setup for each domain. In cross-domain, we followed the same 10-fold cross-validation setup,
                           19
                           The cross-validation here is used only to train our KE-Meta classifier, which needs a splitting of the data to obtain training and testing partitions to generate the final second-level classifier.
                        
                        
                           19
                         in this case, training always with all domains available and excluding the target domain to classify, e.g. we train with Books, Electronics, and DVDs, and we classify Kitchen reviews. We selected as the evaluation metric the accuracy of the classifiers, which is the proportion of correctly classified reviews among the test dataset. We detail the models compared with our approach on its respective evaluation sections. Note that the number of dimensions of all our base classifiers is limited to a maximum number of 20,000. However, similar results were obtained with sizes ranging between 15,000 and 25,000 during the prototyping step.

To evaluate the eight base classifiers that compose our approach (cfr Section 3.2) summarized in Table 2, we first employ a traditional measure of information theory [25]: the information gain ratio (IGR) [45,47]. Once analyzed the IGR, we will continue with the study of the accuracy of classification of each base classifier.

Having a training set T and its set of attributes Attr, the IGR measure provides a normalized estimation (between 0 and 1) of the amount of information that an attribute 
                           
                              a
                              ∈
                              Attr
                           
                         provides to determine the class attribute.
                           20
                           Note that each attribute 
                                 
                                    a
                                    ∈
                                    Attr
                                 
                               corresponds to a base classifier in our approach.
                        
                        
                           20
                         The IGR of an attribute a is calculated as the ratio between the information gain (IG) and the intrinsic value (IV):
                           
                              (4)
                              
                                 IGR
                                 (
                                 T
                                 ,
                                 a
                                 )
                                 =
                                 
                                    
                                       IG
                                       (
                                       T
                                       ,
                                       a
                                       )
                                    
                                    
                                       IV
                                       (
                                       T
                                       ,
                                       a
                                       )
                                    
                                 
                              
                           
                        
                        
                           
                              (5)
                              
                                 IG
                                 (
                                 T
                                 ,
                                 a
                                 )
                                 =
                                 H
                                 (
                                 T
                                 )
                                 -
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          v
                                          ∈
                                          values
                                          (
                                          a
                                          )
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                |
                                                {
                                                x
                                                ∈
                                                T
                                                |
                                                value
                                                (
                                                x
                                                ,
                                                a
                                                )
                                                =
                                                v
                                                }
                                                |
                                             
                                             
                                                |
                                                T
                                                |
                                             
                                          
                                          ·
                                          H
                                          (
                                          {
                                          x
                                          ∈
                                          T
                                          |
                                          value
                                          (
                                          x
                                          ,
                                          a
                                          )
                                          =
                                          v
                                          }
                                          )
                                       
                                    
                                 
                              
                           
                        where we substract to the total entropy H of the train set T the sum of the relative entropies of the different values of a in T. For each of the attributes, if a unique classification can be made for the result attribute, the information gain is equal to the total entropy of a. The IV is a normalization factor estimated as a function of the substracted entropies of H
                           
                              (
                              T
                              )
                           
                         in IG.
                           
                              (6)
                              
                                 IV
                                 (
                                 T
                                 ,
                                 a
                                 )
                                 =
                                 -
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          v
                                          ∈
                                          values
                                          (
                                          a
                                          )
                                       
                                    
                                 
                                 
                                    
                                       |
                                       {
                                       x
                                       ∈
                                       T
                                       |
                                       value
                                       (
                                       x
                                       ,
                                       a
                                       )
                                       =
                                       v
                                       }
                                       |
                                    
                                    
                                       |
                                       T
                                       |
                                    
                                 
                                 ·
                                 
                                    
                                       log
                                    
                                    
                                       2
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                |
                                                {
                                                x
                                                ∈
                                                T
                                                |
                                                value
                                                (
                                                x
                                                ,
                                                a
                                                )
                                                =
                                                v
                                                }
                                                |
                                             
                                             
                                                |
                                                T
                                                |
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

To obtain the IGR of our base classifiers, we estimated the IGR on each tested domain and we calculated the harmonic mean
                           21
                           The harmonic mean is the most adequate measure to average percentages of different domains.
                        
                        
                           21
                         of those results. This test was performed on single and cross-domain polarity classification. We show the results in Fig. 4
                        . As expected, the IGR in cross-domain is lower than working on single domain for almost all of the base classifiers. This is not the case of the model using ML-SentiCon, which, despite getting a low IGR, is able to preserve all its gain when performing at cross-domain level. These results put forward the advantage of knowledge bases to model the information in a domain-independent way. We can see that BOW and (1+2+3)-grams classifiers obtained the highest information gain ratios, with almost identical values. The results prove that these models are a good choice as base classifiers to be complemented with other classifiers. The vocabulary expansion, which does not include the original words of the documents, is able to obtain comparable results. Models disambiguating different POS tags obtained considerably low IGR. Adjective WSD was the most informative classifier. This is unsurprising if we consider that often, the polarity of a text could be given by adjectives. This is followed by the classifier for nouns, verbs, and finally adverbs. These last two with identical results on single-domain. Since WSD has been divided into four models, it is difficult to evaluate its contribution. For this reason, we included also the results of two additional classifiers: All synsets (Post-WSD) and All synsets (Pre-WSD). They represent the IGR of a binary TF
                           22
                           Similarly to the other WSD-based classifiers, binary TF is preferred to TF-IDF to smooth the error in case of a wrong disambiguation.
                        
                        
                           22
                         classifier trained using SVM with: (i) all the disambiguated words together (All synsets (Post-WSD) classifier), and (ii) all the possible senses of the words together before disambiguation (All synsets (Pre-WSD) classifier). As we can see, the performance of All synsets (Post-WSD) significantly outperforms the Pre-WSD model, and obtained similar result to BOW and n-grams based approaches. This highlights the capability of WSD to remove noisy senses, leaving only the appropriate one.

Once evaluated the IGR of the base classifiers, the next step is to evaluate them separately in the polarity classification task. Following the setup of Section 4.2, we can see the results on single-domain in Table 3
                        . The results are in line with those obtained for IGR: (1+2+3)-grams obtained the highest results, followed by BOW. The vocabulary expansion achieved averaged results followed by Adjective WSD and the rest of WSD-based classifiers. Finally, ML-SentiCon was the model with the lowest accuracy. Looking at the results on cross-domain in Table 4
                        , we can see a similar trend. Despite there is a general decrease in the results, as we stated while analyzing its IGR, ML-SentiCon has even improved its results on cross-domain, taking advantage of all the other domains to train a domain independent model which is able to outperform the noun, verb and adverb WSD-based approaches. Note that, as we can see in both tables, All synsets (Post-WSD) classifier outperforms the Pre-WSD model, and gets similar results to the best base classifiers.

Looking at all the previous results, due to the different type of classifiers selected, each one of them should provide different information when combined in a meta-classifier. The next experiment studies the improvement in the accuracy when adding base classifiers one by one to our KE-Meta approach. We can see the single-domain results in Fig. 5
                        . As expected, considering the harmonic mean, there is an improvement when each new base classifier is added. As one classifier might provide information included by others, the improvements were shown to be greater at the beginning. The results on cross-domain are shown in Fig. 6
                        . Also in this case there is a clear improvement compared to the first base classifier included, being BOW, ML-SentiCon and Adjective WSD, the models with higher contribution. However, the vocabulary expansion seems to have a negative contribution in this cross-domain combination. We assume that expanding vocabulary from different domains and combining all the documents together, contributes to obtaining a noisy base classifier with several clusters of vocabulary of concepts related to each training domain. In the next cross-domain experiments we will show also the results without the vocabulary expansion base classifier.

We compared our knowledge-enhanced meta classifier against the state-of-the-art SST model, and those proposed by Dredze et al. [16] and Li and Zong [30]
                        
                           23
                           Results of compared approaches are taken from their original works: Bollegala et al. [9], Dredze et al. [16] and Li and Zong [30].
                        
                        
                           23
                         (cfr Section 2). In addition we included the results of our BOW and (1+2+3)-grams classifiers as baselines.

As we can see from Table 5
                        ,
                           24
                           In this work, statistically significant results according to a 
                                 
                                    
                                       
                                          χ
                                       
                                       
                                          2
                                       
                                    
                                 
                               test are highlighted in bold.
                        
                        
                           24
                         thanks to the additional information included when combining groups of words as single feature, (1+2+3)-grams obtained better results than BOW. However, all of the compared models outperformed these baselines. Dredze et al.’s approach obtained interesting results, specially classifying electronics. This model benefited from confidence-weighted classification to create very precise linear frontiers among classes. The SST model, using its sentiment sensitive thesaurus, took advantage of the type of reviews used in kitchen domain and obtained the best results, with good accuracy in the other domains. Li and Zong’s approach, based on a optimized n-gram selection criteria, obtained the best results on DVD reviews. Our approach obtained the best results on Books domain and considerably high results on the rest. We hypothesize that when reviewers analyze books summarizing parts from the story of the book, our meta-classifier is able to distinguish this pattern by contrasting the probabilities of the base classifiers, and the polarity of the book summary has less influence in the final review classification. Note that our approach is the most stable, with no less than 82.3% of accuracy in all the tests. Using meta-classification, KE-Meta is able to determine which base classifier is better on each domain, maximizing its contribution in the combination. We highlight also that each state-of-the-art approach obtained specially low (or high) results in some domain. This may be produced by the writing style employed by reviewers when commenting on those products. At the end of Section 4.5, in Table 7 we analyze the vocabulary of domains to investigate these differences further.

In this task we compared our KE-Meta approach against the state-of-the-art SFA, SCL-MI and SST approaches.
                           25
                           The results of the approaches compared are taken from Bollegala et al. [9].
                        
                        
                           25
                         As we mentioned in Section 4.3, we included also the results of our approach without the vocabulary expansion-based base classifier: KE-Meta
                           B
                        . The BOW and (1+2+3)-grams models are included as baselines.


                        Table 6
                         shows the cross-domain polarity classification accuracy. The (1+2+3)-grams baseline achieved the lowest results. Training a cross-domain n-gram-based classifier using only three domains does not seem to be sufficient to obtain a good domain-independent n-gram inventory. Evidence of this observation are the close results obtained by SCL-MI and SFA, other two n-gram based approaches. SCL-MI excelled especially in the kitchen domain. We hypothesize that the singular value descomposition used to reduce dimensions worked better with the reduced size of the vocabulary in kitchen domain. The second domain with less vocabulary, electronics, excelled too. The bipartite graph constructed to differentiate domain-specific and independent n-grams helped SFA to obtain significant results on books domain. Precisely despite obtaining the lowest results in that domain, the BOW baseline outperformed SFA and SCL-MI on average. In contrast to n-gram-based approaches, the training data provided was sufficient to infer a vocabulary, which made this classifier more stable. The SST model proved to be a good option in cross-domain, with significative results on electronics and kitchen reviews. Bollegala et al. [9] justified the low results on books because of the low number of unlabeled data available on that domain, which is necessary to create its sentiment sensitive thesaurus. Finally, our KE-Meta approach obtained the best results on books and DVD reviews, being again the most stable approach across domains, thanks to the combination of different base classifiers. KE-Meta
                           B
                        , the classifier that does not consider the vocabulary expansion, obtained not significative better results in all domains. Since the use of this base classifier improved the results in single-domain, future work is needed in order to understand how to improve its performance also in cross-domain.

Experimental results of Tables 5 and 6 show that review polarity classification of evaluated approaches differ across domains. These differences could be due to the different language employed by reviewers when commenting on products of different domains. In Table 7
                         we can see some statistics of the corpus divided by domain. While kitchen appliance and electronic reviewers evaluate using short comments, reviews of book and DVD domains are longer, e.g. some of them include a summary of the story. Interesting also the reduced percentage of nouns in kitchen compared to the rest. It seems that kitchen appliance reviewers do not cite so often other products, and use more qualifying adjectives. This makes this domain the easiest to classify, probably also explained by its shorter length. In general, single-domain n-gram-based approaches obtained better results with the two domains with shorter reviews. However, the same trend is not clearly appreciated for the BOW classifier.

We include in the table also statistics of the disambiguated senses. Note that the ratio between the number of different lemmas per domain and the different senses per domain is a measure of the polysemy employed
                           26
                           A value of 1.0 here highlights 0% of polysemy in the corpus.
                        
                        
                           26
                         by reviewers. As we can see, the results of our KE-Meta approach are better when the percentage of polysemy is lower and, consequently, less WSD effort is required.

@&#CONCLUSIONS@&#

In this work we introduced a knowledge-enhanced meta-classifier for single and cross-domain polarity classification. The main contributions of this work are: (i) KE-Meta, a new generic approach that combines different types of classifiers to categorize documents according to their polarity; and (ii) the study of the impact of WSD and vocabulary expansion-based features as document representation.

In single and cross-domain polarity classification, KE-Meta has proven to perform at par or better than state-of-the-art when classifying Amazon product reviews. Thanks to the combination of different classifiers, our approach obtained the most stable results across domains, and was able to excel in domains such as books and DVDs, which often combine a review and a summary of the product together. In contrast to the state-of-the-art, our meta-classifier does not perform any domain adaptation, which renders our approach generic. Moreover, the study of the information gain of our base classifiers concluded that WSD and vocabulary expansion-based features provide additional information not included in other BOW or n-gram-based classifiers.

Future work will investigate how it affects the inclusion of new base classifiers in KE-Meta. The use of other state-of-the-art approaches combined with our approach should provide better results. In addition, we will improve the current base classifiers, specially the vocabulary expansion-based one, to perform better both at single and cross-domain level. We will study also the performance of our classifier in other popular datasets like the well-known movie review dataset. Moreover, we will evaluate our polarity classification approach in other languages.
                        27
                        As we stated in Section 3.1, our approach is multilingual. This is due to the use of BabelNet, which performs WSD, vocabulary expansion, and mapping of SentiWordNet with the disambiguated words.
                     
                     
                        27
                      Finally, we will investigate how to apply multilingual semantic networks and knowledge graphs in other NLP tasks, from both, monolingual and multilingual perspectives.

@&#ACKNOWLEDGMENTS@&#

This research has been carried out in the framework of the European Commission WIQ-EI IRSES (No. 269180) and DIANA-APPLICATIONS – Finding Hidden Knowledge in Texts: Applications (TIN2012-38603-C02-01) projects. This research is partially funded by the national project ACOGEUS (TIN2012-38536-C03-02) and the regional project AORESCU (P11-TIC-7684 MO). We thank Juan M. Cotelo and Luis A. Leiva for their support and comments.

@&#REFERENCES@&#

