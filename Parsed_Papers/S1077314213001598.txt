@&#MAIN-TITLE@&#Automatic extraction of relevant video shots of specific actions exploiting Web data

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A novel method to automatically extract action video shots from the Web videos.


                        
                        
                           
                           Large-scale experiments with 100 human actions and 12 non-human actions.


                        
                        
                           
                           Exploiting action images helps enhance significantly performance.


                        
                        
                           
                           Employing human pose matching improves results of human actions.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Web video

Web image

Tag relevance

VisualRank

Spatio-temporal feature

Pose estimation

@&#ABSTRACT@&#


               
               
                  Video sharing websites have recently become a tremendous video source, which is easily accessible without any costs. This has encouraged researchers in the action recognition field to construct action database exploiting Web sources. However Web sources are generally too noisy to be used directly as a recognition database. Thus building action database from Web sources has required extensive human efforts on manual selection of video parts related to specified actions. In this paper, we introduce a novel method to automatically extract video shots related to given action keywords from Web videos according to their metadata and visual features. First, we select relevant videos among tagged Web videos based on the relevance between their tags and the given keyword. After segmenting selected videos into shots, we rank these shots exploiting their visual features in order to obtain shots of interest as top ranked shots. Especially, we propose to adopt Web images and human pose matching method in shot ranking step and show that this application helps to boost more relevant shots to the top. This unsupervised method of ours only requires the provision of action keywords such as “surf wave” or “bake bread” at the beginn ing. We have made large-scale experiments on various kinds of human actions as well as non-human actions and obtained promising results.
               
            

@&#INTRODUCTION@&#

The explosion of the Internet as well as the rising need of sharing information between people on the Internet has made it a huge and unstoppably growing data source. People upload to the Internet every kind of data including images, music and videos. YouTube is one of the most popular video sharing websites which allow people to easily upload their own videos and access to those of others. By using Web API like YouTube API, we can obtain a large number of videos of various topics from Web sources without any difficulties. Especially, many of the topics are related to human and human actions; therefore, there is an increasing tendency for action recognition researchers to construct human action database exploiting Web videos.

When users upload their videos, they usually attach to the videos keywords called as “tags”- useful metadata for video retrieval. However, in general, tags are annotated to the whole video sequence, not to specific scenes. Therefore, it cannot be determined which tag corresponds to which part of the video. For example, some videos tagged “eat” might include not only the eating scene but also such other scenes as entering restaurants, ordering foods, or drinking something (see Fig. 1
                     ). People who want to search for eating scenes have to manually skip the scenes of no interest while carefully watching the whole video. This manual search makes Web videos based database construction for specific actions become a very troublesome and time-consuming task.

In this paper, we propose a new method to automatically extract from tagged Web videos relevant video shots of specific actions using metadata as well as visual context of these videos. Note that video shots here refer to small fragments of a video obtained by separating it at each point of a scene or camera change. Our unsupervised method requires only the provision of action keywords at the beginning. As for keywords, we mainly focus on words related to human actions. Our list of human action keywords contains sport activities such as “serve volleyball” or “row dumbbell” as well as activities of daily living like “shave mustache” and “tie shoelace”. The list also includes some music related activities like “play trumpet” and “dance flamenco” or emotion related activities like “slap face” and “cry” as the consequence of “being angry” and “being sad” respectively. Moreover, we also tried several non-human actions such as “flowers bloom” or “leaves fall”. We want to demonstrate that our proposed system can be applied to extract relevant video shots of various types of actions from the Web.

If video shots corresponding to any “action verb” can be acquired automatically from unconstrained videos like Web videos, we can build easily training database for action recognition. So far, as mentioned above, the construction of action training data has been known as exceptionally expensive work, which is totally different from building object recognition database. In fact, while object recognition categorizes up to 10,000 objects, the largest widely used action recognition dataset includes only 51 human action categories [1]. On the other hand, by applying our method, video shots associated with unlimited types of actions can be easily collected from Web sources. Although a few modest manual scanning may still be needed to use these video shots as training data for action recognition, there is no doubt that human effort can be significantly reduced in comparison to fully manual database construction. In addition, the proposed method can be applied to improve Web video searching and tagging.

Our main idea is at first, selecting relevant videos among thousands of Web videos for specified action and then, extracting the most related shots from selected videos. The video selection step is based on our assumption that videos tagged with many relevant words have high probability of being relevant videos so they should be selected. For the extraction of corresponding shots, we apply an efficient unsupervised ranking method called VisualRank [2]. We made large-scale experiments on 100 human action keywords and 12 non-human action keywords. The experimental results reflect the effectiveness of our system as we achieved high precision for many keywords. Note that here precision is considered as the percentage of relevant shots among top ranked 100 shots (Precision@100).

Furthermore, we proposed to take still Web images corresponding to given actions into account, with the intuition that the shots with more similarity to related action images have higher probability of being relevant shots, thus they should be biased in shot ranking. In fact, recent works [3–6] show that action recognition exploiting still images is possible. We collect images related to the given actions automatically via Web image search engines based only on provided keywords and measure visual resemblances between video shots and selected images. Shots with higher similarity scores will have higher chance to be ranked to the top. Note that these Web images involved processes also do not require any supervision, therefore the automaticity of the whole framework can be preserved. We verify the efficiency of introducing Web images by applying Web images exploited framework on 28 human actions and 8 non-human actions with precision achieved by original framework respectively lower than 20% and 15%. The results demonstrate that exploiting Web action images can significantly improve the performance of the original system.

The contributions of this paper can be summarized as follows: (1) a practical system of automatic construction of an action video shot database consisting of tag-based video selection from a large number of tagged videos, and visual-feature-based shot selection using VisualRank extended with novel and effective settings of non-uniform damping vector, and (2) large-scale experiments on 100 categories of human action and 12 categories of non-human action to verify the efficiency of each step of the pipeline as well as the whole system.

This manuscript is the expansion of our previous conference papers [7,8]. We made several modifications in the algorithm over the previous version and obtained significant improvements in the results. We introduce an extension to our approach that integrates pose feature into shot-image similarity measurement (Section 4), as well as new results of the improved system (Section 5).

@&#RELATED WORK@&#

In this section, we refer to some related works on action recognition, Web image mining, tag ranking and cross-media retrieval.

For the past five years, spatio-temporal (ST) features which describe both spatial and temporal description of movement, and their bag-of-features (BoF) representation, due to their effectiveness, have been exploited by many researches on human action recognition and content-based video analysis. By using BoF of ST features, action recognition problem can be almost regarded as the same problem as object recognition except for feature extraction process.

One of recent works which adopt this methodology is Jones et al.’s work [9]. The novelty in their work is that the results are refined based on users’ relevance feedback following previous works in the image domain.

Nevertheless, the BoF model suffers from some limitations, one of which is the loss of some discriminative information in both spatial and temporal dimensions. As one of other effective models of human action recognition, a dense representation proposed by Zhen et al. [10] takes into account the motion and structure information simultaneously. In this work, high dimensional features are first extracted and then embedded into a compact and discriminative representation by discriminative locality alignment (DLA) method. On the other hand, instead of using all frames in the video sequence, Liu et al. [11] proposed to learn the most representative frames called as key frames by AdaBoost algorithm and represent action by the probabilistic distribution and temporal relationships of these frames.

The above mentioned works aimed to label to the whole content of each test video sequence one of the pre-defined categories, while our objective is to search among a large number of Web videos for only video parts which are associated with the given keywords. Moreover, while our proposed system is unsupervised, all of the above works apply supervised learning method which implements Support Vector Machines (SVMs) as the final classifiers. SVM is widely used in computer vision and machine learning in general and pattern recognition in particular.

Beside the supervised methods, there have been several attempts in unsupervised action recognition. Niebles et al. [12] categorized action videos in KTH datasets and their original ice-skating video data adopting the PLSA model. Niebles et al. [13] also proposed a method to extract human action sequences from unconstrained Web videos. Cinbis et al. [14] proposed a method to learn action models automatically from Web images gathered via Web image search engines, and recognize actions for the same video dataset as [13]. Although Cinbis et al. ’s work is the most similar to our work, they exploit only Web images and static features as a training source, while Web videos and spatio-temporal features are also adopted in our work. In addition, while works by both Niebles et al. and Cinbis et al. aim to recognize only human actions, our method does not restrict its applicability within any type of actions. Our method might be applied to collect relevant video shots of non-human actions such as “airplane fly” and “tornado”. As in another similar work, Ballan et al. [15] proposed a method to add tags to video shots by using Web images obtained from Flickr as training samples. Meanwhile, Laptev et al. [16–18] proposed methods to automatically associate movies and movie scripts. Their methods also enable the construction of an action shot database in an unsupervised manner, although targeted videos are limited to only the movies with available scripts.

Regarding still images, many works on automatic construction of image database exploiting images gathered from the Web have been carried out so far [19–24]. Most of these works use object recognition methods to select relevant images to specified keywords from “raw” Web images collected using Web image search engines. In fact, Web images acquired through Web image search engines like Google Image Search are not really so “raw” regarding their relevance to the given keywords. Google so far have applied a technology called VisualRank [2] to a group of initial search results. According to VisualRank, images found to share the most visual characteristics with the group at large would shall be determined as the most relevant ones and brought to the top of search results. To apply this idea to action video shots detection is our initial motivation of this work. Hence our work can be regarded as video shot version of the automatic search for relevant Web data of a given keyword.

In this paper, we perform tag analysis to compute tag based relevance scores. Having tags is a common characteristic of consumer generated media (CGM) data on the Web. Users are recommended to tag their uploaded media data with some words related to the data content so that other users can search for them. Yang et al. [25] proposed a method to evaluate a tag relevance score on each tag based on tag co-occurrence statistics which is called as “Web 2.0 Dictionary”. This method requires only tag analysis and no visual information. We apply this method to initially search for relevant Web videos. As a similar method which does not require visual features, Dong et al. [26] proposed a method to evaluate tag relevance score by combining the probabilistic relevance score estimation and random walk-based refinement. Although this two-step method is similar to ours, they use only tag information, while we exploit both tag and visual features.

Recently, some research effort has been made to cross-media retrieval [27,28]. In cross-media retrieval, the query and the returned results can be of different modalities. Our work seems to be similar to a work on cross-media retrieval which regards an action keyword as a query of type “text” and its relevant video shots as retrieval results of type “video shot”. However, in fact, our work and works on cross-media retrieval are different in terms of both data and research objective. Researches on cross-media retrieval deal with predefined Multimedia Documents (MMDs), each of which is a set of multimedia objects of different media types but carries the same semantic. Their objective is to map a new media object to one of these MMDs. On the other hand, our work is also based on multimedia objects but without guarantee that they are semantically related or not. Our objective is to extract from Web videos of a given action keyword its corresponding video shots. Regarding algorithm, while our proposed system is unsupervised, most of works on cross-media retrieval are supervised or semi-supervised with feedback [27,28].

In this paper, we propose a novel system of automatically extracting video shots corresponding to specific actions from tagged Web videos by providing action keywords only. Fig. 2
                      sketches the overview of proposed system. Our system consists of the four following processing steps:
                        
                           (1)
                           tag based video selection,

shot segmentation, feature extraction and calculation of shot-to-shot similarity matrix,

image selection and measurement of shot-to-image similarity scores,

video shot selection.

It should be noted that video shots are ranked in the final step, while videos are ranked in the first step. Step 3 is optional but highly recommended.

In the first step, video IDs and tag lists of 1000 Web videos corresponding to the keyword via Web API are gathered, and the relevance between each tag and the keyword is evaluated based on their co-occurrence frequency. Then 1000 videos are ranked in the descending order of the tag relevance scores, and only the top 200 videos are downloaded. In the second step, each downloaded video is segmented into several video shots and all the shots are converted into the bag-of-spatio-temporal-features (BoSTF) histograms. In this paper, as for spatio-temporal feature extraction, we apply the method proposed by Noguchi et al. [29].

The third step is an option in which Web images are taken into account to improve the final step. In this step, firstly, hundreds of top results of image search for given action keywords are downloaded using Bing API. Then, Web action images are automatically selected based on human detection method. Finally, similarity scores between shots and images are measured according to their static features. Note that human detected images are selected and images with no human detected are discarded only in case of human actions. In case of non-human actions, images directly retrieved by Bing API are adopted. The third step can be performed in one of two modes: for shots and images, (1) SURF features are extracted, and shot-to-image similarities are measured using feature matching. (2) Simple but efficient pose features which simulate the orientation of human body parts are extracted, and shot-to-image similarities are measured by comparing their pose features.

Both modes can be applied to human actions while the first mode is restricted to non-human actions only. Note that as for shots, we do not extract pose features from all of their frames but only one frame at every second since normally there is no significant change in one second. This also helps to reduce the cost of calculation.

In the final step, we rank video shots by applying graph-based ranking method, VisualRank [2], with a ST feature based shot similarity matrix and a predefined bias damping vector. In the end, we can obtain video shots corresponding to the given keywords in the upper rank of the video shot ranking results.

@&#PROPOSED METHOD@&#

Web videos associated with the given keywords can be obtained easily by using Web API. In case of YouTube, they provide YouTube API to search in their video database for the videos tagged with the given query words. However, since tags are assigned subjectively by the uploaders, sometimes tags are only weakly related or unrelated to the corresponding videos. The objective of this step is to select the more query-related videos to download.

Firstly, the given keywords are sent to the Web API to collect sets of video IDs and tags. Then the relevance scores of Web videos to the given keyword are calculated according to co-occurrence relationships between their tags. To this end, we apply the “Web 2.0 Dictionary” method proposed by Yang et al. [25] with some modifications in relevance measurement. “Web 2.0 Dictionary” corresponds to statistics on tag co-occurrence, which we need to construct in advance using a large number of tags gathered from the Web. This method is based on an idea that tags other than the query are supporters of the query, and the query can be regarded as being relevant to a video whose tags are its strong supporters.

Assume that N(t) is the number of the videos tagged with word t among all the Web videos, and 
                           
                              T
                           
                         is a set of all the words other than t tagged to all the Web videos. The correlation of parent word t and its child word 
                           
                              
                                 
                                    t
                                 
                                 
                                    i
                                 
                              
                              ∈
                              T
                           
                         is defined as
                           
                              (1)
                              
                                 w
                                 (
                                 t
                                 ,
                                 
                                    
                                       t
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       F
                                       (
                                       t
                                       ,
                                       
                                          
                                             t
                                          
                                          
                                             i
                                          
                                       
                                       )
                                    
                                    
                                       N
                                       (
                                       t
                                       )
                                    
                                 
                              
                           
                        where F(t, t
                        
                           i
                        ) is the number of videos tagged with both word t and word t
                        
                           i
                         at the same time. Let 
                           
                              
                                 
                                    T
                                 
                                 
                                    V
                                 
                              
                           
                         represent a set of tags for video V excluding t, we estimate relevance score of video V for word t, P(V—t), by substituting 
                           
                              
                                 
                                    T
                                 
                                 
                                    V
                                 
                              
                           
                         for V and w(t, t
                        
                           i
                        ) for P(t
                        
                           i
                        —t) as follows:
                           
                              (2)
                              
                                 P
                                 (
                                 V
                                 |
                                 t
                                 )
                                 ∝
                                 P
                                 (
                                 
                                    
                                       T
                                    
                                    
                                       V
                                    
                                 
                                 |
                                 t
                                 )
                                 =
                                 
                                    
                                       
                                          ∏
                                       
                                       
                                          
                                             
                                                t
                                             
                                             
                                                i
                                             
                                          
                                          ∈
                                          
                                             
                                                T
                                             
                                             
                                                V
                                             
                                          
                                       
                                    
                                 
                                 P
                                 (
                                 
                                    
                                       t
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 t
                                 )
                                 =
                                 
                                    
                                       
                                          ∏
                                       
                                       
                                          
                                             
                                                t
                                             
                                             
                                                i
                                             
                                          
                                          ∈
                                          
                                             
                                                T
                                             
                                             
                                                V
                                             
                                          
                                       
                                    
                                 
                                 w
                                 (
                                 t
                                 ,
                                 
                                    
                                       t
                                    
                                    
                                       i
                                    
                                 
                                 )
                              
                           
                        
                     

The above equations to calculate relevance of an image video to the given keyword are obtained by applying [25]. However if we multiply all the correlation values between the query tag and the rest of the tags within one video, the value of (2) becomes smaller as the number of tags increases. To prevent this, we modify (2) so that the number of co-occurrence words used for calculation is limited to m at most, and define the relevance score Sc
                        
                           t
                        (V) using average log likelihood as follows:
                           
                              (3)
                              
                                 S
                                 (
                                 V
                                 |
                                 t
                                 )
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       n
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             
                                                t
                                             
                                             
                                                i
                                             
                                          
                                          ∈
                                          
                                             
                                                T
                                             
                                             
                                                ′
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       log
                                    
                                    
                                       2
                                    
                                 
                                 w
                                 (
                                 t
                                 ,
                                 
                                    
                                       t
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       n
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             
                                                t
                                             
                                             
                                                i
                                             
                                          
                                          ∈
                                          
                                             
                                                T
                                             
                                             
                                                ′
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                log
                                             
                                             
                                                2
                                             
                                          
                                          F
                                          (
                                          t
                                          ,
                                          
                                             
                                                t
                                             
                                             
                                                i
                                             
                                          
                                          )
                                          -
                                          
                                             
                                                log
                                             
                                             
                                                2
                                             
                                          
                                          N
                                          (
                                          t
                                          )
                                       
                                    
                                 
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       n
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             
                                                t
                                             
                                             
                                                i
                                             
                                          
                                          ∈
                                          
                                             
                                                T
                                             
                                             
                                                ′
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       log
                                    
                                    
                                       2
                                    
                                 
                                 F
                                 (
                                 t
                                 ,
                                 
                                    
                                       t
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 -
                                 
                                    
                                       log
                                    
                                    
                                       2
                                    
                                 
                                 N
                                 (
                                 t
                                 )
                              
                           
                        
                        
                           
                              (4)
                              
                                 
                                    
                                       Sc
                                    
                                    
                                       t
                                    
                                 
                                 (
                                 V
                                 )
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       n
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             
                                                t
                                             
                                             
                                                i
                                             
                                          
                                          ∈
                                          
                                             
                                                T
                                             
                                             
                                                ′
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       log
                                    
                                    
                                       2
                                    
                                 
                                 F
                                 (
                                 t
                                 ,
                                 
                                    
                                       t
                                    
                                    
                                       i
                                    
                                 
                                 )
                              
                           
                        where 
                           
                              
                                 
                                    T
                                 
                                 
                                    ′
                                 
                              
                           
                         contains at most the top m word t
                        
                           i
                         in the descending order of w(t, t
                        
                           i
                        ), and n
                         (n
                        ⩽
                        m) represents 
                           
                              |
                              
                                 
                                    T
                                 
                                 
                                    ′
                                 
                              
                              |
                           
                        . Since the second term of (3) is always the same in the video set over the same action keyword, we omit it and define the relevance score Sc
                        
                           t
                        (V) as shown in (4). In the experiment, we set m as 10, and select the most relevant 200 videos to the given keyword from the 1000 videos returned by the Web API. This tag-based selection in the first step is important to allow only promising videos to go to the next step which requires more costly processes such as feature extraction and similarity calculation.

Note that in case of compound keywords such as “drink coffee”, we regard N(t) as the number of the videos including all of the element word of the compound keyword in their tag sets and w(t, t
                        
                           i
                        ) as the number of videos having all the words of t and t
                        
                           i
                         even if t
                        
                           i
                         is also a compound word. We ignore videos which do not have any co-occurrence tag since we cannot calculate their relevance scores.

In the experiments, as seed words, we prepared 150 sets of verbs and nouns which are related to such actions as “ride bicycle” or “launch shuttle”. We gathered 1000 video tags for each seed word, and extracted all the tags. As a result, we obtained 12,471 tags which appear more than five times among all the collected tags. For each of 12,471 words, we gathered 1000 video tags again, and constructed “our Web 2.0 Dictionary” by counting tag co-frequencies according to (1).

As a method on visual feature based shot ranking, we apply VisualRank [2] which is an image ranking method motivated by modifying the widely known Web page raking method, PageRank [30]. PageRank calculates ranking positions of Web pages using hyper-link structure of the Web. The ranking values are estimated using the steady state distribution of the random-walk Markov-chain probabilistic model. In the iterative processing, each page gives ranking points to its hyperlink destinations and the ranking point of the page linked by pages with high ranking points also becomes higher. VisualRank uses a similarity matrix of images instead of hyperlink structure. (5) represents the equation to compute VisualRank.
                           
                              (5)
                              
                                 r
                                 =
                                 α
                                 S
                                 r
                                 +
                                 (
                                 1
                                 -
                                 α
                                 )
                                 p
                                 
                                 
                                 (
                                 0
                                 ⩽
                                 α
                                 ⩽
                                 1
                                 )
                              
                           
                        where S is the column-normalized similarity matrix of images, 
                           p
                         is a damping vector, and 
                           r
                         is the ranking vector each element of which represents a ranking score of each image. α plays a role to control the effect of 
                           p
                        . Commonly, α is set as 0.85. The final value of 
                           r
                         is estimated by updating 
                           r
                         iteratively with (5). Because S is column-normalized and the sum of elements of 
                           p
                         is 1, the sum of elements of ranking vector 
                           r
                         also stays 1.

Although 
                           p
                         is defined as a uniform vector in VisualRank as well as PageRank, it is known that 
                           p
                         can be a bias vector which affects the ranking result. Basically, biased images have higher chance than the others to be ranked to the top. Jing et al. [2] proposed to assign bias values to only the top k images in output of a commercial image search engine. On the other hand, Haveliwala [31] proposed to let topic-preferences reflect PageRank scores by giving larger damping values for the elements corresponding to the Web pages related to the given topic.

Analogously, we use a non-uniform damping vector in place of a uniform damping vector. We define two kinds of bias vectors as follows:
                           
                              (6)
                              
                                 
                                    
                                       p
                                    
                                    
                                       i
                                    
                                    
                                       (
                                       1
                                       )
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   1
                                                   /
                                                   k
                                                
                                                
                                                   (
                                                   i
                                                   ⩽
                                                   k
                                                   )
                                                
                                             
                                             
                                                
                                                   0
                                                
                                                
                                                   (
                                                   i
                                                   >
                                                   k
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (7)
                              
                                 
                                    
                                       p
                                    
                                    
                                       i
                                    
                                    
                                       (
                                       2
                                       )
                                    
                                 
                                 =
                                 
                                    
                                       exp
                                       (
                                       SI
                                       (
                                       
                                          
                                             S
                                          
                                          
                                             i
                                          
                                       
                                       )
                                       )
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             j
                                             =
                                             1
                                          
                                          
                                             n
                                          
                                       
                                       exp
                                       (
                                       SI
                                       (
                                       
                                          
                                             S
                                          
                                          
                                             j
                                          
                                       
                                       )
                                       )
                                    
                                 
                              
                           
                        
                     

The bias vector 
                           p
                        
                        (1) represented in (6) is defined by giving uniform bias values to the elements corresponding to the top k shots regarding tag-based video scores which were calculated in the first step. We use this bias vector in case of not introducing Web images. On the other hand, in case that Web action images are exploited (7), a damping value for a shot 
                           
                              
                                 
                                    p
                                 
                                 
                                    i
                                 
                                 
                                    (
                                    2
                                    )
                                 
                              
                           
                         is proportional to its shot-image similarity score SI (S
                        
                           i
                        ) which is estimated in the third step.

In this subsection, we describe how to estimate the similarity matrix which appears in (7). In our work, this similarity matrix holds ST feature based similarity scores between shots. We first divide each downloaded video into several shots and extract ST features from all the shots. We then represent each shot as a BoSTF histogram and calculate similarity between shots as their histogram intersection.

After downloading the most relevant 200 videos to the given keyword regarding tag relevance scores, we segment downloaded videos into video shots based on their RGB histograms. We simply calculate 64 dimensional RGB histogram for each frame and record one segmentation point between two consecutive frames if their histogram intersection is larger than our predefined threshold. As the result, we obtain 10 shots per video on average.

However, there are some shots whose duration is too short or too long. It is hard for us to recognize what happens in a shot which lasts too short. In contrast, excessively long shots are supposed to be uninformative since there is no significant change in them. We consider a shot as too short one if its duration is smaller than one second, or too long one if it lasts more than one minute. Thus we select only shots which last longer than one second and shorter than one minute to go to the next step.

Following the method described in our previous paper [29], firstly, we detect interest points and extract feature vectors using the SURF method [32], and then we select moving interest points applying the Lucas-Kanade method [33]. Since ST features are supposed to represent movement of objects, only moving interest points are considered as ST interest points and static interest points are discarded. After detecting ST interest points, we apply Delaunay triangulation to form triples of interest points which hold both local appearance and motion features. We then track changes of flow directions of interest points as well as the sizes of the triangles within five consecutive frames. This tracking enables us to extract ST features not from only one point but from a triangle surface patch. Thus the features are expected to be more robust and informative. The ST features are extracted from every five frames. This method is relatively faster than the other ST feature extraction methods such as cuboid based method, since it employs SURF detector [32] and Lucas-Kanade detector [33] which are comparatively fast detectors. Fig. 3
                            shows an example of the process for extracting the ST features from a video shot of action “batting”.

Our proposed ST features has been demonstrated as being not only fast and easy to implement but also comparative to the-state-of-art [29].

To apply VisualRank method, we need to compute the similarities among all the shots to find out the shots sharing the most visual characteristics with others. To this end, we first vector-quantize them and convert them into the bag-of-features (BoF). While the standard BoF represents the distribution of local features within one image, the BoF employed in this paper represents the distribution of features within one shot which consists of several frame images. We call our BoF as bag-of-frames (BoFr). In the experiment, we set the size of the codebook as 5000.

The similarity between two shots is measured as their histogram intersection:
                              
                                 (8)
                                 
                                    s
                                    (
                                    
                                       
                                          H
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          H
                                       
                                       
                                          j
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             l
                                             =
                                             1
                                          
                                          
                                             |
                                             H
                                             |
                                          
                                       
                                    
                                    min
                                    (
                                    
                                       
                                          h
                                       
                                       
                                          i
                                          ,
                                          l
                                       
                                    
                                    ,
                                    
                                       
                                          h
                                       
                                       
                                          i
                                          ,
                                          l
                                       
                                    
                                    )
                                 
                              
                           where H
                           
                              i
                           , h
                           
                              i, l
                            and ∣H∣ represents the BoFr vector of the ith shots, its lth element and the dimension number of the BoFr vector, respectively.

Remind that employing Web images is an optional step based on our intuition that the shots which are more similar to corresponding action images have higher probability of being relevant shots. So the idea here is to select action images from Web images, calculate the similarities between shots and images, and then bias the shots with high similarities in the shot ranking step.

When an action keyword is queried on a Web image search engine, thousands of images might be returned. However, in general, even top results may be not relevant images of the queried action due to the wide variety of keyword’s meaning as well as the action itself, especially in the case of human action. Here we want to filter the returned results of Web image search engine so that the fewer irrelevant images the better. On the other hand, we also want to preserve the automaticity of our framework, thus manual selection is not preferred here. We postulate two assumptions: (1) the set of retrieved images contains relevant images of the queried action and (2) humans or body parts should be seen in human action images.

It is reasonable to consider that in case of human actions, images which contain humans are more likely related images than images in which humans do not appear. Based on these assumptions, we select a collection of action images by applying a human detection method [34,35] on Web images. For non-human actions, we simply select the first images returned by Web search engine and evaluate shot-image similarities by local feature matching (see 4.4.3).

Note that in the first proposed mode of shot-image similarity calculation, we only care if images contain humans or not and compute similarities between human detected images and shots based on SURF matching. On the other hand, the second mode requires more detailed analysis of human movements and adopts human pose estimation method (see 4.4.3 and 4.4.4).

In the first mode, we use Poselets method [34] to detect humans. Poselets are demonstrated as effective body part detectors trained by 3D human annotations. We apply Poselets detector tools which are officially offered by the authors
                              1
                              http://www.cs.berkeley.edu/%7Elbourdev/poselets/.
                           
                           
                              1
                            on the set of retrieved Web images using default parameters. Fig. 4
                            illustrates some examples of selected Web images using Poselets-based human detection.

Note that as shown in our previous work [8], the appropriate number of images to use in shot-similarity calculation step should be 20–30. Here we use 30 first human detected images.

In case of human action recognition, not only low-level features such as SURF and our proposed spatio-temporal feature but high-level features like human pose should be also adopted. Even though actions may depend on actors or situations which they are taken, the basic poses for humans to perform them in general are similar. Based on this idea, we extract features of human poses detected in shots and images, and compare poses using these features. We suppose that the similarity calculation based on pose comparison can achieve better performance than local-feature-matching-based calculation.

As for the characteristics of a pose, we pay attention to relations of body parts’ orientation or in other words, to their connection. We apply pose estimation models proposed by [35] which are flexible mixture models for capturing contextual co-occurrence and spatial relations between body parts. For each pose, their full body model
                              2
                              http://phoenix.ics.uci.edu/software/pose/.
                           
                           
                              2
                            detects 26 human body elements where 2 elements correspond to head, 4 elements relate to each limb and 8 elements point out torso (see Fig. 5
                           ).

Since our action category list contains some actions like “play piano” or “eat” which are most frequently taken when only upper bodies of actors appear, we also employ upper body model. In case of upper body pose estimation, the upper body model detects 2 elements of head, 4 elements of each of 2 arms, and 8 elements of torso (see Fig. 6
                           ).

From each detected pose, we simply extract inner orientation and correlation orientation of its parts as its features. Inner orientation here is defined as direction of a body part such as an arm or a torso. Correlation orientation here refers to spatial relations between a pair of body parts such as a head and a leg. Following is how we calculate inner orientation and correlation orientation.
                              
                                 (9)
                                 
                                    
                                       
                                          O
                                       
                                       
                                          in
                                       
                                    
                                    (
                                    P
                                    )
                                    =
                                    [
                                    
                                       
                                          dx
                                       
                                       
                                          1
                                       
                                    
                                    
                                       
                                          dy
                                       
                                       
                                          1
                                       
                                    
                                    …
                                    
                                       
                                          dx
                                       
                                       
                                          n
                                          -
                                          1
                                       
                                    
                                    
                                    
                                       
                                          dy
                                       
                                       
                                          n
                                          -
                                          1
                                       
                                    
                                    ]
                                    ,
                                    
                                    where
                                    
                                    
                                       
                                          dx
                                       
                                       
                                          i
                                       
                                    
                                    =
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    -
                                    
                                       
                                          x
                                       
                                       
                                          i
                                          +
                                          1
                                       
                                    
                                    ,
                                    
                                    
                                       
                                          dy
                                       
                                       
                                          i
                                       
                                    
                                    =
                                    
                                       
                                          y
                                       
                                       
                                          i
                                       
                                    
                                    -
                                    
                                       
                                          y
                                       
                                       
                                          i
                                          +
                                          1
                                       
                                    
                                 
                              
                           
                           
                              
                                 (10)
                                 
                                    
                                       
                                          O
                                       
                                       
                                          co
                                       
                                    
                                    (
                                    
                                       
                                          P
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          P
                                       
                                       
                                          j
                                       
                                    
                                    )
                                    =
                                    [
                                    
                                       
                                          X
                                       
                                       
                                          
                                             
                                                P
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                    -
                                    
                                       
                                          X
                                       
                                       
                                          
                                             
                                                P
                                             
                                             
                                                j
                                             
                                          
                                       
                                    
                                    
                                    
                                       
                                          Y
                                       
                                       
                                          
                                             
                                                P
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                    -
                                    
                                       
                                          Y
                                       
                                       
                                          
                                             
                                                P
                                             
                                             
                                                j
                                             
                                          
                                       
                                    
                                    ]
                                 
                              
                           where O
                           
                              in
                           (P) means inner orientation of part P and O
                           
                              co
                           (P
                           
                              i
                           , P
                           
                              j
                           ) refers to correlation orientation between part P
                           
                              i
                            and part P
                           
                              j
                           . (x
                           
                              i
                           , y
                           
                              i
                           ) represents position of element i of part P which has n elements. (X
                           
                              P
                           , Y
                           
                              P
                           ) is defined as center position of part P.

Finally, for each detected pose, we obtain a 70 dimensional feature. Note that for an image or a shot frame, first we apply the full body model. If the full body model fails to detect human pose, we then try the upper body model. If the upper body model succeeds to detect an upper pose, we calculate its orientation except for leg related orientation which will be regarded as 0. This enables us to compare poses even in case that they are detected by different body models.

For shot-image similarity calculation, we first extract SURF local features [32] from all action images of selected set and each one frame per five consecutive frames of all the shots. For each shot, we count matching points between SURF local features extracted from each frame and each Web image by thresholding Euclidean distances between SURF feature vectors. The similarity SI (S
                           
                              i
                           ) between a shot S
                           
                              i
                            which has M frame images (F
                           
                              j
                           (j
                           =1…M)) and an image set 
                              
                                 I
                              
                            which has N images (I
                           
                              k
                           (k
                           =1…N)) is calculated by the following equations:
                              
                                 (11)
                                 
                                    SI
                                    (
                                    
                                       
                                          S
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             k
                                             =
                                             1
                                          
                                          
                                             N
                                          
                                       
                                    
                                    
                                       
                                          
                                             max
                                          
                                          
                                             j
                                             =
                                             1
                                             :
                                             M
                                          
                                       
                                    
                                    SI
                                    (
                                    
                                       
                                          F
                                       
                                       
                                          j
                                       
                                    
                                    |
                                    
                                       
                                          I
                                       
                                       
                                          k
                                       
                                    
                                    )
                                 
                              
                           
                           
                              
                                 (12)
                                 
                                    whereSI
                                    (
                                    
                                       
                                          F
                                       
                                       
                                          j
                                       
                                    
                                    |
                                    
                                       
                                          I
                                       
                                       
                                          k
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          2
                                          ∗
                                          MatchPoint
                                          (
                                          
                                             
                                                F
                                             
                                             
                                                j
                                             
                                          
                                          ,
                                          
                                             
                                                I
                                             
                                             
                                                k
                                             
                                          
                                          )
                                       
                                       
                                          (
                                          Point
                                          (
                                          
                                             
                                                F
                                             
                                             
                                                j
                                             
                                          
                                          )
                                          +
                                          Point
                                          (
                                          
                                             
                                                I
                                             
                                             
                                                k
                                             
                                          
                                          )
                                          )
                                       
                                    
                                 
                              
                           Match Point (F
                           
                              j
                           , I
                           
                              k
                           ), Point (F
                           
                              j
                           ) and Point (I
                           
                              k
                           ) represent the number of matched points between a frame image F
                           
                              j
                            and a Web image I
                           
                              k
                           , the number of extracted SURF features from F
                           
                              j
                            and the number of extracted SURF features from I
                           
                              k
                           , respectively.

Like the above mode of shot-image similarity calculation, the similarity between a shot and a set of images is regarded as the similarity of its frame with the highest similarity score, and the similarity between a frame and a set of images is equal to normalized total similarity of that frame to all images in the set. Here we simply define pose comparison based similarity between a frame and an image as Euclidean distance between the poses. However, in case of comparison between the upper body pose and the full body pose, we disregard leg associated elements. That means we only compare upper parts of the poses in this case. Moreover, since calculation of distance between two full poses will result in higher value than other cases due to extra leg related distance, we normalize it as follows:
                              
                                 (13)
                                 
                                    
                                       
                                          SI
                                       
                                       
                                          ′
                                       
                                    
                                    (
                                    F
                                    |
                                    I
                                    )
                                    =
                                    SI
                                    (
                                    F
                                    |
                                    I
                                    )
                                    ∗
                                    
                                       
                                          number
                                          
                                          of
                                          
                                          elements
                                          
                                          unrelated
                                          
                                          to
                                          
                                          legs
                                       
                                       
                                          total
                                          
                                          number
                                          
                                          of
                                          
                                          elements
                                       
                                    
                                 
                              
                           
                        

In this calculation of ours, the number of orientation elements unrelated to legs and total number of orientation elements equal to 40 and 70, respectively.

@&#EXPERIMENTS AND RESULTS@&#

In our experiments, we used YouTube videos as our data source. We collected video metadata including video IDs and tags using YouTube Data API. To examine the effectiveness of our proposed method, we make large-scale experiments on 100 human action categories and 12 non-human action categories with video metadata analysis on 112,000 YouTube videos and spatio-temporal feature analysis on 22,400 YouTube videos.

In each experiment, we obtained rankings of 2000 shots in average for each action, since as we mentioned above, we downloaded 200 videos for each action and each video is segmented into 10 shots in average. For the evaluation of recognition results, average precision is widely used. However, here we use the precision rate over top ranked 100 shots since we expect that they are qualified to be used for action database construction while commonly used datasets such as KTH dataset [36] and “in-the-wild” YouTube dataset [37] have approximately 100 video shots per action.
                           3
                           KTH dataset has 599 shots for 6 actions, and “in-the-wild” dataset has 1168 shots for 11 actions.
                        
                        
                           3
                         That means in each experiment, we simply count the number of relevant shots among 100 top ranked shots NR and the precision achieved in that experiment is computed as NR/100.

The purpose of the first experiment is to validate our proposed framework when Web action images are not taken into account. We call this experiment as Exp.1. This means in Exp.1, shot selection step involves only spatio-temporal features and biases the top k shots regarding tag relevance scores (Eq. 6). We conduct Exp.1 on our full action category set which consists of 100 human action categories and 12 non-human action categories. The results for human actions and non-human actions are summarized in Table 1
                         and Table 2
                        , respectively.

As shown in Table 1, the mean of the precision at 100 shots over 100 human actions was 36.6%, and the precision varies from 2 to 100 depending on each action category. Top 34 actions regarding precision obtained 66 relevant shots among top ranked 100 shots in average and 14 actions achieved precision higher than 70%. Fig. 7
                         shows some example results of some of successful action categories. However, the proposed framework failed to extract relevant shots for some actions (Fig. 8
                        ). In the case of “boil egg”, some shots are actually related to “egg” but few of them describe exactly “boil egg” action. In cases of actions like “smile”, the action itself is too ambiguous to recognize. “Smile” is one of facial expressions which are mostly researched by emotion recognition works. Our proposed framework cannot distinguish “smile” and other facial actions. As for action keywords like “jog”, we could not select relevant videos of theirs due to tag noise as well as the variety in meaning of the keywords. Downloaded videos of “jog” mainly consist of videos about TV shows, movies or even motorbikes called as “jog”.

As for non-human actions, we obtained 14.9% as average precision. While some categories like “flower blooming” or “tornado” obtained quite a number of relevant shots at the top, some categories such as “leaves falling” and “waterfall” detected just very few relevant shots (Fig. 9
                        ). In fact, for “leaves falling” or “waterfall” categories, most of collected videos are unrelated to the actions. The main reason is that tag noise led to the failure in relevant video selection. Improving the video selection step is one of our future works.

To examine the efficiency of introducing Web action images, we validate our full system including the optional step on 28 human action categories and 8 non-human action categories which showed the lowest precision in the first experiment. Note that the local feature matching based mode can run on both human actions and non-human actions while pose comparison based mode works with human actions only. Exp.2 and Exp.3 are defined as follows:
                           
                              (1)
                              Exp.2: Web images+local feature matching based shot-similarity calculation exploited.

Exp.3: Web images+Pose comparison based shot-similarity calculation exploited.

We show results for human actions and non-human actions in Tables 3
                         and 4
                         respectively. For human actions dataset, we want to evaluate the effectiveness of adopting Web action images and compare two modes of shot-similarity calculation.

As shown in Table 3, introducing Web images helps to enhance the performance for human actions by 6.2% and 8.8% in average in case of exploiting local feature matching mode and pose matching mode respectively. For non-human actions, experimental results (Table 4) demonstrate that by introducing Web images into shot ranking, we can improve the precision from 4.4% to 18.6% in average. That means even in case where the tag noise led to the selection of irrelevant videos, our proposed method still can extract from those videos a number of action related video shots. Fig. 10
                         and Fig. 11
                         respectively shows some relevant shots which were detected by taking Web images into account in case of human actions and non-human actions.

We realized that local feature matching based method improved the performance in average but degraded it in cases of several categories such as “slap face”, “wash clothes” and “comb hair”. On the other hand, exploiting shot-to-image similarity measurement based on pose comparison not only obtained the highest precision in average but also outperformed Web images unexploited framework for most actions except for “swim” related ones. In case of “swim”, human pose estimation failed to detect humans in water, hence obtained shots are mostly human detected shots such as medal rewarding, interviewing. (Fig. 12
                        ). These results match with our expectation that in general, for human action learning, human poses hold very informative clues that should be exploited (Fig. 13
                        ) and and applying human pose matching to measure similarities between human action images can achieve better results than using low-level features only.

To confirm this hypothesis, we further conducted more experiments on other human action categories using pose matching between video shots and images introduced framework. We selected randomly 17 human action categories from actions which showed precision higher than 20% but lower than 35% in image unexploited framework. As expected, the performance was remarkably improved as it rose from 26.8% to 36.8% in average and the full system outperforms Web images unexploited system in most of categories. The results are summarized in Table 5
                         and result examples are shown in Fig. 14
                        .

@&#CONCLUSIONS@&#

In this paper, we proposed a method of automatically extracting from Web videos video shots corresponding to specific actions by only providing action keywords. The empirical results show that the performance of proposed framework depends on the action categories and selection of action keywords. For some actions the proposed method worked very well. For example, even when action images are not taken into account, precision rates of the best 24 and 35 actions exceed 50% and 40%, respectively. We demonstrated that exploiting action images helps significantly enhance performance of shot ranking step. Particularly in case of human actions, exploiting proposed shot-image pose matching method improved precision rates of most of experimented 48 actions.

To the best of our knowledge, we are the first to aim at automatic construction of such a large-scale database for action recognition. Moreover, we propose a novel method which exploits both metadata and visual features including ST features as well as pose features of Web videos, and also makes use of Web images. Even though we could not achieve high precision for every action category, applying our framework can definitely reduce huge human labor in comparison to traditional exhausted manual database building.

As future works, we plan to improve video selection step and adopt more context features such as human-object interactions or scene information to our framework.

@&#REFERENCES@&#

