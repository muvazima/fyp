@&#MAIN-TITLE@&#Particle swarm optimization for time series motif discovery

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We consider the task of finding repeated segments or motifs in time series.


                        
                        
                           
                           We propose a new standpoint to the task: formulating it as an optimization problem.


                        
                        
                           
                           We apply particle swarm optimization to solve the problem.


                        
                        
                           
                           The proposed solution finds comparable motifs in substantially less time.


                        
                        
                           
                           The proposed standpoint brings in an unprecedented degree of flexibility to the task.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Motifs

Time series

Anytime algorithms

Particle swarm optimization

Multimodal optimization

@&#ABSTRACT@&#


               
               
                  Efficiently finding similar segments or motifs in time series data is a fundamental task that, due to the ubiquity of these data, is present in a wide range of domains and situations. Because of this, countless solutions have been devised but, to date, none of them seems to be fully satisfactory and flexible. In this article, we propose an innovative standpoint and present a solution coming from it: an anytime multimodal optimization algorithm for time series motif discovery based on particle swarms. By considering data from a variety of domains, we show that this solution is extremely competitive when compared to the state-of-the-art, obtaining comparable motifs in considerably less time using minimal memory. In addition, we show that it is robust to different implementation choices and see that it offers an unprecedented degree of flexibility with regard to the task. All these qualities make the presented solution stand out as one of the most prominent candidates for motif discovery in long time series streams. Besides, we believe the proposed standpoint can be exploited in further time series analysis and mining tasks, widening the scope of research and potentially yielding novel effective solutions.
               
            

@&#INTRODUCTION@&#

Time series are sequences of real numbers measured at successive, usually regular time intervals. Data in the form of time series pervade science, business, and society. Examples range from economics to medicine, from biology to physics, and from social to computer sciences. Repetitions or recurrences of similar phenomena are a fundamental characteristic of non-random natural and artificial systems and, as a measurement of the activity of such systems, time series often include pairs of segments of strikingly high similarity. These segment pairs are commonly called motifs [34], and their existence is unlikely to be due to chance alone. In fact, they usually carry important information about the underlying system [41]. Thus, motif discovery is fundamental for understanding, characterizing, modeling, and predicting the system behind the time series. Besides, motif discovery is a core part of several higher-level algorithms dealing with time series, in particular classification, clustering, summarization, compression, and rule-discovery algorithms [see, e.g., 40, 41].

Identifying similar segment pairs or motifs typically implies examining all pairwise comparisons between all possible segments in a time series. This, specially when dealing with long time series streams, results in prohibitive time and space complexities. It is for this reason that the majority of motif discovery algorithms resort to some kind of data discretization or approximation that allows them to hash and retrieve segments efficiently. Following the works by Lin et al. [34] and Chiu et al. [13], many of such approaches employ the SAX representation [35] and/or a sparse collision matrix [9]. These allow them to achieve a theoretically low computational complexity, but sometimes at the expense of very high constant factors. In addition, approximate algorithms usually suffer from a number of data-dependent parameters that, in most situations, are not intuitive to set (e.g., time/amplitude resolutions, dissimilarity radius, segment length, minimum segment frequency, etc.).

A few recent approaches overcome some of these limitations. For instance, Castro and Azevedo [11] propose an amplitude multi-resolution approach to detect frequent segments, Li and Lin [33] use a grammar inference algorithm for exploring motifs with lengths above a certain threshold, Wilson et al. [55] use concepts from immune memory to deal with different lengths, and Floratou et al. [18] combine suffix trees with segment models to find motifs of any length. Nevertheless, in general, these approaches still suffer from other data-dependent parameters whose correct tuning can require considerable time. In addition, approximate algorithms are restricted to a specific dissimilarity measure between segments (the one implicit in their discretization step) and do not allow easy access to preliminary results, which is commonly known as anytime algorithms [58]. Finally, to the best of our knowledge, only the authors in [51,52,56] consider the identification of motif pairs containing segments of different lengths. This can be considered a relevant feature, as it produces better results in a number of different domains [56].

In contrast to approximate approaches, algorithms that do not discretize the data have been comparatively much less popular, with low efficiency generally. Exceptions to this statement achieved efficiency by sampling the data stream [12] or by identifying extreme points that constrained the search [38]. In fact, until the work of Mueen and Keogh [44], the exact identification of time series motifs was thought to be intractable for even time series of moderate length. In said work, a clever segment ordering was combined with a lower bound based on the triangular inequality to yield the true, exact, most similar motif. According to the authors, the proposed algorithm was more efficient than existing approaches, including all exact and many approximate ones [44]. After Mueen et al.’s work, a number of improvements have been proposed, the majority focusing on eliminating the need to set a fixed segment length [39,45,57].

Mueen himself has recently published a variable-length motif discovery algorithm which clearly outperforms the iterative search for the optimal length using the algorithm in [44] and, from the reported numbers, also outperforms further approaches as in [39,45,57]. This algorithm, called MOEN [40], is essentially parameter-free, and is believed to be one of the most efficient motif discovery algorithms available nowadays. However, its execution time may still be unaffordable in a number of situations. Furthermore, MOEN is specifically designed to work with Euclidean distances after z-normalization. In general, exact motif discovery algorithms have important restrictions with regard to the dissimilarity measure, and many of them still suffer from being non-intuitive and tedious to tune parameters. Moreover, few of them allow for anytime versions and, to the best of our knowledge, not one of them is able to identify motif pairs containing segments of different lengths. With the approach we propose here we try to overcome all these shortcomings at the same time.

In this article, we propose a new standpoint to time series motif discovery by treating the problem as an anytime multimodal optimization task. To the best of our knowledge, this standpoint is completely unseen in the literature. We first motivate such a standpoint and discuss its multiple advantages (Section 2). Next, we present SWARMMOTIF (Section 3), an anytime algorithm for time series motif discovery based on particle swarm optimization (PSO). We subsequently evaluate the performance of the proposed approach using 9 different real-world time series from distinct domains (Section 4). These include economics, car traffic, entomology, medical data, audio, climate, and power consumption. Our results show that SWARMMOTIF is extremely competitive when compared to the state-of-the-art, obtaining motif pairs of comparable similarity in considerably less time and with minimum storage requirements (Section 5). Moreover, we show that SWARMMOTIF is significantly robust against different implementation choices. These two aspects, together with its flexibility and extension capabilities, make SWARMMOTIF a unique novel solution for time series motif discovery. The latter implies that SWARMMOTIF can, for instance, deal with motifs of different lengths, apply uniform scaling, use any suitable dissimilarity measure, or incorporate notions of motif frequency. To conclude, we briefly comment on the application of multimodal optimization techniques to time series analysis and mining, which we believe has great potential (Section 6). The data and code used in our experiments are available online
                        1
                     
                     
                        1
                        
                           http://www.iiia.csic.es/~jserra/swarmmotif
                        
                     .

From the work by Mueen et al. [40,44], we can derive a formal, generic similarity-based definition [41] of time series motifs. Given a time series 
                           z
                         of length n, 
                           
                              z
                              =
                              [
                              
                                 z
                                 1
                              
                              ,
                              ⋯
                              
                                 z
                                 n
                              
                              ]
                              ,
                           
                         a normalized segment dissimilarity measure D, and a temporal window of interest between w
                        min and w
                        max samples, the top-k time series motifs 
                           
                              M
                              =
                              {
                              
                                 m
                                 1
                              
                              ,
                              ⋯
                              
                                 m
                                 k
                              
                              }
                           
                         correspond to the k most similar segment pairs 
                           
                              
                                 z
                                 a
                                 
                                    w
                                    a
                                 
                              
                              =
                              
                                 [
                                 
                                    z
                                    a
                                 
                                 ,
                                 ⋯
                                 
                                    z
                                    
                                       a
                                       +
                                       
                                          w
                                          a
                                       
                                       −
                                       1
                                    
                                 
                                 ]
                              
                           
                         and 
                           
                              
                                 z
                                 b
                                 
                                    w
                                    b
                                 
                              
                              =
                              
                                 [
                                 
                                    z
                                    b
                                 
                                 ,
                                 ⋯
                                 
                                    z
                                    
                                       b
                                       +
                                       
                                          w
                                          b
                                       
                                       −
                                       1
                                    
                                 
                                 ]
                              
                              ,
                           
                         for wa, wb
                         ∈ [w
                        min, w
                        max], 
                           
                              a
                              ∈
                              [
                              1
                              ,
                              n
                              −
                              
                                 w
                                 a
                              
                              +
                              1
                              ]
                              ,
                           
                         and 
                           
                              b
                              ∈
                              [
                              1
                              ,
                              n
                              −
                              
                                 w
                                 b
                              
                              +
                              1
                              ]
                           
                        . Thus, we see that the ith motif can be fully described by the tuple 
                           
                              
                                 m
                                 i
                              
                              =
                              
                                 {
                                 a
                                 ,
                                 
                                    w
                                    a
                                 
                                 ,
                                 b
                                 ,
                                 
                                    w
                                    b
                                 
                                 }
                              
                           
                        . To avoid so-called trivial matches [34], we can force that motifs are non-overlapping
                           2
                        
                        
                           2
                           Notice that, following [40], this definition can be trivially extended to different degrees of overlap.
                        , that is, 
                           
                              a
                              +
                              
                                 w
                                 a
                              
                              <
                              b
                           
                         or 
                           
                              b
                              +
                              
                                 w
                                 b
                              
                              <
                              a
                           
                        . The motifs in 
                           M
                         are ordered from lowest to highest dissimilarity such that 
                           
                              D
                              
                                 (
                                 
                                    m
                                    1
                                 
                                 )
                              
                              ≤
                              D
                              
                                 (
                                 
                                    m
                                    2
                                 
                                 )
                              
                              ≤
                              ⋯
                              ≤
                              D
                              
                                 (
                                 
                                    m
                                    k
                                 
                                 )
                              
                           
                         where 
                           
                              D
                              
                                 (
                                 
                                    m
                                    i
                                 
                                 )
                              
                              =
                              D
                              
                                 (
                                 
                                    {
                                    a
                                    ,
                                    
                                       w
                                       a
                                    
                                    ,
                                    b
                                    ,
                                    
                                       w
                                       b
                                    
                                    }
                                 
                                 )
                              
                              =
                              D
                              
                                 (
                                 
                                    z
                                    a
                                    
                                       w
                                       a
                                    
                                 
                                 ,
                                 
                                    z
                                    b
                                    
                                       w
                                       b
                                    
                                 
                                 )
                              
                           
                        . An example of a time series motif pair from a real data set is shown in Fig. 1
                        .

It is important to stress that D needs to normalize with respect to the lengths of the considered segments. Otherwise, we would not be able to compare motifs of different lengths. There are many ways to normalize with respect to the length of the considered segments. Ratanamahatana and Keogh [49] list a number of intuitive normalization mechanisms for dynamic time warping that can easily be applied to other measures. For instance, in the case of a dissimilarity measure based on the Lp norm, we can directly divide by the segment length
                           3
                        
                        
                           3
                           The only exception is with L∞, which could be considered as already being normalized.
                        , using brute-force upsampling to the largest length when wa
                         ≠ wb
                        .

From the definitions above, we can see that a brute-force search in the motif space for the most similar motifs is of 
                           
                              O
                              (
                              
                                 n
                                 2
                              
                              
                                 
                                    
                                       w
                                       Δ
                                    
                                 
                                 2
                              
                              )
                              ,
                           
                         where 
                           
                              
                                 w
                                 Δ
                              
                              =
                              
                                 w
                                 max
                              
                              −
                              
                                 w
                                 min
                              
                              +
                              1
                           
                         (for the final time complexity one needs to further multiply by the cost of calculating D). Hence, for instance, in a perfectly feasible case where 
                           
                              n
                              =
                              
                                 10
                                 7
                              
                           
                         and 
                           
                              
                                 w
                                 Δ
                              
                              =
                              
                                 10
                                 3
                              
                              ,
                           
                         we have 1020 possibilities. Magnitudes like this challenge the memory and speed of any optimization algorithm, specially if we have no clue to guide the search [23]. However, it is one of our main objectives to show here that time series generally provide some continuity to this search space, and that this continuity can be exploited by optimization algorithms.

A fundamental property of time series is autocorrelation, implying that consecutive samples in a time series have some degree of resemblance and that, most of the time, we do not observe extremal differences between them
                           4
                        
                        
                           4
                           If a time series had no autocorrelation, we might better treat it as an independent random process.
                         
                        [29]. This property, together with the established ways of computing similarity between time series [50], is what gives continuity to our search space. Consider a typical dissimilarity measure like dynamic time warping between z-normalized segments and the time series of Fig. 1. If we fix the motif starting points a and b to some random values, we can compute 
                           
                              D
                              (
                              
                                 z
                                 a
                                 i
                              
                              ,
                              
                                 z
                                 b
                                 j
                              
                              )
                           
                         for 
                           
                              i
                              ,
                              j
                              =
                              
                                 w
                                 min
                              
                              ,
                              ⋯
                              ,
                              
                                 w
                                 max
                              
                           
                         (Fig. 2
                        A). We see that these two dimensions have a clear continuity, i.e., that 
                           
                              D
                              
                                 (
                                 
                                    z
                                    a
                                    i
                                 
                                 ,
                                 
                                    z
                                    b
                                    j
                                 
                                 )
                              
                              ∼
                              D
                              
                                 (
                                 
                                    z
                                    a
                                    
                                       i
                                       +
                                       1
                                    
                                 
                                 ,
                                 
                                    z
                                    b
                                    j
                                 
                                 )
                              
                              ∼
                              D
                              
                                 (
                                 
                                    z
                                    a
                                    i
                                 
                                 ,
                                 
                                    z
                                    b
                                    
                                       j
                                       +
                                       1
                                    
                                 
                                 )
                              
                              ∼
                              D
                              
                                 (
                                 
                                    z
                                    a
                                    
                                       i
                                       +
                                       1
                                    
                                 
                                 ,
                                 
                                    z
                                    b
                                    
                                       j
                                       +
                                       1
                                    
                                 
                                 )
                              
                              ,
                           
                         and so forth. Similarly, if we fix the motif lengths wa
                         and wb
                         to some random values, we can compute 
                           
                              D
                              (
                              
                                 z
                                 i
                                 
                                    w
                                    a
                                 
                              
                              ,
                              
                                 z
                                 j
                                 
                                    w
                                    b
                                 
                              
                              )
                           
                         for 
                           
                              i
                              =
                              1
                              ,
                              ⋯
                              n
                              −
                              
                                 w
                                 a
                              
                           
                         and 
                           
                              j
                              =
                              1
                              ,
                              ⋯
                              n
                              −
                              
                                 w
                                 b
                              
                           
                         (Fig. 2B). We see that the remaining two dimensions of the problem also have some continuity, i.e., 
                           
                              D
                              
                                 (
                                 
                                    z
                                    i
                                    
                                       w
                                       a
                                    
                                 
                                 ,
                                 
                                    z
                                    j
                                    
                                       
                                          w
                                          b
                                       
                                       +
                                       j
                                    
                                 
                                 )
                              
                              ∼
                              D
                              
                                 (
                                 
                                    z
                                    
                                       i
                                       +
                                       1
                                    
                                    
                                       w
                                       a
                                    
                                 
                                 ,
                                 
                                    z
                                    
                                       j
                                    
                                    
                                       w
                                       b
                                    
                                 
                                 )
                              
                              ∼
                              D
                              
                                 (
                                 
                                    z
                                    
                                       i
                                    
                                    
                                       w
                                       a
                                    
                                 
                                 ,
                                 
                                    z
                                    
                                       j
                                       +
                                       1
                                    
                                    
                                       w
                                       b
                                    
                                 
                                 )
                              
                              ∼
                              D
                              
                                 (
                                 
                                    z
                                    
                                       i
                                       +
                                       1
                                    
                                    
                                       w
                                       a
                                    
                                 
                                 ,
                                 
                                    z
                                    
                                       j
                                       +
                                       1
                                    
                                    
                                       w
                                       b
                                    
                                 
                                 )
                              
                              ,
                           
                         and so forth. The result is a four-dimensional, multimodal, continuous but noisy
                           5
                        
                        
                           5
                           We use the term noisy here to stress that the continuity of the space may be altered at some points due to potential noise in the time series. It is not the case that we have a noisy, unreliable dissimilarity measurement D that could change in successive evaluations.
                         motif space, where the dissimilarity D acts as the fitness measure (or objective function) and the top-k valley peaks (considering dissimilarity) correspond to the top-k motifs in 
                           M
                        .

Finding an optimization algorithm that can locate the global minima of the previous search spaces faster than existing motif discovery algorithms can be a difficult task. However, we have robust and established algorithms for efficiently locating prominent local minima in complex search spaces [4,6,27]. Hence, we can intuitively devise a simple strategy: if we keep the best found minima and randomly reinitialize the optimization algorithm every time it stagnates, we should, sooner or later, start locating the global minima. In the meantime, we could have obtained relatively good candidates. This corresponds to the basic paradigm of anytime algorithms [58].

Anytime algorithms have recently been highlighted as “very beneficial for motif discovery in massive [time series] datasets” [57]. In an anytime algorithm for motif discovery, 
                           
                              D
                              (
                              
                                 m
                                 i
                              
                              )
                           
                         improves over time, until it reaches the top-k dissimilarity values 
                           
                              D
                              
                                 
                                    (
                                    
                                       m
                                       i
                                    
                                    )
                                 
                                 *
                              
                           
                         obtained by a brute-force search approach. Thus, we gradually improve 
                           M
                         until we reach the true exact solution 
                           
                              M
                              *
                           
                        . A good anytime algorithm will quickly find low 
                           
                              D
                              (
                              
                                 m
                                 i
                              
                              )
                              ,
                           
                         ideally reaching 
                           
                              D
                              
                                 
                                    (
                                    
                                       m
                                       i
                                    
                                    )
                                 
                                 *
                              
                           
                         earlier than its non-anytime competitors (Fig. 3
                        ).

Note that a good but suboptimal 
                           M
                         may suffice in most situations, without the need that 
                           
                              M
                              =
                              
                                 M
                                 *
                              
                           
                        . This is particularly true for more exploratory tasks, where one is typically interested in data understanding and visual inspection [see 41], and can also hold for other tasks, as top-k motifs can be very similar among themselves. In the latter situation, given a seed within 
                           
                              
                                 M
                                 *
                              
                              ,
                           
                         we can easily and efficiently retrieve further repetitions via common established approaches [28,48]. Thus, only non-frequent or singular motifs may be missed. These can be valuable too, as the fact that they are non-frequent does not imply that they cannot carry important information (think for example of extreme events of interest that perhaps only happen twice in a measurement). For those singular motifs, we can wait longer if using an anytime algorithm, or we can resort to the state-of-the-art if that is able to provide its output within an affordable time limit.

The continuity and anytime observations above relax the requirements for the optimization algorithm to be employed in the considered motif spaces (Sections 2.2 and 2.3). In fact, if we do not have to assess the global optimality of a solution, we have a number of approaches that can deal with large, multimodal, continuous but noisy search spaces [4,6,27]. Among them, we choose PSO [2,5,14,46,47]. PSO is a population-based stochastic approach for solving continuous and discrete optimization problems [5] which has been applied to multimodal problems [3]. It is a metaheuristic [4], meaning that it cannot guarantee whether the found solution corresponds to a global optimum. The original PSO algorithm cannot even guarantee the convergence to a local optimum, but adapted versions of it have been proven to solve this issue [53]. Other versions guarantee the convergence to the global optimum, but only with the number of iterations approaching infinity [53].

PSO has gained increasing popularity among researchers and practitioners as a robust and efficient technique for solving difficult optimization problems. It makes few or no assumptions about the problem being optimized, does not require it to be differentiable, can search very large spaces of candidate solutions, and can be applied to problems that are irregular, incomplete, noisy, dynamic, etc. [see 5, [14,47,2,46], 3, and references therein]. PSO iteratively tries to improve a candidate solution with regard to a given measure of quality or fitness function. Hence, furthermore, it can be considered an anytime algorithm.

Notice that treating time series motif discovery as an optimization problem naturally yields several advantages:

                           
                              1.
                              We do not require much memory, as we can basically store only the stream time series and preprocess the required segments at every fitness evaluation.

We are able to achieve a certain efficiency, as optimization algorithms do not usually explore the full solution space and perform few fitness evaluations [23].

We can employ any dissimilarity measure D as our fitness or objective function. Its only requirements are segment length independence and a minimal search space continuity. Intuitively, this holds for the high majority of time series dissimilarity measures that are currently used (Sections 2.1 and 2.2). Additionally, we can straightforwardly incorporate notions of ‘interestingness’, hubness, or complexity [see 50]. This flexibility is very uncommon in current time series motif discovery algorithms (Section 1).

We do not need to force the two segments of the motif to be of the same length. The dissimilarity function D can expressly handle segments of different lengths or we can simply upsample to the largest length [see 49]. Although considering different segment lengths has been highlighted as an objectively better approach, practically none of the current time series motif discovery algorithms contemplates this option (Section 1).

Since we search for the optimal wa
                                  and wb
                                 , together with a and b, we do not need to set the exact segment lengths as a parameter. Instead, we can use a more intuitive and easier to set range of lengths wa, wb
                                  ∈ [w
                                 min, w
                                 max].

We can easily modify our fitness criterion to work with different task settings. Thus, just by replacing D, we are able to work with multi-dimensional time series [22], detect sub-dimensional motifs [37], perform a constrained motif discovery task [38], etc.

We can incorporate notions of motif frequency to our fitness function and hence expand our similarity-based definition of motif to incorporate both notions [41]. For instance, instead of optimizing for individual motifs 
                                    
                                       
                                          m
                                          i
                                       
                                       ,
                                    
                                  we can optimize sets of motifs 
                                    
                                       M
                                       i
                                       ′
                                    
                                  of size ri
                                  such that 
                                    
                                       
                                          1
                                          
                                             r
                                             i
                                          
                                       
                                       
                                          ∑
                                          
                                             
                                                m
                                                j
                                             
                                             ∈
                                             
                                                M
                                                i
                                                ′
                                             
                                          
                                       
                                       D
                                       
                                          (
                                          
                                             m
                                             j
                                          
                                          )
                                       
                                    
                                  is minimal. We can choose ri
                                  to be a minimum frequency of motif appearance or we can even decide to optimize it following any suitable criterion.

In addition, using PSO has a number of interesting properties, some of which may be shared with other metaheurisics:

                           
                              1.
                              We have a straightforward mapping to the problem at hand (Section 3.1).

By construction, we have an anytime algorithm (Section 2.4).

We can obtain accurate and much faster solutions, as compared to the state-of-the-art in time series motif discovery (Section 5.3).

We have an essentially parameter-free algorithm [5]. As will be shown, all our parameter choices turn out to be non-critical to achieve the most competitive performances (Sections 5.1 and 5.2).

We have an easily parallelizable algorithm. The agent-based nature of PSO naturally yields to parallel implementations [2].

We still have the possibility to apply lower bounding techniques to D in order to reduce its computational cost [41,48]. Among others, we may exploit the particles’ best-so-far values or spatially close dissimilarities.

All of these use a simple, easy to implement algorithm requiring low storage capabilities (Section 3.2).

Our PSO approach to time series motif discovery is based on the combination of two well-known extensions to the canonical PSO [47]. On one hand, we employ multiple reinitializations of the swarm on stagnation [17]. On the other hand, we exploit the particles’ “local memories” with the intention of forming stable niches across different local minima [32]. The former emulates a parallel multi-swarm approach [3] without the need of having to define the number of swarms and their communication. The latter, when combined with the former, results in a low-complexity niching strategy [3] that does not require niching parameters [see the related discussion in 7, 8]. SWARMMOTIF, the implementation of the two extensions, is detailed in Algorithm 1
                        . A schematic block diagram is shown in Fig. 4
                        .

SWARMMOTIF takes a time series 
                           z
                         of length n as input, together with a segment dissimilarity measure D, which will conform our objective function, and the range of segment lengths of interest, limited by w
                        min and w
                        max. The user also needs to specify k, the desired number of motifs, and t
                        max, the maximum time spent by the algorithm (in iterations
                           6
                        
                        
                           6
                           The number of iterations is easy to infer from the available time as, for the same input, the elapsed time will be roughly directly proportional to the number of iterations.
                        ). SWARMMOTIF outputs a set of k non-overlapping motifs 
                           M
                        . We implement 
                           M
                         as a priority queue, which typically stores more than k elements to ensure that it contains k non-overlapping motifs. This way, by sorting the motif candidates as soon as they are found, we allow potential queries to 
                           M
                         at any time during the algorithm’s execution. In that case, we only need to dynamically check the candidates’ overlap (Section 3.2). Notice that n, D, w
                        min, w
                        max, k, and t
                        max are not parameters of the algorithm, but requirements of the task (they depend on the data, the problem, and the available time). The only parameters to be set, as specified in Algorithm 1’s requirements, are the number of particles κ, the topology θ, the constriction constant ϕ, and the maximum amount of iterations at stagnation τ. Nevertheless, we will show that practically none of the possible parameter choices introduces a significant variation in the reported performance (Section 5.1).

Having clarified SWARMMOTIF’s input, output, and requirements, we now elaborate on its procedures. Algorithm 1 starts by computing the velocity update constants (line 1) following Clerc’s constriction method [15], i.e.,

                           
                              
                                 
                                    
                                       c
                                       0
                                    
                                    =
                                    
                                       2
                                       
                                          |
                                          2
                                          −
                                          ϕ
                                          −
                                          
                                             
                                                
                                                   ϕ
                                                   2
                                                
                                                −
                                                4
                                                ϕ
                                             
                                          
                                          |
                                       
                                    
                                 
                              
                           
                        and

                           
                              (1)
                              
                                 
                                    
                                       c
                                       1
                                    
                                    =
                                    
                                       c
                                       2
                                    
                                    =
                                    
                                       c
                                       0
                                    
                                    ϕ
                                    /
                                    2
                                    .
                                 
                              
                           
                        Next, a swarm with κ particles is initialized (line 2). The swarm is formed by four data structures: a set of particle positions 
                           
                              X
                              =
                              {
                              
                                 x
                                 1
                              
                              ,
                              ⋯
                              
                                 x
                                 κ
                              
                              }
                              ,
                           
                         a set of particle velocities 
                           
                              V
                              =
                              {
                              
                                 v
                                 1
                              
                              ,
                              ⋯
                              
                                 v
                                 κ
                              
                              }
                              ,
                           
                         a set of particle best scores 
                           
                              S
                              =
                              {
                              
                                 s
                                 1
                              
                              ,
                              ⋯
                              
                                 s
                                 κ
                              
                              }
                              ,
                           
                         and a set of particle best positions 
                           
                              P
                              =
                              {
                              
                                 p
                                 1
                              
                              ,
                              ⋯
                              
                                 p
                                 κ
                              
                              }
                           
                         (the initialization of these four data structures is detailed in Algorithm 2
                        ). Particles’ positions 
                           
                              x
                              i
                           
                         and 
                           
                              p
                              i
                           
                         completely determine a motif candidate, and have a direct correspondence with 
                           
                              m
                              i
                           
                         (see Section 3.2). A further data structure Θ indicates the indices of the neighbors of each particle according to a given social topology θ (line 3). Apart from the swarm, we also initialize a global best score s
                        * (line 4) and the priority queue 
                           M
                         (line 5). We then enter the main loop (lines 6–26). In it, we perform three main actions. Firstly, we compute the particles’ fitness and perform the necessary updates (lines 7–16). Secondly, we modify the particles’ position and velocity using their personal and neighborhood best positions (lines 17–23). Thirdly, we control for stagnation and reinitialize the swarm if needed (lines 24–26). Finally, when we exit the loop, we return the first k non-overlapping motif candidates from 
                           M
                         (line 27).

The particles’ fitness loop (lines 7–16) can be described as follows. For the particles that have a valid position within the ranges used for particle initializations (line 8; see also Algorithm 2 for initializations), we calculate their fitness D (line 9) and, if needed, update their personal bests si
                         and 
                           
                              p
                              i
                           
                         (lines 10–12). As mentioned, D needs to be independent of the segments’ lengths, which is typically an easy condition for time series dissimilarity measures (Section 2.1). In the case that the particles find a new personal best, we save the motif dissimilarity d and its position 
                           
                              x
                              i
                           
                         into 
                           M
                         (line 13). Next, we update t
                        update, the last iteration when an improvement of the global best score s
                        * has occurred (lines 14–16).

The particles’ update loop (lines 17–23) is straightforward. We first select each particle’s best neighbor g using the neighborhood personal best scores sj
                         (lines 18–21). Then, we use the positions of the best neighbor’s personal best 
                           
                              p
                              g
                           
                         and the particle’s personal best 
                           
                              p
                              i
                           
                         to compute its new velocity and position (lines 22–23). We employ component-wise multiplication, denoted by ⊗, and two random vectors 
                           
                              u
                              1
                           
                         and 
                           
                              u
                              2
                           
                         whose individual components 
                           
                              
                                 u
                                 
                                    i
                                    ,
                                    j
                                 
                              
                              =
                              U
                              
                                 (
                                 0
                                 ,
                                 1
                                 )
                              
                              ,
                           
                         being U(l, h) a uniform real random number generator such that l ≤ U(l, h) < h. Note that by considering the particles’ neighborhood personal bests 
                           
                              p
                              g
                           
                         we follow the aforementioned local neighborhood niching strategy [32]. At the end of the loop we control for stagnation by counting the number of iterations since the last global best update and applying a threshold τ (line 24). Note that this is the mechanism responsible for the aforementioned multiple reinitialization strategy [17].

The initialization of the swarm used in Algorithm 1 (lines 2 and 25) is further detailed in Algorithm 2. In it, for each particle, two random positions 
                           
                              x
                              i
                           
                         and 
                           
                              x
                              ′
                           
                         are drawn (lines 2–6) and the initial velocity is computed as the subtraction of the two (line 7). To obtain 
                           
                              x
                              i
                           
                         and 
                           
                              
                                 x
                                 ′
                              
                              ,
                           
                         uniform real random numbers 
                           
                              u
                              =
                              U
                              (
                              0
                              ,
                              1
                              )
                           
                         are subsequently generated. The personal best score si
                         is set to infinite (line 8) and 
                           
                              x
                              i
                           
                         is taken as the current best position 
                           
                              p
                              i
                           
                         (line 9). Note that 
                           
                              u
                           
                         (line 4) is used to ensure a uniform distribution of the particles across the triangular subspace formed by x
                        
                           i, 1 and x
                        
                           i, 3 (line 5; see also Section 2.1).

@&#IMPLEMENTATION DETAILS@&#

Some implementation details are missing in Algorithms 1 and 2. We now summarize them and refer the interested reader to the provided code for a full account of them. Firstly, positions 
                           
                              x
                              i
                           
                         are floored component-wise inside ValidPosition, D, and 
                           M
                         (thus obtaining motif 
                           
                              m
                              i
                           
                        ). Secondly, the motif priority queue 
                           M
                         is implemented as an associative container (logarithmic insertion time) that sorts its elements according to d and stores 
                           
                              m
                              i
                           
                        . Once the capacity of the priority queue is exceeded, the worst element according to 
                           
                              D
                              (
                              
                                 m
                                 i
                              
                              )
                           
                         is popped out. Thirdly, the last visited positions are cached into a hash table (constant lookup time) in order to avoid some of the possible repeated dissimilarity computations. Fourthly, we incorporate the option to constrain the motif search by specifying a maximum segment stretch in Algorithm 2 and ValidPosition. Finally, the function that returns the non-overlapping top-k motifs employs a boolean array of size n in order to avoid O(k
                        2) comparisons between members of the queue [cf. 40]. Notice that we have a memory-efficient implementation, as we basically only need to store 
                           z
                         and the boolean array (both of O(n) space), 
                           M
                         (of O(k) space, k ≪ n), and 
                           
                              X
                              ,
                           
                        
                        
                           
                              V
                              ,
                           
                        
                        
                           
                              S
                              ,
                           
                        
                        
                           
                              P
                              ,
                           
                         and Θ (all of them of O(κ) space, κ ≪ n). The aforementioned hash table (optional) can be allocated in any predefined, available memory segment.
                     

Given the main Algorithm 1, we consider a number of variations that may potentially improve SWARMMOTIF’s performance without introducing too much algorithmic complexity:

                           
                              •
                              Sociability: We study whether a “cognitive-only” model, a “social-only” one, or different weightings of the two yield to some improvements [30]. To do so, we just need to introduce a parameter α ∈ [0, 1] controlling the degree of ‘sociability’ of the particles, and implement lines 1–2 of Algorithm 3 instead of Eq. (1).

Stochastic: We investigate the use of a random inertia weight [17]. This may alleviate the need of using the same c
                                 0 in different environments, providing a potentially more adaptive trade-off between exploration and exploitation (also controlled by α in the previous point). To consider this variant, we just need to replace line 22 in Algorithm 1 by line 3 in Algorithm 3.

Velocity clamping: In addition to constriction, we study limiting the maximum velocity of the particles [31]. Empirical studies have shown that the simultaneous consideration of a constriction factor and velocity clamping results in improved performance on certain problems [16]. To apply velocity clamping we add lines 4–6 of Algorithm 3 between lines 22 and 23 of Algorithm 1.

Craziness: We introduce so-called “craziness” or “perturbation” in the particles’ velocities, as initially suggested by Kennedy and Eberhart [31]. In such variant, inspired by the sudden direction changes observed in flocking birds, the particles’ velocity is altered with a certain probability ρ, with the aim of favoring exploration by increasing directional diversity and discouraging premature convergence [20]. We agree with [20] in that, in some sense, this can be seen as a mutation operation. To implement craziness we add lines 7–10 of Algorithm 3 between lines 22 and 23 of Algorithm 1.

To evaluate SWARMMOTIF’s speed and accuracy we consider plots like the one presented in Fig. 3. As a reference, we draw uniform random samples from the motif search space and compute their dissimilarities (we take as many samples as the length n of the time series). As a baseline, we use the top-25 motifs found by MOEN [40], which we will denote by 
                        
                           M
                           *
                        
                     . Existing empirical evidence suggests that MOEN is the most efficient algorithm to retrieve the top exact similarity-based motifs in a range of lengths
                        7
                     
                     
                        7
                        Besides, we could not find any other promising exact or anytime approach with some available code, nor with sufficient detail to allow a reliable implementation.
                      (Section 1). Notice furthermore that here we are not that interested in obtaining all top-25 true exact motifs, but more concerned on obtaining good seed motifs within these using an anytime approach (Section 2.3).

As its competitors, MOEN has however some limitations (Section 1). Thus, to fairly compare results, we have to apply some constrains to our algorithm. Since MOEN can only use the Euclidean distance between z-normalized segments, here we also adopt this formulation for D. In addition, as MOEN only considers pairs of segments of the same length (without resampling), we have to constrain SWARMMOTIF so that 
                        
                           
                              x
                              
                                 i
                                 ,
                                 2
                              
                           
                           =
                           
                              x
                              
                                 i
                                 ,
                                 4
                              
                           
                        
                     . Therefore, the reported motif dissimilarities 
                        
                           D
                           (
                           
                              m
                              i
                           
                           )
                        
                      correspond to the Euclidean distance between two z-normalized segments of the same length (we divide by the length of the segments to compare different segment lengths, Section 2.1). In the reported experiments, SWARMMOTIF is run 10 times with 
                        
                           k
                           =
                           10
                        
                     . We stop its execution when we find 95% of 
                        
                           D
                           (
                           
                              m
                              i
                           
                           )
                        
                      within 
                        
                           M
                           *
                        
                     . This way, we assess the time taken to retrieve any 10 motifs from those with at least 95% confidence. All experiments are performed using a single core of an Intel® Xeon® CPU E5-2620 at 2.00 GHz.

To demonstrate that SWARMMOTIF is not biased towards a particular data source, time series length, or motif length, we consider 9 different time series of varying length, coming from distinct domains, and a number of arbitrary but source-consistent motif lengths (Table 1
                     ). As mentioned, we make these time series and our code available online (Section 1). Four of the time series have been used for motif discovery in previous studies [43,44], while the other five are employed here for the first time for this task:

                        
                           1.
                           
                              DowJones: The daily closing values of the Dow Jones average in the USA from May 2, 1885 to April 22, 2014 [54].


                              CarCount: The number of cars measured for the Glendale on ramp for the 101 North freeway in Los Angeles, CA, USA [26]. The measurement was carried out by the Freeway Performance Measurement System
                                 8
                              
                              
                                 8
                                 
                                    http://pems.dot.ca.gov.
                               and the data was retrieved from the UCI Machine Learning Repository [1]. Segments of missing values were manually interpolated or removed.


                              Insect: The electrical penetration graph of a beet leafhopper (circulifer tenellus) [44]. The time series was retrieved from Mueen’s website
                                 9
                              
                              
                                 9
                                 
                                    http://www.cs.ucr.edu/~mueen/MK.
                              .


                              EEG: A one hour electroencephalogram (in μV) from a single channel in a sleeping patient [44]. The time series was retrieved from Mueen’s website
                                 10
                              
                              
                                 10
                                 
                                    http://www.cs.ucr.edu/~mueen/OnlineMotif
                                 
                               and, according to the authors, was smoothed and filtered using domain-standard procedures.


                              FieldRecording: The spectral centroid (in Hz) of a field recording retrieved from Freesound
                                 11
                              
                              
                                 11
                                 
                                    http://www.freesound.org/people/JeffWojo/sounds/121250
                                 
                               
                              [19]. We used the mean of the stereo channels and the spectral centroid (linear frequency) Vamp SDK example plugin from Sonic Visualizer [10]. We used a Hann window of 8192 samples at 44.1 KHz with 75% overlap.


                              Wind: The wind speed (in m/s) registered in the buoy of Rincon del San Jose, TX, USA, between January 1, 2010 and April 11, 2014. The time series was retrieved from the Texas Coastal Ocean Observation Network website
                                 12
                              
                              
                                 12
                                 
                                    http://lighthouse.tamucc.edu/pq
                                 
                              . Segments of missing values were manually interpolated or removed.


                              Power: The electric power consumption (in KW) of an individual household
                                 13
                              
                              
                                 13
                                 
                                    http://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption
                                 
                              . The data was retrieved from the UCI Machine Learning Repository [1]. We took the global active power, removed missing values, and downsampled the original time series by a factor of 5 using averaging and 50% overlap.


                              EOG: An electrooculogram tracking the eye movements of a sleeping patient [21]. We took the downsampled time series from Mueen’s web page
                                 14
                              
                              
                                 14
                                 
                                    http://www.cs.ucr.edu/~mueen/DAME
                                 
                               
                              [43].


                              RandomWalk: A random walk time series. This was artificially synthesized using 
                                 
                                    
                                       z
                                       
                                          i
                                          +
                                          1
                                       
                                    
                                    =
                                    
                                       z
                                       i
                                    
                                    +
                                    N
                                    
                                       (
                                       0
                                       ,
                                       1
                                       )
                                    
                                 
                               for 
                                 
                                    i
                                    =
                                    2
                                    ,
                                    ⋯
                                    n
                                 
                               and 
                                 
                                    
                                       z
                                       1
                                    
                                    =
                                    0
                                    ,
                                 
                               where N(0, 1) is a real Gaussian random number generator with zero mean and unit variance.

To assess the statistical significance of the differences between alternative parameter settings, we employ a two stage approach. First, we consider all settings at the same time and perform the Friedman’s test [24], which is a non-parametric statistical test used to detect differences in treatments across multiple test attempts. We use as inputs the median values for all settings for 25 equally-spaced time steps. In the case some difference between settings is detected (i.e., we reject the null hypothesis that the settings’ performances come from the same distribution), we proceed to the second stage. In it, we perform all possible pairwise comparisons between settings using the Wilcoxon signed-rank test [24], another non-parametric statistical hypothesis test used for comparing matched samples. To counteract the problem of multiple comparisons and control the so-called family-wise error rate, we employ the Holm-Bonferroni correction [25]. In all statistical tests, we consider a significance level of 0.01.

@&#RESULTS@&#

In pre-analysis, and according to common practice, we set 
                           
                              κ
                              =
                              100
                              ,
                           
                        
                        
                           
                              ϕ
                              =
                              4.1
                              ,
                           
                         and 
                           
                              τ
                              =
                              2000
                           
                        . We then experimented with 6 different static topologies θ 
                        [36]: global best, local best (two neighbors), Von Neumann, random (three neighbors), wheel, and binary tree. The results showed the qualitative equivalence of all topologies except, perhaps, global best and wheel (Fig. 5
                        ). In some data sets, these two turned out to yield slightly worse performances for short-time runs of the algorithm (small t), although for longer runs they gradually became equivalent to the rest. However, in general, no systematic statistically significant difference was detected between topologies. With this in mind, we chose the local best topology to further favor exploration and parallelism, and to be more consistent with our local neighborhood design principle of Section 3.1.

Next, we studied the effect of the number of particles κ and the stagnation threshold τ. To do so, we kept the previous configuration with the local best topology and subsequently evaluated 
                           
                              κ
                              =
                              {
                              20
                              ,
                              40
                              ,
                              80
                              ,
                              160
                              ,
                              320
                              }
                           
                         and 
                           
                              τ
                              =
                              {
                              500
                              ,
                              1000
                              ,
                              2000
                              ,
                              4000
                              ,
                              8000
                              }
                           
                        . Essentially, we observed almost no performance changes under these alternative settings (Figs. 6
                         and 7
                        ). We only found a statistically significant difference in the case of the CarCount data set. Specifically, the performance with 
                           
                              τ
                              =
                              500
                           
                         was found to be statistically significantly worse than τ ≥ 2000. Regarding κ, and after considering different n, 
                           
                              w
                              Δ
                           
                         and k, a partial tendency seemed to emerge: an increasing number of particles κ was slightly beneficial for increasing lengths n, increasing 
                           
                              
                                 w
                                 Δ
                              
                              ,
                           
                         and increasing k. Unfortunately, we could not obtain strong empirical evidence nor formal proof for this statement. Nonetheless, in subsequent experiments, we decided to use a value for κ and τ that dynamically adapts SWARMMOTIF’s configuration to such predefined task parameters. We arbitrarily set τ proportional to κ, and κ proportional to n and in direct relation to 
                           
                              w
                              Δ
                           
                         and k (we refer to the provided code for the exact formulation).

To conclude our pre-analysis, we studied the influence of the constriction constant ϕ. Following common practice, we considered 
                           
                              ϕ
                              =
                              {
                              4.02
                              ,
                              4.05
                              ,
                              4.1
                              ,
                           
                         4.2, 4.4, 4.8}. In this case, we saw that high values had a negative impact on performance (Fig. 8
                        ). In particular, values of ϕ ≥ 4.2 or ϕ ≥ 4.4, depending on the data set, statistically significantly increased the motif dissimilarities at a given t. Contrastingly, values 4 < ϕ < 4.2 yielded stable dissimilarities with no statistically significant variation (in some data sets, this range could be extended to 4 < ϕ ≤ 4.4). It is well-known that higher ϕ values favor exploitation rather than exploration [15]. Hence, it is not strange to observe that low ϕ values are more appropriate for searching the large motif spaces we consider here. We finally chose 
                           
                              ϕ
                              =
                              4.05
                           
                        .

Overall, the result of our pre-analysis suggests a high degree of robustness with respect to the possible configurations. The topology θ, the number of particles κ, the stagnation threshold τ, and the constriction constant ϕ have, in general, no significant influence on the obtained results. The only consistent exception is observed for values of ϕ ≥ 4.4, which are not the most common practice [46]. The global best and wheel topologies could also constitute a further exception. However, as we have shown, these become qualitatively equivalent to the rest as execution time t increases, yielding no statistically significant difference. We believe that the reported stability of SWARMMOTIF against the tested configurations and data sets justifies the use of our setting for finding motifs in diverse time series coming from further application domains.

Using the configuration resulting from the previous section, we subsequently assessed the performance of the variations considered in Section 3.3. We started with the sociability variant, experimenting with social-only models, 
                           
                              α
                              =
                              1
                              ,
                           
                         cognitive-only models, 
                           
                              α
                              =
                              0
                              ,
                           
                         and intermediate configurations, 
                           
                              α
                              =
                              {
                              0.2
                              ,
                              0.33
                              ,
                              0.66
                              ,
                              0.8
                              }
                           
                        . Apart from the fact that no clear tendency could be observed, none of the previous settings was able to consistently reach the performance achieved by the original variant (
                           
                              α
                              =
                              1
                              /
                              2
                              ,
                           
                         
                        Eq. (1)) in all time series. That is, none of the previous settings could statistically significantly outperform 
                           
                              α
                              =
                              1
                              /
                              2
                           
                         in the majority of the data sets.

Next, we experimented with the stochastic and the velocity clamping variants. While the former did not improve our results, the latter led to a statistically significant improvement for some time series. Because of that, we decided to discard the use of a stochastic variant but to incorporate velocity clamping to our main algorithm. The former could be difficult to justify while the latter has empirical evidence behind it (Section 3.3).

Finally, we experimented with craziness and its probability ρ. The results showed a similar performance for 0 ≤ ρ < 0.001, a slightly better performance for 0.001 ≤ ρ ≤ 0.01, and an increasingly worse performance for ρ > 0.01 (Fig. 9
                        ). A statistically significant difference was found between ρ ≤ 0.01 and ρ > 0.1, being ρ > 0.1 a consistently worse setting. These results were expected, as the swarm performs a more random search with increasing ρ, being completely random in the limiting case of 
                           
                              ρ
                              =
                              1
                           
                        . The slightly better performance for 0.001 ≤ ρ ≤ 0.01 was not found to be statistically significant under our criteria. However, it was visually noticeable for some data sets. For instance, with the EEG data set, we see that curves 33 and 34 hit the dissimilarities of the true exact motif set 
                           
                              M
                              *
                           
                         (gray area) two or three times earlier than curves 30, 31, and 32 (Fig. 9). With these results, and seeing that ρ values between 0.001 and 0.01 never harmed the performance of the algorithm, we chose 
                           
                              ρ
                              =
                              0.002
                           
                        .

After extending SWARMMOTIF with velocity clamping and craziness, we assess its performance on all considered time series using the default parameter combination resulting from the previous two sections. As can be seen, the obtained motif dissimilarities are far from the random sampling in all cases (Fig. 10
                        ; notice the logarithmic axes). In addition, we see that SWARMMOTIF is able to already obtain dissimilarities within 
                           
                              M
                              *
                           
                         as soon as its execution begins. Specifically, motif dissimilarities in 
                           M
                         start to overlap the ones in 
                           
                              M
                              *
                           
                         at t < 10 s for practically all time series. The only exceptions are EOG and RandomWalk, where 
                           M
                         starts to overlap with 
                           
                              M
                              *
                           
                         at t < 100 s. We hypothesize that taking a smaller number of particles κ could make 
                           M
                         overlap with 
                           
                              M
                              *
                           
                         earlier, but leave the formal assessment of this hypothesis for future work.

Finally, as execution time t progresses, we see that SWARMMOTIF consistently retrieves lower dissimilarities, up to the point that 
                           
                              M
                              ≃
                              
                                 M
                                 *
                              
                           
                         (Fig. 10 and Table 2
                        ). Following the condition we specify in Section 4, this means that the distances in the motif set obtained by SWARMMOTIF are not statistically worse than the ones of the true exact motif set. With respect to MOEN’s execution time, this happens 1487 (DowJones), 184 (CarCount), 50 (Insect), 179 (EEG), 100 (FieldRecording), 241 (Wind), 286 (Power), 74 (EOG), and 287 (RandomWalk) times faster. This implies between one and three orders of magnitude speedups (more than that for DowJones). Overall, we believe this is an extremely competitive performance for an anytime motif discovery algorithm.

@&#CONCLUSION@&#

In this article, we propose an innovative standpoint to the task of time series motif discovery by formulating it as an anytime multimodal optimization problem. After a concise but comprehensive literature review, we reason out the new formulation and the development of an approach based on evolutionary computation. We then highlight the several advantages of this new formulation, many of which relate to a high degree of flexibility of the solutions that come from it. To the best of our knowledge, such a degree of flexibility is unseen in previous works on time series motif discovery.

We next present SWARMMOTIF, an anytime multimodal optimization algorithm for time series motif discovery based on particle swarms. We show that SWARMMOTIF is extremely competitive when compared to the best approach we could find in the literature. It obtains motifs of comparable similarities, in considerably less time, and with minimum memory requirements. This is confirmed with 9 independent real-world time series of increasing length coming from a variety of domains, including economics, car traffic, entomology, medical data, audio, climate, and power consumption. Besides, we find that the high majority of the possible implementation choices lead to non-significant performance changes in all considered time series. Thus, given this robustness, we can think about the proposed solution as being parameter-free from the user’s perspective. Overall, if we add the aforementioned, unprecedented degree of flexibility, SWARMMOTIF stands out as one of the most prominent choices for motif discovery in long time series streams. Since the used data and code are available online (Section 1), the research presented here is reproducible, and SWARMMOTIF is freely available to researchers and practitioners.

We believe that the consideration of multimodal optimization algorithms is a relevant direction for future research in the field of time series analysis and mining. Not only with regard to motif discovery, but also in other tasks such as querying for segments of unknown length or determining optimal alignments and similarities. Finally, we believe that considering the search spaces and the time constraints derived from time series problems can be a challenge for the evolutionary computation community. We look forward to exploring all these topics in forthcoming works, together with other multimodal optimization techniques that could be easily mapped to the problem of time series motif discovery.

@&#ACKNOWLEDGMENT@&#

We would like to thank all the people who contributed the data sets used in this study and Abdullah Mueen for additionally sharing his code. We would also like to thank Xavier Anguera for useful discussions that motivated the present work. This research has been funded by 2014-SGR-118 from Generalitat de Catalunya, JAEDOC069/2010 from Consejo Superior de Investigaciones Científicas (JS), TIN2012-38450-C03-03 from the Spanish Government, and E.U. Social and FEDER funds (JS).

@&#REFERENCES@&#

