@&#MAIN-TITLE@&#Clustering boundary detection for high dimensional space based on space inversion and Hopkins statistics

@&#HIGHLIGHTS@&#


               Highlight
               
                  
                     
                        
                           
                           Propose a high dimensional space inversion technique to extract the local space features.


                        
                        
                           
                           Propose a Symmetry Statistics to describe the uniformity of high dimensional data space.


                        
                        
                           
                           Propose a clustering boundary detection algorithm for high dimensional data space named Spinver.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Clustering boundary

High dimensional space

Space inversion

Symmetry Statistics

@&#ABSTRACT@&#


               
               
                  Physicists research the symmetry of particle space through the contrast of motion law in the real space and inversion space which is created by space inversion techniques. Inspired by this theory, we propose the idea of using local space transformation and dynamic relative position to detect the clustering boundary in high dimensional space. Due to the curse of dimensionality, global space transformation approaches are not only time-consuming, but also fail to keep the original distribution characteristics. So, we inverse the space positions of the k nearest neighbors and project them on the high dimensional space coordinate system. To address the lack of statistics that can describe the uniformity of high dimensional space, we propose the Symmetry Statistics based on the Hopkins Statistics. It is employed to judge the uniformity of k nearest neighbor space of coordinate origin. Moreover, we introduce a filter function to remove some special noises and isolated points. Finally, we use boundary and filter ratios to detect the clustering boundary and propose the corresponding detection algorithm, called Spinver. Experimental results from synthetic and real data sets demonstrate the effectiveness of this algorithm.
               
            

@&#INTRODUCTION@&#

Extracting the valuable patterns [1–3] in complex information space is the key problem of data mining. The patterns will help people to understand data space, and get more valuable information. So, there is no doubt that the extraction process can be described as segmenting the information space, and different segmentation methods will lead to different pattern results. The clustering techniques [4–8], which use none experience way [9], classify the unlabeled data objects by grouping objects that are similar to each other. By analyzing the similarities and differences between different classes, it can get the stable structure of data space, i.e. clusters. In addition to this familiar pattern, we find a more interesting pattern-clustering boundary.

Clustering boundary [10], which is located at the edge of a cluster is a special pattern. Data objects within the cluster have the same class label, but there exists some differences between the interior objects and the boundary objects. Boundary objects, in many applications, may indicate special targets that need to pay close attention on, for example, people who have infected with some virus but not be attacked, side face images in the front face images, irregular handwriting signatures, the gene mutation individuals in the gene expression datasets, and etc. When scholars are delighted to find such interesting researches, they also find the traditional data mining technology cannot extract the clustering boundary. In recent years, some clustering boundary detection techniques have been proposed, such as BORDER [11,12], BRIM [13], BAND [14], BRINK [15], and etc. However, the research on clustering boundary is not so extensive as that clustering technology, especially for the high dimensional data space [16–18]. So, this study will be aimed at the clustering boundary pattern discovery in high dimensional space.

The remainder of this paper is organized as follows. Section 2 introduces the related work. Section 3 reports the clustering boundary model. Section 4 presents experiments and performance results on a number of synthetic and real data sets. Section 5 provides an intuitive discussion on parameters analysis and scalability. Our conclusion is given in Section 6.

@&#RELATED WORK@&#

Compared with low dimensional space, high dimensional space has more complex space structure [19,20]. Because of the inherent sparsity of the data objects, the most existing clustering algorithms that based on only the similarity measures between data objects will become substantially inefficient. To solve the problem, PCA technique [21] chooses the main dimensions to represent the whole space. Then, a series of similar techniques which focus on the choice of a reasonable subspace have been proposed, including Linear Discriminant Analysis (LDA) [22], Isomap [23], Locally Linear Embedding (LLE) [24], Laplacian Eigenmaps [25], Local Preserving Projection (LPP) [26], Local Tangent Space Alignment (LTSA) [27,28], Maximum Variance Unfolding (MVU) [29], and etc. From the perspective of information theory [30], these techniques can be described as a process of information compression. However, the information compression will lose some important information, and the subspace cannot reflect the whole space structure. So, the data analysis results depend on the selection of subspace, and different subspace will lead to different patterns. To keep the complete space information, spectral clustering technique [31] transforms the whole space to a new space to finish the clustering task. Besides, we find that there exist many techniques using the method of space transformation, such as the clustering techniques using SVM [32,33] and artificial neural network [34–36], etc. SVM transforms the data space from low dimensional to high dimensional to deal with the problem of linearly inseparable. In other words, this method will analyze the data objects in a high dimensional space, but it may be caught in ‘Dimension Disaster’. Artificial neural network transforms the data space to a similar brain system or a map structure system. Although this method will solve many problems in complex structure space, but it may make a simple problem more complex in the high dimensional space with a small number of samples, and make a complex problem even more complex in the high dimensional space with a large number of samples.

To tackle the problems above, we hold a long-term research. We find that the space inversion [37,38] of the particle space physics and uniform distribution provide the theoretical basis for the research of high dimensional space. Space inversion is a method used to study the symmetry of particle space in the microscopic world. It reverses the spatial feature, such as the direction of forces, the direction of time, and etc. In other words, physicists take the inverse values to replace the position of each particle to establish an image space which has the similar structure in the original space. Compared with the motion law of particles in the image space and original space, scientists can judge the symmetry of particle space. Therefore, scientists propose the vector inversion about time, the geometric inversion about mathematics, the quantitative inversion about geography, and etc.

Inspired by this idea, we establish a high dimensional coordinate system for the current data points, and use the relative position to finish the detection task of clustering boundary. Unlike the traditional space transformation methods which transform the whole data space to a new space, we transform the local space to a new space, and use the dynamic relative position to replace the static position. Particularly, The specify way is that we give each data object a different positions in different local spaces, so that the relative position changes dynamically.

Another theory used in this paper is the uniform distribution. It describes the uniformity of data space based on probability statistics, and has widely used in the fields of computer science and physics, and etc. For example, researchers of data mining use the Hopkins statistics to describe the uniformity of clusters or evaluate the clustering quality of clustering analysis results. However, this statistics cannot describe the uniformity in the high dimensional data space. Generally, researchers use the dimension projection technique to analyze the space distribution with respect of each dimension. In other words, it projects data objects on certain dimensions to get the distribution of the dimensions. Compared with the dimension reduction methods, dimension oriented technique keep the whole space information, and has the characteristics of rapid calculation. After all, it costs a relatively small amount of time in one dimensional space or one dimensional array.

Though many algorithms about clustering in high dimensional space have been proposed, there are few papers which focus on clustering boundary in high dimensional space. So, these observations motive our effort to propose a clustering boundary detection algorithm based on space inversion and Hopkins statistics, called Spinver. The main contributions of this paper are as follows:

                        
                           (1)
                           propose a high dimensional space inversion technique to extract the local space features;

propose a Symmetry Statistics to describe the uniformity of high dimensional data space;

propose a clustering boundary detection algorithm for high dimensional data space named Spinver.

In this section, we will provide the idea of space inversion and projection technique. Then, we propose the Symmetry Statistic which could describe the uniformity of high dimensional space based on Hopkins Statistics. Lastly, we develop the Spinver algorithm.

Spherical [39] and cube sampling [40,41] are popular sampling methods used to analyze data. They all pay attention on fixed sampling window, and less attention on the data points located outside the window. So, they all belong to static sampling. More importantly, they will prejudice to the data points located at the surface of space, and cannot present the true feature of data distribution. So, in this paper we use the k nearest neighbor [42,43] as sampling method.

Given a n dimensions space S, we take xi
                         as the center of data space to establish a n dimensions coordinates system, where 
                           
                              
                                 x
                                 i
                              
                              =
                              
                                 (
                                 
                                    x
                                    
                                       i
                                       1
                                       ,
                                    
                                 
                                 
                                    x
                                    
                                       i
                                       2
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    x
                                    
                                       i
                                       n
                                    
                                 
                                 )
                              
                           
                        .Then we calculate the k nearest neighbors of xi
                         and reverse their positions to get the relative positions. The rule of space inversion is described as follows:

                           
                              (1)
                              
                                 
                                    
                                       
                                          
                                             R
                                             L
                                             o
                                             c
                                             a
                                             t
                                             i
                                             o
                                             n
                                             (
                                             
                                                x
                                                j
                                             
                                             )
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                x
                                                j
                                             
                                             −
                                             
                                                x
                                                i
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                          =
                                       
                                       
                                          
                                             (
                                             
                                                x
                                                
                                                   j
                                                   1
                                                
                                             
                                             −
                                             
                                                x
                                                
                                                   i
                                                   1
                                                
                                             
                                             ,
                                             
                                                x
                                                
                                                   j
                                                   2
                                                
                                             
                                             −
                                             
                                                x
                                                
                                                   i
                                                   2
                                                
                                             
                                             ,
                                             
                                                x
                                                
                                                   j
                                                   3
                                                
                                             
                                             −
                                             
                                                x
                                                
                                                   i
                                                   3
                                                
                                             
                                             ,
                                             .
                                             .
                                             .
                                             ,
                                             
                                                x
                                                
                                                   j
                                                   n
                                                
                                             
                                             −
                                             
                                                x
                                                
                                                   i
                                                   n
                                                
                                             
                                             )
                                          
                                       
                                    
                                 
                              
                           
                        where xj
                         is the k nearest neighbors of xi
                         and 
                           
                              
                                 x
                                 j
                              
                              =
                              
                                 (
                                 
                                    x
                                    
                                       j
                                       1
                                       ,
                                    
                                 
                                 
                                    x
                                    
                                       j
                                       2
                                    
                                 
                                 ,
                                 .
                                 .
                                 .
                                 ,
                                 
                                    x
                                    
                                       j
                                       n
                                    
                                 
                                 )
                              
                           
                        . We take xi
                         as the original point of local space, and new coordinates are assigned to the k nearest neighbors.

Clustering boundary objects are located at the edge of clusters, and it is k nearest neighbors are not uniformly distributed. Core points are located inside clusters, and their k nearest neighbors sit around them evenly. However, different with boundary and core points, noises always have large distances between their neighbors and themselves, so noises are distributed not uniformly. Based on the analysis above, we can detect the clustering boundary by judging the local uniformity of every data point. To describe the uniformity of k nearest space, we project the data objects on each dimension. The rule of projection is described as follows:

                           
                              (2)
                              
                                 
                                    
                                       
                                          
                                             P
                                             L
                                             o
                                             c
                                             a
                                             t
                                             i
                                             o
                                             n
                                             (
                                             
                                                x
                                                j
                                             
                                             )
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                [
                                                
                                                   
                                                      
                                                         
                                                            
                                                               x
                                                               
                                                                  j
                                                                  1
                                                               
                                                            
                                                            −
                                                            
                                                               x
                                                               
                                                                  i
                                                                  1
                                                               
                                                            
                                                         
                                                      
                                                      
                                                         0
                                                      
                                                      
                                                         0
                                                      
                                                      
                                                         
                                                            .
                                                            .
                                                            .
                                                         
                                                      
                                                      
                                                         0
                                                      
                                                   
                                                   
                                                      
                                                         0
                                                      
                                                      
                                                         
                                                            
                                                               x
                                                               
                                                                  j
                                                                  2
                                                               
                                                            
                                                            −
                                                            
                                                               x
                                                               
                                                                  i
                                                                  2
                                                               
                                                            
                                                         
                                                      
                                                      
                                                         0
                                                      
                                                      
                                                         
                                                            .
                                                            .
                                                            .
                                                         
                                                      
                                                      
                                                         0
                                                      
                                                   
                                                   
                                                      
                                                         0
                                                      
                                                      
                                                         0
                                                      
                                                      
                                                         
                                                            
                                                               x
                                                               
                                                                  j
                                                                  3
                                                               
                                                            
                                                            −
                                                            
                                                               x
                                                               
                                                                  i
                                                                  3
                                                               
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            .
                                                            .
                                                            .
                                                         
                                                      
                                                      
                                                         0
                                                      
                                                   
                                                   
                                                      
                                                         :
                                                      
                                                      
                                                         
                                                            .
                                                            .
                                                            .
                                                         
                                                      
                                                      
                                                         
                                                            .
                                                            .
                                                            .
                                                         
                                                      
                                                      
                                                         
                                                            .
                                                            .
                                                            .
                                                         
                                                      
                                                      
                                                         :
                                                      
                                                   
                                                   
                                                      
                                                         0
                                                      
                                                      
                                                         0
                                                      
                                                      
                                                         0
                                                      
                                                      
                                                         
                                                            .
                                                            .
                                                            .
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               x
                                                               
                                                                  j
                                                                  n
                                                               
                                                            
                                                            −
                                                            
                                                               x
                                                               
                                                                  i
                                                                  n
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                                ]
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                       
                                          
                                             ×
                                             
                                             (
                                             1
                                             ≤
                                             i
                                             ≤
                                             n
                                             )
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (3)
                              
                                 
                                    P
                                    L
                                    o
                                    c
                                    a
                                    t
                                    i
                                    o
                                    n
                                    
                                       (
                                       
                                          x
                                          j
                                       
                                       ,
                                       
                                          d
                                          i
                                       
                                       )
                                    
                                    =
                                    
                                       (
                                       0
                                       ,
                                       0
                                       ,
                                       .
                                       .
                                       .
                                       ,
                                       
                                          x
                                          
                                             j
                                             
                                                d
                                                i
                                             
                                          
                                       
                                       −
                                       
                                          x
                                          
                                             i
                                             
                                                d
                                                i
                                             
                                          
                                       
                                       ,
                                       .
                                       .
                                       .
                                       ,
                                       0
                                       )
                                    
                                    
                                       (
                                       1
                                       ≤
                                       i
                                       ≤
                                       n
                                       )
                                    
                                 
                              
                           
                        where PLocation(xj, di
                        )is the projection coordinate of xi
                         on the di
                         dimension. In this formula, we use the way of dimension oriented to extract the features of local space. So, the high dimensional space is divided into n one-dimensional spaces. This means that the high dimensional space is be decomposed.

To clearly describe the space inversion technique, we give a group of figures to show this technique on two-dimensional space (see Fig. 1
                        ). Fig. 1(a) shows two dimensions data space, and the blue points are the k nearest neighbors of the red point. Then, we extract the local space from the original space and show it in the Fig. 1(b). In this space, we give new positions to these points based on formula (2). It is important to note that theses points may be the k nearest neighbors of other points, so they will appear in some other inversion spaces and given different positions. After that, we project these points on each dimension, and form the projection space.

Hopkins statistics is proposed to describe the uniformity of two dimensions space, and its formula is as follows:

                           
                              (4)
                              
                                 
                                    H
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             n
                                          
                                          
                                             X
                                             i
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             n
                                          
                                          
                                             X
                                             i
                                          
                                          +
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             n
                                          
                                          
                                             Y
                                             i
                                          
                                       
                                    
                                 
                              
                           
                        where Xi
                         is the position of X-axis of data points and Yi
                         is position of the Y-axis of data points. From the formula, we can find that when the value of Hopkins statistics is 0.5, the data space is absolutely uniform. So, the more Hopkins statistics close to 0.5, the more uniform of the data space it is.

The statistics only pay attention on the first quadrant, and cannot describe completely the distribution of data space. For example, if the data points distribute evenly in the second quadrant of a second dimensions space, 
                           
                              
                                 ∑
                                 
                                    i
                                    =
                                    1
                                 
                                 n
                              
                              
                                 X
                                 i
                              
                              +
                              
                                 ∑
                                 
                                    i
                                    =
                                    1
                                 
                                 n
                              
                              
                                 Y
                                 i
                              
                           
                         will become zero, and the Hopkins statistics will be of no effect. More importantly, it can only describe the two dimensions space, and cannot be used in any higher dimensional data space. Considering the drawbacks of Hopkins statistics, we propose the Symmetry Statistics which can describe the uniformity of high dimensional data space and the formula is as follows:

                           
                              (5)
                              
                                 
                                    S
                                    =
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       n
                                    
                                    
                                    a
                                    b
                                    s
                                    
                                       (
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          m
                                       
                                       
                                          X
                                          
                                             i
                                             j
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        where abs is the operation of calculating the absolute value, m is the sample number of the n dimensional space.

This statistic considers all quadrants of the coordinate system, and can effectively describe the uniformity of high dimensional space. If the space is symmetrical about the origin point, each one -dimensional space will be symmetrical about the origin point too, and the value of Symmetry Statistics is more close to 0. If a space not satisfies the condition, it is not absolutely uniform. Based on this observation, we propose the formula (5) in a way of dimensions oriented. The smaller the value of Symmetry Statistics it is, the more uniform of the high dimensional data space will be. Therefore, we introduce formula (1) to this formula, and the Symmetry Statistics of projection space is as follows:

                           
                              (6)
                              
                                 
                                    S
                                    y
                                    m
                                    
                                       (
                                       
                                          x
                                          i
                                       
                                       )
                                    
                                    =
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       n
                                    
                                    
                                       a
                                       b
                                       s
                                       
                                          (
                                          
                                             ∑
                                             
                                                j
                                                ∈
                                                
                                                   C
                                                   i
                                                
                                             
                                          
                                          P
                                          L
                                          o
                                          c
                                          a
                                          t
                                          i
                                          o
                                          n
                                          
                                             (
                                             
                                                x
                                                j
                                             
                                             ,
                                             
                                                d
                                                i
                                             
                                             )
                                          
                                          )
                                       
                                    
                                 
                              
                           
                        where xj
                         is the k nearest neighbor of xi
                        , and Ci
                         is the k nearest neighbors space of xi
                         .In the calculation process of this formula, we first calculate the Symmetry Statistics on each dimension. Then we calculate the sum of these values as the uniformity coefficient of the projection space.

Clustering boundary objects are located on the edge of clusters. So the distribution of k nearest space is not uniform and the value of Symmetry Statistics is large. However, the distribution of core points are uniform and the value of Symmetry Statistics is small. The distributions of different types of data points in a four dimensional space are shown in Fig. 2
                        . From the figures, we can find the distribution of core point is uniform and its k nearest neighbors are close to it. In addition, the k nearest neighbor space of boundary point is not uniform and the neighbors are located in one side of space. However, the distributions of some special noises and isolated points are sparse, and they are uniform too. So these noises have similar values of Symmetry Statistics with core points.

Though these special noises and isolated points may have relatively small value of Symmetry Statistics, they exhibit an especially obvious characteristic, i.e. very sparse density. Compared with core and boundary points, the sum of squared of Euclidian distances between the noise and its k nearest neighbors are relative big. We focus on this characteristic and construct the Filter function to smooth noises.

To reduce the impacts of noises and isolated points in extracting clustering boundary, we use a filter function to smooth noises, and its definition is described as follows:

                           
                              (7)
                              
                                 
                                    F
                                    i
                                    l
                                    t
                                    e
                                    r
                                    
                                       (
                                       
                                          x
                                          i
                                       
                                       )
                                    
                                    =
                                    
                                       ∑
                                       
                                          j
                                          ∈
                                          
                                             C
                                             i
                                          
                                       
                                    
                                    
                                       exp
                                       (
                                       d
                                       i
                                       s
                                       
                                          t
                                          2
                                       
                                       
                                          (
                                          
                                             x
                                             j
                                          
                                          −
                                          
                                             x
                                             i
                                          
                                          )
                                       
                                       /
                                       k
                                       )
                                    
                                 
                              
                           
                        where Ci
                         is the k nearest neighbors space of xi
                        , 
                           
                              d
                              i
                              s
                              
                                 t
                                 2
                              
                              
                                 (
                                 
                                    x
                                    j
                                 
                                 −
                                 
                                    x
                                    i
                                 
                                 )
                              
                           
                        is the square of Euclidian distance.

Next, we show the specific description of Spinver algorithm. It is composed of three steps: step 1 calculates the Symmetry Statistics (i.e. Sym) and Filter for each point according formulas (6) and (7); Step 2 calculates the boundary threshold value ε1
                         and noise threshold value ε2
                        ; Step 3 detects the clustering boundary according the two values above.

In this algorithm, num is the number of samples, br controls the number of boundary objects. When the value of Sym of the data point is bigger than ε1
                        , it may be clustering boundary. fr controls the number of noises. When the value of Filter of the data point is bigger than ε2
                        , it is a noise.
                     

@&#EXPERIMENTS@&#

In this section, we carry out two groups of experiments:

                           
                              (1)
                              the comparison of boundary detection ability of different algorithms on some synthetic and real data sets;

the validation of boundary detection ability on Mnist
                                    1
                                 
                                 
                                    1
                                    
                                       http://yann.lecun.com/exdb/mnist.html.
                                  and Pointing
                                    2
                                 
                                 
                                    2
                                    
                                       http://www-prima.inrialpes.fr/Pointing04.
                                  data sets;

Before the experiments, we need to preprocess some data sets. The details are described in Table 1
                        , and the pretreatment methods are as follows:

                           
                              (a)
                              each data object is divided by the 103;

each data object is divided by the 104;

read the grayscale matrix with the size of n∗m for each image and average each dimension of gray level matrix, and get the matrix with the size of 1∗m to represent the image data.

Because an object is either a boundary point or not, we use the accuracy rate and recall rate to evaluate the capability of the detection results, and take the value of F-measure as a comprehensive performance evaluation standard.


                        Fig. 3
                        (a) is a synthetic data set which includes three elliptic clusters and 5400 points, called Syn1. Since there are a lot of noises located near the edge of clusters, they increase the difficulty of detecting the boundary. Fig. 3(b) describes the synthetic data set named Syn2. The data set includes a circular cluster and an annulus cluster, with 4800 points. The noises are distributed uniformly between the circular and annulus clusters, and the circular cluster nests inside the annulus cluster. Because the clustering techniques based on connectionist theory will fail to separate the two clusters, they may identify the two clusters as one cluster. Compared with general data sets, the distribution of noises of Syn2 are uniform. Therefore the core points and noises are difficult to distinguish with each other, and it also weakens the neighborhood distribution's differences of noises and boundary.

As there are no standards of boundary points in the two synthetic data sets, we try to label the clustering boundary of the two data sets according the geometry curve equation. We take the points that are located inside the clusters and the distance between which and curve is 0.5 units as clustering boundary. Fig. 4
                        (a) is the result of labeling the clustering boundary for Syn1. Fig. 5
                        (b) is the result of labeling the clustering boundary for Syn2. The other figures in
Figs. 4 and 5 are the clustering boundary detection results of some different algorithms on these two data sets.

In order to analyze the detection capability of the several algorithms more clearly, the values of F-measure of different algorithms are reported in Table 2
                        . BORDER algorithm's performance is sensitivity to noises, so the detection results contain a lot of noises. BRIM and BAND algorithms' results contain some noise points near the edge of clusters. BRINK uses the weighted Euclidean distance as the similar measure between data points, but the measure's performance may be bad in the two-dimensional data space. After all, we cannot guarantee this similar measurement is better than the general Euclidean distance. From the coefficient analysis, we find that the Spinver's performance is better than other algorithms in various aspects.

In this section, we implement our experiments on some different medical data sets. The clustering boundary of normal people indicates the people who have infected some virus, but not be suffered from the diseases. Effective detection of these people may have significant uses. Some preventive treatments can be taken, or the patients can be cured at early stage. In addition, the clustering boundary of gene expression profile dataset means the people who carried some kind of invisible hereditary diseases or may appear genetic mutation. Effective detecting these individuals means that not only they can be treated, but also it's convenient to carry out further study on species evolution by the gene fragments which appear genetic mutations.

Biomed
                           3
                        
                        
                           3
                           
                              http://lib.stat.cmu.edu/datasets/biomed.data.html.
                         data set has 134 normal objects and 75 virus infected objects. But there are 30 virus carriers in the normal objects, and these people can be defined as the clustering boundary of normal people. Cancer
                           4
                        
                        
                           4
                           
                              http://archive.ics.uci.edu/ml/datasets.html.
                         data set has 241 malignant tumor objects and 75 benign tumor objects. But there are 30 benign tumor objects which may become malignant tumor patients, and these people are clustering boundary of normal people too. Colon
                           5
                        
                        
                           5
                           
                              http://genomics-pubs.princeton.edu/oncology/affydata/.
                         data set is a colon cancer gene expression data set which has 62 samples, including 22 normal samples and 40 colon cancer samples. In addition, each sample has 2000 genes. Prostate
                           6
                        
                        
                           6
                           
                              http://www.gems-system.org/.
                         data set is also a gene data set which has 102 samples, including 50 norm samples and 52 prostate cancer samples. In this data set, each sample has 10,509 genes. Before the experiments, we take the statistical experiments to get 7 clustering boundary objects on Colon data set, 18 clustering boundary objects on Prostate data set, using DBSCAB algorithm. Then we pretreat these data sets according Table 1. Because BRIM algorithm cannot be used in high dimensional space, so we will not report its result in Table 2.
                     

In this table, Real.boun is the real number of clustering boundary, Num.det is the detection number of boundaries in different algorithms, Num.C is the detection number of correct boundary points.

Since some noises and boundary objects may have the same coefficient of variation, BAND algorithm cannot detect completely the clustering boundary. The detection results of BORDER algorithm contain all noises in the general data sets, but the algorithm will performance better in the no-noises data sets. BRINK acts as an improved algorithm based on BAND, and its performance is better than that of BAND algorithm in high dimensional space. But when the dimensionality increases dramatically, the performance of BRINK will decrease. From the observation of F-measure in Table 2, we find that the clustering boundary detection ability of Spinver is more effective than that of other algorithms.

Handwritten recognition [44,45] is an important research topic in the fields of pattern recognition and artificial intelligence, and has important applications in many areas, such as identity authentication, E-mail scanning, and etc. Since the influence of personal preferences and habits, there are big differences on digital shape, size, line width, and etc. for the same digit. The clustering boundary can be defined as the digit images which appear overlapping, ink, and etc. Effectively extracting the clustering boundary of handwritten digits can improve the clustering accuracy.

In this section, we finish our experiments on the Mnist data set. The data set includes 10 types of handwritten digits, including 60,000 training image samples and 10,000 test image samples. There are 8 bit depth of BMP image for all the image size, stored in the form of 28∗28pixel size, and each pixel gray value is in the range of 0–255. We select the handwritten digit ‘3’ from the test image samples, a total of 1010 images, to verify the clustering boundary detection ability of Spinver.


                        Fig. 6
                         shows the clustering boundary detection result of Spinver on the digit ‘3’, and contains 80 images. Compared to clustering boundary objects, Fig. 7
                         shows 80 clustering center objects using Spinver.

From the experiments of Figs. 6 and 7, we find Spinver can effectively detect the boundary of ‘3’. Compared to the clustering center objects, the images of clustering boundary are irregular, and hard to recognize. So, it verifies the clustering boundary detection ability of Spinver.

Face recognition [46,47] is an important study in computer graphics, computer vision, machine learning, pattern recognition, and etc. The study takes the facial features as the medium of human, uses computer image processing techniques to capture the individual's stationary or moving features to identity match. Compared to normal face, face boundary can be defined as the face images which have the features of strong illumination or faint illumination, sunglasses, profile face, and etc. However, these images affect the accuracy of face recognition, so effectively detect the clustering boundary will provide an important reference for the face image feature extraction and face recognition accuracy.

Pointing data set contains different head postures images of 15 volunteers, containing nine postures in the vertical direction and thirteen postures in the horizontal direction. This data set has two sequences and we take the first sequence to finish our experiment. The first sequence contains 93 images of each volunteer and a total of 1395 images. The image format is JPG with 8 bit depth, and the pixel size is 384∗288 and each pixel's grey level range from 0 to 255. Before the experiments, we transform these images to a matrix with the size of 1395∗384.

We choose 93 face images of a volunteer from the first sequence to detect boundary objects, and the detection result is showed Fig. 8
                        . Then, we finish the boundary detection experiment on the first sequence of Pointing data set, and the detection result is showed in Fig. 9
                        .


                        Figs. 8 and 9 show the clustering boundary detection results on Pointing data set. All the detection results include the images which has a big side angle on horizontal or vertical directions. Through the above experimental analysis, we can further verify the effectiveness of the Spinver algorithm in high dimensional data space.

@&#DISCUSSION@&#

Spinver algorithm has three parameters: k, br, fr. To analyze the parameters in more detail, we show some experiments on some data sets. These experiments explain the influence of parameter selection in the process of clustering boundary detection. Among them, N means the sample number of datasets. Digit `3’ means the handwritten images of ‘3’ in Mnist. Digit `8’ means the handwritten images of ‘8’ in Mnist.


                        Fig. 10
                         shows k with regard to the large number of samples. When k is relative small or big, Spinver's performance will be bad. So the curves are first rise, and then decline. Furthermore, when k∈[0.002N,0.012N], Spinver can get good result. Compared to the dataset size (i.e. N), the value of k is relatively small. Fig. 11
                         shows k with regard to the small number of samples. When k∈[0.05N,0.55N], Spinver can get good result. Compared to the dataset size, k is relatively big. From the group of experiments, we find: for large sample dataset, k may be relatively small; for small sample dataset, k will be relatively big.


                        Fig. 12
                         shows k with regard to the dataset dimensionality. We take the detection results when k
                        =10 as the standards for each data set. Compared to Figs. 10 and 11, we can find k generates greater influence on the detection result. Fig. 13
                         shows the change of F-measure when choose different real k on various kinds of dataset. We can find when k∈[10,100], Spinver could get good result (Fig. 14
                        ).


                        br is the boundary ratio, and controls the number of clustering boundary. When br is small, Spinver will detect less boundary points. When br is big, Spinver will detect too many points as boundary. fr is the filter ratio, and controls the number of noises. When fr is small, the parameter will smooth less noises and isolated points, we still can use br to get good boundary detection results. But if the data sets have a lot noises, the parameter is very important. When fr is big, Spinver will recognize many points as noises. Certainly, these noises include some boundary points, so Spinver's performance will be very bad. Generally, fr should be small. From
Fig. 15
                        , we can see the curve will decline quickly when fr increases.

A large number of statistical experiments show that Spinver gets good boundary detection result when k∈[10,100], br∈[0.2,0.3] and fr∈(0,0.1] for general data sets. Through the experimental analysis and theoretical research, we think the number of clustering boundary occupies the proportion of 20% to 30% of the whole data set, and noises occupies the proportion of 0% to 10%. But it is important to note that fr = 0 in the none noises data sets. Generally, we suggest k
                        =10, br
                        =0.3, fr
                        =0.1 to finish the detection task.
                        
                     

In this section, we will discuss the scalability of the proposed algorithm with increasing the sample number and dimensionality of datasets. Fig. 16 shows the relations between runtime and dataset size. In this experiment, we vary the dataset size from 1000 to 10,000. As we can see, though the time complexities of BAND, BRINK, Spinver are allO(kn
                        2), and Spinver algorithm's performance is the best. Since BORDER algorithm costs much time on calculating the revere k nearest neighbors, its performance is the worst. The time complexity of BRIM isO(nlog n). However, it is important that BRIM only can detect the clustering boundary for two-dimensional space.


                        Fig. 17 shows the relations between runtime and dataset dimensionality. In this experiment, we vary the dataset dimension from 500 to 5000. As we can see, with the dimensionality of dataset rapidly rising, all algorithms’ runtimes are increasing. However, the execution time of Spinver is always better than that of other algorithms.

@&#CONCLUSIONS@&#

We propose a high dimensional clustering boundary detection algorithm. Inspired by the space inversion of particle physics, we propose the idea of local space transformation and dynamic relative position, and construct a clustering boundary detection algorithm for the challenging problem of high dimensional space. Experiments show that Spinver algorithm provides meaningful results and significantly improves the detection quality no matter the data space has a low or high dimension. Moreover, we provide a Filter function to help the algorithm to handle the outliers. The performance of our algorithm on real data sets verifies the application value in the medicine field. Experiments on image data sets extend our research range.

@&#ACKNOWLEDGMENTS@&#

This work is supported by Basic and Advanced Technology Research Project of Henan Province (Grant no. 152300410191). And the authors are grateful to the anonymous referees for their valuable comments and suggestions.

@&#REFERENCES@&#

