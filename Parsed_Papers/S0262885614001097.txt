@&#MAIN-TITLE@&#A spectral independent approach for physiological and geometric based face recognition in the visible, middle-wave and long-wave infrared bands

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Propose new automated tri-spectral (visible, MWIR and LWIR) FR approach


                        
                        
                           
                           Design experiments to quantitatively measure benefits of global vs. local matchers


                        
                        
                           
                           Evaluation of global vs. local based matchers when fused at the score level


                        
                        
                           
                           Achieve rank-1 identification rate of at least 99.43% per spectrum of operation


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Face recognition

Visible

Middle-wave infrared

Long-wave infrared

Fiducial point extraction and matching

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

@&#MOTIVATION@&#

Biometrics is the science and technology of using traits, preferably inherent, for the identification of individuals. There are a number of biometric traits that exist, using either physiological (fingerprint, face, iris, voice and hand geometry) or behavioral characteristics (gait, signature, and keystroke), being researched substantially, and used in academia, industry, and government applications [14,15]. Face-based recognition (FR) systems, in particular, are gaining interest because face has several advantages over other biometric traits: it is non-intrusive, understandable, and can be captured in a non-cooperative or covert (when necessary) manner at variable standoff distances.

Most face recognition systems depend on the usage of face images captured in the visible range of the electromagnetic spectrum, i.e. 380–750nm. However, in real-world scenarios (military and law enforcement) we have to deal with harsh environmental conditions characterized by unfavorable lighting and pronounced shadows [6]. Such an example is a night-time environment [6], where human recognition based solely on visible spectral images may not be feasible. In order to deal with such difficult FR scenarios, multi-spectral camera sensors are very useful because they can image day and night. Thus, recognition of faces across the infrared spectrum has become an area of growing interest [6].

The goals of this work include 1) the proposal of a new fully automated pre-processing FR approach that is applied on full-frontal face images acquired using visible, MWIR and LWIR camera, and 2) the design of experiments to quantitatively illustrate the benefits of global vs. local based matchers, and finally, the performance of such matchers when fused at the score level. In regard to the fully automated pre-processing approach, human eyes and eye centers are detected. Then, eye coordinates are used to geometrically normalize faces. Also, the inter-ocular distance is fixed by setting the dimensions of the input face images at a specific spatial resolution and the eye centers of these images at predetermined (x,
                        y) locations. Prior to the feature extraction stage, the parameters of the image diffusion and face segmentation algorithms are optimized. Anisotropic image diffusion is also used to smooth each face image, prior to applying top-hat segmentation.

In our experiments, we use the segmented features to demonstrate that our face recognition system performance improves when fusing global and local-based matchers. The global matcher uses the feature segmented image for matching, i.e. this matcher analyzes the morphology of the face through the convolution of face images, producing a match ratio [22]. Although the global matching algorithm is based on the overlap of neighborhood pixels, the accuracy of the algorithm increases when unique segments are matched. The local matcher requires fiducial points that we obtain using the extracted feature segment, and matches all points to one another (point to point). These fiducial points are minutiae extracted from physiologically-based (when using subcutaneous facial characteristics) and geometrically-based face features (e.g. eye edges and eyelashes), which are unique for each individual. The efficiency of our local matcher is tested using automated methods for extraction of correspondence points, such as Scale-Invariant Feature Transform (SIFT) [18,19] and Speeded Up Robust Feature (SURF) [1]. In the matching and decision making step for both matchers, after our feature segmentation scheme is applied to each image, input images (probes) are matched with a stored template (gallery). Both matchers perform well individually, but optimal performance is achieved fusion. At the matcher level, we used score-sum with min–max normalization for fusion. The proposed system is depicted in Fig. 1
                        .

In our previous work [24], our fully automated approach was applied solely in the MWIR spectrum on 50 subjects using our local matcher. In this work, we use double the number of the subjects tested in our preliminary study [24] to evaluate the recognition performance of our FR system. The proposed matching scheme, which is obtained by fusing a global matcher (e.g. segment matcher) along with a local matcher (e.g. fiducial point matcher), results in an impressively high performance when applied on data captured across three different bands. To the best of our knowledge, this is the first time that a fully automated, unified FR approach is proposed and successfully applied across the visible, MWIR and LWIR bands with very promising results.

The rest of this paper is organized as follows. Section 2 briefly reviews the background information. Section 3 describes our proposed face image extraction and matching methodology. Section 4 details the assembly and construction of our multi-spectral face image database. Section 5 reviews the steps carried out on the multi-spectral database for our face recognition methodology. Section 6 is the empirical evaluation of the proposed methodology. We close out the paper with conclusions and possible future work in Section 7.

@&#BACKGROUND@&#

Face recognition systems utilize independent and unique face characteristics from individuals, but system performance may be influenced by the approach and design of a system [17]. There are a number of face recognition techniques (e.g. appearance, model and texture), as well as matching approaches that may be utilized and combined for optimal system performance [38]. Appearance-based methods (i.e. PCA and LDA [2]) derive feature vectors based on pixel values of face images and transform the features to low-dimensional salient features. Model-based methods (i.e. Elastic Bunch Graph Matching [37]) encode aspects of the face by using a face model that is created using the subject's face appearance and shape. Texture-based (i.e. LBP and SIFT) approaches extract textural features from images and try to find patches or relationships between pixels. On the other hand, standard commercial products such as L1 Systems G8® (G8) are essentially a black box because its algorithms are proprietary, but some combination of the aforementioned approaches is probably utilized.

There are trade-offs to the type of features (e.g. global or local) that are used in matching face templates regardless of the approach chosen. Global-based features come from the entire face and are characterized by vectors fixed in length, therefore they are compared in a time efficient manner. Local-based features are detected sets of points which may vary depending on the local information of the subject. Local-based features are less susceptible to variations in the face, but computation maybe exhaustive. The computational complexity of a face matching system may also be increased using a training-based approach instead of utilizing direct matching. Although, under certain conditions, a larger training dataset may increase performance accuracy, performance maybe sensitive to the alternate training sets available and used especially when the size of the training set is not sufficient. With the use of direct matching approaches, computational overload can be reduced and the designed and developed FR system can provide an attractive alternative to training-based ones. Our proposed approach follows such a concept.

The infrared band is divided into the active and passive IR bands. The active IR band consists of the Near IR (0.7–0.9μm) and the Short-Wave IR (0.9–2.5μm) bands. During data acquisition in the active IR band, the subject's face is usually illuminated using an external light source that can be detectable (e.g. NIR) or not (e.g. SWIR). When data is acquired in the passive IR band, the camera sensor detects IR radiation in the form of heat, which is emitted from the subject's face. This band is divided into the Mid-Wave Infrared (MWIR) and the Long-Wave Infrared (LWIR) band, where MWIR ranges from 3 to 5μm, while LWIR ranges from 7 to 14μm [21]. Recent research in the area of FR at different bands illustrates that working in the passive IR band is very challenging [4]. However, the main benefit of using thermal-based camera sensors is the capability to use FR systems when operating at difficult environmental conditions, such as low light or complete dark environments. This allows for the potential to have obscure detection and acquisition of face images of different subjects, given that they are within an appropriate (i.e. depending on the sensor capabilities) distance from the camera sensors used. Although the appearance of MWIR and LWIR face images looks visually similar (see Fig. 2
                        ), they both capture different characteristics of the human face. MWIR has both reflective and emissive properties and works better for long-range operations. On the other hand, LWIR consists primarily of the subject's emitted radiation. Face images acquired in the passive band aren't normally affected by external illumination. Another advantage is that unique physiological features for each individual (useful for human recognition), such as subcutaneous information (veins), edges of facial features, and wrinkles, are detectable in this band. Finally, image acquisition in the passive band is unobtrusive, and therefore, FR in this band is considered very useful for both military and law enforcement applications [5].

FR in the visible spectrum is an immensely explored research area. Visible images are sometimes fused with images captured at another band (such as the IR band) in order to recognition accuracy [9,35]. However, to the best of our knowledge, there are no face-based matching algorithms designed to work efficiently when operating across different bands, visible and passive IR bands. In [20] the authors reported that FR in the LWIR band achieves a rank-1 accuracy of 97.3% when using local binary patterns (LBP), while no cropping or geometrical normalization step is required. In Socolinsky et al. and [30], the authors used two standard FR algorithms to show that, under variable illumination conditions, the usage of LWIR face images yields a higher recognition performance than visible ones. However, the drawback of the approaches is that LWIR and visible images were divided into multiple training and testing sets, resulting in an increase of the FR system design time. In addition, [30] performed the study using co-registered images that were captured simultaneously by both visible and LWIR cameras — this is not usually possible in operational environments. In other IR-based FR approaches, such as Trujillo et al.'s and [32], the authors proposed an unsupervised local and global feature extraction paradigm to classify different facial expressions. In Chen et al. and [10] the authors combined visible and thermal-based images and compared them using Principal Component Analysis. However, neither Trujillo's [32] nor Chen's [10] work focused on the MWIR band.

The proposed methodology is composed of a feature extraction and matching process. The feature extraction process has two steps. First, anisotropic diffusion is used to reduce noise while preserving vital image content. Next, top hat segmentation is carried out in order to segment and extract face-based features. The features extracted include: (a) veins, (b) edges, (c) wrinkles, and (d) face perimeter outlines (see Fig. 3
                     ). After features are extracted, different matchers (e.g. global and local) are utilized, before finally, they are fused together at the score level in an effort to achieve increased rank-1 identification performance. The aforementioned methodological steps are described below.

All face images are processed to remove background noise added during video acquisition, and enhance edges, the use of Perona–Malik anisotropic diffusion [25]. This is important because noise is reduced without the removal of significant image content, such as edges and lines. The mathematical representation for this process is described as follows:
                           
                              (1)
                              
                                 
                                    
                                       ∂
                                       I
                                       
                                          
                                             x
                                             ¯
                                          
                                          t
                                       
                                    
                                    ∂
                                 
                                 =
                                 ∇
                                 
                                    
                                       c
                                       
                                          
                                             x
                                             ¯
                                          
                                          t
                                       
                                       ∇
                                       I
                                       
                                          
                                             x
                                             ¯
                                          
                                          t
                                       
                                    
                                 
                              
                           
                        where I
                        
                           N,t
                        
                        =
                        I
                        
                           t
                        (x,
                        y
                        +1)−
                        I
                        
                           t
                        (x,
                        y). The diffusion operator inherently behaves differently, depending on which spectrum we are operating in. For images in the visible spectrum, diffusion is used in edge detection between lines of dissimilar contrast. The face-based features (wrinkles, veins, edges, and perimeters) are segmented with the use of image morphology. For the passive infrared band, heat diffusion generally produces weak sigmoid edges in the thermal band during heat conduction, which in turn creates smooth temperature gradients at the intersecting boundary of multiple objects with dissimilar temperatures in contact. In order to segment these features, morphological top-hat filtering is employed.
                           
                              (2)
                              
                                 
                                    I
                                    open
                                 
                                 =
                                 
                                    
                                       I
                                       ⊖
                                       S
                                    
                                 
                                 ⊕
                                 S
                                 ,
                              
                           
                        
                        
                           
                              (3)
                              
                                 
                                    I
                                    top
                                 
                                 =
                                 I
                                 −
                                 
                                    I
                                    open
                                 
                              
                           
                        
                     


                        I,
                        I
                        
                           open
                        , and I
                        
                           top
                         are the original, opened, and white top hat segmented images, respectively. S is the structuring element, and ⊖ and ⊕ are the morphological operations for erosion and dilation respectively. Parameters for Perona–Malik anisotropic diffusion process along with top hat segmentation, were empirically optimized to ensure that the resultant images (see Fig. 3) did not contain noise, i.e. outlier edges that do not represent clear face-based physiological and geometrical features. Pixel normalization was the only pre-processing done on the images during this experiment, unlike the original work, which dilated, skeletonized, and bridged MWIR images [8].

It is imperative that the elliptical mask is applied after the feature extraction stage to ensure that spurious, artificial feature points are not created by the mask's presence during top hat segmentation. In practice, image masking ensures that no clothing is present during feature detection. When the elliptical mask segments the face, the part of the original image that is masked is set to a black background. Note also that the dimensions of the ellipse used for all images were fixed.

Many FR matching methods can be categorized as being global or local, depending on whether features are extracted from the entire face or from a set of local regions. Global features are usually represented by vectors fixed in length, which are compared during the matching process in a time efficient manner. On the contrary, local feature-based approaches first detect a set of fiducial points, using the surrounding pixel values. The number of matched fiducial points between an input and gallery template is used to calculate the match score. Since the number of fiducial points may vary depending on the subject, two sets of points from two different subjects cannot be compared directly. As a result, the matching scheme has to compare each fiducial point from one template against all the other fiducial points in another template, increasing time for matching. Global features are more susceptible to variations in the face when all pixel values are encoded into a feature vector, especially with respect to geometric transformations.

It is usually a difficult task to extract on face information using only simple techniques. Our feature segmentation step extracts lines, edges, and wrinkles, which are unique to each subject, such as in the case of contours. In two face images of the same subject, similar features may still be found in contours, especially in the case of subjects which may be identical twins. However, there may be remarkable differences in not only the shape of these contours but in the size as well, across subjects. Hence, identification may be carried out using matching of the segmented features. The segmented face features are compared using template matching.

To find the maximum similarity between a gallery and probe image, the two feature segmented images are convoluted. The probe image is slid pixel by pixel across the gallery image in a top to bottom and left to right fashion. If f(i,
                           j) and g(k,
                           l) are the pixels at positions (i,
                           j) and (k,
                           l) of probe and gallery feature segments respectively, then α,
                           β measure the horizontal and vertical displacement between f(i,
                           j) and g(k,
                           l) during the traversal process. To account for the small differences that exist in the segmented features from different samples of the same subject, a 5×5 window around a white pixel is used for template matching. If for a white pixel in the probe segmented image, there is another white pixel in the 5×5 neighborhood of the corresponding position on the gallery segmented image, then the pixels are said to be matched. If 
                              
                                 α
                                 ^
                              
                              ,
                              
                                 β
                                 ^
                              
                            are the horizontal displacement and vertical displacement respectively, which give the best matching result, then the maximum similarity 
                              
                                 h
                                 
                                    i
                                    ,
                                    j
                                 
                              
                              
                                 
                                    α
                                    ^
                                 
                                 
                                    β
                                    ^
                                 
                              
                            defined by Eq. (4) between corresponding feature segments can be obtained [22].
                              
                                 (4)
                                 
                                    
                                       h
                                       
                                          i
                                          ,
                                          j
                                       
                                    
                                    
                                       
                                          α
                                          ^
                                       
                                       
                                          β
                                          ^
                                       
                                    
                                    
                                       =
                                       Δ
                                    
                                    
                                    α
                                    ,
                                    βmaxi
                                    ,
                                    j
                                    ∑
                                    
                                       
                                          h
                                          
                                             i
                                             ,
                                             j
                                          
                                       
                                    
                                    
                                       α
                                       β
                                    
                                 
                              
                           
                           
                              
                                 (5)
                                 
                                    
                                       h
                                       
                                          i
                                          ,
                                          j
                                       
                                    
                                    
                                       α
                                       β
                                    
                                    =
                                    ϕ
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   x
                                                   =
                                                   −
                                                   2
                                                
                                                2
                                             
                                             
                                          
                                          
                                          
                                             
                                                ∑
                                                
                                                   y
                                                   =
                                                   −
                                                   2
                                                
                                                2
                                             
                                             
                                          
                                          
                                          
                                             
                                                
                                                   f
                                                   
                                                      i
                                                      ,
                                                      j
                                                   
                                                
                                                ⋅
                                                
                                                   g
                                                   
                                                      k
                                                      +
                                                      x
                                                      ,
                                                      l
                                                      +
                                                      y
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                           
                              
                                 
                                    k
                                    =
                                    i
                                    ±
                                    α
                                    ,
                                    l
                                    =
                                    j
                                    ±
                                    β
                                    ,
                                 
                              
                           
                           
                              
                                 
                                    α
                                    =
                                    0
                                    ,
                                    1
                                    ,
                                    2
                                    ,
                                    …
                                    320
                                    ,
                                    β
                                    =
                                    0
                                    ,
                                    1
                                    ,
                                    2
                                    ,
                                    …
                                    256
                                 
                              
                           
                           
                              
                                 (6)
                                 
                                    ϕ
                                    
                                       x
                                    
                                    =
                                    
                                       
                                          
                                             
                                                1
                                                ,
                                             
                                             
                                                for
                                                
                                                x
                                                ≥
                                                1
                                             
                                          
                                          
                                             
                                                0
                                                ,
                                             
                                             
                                                for
                                                
                                                x
                                                =
                                                0
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

For images of the same subject, after template matching, long overlapping segments (in the sense of Eq. (5)) are obtained. On the other hand, for different subjects, even if a high match score is obtained from Eq. (4), the existence of abundant segments overlapped by chance is expected. Therefore, if these short segments can be eliminated effectively, a stable performance of the discrimination method can be achieved. The final matching score H(f,
                           g) can be calculated using Eq. (7).
                              
                                 (7)
                                 
                                    
                                       H
                                       
                                          f
                                          ,
                                          g
                                       
                                    
                                    =
                                    
                                       2
                                       
                                          F
                                          +
                                          G
                                       
                                    
                                    ⋅
                                    
                                       
                                          ∑
                                          
                                             i
                                             ,
                                             j
                                          
                                       
                                       
                                    
                                    
                                    
                                       h
                                       
                                          i
                                          ,
                                          j
                                       
                                    
                                    
                                       
                                          α
                                          ^
                                       
                                       
                                          β
                                          ^
                                       
                                    
                                 
                              
                           
                        


                           F
                           and
                           G denote the number of pixels in the feature segmented lines of the probe and gallery images respectively. The fragment removal threshold θ
                           
                              i
                            was optimized experimentally, and held constant for each image. Prior to matching, all images are filtered by removing pixel values below θ
                           
                              i
                           .

Fiduciary points are used as points of measure in many face recognition applications and play an important role in our localized matching algorithm. The use of fiducial points is appropriate due to the permanent and unique nature of the extracted features related to face-based subcutaneous information. Note also that although only face images with neutral expressions and poses were used in our experiments, fiducial points can be extracted and used for matching regardless of facial pose or expression [7].
                              
                                 •
                                 
                                    Fingerprint-based minutiae detector: The proposed method, a fingerprint based minutia point recognition system is used to detect features in the face [39]. Beforehand, the pixel intensities of the images are normalized so that they are binary. For our normalization, the minimum pixel intensity of the image is set as the threshold for visible images and the mean pixel intensity of the image is set as the threshold for passive infrared images. While traversing the image, if the current pixel intensity is smaller than the threshold, it receives a binary value of zero. If the current pixel intensity is larger than the threshold, it receives a binary value of one. During the traversal process, if the 3×3 neighborhood around a center white pixel had exactly three white pixels, it was labeled a branch point. If the center white pixel with a 3×3 neighborhood around it contained only 1 white pixel neighbor, then it was labeled an end point (see Fig. 4
                                    ). Depending on the normalization technique, large clusters of both branch points and end points may be found.

Occasionally, artifacts may still be introduced into the segmented image, which later lead to spurious minutia. These false minutiae will significantly impact the performance of our fiducial point matching algorithm if they are classified as genuine minutia. Therefore, some mechanisms of removing false minutia are important in efficiency and effectiveness of extracted points. This is consistent mostly of the removal of either branch points or end points that are close to each other, using a threshold distance. The threshold distance is computed as the average distance between two parallel neighboring segments in an image.

A point alignment-based Random Sample Consensus (RANSAC) matching algorithm with the ability of finding the correspondences between a stored set of gallery points and input set of probe points is used. The set of input points are first aligned with the gallery points and then a match score is computed, based on corresponding points. Let G and P be the gallery and probe points we are trying to match. Because the correspondence points are computed based on local information of each subject, G and P may not have the same number of correspondence points: let m and n be the number of points for G (Eq. (8)) and P (Eq. (9)).
                              
                                 (8)
                                 
                                    G
                                    =
                                    
                                       
                                          
                                             c
                                             1
                                          
                                          ,
                                          
                                             c
                                             2
                                          
                                          ,
                                          …
                                          
                                             c
                                             m
                                          
                                       
                                    
                                    ,
                                    
                                    with
                                    
                                    
                                       c
                                       i
                                    
                                    =
                                    
                                       
                                          x
                                          i
                                       
                                       
                                          y
                                          i
                                       
                                    
                                    ,
                                    i
                                    =
                                    1
                                    …
                                    m
                                 
                              
                           
                           
                              
                                 (9)
                                 
                                    P
                                    =
                                    
                                       
                                          
                                             c
                                             1
                                             
                                                
                                                ′
                                             
                                          
                                          ,
                                          
                                             c
                                             2
                                             
                                                
                                                ′
                                             
                                          
                                          ,
                                          …
                                          
                                             c
                                             n
                                             
                                                
                                                ′
                                             
                                          
                                       
                                    
                                    ,
                                    
                                    with
                                    
                                    
                                       c
                                       j
                                       
                                          
                                          ′
                                       
                                    
                                    =
                                    
                                       
                                          x
                                          j
                                          
                                             
                                             ′
                                          
                                       
                                       
                                          y
                                          j
                                          
                                             
                                             ′
                                          
                                       
                                    
                                    ,
                                    j
                                    =
                                    1
                                    …
                                    n
                                 
                              
                           
                        

Two correspondence points c
                           
                              i
                            and c
                           
                              i
                           
                           ′ are considered matching if close in position. This can be written according to Eq. (10), using the spatial distance sd. Two minutia points from each set are considered matching if their spatial distance (sd) is within a threshold (R
                           
                              θ
                           ).
                              
                                 (10)
                                 
                                    sd
                                    
                                       
                                          c
                                          i
                                       
                                       
                                          c
                                          j
                                          
                                             
                                             ′
                                          
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      x
                                                      i
                                                   
                                                   −
                                                   
                                                      x
                                                      j
                                                      
                                                         
                                                         ′
                                                      
                                                   
                                                
                                             
                                             2
                                          
                                          +
                                          
                                             
                                                
                                                   
                                                      y
                                                      i
                                                   
                                                   −
                                                   
                                                      y
                                                      j
                                                      
                                                         
                                                         ′
                                                      
                                                   
                                                
                                             
                                             2
                                          
                                       
                                    
                                    <
                                    
                                       R
                                       θ
                                    
                                 
                              
                           
                        

The distance between each possible point in the gallery set and point in the probe set is computed and sorted in increasing order. We then count the number of matches below a threshold, which is optimized for the fixed dimension of our face images. The highest counted number of correspondences; C
                           
                              best
                           , for each set of points is stored and used to compute the match score, M
                           
                              S
                           . We let:
                              
                                 (11)
                                 
                                    
                                       M
                                       S
                                    
                                    =
                                    
                                       
                                          
                                             
                                                C
                                                best
                                             
                                          
                                          2
                                       
                                       
                                          tot
                                          
                                             G
                                             P
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        

To enhance the recognition performance, confidence level fusion schemes can be invoked [13]. In this work, score level fusion is implemented to combine the match scores obtained from multiple matchers (global and local). The respective scores obtained individually for each matcher are fused together by the sum rule, after using min–max normalization. For a given vector S of similarity based scores of length Q, the min–max normalization of the input score vector may be computed using Eq. (14) for each score S
                        
                           i
                        , for all individual scores (i.e. ∑
                        
                           i
                           =1
                        
                           Q
                        ). The operators min(S)and
                        max(S) represent the minimum and maximum values of the score vector S respectively. Simple sum rule for both of our score vectors (H
                        
                           f,g
                        
                        and
                        M
                        
                           S
                        ) is carried out using Eq. (13), after min–max normalization (Eq. (14)) of each vector of matcher scores. For Eq. (13), m represents the total number of matchers.
                           
                              (12)
                              
                                 
                                    S
                                    
                                       N
                                       i
                                    
                                 
                                 =
                                 
                                    
                                       
                                          S
                                          i
                                       
                                       −
                                       min
                                       
                                          S
                                       
                                    
                                    
                                       max
                                       
                                          S
                                       
                                       −
                                       min
                                       
                                          S
                                       
                                    
                                 
                                 ,
                              
                           
                        
                        
                           
                              (13)
                              
                                 
                                    S
                                    
                                       F
                                       i
                                    
                                 
                                 =
                                 
                                    
                                       
                                          S
                                          
                                             N
                                             i
                                          
                                       
                                       +
                                       …
                                       +
                                       
                                          S
                                          
                                             N
                                             m
                                          
                                       
                                    
                                    m
                                 
                                 .
                              
                           
                        
                     

In this section, we will describe the hardware used for 1) the acquisition of the high-quality face images, and 2) MWIR and LWIR face images. We will also describe the live subject-capture setup used during the data collection process and the three spectral face image databases utilized in this paper. Finally, the IR sensitivity and calibration necessary for the passive IR sensors are discussed.
                        
                           1
                           
                              Hardware and subject-capture setup: Three different spectral cameras were employed for data collection in order to assemble our multi-spectral face image database. The live face capture configuration we used had a standoff distance of 5ft (~1.5m) between the subject and our cameras. The database was collected indoors over multiple sessions spanning over a time period of 3months. At the first session, the subjects were briefed in regard to the data collection process, after which they signed a consent document. In total, 138 subjects were used in the construction of the databases. Each database consists of only full frontal face images with a neutral facial expression for every subject. Of the 15 samples that exist for each subject, only 4 samples were used for our matching experiments. The first 2 samples were the gallery images, while the remaining 2 samples were the probe images.


                              Experimental protocol: A Canon EOS 5D Mark III digital camera was used for the acquisition of high-quality visible face images (1920×1080). A FLIR SC8000 MWIR camera was used for the acquisition of high-quality MWIR face images (1920×1080). A FLIR SC600 LWIR camera was used for the acquisition of LWIR face images (640×480).
                                 
                                    •
                                    
                                       Visible dataset: In this work, the Canon SLR is used to obtain standard RGB, ultra-high resolution fontal pose face images in the visible spectrum (Fig. 2(a)). This digital camera has a 21.1-megapixel full-frame CMOS sensor with DIGIC 4 Image Processor and a vast ISO Range of 100–6400. It also has auto lighting optimizer and peripheral illumination correction that enhances its capability. The visible images are extracted from the movie files in JPEG format. The JPEG images extracted had an approximate file size of 100kB per image. This dataset consists of 408 images (204 for probe and 204 for gallery) with four images per subject (102 subjects).


                                       MWIR dataset: A FLIR SC8000 camera served as the imager for obtaining MWIR images (see Fig. 2(b)). The camera features an Indium Antimonite (InSB) Focal Plane Array (FPA) achieving mega-pixel image resolution in a single MWIR image. The spectral range of the camera is 3–5μm, and it has a 14-bit dynamic range. The MWIR images were extracted from the thermal sequential files in BMP format. The BMP images extracted had an approximate file size of 619kB per image. This dataset consists of 408 images (204 for probe and 204 for gallery) with four images per subject (102 subjects).


                                       LWIR dataset: A FLIR SC600 camera served as the imager for obtaining LWIR images (see Fig. 2(c)). The camera features Indium Antimonite (InSB) Focal Plane Array (FPA) achieving mega-pixel image resolution in a single LWIR image. The spectral range of the camera is 7–14μm, and provides imaging performance from 16-bit data. The LWIR images were extracted from the thermal sequential files in BMP format. The BMP images extracted had an approximate file size of 922kB per image. This dataset consists of 404 images (202 for probe and 202 for gallery) with four images per subject (101 subjects).


                              Passive infrared sensitivity and calibration: The MWIR camera used in this work has a Noise Equivalent Temperature Difference (NETD) of less than 25mK. NETD indicates the sensitivity of a detector of thermal radiation. FLIR software was used: (a) to perform regular calibration, before acquiring each set of thermal face images. We used a black body and a two-point non-uniform correction process that performs both gain and offset normalization of pixel-to-pixel non-uniformity; (b) to remove noise (e.g., dead pixels) from face images and control the temperature scale limits (e.g., setting the temperature range from 28° to 40°C that is the typical range of human body temperature) during data collection. The LWIR camera used in this work has a NETD of less than 50mK. The FLIR software automatically calibrates the sensor regularly and removes noise from the face images.

We are operating in two types of bands (visible and passive infrared), so the skin segmentation and eye detection techniques are dependent on the band of operation. For classification of our multi-spectral database, simplistic approaches may be utilized because raw images in the visible band need to be separated from raw images in the passive infrared band. To filter visible images from infrared images, approaches such as hue, saturation and value (HSV) may be used to classify images (assuming RGB visible images), along with image metadata. In order to separate MWIR images from LWIR images, more advanced approaches, such as the Statistical Pattern Recognition Toolbox [12] should be utilized. The normalization used to standardize the subject faces is not dependent on the band of operation. An overview of the pre-processing methodology is depicted in Fig. 1. The salient stages of the proposed method are described below:
                        
                           1
                           
                              Skin segmentation: The first step of the face image matching pipeline is segmenting the face from the subject's body, clothing and background noise to eliminate any artificial features.
                                 
                                    •
                                    
                                       Visible: Skin segmentation is carried out by first converting the color (RGB) image to a hue saturated value (HSV) image. However, we are only interested in the hue of the HSV image, along with chroma components C
                                       
                                          B
                                        and C
                                       
                                          R
                                        of the original color image. We traverse the image and if the chroma components and hue value both fall within acceptable thresholds, then the pixel is set white, representing the skin. Due to variations in illumination among visible face images, there may be shadowing effects on the skin. Therefore, a range of thresholds is considered in order to avoid over/under-segmentation of skin in the visible spectrum. The optimal segmented face image is visually chosen and used for experimentation. Before using the binary mask created to mask the original image, the mask should be filled for holes. It is important to note that the more you decrease the max value or range for C
                                       
                                          R
                                        threshold, the more skin pixels are chosen. Mustafa Ucak originally implemented the skin segmentation method described above, however it was adapted for this work.


                                       Passive infrared: A blob detection algorithm is used to detect the bright portions of the thermal face using a predetermined threshold value. The threshold value for the blobs is the numerically larger pixel of the two intensity values that produce the greatest intensity difference. Using this threshold, a binary image is created, where pixel intensities above the threshold are assigned a value of one and pixel intensities below the threshold receive a value of zero. All blobs, which represent the human skin, are analyzed and the largest one is selected. The human face is the largest uncovered area of skin in our images due to clothing, so the largest blob represents the subject's face and neck. Background pixels (holes) inside the blob are then removed so that the mask is a solid. The face blob is then used as a mask against the original infrared image, setting the background and clothing black.


                              Eye detection: Important step in the face image matching pipeline because it is used twice, when normalizing images.
                                 
                                    •
                                    
                                       Visible: Whitelam et al. proposed a multi-spectral eye detection method that detects the pupils, although we only use it for the visible band [36]. This is achieved by first scaling all images up or down so that they are of the same size face width estimation. Then, by cropping the left and right eyes of a number of subjects' face images, we create synthetic eye templates. Varying pupil diameters caused by the exposure of light are compensated for using the previously cropped, synthesized eye templates. Viola & Jones face detection was then used to determine the spatial boundary of the face. After face detection, the coordinates of the face boundaries are used to align the template for convolution. The coordinates of the two highest Pearson product moment correlation coefficient — (r) calculated for each image represent the locations of the left and right eyes. The lowest pixel intensity within each eye is the detected pupil location, which is used for normalization of the visible images.


                                       Passive infrared: The problem of eye detection on IR images was studied by Jafri and Bourlai in [4]. Through experimentation, we find that this approach can be applied on LWIR images as well, in addition to MWIR images. First, the face is detected, which is trivial due to the previous skin segmentation method. Next, integral projections are used to find the general locations of the eyes and eyebrows on the face. Then, synthetic passive infrared eye templates are created from the left and right eyes of a number of subjects' face images. The generated templates are passed either (a) each face side of an input face image, or, (b) when integral projections are employed, the reduced search space that was determined by the location of the detected left and right eyebrows. The similarity score between the average eye template and a searched region is finally calculated using the Pearson product moment correlation coefficient. The highest correlation per search determines the location of each eye center. The coordinates of the eye centers obtained by this method are used for normalization of all of our passive infrared face images.


                              Normalization: There are two types of normalization done, the first of which is inter-ocular normalization, followed by geometric normalization. This is valuable because shape and size of the head vary among subjects. A canonized image is especially important because both of our matchers depend on location and alignment of extracted features.
                                 
                                    •
                                    
                                       Inter-ocular normalization: It standardizes all face images so head sizes are relatively similar. Once the eyes are detected on the face images, they are used to normalize all images so that the inter-ocular distance is fixed to 60pixels. This is accomplished by resizing the image acquired after skin segmentation using a ratio computed from the desired inter-ocular distance (60pixels) and the actual inter-ocular distance, i.e. the one computed when using the image after skin segmentation.


                                       Geometric normalization: A geometric normalization scheme is applied to images acquired after inter-ocular normalization. The normalization scheme compensates for slight perturbations in the frontal pose, and consists of eye detection and affine transformation. Automated eye detection is performed once more using the same template matching algorithm described in Step 2 of our pre-processing approach (where the coordinates of the eye were automatically obtained). After the eye centers are found, the canonical faces are automatically constructed by applying a similarity transformation. Finally, all faces are canonicalized to the same dimension of 320×256.


                              Feature Extraction and Matching: The methodology described in Section 3 is used. By employing this algorithm, we process the datasets described in Section 4, for both global and local feature based matchers. In order to show the effectiveness of our proposed local fiducial point matcher, we compare our fingerprint minutiae detector to popular corner detection methods, such as SIFT and SURF in each respective spectrum. It is important to note that the SIFT and SURF detector were only tested on a subset of our multi-spectral database (50 subjects) and sub-facial regions. For a larger dataset, we achieved better results using our fingerprint minutiae detector with the entire multi-spectral database.
                                 
                                    •
                                    
                                       SIFT: The detector extracts a number of frames from an image in a manner that is consistent with certain variations that may affect image quality such as external illumination. The SIFT feature detector was required in our work because we are only interested in 19 x-coordinate and y-coordinate points (distinct feature key points). Such points are defined as resultant maxima and minima of Difference of Gaussians (DoG) applied to smoothed and re-sampled images [18]. In implementation, our SIFT detector is controlled primarily by two parameters, the peak threshold and the non-edge threshold. The peak threshold filters peaks of the scale-space that are too little in absolute value. As the peak threshold increases, fewer features are obtained. The edge threshold removes peaks of the DoG scale-space whose curvature is miniscule. As the edge threshold increases, more features are obtained. After applying this detector, for each key feature location, the x-axis and y-axis coordinates are extracted [34].


                                       SURF: The SURF method is very similar to SIFT. However, SURF is only a scale and rotation-invariant interest point detector and descriptor, offering a compromise between feature complexity and robustness for commonly occurring deformations [1]. The use of integral images allows for fast implementation of box type convolution filters. SURF aims to find salient regions in near constant time its use of integral images and box filters. When features are detected using the SURF algorithm, there is no need to iteratively apply the same filter to the output of a previously filtered level's images. After obtaining x-coordinate and y-coordinate, we move on to applying our local (e.g. fiducial point) matching algorithm using the detected points generated by SURF.


                              Face recognition systems: Both commercial software and academic software were employed to perform the face recognition experiments: 1) Commercial software Identity Tools G8 provided by L1 Systems; 2) standard training-based face recognition methods provided by the CSU Face Identification Evaluation System [3], including Principal Component Analysis (PCA) [29,33,11], a combined Principal Component Analysis and Linear Discriminant Analysis algorithm (PCA+LDA) [2], the Bayesian Intrapersonal/Extra-personal Classifier (BIC) using either the maximum likelihood (ML) or the maximum a posteriori (MAP) hypothesis [31] and (3) local binary pattern (LBP) method [26]. PCA rotates feature vectors from large, highly correlated sub-space to a small subspace which has no sample covariance between features. PCA is also used as part of an LDA algorithmic approach in order to reduce dimensionality of the feature vectors, and then the training data is further used to reduce dimensionality in a way that preserves distinguishing features. Both PCA and LDA algorithms may use distance metrics such as Euclidean Distance (EU), which result in the ordinary or standard distance between two feature vectors (PCA EU+LDA EU). BIC uses PCA during density estimation, in which parameters that define the Gaussian distributions, using the statistical method properties of two subspaces: one for difference images that belong to the intrapersonal class and another for difference images that belong to the extrapersonal class. The two variants of the BIC algorithm include maximum a posteriori (MAP) and maximum likelihood (ML) classifier (BMAP+BML) [3]. The LBP operator is an efficient, nonparametric, and unifying approach to traditional divergent statistical and structural models of texture analysis. For each pixel in an image, LBP produces a binary code by thresholding its value with the value of the center pixel. A histogram is created to count up occurrences of different binary patterns [26]. Cumulative match curves (CMC) comparing the proposed methodology with commercial and academic software for each respective spectrum can be seen in Fig. 7
                              
                              
                              .

The experimental scenarios investigated in this paper are the following:1) validation of skin segmentation; 2) evaluation of eye detection; 3) evaluation of global-based matching; 4) evaluation of local-based matching; 5) identification performance before and after fusion; 6) evaluation of matching using sub-facial face images; 7) evaluation of system robustness and 8) computational performance of both global-based and local-based matchers. Our entire multi-spectral database was used for the evaluation of eye detection, global-based matcher, local-based matcher, and fusion experiments. Due to the computational complexity of our system, 50 subjects are used for evaluation of matching using sub-facial face images, and 35 subjects are used to evaluate the robustness of our proposed system. For evaluation of global-based matcher, local-based matcher, fusion, and matching of sub-facial face images, we manually correct all erroneously segmented skin, as well as eye locations that are incorrectly detected.
                        
                           -
                           
                              Skin segmentation validation: The skin segmentation algorithms presented in this work have yet to be validated in literature, therefore it is important we validated the performance of the algorithms. As with most segmentation methods that utilize thresholds, over-segmentation is a major concern because the number of potential features that may be used for matching is reduced. Skin segmentation is more of a challenge in the visible spectrum due to variations in illumination. Fig. 5 gives examples of erroneous skin segmentation and correct skin segmentation results for all three spectrums.


                              Eye detection experiments: To validate the performance of our eye detection methodology per spectrum, we used the relative error measure (D
                              
                                 eye
                              ) based on the distances between the expected (true eye positions acquired by manual annotation), and the estimated eye positions [16]. First, for each eye, we compute the distance between the true (manually annotated) eye center C
                              
                                 r
                              ,
                              C
                              
                                 l
                              
                              ∈
                              ℜ
                              2; and the estimated one C
                              
                                 r
                              ,
                              C
                              
                                 l
                              
                              ∈
                              ℜ
                              2 (Fig. 6), i.e., d
                              
                                 r
                               for the right eye and d
                              
                                 l
                               for the left eye. Then, we determine the maximum distance between d
                              
                                 l
                               and d
                              
                                 r
                              , i.e., (max(d
                              
                                 l
                              ,
                              d
                              
                                 r
                              )), which is, finally, normalized by dividing it by the distance between the expected eye centers, denoted as ‖C
                              
                                 l
                              
                              −
                              C
                              
                                 r
                              ‖. The measure is shown in the following equation:
                                 
                                    (14)
                                    
                                       
                                          D
                                          eye
                                       
                                       =
                                       
                                          
                                             max
                                             
                                                
                                                   d
                                                   l
                                                
                                                
                                                   d
                                                   r
                                                
                                             
                                          
                                          
                                             
                                                
                                                   C
                                                   l
                                                
                                                −
                                                
                                                   C
                                                   r
                                                
                                             
                                          
                                       
                                       .
                                    
                                 
                              
                           

In an average human face, the distance between the inner eye corners equals the width of a single eye. Thus, a relative error of 0.25 equals a distance of half an eye width (Fig. 6(b)). In this work, a pupil is considered detected if D
                              
                                 eye
                              
                              ≤0.25, and is rejected, otherwise. For 102 subjects in the visible spectrum, we achieved an average detection performance of 95.83% (right eye detection accuracy of 95.34% and left eye detection accuracy of 96.32%). For 102 subjects in the MWIR spectrum, we received an average detection performance of 79.26% (right eye detection accuracy of 80.63% and left eye detection accuracy of 77.94%). For 101 subjects in the LWIR spectrum, we received an average detection performance of 70.79% (right eye detection accuracy of 70.04% and left eye detection accuracy of 71.53%). Erroneously detected eye positions are rectified manually prior to forthcoming experimentation.


                              Global identification experiments: In this experiment, we utilize our entire multi-spectral database to analyze the performance of our feature segmentation algorithm in combination with our global-feature matcher (assuming no errors in pre-processing). We report the experimental results as CMC curves at the score level, for each spectrum separately. For 102 subjects in the visible spectrum, we achieved a rank-1 accuracy of 98.95%. For 102 subjects in the MWIR spectrum, we received a rank-1 accuracy of 94.21%. For 101 subjects in the LWIR spectrum, we achieved a rank-1 accuracy of 94.21%.


                              Local identification experiments: In this experiment, we utilize our entire multi-spectral database to investigate the performance of our feature segmentation algorithm in combination with our local-feature matcher (assuming no errors in pre-processing). We report the experimental results as CMC curves at the score level, for each spectrum individually. For 102 subjects in the visible spectrum, we achieved a rank-1 accuracy of 98.95%. For 102 subjects in the MWIR spectrum, we received a rank-1 accuracy of 99.47%. For 101 subjects in the LWIR spectrum, we achieved a rank-1 accuracy of 93.10%.


                              Fusion identification experiments: For our entire visible, MWIR, and LWIR datasets, our two matchers achieve a rank-1 accuracy of 99.47%, 100%, and 99.43% respectively, when fused at the score level (assuming no errors in pre-processing). It is evident from this experiment that fusion results in the best matching accuracy for our entire multi-spectral database. It is important to note that in the subsequent experiment, which uses a subset of our database (50 subjects per spectrum) in addition to SIFT and SURF, score-sum with min–max normalization does not always outperform each independent matcher.


                              Matching using sub-facial face images: In this experiment, we use a subset of our entire multi-spectral face image database (50 subjects per spectrum). The automatically detected points extracted from holistic face images are partitioned depending on specific face regions, i.e. left eye, right eye, nose, chin, both eye regions, both eye and nose regions, both eye and nose regions along with the chin regions, and finally all disjoint points that are not in any aforementioned regions, but still part of the set (see Fig. 8
                              ). Each sub-region (e.g. left or right eye, nose and chin) is extracted using a 70×70 window around user-specific selected points centered on the left eye, right eye, nose, and chin respectively (see Fig. 8). Our automatic feature extraction method is evaluated against automatic point detection methods (SIFT and SURF), for all parts of the face. The results are summarized in Tables 1, 2 and 3
                              
                              
                               for each spectrum respectively. Fig. 9
                               shows each sub-region extracted for the global matcher and Fig. 10
                               shows minutia points detected per sub-facial region for the local matcher.


                              Robustness experiments: In this experiment, we use a subset of our entire multi-spectral face image database (35 subjects per spectrum). Two separate sub-experiments are conducted, where we evaluate the sensitivity of our system (including errors from segmentation and eye detection) as well as the application of photometric normalization technique, contrast limited adaptive histogram equalization (CLAHE) [28]. The results are summarized in Table 4
                               for each sub-experiment and spectrum.


                              Computational performance: For a subset (10 subjects per spectrum) used in matching holistic faces across each spectrum, we provide a rough order of magnitude in the time it takes for 1:1 match comparison between subjects using our global and local matchers. For our global matcher, the average time (across all spectrums) required for one to one matching is about .35s. However, of our local matcher, the average time (across all spectrums) required for one to one matching is about 10.01s, for an average of 92 points. Matlab 7.12 version 2011a was used to carry out central tendencies in performance, specifically the extraction of fiducial points and matching of features. A 64-bit Windows 7 OS was run on a 3.06-GHz CPU and 12-GB ram PC. The time it takes for the matching of fiducial points could minimize the use of parallel computing or object-oriented programming.

@&#CONCLUSIONS AND FUTURE WORK@&#

In this paper, we investigated the use of extracted face based features composed of veins, edges, wrinkles, and perimeter lines, using two different matchers for face recognition. The experiments were carried out across the visible band and passive (MWIR and LWIR) bands for canonized faces. For the global matcher, the best performance was achieved in the MWIR spectrum with 99.47% rank-1 accuracy. For the local matcher, the best performance was achieved in the visible spectrum with 98.95% rank-1 accuracy. The number of subjects our system incorrectly identifies can be decreased, and overall performance increased with the addition of samples to the image gallery. Although the global matcher is computationally efficient, the local matcher outperforms the global matcher across the visible and MWIR spectrums. When matchers are fused, rank-1 accuracy of at least 99.43% is achieved across all spectrums.

A subset of each dataset was used for experimenting on different methods for feature point detection, along with matching individual parts of the face, such as the eye, nose, and chin regions. A very interesting result from our study was in the case where our proposed methodology was applied to the disjointed face sub-region (i.e. the region of the face excluding the eyes, nose, and chin regions) in the MWIR spectrum. In this case we achieved at least a 93% rank-1 accuracy using the global matcher and 99% rank-1 accuracy using the local matcher. Another interesting result in the MWIR spectrum was that the mouth region could be matched using our global matcher with 94% confidence. These partial face matching results suggest that the features extracted around the cheek, forehead and chin regions are unique per individual, particularly in the MWIR spectrum. It appears that our global matcher outperforms our local matcher in matching sub-facial regions for each spectrum except the MWIR spectrum. Fusing matchers for sub-facial regions does not always increase the recognition performance, but performance is always increased for holistic face matching when matchers are fused. For each subset of our databases, it took less time to match subjects using the global matcher in comparison to the local matcher. Through our experiments and baseline comparison, we can conclude that the FR in LWIR spectrum doesn't perform as well as FR in the visible spectrum. However, a reason for the decrease in performance may be due to the lower resolution images in our LWIR dataset.

Skin segmentation remains a difficult task, particularly in the visible spectrum due to variations in illumination, therefore requiring a number of different thresholds for the task. However, because the visible spectrum is a traditional research area in biometrics, pre-processing tasks such as eye detection hold an advantage in the visible spectrum. Eye detection in the passive infrared band is an area that still needs some attention. Per our robustness experiments, photometric normalization techniques such as CLAHE appear to offer little to no advantage for our local matcher, however CLAHE may be advantageous for our global matcher. Due to the use of physiological and geometric features for our matchers, eye detection accuracy is a crucial to the sensitivity of our automated system. An advantage to our proposed approach is that it is fully automated and does not require a design and the usage of a training set. This saves a lot of computational time and effort in processing. Another observation is that our pre-processing step reduces image storage size for a given subject from as large as 3MB after video extraction, down to 1kB, which is beneficial when storing a large number of subjects. Despite the fact that we only dealt with frontal face images, the design of our pre-processing step turned to be very important in achieving high recognition rates. Furthermore, although users could be identified using parts of the face instead of the whole face, face sub-regions should be used in conjunction with other biometric approaches to boost performance. This needs further investigation, and if such an observation holds in large datasets, it can assist current FR systems in cases of facial occlusion or disguise. Following that, the application of our proposed methodology to datasets consisting of subjects with facial occlusion or disguise intending to counterfeit a biometric system would provide further insights in this research field. Also, to further validate the efficiency of our proposed FR approach and matchers, other datasets such as that of the University of Notre Dame [23,27] should also be considered. Our results suggest that our algorithm is robust to minor pose and expression variations, assuming that the face is normalized accurately and similar poses are matched to one another. In the context of a biometric system, we need to be able to estimate a pose angle for the subject's face in order to constrain matches to similar poses. A quick solution that alleviates this challenge is the use of different poses as samples for each respective subject in both gallery and probe. Nevertheless, this would result in an increase of the computational complexity of our proposed system that could be avoided with prior knowledge of the pose angle. The problem of automated pose estimation and side-profile FR across the visible, MWIR and LWIR spectrums will be part of our future work. Finally, another area that merits further investigation is the use of object-oriented code and parallel computing to speed up local fiducial point extraction and matching.

@&#ACKNOWLEDGMENTS@&#

This work is sponsored in part by a grant from the Office of Naval Research (ONR), contract N00014-09-C-0495. We would like to acknowledge Dr. L. Hornak for his valuable input during the initial phase of this work. Special thanks to Cameron Whitelam and Zain Jafri for their valuable input and help in eye detection experiments, as well as all WVU faculty and students that contributed in various parts of this research effort.

@&#REFERENCES@&#

