@&#MAIN-TITLE@&#Classification of multiple sclerosis lesions using adaptive dictionary learning

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           We classify multiple sclerosis lesions using adaptive dictionary learning.


                        
                        
                           
                           Separate dictionaries are learned for the healthy brain tissues and lesion classes.


                        
                        
                           
                           Tissue-specific information is incorporated by learning dictionaries for each tissue.


                        
                        
                           
                           Adapting dictionary sizes based on complexity of data gives better classification.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Sparse representations

Adaptive dictionary learning

Computer aided diagnosis

Magnetic resonance imaging

@&#ABSTRACT@&#


               
               
                  This paper presents a sparse representation and an adaptive dictionary learning based method for automated classification of multiple sclerosis (MS) lesions in magnetic resonance (MR) images. Manual delineation of MS lesions is a time-consuming task, requiring neuroradiology experts to analyze huge volume of MR data. This, in addition to the high intra- and inter-observer variability necessitates the requirement of automated MS lesion classification methods. Among many image representation models and classification methods that can be used for such purpose, we investigate the use of sparse modeling. In the recent years, sparse representation has evolved as a tool in modeling data using a few basis elements of an over-complete dictionary and has found applications in many image processing tasks including classification. We propose a supervised classification approach by learning dictionaries specific to the lesions and individual healthy brain tissues, which include white matter (WM), gray matter (GM) and cerebrospinal fluid (CSF). The size of the dictionaries learned for each class plays a major role in data representation but it is an even more crucial element in the case of competitive classification. Our approach adapts the size of the dictionary for each class, depending on the complexity of the underlying data. The algorithm is validated using 52 multi-sequence MR images acquired from 13 MS patients. The results demonstrate the effectiveness of our approach in MS lesion classification.
               
            

@&#INTRODUCTION@&#

Multiple sclerosis is a chronic, autoimmune disease of the central nervous system (CNS). It is characterized by the structural damages of axons and their myelin sheathes. Magnetic resonance imaging is the best paraclinical method for the diagnosis of MS, assessment of disease progression and treatment efficacy [1,2]. These images are analyzed to find the number and spatial patterns of the lesions, appearance of new lesions and the total lesion load, which are key parameters in the current MS diagnostic setup. Manual segmentation of MS lesions, however, is a laborious and time consuming task, pertaining to the requirement of analyzing large number of MR images. Furthermore, it is prone to high intra- and inter-expert variability. Several MS lesion segmentation methods have been proposed over the last decades, with an objective of handling large variety of MR data and which can provide results that correlate well with expert analysis. These methods are based on supervised or unsupervised approach and use different image features and classification strategies to model lesions [3,4].

Over the last few years, sparse representation has evolved as a model to represent an important variety of natural signals using a linear combination of a few atoms of an over-complete dictionary. Dictionary learning, a particular sparse signal model, aims at learning a non-parametric dictionary from the underlying data. The representation of data in such a manner has led to the use of sparse representations and dictionary learning in many image processing applications such as image restoration [5,6], inpainting [7], face recognition and texture classification [8,9].

The ability of sparse representations to approximate high-dimensional images using a few representative signals in a low-dimensional subspace and the development of efficient sparse coding and dictionary learning techniques offer a great advantage in medical image analysis. Recent publications have demonstrated the effectiveness of sparse representation techniques in medical applications such as shape modelling [10], constructing a structural brain network model [11] and predicting cognitive data from medical images [12]. In addition, the dictionary learning framework has been used in deformable segmentation [13], image fusion [14], super-resolution analysis [15], denoising [16,17], deconvolution of low-dose computed tomography perfusion [18,19] and low-dose blood-brain barrier permeability quantification [20]. In each of these applications, the dictionaries are learned from the underlying data so that they are better suited for representation of the signal of interest. On the other hand, the discriminative dictionary learning approaches proposed for image segmentation focus on learning dictionaries which are representative as well as discriminative [21,22]. In this paper, we propose a novel algorithm, for classification of multiple sclerosis lesions, which incorporates discrimination in the dictionary learning framework by adjusting the size of the dictionaries according to the complexity of the underling data. Previous works [23,24] have also reported the effects of the dictionary size in image classification. We investigate this in the particular case of classification.

In the past, Weiss et al. [25] proposed dictionary learning based MS lesion segmentation method by learning a single dictionary with the help of healthy brain tissue and lesion patches. The lesions are treated as outliers and lead to a higher reconstruction error when decomposed using this dictionary. There are several shortcomings in this method. The method uses only FLAIR MR images for analysis of clinical data. However, MS lesions appear in different intensity patterns in various MR sequences, which include T1- (T1-w MPRAGE), T2- (T2-w) and Proton Density-weighted (PD-w). The complementary information in these MR images can further assist in classifying MS lesions. We, therefore, build our analysis using multi-channel MR data.

The former method also uses an unsupervised approach and it was observed that one of the crucial parameters used in this approach is the threshold on error map. This parameters drives the segmentation results and is not easy to tune. Furthermore, it could lead to worse segmentation results for small errors in the brain extraction procedure. We suggest a solution to this problem by proposing a fully automatic supervised classification method that eliminates this parameter. As outlined in many classification approaches using dictionary learning, we learn class specific dictionaries for the healthy brain tissues and the lesions that promote the sparse representation of the healthy and lesion patches, respectively. The lesion patches are well adapted to their own class dictionary, as opposed to the other. Thus, we can use the reconstruction error derived from the sparse decomposition of the test patch on to these dictionaries for obtaining the classification.

There exist several MS lesion segmentation methods that use tissue segmentation to help segment the lesions [26]. We can thus further enrich our model by taking into account the tissue specific information and learning dictionaries specific to different tissue types, such as white matter (WM), gray matter (GM) and cerebrospinal fluid (CSF), as opposed to learning a single dictionary for healthy tissue patches. We explore the fact that various tissues as well as lesions appear in different intensity patterns in distinct MR modality images. For example, WM appears as the brightest tissue in T1-weighted image, but the darkest in T2-weighted images. Therefore, learning class specific dictionaries for individual tissues should further discriminate between lesion and non-lesion classes.

The dictionaries learned for each class are aimed at better representation of an individual class. However, if there exists differences in the data-complexity between classes, the relative under- or over-representation of either class will lead to worse classification. One idea for better classification could be to learn the dictionaries with adaptive sizes, in order to take into account the data variability between different classes. Thus, in addition to the dictionary learning strategy mentioned above, we also investigate the effect of modifying the dictionary sizes, leading to the proposition of adaptive dictionary learning. The basic idea is to learn the class specific dictionaries which are better adapted to the data and also complexity of the data.

The main contributions of this paper can be outlined as follows: (1) Supervised classification approach is developed using multi-channel MR data by learning dictionaries for the healthy brain tissues and the lesion classes. (2) Tissue-specific information is incorporated by learning dictionaries specific to each tissue class as opposed to learning a single dictionary for representation of the healthy brain tissue class. (3) The dictionary sizes are adapted according to the complexity of the underlying data so that the dictionaries are better suited for representation of each class data as well as classification of MS lesions.

This paper is organized as follows. We first describe sparse coding and dictionary learning in Section 2. The materials and methods are explained in Section 3, followed by results and discussions in Section 4, and conclusion in Section 5.

Sparse representation of the data allows decomposition of signals into linear combination of few basis elements in the over-complete dictionary. Consider a signal 
                        
                           
                              x
                           
                        
                        ∈
                        
                           
                              
                                 ℝ
                              
                           
                           N
                        
                      and an over-complete dictionary 
                        
                           
                              D
                           
                        
                        ∈
                        
                           
                              
                                 ℝ
                              
                           
                           
                              N
                              ×
                              K
                           
                        
                     . The sparse coding problem can be stated as 
                        
                           min
                           
                              
                                 a
                              
                           
                        
                        
                           
                              
                                 
                                    
                                       a
                                    
                                 
                              
                           
                           0
                        
                      s.t. x
                     =
                     Da or 
                        
                           
                              
                                 
                                    
                                       
                                          x
                                       
                                    
                                    −
                                    
                                       
                                          Da
                                       
                                    
                                 
                              
                           
                           2
                           2
                        
                      ≤ɛ, where 
                        
                           
                              
                                 
                                    
                                       a
                                    
                                 
                              
                           
                           0
                        
                      is l
                     0 norm of the sparse coefficient vector 
                        
                           
                              a
                           
                        
                        ∈
                        
                           
                              
                                 ℝ
                              
                           
                           K
                        
                      and ɛ is error in the signal representation. The efficient solvers for sparse coding include matching pursuit, orthogonal matching pursuit and basis pursuit, where the later solves the convex approximation of the problem above by replacing l
                     0 norm with l
                     1 norm, which also results in a sparse solution [27–29]. The sparse coding problem can be given by


                     
                        
                           (1)
                           
                              
                                 min
                                 
                                    
                                       a
                                    
                                 
                              
                              
                                 
                                    
                                       
                                          
                                             
                                                x
                                             
                                          
                                          −
                                          
                                             
                                                Da
                                             
                                          
                                       
                                    
                                 
                                 2
                                 2
                              
                              +
                              λ
                              
                                 
                                    
                                       
                                          
                                             a
                                          
                                       
                                    
                                 
                                 1
                              
                           
                        
                     where λ is called sparsity induced regularizer and balances the trade-off between the reconstruction error and the sparsity of the coefficient vector a.

The fixed dictionaries like wavelets can be efficient, but over the past years, the dictionary learning from underlying data has produced exciting results with greater data adaptability. For a set of signals 
                        
                           
                              
                                 
                                    
                                       
                                          
                                             x
                                          
                                       
                                       i
                                    
                                 
                              
                           
                           
                              i
                              =
                              1
                              ,
                              …
                              ,
                              m
                           
                        
                     , the dictionary learning problem is to find D such that each signal can be represented by sparse linear combination of its atoms. This can be stated as the following optimization problem


                     
                        
                           (2)
                           
                              
                                 min
                                 
                                    
                                       
                                          D
                                       
                                    
                                    ,
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         a
                                                      
                                                   
                                                   i
                                                
                                             
                                          
                                       
                                       
                                          i
                                          =
                                          1
                                          ,
                                          .
                                          .
                                          ,
                                          m
                                       
                                    
                                 
                              
                              
                                 ∑
                                 
                                    i
                                    =
                                    1
                                 
                                 m
                              
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   x
                                                
                                             
                                             i
                                          
                                          −
                                          
                                             
                                                
                                                   
                                                      D
                                                      a
                                                   
                                                
                                             
                                             i
                                          
                                       
                                    
                                 
                                 2
                                 2
                              
                              +
                              λ
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   a
                                                
                                             
                                             i
                                          
                                       
                                    
                                 
                                 1
                              
                           
                        
                     
                  

The optimization is carried out as an iterative two-step process: (i) sparse coding with a fixed D, and (ii) the dictionary update with a fixed a.

Among the methods available in the literature for learning the dictionary, KSVD, MOD and online dictionary learning are widely used algorithms [30–32].

The proposed approach involved MR dataset of 14 MS patients acquired via Verio 3T Siemens scanner. T1-w MPRAGE, T2-w, PD-w and FLAIR modalities were chosen for the analysis. The volume size for T1-w MPRAGE and FLAIR was 256×256×160 and voxel size was 1×1×1mm3. For T2-w and PD-w, the volume size was 256×256×44 and voxel size was 1×1×3mm3. Annotations of the lesions were carried out on T2-w volume by an expert neuroradiologist. These manual segmentation images are referred to as ground truth lesion masks.

The overview of the method proposed is shown in Fig. 1
                        . MR images for all patients are first preprocessed for noise-reduction and the elimination of extracranial brain tissues. The images are then registered into the same space. We represent image volumes as patches of a predefined size and normalize these extracted patches. This is followed by labeling patches in two ways: (i) Healthy brain tissue patches and lesion patches, using manual segmentation images and (ii) WM, GM, CSF or lesion patches, with the help of manual lesion segmentation and tissue segmentation images. The patches are then divided into the training and test dataset. For various classification strategies, we learn the dictionaries, using training data, in different configurations as follows: a single dictionary, two separate dictionaries for the healthy and lesion classes, or the class specific dictionaries for the lesions and each healthy brain tissue – WM, GM, CSF. For the last two approaches, we also study the role of the dictionary size in the classification. Finally, for a given test subject, we developed a reconstruction error based patch-classification method, which is followed by the voxel-wise classification. The following subsections briefly describe these steps.

The noise introduced during MR acquisition is removed using non-local means and intensity inhomogenity (IIH) correction [33,34]. To ensure the spatial correspondence, the images are registered with respect to T1-w MPRAGE volume [35] and are processed further to extract the intra-cranial mask [36]. We limit our further analysis to this brain region.

For local image analysis in the dictionary learning framework, the images are divided into the overlapping patches. Each patch is then represented as a signal in the dictionary learning process. We follow this patch-based approach and divide the whole intracranial MR volume for each patient into 3-D patches, with a patch around every 2 voxels in each direction. The individual image patches of each MR modality are then flattened to form a vector and are concatenated together. The patches so obtained are normalized for a unit l
                        1 norm.

Next step is to label the normalized patches obtained from every patient. We label them in two different ways for the experiments to be preformed next. Firstly, the patches are labeled as belonging to either healthy or lesion class, using the manual segmentation image. If the number of lesion voxels in the corresponding image block of the manual segmentation image exceeds a pre-defined threshold T
                        
                           L
                        , we assign this patch to the lesion class. Otherwise, it is labeled as a healthy patch. The image patches obtained in this manner form the dataset for the classification approaches which use a single dictionary or two class specific dictionaries. For other classification methods, the patches are labeled as either WM, GM, CSF or lesion class. We use the same rule, as explained above, to label the patch to the lesion class. In addition, the patch is now assigned to either WM, GM or CSF class, depending on the maximum number of voxels that belong to corresponding class in the brain tissue segmentation image obtained using Statistical Parametric Mapping (SPM) [37].

The labeled image patches are then divided into training and test data, and the experiments are performed by following Leave-One-Subject-Out-Cross-Validation (LOSOCV).

Let n be the number of voxels per patch. For each class c, we write patches as vectors 
                           
                              
                                 
                                    x
                                 
                              
                              i
                              c
                           
                           ∈
                           
                              R
                              n
                           
                        . Learning an over-complete dictionary D
                        
                           c
                        
                        ∈
                        R
                        
                           n×k
                         that is adapted to m patches, with sparsity parameter λ, is addressed by solving the optimization problem, similar to Eq. (2).


                        
                           
                              (3)
                              
                                 
                                    min
                                    
                                       
                                          
                                             
                                                D
                                             
                                          
                                          c
                                       
                                       ,
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            a
                                                         
                                                      
                                                      i
                                                      c
                                                   
                                                
                                             
                                          
                                          
                                             i
                                             =
                                             1
                                             ,
                                             .
                                             .
                                             ,
                                             m
                                          
                                       
                                    
                                 
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    m
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      x
                                                   
                                                
                                                i
                                                c
                                             
                                             −
                                             
                                                
                                                   
                                                      D
                                                   
                                                
                                                c
                                             
                                             
                                                
                                                   
                                                      a
                                                   
                                                
                                                i
                                                c
                                             
                                          
                                       
                                    
                                    2
                                    2
                                 
                                 +
                                 λ
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      a
                                                   
                                                
                                                i
                                                c
                                             
                                          
                                       
                                    
                                    1
                                 
                              
                           
                        
                     

The subsections below detail the different strategies adopted while learning these dictionaries and the scheme of patch based classification. In every method, we obtain the sparse codes for the test patches using Eq. (1), knowing the dictionary D
                        
                           c
                         for the class c.

In the context of MS lesion classification, the simplest idea, similar to [25], could be to use a single dictionary learned from the healthy and lesion class patches. Such dictionary is mainly representative of the healthy brain image patches, based on the fact that the number of lesion patches is very small as compared to that of the healthy class.

As lesions are outliers with respect to the healthy brain intensities, the decomposition of the lesion patch using such dictionary would result in a higher representation error than that for the healthy tissue patches. Thus, for a given test patch, we calculate the sparse coefficients with appropriately chosen sparse penalty factor and the reconstruction error. The test patch with representation error greater than chosen threshold would be classified as a lesion patch. For calculation of the threshold, we used the histogram of the error map, as proposed by the authors [25].

In this method, we learn the class specific dictionaries D
                           
                              c
                            of the same size, for the healthy (c
                           =1) and lesion (c
                           =2) class. The dictionaries learned in this manner are better suited to represent the corresponding class data. The decomposition of the test patch using other class dictionary would give rise to a higher representation error.

For a given test patch yi
                           , the classification is performed by calculating the sparse coefficients 
                              
                                 
                                    
                                       a
                                    
                                 
                                 i
                                 c
                              
                            for each class and the test patch is then assigned to the class with a minimum representation error.


                           
                              
                                 (4)
                                 
                                    
                                       c
                                       pred
                                    
                                    =
                                    
                                       argmin
                                       c
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            y
                                                            i
                                                         
                                                      
                                                   
                                                
                                                −
                                                
                                                   
                                                      
                                                         D
                                                      
                                                   
                                                   c
                                                
                                                
                                                   
                                                      
                                                         a
                                                      
                                                   
                                                   i
                                                   c
                                                
                                             
                                          
                                       
                                       2
                                       2
                                    
                                    .
                                 
                              
                           
                        

The dictionaries learned using above-mentioned approach do not take into account the data variability between two classes. The size of the dictionary plays a major role in data representation. The healthy class data is associated with more variability as compared to the lesion class, because it represents complex anatomical structures such as white matter, grey matter and cerebrospinal fluid. The number of training samples for the healthy class also outnumber the lesion class training samples. To account for more variability and the number of training samples, we allow larger dictionary size for the healthy class and study its effect on MS lesion classification.

As explained before, the healthy brain tissues contain anatomically different regions such as WM, GM and CSF. The fact that every tissue, WM, GM and CSF, appears in different intensity pattern in each MR modality, using a single dictionary for representing the healthy brain tissues might not be as effective as learning separate dictionaries for each tissue. Adding tissue specific information in the dictionaries used for the classification would enhance the prior knowledge in the learning step, thus highlighting the differences between individual tissues and also improving the lesion classification.

After learning class specific dictionaries for WM, GM, CSF and lesion, we perform classification based on reconstruction error in a similar manner, as mentioned in Section 3.5.2. Each dictionary is representative of its own class and the reconstruction of the test data using true class dictionary would give a minimum reconstruction error.

Here, we experiment with different dictionary sizes for WM, GM, CSF and lesion classes, for the similar reasons mentioned in Section 3.5.3.

As already stated, we classify the patches centered around every 2 voxels in each direction. For voxel-wise classification, we assign each voxel to either of the classes by using majority voting. The voxel is assigned to a class using majority votes of all patches that contain the voxel.

Finally, in the context of lesion classification, we record the number of voxels that belong to True Positives (TP), False Negatives (FN) or False Positives (FP), and calculate percentage sensitivity (SEN)=(TP
                        ×100)/(TP
                        +
                        FN), percentage Positive Predictive Value (PPV)=(TP
                        ×100)/(TP
                        +
                        FP) and percentage dice-score (Dice)=(2×
                        TP
                        ×100)/(2×
                        TP
                        +
                        FP
                        +
                        FN).

@&#EXPERIMENTS AND RESULTS@&#

We implemented our method using MATLAB and Python. Neuroimaging softwares N4ITK and Brain Extraction Tool (BET) were used for IIH correction and brain extraction, respectively [33,36]. The brain tissue segmentation is obtained using Statistical Parametric Mapping (SPM) [37]. The dictionary learning and sparse coding is performed with the use of SPArse Modeling Software (SPAMS) package [32].

For labeling patches, we used the threshold T
                     
                        L
                     
                     =6, as mentioned in Section 3.5. For patch size of 5×5×5, the number of lesion patches for each patient varied from 1K to 30K, depending on the lesion load for the corresponding patient, whereas the average number of patches for the healthy brain tissue class was 1.5×106. For WM, GM and CSF classes, the numbers of patches obtained per patient were 50K, 90K and 30K, respectively. The classification was performed using LOSOCV and different parameters were tested. It was found that the patch size of 5×5×5 and the sparsity parameter λ
                     =0.95 were optimal choices. Changing λ in steps of 0.5 from 0.5 to 0.95 did not influence the results much and the value of 0.95 provided good results for all patients. All these experiments were performed on 2.5GHz, 120GB RAM Xeon processor. The dictionaries of sizes ranging from 500 to 5000, were learned from the training data and the best results, in terms of both sensitivity and PPV, were selected. For the dictionary sizes varying from 500 to 5000, the dictionary learning step required 5min to 3h, where as the classification step took 4–38min, respectively. We used these parameters for validation of classification approaches using multi-channel MR data. We, however, excluded one patient with strong MR artifacts from this analysis.

The results of voxel-wise classification, obtained using all the methods described above, are shown in Table 1
                     . Method (a) indicates classification obtained using single dictionary learned with the help of both healthy brain tissue and lesion patches. Here, we chose the sparse penalty factor λ
                     =0.85 in the sparse coding step and performed the classification for various threshold values on the histogram of error map, as explained in Section 3.5.1. The threshold, which produced the best voxel-wise classification results in terms of both sensitivity and PPV, was then selected and the classification results were reported. It can be observed from very low PPV and dice-scores that this method suffers with a very large number of false positive detections.

In the second experiment, we used the class specific dictionaries of same size, for the healthy and the lesion class. As indicated by method (b), the classification obtained using dictionaries with 5000 atoms each resulted in high sensitivity but PPV and dice-scores were still low. One possible reason behind these low values is that there exists a difference in variability of the data for two classes. Considering more variability associated with the healthy class data, we then used different dictionary sizes, 5000 for the healthy class and 1000 for the lesion class. As shown in method (c), this drastically reduced FP, improving PPV and dice-scores, but also decreased the sensitivity.

We further enriched this model by learning separate dictionaries for each healthy brain tissue – WM, GM, CSF, in addition to the dictionary learned for the lesion class. Using four such dictionaries with 5000 atoms each, it can be observed that a better compromise between sensitivity and PPV is achieved, as compared to methods (b) and (c) described above. This is shown by method (d). Finally, the classification using four dictionaries of different sizes, 4000 each for WM, GM and CSF classes, and 2000 for lesion class, was obtained. This reduced the mean sensitivity but improved both the mean PPV and the mean dice-score, as compared to method (d) and is indicated by method (e) in Table 1.

The methods (c) and (e), which consider the inter-class data variability and use different dictionary sizes in classification, offer a better compromise between sensitivity and PPV, as compared to their counterpart methods (b) and (d), which use the same dictionary size for all classes. Between methods (c) and (e), each employing either two or four dictionaries, respectively, the later method performs better than the former with a higher mean sensitivity, PPV and dice-score. Their comparison also shows a significant difference in PPV and dice-scores, with respective p-values of 0.0008 and 0.003. This confirms that the classification improves using dictionaries for each brain tissue.

To investigate the effect of dictionary size on the performance of classification, we performed the experiments using methods (d) and (e) that use three separate dictionaries for the healthy brain tissues and one for the lesion class. Table 2
                         summarizes the results of classification. For method (d), which uses the same dictionary size for all classes, the results along the diagonal of the table from top-left to bottom-right show that the sensitivity and PPV increase until the dictionary size is increased to 2000. The possible reason for this is that the dictionaries capture more details with the increase in their size. However, the sensitivity reduces or remains constant thereafter, possibly because of the over-fitting occurring in either class. Excluding values along the diagonal mentioned above, all other entries in the table indicate the sensitivity and PPV values obtained with method (e), which uses different dictionary sizes for tissues and the lesion class. By referring to values in the columns from a single row, which suggests using a constant dictionary size for each tissue while varying the dictionary size of lesion class from 500 to 5000, we can observe that sensitivity keeps increasing but PPV value reduces, resulting in false positive detections. On the other hand, if we fix the dictionary size for the lesion class and increase the dictionary size for the tissues, PPV increases but sensitivity reduces, resulting in under-detection. Very low PPV scores above-diagonal from top-left to bottom-right suggest that the lesion dictionary over-fits the data corresponding to the lesion class, with the use of higher dictionary size for the lesion class than that for the tissue classes. The best results, for both sensitivity and PPV together, are obtained for the dictionary size of 4000 for each tissue class and 2000 for the lesion class. It can also be observed that it is the relative dictionary size that drives the classification and is more important than just the absolute dictionary size for each class.

It is crucial to adapt the size of the dictionaries to better control the classification. For such purpose, we analyzed the data using Principal Component Analysis (PCA), which gives an estimate of the intrinsic dimensionality of the data. Fig. 2
                         shows the cumulative variance explained by the Eigen-vectors of different classes such as WM, GM, CSF, lesion and healthy. The number of Eigen-vectors required for explaining the mentioned percentages of the total variances for each class are shown in Table 3
                        . It can be seen that, for each brain tissue – GM, WM and CSF, approximately twice as many Eigen-vectors are required for an arbitrary proportion of the percentage cumulative data variance (90%, 95% or 98%), as that required for the lesion data. As exhibited by method (e), this observation supports our adaption of dictionary size for each brain tissue twice that for the lesion dictionary. In case of method (c), which uses dictionaries for healthy and lesion classes, the experimentally observed optimal dictionary size ratio of 5 for the healthy and the lesion class was not found with PCA. Although, the factor 2 indicated by PCA still favors using a higher dictionary size for the healthy class. One reasoning behind this failure might be the inability of PCA to analyze the non-linearity in the data. The intrinsic dimensionality estimation for this highly non-linear data could be further point of investigation.

We are aware that we do not have a very large population for training. To investigate if the size of the training data has any effect on the classification results, we incorporated longitudinal database into our analysis. MR volumes are acquired for each patient, at time points M0, M3 and M6, with an interval of three months. As the lesions evolve over the course of time, some lesions might disappear partially or fully, and there might be some newly appearing lesions. It is therefore fair to consider that each dataset at consecutive time point would result in adding different lesion patterns in the training procedure. Thus, we modified the training data, for each patient, in two ways: (1) Data at time-points M0 and M3, with 24 datasets and (2) Data at time-points M0, M3 and M6, with 36 datasets. However, the lesion classification experiments for the same test subjects, as in previous experiments, using class specific dictionaries with 5000 and 1000 atoms for the healthy and lesion class, respectively, did not show any significant improvement in the sensitivity and PPV. This suggests that the size of the population for training the dictionaries is not a critical issue for such approaches.

In Figs. 3 and 4
                        
                        , we show the voxel-wise classification results obtained using all methods discussed above. We arbitrarily selected a slice for the patients 4 and 6, as referred to in Table 1. It can be seen from Fig. 3F that method (a) suffers with a large number of FP. The over-detections are reduced in methods (b) and (d), which use dictionaries of the same size for each class. This is indicated in Fig. 3G and I, respectively. Methods (c) and (e) further improve the classification, as shown in Fig. 3H and J, by employing the dictionaries of adapted sizes. However, the 2-class method (c) has many FN. As shown by method (e), including tissue specific information in such adaptive dictionary learning based approach results in significant improvement in the lesion classification with reduction in both FP and FN. This supports our claim that the method with the tissue specific dictionaries and adapted dictionary sizes is a better choice over the 2-class methods and those using the same dictionary size for all classes.

@&#CONCLUSION@&#

To automatically classify multiple sclerosis lesions, we proposed a novel supervised approach using sparse representations and dictionary learning. The methods using class specific dictionaries outperform the classification obtained using a single dictionary, in which the lesions are modeled as outliers. Learning more specific dictionaries for each anatomical structure in the brain helps improve the classification on account of specific intensity patterns associated with each of these structures in multi-channel MR images. We have also demonstrated the effectiveness of adapting the dictionary sizes for better amplification of differences among multiple classes, hence improving the classification. If performing PCA on input data can successfully adapt the dictionary size for the classification, it is not as much efficient when the classes represent more a mixture of different tissues. Knowing the limitation of PCA to handle only linear data, future work will be to use the intrinsic dimension estimation techniques, which can better analyze complexity of the non-linear data.

@&#REFERENCES@&#

