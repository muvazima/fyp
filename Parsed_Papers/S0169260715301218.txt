@&#MAIN-TITLE@&#Classification of THz pulse signals using two-dimensional cross-correlation feature extraction and non-linear classifiers

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A 2-D cross correlation based scheme is introduced for classification of THz signals.


                        
                        
                           
                           This work establishes a general way for assessing performance of other THz datasets.


                        
                        
                           
                           The proposed approach yields better performance than the existing method.


                        
                        
                           
                           The results confirm the superiority in classification accuracy of the MLR and KNN.


                        
                        
                           
                           It advances the wider proliferation of automated THz signals across new applications.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Terahertz spectroscopy

2-D cross-correlation

Multinomial logistic regression classifier




                     k-Nearest neighbours

Support vector machine

Naïve Bayes

@&#ABSTRACT@&#


               
               
                  This work provides a performance comparison of four different machine learning classifiers: multinomial logistic regression with ridge estimators (MLR) classifier, k-nearest neighbours (KNN), support vector machine (SVM) and naïve Bayes (NB) as applied to terahertz (THz) transient time domain sequences associated with pixelated images of different powder samples. The six substances considered, although have similar optical properties, their complex insertion loss at the THz part of the spectrum is significantly different because of differences in both their frequency dependent THz extinction coefficient as well as differences in their refractive index and scattering properties. As scattering can be unquantifiable in many spectroscopic experiments, classification solely on differences in complex insertion loss can be inconclusive. The problem is addressed using two-dimensional (2-D) cross-correlations between background and sample interferograms, these ensure good noise suppression of the datasets and provide a range of statistical features that are subsequently used as inputs to the above classifiers. A cross-validation procedure is adopted to assess the performance of the classifiers. Firstly the measurements related to samples that had thicknesses of 2mm were classified, then samples at thicknesses of 4mm, and after that 3mm were classified and the success rate and consistency of each classifier was recorded. In addition, mixtures having thicknesses of 2 and 4mm as well as mixtures of 2, 3 and 4mm were presented simultaneously to all classifiers. This approach provided further cross-validation of the classification consistency of each algorithm. The results confirm the superiority in classification accuracy and robustness of the MLR (least accuracy 88.24%) and KNN (least accuracy 90.19%) algorithms which consistently outperformed the SVM (least accuracy 74.51%) and NB (least accuracy 56.86%) classifiers for the same number of feature vectors across all studies. The work establishes a general methodology for assessing the performance of other hyperspectral dataset classifiers on the basis of 2-D cross-correlations in far-infrared spectroscopy or other parts of the electromagnetic spectrum. It also advances the wider proliferation of automated THz imaging systems across new application areas e.g., biomedical imaging, industrial processing and quality control where interpretation of hyperspectral images is still under development.
               
            

@&#INTRODUCTION@&#

Over the past 20 years, terahertz (THz or T-rays) pulsed imaging has become an increasingly popular complementary imaging modality due to its ability to simultaneously acquire both spatial and spectral information at a previously inaccessible part of the electromagnetic spectrum [1]. The technique nicely complements existing methods in the XUV, UV, visible and infrared parts of the spectrum. T-rays have a number of unique characteristics, which give rise to a large number of potential applications in very diverse fields such as security, pharmaceutical quality control, medical imaging and material science [2]. In addition, owing to their low photon energy, T-rays are non-ionizing and are thus considered of not inducing damage to tissue or DNA. Therefore, they are currently considered as viable alternatives to X-rays for imaging in biomedical applications where the subject may not be irradiated by X-rays e.g., for mammograms in pregnant or lactating women. Alternative applications benefitting from this technology include retection (detection of hidden objects or substances within a package), where THz tomographic image contrast can be superior to conventional methods such as X-rays that only differentiate objects or regions in an image mainly on the basis of different sample density but have difficulties in detecting plastic objects or soft biological materials of similar density. In contrast, T-ray wavelengths can pass through dry substances (e.g. thin cardboard and plastics), as well as through non-polar, non-metallic materials and can show spectral differences due to a different extinction coefficient between samples. Concealed weapons or products contained in plastic packages and non-metallic components that are not readily detectable by other means can therefore be easily detected using THz imaging techniques. The approach is also particularly promising for the detection of specific chemical and biological agents [3,4], through chemical fingerprinting. Within a pharmaceutical setting, such systems can perform multiple functions [5] enabling the identification of drug polymorphisms [6], providing information on coating structures [7–10], enabling the identification of phase transitions in chemical compounds [11] or degree of substance crystallinity [12–15] providing opportunities for tailoring the formulations at each processing step or enabling the monitoring of physicochemical product deterioration during processing or storage [16,17]. Furthermore, the high transparency of polymer materials to THz waves enables non-destructive inspection of encapsulated substances such as drugs [18], making this imaging modality particularly useful to the pharmaceutical industry. It is therefore clear that quality control for pharmaceutical industry is therefore seen as a potentially important application area for THz imaging systems [19–22] provided reliable machine learning techniques can be integrated with the sensing equipment. The use of T-ray pulse transients for simultaneously extracting information on densities, thicknesses and number of absorber molecules per unit volume in different powder samples forms the basis for simultaneously addressing detection and classification requirements across both pharmaceutical [23–25] as well as security industries [25,26].

It is worth noting that THz imaging spectrometers excite samples with femtosecond duration pulses which are extremely broadband, where a pulse spectrum spans over a frequency range between 100GHz (such excitation is associated with a wavelength of 3mm) up to 3THz (with a corresponding wavelength of 0.1mm) and in some systems all the way up to 10THz (with a corresponding wavelength of 0.03mm). As a consequence, many experiments may also contain spectral signatures associated with measurement artefacts at the Rayleigh to Mie transition region where the excitation wavelength becomes similar to the size of the particles that need to be characterized. As a consequence, in all femtosecond pulse based THz imaging systems it is not uncommon that measurements of many powdered samples can miss out a scattering component of the THz radiation, especially at frequencies closer to the infrared part of the spectrum. Scattering can cause particularly severe problems in THz time domain spectrometry, such instruments are only reliable at measuring transmittance (by measuring attenuation), or reflection (impedance mismatch) within a well-defined aperture, at a well-defined sample–air interface and across a single plane defined perpendicularly to the direction of propagation of the THz pulse. From these measurements absorption can finally be estimated, under the provision that scattering is negligible. In the datasets chosen to be investigated in the current study, there is some unquantifiable by other means scattering component because the samples have grains of different dimensions, hence there is a problem in adopting standard processing and perform classification solely based on information associated with specific spectral features. Since THz pulse imaging is extremely broadband, there may be different degree of scattering associated with the spectral signatures across different spectral bands, this is especially true if samples are in powdered form. Such problems may further be exacerbated if the powdered sample is elliptical in shape [27]. As a result one would expect different degree of deviation of the obtained absorption results associated with complex insertion loss measurements at different spectral bands and the calculated spectral extinction coefficient may significantly deviate from its true value. Finally, contrary to continuous wave based measurement systems [28], pulse transient systems spatially focus the THz radiation dramatically so as to improve on the signal-to-noise ratio during the measurement process, this has additional adverse effects in that there are deviations in the extraction of the complex insertion loss function which requires an assumption that an angular spectrum of plane waves is incident on the sample, clearly such focusing can lead to additional systematic errors in estimating the complex insertion loss function while also exacerbates the effects of scattering as sample excitation takes place over a range of angles across the sample aperture; such angular dependency of the degree of scattering makes also collection of scattered energy difficult to perform and quantify [29]. These problems lead to a need for reassessment of what can be considered as useful features that can be meaningfully extracted in a THz imaging experiment so that an automated machine learning methodology for the classification of samples using THz imaging systems can be developed.

A further aim of the proposed approach is to preserve compatibility with other de-noising techniques. Typically, the THz pulse signals contain noise due to both systematic and random errors and thus the signal-to-noise ratios in the acquired THz spectra are low. This introduces significant problems in the analysis and interpretation of spectra as well as the classification of samples (there are collinearity issues at spectral bands where the signal to noise ratio is low, such collinearity results in spikes in the error because calculation of the complex insertion loss is based on a ratiometric process). It is therefore often the case that the acquired complex insertion loss signatures may contain limited discriminative information. One method to reduce errors due to noise is to co-average subsequent measurements for the same pixel, however this dramatically increases the time required to perform the measurement, with several images reported in the current literature being acquired over a period of several minutes or even several hours. Such approach also does not address spectral bands where the source output spectral power is low.

Although there is an extensive literature on the signal processing of THz spectra, 2-D cross-correlation techniques [30–32] have attracted less attention despite their de-noising or feature extraction potential. Such approach represents a natural extension of existing THz deconvolution approaches [33] and complements de-noising algorithms using auto-regression with exogenous inputs (ARX) and subspace approaches [34,35], or other state-of-the-art signal analysis approaches [e.g. 36,37]. It is also interesting to note that cross-correlations are extensively used in different spectral bands (XUV, UV, visible, infra-red) but are not as widespread within the THz community. By performing a 2-D cross correlation between the sample and background time domain signals, excellent de-noising is achieved while preserving any phase differences (which are associated with the dispersion of the sample) that might be present between the two signals. The obtained cross-correlogram is a nearly noise-free signal that can convey superior discriminative phase information compared to the original time domain interferogram signal [30].

In recent years, a number of methods have been proposed for feature extraction in conjunction with sample classification on the basis of THz pulsed signatures. Most recently, Yin et al. [38] used directly both the real as well as complex values associated with the Fourier Transform (FT) of the corresponding time domain signatures to perform de-noising and sample classification. Furthermore, in [39], Yin et al., established that it is possible to use specific features form the Fourier spectrum of the sample to extract T-ray feature sets for binary and multi-class classification. The general approach in that method is based on selecting specific feature vectors in the frequency-domain by taking the FT after de-convolving the measured signals with a reference pulse. Alternative feature extraction algorithms using adaptive wavelet coefficients in conjunction with ARX, ARMAX as well as subspace algorithms for signal de-embedding have also been suggested in the THz literature, confirming the merits of this approach [34,35,40]. These measurements, however, were not performed on powdered samples but on samples having uniform thickness or well controlled thickness (micro-spectroscopy using waveguides). Furthermore, in order to use information associated with the dispersion of the sample in conjunction with the molecular extinction coefficient and number of absorbers across the spectrum of the measurements, alternative classification approaches making use of the discrete wavelet transforms (DWT) in T-ray measured powder samples have also been reported [41]. The goal to further reduce the input vector of the classifier so as not to compromise its generalization ability has led to the development of a hybrid pre-processing algorithm that used Auto Regressive (AR) modelling within the wavelet decomposed sub-bands of the THz pulsed signals [24]. The work complemented previous attempts by Ferguson et al., [37] to classify powders concealed within envelopes, despite the presence of strong scattering. To our knowledge these studies and the extreme learning approach recently developed [38] are the only ones that combine advanced signal pre-processing with classification for powdered samples imaged using THz transient spectrometry. In addition, to the best of our knowledge, 2-D cross-correlation techniques have never been used for feature extraction of T-ray spectra of powdered samples. Finally, within an Analytical Chemistry context, cross-correlation techniques are not usually explored within a machine learning perspective but are mainly discussed as a viable de-noising tool or to elucidate fast transient processes observed using pump-probe techniques. This differentiates the current study as it is focused in advancing current algorithms from a machine learning perspective. Such considerations have led us to develop the proposed methodology.

A further aim of the work is also to assess the potential of combining 2-D cross-correlation at the pre-processing feature extraction step while systematically assessing its impact to the performance of different classifiers. This is achieved by focusing the investigations on the identification of several powder samples of different composition. Our goal is to demonstrate a generic feature extraction approach that fully utilizes the different characteristic features found in THz pulse signals so that may be used with minimal reformulation across different T-ray data sets. Such approach paves the way towards the development of a suitable machine learning classification algorithm that could be reliably used to identify different materials independent of their thickness on the basis of their estimated spectrally dependent extinction coefficient even in the presence of some unquantifiable scattering. Feature extraction is the most crucial step in this type of pattern recognition because the classification performance will be significantly degraded if the features are not chosen wisely [42]. A further aim is to reduce the extracted features to prevent over fitting while retaining most of the useful information residing in the original vector. In order to reduce the dimensionality of the cross-correlation sequences, it is also proposed that ten statistical features are extracted from each cross-correlation sequence. The validity of the cross-correlogram features as preferred inputs is subsequently evaluated in a systematic manner by considering four machine learning algorithms: multinomial logistic regression classifier with ridge estimators (MLR), k-nearest neighbours (KNN), support vector machine (SVM) and naïve Bayes (NB). The choice of these classifiers is based on their simplicity and effectiveness in their implementation. Investigations are performed to test both multi-class as well as binary classification of T-ray pulse transmission signals. A 10-fold cross-validation method is used for assessing the performance of the proposed methodology. This procedure divides the feature vector sets into ten approximately equal-sized distinct partitions. One partition is used for testing, whereas the other partitions are used for training the classifiers. To further improve the estimate, the procedure was repeated 10 times and all performance metrics over these runs are averaged. The average performances associated with the test data is then adopted as the preferred overall performance evaluation criterion. The investigations aim to elucidate which one of the four classifiers would consistently achieve the most reliable classification. The powder samples used in the study have similar optical properties but different composition and different complex insertion loss at the THz part of the spectrum.

The paper is organized as follows: Section 2 provides an overview of the algorithm adopted to perform the cross-correlation process, details of the statistical feature extraction process, feature aggregation and cross-validation as well as a brief outline of the methods associated with the four classifiers. This section also provides information regarding the nature of the datasets. In Section 3, the application of the 2D cross-correlation procedure to the THz datasets is discussed. The selection of optimum parameter values for the reported classifiers and the performance evaluation criteria are also discussed in this section. A performance comparison of all the classifiers is presented and discussed in Section 4. Finally Section 5 draws some conclusions and provides directions for further research.

The general classifier structure consists of four main processing blocks: computation of cross-correlation sequence, statistical feature extraction, feature aggregation and cross validation and classifier decision observation. The 2-D cross correlation technique extracts the information from the T-ray pulsed signals and acquires cross-correlation sequences from each sample class. In this study, each powder substance is considered to belong to a single class: sand (class 1), talcum (class 2), salt (class 3), powdered sugar (class 4), wheat flour (class 5), and baking soda (class 6). The sample holder (free-space equivalent of a cuvette) signal is the reference signal used for evaluating the complex insertion loss. Using the reference signal in conjunction with the other sample signal in a class, a cross-correlation sequence is computed on a pixel by pixel basis by the 2-D cross correlation. Once the characteristic features are extracted from each cross-correlation sequence associated with every class, all features are integrated forming a feature set. Following this process, cross-validation is applied to generate training and testing sets for evaluation. The detection stage identifies the several powder categories on the basis of the feature sets. Finally classifier decisions are observed.

The 2-D cross-correlation technique [30–32] is used to calculate a cross-correlation sequence (denoted by ‘CC(k,l)’) between the reference signal and any other signal belonging to a distinct class. The graphical presentation of a cross-correlation sequence is commonly known as a cross-correlogram. The 2-D cross-correlation of X (M-by-N matrix) and H (P-by-Q matrix) is a matrix CC of size (M
                           +
                           P
                           −1)×(N
                           +
                           Q
                           −1):
                              
                                 (1)
                                 
                                    
                                       C
                                       C
                                       (
                                       k
                                       ,
                                       l
                                       )
                                       =
                                       
                                          ∑
                                          
                                             m
                                             =
                                             0
                                          
                                          
                                             M
                                             −
                                             1
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                n
                                                =
                                                0
                                             
                                             
                                                N
                                                −
                                                1
                                             
                                          
                                          
                                             X
                                             (
                                             m
                                             ,
                                             n
                                             )
                                             
                                                H
                                                ¯
                                             
                                             (
                                             m
                                             −
                                             k
                                             ,
                                              
                                             n
                                             −
                                             l
                                             )
                                             ;
                                              
                                             −
                                             (
                                             P
                                             −
                                             1
                                             )
                                             ≤
                                             k
                                             ≤
                                             M
                                             −
                                             1
                                             ;
                                                
                                             −
                                             (
                                             Q
                                             −
                                             1
                                             )
                                             ≤
                                             l
                                             ≤
                                             N
                                             −
                                             1
                                          
                                       
                                    
                                 
                              
                           where X is considered as the reference signal and H is regarded as any other signal belonging to a class of T-ray pulsed signals. The bar over H denotes complex conjugation. The output matrix, CC(k,l), has negative and positive row and column indices. A negative row index corresponds to an upward shift of the rows of H. A negative column index corresponds to a leftward shift of the columns of H. A positive row index corresponds to a downward shift of the rows of H. A positive column index corresponds to a rightward shift of the columns. It is worth mentioning that if each of the signals, X and H, consist of a finite number samples S, the resultant cross-correlation sequence has 2S
                           −1 samples.

The THz transient transmission reference signal is considered as noiseless for most parts of the spectrum, so the variance in the noise when ratioing a sample with a background does not get disproportionally amplified [36]. Each powder sample is considered as belonging to a distinct class. Fig. 1
                            illustrates how a cross-correlogram is obtained from a reference signal (holder) and any of the other sample signals, on the basis of Eq. (1).

The cross-correlogram signals convey greater information than the original powder spectra of the sample and reference signals and thus have superior signal to noise ratio than the original signals. In addition, cross-correlograms contain additional information regarding the spectral coherence of the waveforms. As the cross-correlation sequences contain a large number of data points, these needs to be further compressed into a more parsimonious feature space so as not to overwhelm the classifier.

In order to reduce the dimensions of the cross-correlation sequences, this study considers ten statistical features. These are: mean, standard deviation, skewness, kurtosis, 1st quartile (Q
                           1), 3rd quartile (Q
                           3), inter-quartile range (IQR), median, maximum and minimum that are calculated from each cross-correlation sequence. This information is used to create the feature vector sets. There are several valid reasons for the considerations of these ten quantitative feature descriptors. Mean and standard deviation are particularly informative in describing a distribution [43,44]. Skewness provides information on the degree of asymmetry of the observed distribution around its mean [31]. Kurtosis provides a measure of flatness relative to a normal distribution. Q
                           1 and Q
                           3, measure how the data are distributed in the two sides of the median. IQR is the difference between Q
                           3 and Q
                           1 that is used in measuring the spread of a data set, such information can be used to exclude outliers [45,46]. Median which is associated with the observation encountered most often is also an additional valuable metric that needs to be retained for classification purposes. Maximum and minimum values are also used to describe the range of observations within the distribution. Each of the above subroutines is run for each cross-correlation sequence associated with each powder substance. All ten statistical features from each cross correlation sequence and each powder substance form the content of a feature set that is finally associated with each powder material.

In this stage, the obtained feature set from each powder material are combined to form a composite feature set that contains all the features from all T-ray pulse signals of each powder substance. This feature set is used to generate training and testing sets through the cross-validation process. In order to reduce any bias of training and test data, a k-fold cross-validation technique is employed [45,47,48] setting k
                           =10. This technique is implemented to create the training set and testing set for evaluation. Generally, with k-fold cross validation, the feature vector set is divided into k subsets of (approximately) equal size. The proposed classifiers are trained and tested k times. Each time, one of the subsets from training is left out. One of the subsets (folds) is used as a test set and the other k
                           −1 subsets (folds) are put together to form a training set. Then the average accuracy across all k trials is computed to assess the performance of the classifier.

In the following section, the utility of the calculated feature sets is evaluated through four well established machine learning classifiers: multinomial logistic regression classifier with ridge estimators (MLR), k-nearest neighbours (KNN), support vector machine (SVM) and naïve Bayes (NB). Overviews of the adopted algorithms are provided below.


                           
                              Multinomial logistic regression classifier with ridge estimators (MLR)
                           
                        

Ridge estimators are used in multinomial logistic regression to improve the parameter estimates and to diminish the error made by further prediction when the application of maximum likelihood estimators (MLE) is inappropriate because of the non-uniqueness of the solution in the data fitting process. When the number of explanatory variables are relatively large and/or when the explanatory variables are highly correlated, the estimates of parameters are unstable, and are not uniquely defined (some are infinite) so the maximum of log-likelihood is achieved at 0 value [49,50]. In this situation, ridge estimators are used to generate finiteness and uniqueness of MLE to overcome such problems. The above rationale provides the necessary justification for considering the use of such classifier to the current task. For a response variable Y
                           ∈{1, 2, ..., k} with k possible values (categories), there are k classes for n instances with m attributes (explanatory variables), the parameter matrix B that requires to be calculated will have dimension m
                           ×(k
                           −1). In this case, the probability for class j with the exception of the last class is given from:
                              
                                 (4)
                                 
                                    
                                       
                                          P
                                          j
                                       
                                       (
                                       
                                          X
                                          i
                                       
                                       )
                                       =
                                       
                                          
                                             exp
                                             (
                                             
                                                X
                                                i
                                             
                                             
                                                B
                                                j
                                             
                                             )
                                          
                                          
                                             
                                                
                                                   
                                                      ∑
                                                      
                                                         j
                                                         =
                                                         1
                                                      
                                                      k
                                                   
                                                   
                                                      exp
                                                      (
                                                      
                                                         X
                                                         i
                                                      
                                                      
                                                         B
                                                         j
                                                      
                                                      )
                                                   
                                                
                                             
                                             +
                                             1
                                          
                                       
                                    
                                 
                              
                           The last class has a probability of occurring given by:
                              
                                 (5)
                                 
                                    
                                       1
                                       −
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          
                                             k
                                             −
                                             1
                                          
                                       
                                       
                                          
                                             P
                                             j
                                          
                                          (
                                          
                                             X
                                             i
                                          
                                          )
                                       
                                       =
                                       
                                          1
                                          
                                             
                                                ∑
                                                
                                                   J
                                                   =
                                                   1
                                                
                                                
                                                   K
                                                   −
                                                   1
                                                
                                             
                                             
                                                exp
                                                (
                                                
                                                   X
                                                   i
                                                
                                                
                                                   B
                                                   j
                                                
                                                )
                                                +
                                                1
                                             
                                          
                                       
                                    
                                 
                              
                           and the (negative) multinomial log-likelihood is given from:
                              
                                 (6)
                                 
                                    
                                       L
                                       =
                                       −
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          n
                                       
                                       
                                          
                                             
                                                
                                                   ∑
                                                   
                                                      j
                                                      =
                                                      1
                                                   
                                                   
                                                      k
                                                      −
                                                      1
                                                   
                                                
                                                
                                                   (
                                                   
                                                      Y
                                                      
                                                         i
                                                         j
                                                      
                                                   
                                                   ×
                                                   I
                                                   n
                                                   (
                                                   
                                                      P
                                                      j
                                                   
                                                   (
                                                   
                                                      X
                                                      i
                                                   
                                                   )
                                                   )
                                                   )
                                                   +
                                                   
                                                      
                                                         1
                                                         −
                                                         
                                                            ∑
                                                            
                                                               j
                                                               =
                                                               1
                                                            
                                                            
                                                               k
                                                               −
                                                               1
                                                            
                                                         
                                                         
                                                            
                                                               Y
                                                               
                                                                  i
                                                                  j
                                                               
                                                            
                                                         
                                                      
                                                   
                                                   ×
                                                   I
                                                   n
                                                   
                                                      
                                                         1
                                                         −
                                                         
                                                            ∑
                                                            
                                                               j
                                                               =
                                                               1
                                                            
                                                            
                                                               k
                                                               −
                                                               1
                                                            
                                                         
                                                         
                                                            
                                                               P
                                                               j
                                                            
                                                            (
                                                            
                                                               X
                                                               i
                                                            
                                                            )
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       +
                                       r
                                       i
                                       d
                                       g
                                       e
                                       ×
                                       
                                          B
                                          2
                                       
                                    
                                 
                              
                           
                        

In order to find the matrix B for which L is minimized, a Quasi-Newton method is used to search for the optimized values of the m
                           ×(k
                           −1) variables [49]. At this stage it is worth noting that in the current implementation of the algorithm, before we use the optimization procedure, we ‘squeeze’ the matrix B into an m
                           ×(k
                           −1) matrix. A more detail description of the MLR adopted can be found in [49,50]. In the current study, X indicates the obtained feature set associated with the six powder substances and Y denotes the different categories associated with the six the powder substances.


                           
                              K-nearest neighbours (KNN) classifier
                           
                        

The rationale for choosing the use of a KNN algorithm is based on the fact that it is a very intuitive method in which the classifier labels the observations based on their similarity in the training dataset. Among the various methods of supervised statistical pattern recognition, the KNN rule is known to achieve consistently high performance, without a priori assumptions regarding the distributions from which the training examples are drawn [51]. Given a query vector x
                           0 and a set of N labelled instances 
                              
                                 
                                    
                                       {
                                       
                                          x
                                          i
                                       
                                       ,
                                       
                                          y
                                          i
                                       
                                       }
                                    
                                    1
                                    N
                                 
                              
                           , the task of the classifier is to predict the class label of x
                           0 on the predefined P classes. The KNN classification algorithm tries to find the k nearest neighbours of x
                           0 and uses a majority vote to determine the class label of x
                           0. Without prior knowledge, the KNN classifier usually evaluates Euclidean distances as a metric [52]. An appropriate value should be selected for k, because the success of classification is very much dependent on this value. There are several methods to choose the k-value; a well-established practical approach is to run the algorithm many times with different k-values (k
                           =1, 2, …, 20), and choose the one with the best performance. A detailed discussion of this method can be found in [53,54]. In the current investigation, we consider the feature vector associated with the powder sample datasets as {x
                           
                              i
                           } and the six powder categories as class label {y
                           
                              i
                           }.


                           
                              Support vector machine (SVM) classifier
                           
                        

The SVM is most popular machines learning tool that can classify data separated by non-linear and linear boundaries, originated from Vapnik's statistical learning theory [55]. The main concept in all SVM algorithms is to first transform the input data into a higher dimensional space and then construct an optimal separating hyper-plane (OSH) between the two classes in the transformed space [39,56]. Those data vectors nearest to the constructed line in the transformed space are referred to as the support vectors. SVM algorithms belong to the more general area of “structural risk minimization” algorithms which have been developed specifically to attain a low probability of generalization error. Because of their versatility and universal applicability to a variety of classification tasks, they have also been considered in the current study. In order to solve nonlinear problems, when the data are not linearly separable, SVMs usually adopt a nonlinear kernel function [39,56], which allows better fitting of the hyperplane to the datasets that need to be classified. Recently, SVMs have also been extended to solve multi-class classification problems. One frequently used method in practice is to use a set of pair-wise classifiers, based on one-against-one decomposition [39]. The decision function for binary classification is given from:
                              
                                 (7)
                                 
                                    
                                       f
                                       (
                                       x
                                       )
                                       =
                                       sgn
                                       
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                s
                                             
                                             
                                                
                                                   y
                                                   i
                                                
                                                
                                                   α
                                                   i
                                                
                                                k
                                                (
                                                
                                                   x
                                                   i
                                                
                                                ,
                                                x
                                                )
                                                +
                                                b
                                             
                                          
                                       
                                       ;
                                        
                                       0
                                       <
                                       
                                          α
                                          i
                                       
                                       <
                                       C
                                    
                                 
                              
                           where, sgn is the signum function, K(x
                           
                              i
                           , x) is a kernel function and b is the bias of the training samples. In this work, a radial basis function (RBF) kernel is considered as a choice for identifying different categories of T-ray signals because this was found to give the best classification performance. Here C is the regularization parameter used to tune the trade-off between minimizing empirical risk (e.g. training error). In the current work, the complexity of the machine 
                              
                                 C
                                 =
                                 N
                                 /
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    N
                                 
                                 
                                    K
                                    (
                                    
                                       x
                                       i
                                    
                                    ,
                                    x
                                    )
                                 
                              
                            is always set to its default value, where N denotes the size of the training set, x
                           
                              i
                            indicates the ith input feature vector set (with a dimensionality of 6) and y
                           
                              i
                            (i
                           =1, 2, 6) is the class label of x
                           
                              i
                           , containing one of six categories of powder substances.

In the multiclass problem, SVM classification is performed using a collection of decision functions f
                           
                              kl
                           . Here kl indicates each pair of classes selected from separated target classes. The class decision can be achieved by summing up the pairwise decision functions [39].
                              
                                 (8)
                                 
                                    
                                       
                                          f
                                          k
                                       
                                       (
                                       x
                                       )
                                       =
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          n
                                       
                                       
                                          sgn
                                           
                                          (
                                          
                                             f
                                             
                                                k
                                                l
                                             
                                          
                                          (
                                          x
                                          )
                                          )
                                       
                                    
                                 
                              
                           Here n refers to the number of separated target classes. The algorithm proceeds as follows: first assign a label to the class: arg max f
                           
                              k
                           (x), (k
                           =1, 2, …, n). In the above equation, the signum function (sgn) is used to denote a hard threshold decisions [39] i.e., 
                              
                                 sgn
                                 (
                                 
                                    f
                                    
                                       k
                                       l
                                    
                                 
                                 (
                                 x
                                 )
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             1
                                              
                                             
                                                f
                                                
                                                   k
                                                   l
                                                
                                             
                                             (
                                             x
                                             )
                                             >
                                             0
                                          
                                       
                                       
                                          
                                             −
                                             1
                                              
                                             
                                                f
                                                
                                                   k
                                                   l
                                                
                                             
                                             (
                                             x
                                             )
                                             ≤
                                             0
                                          
                                       
                                    
                                 
                              
                           .

The pairwise classification then converts the n-class classification problem into n(n
                           −1)/2 two-class problems which cover all pairs of classes. An overview of SVM pattern recognition techniques associated with the proposed methodology may be found in [39,55,56].


                           
                              Naive Bayesian (NB) classifier
                           
                        

The NB is chosen for the current study as it is a straightforward and frequently used probabilistic classifier based on applying Bayes’ theorem with strong (naive) independence assumptions [57–59]. The NB classifier assumes that the presence (or absence) of a particular feature of a class is unrelated to the presence (or absence) of any other feature. Depending on the precise nature of the adopted probability model, the NB classifier can be trained very efficiently in a supervised learning setting. In practical applications, parameter estimation for naive Bayes models uses the method of maximum likelihood. In this classifier, each class with highest post-probability is addressed as the resulting class.

Suppose, X
                           ={X
                           1, X
                           2, X
                           3, …, X
                           
                              n
                           } is a feature vector set that contains C
                           
                              k
                            (k
                           =1, 2, m) classes of data to be classified. Each class has a probability P(C
                           
                              k
                           ) that represents the prior probability of identifying a feature into C
                           
                              k
                            and the values of P(C
                           
                              k
                           ) can be estimated from the training dataset. For the n feature values of X, the goal of classification is clearly to find the conditional probability P(C
                           
                              k
                           
                           |x
                           1, x
                           2, x
                           3,…, x
                           
                              n
                           ). By Bayes's rule, this probability is equivalent to
                              
                                 (9)
                                 
                                    
                                       P
                                       (
                                       
                                          C
                                          k
                                       
                                       |
                                       
                                          X
                                          1
                                       
                                       ,
                                       
                                          X
                                          2
                                       
                                       ,
                                       
                                          X
                                          3
                                       
                                       ,
                                       …
                                       ,
                                       
                                          X
                                          n
                                       
                                       )
                                       =
                                       
                                          
                                             P
                                             (
                                             
                                                C
                                                k
                                             
                                             )
                                             P
                                             (
                                             
                                                X
                                                1
                                             
                                             ,
                                             
                                                X
                                                2
                                             
                                             ,
                                             
                                                X
                                                3
                                             
                                             ,
                                             .....
                                             ,
                                             
                                                X
                                                n
                                             
                                             |
                                             
                                                C
                                                k
                                             
                                             )
                                          
                                          
                                             ∑
                                             
                                                P
                                                (
                                                
                                                   C
                                                   k
                                                
                                                )
                                                P
                                                (
                                                
                                                   X
                                                   1
                                                
                                                ,
                                                
                                                   X
                                                   2
                                                
                                                ,
                                                
                                                   X
                                                   3
                                                
                                                ,
                                                .....
                                                ,
                                                
                                                   X
                                                   n
                                                
                                                |
                                                
                                                   C
                                                   k
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

The final decision rule for the NB classifier is:
                              
                                 (10)
                                 
                                    
                                       c
                                       l
                                       a
                                       s
                                       s
                                       i
                                       f
                                       y
                                       (
                                       
                                          X
                                          1
                                       
                                       ,
                                       
                                          X
                                          2
                                       
                                       ,
                                       …
                                       ,
                                       
                                          X
                                          n
                                       
                                       )
                                       =
                                       
                                          
                                             
                                                arg
                                                max
                                             
                                          
                                          
                                             
                                                C
                                                k
                                             
                                          
                                       
                                        
                                       p
                                       (
                                       
                                          C
                                          k
                                       
                                       )
                                       
                                          ∏
                                          
                                             i
                                             =
                                             1
                                          
                                          n
                                       
                                       
                                          P
                                          (
                                          
                                             X
                                             i
                                          
                                          |
                                          
                                             C
                                             k
                                          
                                          )
                                       
                                    
                                 
                              
                           
                        

In the current study, we used the obtained feature vector set as the input in Eq. (10) and C
                           
                              k
                            (k
                           =1, 2, 6) indicates the number of the six powder categories that the data had to be classified. In the training stage, P(X
                           
                              i
                           
                           |C
                           
                              k
                           ) is estimated with respect to the training data. In the testing stage, based on the posterior probability P(C
                           
                              k
                           
                           |X
                           
                              i
                           ), a decision whether a test sample belongs to a class C
                           
                              k
                            is made. A detailed description of the method can be found elsewhere [51,57–59].

The current study explores the ability of T-ray spectroscopy to detect different densities, thicknesses, and concentrations of specific powder samples. This is a powder recognition task for six different powdered substances of 2mm and 4mm thickness where their spectroscopic signature needs to be de-convolved from that of the holder. The powders are: sand, talcum, salt, powdered sugar, wheat flour, and baking soda. In addition, we also explore the classification fidelity attained for a mixture of 2mm and 4mm thickness samples across all powder substances.

In order to further assess the performance and consistency of the proposed methods, data from 3mm thickness powder samples for the same six powder substances is also considered in this study. The 3mm thickness powder samples have the same composition as their corresponding 2mm and 4mm thickness powder sample datasets. A well set-up T-ray imaging system which generates femtosecond duration terahertz pulses is used to detect the T-ray sample responses [36,39]. The 2-D T-ray image of the sample is obtained after separately recording the sample holder transmittance and then inserting the powder sample. The geometry of the experiment preserves the ambiguities associated with the effects of different scattering paths and minor variations in powder thickness across the aperture (pseudo-coherence effects) and density due to slightly different compaction levels across the six substances observed. Sample transmittance is recorded by broadband time-domain THz transient spectrometry. The reported measurements have been conducted at the University of Adelaide Australia [39]. A detailed description of the dataset acquisition process using the THz imaging spectrometer can be found in [37–39].

To systematically evaluate the performance of the proposed 2-D cross-correlation based machine learning algorithms, THz time-domain spectra from all six known powder substances were used. These samples had very similar optical properties but different absorption features at the THz part of the spectrum. The classification task was to correctly identify the specific powders given they had unknown density, thickness and concentration. A preliminary exploration of different powder recognition tasks was first conducted with 2mm and 4mm thickness samples. Collected spectra incorporated the distortion from the sample holder, this signature was eliminated by assigning the holder spectrum in the experiments as background (reference) and ratioing the powdered sample spectrum with that of the background so as to extract the complex insertion loss. The following investigations were carried out: (i) multiclass classification of the six categories of powder samples at a thickness of 2mm; (ii) multiclass classification of the six categories of powder samples at a thickness of 4mm; (iii) binary classification in each powder substance for a mixture of 2mm and 4mm thickness samples. In order to obtain a further assessment of the consistency of the proposed methodology, we performed the multiclass classification of the six categories of powder samples at a thickness of 3mm and also evaluated the success of the algorithm to perform multiclass classification in each powder substance for a mixture of 2mm, 3mm and 4mm thickness samples. All the powder sample classification runs were performed using the MATLAB version R2013b software on a personal computer running Windows 7 with an Intel(R) Core(TM) i5-4570S CPU (2.90GHz) and 8 GB of memory. The following four classification algorithms were used: MLR, KNN, SVM and NB implemented in WEKA machine learning toolkit [60]. LIBSVM (version 3.2) [61] is used for the SVM classification in WEKA.

In the MLR method, the parameters are obtained automatically through the ridge estimator. The KNN model has only one parameter k which refers to the number of nearest neighbours. By varying k, the model can be made more flexible. In the current study, we have chosen the appropriate k value through an automatic process following a k selection error log as there is no simple rule for selecting k. We consider the range of k values between 1 and 20, and picked an appropriate k value that results in lowest error rate as this is associated with the best model. In the experimental results, we obtain the lowest error rate for k
                        =1. For the SVM, the RBF kernel function was employed as an optimal kernel function over several different kernel functions that were tested. As there are no specific guidelines to set the values of the parameters for the MLR and the SVM classifiers, we considered the parameter values that have been used in WEKA as default parameter settings. The NB consists of number of parameters that are estimated from the training examples. Parameter estimation for the NB models uses the method of maximum likelihood.

In this study, we assess the performance of the proposed classifiers using widely accepted metrics such as accuracy, true positive rate (TPR) (also called sensitivity or recall), false positive rate (FPR) (also called false alarm rate or (1−specificity)), precision (also called positive predictive value), F-measure, mean absolute error (MAE) and kappa statistics. These criteria were applied to assess all extracted feature data. The evaluation metric adopted is accuracy rate as percentage of correct prediction [62–64]. The TPR provides the fraction of positive cases that are classified as positive [46,65]. The FPR [46,66] is the percentage of false positives predicted as positive from samples belonging to the negative class. The FPR usually refers to the expectancy of the false positive ratio. Precision is a measure which is used to estimate the probability that a positive prediction is correct. F-measure is a combined measure for precision and recall calculated as 2*Precision*Recall/(Precision+Recall) [46]. Mean absolute error (MAE) is used to measure how close predictions are to the eventual outcomes [46]. Kappa is a chance-corrected measure of agreement between the classifications and the true classes [46,67]. It is calculated by taking the agreement expected by chance away from the observed agreement and dividing by the maximum possible agreement.

The images of powder samples consist of 6×51=306 pixels. For each pixel, the number of samples associated to a pulse time transient is set to 401. Fig. 2
                        (a) and (b) shows the time domain responses associated with the THz transmittance of the powdered samples with 2mm and 4mm thickness, respectively. It can be seen that the weakest (most attenuated) signals are seen for the powders with sand and salt. According to expectations, as the thickness of the powders were varied the T-rays pulse showed a linear increase in phase (or delay of the time domain pulse) and an exponential decay in amplitude with thickness.

In the proposed methodology, each pixel (a T-ray pulse signal) signal in a powder substance is cross-correlated with the reference signal (holder signal) so that it produces a cross-correlogram sequence. Each of the six powder substances is composed of a 51 pixels signal irrespective of thickness (e.g. 2mm, 4mm, 3mm, the mixture of 2mm and 4mm, and the mixture of 2mm, 3mm and 4mm thick samples whether they are in pure form or mixture). The reference signal also is composed of 51 pixel signals and each pixel signal contains 401 data points. In the proposed scheme, the reference signal is cross-correlated with the data of a class with the 51 pixel signals using Eq. (1) and thus for each powdered substance, 51 cross-correlation sequences are obtained where each sequence contains 801 data points. As mentioned in Section 2.1.1, if a reference signal (X) and any other signal (H) of a class consists of S number of samples, the resultant cross-correlation sequence has 2S
                        −1 samples. Here, S
                        =401. Hence, each class powder samples corresponds a cross-correlation sequence matrix with dimension 801×51. The proposed 2D cross-correlation approach ensures far superior de-noising than a traditional single pixel by pixel cross-correlation but at the expense of additional computations. Fig. 3
                         shows an example of the calculated cross-correlogram patterns. Each cross-correlogram is calculated using Eq. (1) for each time lag. From this figure, one can see that in most of the cases, the shapes of the curves are not exactly the same, this indicates statistical independency.

This pre-processing stage is followed by calculation of the ten statistical parameters (see discussion in Section 2.1.2) from each of the 51 cross-correlation sequences in a class so as to obtain feature matrices with dimension 51×10. Thus, for all six categories of powder data samples, we acquire a total of 306 feature vectors with 10 dimensions. MATLAB functions were employed for calculating mean, standard deviation, skewness, kurtosis, Q
                        1, Q
                        3, IQR, median, maximum and minimum from each cross-correlation sequence. Using the10-fold cross validation method, the obtained feature vector set is divided into a training set and a testing set. The training set is applied to train the classifier and the testing vectors are used to verify the performances and the effectiveness of the classifiers. The feature vectors were evaluated through all four classifiers. Classification performances are evaluated in term of accuracy, TPR, FPR, precision and F-measure.


                        Fig. 4
                        (a)–(e) shows the variation in performances for the mentioned four classifiers as a function of increased number of input features in the 2mm thickness powder dataset. The number of the input features is varied from 2 to 10. It can be seen that the corresponding accuracy, TPR, precision and F-measure for each four classifiers are increased monotonically and almost linearly with the number of feature vectors and the FPR of each four classifiers are going to decrease with the increase number of feature vectors, this indicates consistency in the proposed analysis. From these figures, it is also observed that in all performance evaluations, the MLR classifier yields a better performance individually, for 2, 4, 6, 8 and 10 features compared to the KNN, SVM and NB classifiers. As shown in Fig. 4(a)–(e), among the reported four classifiers, the MLR classifier produces the best performances when using 10 features while the NB classifier consistently displays the lowest performances.


                        Fig. 5
                        (a)–(e) depicts the performance of all the classifiers on the basis of the number of features in the 4mm thickness powder sample datasets. Similarly to the results in Fig. 4, the classification performance for each of the four classifiers increases when the number of features is increased. The MLR classifier yields better performance in most of the cases compared to the other three classifiers while the NB classifier performance is the lowest.


                        Fig. 6
                        (a)–(e) illustrates the classification accuracy, TPR, FPR, precision and F-measure for all classifiers as a function of number of features for the mixture of 2mm and 4mm thickness soda powder data. As can be seen, the performance of each of classifiers improves when the number of features considered increases. The highest performances are obtained when assuming 10 features and the lowest for 2 features. In these figures, both MLR and KNN show similar performance, this is superior to that of the other two classifiers on the mixture of 2mm and 4mm thickness soda sample. It can also be seen that the NB classifier is the least successful in the classification task than the other three. This is a very positive overall outcome as it indicates stability consistency and robustness in the results with the 2-D cross correlation feature extraction methodology and the adopted classifier performance evaluation method. These results point to a necessity to use all 10 features for the further evaluation of the proposed classifiers as discussed in the following sections.

@&#RESULTS AND DISCUSSIONS@&#


                     Tables 1–3
                     
                     
                      presents the classification results for all four classifiers in more detail assuming 10 features are used for all powder sample compositions for 2mm, 4mm and the mixture of 2mm and 4mm sample thicknesses, respectively. In these three tables, the class-specific performances for each powder substance and also overall performances in terms of accuracy, TPR, FPR, precision and F-measure are reported. In Table 1, it can be observed that the performances (the values of accuracy, TPR, precision and F-measure) for the MLR classifier are most promising, which is 100% across every category irrespective of powder substance and the FPR is also 0%. Furthermore, the performance parameter values for the KNN classifier is slightly better than those of the SVM and NB classifiers while the SVM classifier performs better than the NB classifier. In addition, the soda powder samples are the easiest to be separated, with classification accuracy of 100% in all cases, whereas the talc and sugar powder samples are the most difficult to classify. The results in Table 1 also clearly shows that the MLR classifier using a10 feature set yields the best performance across all classifiers and the NB classifier shows a consistently inferior performance.

As shown in Table 2, the overall accuracy of the MLR, KNN, SVM and NB classifiers are 98.69%, 98.37%, 95.75% and 87.26%, respectively for the 4mm thickness powder samples on the basis of 10 features being presented at their inputs. The overall TPR for the MLR, KNN, SVM and NB classifiers are 98.7%, 98.37%, 94.45% and 85.95%, respectively and the FPR values are 0.27%, 0.33%, 01.12% and 2.80% respectively. The overall precision and F-measure are 98.7% and 98.68% for the MLR, 98.37% and 98.35% for the KNN, 94.80%, 94.40% for the SVM and 85.87% and 85.93% for the NB. Thus, in most of the cases, the MLR classifier yields the highest performance and the NB lowest one. Moreover, the sand powder samples are easiest to separate (classification accuracy of 100% across all four classifiers), whereas the talc and sugar powder samples are more challenging to classify.


                     Table 3 reports the experimental classification outcomes for the mixture of 2mm and 4mm thickness samples for all six powder substances. This classification is performed as a binary process (2 class classification). Here, the 2mm powder substance is considered as one class and the powder substance of 4mm thickness is considered as another class e.g. classification of a 2mm sand sample and a 4mm sand sample. As can be seen from this table, the powder samples of sand, talc, salt, sugar and flour are easiest to be separated by the MLR, KNN and SVM classifiers, (where a classification accuracy of 100% was achieved under all the cases), whereas the soda powder sample proved more difficult to classify. The NB classifier could not classify successfully powder substance mixtures. Also, the soda powder sample was consistently more difficult to classify.

In order to further demonstrate the effectiveness of the proposed methods, we also apply our methodology on results obtained using 3mm sample thicknesses and the results are reported in terms of accuracy, TPR, FPR, precision and F-measure. These details are shown in Table 4
                     . It can be seen that the overall accuracy of the MLR, KNN, SVM and NB classifiers with the 10 features set are 96.73%, 97.38%, 95.42% and 89.87%, respectively for the 3mm thickness powder samples. Here, the accuracy of the KNN classifier is a little bit higher than the MLR classifier while it is the lowest for the NB classifier, this result is reasonably consistent to those obtained by classifying the 2mm and 4mm powder datasets. The other performance criteria show also similar consistency in classification accuracy. Similarly to the case of the 2mm and 4mm thickness sand and soda powder samples, the 3mm samples are the easiest to be separated, with classification accuracy of 100% in all cases for all four reported classifiers, whereas the talc and sugar powder samples are the most difficult to classify.


                     Table 5
                      reports the classification outcomes for the mixture of 2mm, 3mm and 4mm thickness samples for all six powder substances. This classification task is set up as a three class problem. Here, the 2mm thickness powder substance is considered as belonging to the first class, the 3mm thickness powder substance is considered as belonging to the second class and the 4mm thickness powder substance is considered as belonging to the third class. As can be seen from this table, the overall accuracy for the MLR is 99.56% for all the powder samples while this value is 99.35% for KNN, 91.83% for SVM and 91.82% for NB classifier. Similarly to the classification results discussed in the previous sections, in most of the cases, the MLR classifier consistently yields the highest performance whereas the NB classifier the lowest one. As shown in Table 5, the good classification performance and classification consistency of the proposed method in discriminating across samples in a mixture consisting of three thickness (2mm, 3mm and 4mm) powder data sets when from a compositional perspective these samples were originally very hard to discriminate, demonstrate that the 2D cross correlation based feature extraction approach successfully de-noises the datasets while at the same time enables us to resolve useful features in the time domain signals associated with each pixel in the image in a consistent manner. This is significant bearing in mind that classification tasks that were difficult to perform in the past due to the presence of some unquantifiable scattering become now possible. It is also worth noting that although in analytical sciences, cross-correlation techniques have been mainly explored within a de-noising context, the proposed methodology places these algorithms within a machine learning context. It may also be concluded that the MLR is a powerful and less complex algorithm for THz pulse signals classification. The proposed technique should extend the use of classification algorithms to experiments where samples are not placed in a cuvette, a sample holder or compressed in pellet form in order to perform the spectroscopic investigations, and points towards a new way of performing industrial quality control using THz imaging systems ‘in situ’ when samples are still in powder form where different degree of scattering may also be present in the measurement process across the different spectral bands. The proposed methodology therefore has the potential to significantly extend the applications domain of classifiers for material characterization; this has important applications in high value manufacturing such as the pharmaceutical industry as well as for tissue differentiation and characterization in biomedical imaging.


                     Fig. 7
                      displays the proposed algorithm execution time for all four classifiers for a 10 feature input across all sample thicknesses. It can be seen that, in every cases the SVM classifier takes more time than all other reported classifiers and the NB and KNN algorithms are the fastest to execute.

The shape of the MAE for each of the four reported classifiers is illustrated in Fig. 8
                     . The lower MAE score indicates a higher performance in the proposed approach. We can see that irrespective of thickness the score of MAE is significantly lower for the MLR classifier compared to the other three classifiers. On the other hand, the NB classification method consistently yields a very high MAE score. Particularly, in the cases of mixture of 2mm, 3mm and 4mm powder samples, both NB and SVM generate very high MAE scores while the values are very low for the MLR classifier. Once again the MLR classifier seems to be the best choice to classify the THz pulses signal datasets associated with different powder compositions.


                     Fig. 9
                      displays kappa statistics for all classifiers assuming a 10 feature input. The aim of the kappa statistics test is to evaluate the consistency of the classifiers. Consistency is considered mild if kappa values are less than 0.2 (20%), fair if it lies between 0.21–0.40 (21–40%), moderate if it lies between 0.41–0.60 (41–60%), good if it is between 0.61–0.80 (61–80%), and excellent if it is greater than 0.81 (81%). As shown in Fig. 9, the highest kappa values are obtained by the MLR on both 2mm thickness sample datasets (100%), as well as 4mm (98.43%) datasets. In addition, highest kappa values are obtained for the mixture of 2mm, 3mm and 4mm samples of talc (100%), salt (100%), flour (100%) and soda (100%). The KNN algorithm also demonstrated very good performance (second best overall) as can be seen in the case of the 3mm thickness sample datasets (96.86%), and the mixtures of 2mm, 3mm and 4mm sand (98.04%), talc (100%), sugar (100%) and flour (100%). The kappa values of the other two classifiers (SVM and NB) are systematically lower compared to those achieved by the MLR and KNN irrespective of sample type, furthermore the values are consistently lowest for the NB classifier. In this figure, the error bars indicate the associated kappa value standard error. In most of the cases, the highest kappa values are obtained using the MLR algorithm.

In order to compare our research outcomes with existing ones in the literature the only reference that can be found is discussed in [24]. In their work, however, the focus on the study was placed on the derivation of hybrid AR and ARMA models with further wavelet compression for very parsimonious feature extraction aiming to improve on the generalization ability of the classifier. In that study, wavelet-based de-noising with soft threshold shrinkage was applied to the measured T-ray signals prior to modelling. It is also worth noting that a simple Mahalanobis distance classifier was used at that time for the classification of the powder samples and the emphasis was placed on feature extraction as opposed to address state-of-the-art machine learning approaches. An overall 98% classification accuracy for all thickness powders was achieved with that approach whereas the proposed method based on 2D-cross-correlation and an MLR classifier yielded a classification accuracy of 99.56%.

In this study, in most of the circumstances, the MLR algorithm produced better results compared to other reported three classifiers and the total performance of the KNN classifier was alike to the MLR classifier. As mentioned before, T-rays pulse signals contains multi-correlation in different powder substance data because screening items are often highly correlated in terms of particle shape and dimension, the effect can also manifest itself at specific measurement angles due to possible scattering. The one of the main advantages of the MLR is to properly handle multicollinearity within a large number of covariates that cause unstable in the parameter estimation and larger variance in the associated distributions used as inputs to the classifiers. These effects collectively have an overall effect of systematically degrading classification accuracy. One drawback of the MLR may be the computational demand needed when images are composed of a very large number of pixels. On the other hand, the key advantage of the KNN classifier is, it does not require a priori assumptions regarding the distributions from which the training examples are drawn. It makes this method to be simple in implementation with less computation time. But the core limitation of this method is, it is classifying accuracy decreases in the presence of high dimensional feature data. Hence, it seems that from the current study, the MLR promises to offer the better performance for detection of powder substance using T-rays pulse signal datasets reducing overfitting error.

@&#CONCLUSIONS@&#

This paper presented the first systematic evaluation of machine learning algorithms tailored specifically to the classification of THz datasets obtained using a THz transient spectrometer. A further aim was to establish alternative new criteria that would capture some of the features present in the time-domain signals so that unquantifiable scattering effects that would otherwise degrade the discriminating ability of the classifiers would be minimized. A 2-D cross correlation technique was adopted for feature extraction prior to sample classification. The dimensions of the calculated cross-correlation sequences were also further compressed extracting additional statistical features. Several powder substances of various thickness and composition were successfully classified using the proposed algorithms. Systematic evaluation of the performance of the four classifiers considered using multiple datasets of powdered samples and a comprehensive cross-validation methodology showed that, in most of the cases, the MLR classifier with ridge estimator outperformed the KNN, SVM and NB classifiers. It is worth noting however, that the overall performance of the KNN classifier was very similar to that of the MLR classifier. Thus the study concluded that the 2-D cross-correlation based MLR or KNN algorithm in conjunction with the proposed 2-D cross-correlation technique can lead to a systematic enhancement in THz transient dataset classification success rate. The proposed methodology paves the way for establishing new robust and consistent approaches for the analysis and automated classification of THz transient biomedical imaging datasets which are currently difficult to classify because of the large signal attenuation of tissue associated with the quenching from the tissue's water content. Algorithmic expert systems are currently considered to be the Achilles’ heel in THz signal analysis. Thus the work addresses a fundamental problem which so far has consistently delayed the further proliferation and commercialization of THz transient spectrometers. Future investigations will assess classifier performance when the THz transient data vectors are systematically over-sampled or under-sampled (always above the Nyquist's criterion), so as to assess opportunities for optimizing signal to noise ratio for a given data acquisition time frame, this is a topic of significant interest to clinicians considering the adoption of this imaging modality for routine patient screening. Beyond the THz community, the proposed methodology may also be used for the systematic assessment of different classifiers and automated expert systems as applied to other datasets across the entire electromagnetic spectrum e.g. X-ray, UV, visible, infrared or microwave spectrometry, electron spin resonance spectrometry, nuclear magnetic resonance imaging, positron emission tomography etc. Thus the work is generic and of relevance across all physical sciences.

@&#REFERENCES@&#

