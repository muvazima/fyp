@&#MAIN-TITLE@&#Facial expression recognition experiments with data from television broadcasts and the World Wide Web

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A new realistic facial expression recognition (FER) database


                        
                        
                           
                           Compared feature and performance differences between lab-based and realistic data


                        
                        
                           
                           Examined factors that affect FER performance regarding data, feature and training


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Facial expression recognition

Realistic

Texture

Geometry

Experiment

@&#ABSTRACT@&#


               
               
                  Facial expression recognition (FER) systems must ultimately work on real data in uncontrolled environments although most research studies have been conducted on lab-based data with posed or evoked facial expressions obtained in pre-set laboratory environments. It is very difficult to obtain data in real-world situations because privacy laws prevent unauthorized capture and use of video from events such as funerals, birthday parties, marriages etc. It is a challenge to acquire such data on a scale large enough for benchmarking algorithms. Although video obtained from TV or movies or postings on the World Wide Web may also contain ‘acted’ emotions and facial expressions, they may be more ‘realistic’ than lab-based data currently used by most researchers. Or is it? One way of testing this is to compare feature distributions and FER performance. This paper describes a database that has been collected from television broadcasts and the World Wide Web containing a range of environmental and facial variations expected in real conditions and uses it to answer this question. A fully automatic system that uses a fusion based approach for FER on such data is introduced for performance evaluation. Performance improvements arising from the fusion of point-based texture and geometry features, and the robustness to image scale variations are experimentally evaluated on this image and video dataset. Differences in FER performance between lab-based and realistic data, between different feature sets, and between different train-test data splits are investigated.
               
            

@&#INTRODUCTION@&#

The natural expression of an emotion on the face is a highly reliable component of human perception in social interactions. An angry look conveys more than words can. Facial expression recognition (FER) with computers has potential applications in human computer natural interfaces, video surveillance etc. For these applications to be realized, FER algorithms must work on real emotion data, which reflects the real reaction of humans and real environmental conditions more closely than lab-based data, which is normally collected under a pre-set simplified laboratory environment [1] and is a product of acting.

The vast majority of FER studies have focused on lab-based data. It is difficult to obtain facial expression data from real world events on a large scale for benchmarking because of privacy laws. If data are sourced from TV broadcasts and web sources, privacy is less of an issue but copyrights must be obtained. There are also challenges because such data are not constrained and will have large variations in lighting, pose, etc. Current FER methodologies developed on lab collected data are not designed to generalize to such scenarios. However, even before these processing challenges are addressed there are some questions to be answered:
                        
                           (1)
                           Are facial expressions in video obtained from TV and World Wide Web sources different from those in lab collected data? In what aspects?

Is the recognition performance on such data significantly different using different types of features, and different FER algorithms?

Does the performance differ between different types of realistic data?

In this work, a database from television (TV) broadcasts and the World Wide Web (referred to as the QUT FER dataset) has been collected. Two public lab-based databases—FEEDTUM and NVIE, and one realistic SFEW database are used for the purpose of comparisons of feature and performance differences between lab-based and realistic data and between different realistic data. The evaluation is based on an automatic approach that is specifically designed using fusion of texture (scale invariant feature transform—SIFT) and geometry (facial animation parameters—FAP-based distances). The main objective is to explore the differences in FER performance to provide new insights as the data moves from lab-based to more realistic. The contributions are original primarily in the range covered by the data and the comparisons.

The rest of the paper is organized as follows. Section 2 gives a brief review of existing realistic FER databases. Section 3 describes the collection of the QUT FER database and explores the difference in feature distributions between lab-based and realistic data. Section 4 introduces a benchmark approach and reports its performance across different datasets, feature sets and train-test splits. Performance is also compared with previous approaches. Conclusions are drawn in Section 5.

This section reviews available realistic databases. A recent comprehensive survey of FER can be found in [2].

Existing FER databases approximately fall into two categories: lab-based data where the emotions are intentionally expressed by selected actors or deliberately induced by outside perceived stimuli in a pre-set laboratory environment, and realistic data where the emotions naturally occur in real-world conditions. The majority of current FER databases belong to the first category, and popular examples include the Cohn–Kanade (CK), CK+, JAFFE, FEEDTUM, BU-3DFE, Semaine, MMI, UT-Dallas, SAL, NVIE, RU-FACS, AAI, PETS2003 and GEMEP-FERA. Compared with lab-based emotions, realistic expressions (samples shown in Fig. 1
                     ) are also often accompanied by big variations in face pose, size, illumination, facial occlusions etc., and thus they are more challenging to classify and hold more significance in real-world applications.


                     Table 1
                      reviews typical examples of existing realistic databases. Early attempts at collecting realistic data, such as luggage lost and Belfast, tried to capture facial reactions in real scenarios, such as interviews and conversations between subjects. Recent studies have collected emotional video segments from TV broadcasts or the World Wide Web to more closely mimic real situations, yielding databases including HUMAINE [1], VAM, TV data [3], acted facial expressions in the wild (AFEW) [4], static facial expressions in the wild (SFEW) [5], happy people images (HAPPEI) [6], GENKI-4K [7] and Gv [8]. Video frames in the HUMAINE and VAM databases are not directly annotated with basic emotion categories. TV broadcast data are presented by Yeasin et al. in [3] but details have not been provided. The AFEW is a relatively new dataset comprising of both a video set and an image set collected from 37 movies. The SFEW database contains 700 images collected from frames of AFEW videos. The HAPPEI database is composed of 4600 images collected from the Flickr website and was built for happiness intensity estimation for groups of people. The GENKI-4K images are primarily for smile, while details of Gv images have not been given. All the above databases are collected from only one source (i.e. either TV programs or movies, or the web), or are annotated with a limited number of emotion types (such as only happiness in the HAPPEI database). In this work, a realistic QUT FER database has been compiled with some unique characteristics:
                        
                           (1)
                           Data collected from three sources (i.e. TV drama, TV news and the web), which contain variations in illumination, pose etc. closer to those expected in real applications.

More types of emotions (i.e. positive, negative and neutral), which are particularly significant to specific applications such as sentiment analysis, as not all realistic emotions can be categorized into the six basic emotions.

Intensity for nine emotions, which can be used for evaluating and estimating the level of emotional reactions of subjects to outside stimulus.

This section describes the QUT FER realistic database collected from web-based and broadcast video recordings. Feature distributions for three emotions in the database are compared with those in two public lab-based FER databases—FEEDTUM and NVIE.

To collect video segments from real multi-media materials, we chose to use three sources: TV News, TV drama, and YouTube. These sources are publicly available, represent real day-to-day situations, and contain lots of interpersonal communications and performances, which lead to a wide range of emotional states. As a result, these materials can provide a lot of basic and non-basic emotions in various real conditions, and cover a wide range of people with different ages. Table 2
                         shows the number of videos and contents for each source.

All video segments are converted to the AVI format with the original resolution ranging from 480×360 to 1024×576pixels and a frame rate from 14fps to 25fps. The Video Splitter software is used to segment all videos into short clips, which contain a whole process of one emotion (i.e. neutral–onset–apex–offset–neutral) expressed by the person. For videos that do not contain the whole process, only the clips covering a neutral–onset–apex process are segmented. If there is more than one person, only the central one with the largest face size is used for subsequent processing. To facilitate image based analysis, we also use the Sony Vegas Pro 9.0 software to extract all frames from each clip. The frames are saved with a JPEG format and with the same resolution as the corresponding original videos. Then two to six typical frames are manually selected from each clip to represent facial expressions with different emotional intensities, corresponding to the process from onset to apex. The image part of the dataset is a subset of the frames from the clip part. It should be noted that these images also have substantial changes in aspects such as face pose, occlusion, and illumination that are expected in realistic conditions.

The categories of emotion types used here include six basic emotions plus positive, negative and neutral. The six basic emotions include anger, disgust, fear, happiness, sadness, and surprise, while the positive, negative and neutral are defined as:
                              
                                 
                                    Positive: Emotions that are psychologically elevating such as happiness


                                    Negative: Emotions that are psychologically depressing such as anger, fear, disgust and sadness.

Neutral: Emotions that are neither positive nor negative.


                           Figs. 2a and b show a subset of samples for these emotions (faces manually cropped and annotated). It may be noted that facial expressions could belong to more than one of the basic emotion categories. In such cases where multiple emotions are mixed (e.g. mixture of emotions shown in Fig. 2c) in a single facial expression, the adoption of positive, negative and neutral intensity categories is easier.

There is little information provided in existing databases about how to train annotators specifically for realistic FER data, such as AFEW [4], SFEW [5] and HUMAINE [1]. Following the annotation method in the NVIE database [12], a total of five college students, two males and three females, were volunteer participants in the emotion annotation. Before starting the annotation, the three students were given an introduction about the project and received a pre-training session to guarantee accuracy of the annotation results. The training materials such as “The Six Basic Facial Expressions” from the University of North Carolina and a sample set of 600 peak spontaneous emotion images from the NVIE database were provided to assist them to identify the basic facial expressions (100 images per emotion). After they were familiar with the six basic facial expressions, a series of quick tests which consist of 20 sets of six facial expression plus neutral images was conducted by one of our team members (more than 4-year research experience on emotion recognition) to test whether they can accurately identify the given expressions. The whole process was cycled again for subjects with misrecognized emotions, until all annotators achieved satisfying accuracy (more than 95%).

To start the annotation, all video clips and image files were given to the five annotators. They were allowed to use their own computers to annotate files, but with the supervision of one of our team members. All annotators were told to classify all videos and images into one of the six basic emotions plus neutral. For those difficult to be classified, a choice between positive and negative emotion was made. In addition, the emotional intensity was also estimated within a range [0, 3] for each video clip based on the frame with the maximum/peak expression, where 0 and 3 correspond to neutral and the peak expression respectively. All results were written into an Excel file and rechecked by an experienced researcher in the area of facial expression recognition who is familiar with annotations of several available databases. Each clip or image where the annotations conflicted was classified into positive, negative or neutral based on the majority of annotated results. Images and clips with mixed emotions are excluded from categorization into the six basic emotions, and are classified into positive, negative or neutral only. For ties that result from ambiguity in interpretation (examples shown in Fig. 2d), the clips or images were simply removed from the evaluation dataset. Thus, the dataset of the six basic emotions plus neutral is a subset of the dataset of positive, negative and neutral emotions.

The current version of the database contains an image part
                           1
                        
                        
                           1
                           The image part of the QUT database is available to researchers in this field. Interested researchers may send emails to the first author to obtain a copy.
                         and a video part, consisting of 2927 images and 458 clips respectively. The clips are collected from 219 subjects, comprising 102 females and 117 males. Their ages range from about 3 to 70years old, with a bias on the adult as shown in Table 3
                        . Because it is not possible to determine the ages of subjects in video and images collected by others where such metadata have not been recorded, the age group of each subject was based on an educated guess by the annotator. The face region is also detected and tracked using the Viola–Jones (VJ) detector and active shape model (ASM), returning 2790 images and 394 clips (i.e. the failure rate of VJ and ASM is 4.7%). Table 4
                         shows the distribution of all images and clips over (a) six basic (the first 6 columns) and (b) three categorized emotions (the last 3 columns). As can be seen, in subset (a), happiness takes a large portion of the total, while fear and disgust only take a small proportion. This is due to the difficulty of collecting many disgust and fear videos from the chosen real materials. In subset (b), negative and positive have a similar number, which is about three times of that of neutral.

The extent of the differences in the facial appearance expressed by subjects in a laboratory and that expressed in a more realistic environment such as those observed in television and the World Wide Web sources is unknown – and whether they are indeed different is an unanswered question. An approach to answering this would be to examine whether features such as the openness of the mouth or the eye during a facial expression (at its apex) are different for the two types of data. This section compares FAP-based geometric features between lab-based images and realistic images. The comparison is performed on two FAPs No. 3 and 29, indicating the openness of the mouth and the eye respectively, between FEEDTUM (or NVIE) and QUT databases.

The FEEDTUM database [13] contains 399 video sequences from 18 subjects. Each subject performed all six basic plus neutral emotions three times, and each sequence starts and finishes with a neutral state. This database was collected with attempts to awake or induce real emotions by playing video clips or still images after a short introduction phase instead of telling the person to play (or act out) a role. It lets the observed subjects react as naturally as possible and allows head movements in all directions. The visual subset of the NVIE database [12] contains video and images from 105, 111, and 112 subjects under front, left and right illumination, respectively. The spontaneous expressions are induced by film clips deliberately selected from the Internet. During recording, all subjects are allowed to seat themselves comfortably and move the chair forward or backwards. The images with the peak emotion are labeled by five students as one of six basic emotions.

To avoid a bias arising from the data selection, five images are evenly selected from each of 359 FEEDTUM sequences,
                           2
                        
                        
                           2
                           Note that the distributed version of the FEEDTUM database contains 380 video. A subset of 21 video is excluded in the experiment due to problems such as their content cannot be read by the computer and most of the selected frames have a neutral emotion rather than the emotion annotated for the video.
                         one from every 10 frames starting from the peak emotion frame, leading to a total of 1795 images. All of 1491 visible NVIE images with final annotations at the peak emotion are used here and thus the used subset contains only emotions at its apex. After face and fiducial point detection, 1787 FEEDTUM and 1472 NVIE images are obtained (i.e. the failure rates of VJ and ASM are 0.4% and 1.3% on the two databases respectively). Fig. 3
                         and Table 5
                         display samples and their distribution over seven emotions, respectively.


                        Fig. 4
                         compares the normalized distribution of FAP values for happiness, neutral and surprise between lab-based and realistic images, while Table 6
                         illustrates their statistics. QUT images exhibit higher mean and standard deviation than lab-based FEEDTUM and NVIE images, for all three emotions and both FAP features selected. QUT images not only cover all values of FAPs No. 3 and 29 in FEEDTUM and NVIE images, but also exhibit higher values for all emotions. Furthermore, the skewness indicates that the distribution for QUT images is more likely to spread out to the left of its mean value compared with those for FEEDTUM and NVIE images. The P-values are zeros for all entries, and this indicates that all distributions between lab-based and realistic images are statistically different at the 5% significance level. This implies that facial expressions in lab-based images are significantly different from data obtained from TV and web sources and the latter may be described ‘more realistic’. This warrants the need to benchmark algorithms on the latter type of data to test their effectiveness in products for real-world applications.

The same comparisons (not shown here) between QUT images and all QUT video frames indicate that the overall distributions of FAPs No. 3 and 29 are similar for all three emotions, but the cluster of video frames is distributed with a larger shift to its mean value than that of images and it also contains a small proportion of lower FAP ranging from 0 to 0.1, caused by closed mouths and eye blinks. Thus, there are also significant differences between still images and video sequences and algorithms should be benchmarked with realistic video data.

This section introduces a benchmark FER approach and reports its performance on the QUT, FEEDTUM and NVIE databases. We also explore the differences in FER performance between lab-based and realistic data, between different train-test data, and between different feature sets. The performance measure is the average classification accuracy over 10 subject-independent disjoint train-test validation tests. All images are divided into different sets according to the subject identity. Images are then randomly selected, 90% for the training and the other 10% sets for the testing, and the process is repeated 10 times to obtain averaged performance from the multiple Monte Carlo trials.


                        Fig. 5
                         shows a framework for the benchmark FER approach. For an input image or the first frame in a video, the face region and 68 facial points are detected using multi-view versions of the Viola–Jones (VJ) detector and the active shape model (ASM) respectively. The facial points in subsequent frames are then tracked using the same ASM, and only these successfully tracked frames are selected for feature extraction. SIFT descriptors are extracted around 53 interior facial points and further concatenated into a single vector representing texture features. A subset of the most discriminative features is selected using the minimal redundancy maximal relevance criterion (mRMR) algorithm. Geometric features composed of 43 distances defined using an ASM and FAPs are also extracted. A feature-level fusion of the top 200 SIFT texture subset and 43 FAP-based distances (denoted as “SIFT+FAP” in the following sections) is then employed and a support vector machine (SVM) with a radial basis function (RBF) kernel is trained for classifying six basic emotions (ANGer, FEAr, HAPpiness, SADness, SUrPrise, and NEUtral), and three dimensionally categorized emotions (POSitive, NEGative, and NEUtral). DISgust is not included here because of the difficulty in collecting sufficient real-world data for it.

This framework is designed based on our previous experiments [14] where SIFT consistently outperformed LBP and Gabor wavelet as a feature set, and mRMR outperformed Adaboost and SVM for feature selection on multiple FER databases. SIFT has also shown to outperform local phase quantization and discrete cosine transform [15], and mRMR outperforms PCA, mutual information, and genetic algorithm [16]. FAP-based distances have been demonstrated as a sparse, compact, yet information-rich representation of the facial shape [17]. The combination of point-based texture and geometry features has been used in previous studies [18–20] to obtain more robustness to various changes in the face and environment, but such feature fusion has not been investigated on realistic data previously.

The popular VJ detector [21] and ASM [22] are adopted here to detect facial regions and 68 fiducial facial points as shown in Fig. 6a, respectively. Among various types of facial landmark detection models, such as constrained local model (CLM) [23] and mixtures of parts [24], we chose to use ASM as it is arguably the simplest and fastest method among deformable models, and is also known to generalize well to new subjects due to its simplicity [25]. Additionally, it has shown good performance in detecting and tracking facial points with good robustness to reasonable face movements and facial variations [26]. The ASM implementation in the ASMLibrary [27] is used here. To achieve more robust to pose variations, multi-view versions of the VJ detector and ASM are built. The multi-view VJ detector is built in such a way that the frontal, left and right profile detectors available in OpenCV are applied consecutively when no face is found by previous detectors. The right profile is implemented by applying the left profile detector on left-right clipped images. To train the multi-view ASM, three image sets were collected from the Internet covering different natural emotions, corresponding to pose ranges of [−60, 20], [−20, 20], and [20, 60] degrees, respectively. Each set has approximately 100 images and a sample set is shown in Fig. 6b. Then 68 points are manually annotated with x and y locations, and used to train the ASM. The trained ASM is subject-independent in the sense that the training images do not appear in the FER experimental data. The points in consequent frames are tracked using optical flow. In case of frames with failure in tracking, the VJ detector and ASM are applied again. Only the largest facial region in successfully detected images or tracked frames is retained for feature extraction.

To maintain a reasonable degree of tolerance to face movements and pose changes, texture and geometry features are extracted based on 53 interior points detected by ASM (index from 16 to 68 in Fig. 6a). The points on the face boundary are excluded here because they are not always accurately detected and the regions around these points contain background information. Note that the point-based feature extraction approach is based on the assumption that ASM can detect the location of facial points with an acceptable level of accuracy, and both geometric distances and texture are impacted little by small deviations of the detected location from the real location of facial points. In the case that ASM fails to detect facial points accurately, the accuracy of the extracted texture and geometric features will be severely impacted.

SIFT features [28] are extracted around each of 53 interior points. The features of all 53 points are then combined into a texture feature vector. Following the settings in [29], the SIFT descriptor is computed from the gradient vector histograms of the pixels in a 4×4 patch around each point of 53 interior points. Instead of setting a fixed orientation, the program is allowed to compute the eight possible gradient orientations. Therefore, each descriptor contains a total size of 128 elements. By computing one such descriptor at each point, a final feature vector with 6784 elements is obtained.

Geometric features include 43 distances between 53 interior points as listed in Table 7
                              . These distances are calculated based on facial animation parameters (FAPs) [30] defined in the ISO MPEG-4 standard (part 2, visual), which provide a concise representation of the evolution of the expression of the face. Because the ASM produces several points on the eyebrow that are around the middle, there are several features for FAP No. 33 (marked FAP 33*). Similarly, there are multiple distances for FAPs No. 34, 61 and 62. Furthermore, FAPs also can handle arbitrary faces through the use of FAP units. Compared with the commonly used facial movement vectors, distances have the merits of being robust to translations and rotations of the facial geometry, and do not require compensation for face movements. Therefore, they are suitable for working on real-world data.

The obtained texture vector has a much higher dimension than the geometry vector. To keep a comparable dimension between these two vectors and also remove possible noise, the mRMR [31] algorithm is used to select a subset of the most discriminative texture features. The algorithm selects a subset of features that jointly have the largest dependence on the ground truth class and the least redundancy among the features. From a continuous input feature D
                           
                              k
                           , its discrete version 
                              
                                 
                                    D
                                    ¯
                                 
                                 k
                              
                            is obtained based on the mean value μ
                           
                              k
                            and the standard deviation σ
                           
                              k
                            of all features:
                              
                                 (1)
                                 
                                    
                                       
                                          
                                             D
                                             ¯
                                          
                                          k
                                       
                                       =
                                       
                                          
                                             
                                                
                                                   −
                                                   2
                                                
                                                
                                                   if
                                                   
                                                
                                                
                                                
                                                   
                                                      D
                                                      k
                                                   
                                                   <
                                                   
                                                      μ
                                                      k
                                                   
                                                   −
                                                   σ
                                                   *
                                                   
                                                      σ
                                                      k
                                                   
                                                   
                                                
                                             
                                             
                                                
                                                   0
                                                
                                                
                                                   if
                                                
                                                
                                                   
                                                      μ
                                                      k
                                                   
                                                   −
                                                   σ
                                                   *
                                                
                                                
                                                   
                                                      σ
                                                      k
                                                   
                                                   ≤
                                                   
                                                      D
                                                      k
                                                   
                                                   ≤
                                                   
                                                      μ
                                                      k
                                                   
                                                   +
                                                   σ
                                                   *
                                                   
                                                      σ
                                                      k
                                                   
                                                
                                             
                                             
                                                
                                                   2
                                                
                                                
                                                   if
                                                
                                                
                                                
                                                   
                                                      D
                                                      k
                                                   
                                                   >
                                                   
                                                      μ
                                                      k
                                                   
                                                   +
                                                   σ
                                                   *
                                                   
                                                      σ
                                                      k
                                                   
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where σ is set to 0.5, as recommended in [31]. A discretization of continuous variables typically leads to better results than directly using continuous variables [31]. The algorithm returns a list of selected features starting from the most to the least importance, and the number of the selected features is set to 200.

In feature-level fusion, discriminative texture and geometric feature vectors are normalized individually into a range of [0, 1] by dividing by the corresponding maximum value among all elements. The normalized vectors are then concatenated into one fused vector.

SVM is a supervised learning algorithm that is widely used for analyzing data and recognizing patterns. The implementation of a multiple-class SVM with a RBF kernel in the LIBSVM [32] is adopted (type of SVM: C-SVC, and parameters: cost C=1 and gamma=1/num_features). The multiple-emotion-class problem is solved by the one-versus-all strategy and the average accuracy over all emotions is used as the final result. A frame-by-frame strategy is adopted in video based classification for its simplicity, direct extension from image based methods and ease of implementation. This strategy avoids many challenges in real-world video such as finding points to segment video and handling transitions from one emotion to another, and has been successfully used previously [33]. A majority voting among all frames is then used to obtain the emotion label for the corresponding video. Let E
                           
                              i
                            (i
                           =1, 2, …, N) be the emotion classes and E
                           
                              i,j
                            be the emotion class produced by SVM for the jth frame (j
                           =1, 2, …, M) in a video clip C, where N and M are the total number of emotions and clip frames, respectively. The clip C is classified as having the emotion E
                           
                              i
                            that has the majority voting among all frames:
                              
                                 (2)
                                 
                                    
                                       C
                                       ∈
                                       
                                          E
                                          i
                                       
                                       
                                       if
                                       
                                       
                                          E
                                          i
                                       
                                       =
                                       max
                                       
                                          
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                M
                                             
                                             
                                                
                                                   E
                                                   
                                                      i
                                                      ,
                                                      j
                                                   
                                                
                                             
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        


                           Fig. 7
                            shows the accuracy in classifying six basic and three categorized emotions. For six emotions, a feature-level fusion of the top 200 SIFT texture subset and FAP-based distance features (denoted as “SIFT+FAP”) shows stable accuracy around 70%, and at least 15% higher overall performance than using SIFT or FAP alone. When three categorized emotions are used, the overall performance of SIFT+FAP becomes slightly lower (around 65%), but it is still 20% higher than those of SIFT or FAP alone. This confirms substantial performance improvements in realistic images using fused features. The lower accuracy for classifying three categorized emotions is probably due to: (a) positive, neutral, and negative image sets contain more subtle emotions, which add confusion to the SVM classifier; (b) the dataset contains a large proportion of happy images, which generally have high accuracy (shown in Table 8A). By contrast, classification of fewer emotions often leads to higher accuracy on lab-based data [34].


                           Table 8 shows the confusion matrix of the classified six basic and three categorized emotions. Among six emotions, happiness is the easiest emotion for classification with 85.7% accuracy, which is followed by surprise and neutral. In contrast, fear and anger are the most difficult ones. This agrees with previous results using both public lab-based databases [20,34] and realistic video [3]. Fear images are mostly misclassified as surprise and neutral, which is expected because realistic fear is found to be often mixed with surprise [3]. Furthermore, realistic neutral images also contain richer texture (e.g. wrinkles) than lab-based images. Anger is mostly misclassified as happiness. This is probably because realistic angry faces often have an open mouth, which is also a characteristic of happiness. For three emotions, negative is easier than positive and neutral for recognition. Most negative images are well detected with 81.9% accuracy, while neutral is the most difficult one. Neutral is most misclassified as positive and vice versa.


                           Fig. 8
                            shows the classification accuracy on video clips based on the top 200 SIFT features selected by mRMR. For each cross-validation, the training set comprises all selected images from the corresponding training clips (i.e. the same training set as image based classification). SIFT+FAP outperforms SIFT and FAP, for both six emotions and three emotions. This confirms the benefit of fused features to improve the performance of both image and video based classification. Compared with image based results, SIFT+FAP has lower accuracy for classifying three and six emotions, and the performance advantage of SIFT+FAP over SIFT (or FAP) becomes less significant. This is because real video often contains a substantial amount of frames with an open mouth and eye blinks, and the SVM classifier is trained using static images, which cannot fully reflect continuous emotional intensities in video. Contrary to image based results, classification of three emotions outperforms that of six emotions, which implies that multiple frame features help to discriminate subtle emotions.

Accurate normalization of the size of facial regions is still a prerequisite but challenging task for most FER systems, particularly for realistic data. We evaluate the robustness of the benchmark FER approach to train vs. test image scale variations on QUT clips as shown in Fig. 9
                           , where the training frames have 2, 1, 0.7 and 0.5 times of the size of the testing frames. Note that the testing frames may have different resolution and may have already been sized to half resolution ranging from 240×180 to 512×288pixels (indicated by “×1”). As expected, adopting a larger or the same size of training images results in higher performance than using smaller sizes. SIFT+FAP has less than 10% accuracy decline even when the scale of training images decreases from 1 to 0.5 times that of testing frames. This result provides evidence that the benchmark approach is robust to scale changes between training and test datasets.

The use of smaller scale image brings about significant reductions in the computational time, and the trade-off in the classification accuracy for the increased speed is quite favorable. For instance, when the scale factor decreases from 1 to 0.5, as much as 85% reduction in computation can be achieved by sacrificing only 15% of the accuracy. Thus, SIFT+FAP achieves parsimony in computation while sacrificing little from performance using low-resolution training images. The program was developed using Matlab 7.6.0 under a laptop configuration of core duo 1.66GHz CPU and 2GB memory.

As demonstrated in Fig. 10
                            for sample News clips,
                              3
                           
                           
                              3
                              More demos are available at: http://203.202.17.214/VideoLibrary/Emotion/EmotionDemo.html.
                            the probability generated by the benchmark approach changes smoothly across all frames although each frame is tested separately. The transitions between emotions and facial appearance changes are well reflected by the probability values in overall. It has been observed that eye blinks lead to a lower probability of happiness in Fig. 10a, and the blurred content caused by transitions between frames also results in errors (frame No. 104 in Fig. 10b). It is noted that the frame No. 16 in Fig. 10b is misclassified as happiness, possibly caused by pose variations from previous frames. Our experiments in other clips also show that a ‘speaking mouth’ leads to the most confusion between neutral, surprise and happiness, and between fear and surprise, among other groups of emotions. In the frame No. 76 in Fig. 10b, the youngster's expressions undergo a steep change from a mixture of negative and neutral to a dominant emotion of happiness, which may imply that realistic emotions in the collected TV and web data resources, to some extent, do not strictly follow four temporal segments (i.e. neutral, onset, apex, and offset) as indicated in lab-based data [33].


                        Fig. 11
                         shows the performance of the benchmark approach on the FEEDTUM and NVIE databases. The top 200 SIFT features selected by mRMR are used here. The overall performance is around 61.0% and 82.0% on the two databases respectively. The performance on FEEDTUM images is lower than that on QUT images, which is largely because their emotions are not as exaggerated as those expressed in the QUT-FER database. This indicates the possibility of achieving higher accuracy on realistic data than on lab-based data. The performance in NVIE images is higher than that in QUT images, which is mainly contributed by their near-frontal faces and simplified illumination conditions. For both FEEDTUM and NVIE databases, SIFT+FAP outperforms SIFT or FAP alone. This again confirms the benefit of fusing point-based texture and geometry to improve the performance.

A contrasting difference in the performance has been observed between SIFT+FAP and FAP on the FEEDTUM and NVIE databases. This is probably because NVIE images often have bigger differences in size of the face than FEEDTUM images. Therefore, the normalized FAP features are less likely to be influenced by scale changes compared to SIFT. Additionally, as NVIE images have more exaggeratedly expressed emotions than FEEDTUM images, geometry tends to play a more important role.


                        Table 9
                         demonstrates the confusion matrix for seven emotions on the FEEDTUM database. Similar to the results in Table 8 for QUT images, happiness and surprise are the two easiest recognized emotions, while anger is one of the most difficult ones, for both lab-based and realistic data. Fear appears to be more easily classified correctly on FEEDTUM than on QUT. A contrary result is observed for sadness. The results imply that the same emotion may have contrasting recognition performance depending on the use of lab-based or realistic data. Thus, the nature of data tends to have a significant impact on the classification of fear and sadness, but little on happiness, surprise, and anger.

The differences in the features between train-test data are likely to have a big impact on FER performance. This section investigates the performance across lab-based and realistic data, and across realistic data based on the image part of the QUT database. The top 200 SIFT features selected by mRMR are used in the evaluations.


                           Table 10
                            shows the classification results using cross train-test images across lab-based FEEDTUM and NVIE and realistic QUT images. To keep the same types of emotion classes between train-test datasets, six emotions (i.e. ANG, FEA, HAP, SAD, SUP and NEU) are used for two pairs of FEEDTUM-QUT tests, while five emotions (i.e. ANG, FEA, HAP, SAD and SUP) are for NVIE-QUT tests. In each test, all images from the corresponding training/test database are used.

For both pairs of tests that adopt QUT images as the test data, fusion of SIFT and FAP has higher accuracy than either feature alone, whereas fusion leads to lower accuracy than SIFT for the tests that adopt QUT images as the training data. The result suggests that FAP based distances obtained from public lab-based databases can provide useful information for emotion classification in realistic images, but not vice versa. The reason may lie in the fact that FAP features tend to be “normal” in lab-based images where most of faces are frontal, with similar size, and from subjects with similar ages, while faces in realistic images, on the other hand, are often accompanied by pose variations, big size changes and various aged subjects. This agrees with the results in Fig. 4 that expressions in lab-based images are significantly different from those in realistic images. Another possible reason is that the ASM fails to produce accurate facial fiducial point detection, particularly for some realistic data where many large variations are present, and this leads to inaccurate FAP results. For all pairs of tests, SIFT has higher accuracy than FAP.

We also evaluate the performance across realistic QUT and realistic SFEW images. The SFEW [5] database includes 700 images collected from movie programs, which are expected to exhibit different real-world conditions from QUT images. Similar to processing QUT images, the face and fiducial points in all SFEW images are detected by the VJ detector and the trained ASM respectively, and 136 images that failed in face detection or correct fiducial point detection are removed from the evaluation (i.e. the failure rate of VJ and ASM is 19.4%). Table 11
                            illustrates the classification accuracy obtained using train-test images across the two databases, for both seven emotions (i.e. ANG, DIS, FEA, HAP, SAD, SUP and NEU) and six emotions (i.e. ANG, FEA, HAP, SAD, SUP and NEU). For both groups of emotions when QUT images are used for testing, fusion of SIFT and FAP leads to higher accuracy than using either feature alone. By contrast, fusion has inferior performance to using FAP alone when SFEW images are used for testing. The findings suggest that it is not necessary to adopt both texture and geometry for optimal FER performance when the training is done on a different realistic database to the testing. For all four pairs of train-test evaluations, FAP outperforms SIFT, which implies more importance of geometry over texture for FER across realistic data.

To test whether there is any difference in performance of the benchmark approach when texture features other than SIFT are employed, recognition results obtained using Gabor and LBP features are compared. Gabor features are obtained using five scales 
                           
                              
                                 λ
                                 m
                              
                              =
                              4
                              ×
                              
                                 2
                                 
                                    
                                       m
                                       −
                                       1
                                    
                                 
                              
                           
                        ; m
                        =(1,…,5) and eight orientations θ
                        
                           n
                        
                        =
                        π(n−1)/8 Gabor filters. LBP features are computed from a 14×18 patch centered at each point using the uniform patterns LBP
                        8,2
                        
                           u2 with 59 labels [36]. Gabor and LBP features are extracted in the same manner as SIFT for fair comparisons, yielding two feature vectors with 2120 and 2597 elements respectively. The top 200 LBP or Gabor features selected by mRMR are input into SVM classification based on 10 subject-independent disjoint train-test validation tests.


                        Table 12A shows the comparison results. As can be seen, fusion leads to consistently better performance than texture or FAP alone, for all three types of texture and three databases (13%, 30%, 1% improvements for QUT, FEEDTUM and NVIE respectively). LBP, SIFT and Gabor perform similarly on the FEEDTUM database where images are roughly in the same size, while the performance of Gabor is notably inferior to those of LBP and SIFT on the NVIE and QUT databases where images are normally with different sizes. This provides evidence that Gabor is more sensitive to scale changes than SIFT and LBP in both realistic and lab-based datasets. In addition, the performance differences between three types of texture are little impacted by the use of realistic or lab-based data. Among all features, SIFT+FAP is the best overall performer, while LBP+FAP performs the best on the QUT database. However, the performances of SIFT+FAP, LBP+FAP and Gabor+FAP are comparable and not statistically significantly different as evidenced by overlapped accuracy intervals (note the deviation intervals have both negative and positive values).

The better performance of SIFT over LBP and Gabor can be explained as follows. SIFT and LBP descriptors represent facial texture using histograms of orientation and histograms of binary values on pixel neighborhoods, respectively. The texture features in this paper are calculated around 53 key points, whose neighborhoods often contain rich orientation information which is more effectively captured by the SFIT descriptor. The most discriminative features from LBP, on the other hand, may not come from these key points, as shown by the LBP bin distribution over the face region in [37]. The higher performance of LBP over SIFT on the QUT database is probably owing to threshold operations in LBP, which have a better tolerance to big illumination variations in realistic images than the vector normalization in the SIFT algorithm. Gabor features only have a small degree of tolerance to illumination variations, and are also sensitive to shifting of key points from inaccurate ASM detection and scale changes [36].


                        Table 12B displays the average computational time used for calculating LBP, SIFT and Gabor features in an image. As expected, calculating SIFT demands the most time as it contains computationally expensive steps, such as difference of Gaussian images, while LBP requires the least time. Therefore, SIFT and LBP are recommended respectively for the cases that only accuracy or both accuracy and time are important.

To give an indication of the performance of the benchmark approach on lab-based and realistic data, we also compare the results to the performance reported in previous studies. Table 13A shows the comparison results on the FEEDTUM database using SVM classifier. The benchmark method using SIFT+FAP obtains 1.3%, 29.4%, 20.7%, and 2.0% higher accuracy for classifying seven emotions than the approaches using motion blocks, global motion, DCT, and human perception in [38]. For the classification of six emotions, the SIFT+FAP method has 6.7%, 1.0% and 3.1% higher accuracy than the approaches using raw image, LBP and Gabor in [39], and it also slightly outperforms that using AAM in [40] for classifying five emotions in an image set from only 12 subjects (FEEDTUM has 18 subjects). Note that in [39] only a subset of images with frontal faces and without large head movement are used, and the faces are also registered. Although the distance based method in [41] has higher accuracy than the benchmark approach, it only classifies three emotions.


                        Table 13B illustrates the comparison results on the NVIE database. The benchmark approach outperforms all previous studies, as it obtains 15.2%, 17.7%, and 21.9% higher accuracy in classifying six emotions than those in classifying three emotions using AAM and PCA in [12] and AMM plus thermal features in [42], respectively. Bigger performance improvements (>25%) are also observed when comparing with methods in [43].


                        Table 13C shows performance comparison results on the SFEW and GENKI-4K realistic databases. For fair comparisons, the same evaluation protocols defined for the two databases in [5,7], respectively, are used for obtaining the performance of the SIFT+FAP approach. The same SVM classifier is used for the different sets of features as indicated in the ‘feature’ column of Table 13C. For the strictly person independent (SPI) test on SFEW images, our approach using SIFT+FAP has 7.1% higher accuracy than the baseline classification accuracy (i.e. 19%) in [5], and comparable performance to using HOG features in [44]. The approaches in [7,44] have slightly better performance than the SIFT+FAP approach on GENKI-4K images, but they require strict face registration. Note that in the case that ASM fails to detect facial points to an acceptable level of accuracy, the SIFT+FAP approach will not work as well. Thus, the benchmark approach achieves the highest performance on lab-based data, and is competitive on realistic data.

@&#CONCLUSION@&#

This paper evaluates classification performance and robustness of facial expression recognition algorithms on data obtained from TV broadcasts and web sources and compares such data with lab-collected data. It investigates the influence of factors such as the type of data, the type of feature set, the fusion strategy and the extent of training. A new and large database of FER data was collected and made publicly available to researchers in this field. A complete FER system was used to provide a benchmark for performance and it was observed that
                        
                           (1)
                           Facial expressions in lab-based images are significantly different from more realistic data obtained from TV and web sources, based on comparisons of FAP-based feature distributions.

Classification of positive, negative and neutral emotions is more difficult than that of six basic emotions in the collected QUT FER realistic images, possibly arising from the higher likelihood of mixtures of emotions. This is contrary to the results in the QUT FER video clips and previously reported results [34] on lab-based images where classification of fewer emotions often leads to higher accuracy.

The nature of the data (lab-based vs. realistic) has a big impact on the FER performance for fear and sadness, but little on happiness, surprise, and anger.

Fusion of fiducial point-based texture and geometry leads to consistently better performance when the train-test data splits are from the same database. However, using train-test data splits fusion sometimes leads to lower accuracy. This is observed across different feature sets and different datasets.

Different point-based texture features are not statistically significantly different in performance on either type of data, but SIFT is competitive in performance with Gabor and LBP systems on realistic data and is superior on lab-based data.

FER performance on video frames can be affected by factors such as speech related mouth movements, and emotional intensity variations in a sequence. The facial expressions in video sequences from TV and web sources do not always strictly follow the four temporal segments (i.e. neutral, onset, apex and offset) as indicated previously with many existing lab-based datasets [33].

The benchmark approach used in this work is shown by comparison to be competitive with the best reported performances on many other publicly available datasets. It also achieves parsimony in computation while sacrificing little from performance using low-resolution training images.

For robust and high performance in realistic FER, particular attention should be paid to handling the confusion between emotions caused by mouth movements, eye blinks, transition content, and mixing of emotions. Future work stemming from these investigations can investigate other facial point detection techniques such as constrained local model (CLM) rather than ASM, and the use of these findings in the implementation of FER for real applications such as driver fatigue detection.

@&#ACKNOWLEDGMENTS@&#

We would like to thank the emotion annotators of the QUT FER dataset. This work was supported in part by the Smart Services CRC in Australia.

@&#REFERENCES@&#

