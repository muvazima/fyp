@&#MAIN-TITLE@&#Missing value imputation using decision trees and decision forests by splitting and merging records: Two novel techniques

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Two novel missing value imputation techniques.


                        
                        
                           
                           Justification of the basic concepts of the techniques through some empirical analyses.


                        
                        
                           
                           Experimentation on nine data sets, four evaluation criteria.


                        
                        
                           
                           Comparison with two existing techniques.


                        
                        
                           
                           A complexity analysis of all techniques.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Data pre-processing

Data cleansing

Missing value imputation

Decision tree algorithm

Decision forest algorithm

EM algorithm

@&#ABSTRACT@&#


               
               
                  We present two novel techniques for the imputation of both categorical and numerical missing values. The techniques use decision trees and forests to identify horizontal segments of a data set where the records belonging to a segment have higher similarity and attribute correlations. Using the similarity and correlations, missing values are then imputed. To achieve a higher quality of imputation some segments are merged together using a novel approach. We use nine publicly available data sets to experimentally compare our techniques with a few existing ones in terms of four commonly used evaluation criteria. The experimental results indicate a clear superiority of our techniques based on statistical analyses such as confidence interval.
               
            

@&#INTRODUCTION@&#

Organisations are extremely dependent nowadays on data collection, storage and analysis for various decision-making purposes. Collected data often have incorrect and missing values [1–3]. For imputing a missing value r
                     
                        ij
                      (i.e. the jth attribute value of the ith record r
                     
                        i
                     ) in a data set D
                     
                        F
                     , an existing technique kNNI [4] first finds the k-most similar records of r
                     
                        i
                      from D
                     
                        F
                      by using the Euclidean distance measure. If A
                     
                        j
                     
                     ∈
                     A is a categorical attribute the technique imputes r
                     
                        ij
                      by using the most frequent value of A
                     
                        j
                      (the jth attribute) within the k-Nearest Neighbor (k-NN) records. If A
                     
                        j
                      is numerical the technique then utilizes the mean value of A
                     
                        j
                      for the k-NN records in order to impute r
                     
                        ij
                     . While simplicity is an advantage of kNNI, a major disadvantage of the technique is the fact that for each record having missing value/s it needs to search the whole data set in order to find the k-nearest neighbors. Therefore, the technique can be expensive for a large data set [4]. Moreover, kNNI requires user input for k as it does not find the best k value automatically. It can be difficult for a user to estimate a suitable k value.

Therefore, a suitable k value is often estimated automatically in some existing techniques such as Local Weighted Linear Approximation Imputation (LWLA) [5]. In order to estimate k automatically, LWLA first artificially creates a missing value r
                     
                        il
                      for a record r
                     
                        i
                      that has a real missing value r
                     
                        ij
                     . Note that the original value of r
                     
                        il
                      is known. The technique uses all possible k values, and for each k value it finds the set of k-NN records of r
                     
                        i
                     . For each set of k-NN records, it then imputes the missing value r
                     
                        il
                      using the mean of the lth attribute of all records belonging to the set of k-NN records. Based on the imputed value and the actual value of r
                     
                        il
                     , LWLA calculates the normalized Root Mean Squared Error (RMSE). RMSE is calculated for all sets of k-NN records. The best k value is estimated from the set of k-NN records that produces the minimum RMSE value. Finally LWLA imputes a real missing value r
                     
                        ij
                     , by using each record r
                     
                        m
                      belonging to the set of k-NN records (using the best k) of r
                     
                        i
                     , the similarity between r
                     
                        i
                      and r
                     
                        m
                     , and the jth attribute value of r
                     
                        m
                     .

LWLA uses the most relevant horizontal segment of a data set through the use of the best k-NN records. However, it does not pay attention to the relevance of the attributes and thus it does not divide a data set vertically. A recent technique called “Iterative Bi-Cluster based Local Least Square Imputation” (IBLLS) [6] divides a data set in both horizontal and vertical segments for imputing a numerical missing value, r
                     
                        ij
                     , of a record r
                     
                        i
                     .

Similar to LWLA, it first automatically identifies the k-most similar records for r
                     
                        i
                     . Based on the k-most similar records it then calculates the correlation matrix R for the attributes with available values in r
                     
                        i
                      and the attributes whose values are missing in r
                     
                        i
                     . It next re-calculates a new set of k number of records (for the same k that was automatically identified) using R, and a weighted Euclidean distance (WED) [6]. For the WED calculation the attributes having high correlation with the jth attribute (i.e. the attribute having a missing value for r
                     
                        ij
                     ) are taken more seriously than the attributes having low correlations.

IBLLS further divides the k-NN records vertically by considering only the attributes having high correlations with the jth attribute. Within this partition it then applies an imputation technique called Local Least Square framework [7] in order to impute r
                     
                        ij
                     . This procedure is repeated for imputing each missing value of r
                     
                        i
                     . Similarly, all other records of the data set having missing values are imputed.

Another group of techniques [8–10] uses classifiers such as a neural network for imputation. For example, a technique [10] uses a three layered perceptron network in which the number of neurons in both input and output layers are equal to the number of attributes of the data set D
                     
                        F
                     . The method first generates a perturbed data set, D
                     
                        p
                      from D
                     
                        F
                      by considering some available values as missing. Using the values of D
                     
                        p
                      into the input layer and the values of D
                     
                        F
                      into the output layer, it then trains the network. It finally imputes the missing values of D
                     
                        F
                      using the trained neural network. The performance of a neural network can be improved by using a Genetic Algorithm (GA) in the training process of the network [8].

Genetic Algorithm (GA) can also be used to estimate a suitable set of parameters of a Fuzzy C-means algorithm (FCM) which can then be used to impute missing values [11]. In an existing technique [11], a missing value is first imputed separately using a Support Vector Regression (SVR) and an FCM with user defined parameters. The imputed values are then compared to test their mutual agreement. If the imputed values are not similar then a GA technique is applied to re-estimate the parameters of FCM. The new parameters are used in FCM to impute the missing value again. The process continues until the FCM and SVR imputed values are similar to each other. GA can also be used to estimate suitable parameters for FCM by first artificially creating missing values, and then imputing the artificial missing values by FCM. Once the artificially created missing values are imputed then the difference between the original and imputed values can be used in GA as a fitness measure for further improvement of imputation [11]. Often FCM based imputation techniques are more tolerant of imprecision and uncertainty [12].

Unlike the family of k-NN imputation techniques, EMI [13] takes a global approach in the sense that it imputes missing values using the whole data set, instead of a horizontal segment of it. For imputing a missing value EMI relies on the correlations among the attributes. The techniques using a localized approach, such as the family of k-NN imputation methods, have an advantage of imputing a missing value based on the k nearest neighbors (i.e. k most similar records) instead of all records. However, k-NN techniques find the k nearest neighbors based only on similarity and without considering correlations of attributes within the k nearest neighbors. Since EMI relies on correlations it is more likely to perform better on a data set having high correlations for the attributes. Correlations of the attributes are natural properties of a data set and they cannot be improved or modified for the data set. However, we realize that it is often possible to have horizontal segments (within a data set) where there are higher correlations than the correlations over the whole data set. Hence, the identification of the horizontal segments having high correlations and application of EMI algorithm within the segments is expected to produce a better imputation result.

In this paper, we propose two novel imputation techniques called DMI [14] and SiMI. DMI makes use of any existing decision tree algorithm such as C4.5 [15], and an Expectation Maximisation (EM) based imputation technique called EMI [13]. SiMI is an extension of DMI. It uses any existing decision forest algorithm (such as SysFor [16]) and EMI, along with our novel splitting and merging approach. DMI and SiMI can impute both numerical and categorical attributes. We evaluate our techniques on nine data sets based on four evaluation criteria, namely co-efficient of determination (R
                     2), Index of agreement (d
                     2), root mean squared error (RMSE) and mean absolute error (MAE) [17,18]. Our experimental results indicate that DMI and SiMI perform significantly better than EMI [13] and IBLLS [6].

The organization of the paper is as follows. Section 2 presents our techniques (DMI and SiMI). Experimental results are presented in Section 3. Section 4 gives concluding remarks.

EMI based imputation techniques [13] rely on the correlations of the attributes of a data set. We realize/argue that their imputation accuracies can be high for a data set having high correlations for the attributes. We also realize that perhaps correlations among the attributes can be higher within some horizontal partitions of a data set than within the whole data set. Therefore, we focus on exploring such partitions for higher imputation accuracy. We use a decision tree or a decision forest to identify such natural partitions. A decision tree divides a data set into a number of leaves having sets of mutually exclusive records. A decision forest builds a number of decision trees.

Let us assume that we build a forest having two trees T
                        
                           x
                         and T
                        
                           y
                         (Fig. 1
                        b and c) from a sample data set (Fig. 1a). Suppose a leaf L
                        1 belonging to T
                        
                           x
                         has five records, L
                        1
                        ={R
                        1,
                        R
                        2,
                        R
                        3,
                        R
                        4,
                        R
                        5} (Fig. 1b and d) and a leaf L
                        4 belonging to T
                        
                           y
                         has five records, L
                        4
                        ={R
                        1,
                        R
                        2,
                        R
                        3,
                        R
                        6,
                        R
                        7} (Fig. 1c and e). Records belonging to any leaf are considered to be similar to each other since they share the same or very similar values for the test attributes [19,20]. For example, records belonging to the leaf L
                        1 (in T
                        
                           x
                        ) can be considered to be similar to each other since they share similar values for A
                        
                           i
                         and A
                        
                           j
                        . Similarly, records belonging to L
                        4 (in T
                        
                           y
                        ) are also similar to each other. If we take the intersection of L
                        1 and L
                        4 then we get their common records in the intersection, i.e. L
                        1
                        ∩
                        L
                        4
                        ={R
                        1,
                        R
                        2,
                        R
                        3} (Fig. 1f). Here, R
                        1, R
                        2 and R
                        3 share the same or similar values for the test attributes (A
                        
                           i
                        , A
                        
                           j
                        , A
                        
                           l
                         and A
                        
                           m
                        ) in both T
                        
                           x
                         and T
                        
                           y
                        , whereas records belonging to the leaf L
                        1 only share the same or similar values for the test attributes (A
                        
                           i
                        , and A
                        
                           j
                        ) in the single tree T
                        
                           x
                        . Therefore, we expect that records belonging to a leaf have a strong similarity with high correlations for the attributes, and records belonging to an intersection have even stronger similarity and higher correlations.

We now empirically analyse the concepts on a real data set. Applying C4.5 algorithm [15] on the Credit Approval (CA) data set [21] we build a decision tree, which happens to have seven leaves. We then compute the correlations among the attributes for all records of Credit Approval, and for the records belonging to each leaf separately. That is, first we compute the correlation 
                           
                              
                                 
                                    C
                                 
                                 
                                    ij
                                 
                                 
                                    W
                                 
                              
                           
                         between two attributes, say A
                        
                           i
                         and A
                        
                           j
                         for the whole data set (i.e. for all records). We then compute the correlation 
                           
                              
                                 
                                    C
                                 
                                 
                                    ij
                                 
                                 
                                    L
                                 
                              
                           
                         between the same attributes A
                        
                           i
                         and A
                        
                           j
                         using only the records within a leaf L, instead of all records in the data set. If the absolute value of 
                           
                              
                                 
                                    C
                                 
                                 
                                    ij
                                 
                                 
                                    L
                                 
                              
                           
                         is greater than the absolute value of 
                           
                              
                                 
                                    C
                                 
                                 
                                    ij
                                 
                                 
                                    W
                                 
                              
                           
                         then we consider 
                           
                              
                                 
                                    C
                                 
                                 
                                    ij
                                 
                                 
                                    L
                                 
                              
                           
                         as a better correlation than 
                           
                              
                                 
                                    C
                                 
                                 
                                    ij
                                 
                                 
                                    W
                                 
                              
                           
                        .

For the six numerical attributes of the CA data set there are 15 possible pairs (6 choose 2) among the attributes, and therefore 15 possible correlations. Column B shows the number of pairs of attributes A
                        
                           i
                         and A
                        
                           j
                         for a leaf L (shown in Column A) where we get 
                           
                              
                                 
                                    C
                                 
                                 
                                    ij
                                 
                                 
                                    L
                                 
                              
                              >
                              
                                 
                                    C
                                 
                                 
                                    ij
                                 
                                 
                                    W
                                 
                              
                           
                        . For example, for Leaf 1 (as shown in Column A) we get 11 pairs of attributes (out of 15 pairs) that have higher correlations within the leaf than the whole data set. The first three columns, of Table 1
                        , show that for all leaves (except Leaf 2 and Leaf 3), at least 11 out of 15 pairs have higher correlations within a leaf. Leaf 2 and Leaf 3 have a small number of records, as shown in Column C.

A decision forest can be used for identifying records with even better similarity (among the records) and higher correlations (among the attributes). To investigate the concept, we apply an existing forest algorithm called SysFor [16] on the CA data set. For the purpose of demonstration of basic concepts, we build a forest of only two trees from CA and then take the intersections of the leaves. Note that the leaves of a single tree do not contain any common records. Therefore, we only take the intersection of two leaves L
                        
                           i
                         and L
                        
                           j
                         belonging to two different trees T
                        
                           x
                         and T
                        
                           y
                        . Columns D, E and F of Table 1 are derived from the intersections of leaves from two different trees that are not shown due to space limitations. They show higher correlations within the intersections. For example, for Intersection 1 (shown in Column D) there are 13 pairs of attributes (shown in Column E), out of 15 pairs, having higher correlations within the intersection than the correlations within the whole data set. A number of intersections such as Intersection 2 (not shown in the table) have only 1 or no records in them, for which correlation calculation is not possible.

We also calculate similarity among all records of the CA data set, all records within each leaf separately and all records within each intersection separately. Fig. 2
                         indicates that the average similarity of the records within an intersection is higher than the average similarity of the records within a leaf. Additionally, the records within a leaf have higher average similarity than the similarity of the records within the whole data set. Similarity calculation steps are introduced in Step 4 of Section 2.3.

Often the intersections can have a small number of records. We realize that the EMI based imputation can be less effective on such a small sized intersection. In order to explore the impact of an intersection size on the imputation accuracy, we apply EMI on different numbers of records of the Credit Approval and CMC data sets [21]. We first artificially create a missing value in a record, and then create a sub-data set of size τ by taking τ-nearest neighbors (τ-NNs) of the record having a missing value. We then apply EMI on each sub-data set to impute the missing value. Fig. 3
                         shows that both CMC and Credit Approval achieve high accuracy for τ
                        =25 in terms of RMSE evaluation criteria. The accuracy then does not increase drastically with the increase of size any more.

In order to avoid applying EMI on a small sized intersection, another basic concept of our technique is to merge a small sized intersection with a suitable target intersection in such a way so that the merged intersection produces the best possible correlations and average similarity among the records. The merging process is shown in Fig. 1f and g, where the record R10 is merged with the records R8 and R9 in order to increase the similarity and correlations. The merging process is presented in details in Step 4 of Section 2.3.

We argue that the accuracy of EMI may depend on three factors: (a) size of an intersection, (b) similarity (i.e. inverse distance) among the records of the intersection and (c) correlations among the attributes within the intersection. A suitable size for an intersection can be 25 records as shown in Fig. 3. While merging a small sized intersection with a target intersection we aim to increase both similarity and correlations. In order to explore a balance between similarity and correlations we carry out another initial test, where we first introduce a Similarity–Correlation threshold λ, the value of which varies between 0 and 1. Now, λ
                        =1 indicates that the target intersection is selected based on only the influence of similarity and λ
                        =0 indicates that it is selected by considering only the correlations. We informally test the impact of various λ values on Credit Approval [21] by imputing missing values using our algorithm called SiMI, which we introduce in detail in Section 2.3, with different λ values. Fig. 4
                         shows that we generally get the best imputation accuracy at λ
                        =0.7 for R
                        2, d
                        2, RMSE and MAE evaluation criteria. Since λ is a proportion between similarity and correlation we call it the Similarity–Correlation threshold.

Please note that in the above example, we use a forest of two trees only to demonstrate the basic concepts. However, as shown in Algorithm 2 by default we use 5 trees unless a user defines it otherwise. In all our experiments forests of 5 trees are used. If we use more trees then they will produce a bigger number of intersections where many of them will have a very low number of records resulting in a higher number of intersections requiring merging. Both the creation of a bigger number of intersections and merging many intersections will cause higher computational complexity. Additionally, since many intersections will be merged there is no point in having the huge number of intersections. We also carry out empirical analysis on the CA and CMC data sets to evaluate the impact of the number of trees, k in terms of accuracy of imputation (RMSE), number of intersections (NOI) and execution time (in ms). Fig. 5
                         shows that k
                        =5 gives an overall good result on Credit Approval (CA) and CMC data sets.

Generally τ
                        =25, k
                        =5, λ
                        =0.7 and l
                        =100 can be a good option. However, these values can differ from data set to data set. A possible solution for a user to make a sensible assumption on these values can be to try different values and compare the imputation accuracies. For example, if a user has a data set D
                        
                           F
                         with some missing values then he/she can first produce a pure data set D
                        
                           C
                         by removing the records having missing values. The user can then randomly select attribute values of D
                        
                           C
                         in order to artificially create missing values where the original values are known to him/her. He/she can then use different values of τ, k, λ and l and impute the artificially created missing values. The values of τ, k, λ and l that produce the best imputation accuracy can then be used in imputing the real missing values in D
                        
                           F
                        . A user may run the evaluation many times. For example, he/she can run it 30 times to create 30 data sets with missing values. A similar approach was taken in the literature to find a suitable k value for k-NN based imputation [22].

We first mention the main steps of DMI as follows and then explain each of them in detail.
                           
                              
                                 Step-1:
                              Divide a full data set D
                                 
                                    F
                                  into two sub data sets D
                                 
                                    C
                                  and D
                                 
                                    I
                                 .

Build a set of decision trees on D
                                 
                                    C
                                 .

Assign each record of D
                                 
                                    I
                                  to the leaf where it falls in. Impute categorical missing values.

Impute numerical missing values using EMI algorithm within the leaves.

Combine records to form a completed data set 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      D
                                                   
                                                   
                                                      F
                                                   
                                                   
                                                      ′
                                                   
                                                
                                             
                                          
                                       
                                    
                                  without any missing values.

DMI
                                 
                                    
                                 
                              
                           


                        
                           
                              
                                 Step-1:
                              
                                 Divide a full data set 
                                 D
                                 
                                    F
                                  
                                 into two sub data sets 
                                 D
                                 
                                    C
                                  
                                 and 
                                 D
                                 
                                    I
                                 .

To impute missing values in a data set, we first divide the data set D
                                 
                                    F
                                  into two sub data sets D
                                 
                                    C
                                  and D
                                 
                                    I
                                 , where D
                                 
                                    C
                                  contains records having no missing values and D
                                 
                                    I
                                  contains records having missing values (see Step-1 of Algorithm 1).


                                 Build a set of decision trees on 
                                 D
                                 
                                    C
                                 
                                 .
                              

In this step we first identify attributes A
                                 
                                    i
                                  (1⩽
                                 i
                                 ⩽
                                 M), where M is the total number of attributes in D
                                 
                                    I
                                 , having missing values. We make a temporary copy of D
                                 
                                    C
                                  into 
                                    
                                       
                                          
                                             D
                                          
                                          
                                             C
                                          
                                          
                                             ′
                                          
                                       
                                    
                                 . For each attribute A
                                 
                                    i
                                  we build a decision tree (considering A
                                 
                                    i
                                  as the class attribute) from the sub data set 
                                    
                                       
                                          
                                             D
                                          
                                          
                                             C
                                          
                                          
                                             ′
                                          
                                       
                                    
                                 , using any existing algorithm (such as C4.5 [15]). If A
                                 
                                    i
                                  is a numerical attribute, we first generalize A
                                 
                                    i
                                  of 
                                    
                                       
                                          
                                             D
                                          
                                          
                                             C
                                          
                                          
                                             ′
                                          
                                       
                                    
                                  into N
                                 
                                    C
                                  categories, where N
                                 
                                    C
                                  is the squared root of the domain size of A
                                 
                                    i
                                  (see Step-2 of Algorithm 1).

In Step 2 of Algorithm 1, we also introduce a boolean variable I
                                 
                                    j
                                  with initial value “FALSE” for a data set d
                                 
                                    j
                                  belonging to a leaf, for each leaf of all trees. The boolean variable is used in Step 4 to avoid unnecessary multiple imputation on a data set.


                                 Assign each record of 
                                 D
                                 
                                    I
                                  
                                 to the leaf where it falls in. Impute categorical missing values.
                              

Each record r
                                 
                                    k
                                  from the sub data set D
                                 
                                    I
                                  has missing value/s. If r
                                 
                                    k
                                  has a missing value for the attribute A
                                 
                                    i
                                  then we use the tree T
                                 
                                    i
                                  (that considers A
                                 
                                    i
                                  as the class attribute) in order to identify the leaf R
                                 
                                    j
                                 , where r
                                 
                                    k
                                  falls in. Note that r
                                 
                                    k
                                  falls in R
                                 
                                    j
                                  if the test attribute values of R
                                 
                                    j
                                  match the attribute values of r
                                 
                                    k
                                 . We add r
                                 
                                    k
                                  in the segment d
                                 
                                    j
                                  representing R
                                 
                                    j
                                  of T
                                 
                                    i
                                  (see Step 3 of Algorithm 1). If A
                                 
                                    i
                                  is a categorical attribute then the missing value of A
                                 
                                    i
                                  in r
                                 
                                    k
                                  is imputed by the majority class value of A
                                 
                                    i
                                  in d
                                 
                                    j
                                 . Otherwise, we use the Step 4 to impute numerical missing values.

While identifying the leaf of T
                                 
                                    i
                                  where r
                                 
                                    k
                                  falls in, if we encounter a test attribute the value of which is also missing in r
                                 
                                    k
                                  then we will not be able to identify the exact leaf in T
                                 
                                    i
                                  for r
                                 
                                    k
                                 . In that case we consider any leaf that belongs to the sub-tree of the test attribute the value of which is missing in r
                                 
                                    k
                                 .


                                 Impute numerical missing values using EMI algorithm within the leaves.
                              

We now impute numerical missing values for all records in D
                                 
                                    I
                                  one by one. For a record r
                                 
                                    k
                                  we identify a numerical attribute A
                                 
                                    i
                                  having a missing value. We also identify the data set d
                                 
                                    j
                                  where r
                                 
                                    k
                                  has been added (in Step 3) for the imputation of a missing value in the numerical attribute A
                                 
                                    i
                                 . If d
                                 
                                    j
                                  has not been imputed before (i.e. if I
                                 
                                    j
                                  is FALSE) then we apply the EMI algorithm [13] for imputation and thereby impute the values of A
                                 
                                    i
                                  in d
                                 
                                    j
                                  (see Step-4 of Algorithm 1).

There are two problematic cases where the EMI algorithm needs to be used carefully. First, the EMI algorithm does not work if all records have the same value for a numerical attribute. Second, the EMI algorithm is also not useful when, for a record, all numerical values are missing. DMI initially ignores the attribute having the same value for all records. It also ignores the records having all numerical values missing. DMI then imputes all others values as usual. Finally, it uses the mean value of an attribute to impute a numerical missing value for a record having all numerical values missing. It also uses the mean value of an attribute to impute a missing value belonging to an attribute having the same value for all records.


                                 Combine records to form a completed data set 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      D
                                                   
                                                   
                                                      F
                                                   
                                                   
                                                      ′
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 .
                              

We finally combine D
                                 
                                    C
                                  and D
                                 
                                    I
                                  in order to form 
                                    
                                       
                                          
                                             D
                                          
                                          
                                             F
                                          
                                          
                                             ′
                                          
                                       
                                    
                                 , which is the imputed data set.

We first introduce the main steps of SiMI as follows, and then explain each of them in detail.
                           
                              
                                 Step-1:
                              Divide a full data set (D
                                 
                                    F
                                 ) into two sub data sets D
                                 
                                    C
                                  and D
                                 
                                    I
                                 .

Build a decision forest using D
                                 
                                    C
                                 .

Find the intersections of the records belonging to the leaves of the forest.

Merge a small sized intersection with the most suitable intersection.

Impute numerical missing values using EMI algorithm and categorical missing values using the most frequent values.

Combine records to form a completed data set 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      D
                                                   
                                                   
                                                      F
                                                   
                                                   
                                                      ′
                                                   
                                                
                                             
                                          
                                       
                                    
                                  without any missing values.


                        
                           
                              
                                 Step-1:
                              
                                 Divide a full data set (
                                 D
                                 
                                    F
                                 
                                 ) into two sub data sets 
                                 D
                                 
                                    C
                                  
                                 and 
                                 D
                                 
                                    I
                                 
                                 .
                              

Similar to DMI, we first divide a data set D
                                 
                                    F
                                  into two sub data sets D
                                 
                                    C
                                  and D
                                 
                                    I
                                  (see Step-1 of Algorithm 2). In this step, we also use an existing algorithm [23] to estimate the similarity between a pair of categorical values belonging to an attribute, for all pairs and for all categorical attributes. The similarity of a pair of values can be anything between 0 and 1, where 0 means no similarity and 1 means a perfect similarity.


                                 Build a decision forest on 
                                 D
                                 
                                    C
                                 
                                 .
                              

We build a set of k number of decision trees T
                                 ={T
                                 1,
                                 T
                                 2,⋯,
                                 T
                                 
                                    k
                                 } on D
                                 
                                    C
                                  using a decision forest algorithm such as SysFor [16], as shown in Step-2 of Algorithm 2. k is a user defined number.


                                 Find the intersections of the records belonging to the leaves of the forest.
                              

Let the number of leaves for T
                                 1,
                                 T
                                 2,⋯,
                                 T
                                 
                                    k
                                  be L
                                 1,
                                 L
                                 2,⋯,
                                 L
                                 
                                    k
                                 . The total number of possible intersections of all trees is L
                                 1
                                 ×
                                 L
                                 2
                                 ×⋯× 
                                 L
                                 
                                    k
                                 . We then ignore the intersections that do not have any records in it. See Algorithm 2 and Procedure Splitting (see Algorithm 3).

SiMI
                                 
                                    
                                 
                              
                           

Procedure Splitting ()
                                 
                                    
                                 
                              
                           

Procedure Merging ()
                                 
                                    
                                 
                              
                           


                        
                           
                              
                                 Step-4:
                              
                                 Merge a small sized intersection with the most suitable intersection.
                              

We first normalize the numerical values of the whole data set D into D′, and thereby convert the domain of each numerical attribute to [0,1]. The CreateIntersections() function (see Algorithm 4) re-creates all intersections 
                                    
                                       
                                          
                                             Y
                                          
                                          
                                             i
                                          
                                          
                                             ′
                                          
                                       
                                       ;
                                       ∀
                                       i
                                    
                                  from D′. If the size of the smallest intersection is smaller than a user defined intersection size threshold τ then we merge the smallest intersection with the most suitable intersection. The most suitable intersection is the one that maximizes a metric S
                                 
                                    j
                                  (see Algorithm 4) based on the similarity among the records and correlations among the attributes within the merged intersection.

Similarity W
                                 
                                    j
                                  between two intersections is calculated using the average distance of the pairs of records, where the records of a pair belong to two intersections. Distance between two records is calculated using the Euclidean distance for numerical attributes and a similarity based distance for categorical attributes. If similarity between two categorical values i and j is s
                                 
                                    ij
                                  then their distance d
                                 
                                    ij
                                 
                                 =1−
                                 s
                                 
                                    ij
                                 . Similarity of categorical values belonging to an attribute varies between 0 and 1, and is estimated based on an existing technique [23].

Once two intersections are merged, we repeat the same process if the number of records in the smallest intersection is smaller than τ. L
                                 2 norm [24] of the correlation matrix of a merged intersection is calculated by 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         ∑
                                                      
                                                      
                                                         i
                                                         =
                                                         1
                                                      
                                                      
                                                         r
                                                      
                                                   
                                                   
                                                      
                                                         ∑
                                                      
                                                      
                                                         j
                                                         =
                                                         1
                                                      
                                                      
                                                         c
                                                      
                                                   
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         i
                                                         ,
                                                         j
                                                      
                                                      
                                                         2
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             |
                                             A
                                             |
                                             ×
                                             |
                                             A
                                             |
                                          
                                       
                                    
                                 , where r is the number of rows of the correlation matrix, j is the number of columns, x
                                 
                                    i,j
                                  is the value of the ith row and jth column of the correlation matrix, and ∣A∣ is the number of attributes of the data set.


                                 Impute numerical missing values using EMI algorithm and categorical missing values using the most frequent values.
                              

Missing values of all records in D
                                 
                                    I
                                  are imputed one by one. For a record r
                                 
                                    k
                                 
                                 ∈
                                 D
                                 
                                    I
                                 , we first identify the intersection Y
                                 
                                    j
                                 
                                 ∈
                                 Y, where r
                                 
                                    k
                                  belongs to. If a missing attribute A
                                 
                                    i
                                  is numerical then we apply the EMI algorithm [13] for imputing all numerical missing values in Y
                                 
                                    j
                                 . If the attribute A
                                 
                                    i
                                  is categorical, we use the most frequent value of A
                                 
                                    i
                                  in Y
                                 
                                    j
                                  as the imputed value of A
                                 
                                    i
                                 .

We finally combine D
                                 
                                    C
                                  and D
                                 
                                    I
                                  in order to form a completed data set 
                                    
                                       
                                          
                                             D
                                          
                                          
                                             F
                                          
                                          
                                             ′
                                          
                                       
                                    
                                  without any missing values.

We now analyse complexity for DMI, SiMI, EMI and IBLLS. We consider that we have a data set with n records, and m attributes. We also consider that there are m′ attributes with missing values over the whole data set, n
                        
                           I
                         records with one or more missing values, and n
                        
                           c
                         records (n
                        
                           c
                        
                        =
                        n
                        −
                        n
                        
                           I
                        ) with no missing values. DMI and SiMI uses the C4.5 algorithm [25,15] to build decision trees, which requires a user input on the minimum number of records in a leaf. We consider this to be l.

First we analyse the complexity of DMI. Step 2 of the DMI algorithm uses the C4.5 algorithm which has a complexity O(n
                        
                           c
                        
                        m
                        2) [26], if it is applied on n
                        
                           c
                         records and m attributes. Step 4 uses the EMI algorithm which has the complexity O(nm
                        2
                        +
                        m
                        3), if EMI is applied on n records and m attributes. However, in Step 4 EMI is applied repeatedly on different data segments representing the leaves or logic rules. In the worst case scenario for the ith tree we can have at most 
                           
                              
                                 
                                    
                                       
                                          n
                                       
                                       
                                          c
                                       
                                    
                                 
                                 
                                    l
                                 
                              
                           
                         leaves where the minimum number of records for a leaf is l. If 
                           
                              
                                 
                                    n
                                 
                                 
                                    I
                                 
                              
                              ⩽
                              
                                 
                                    
                                       
                                          n
                                       
                                       
                                          c
                                       
                                    
                                 
                                 
                                    l
                                 
                              
                           
                         then EMI is applied at most n
                        
                           I
                         times, else if 
                           
                              
                                 
                                    n
                                 
                                 
                                    I
                                 
                              
                              >
                              
                                 
                                    
                                       
                                          n
                                       
                                       
                                          c
                                       
                                    
                                 
                                 
                                    l
                                 
                              
                           
                         then it is applied at most 
                           
                              
                                 
                                    
                                       
                                          n
                                       
                                       
                                          c
                                       
                                    
                                 
                                 
                                    l
                                 
                              
                           
                         times. Therefore, the maximum times EMI can be applied is 
                           
                              
                                 
                                    
                                       
                                          n
                                       
                                       
                                          c
                                       
                                    
                                 
                                 
                                    l
                                 
                              
                           
                         resulting in the maximum complexity for EMI to be 
                           
                              O
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   n
                                                
                                                
                                                   c
                                                
                                             
                                          
                                          
                                             l
                                          
                                       
                                       (
                                       
                                          
                                             lm
                                          
                                          
                                             2
                                          
                                       
                                       +
                                       
                                          
                                             m
                                          
                                          
                                             3
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        . Considering that there are m′ attributes with missing values resulting in m′ trees (following the for loops of Step 4) the complexity of the step is 
                           
                              O
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   m
                                                
                                                
                                                   ′
                                                
                                             
                                             
                                                
                                                   n
                                                
                                                
                                                   c
                                                
                                             
                                          
                                          
                                             l
                                          
                                       
                                       (
                                       
                                          
                                             lm
                                          
                                          
                                             2
                                          
                                       
                                       +
                                       
                                          
                                             m
                                          
                                          
                                             3
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        .

Generally, l is chosen to be a small number which is significantly smaller than n
                        
                           c
                        . Therefore, we can ignore l from the complexity analysis. Moreover, we assume that n
                        
                           I
                        
                        ≪
                        n
                        
                           c
                        , and n
                        
                           c
                        
                        ≈
                        n. Therefore the overall complexity for Algorithm 1 can be estimated as O(n
                        2
                        mm′+
                        nm
                        3
                        m′). For typical data sets (such as those used in the experiments of this study) having n
                        ≫
                        m the complexity is O(n
                        2). However, for the data sets having m
                        ⩾
                        n the complexity is O(m
                        3) if m′≪
                        m or O(m
                        4) if m′≈
                        m.

We now analyse the complexity of SiMI algorithm (see Algorithm 2). Complexity of Step 1 for preparing D
                        
                           I
                         and D
                        
                           C
                         is O(nm). However, the complexity of similarity calculation [23] in Step 1 is O(nm
                        2
                        d
                        2
                        +
                        m
                        3
                        d
                        5), where we consider that the domain size of each attribute is d.

The overall complexity of Step 2 is 
                           
                              O
                              
                                 
                                    
                                       
                                          
                                             n
                                          
                                          
                                             c
                                          
                                       
                                       
                                          
                                             m
                                          
                                          
                                             2
                                          
                                       
                                       k
                                       +
                                       
                                          
                                             
                                                
                                                   n
                                                
                                                
                                                   c
                                                
                                                
                                                   2
                                                
                                             
                                             mk
                                          
                                          
                                             l
                                          
                                       
                                    
                                 
                              
                           
                        . Step 3 (see Algorithm 3) finds the non-empty intersections of the leaves for k trees. However, note that the number of non-empty intersections can only be at most n in the worst case scenario, and therefore L (in the algorithm) can be at most equal to n. We consider that each leaf has the minimum number of records (i.e. l) in order to allow us to consider the worst case scenario where the number of leaves is the maximum i.e. 
                           
                              
                                 
                                    n
                                 
                                 
                                    l
                                 
                              
                           
                        . Therefore, the complexity of this step is O(n
                        2
                        lk).

Step 4 (see Algorithm 4) can have two extreme situations where initially we can have n number of intersections, each having 1 record. After merging the records we may eventually end up in another extreme scenario where we have only two intersections; one having τ records and the other one having (n
                        −
                        τ) records. Considering both situations in two iterations of the while loop we find the worst case complexity of the step is O(n
                        2
                        m
                        +
                        nm
                        2).

In Step 5 we can have at most 
                           
                              
                                 
                                    n
                                 
                                 
                                    τ
                                 
                              
                           
                         number of intersections altogether. Therefore, EMI algorithm may need to be applied at most 
                           
                              
                                 
                                    n
                                 
                                 
                                    τ
                                 
                              
                           
                         times resulting in the complexity of the step to be 
                           
                              O
                              
                                 
                                    
                                       
                                          
                                             n
                                          
                                          
                                             τ
                                          
                                       
                                       (
                                       τ
                                       
                                          
                                             m
                                          
                                          
                                             2
                                          
                                       
                                       +
                                       
                                          
                                             m
                                          
                                          
                                             3
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        . Considering τ to be very small the complexity of the Step is O(nm
                        3). The complexity of Step 6 is O(nm).

Typically, d, k, l and τ values are very small, especially compared to n. Besides, we can also consider n
                        
                           I
                         to be very small and therefore n
                        
                           c
                        
                        ≈
                        n. Hence, the overall complexity of SiMI is O(n
                        2
                        m
                        +
                        nm
                        3). Moreover, for low dimensional data sets such as those used in this study the complexity is O(n
                        2). We estimate the complexities of EMI and IBLLS (i.e. the techniques that we use in the experiments of this study) as O(nm
                        2
                        +
                        m
                        3) and O(n
                        3
                        m
                        2
                        +
                        nm
                        4), respectively. This is also reflected in the execution time complexity analysis in the next section (see Table 6).

We implement our imputation techniques i.e. DMI and SiMI and two existing techniques, namely EMI [13] and IBLLS [6]. The existing techniques were shown in the literature to be better than many other techniques including Bayesian principal component analysis (BPCA) [27], LLSI [7], and ILLSI [28].

We apply the techniques on nine real life data sets as shown in Table 2
                        . The data sets are publicly available in UCI Machine Learning Repository [21]. There are some data sets that already contain missing values as indicated by the column called “Missing” in Table 2. For the purpose of our experiments, we first need a data set without any missing values to start with. Therefore, we remove the records having any missing values in order to prepare a data set without any missing values. We then artificially create missing values, in the data set, which are imputed by the different techniques. Since the original values of the artificially created missing data are known to us, we can easily evaluate the accuracy/performance of the imputation techniques.

The performance of an imputation technique typically depends on both the characteristics/patterns of missing data and the amount of missing data [17]. Therefore, we use various types of missing values [14,17,29] and different amount of missing values as explained below.

We consider the simple, medium, complex and blended missing patterns. In a simple pattern we consider that a record can have at most one missing value, whereas in a medium pattern if a record has any missing values then it has minimum 2 attributes with missing values and can have up to 50% of the attributes with missing values. Similarly, a record having missing values in a complex pattern has minimum 50% and maximum 80% attributes with missing values. In a blended pattern we have a mixture of records from all three other patterns. A blended pattern contains 25%, 50%, and 25% records having missing values in the simple pattern, medium pattern and complex pattern, respectively [17]. Additionally, for each of the missing patterns, we use different missing ratios (1%, 3%, 5% and 10%) where x% missing ratio means x% of the total attribute values (not records) of a data set are missing.

We also use two types of missing models namely Overall and Uniformly Distributed (UD) [14]. In the overall model, missing values are not necessarily equally spread out among the attributes, and in the worst case scenario all missing values of a data set can even belong to a single attribute. However, in the UD model each attribute has equal number of missing values.

Note that there are 32 combinations (4 missing patterns×4 missing ratios×2 missing models) of the types of missing values. For each combination, missing values are created randomly. Due to the randomness, every time we create missing values in a data set we are likely to have a different set of missing values. Therefore, for each combination we create 10 data sets with missing values, i.e. we create all together 320 data sets (i.e. 32 combinations×10 data sets per combination) with missing values for each natural data set.

Due to the space limitation we present summarized results in Tables 3 and 4
                        
                        . For each data set, we aggregate the results based on missing ratios, missing models, and missing patterns. Aggregated result for 1% missing ratio means the average value for all combinations having 1% missing ratio, any missing pattern and any missing model. Bold values represent the best results and the values with italic font represent the second best results. It is clear from the tables that SiMI performs significantly better than other three techniques. SiMI and DMI never lose to EMI and IBLLS even for a single case for R
                        2, d
                        2 and RMSE. They only lose twice for MAE.

Since due to space limitations we cannot present the detailed non-aggregated result, we present Table 5
                         that displays the number of times a technique performs the best for an evaluation criteria. There are nine data sets and 32 combinations per data set, resulting in 288 combinations altogether. SiMI performs the best in 243 out of 288 combinations for R
                        2. DMI performs the best in the remaining 45 combinations.

In Fig. 6
                         we present line graphs for average d
                        2 values and 95% confidence intervals for all four techniques and all combinations with the Simple pattern. There are only three combinations that are encircled, where SiMI has either “lower averge”, or “higher average, but overlapping confidence intervals” with DMI. Obviously, DMI performs the second best. Higher average d
                        2 values and non-overlapping confidence intervals indicate a statistically significant superiority of SiMI and DMI over existing techniques. Confidence interval analysis for all other patterns (not presented here) also demonstrates a similar trend. We also perform a t-test analysis, at p
                        =0.0005, suggesting a significantly better result by SiMI and DMI over EMI and IBLLS. For the comparison between SiMI and IBLLS, the t values are higher than the t-ref value for all evaluation criteria on all data sets. For the comparison between SiMI and EMI, the t values are also higher than the t-ref value for all evaluation criteria, except some evaluation criteria on the CMC and CA data sets.


                        Fig. 7
                         presents the average performance indicators for 320 data sets (i.e. 32 missing combinations×10 data sets/combinations) for all techniques. SiMI and DMI clearly perform better than the existing techniques.

Another advantage of SiMI and DMI is that unlike EMI and IBLLS, both of them can impute categorical missing values in addition to numerical missing values. Therefore, we also compare the imputation accuracy of SiMI and DMI based on RMSE and MAE for categorical missing values as shown in Fig. 8
                        . In the figure we present the overall average (for all 32 combinations) of RMSE and MAE values for SiMI and DMI on all nine data sets.

We now present the average execution time (in milliseconds) for 320 data sets (32 combinations×10 data sets per combination) for each real data set in Table 6
                        . We carry out the experiments using two different machines. However, for one data set we use the same machine for all techniques. The configuration of Machine 1 is 4×8 core Intel E7-8837 Xeon processors, 256GB RAM. The configuration of Machine 2 is Intel Core i5 processor with 2.67GHz speed and 4GB RAM. Both SiMI and DMI take less time than IBLLS, whereas they take more time than EMI to pay the cost of a significantly better quality imputation.

@&#CONCLUSION@&#

We argue that real life data sets are likely to have horizontal segments, where records within a segment have higher similarity and attribute correlations than the similarity and correlations of the whole data set. We empirically justify that decision trees and forests can be used to explore such segments. Additionally, the use of the segments for imputing missing values can improve the imputation accuracy. However, if the size of a segment is very small then it can result in a low imputation accuracy. Therefore, we use another novel approach to merge a small sized segment with a suitable segment in such a way so that the similarity among the records and correlations among the attributes increase within the merged segment. Numerical and categorical missing values are then imputed using the records of a segment.

Both of our proposed techniques have basic differences with the existing techniques that we have used in this study. An existing technique called EMI imputes a missing value by using all records of a data set. Whereas the proposed techniques (DMI and SiMI) impute a missing value by using only a group of records (instead of all records) that are similar to the record having the missing value. Another existing technique IBLLS also imputes a missing value by using the similar records. However, it also uses only a subset of all the attributes where the correlation between the attribute having the missing value and each attribute belonging to the subset is high. It completely ignores all other attributes having a correlation lower than a threshold value. On the other hand, instead of totally ignoring the attributes with low correlation both DMI and SiMI consider all attributes according to their correlation strength. An attribute having higher correlation has higher influence in the imputation than another attribute having a lower correlation. Additionally, IBLLS uses k-NN approach to find the similar records, whereas our techniques use decision trees and forests for finding similar records. The correlations of the attributes and the similarities of the records are generally high when we use trees and forests to find the similar records.

In addition to two novel imputation techniques, we also present a complexity analysis of our techniques. Nine publicly available data sets are used for experimentation which suggests a clear superiority of our techniques as indicated in Table 5, and Fig. 7, along with other tables and figures. However, both DMI and SiMI have complexity of O(n
                     2) for low dimensional data sets, and O(m
                     3) for high dimensional data sets. Therefore, our future research plans include a further improvement of our techniques for achieving a better computational complexity, and memory usage. We also plan to explore the suitability of our algorithms for parallel processing.

@&#REFERENCES@&#

