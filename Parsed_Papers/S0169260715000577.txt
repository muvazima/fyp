@&#MAIN-TITLE@&#A hybrid classifier combining Borderline-SMOTE with AIRS algorithm for estimating brain metastasis from lung cancer: A case study in Taiwan

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Developing a new classifier approach by combining borderline SMOTE and AIRS.


                        
                        
                           
                           Eight medical datasets from UCI machine learning repository were evaluated.


                        
                        
                           
                           Outperform well-known classifiers.


                        
                        
                           
                           The proposed method can handle imbalanced class problems efficiently.


                        
                        
                           
                           Applying successfully to the prediction of lung cancer to brain metastasis.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Artificial immune recognition system

Brain metastasis

Imbalance dataset

Lung cancer

Borderline-synthetic minority over sampling technique

@&#ABSTRACT@&#


               
               
                  Classifying imbalanced data in medical informatics is challenging. Motivated by this issue, this study develops a classifier approach denoted as BSMAIRS. This approach combines borderline synthetic minority oversampling technique (BSM) and artificial immune recognition system (AIRS) as global optimization searcher with the nearest neighbor algorithm used as a local classifier. Eight electronic medical datasets collected from University of California, Irvine (UCI) machine learning repository were used to evaluate the effectiveness and to justify the performance of the proposed BSMAIRS. Comparisons with several well-known classifiers were conducted based on accuracy, sensitivity, specificity, and G-mean. Statistical results concluded that BSMAIRS can be used as an efficient method to handle imbalanced class problems. To further confirm its performance, BSMAIRS was applied to real imbalanced medical data of lung cancer metastasis to the brain that were collected from National Health Insurance Research Database, Taiwan. This application can function as a supplementary tool for doctors in the early diagnosis of brain metastasis from lung cancer.
               
            

@&#INTRODUCTION@&#

Lung cancer is known as the major common cause of cancer-related death in the world compared with other cancers. This cancer is the leading cause of cancer deaths for men and women and overcomes prostate cancer in men and breast cancer in women. About 1.4 million deaths in the world are caused by lung cancer [1]. In Taiwan, lung cancer is the second leading cause of cancer deaths with 8094 cases per 100,000 people [2]. About 95% of the people diagnosed with lung cancer develop symptoms related to the disease; however, the disease occurs late in the cancerous process. Metastasis by lung cancer denotes that the lung cancer cell has spread to other organs. Lung cancer is the most common type of cancer that spreads to the brain and can spread within 4 months to 12 months since diagnosis [3]. Metastasis to the brain develops in 20% to 40% of all patients with lung cancer, and 33% of these cases have no symptoms of brain metastasis [4]. Patients diagnosed with brain metastasis have relatively short survival rates [5].

Since 1995, Taiwan government initiated the National Health Insurance for every Taiwan residents. In 1998, the National Health Research Institute (NHRI) established an NHI research database, manage by the Bureau of National Health Insurance (BNHI). This dataset contains the insured claims data of all registered patients both inpatients and outpatients. This valuable database has been used by many researchers [6,7] with collaboration with medical doctors to find the useful information that can improve the healthcare in Taiwan.

Considering the high mortality rate of lung cancer patients and the threat of brain metastasis in Taiwan, we have been motivated to utilize data mining techniques for cancer and metastasis classification tasks by using the data of patients from National Health Insurance Research Database, Taiwan [8]. This classification model can assist doctors in the early diagnosis/prediction of lung cancer and its metastasis to brain.

Among all metaheuristics, artificial immune recognition system (AIRS) is a popular algorithm that inspired by the immune algorithm has been used extensively in medical classification problems [9–11]. Due to its popularity, many researchers have employed AIRS in the application of medical diagnosis and prognosis. Saidi et al. [11] developed AIRS for diabetes diagnoses, Polat and Günes [9] and Polat et al. [10] utilized AIRS for breast cancer and liver disorders diagnosis. Basing on tests on standard and specific case datasets, researchers have discovered that AIRS can be applied successfully as supervised classification algorithm [12].

However, the problem in the medical datasets occurs when one target class is much higher than the other class. This condition is known as the imbalanced data problem [13], which cannot be easily resolved by naïve metaheuristics. Most of the techniques tend to generalize the patterns observed over the majority of the data (i.e., the survival population) and ignore those observed patterns over the small portions of the data. Considering this condition, negative effect on classification performance transpires, which is particularly harmful to life-threatening decisions, such as cancer-related issues.

Synthetic minority oversampling technique (SMOTE), which generates minority class within overlapping regions, is one of the promising approaches that deal with the class imbalance problem. SMOTE has been used to solve imbalanced dataset problems in some medical areas, such as medical imaging intelligence [14] and prostate cancer staging [15]. Wang and Adrian [16] proposed SAIRS, which is a trial on the combination of SMOTE technique with AIRS algorithm, and tested SAIRS on breast cancer dataset from a UCI public repository. Preliminary results revealed that SAIRS is an efficient method to handle imbalanced class problems in classifying breast cancer dataset.

This study extends the previous research by exploring the capability of Borderline-SMOTE (BSM) in conjunction with AIRS algorithm. The proposed method namely BSMAIRS was applied to predict the occurrence of lung cancer metastasis to the brain (LCBM) for patients in Taiwan. To validate the performance of BSMAIRS, we conduct experiments on eight electronic medical datasets obtained from the UCI databank [17] and compared the performance of the proposed method with the standard version of the AIRS algorithm, AIRS2, SAIRS, a SMOTE+AIRS version, and other well-known classifiers, including clonal selection algorithm, C4.5, and back-propagation neural network (BPNN), support vector machine (SVM) and random forest algorithm. Performance measure is evaluated based on accuracy, sensitivity, specificity, and G-mean. Full factorial design is used to statistically analyze the response data on the case of diagnosing LCBM in patients in Taiwan.

The rest of the paper is organized as follows. Section 2 reviews the related work. Section 3 presents the proposed BSMAIRS approach and justifies the performance by using UCI datasets. Section 4 applies BSMAIRS to the cancer data in Taiwan with regard to LCBM. Finally, Section 5 concludes the paper.

The imbalanced data problem can be solved by using two sampling methods, namely, pre-processing of data by undersampling the majority instance and pre-processing of data by oversampling the minority instance. SMOTE is an oversampling method proposed by Chawla et al. [18] and has been used to solve the imbalance problem [19]. SMOTE oversampled the positive class by creating synthetic instances in the decision regions that are formed by the instance and its k-nearest neighbors (KNN) [20]. Han et al. [21] proposed BSM as an improvement to the existing method. Unlike SMOTE, BSM only creates the synthetic instances near the boundaries. The instances on the borderline or nearby are more likely to be misclassified than those which are far from the borderline. Therefore, in classification task, BSM can help increase the classifier performances.

BSM can perform better than its predecessor [21]. However the utilization of BSM in handling the imbalance class problems need to be explored more.

AIRS is included in the field of Artificial Immune Systems (AIS) and was proposed by Watkins et al. in 2004. AIRS uses the concepts of artificial recognition balls (ARBs), resource limitation, memory cells, and hypermutation. ARBs are essentially B-cells supplemented with information on the resources available in the system. The idea of ARBs that was originally utilized in AIS was proposed by Timmis et al. [22] and further fully utilized in AIRS by Watkins and Timmis [23]. AIRS2 is the extended version of AIRS with main difference in clone mutation procedure. Results between these two algorithms are similar in terms of classification accuracy [12].

In AIRS, antigens are allocated to the closest matching ARB in the pool of ARBs, followed by a competitive stage, in which ARBs either survive or die depending on their fitness with regard to capturing antigens of the right class. Resources are re-allocated throughout the ARBs depending on which ARBs survive or die. Memory cells are produced from the surviving ARBs. At the end of the one-shot approach, the memory cells adopt a KNN-voting method by presenting test samples to all memory cells and reporting the stimulation values returned by each memory cell. AIRS is composed of four main stages, namely, initialization, memory cell identification and ARB generation, competition for resources and development of a candidate memory cell, and memory cell introduction. Fig. 1
                         shows the standard AIRS schema.

KNN is one of the successful techniques used in classification tasks in data mining area [24] and has been widely applied to solve various classification problems. KNN uses Euclidean distance to calculate the differences between the attributes for continuous data [25]. Boggess and Hamaker [26] and Hamaker and Boggess [27] reported that using Euclidian distance for a problem like the Wisconsin breast cancer dataset with many features and the addition of an irrelevant feature caused very small or no loss of accuracy. Considering its simplicity and high convergence speed, this technique has become a popular classifier. Despite its benefit, KNN also has drawback of large memory requirement.

Researchers have exposed some remarkable performance of AIRS with conjunction to classifiers. Saidi et al. [11] proposed modified AIRS2, in which KNN algorithm was replaced with the fuzzy KNN to improve the diagnostic accuracy of diabetes. Majid et al. [28] proposed a hybrid KNN with SVM combining with oversampling technique to predict the occurrences of human breast and colon cancers. Wang and Adrian [16] combined SMOTE and AIRS to predict the occurrence of breast cancer. Their results on several limited datasets from UCI confirmed that AIRS1 can perform better than AIRS2 when combining with SMOTE technique. Thus in this study we will use AIRS1 to combine with the BSM.

AIRS2 is the extended version of AIRS. One of their differences is the way AIRS2 treated the ARB pool as a temporary resource for each antigen. By contrast, AIRS treated the ARB pool as a persistent resource throughout the training scheme. Another important difference is the manner in which clones are mutated. AIRS uses a user-defined mutated rate parameter, whereas AIRS2 uses a concept of somatic hypermutation, in which the received amount of mutant clones is related to its affinity to the antigen [29]. SAIRS is the implementation of SMOTE combining with AIRS1 algorithm. Clonal-G algorithm is included in the immune algorithm, which is used to explain the basic features of an adaptive immune response to an antigenic stimulus. This algorithm indicates that only those cells that can recognize the antigens are selected to proliferate. Clonal-G algorithm has been used to perform machine-learning and pattern recognition tasks and is adapted to solve optimization problems, emphasizing multimodal and combinatorial optimization [30]. C4.5 is often referred as a statistical classifier and was proposed by Ross Quinlan [31] as an extension of his ID3 algorithm. The decision trees generated by C4.5 are popular for solving various classification tasks. BPNN was first introduced by Paul Werbos in his thesis in the 1970s [32]. BPNN functions by training the multi-layered neural networks to learn the appropriate internal representations and will result in knowledge regarding arbitrary mapping of input to output [33]. SVM works by constructing a hyper-plane or a set of hyper-planes in a high or infinite-dimensional space. The idea of SVMs was first introduced by Vladimir Vapnik in 1963 and the standard soft margin was introduce by [34]. It is known as a powerful machine learning techniques that have been extensively used in classification task and also in handling imbalance data problem that are prominent in the real dataset [35,36]. Akbani et al. [36] proposed SVM combining with SDC (SMOTE with different cost). Their results shows that their proposed method outperformed other compared methods. However, the SVM in their method becomes more sensitive to the positive instance than to the negative instances. Random Forest uses bootstrap samples of training data to generate random trees and then form an ensemble. Liu et al. [37] uses the combination of balanced random forest and undersampling technique to dealing with class imbalanced problem. The result shows their method achieved higher AUC, F-measure and G-mean compared to other methods.

@&#METHOD@&#

@&#OVERVIEW@&#

BSMAIRS consists of two main parts, namely, data sampling by BSM and classification by AIRS algorithm. Data sampling by BSM provides a mechanism to eliminate the imbalance that exists in the data by oversampling the minority class on the boundaries and then using the resulting balanced dataset to do the classification by AIRS algorithm. Fig. 2
                         shows the BSMAIRS procedure.

In the BSMAIRS method, we generate the balanced dataset by using the BSM technique that is implemented in Java language.

The BSM technique is briefly stated as follows.
                              
                                 Step 1: Calculate the distance of each of positive and negative samples on the training set T.

Step 2: If the number of negative samples is larger than the positive ones, they will be put into a set called “Danger”. This set is the borderline of the positive class.

Step 3: Generate the synthetic instance from each of the data in Danger.

The next step is to classify the data by using the AIRS classifier.

Population is initialized by normalizing all items in the dataset to obtain a Euclidian distance between the feature vectors of any two items within the range of [0,1]. After normalization, we calculate the affinity threshold according to Eq. (1).
                              
                                 (1)
                                 
                                    
                                       
                                          affinity threshold
                                       
                                       =
                                       
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                
                                                   ∑
                                                   
                                                      j
                                                      =
                                                      j
                                                      +
                                                      1
                                                   
                                                   n
                                                
                                                
                                                   
                                                      affinity
                                                   
                                                   (
                                                   a
                                                   
                                                      g
                                                      1
                                                   
                                                   ,
                                                   a
                                                   
                                                      g
                                                      2
                                                   
                                                   )
                                                
                                             
                                          
                                          
                                             n
                                             (
                                             n
                                             −
                                             1
                                             )
                                             /
                                             2
                                          
                                       
                                    
                                 
                              
                           where n is the number of training data items (antigens), ag
                           
                              i
                            and ag
                           
                              j
                            are the ith and jth training antigen in the antigen training dataset.
                              
                                 (2)
                                 
                                    
                                       
                                          affinity
                                       
                                       (
                                       a
                                       
                                          g
                                          i
                                       
                                       ,
                                       a
                                       
                                          g
                                          j
                                       
                                       )
                                       =
                                       1
                                       −
                                       
                                          Euclidean distance
                                       
                                       (
                                       a
                                       
                                          g
                                          i
                                       
                                       ,
                                       a
                                       
                                          g
                                          j
                                       
                                       )
                                    
                                 
                              
                           Afterward, we create a random base called the memory pool (M) and ARB pool (P) from the training data.


                           Clonal Expansion: Define the affinity to the antigenic pattern for each element of M, that resides in the same class. Select the highest affinity memory cell (mc
                           
                              match
                           ) and clone mc in proportion to its antigenic affinity to add to the set of ARBs (P).
                              
                                 (3)
                                 
                                    
                                       
                                          m
                                       
                                       
                                          
                                             c
                                          
                                          
                                             
                                                match
                                             
                                          
                                       
                                       =
                                       
                                          argmax
                                       
                                       (
                                       
                                          stimulation
                                       
                                       (
                                       
                                          mc
                                       
                                       ,
                                       
                                          ag
                                       
                                       )
                                       )
                                    
                                 
                              
                           
                           
                              
                                 (4)
                                 
                                    
                                       
                                          stimulation
                                       
                                       (
                                       
                                          mc
                                       
                                       ,
                                       
                                          ag
                                       
                                       )
                                       =
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            affinity
                                                         
                                                         (
                                                         
                                                            mc
                                                         
                                                         ,
                                                         
                                                            ag
                                                         
                                                         )
                                                          
                                                         
                                                            if
                                                         
                                                          
                                                         
                                                            mc
                                                         
                                                         .
                                                         
                                                            class
                                                         
                                                         =
                                                         
                                                            ag
                                                         
                                                         .
                                                         
                                                            class
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         1
                                                         −
                                                         
                                                            affinity
                                                         
                                                          
                                                         
                                                            otherwise
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        


                           Affinity Maturation: Mutate each ARB descendant of mc
                           
                              match
                           . Place each mutated ARB into P.


                           
                              
                                 (a)
                                 
                                    Metadynamics of ARBs: Resource allocation mechanism will process each ARBs and will result in some death, and ultimately control the population. Calculate the average stimulation for each ARB, and check for the stopping criteria.
                                       
                                          (5)
                                          
                                             
                                                
                                                   S
                                                   i
                                                
                                                =
                                                
                                                   
                                                      
                                                         ∑
                                                         
                                                            j
                                                            =
                                                            1
                                                         
                                                         
                                                            |
                                                            
                                                               AR
                                                            
                                                            
                                                               
                                                                  B
                                                               
                                                               i
                                                            
                                                            |
                                                         
                                                      
                                                      
                                                         
                                                            ar
                                                         
                                                         
                                                            
                                                               b
                                                            
                                                            j
                                                         
                                                         ⋅
                                                         
                                                            stimulation
                                                         
                                                      
                                                   
                                                   
                                                      |
                                                      
                                                         AR
                                                      
                                                      
                                                         
                                                            B
                                                         
                                                         i
                                                      
                                                      |
                                                   
                                                
                                                
                                                   ar
                                                
                                                
                                                   
                                                      b
                                                   
                                                   j
                                                
                                                ∈
                                                
                                                   AR
                                                
                                                
                                                   
                                                      B
                                                   
                                                   i
                                                
                                             
                                          
                                       
                                    
                                 


                                    Clonal expansion and affinity maturation: Clone and mutate a randomly selected subset of the remaining ARBs in P in proportion to their stimulation level.


                                    Cycle: Repeat from (a) when the average stimulation value of each ARB of the same class as the antigen is less than the given stimulation threshold.


                           Metadynamics of memory cells: Select the ARB with the highest affinity from the last antigenic interaction. ARB with better antigenic pattern will be added to the memory set M. Additionally, if the affinity of mc
                           
                              match
                            and mc-candidate is below the affinity threshold, this ARB will be removed from M. The ARB with the highest affinity will result in high fitness value for the KNN classifier. The value of the affinity threshold in our experiment is set to 0.2. This value is chosen on the basis of the trial and error in the experiment to give the best performance.


                           Cycle: Repeat steps 3.2.3 to 3.2.5 until all antigenic patterns have been presented.

Eight electronic medical datasets, i.e., appendicitis, breast cancer, breast cancer Wisconsin (Breast W), diabetes, Haberman's survival, heart Cleveland (Heart C), heart statlog (Heart S), and hepatitis dataset, with different characteristics were used to evaluate BSMAIRS performance. These datasets were accessed from a UCI machine learning repository [17]. A fivefold cross validation strategy was used to guarantee the impartial comparison of the classification results and prevent generating random results. Table 1
                            summarizes the eight dataset characteristics, whereas the ninth electronic dataset is for the case study in Taiwan that will be addressed in Section 4.

Our proposed method was coded in Java by using Netbeans IDE 6.0 and run in a Core 2 Duo P8400 (2.2GHz) PC equipped with 4 GB of RAM under Windows 7 environment. The BSM was implemented using Java library available from [38].

The parameters used for AIRS and BSMAIRS are as follows: stimulation threshold: 0.9; mutation rate: 0.1; clonal rate: 10; mutation rate: 2; affinity threshold: 0.2; KNN: 5. The parameter was obtained using multiple trials to give the best result.

To evaluate the effectiveness of BSMAIRS, we compared the result of our proposed method with the standard version of AIRS, AIRS2, SAIRS and with three popular classification methods, namely, clonal selection algorithm, C4.5, BPNN, SVM and Random Forest on the basis of certain factors. To be noted that BSMAIRS is the combination of SMOTE technique with AIRS algorithm. The percentage of SMOTE is similar to those in the BSMAIRS method.

These measures are as follows:
                              
                                 (6)
                                 
                                    
                                       
                                          Accuracy
                                       
                                       =
                                       
                                          
                                             T
                                             P
                                             +
                                             T
                                             N
                                          
                                          
                                             T
                                             P
                                             +
                                             T
                                             N
                                             +
                                             F
                                             P
                                             +
                                             F
                                             N
                                          
                                       
                                    
                                 
                              
                           
                           
                              
                                 (7)
                                 
                                    
                                       
                                          Sensitivity
                                       
                                        
                                       (
                                       S
                                       E
                                       )
                                       =
                                       
                                          
                                             T
                                             P
                                          
                                          
                                             T
                                             P
                                             +
                                             F
                                             N
                                          
                                       
                                    
                                 
                              
                           
                           
                              
                                 (8)
                                 
                                    
                                       
                                          Specificity
                                       
                                        
                                       (
                                       S
                                       P
                                       )
                                       =
                                       
                                          
                                             T
                                             N
                                          
                                          
                                             T
                                             N
                                             +
                                             F
                                             P
                                          
                                       
                                    
                                 
                              
                           
                           
                              
                                 (9)
                                 
                                    
                                       
                                          G-mean
                                       
                                       =
                                       
                                          
                                             
                                                sensitivity
                                             
                                             ×
                                             
                                                specificity
                                             
                                          
                                       
                                    
                                 
                              
                           where TP denotes true positives, TN denotes true negatives, FP denotes false positives, and FN denotes false negatives. These values are often displayed in a confusion matrix, as presented in Table 2
                           .


                           Table 3
                            presents the classification accuracy and comparison with the other benchmark methods. It shows that the BSMAIRS method is superior to the other methods in terms of classification accuracy. Table 4
                            shows the comparison of sensitivity and specificity, whereas Table 5
                            illustrates the G-mean comparison. The details are listed in Appendix. The average results state that BSMAIRS performs better than the other compared methods in terms of accuracy, sensitivity, specificity, and G-mean. To statistically justify the comparison, data were analyzed using Friedman test and Bonferroni–Dunn test [39]. The Friedman test reveals the p-value: 0.000 which means that there is an overall statistically significant difference between the mean ranks of the classifiers performance indexes. Further, this study conducted post hoc Bonferroni–Dunn test to check which groups in particular differ from each other. The Bonferroni–Dunn test controls the family-wise error rate by dividing the alpha by the number of comparisons made (k
                           −1). This significance level is usually used instead of the alpha=0.05. In our comparison we determined the α-level by using the Bonferroni correction expression α
                           :0.05/k, where k is the number of pairwise tests. Since there are 36 pairwise, therefore α
                           =0.001 is used. Tables 6 and 7
                           
                            provides the p-value for this comparison. Statistically, BSMAIRS results showed significant mean differences in accuracy. While for sensitivity, specificity, and G-mean are comparable to SAIRS. Tables 8 and 9
                           
                            shows run time and the memory usage. Figs. 3–6
                           
                           
                           
                            are the box plots for accuracy, SE, SP and G-mean on the UCI dataset. Here, BSMAIRS was considered the best among all compared methods since it has the highest median for all performance indexes. Basing on this superior performance, we will apply BSMAIRS to the real cases of LCBM.

The lungs are vital organs. Working with the heart and circulatory system, the lungs provide life-sustaining oxygen and rid the body of carbon dioxide. Normal lungs have a great reserve capacity to meet the body's need for oxygen across a wide variety of circumstances.

The reserve capacity permits cancerous lung tumors to grow for years without compromising lung function. Furthermore, the lungs do not have many nerves to transmit pain messages. Therefore, a cancerous lung tumor can grow for many years without causing any symptoms. Thus, most people remain undiagnosed with lung cancer until late in the disease process. Even more unfortunate is the fact that this long period of silent growth gives the cancer the time to spread before it is diagnosed. Lung cancer that has spread beyond the original tumor is difficult to cure [4].

Metastatic brain tumors from lung cancer begin when cancer that is originally located in the lung spreads to the brain. Brain metastasis is a frequent complication in patients suffering from lung cancer and is a significant cause of morbidity and mortality. Brain metastasis is found in about 10% of patients at the time of diagnosis, and approximately 40% of all patients with lung cancer develop brain metastasis during the course of their disease [25]. Patients with lung cancer can develop brain metastasis within 4 months to 12 months, and patients with brain metastasis have relatively short survival (2.3 months to 7.1 months or longer). The standard treatment for brain metastasis has been whole-brain radiation therapy and surgery focusing on symptom palliation. The use of chemotherapy for the treatment of brain metastasis has been limited because of a presumed lack of effectiveness because of the blood–brain barrier.

People with lung cancer cannot be diagnosed without physical examination. Many physical diagnoses to identify lung cancer, such as chest X-ray, test positron emission tomography, CTI scan, and MRI scan, are available [40]. Thus, our proposed approach can serve as a second opinion for doctors after a physical diagnosis was carried out. The result can help doctors make decisions regarding the treatment of patients.

An electronic dataset of an actual case from the National Taiwan Heath Insurance Database was used in this study. Data were recorded from 2004 to 2010. Data on 367 patients who were diagnosed with lung cancer and brain metastasis were collected. Another 3112 patients were diagnosed with lung cancer with no metastasis during the time of research. These data represent the imbalanced class problem. The majority class is the lung cancer patients, whereas the minority class is the patients with LCBM.

In this case, the evaluations are based on accuracy, sensitivity, specificity, and G-mean. We performed several steps on data preprocessing. (1) The missing value in the analysis was handled. We impute the missing data with replacement values using the mean of the data. The outliers (noise) data were removed from the dataset. In the LCBM dataset, totally there are 702 records were imputed from the missing values, and 23 records as outliers were removed from 3479 records in total. (2) Query was constructed based on the International Statistical Classification of Diseases and Related Health Problems (ICD-9) Code, the brain metastasis code 198.3 and the lung cancer code 162.x. (3) Patient age varied from 15 years old to 99 years old, with average of 66 years old. (4) The total number of male and female patients was 2087 and 1392, respectively. (5) The collected data contain 12 attributes, i.e., age, gender, drug type, drug duration, treatment 1, treatment 2, treatment 3, treatment 4, examination, initial cancer, cancer 2, and cancer 3.


                        Table 10
                         shows the comparison of the accuracy, sensitivity, specificity, and G-mean for LCBM dataset on the basis of the average of five runs on each algorithms. Data were analyzed by using one-way ANOVA. Result showed a p-value equal to 0.000, which indicates significant mean differences among the seven methods for the LCBM dataset. Fig. 7
                         shows the main effect plots for algorithm comparison on the LCBM dataset. This figure also depicts that BSMAIRS outperformed the other methods in terms of accuracy, SE, SP and G-mean. SAIRS algorithm performed the second best on the LCBM dataset except for SP index.


                        Figs. 3–7 show that differences exist in BSMAIRS performance. Figs. 3–6 shows that BSMAIRS is consistently superior to the other compared methods on the basis of all metrics evaluation. Fig. 7 shows that BSMAIRS had the same performance except for specificity. The extremely large amount on synthetic instance might cause BSMAIRS to perform less in recognizing the true negative rate. However, BSMAIRS still performed well for the small-sized datasets, i.e., the eight UCI datasets.

BSMAIRS sensitivity, specificity, and G-mean were higher than that in SAIRS. Results showed that accuracy and sensitivity, specificity, and G-mean were in the same trend, which is attributed to the contribution of BSM. SVM achieved the best performance in terms of SP, however it also performed the worst in terms of accuracy, SE and G-mean.

Results showed that the BSM technique enhanced the performance of AIRS with high accuracy. Moreover, SAIRS appears to be the second best methods in both of the cases, this due to the influence of SMOTE techniques.

@&#CONCLUSIONS@&#

This study presented a classifier approach known as BSMAIRS, which combines BSM with AIRS as a global optimization searcher and the nearest neighbor algorithm as a local classifier. We compared with several well-known classifiers, namely AIRS, AIRS2, SAIRS, Clonal G, C4.5, BPNN, SVM and Random Forest from the perspectives of accuracy, sensitivity, specificity, and G-mean.

The proposed approach was applied on eight electronic medical datasets from UCI databank to assess its performance. Experimental results revealed that BSMAIRS performed better (achieved higher median values) than the other classifiers including SAIRS. Statistically, BSMAIRS and SAIRS means difference is not significant by p
                     =0.001. It is clearly shows the advantage of oversampling to handle the imbalanced class. Moreover, the results showed that the combination of BSM and classifier algorithm can enhance classifier performance, particularly in terms of accuracy. This result leads to the conclusion that the classifier algorithm can perform better on balanced datasets. In addition, we successfully applied BSMAIRS to predict the occurrence of LCBM in the cases studied in Taiwan.

Basing on these results, we confirmed that BSMAIRS can be applied as an alternative diagnosis tool in conjunction with other medical tests for the early detection of LCBM. This method can function as a second opinion that can assist doctors to diagnose lung cancer patients after physical examination has been performed. Thus, preventive treatment for brain metastasis can be performed early. For future work, we suggest to investigate fuzzy KNN to improve the classification accuracy and recognition rate under fuzzy situations. The issues related to missing data and noise in the feature selection are worthy to further investigate. The Euclidean measure would have difficulty when it comes to outliers in the data. Other measures such as correlation measure, local binary decisions or Shepherd's similarity measures would help sort this issue.

@&#ACKNOWLEDGMENTS@&#

The authors gratefully acknowledge the comments and suggestions of the editor and the anonymous referees. This work is partially supported by the National Science Council Taiwan. This study is based in part on electronic data from the National Health Insurance Research Database provided by the Bureau of National Health Insurance, Department of Health and managed by National Health Research Institutes, Taiwan. The interpretation and conclusions contained herein do not represent those of Bureau of National Health Insurance, Department of Health or National Health Research Institutes.

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.cmpb.2015.03.003.

The following are the supplementary data to this article:
                        
                           
                        
                     
                  

@&#REFERENCES@&#

