@&#MAIN-TITLE@&#Windowing improvements towards more comprehensible models

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose several improvements for the windowing algorithm.


                        
                        
                           
                           We evaluated model performance, interpretability, and stability.


                        
                        
                           
                           Our methodology focuses on the interpretability of the model.


                        
                        
                           
                           Our approach shows differences in terms of interpretability, without harming performance.


                        
                        
                           
                           Our approach may yield better classification models.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Windowing

Decision tree metrics

High dimensional data

@&#ABSTRACT@&#


               
               
                  The induction of decision tree searches for relevant characteristics in the data which would allow it to precisely model a certain concept, but it also worries about the comprehensibility of the generated model, helping human specialists to discover new knowledge, something very important in the medical and biological areas. On the other hand, such inducers present some instability. The main problem handled here refers to the behavior of those inducers when it comes to high-dimensional data, more specifically to gene expression data: irrelevant attributes may harm the learning process and many models with similar performance may be generated. In order to treat those problems, we have explored and revised windowing: pruning of the trees generated during intermediary steps of the algorithm; the use of the estimated error instead of the training error; the use of the error weighted according to the size of the current window; and the use of the classification confidence as the window update criterion. The results show that the proposed algorithm outperform the classical one, especially considering measures of complexity and comprehensibility of the induced models.
               
            

@&#INTRODUCTION@&#

One of the greatest challenges for machine learning (ML) is to build precise models out of high-dimensional data, like gene expression data. Furthermore, in biology and medicine, the comprehensibility and interestingness of the model is also important for the specialists to trust it and have insights about the phenomena being studied, which might lead to creating new knowledge [1,2]. In this paper, we have studied the performance of decision trees (DTs) built from high dimensional data like gene expression, focusing on not only measures like accuracy, but also on those related to the comprehensibility of the classifiers.

On the other hand, given the fact that ML algorithms, including DTs, have been designed for a broad spectrum of the human knowledge, they are based on flexible and powerful representations. This flexibility may, in some cases, present the disadvantage of making inducers more susceptible to the training data. DTs, for example, are known to produce very different models with small changes in the data, which may have the opposite effect: the specialists get confused and stop trusting the models [3,4]. This paper has also evaluated how to improve the stability of DTs.

Specifically, in gene expression data, the number of genes (attributes) is typically much greater than the number of tissue samples (instances), and only a small subset of the genes is relevant to the task at hand [5,6]. The great advantage of DTs over other learning paradigms, such as function-based and statistical paradigms, is that they possess an embedded process of attribute selection, allowing them to use only those attributes which were considered to be relevant and informative [7]. As a result, the final DT models are smaller, syntactically simpler and more comprehensible. Considering stability again, in general, DTs built from gene expression data tend to include very few attributes in the model; however, given the huge number of attributes, many trees with similar accuracy can be built, each of which with a different subset of attributes [8].

Gene expression profiles are obtained via Molecular Biology techniques like Microarray [9] (based on intensity measures of DNA hybridization) and SAGE [10] (based on tag counting). This kind of data helps in the discovery of diagnostic and prognostic methods, diseases treatment, drug development [11], etc. Many research works have concentrated efforts on applying ML algorithms on gene expression analysis [1,2,12–19].

In order to deal with the problems described above, we have used windowing [20], a technique whose idea is to find a subsample of a dataset that provides enough information for an inducer to train a classifier. It has results similar to those achieved by training a model from the entire dataset, reducing the complexity of the learning problem [21]. This way, windowing can be seen as a subsampling technique [22], but, unlike other subsampling techniques (e.g., bootstrapping), windowing tend to provide more class balanced and informative samples. The technique was first proposed by [23] in the context of decision trees as a way to deal with memory restrictions imposed by computers in the late 1970’s to some relatively large datasets. The work of [20] argues windowing is interesting for two reasons: i) in some cases, especially those free of noise, it may make the time taken to build the model shorter (e.g., when the dataset is very large and a model that perfectly classifies all training instances is achieved in the first iterations). For most cases, however, windowing makes that time longer; ii) windowing may produce more accurate classifiers, since it explores the solution space a little further. Memory is still an issue, given the existence of huge databases on medical and biological domains.

The study performed by [24] states that windowing contributes to better threshold choices for continuous attributes in decision tree induction. In [25], windowing is seen as a way to make unstable learning algorithms more stable. Some other studies have explored windowing in the context of rule induction [26], as they have noted the technique produces better results with that kind of inducer than with decision trees, since rules are learned independently of each other and are less susceptible to changes in the class distribution. In spite of the experience Quinlan had with windowing, those studies also state that the technique is not good to be used with decision trees, especially in noisy domains [27]. Possibly because of that, very little has been researched towards improving the combination between windowing and decision trees, although there still are some work on it [28]. Our research group believes there still are some aspects of windowing that could be explored further and has proposed some changes in the original algorithm, which will be discussed in the next sections. These changes are focused on improving the performance of decision trees built from gene expression data (which are typically noisy).

Our implementation of windowing can be applied to any learning algorithm that works with classification. Our group has been focusing on symbolic classifiers, i.e., those which can be written as a set of rules. In this context, one advantage of windowing over other meta-inducers is that its use with symbolic classifiers still provides symbolic classifiers. Instead of just concerning about accuracy, we have used comprehensibility-related measures to assess the models. Such measures are well established when it comes to rule induction, but not when it comes to DTs. We have proposed some of those measures but specifically applied to DTs.

The remaining sections are organized as follows: Section 2 gives details about the original version of windowing; Section 3 describes the alterations proposed in windowing and how the experiments were conducted; Section 4 presents the measures used to assess the results; Section 5 presents the experimental setup; results and discussion can be found in Section 6; conclusions are presented in Section 7.

The version of windowing used here is that of C4.5 [20], an algorithm of top-down induction of decision trees. Starting from the root of the tree, the algorithm performs a greedy search for attributes to be used in tests in the internal nodes. Each internal node performs a test on only one attribute and has two or more branches, each of which representing a test outcome. The leaf nodes contain class labels. To label a new example: from the root node, tests are performed and, according to their outcomes, the example goes down the tree until it reaches a leaf node, receiving its label. The decision tree inducer has a Java implementation in Weka [29] called J48.

This section describes windowing as it is implemented in C4.5 Release 8 [20]. Algorithm 1
                      shows the pseudo-code of windowing, where N represents the number of instances in the training set and 
                        
                           
                              x
                              →
                           
                           i
                        
                      and yi
                      (
                        
                           i
                           =
                           1
                           ,
                           …
                           ,
                           N
                        
                     ) represent a vector containing the attribute values and the class label for instance i, respectively. The ∥E∥ operator returns 1, if E is true, or 0, otherwise.

Before the learning process begins, a subset of the training set is chosen, forming the initial window (Line 2), from which a model is induced (Line 5); the model is then used to predict the class of all training examples inside (windowErrors) and outside the window (testErrors), which might produce some misclassifications (Lines 6–7); if the errors found are less than the errors of the best classifier so far (initially, 
                        
                           N
                           +
                           1
                        
                     ), the current classifier is kept as the best one (Lines 9–12); if there were errors outside the window, the window is updated and used to train another classifier; the resulting model is then tested again, and the process is repeated until no misclassifications occur outside the window.

The initial window (Line 2) is not actually sampled randomly. First the training set is shuffled; then the algorithm tries to build a window as uniform as possible, i.e., the class distribution gets to be as balanced as possible. Considering c as the actual number of all class values of a dataset and 
                        
                           E
                           =
                           W
                           /
                           c
                        
                      as the expected number of instances for each class value in the initial window W, for any given class value, if the number of instances labeled with it is at least equal to 
                        
                           E
                           ,
                        
                      then it will be represented by 
                        E
                      instances in the initial window; otherwise, all instances representing that class value will be added to the window. This often leads to better results, especially in cases of unbalanced classes [20]; it also contributes to better threshold choices for continuous attributes [24]. In spite of being as uniform as possible, the class distribution and the examples chosen to be in the initial window are subject to a random sampling, which means that one may have different initial windows, leading to different final classifiers. As it can be seen in Algorithm 1, at least half of the misclassified examples outside the window is added to it at each iteration (Lines 13–16), provided that there are enough examples. This is done to make the model converge faster. Lines 15–16 take I misclassified examples from outside the window and adds them to it.

The process can be repeated more than once. Each repetition is called a trial and starts with a different initial window, which often generates a different final classifier. By default, C4.5 uses 10 trials. The best tree classifier from all trials is returned as the final output.

The revised (extended) windowing is presented in Algorithm 2
                     . The initial window (Algorithm 2, Line 2) is chosen the same way as the original (Algorithm 1, Line 2), as explained at the end of the previous section.

In the original implementation, only the best tree of each trial is pruned (Algorithm 1, Line 18), but all intermediary trees within a trial are kept unpruned. In order to decide which tree is the best among all trials, the algorithm takes the unpruned version of the best classifier returned by each trial and calculate their estimated error. The best final classifier is the one with the least estimated error. In Algorithm 2, there is the possibility of not using Line 42 (which corresponds to Algorithm 1, Line 18), but using Algorithm 2, Line 8 (not available in Algorithm 1), which results in the pruning of all trees. The reason for this alteration is simple: the process of pruning trees is known to be important, because it helps the tree improve its generalization error [30,42]. In a previous work [31], we have analyzed this aspect of windowing, and the results showed this change, by itself, made no improvements to the algorithm. In spite of having a greater computational cost, we wanted to verify if this change, by itself or in combination with the other ones, would bring any benefits over the original version of windowing and the traditional decision tree.

It is known windowing has its performance deteriorated in noisy domains, because it ends up adding all noisy instances to the window, since they are typically misclassified even by good models [32,33]. In order to treat this problem, another alteration we propose is based on the confidence in the classification performed, and its inspiration was provided by co-training semi-supervised learning [34], which uses the confidence in the classification as the criterion to add unlabeled instances to the set of labeled instances. Following this idea, we implemented the possibility of using the classification confidence criterion to update the window in Algorithm 2, Line 24.

For instance, the inducer J48 assigns an array of values to each prediction it performs. Each position of the array corresponds to one of the possible class values and represents the distribution of a given class in the leaf node that was reached. The predicted class will be the one with the greatest such distribution. These values can be seen as a factor that represents how confident the model is in the prediction it has just performed.

In a given iteration of the algorithm, there may be misclassified instances outside the window. In the proposed alteration, such instances are sorted according to the confidence with which the model would predict the true class. Keep in mind that, in this case, the predicted class was not the true one, because the instance was misclassified, but the true class also has an associated confidence. These confidence values (of the true class) are put in descending order by our implementation. The heuristic here is the following: although the instance has been misclassified, the greater the confidence the model has in the prediction of the true class, the closer it is to classify the instance correctly. For example: in a certain problem, in which the class attribute can assume values “Yes” or “No”, the model built during an iteration of windowing misclassified two instances outside the window. One of them had “Yes” as the true class, and the model assigned a confidence of 0.40 to value “Yes” and 0.60 to value “No” (the one predicted by the model). The other instance had “No” as the true class, and the model assigned a confidence of 0.05 to value “No” and 0.95 to value “Yes” (the one predicted by the model). In both cases, the model made mistakes, but one could say that, in the first case, the model was closer to a good classification.

The algorithm calculates, at each iteration, the size of the increment used to update the window. When the window is updated, the instances to be added are the ones with the greater confidence values. But, when the confidence criterion is used, the algorithm also makes some changes to the increment calculation. As the instances are sorted by their confidence values, the algorithm counts how many confidence values are greater than zero, counter to be referred to as N
                     0 from now on (Algorithm 2, Line 26). N
                     0 is then tested against the calculated increment. If it is greater than half of the calculated increment, the increment is assigned value N
                     0, provided that N
                     0 < increment. Otherwise, the increment gets to be half of itself. This is done for two reasons: i) the fact that N
                     0 is small may be due to the presence of noise, in which case adding too many instances to the window is not interesting; ii) the algorithm is having difficulty in learning the concept, in which case it is better if we fine-tune the increment. This way, the algorithm would converge more slowly, but it would minimize the risk of adding too many instances that won’t bring useful information at the moment.

The confidence criterion was also used as a way to stop the algorithm before it finds no misclassified instances outside the window or there are no more instances outside the window, preventing it from continuing the calculations in situations where adding more instances to the window would not improve the classifier: if N
                     0 has had value zero for more than three consecutive iterations, i.e., if the models produced in the last iterations have not had a single confidence value greater than zero, the trial is stopped (Algorithm 2, Line 30).

The third change proposed refers to the error calculation of the intermediary trees (those produced during a trial). In the original algorithm, the error of such a tree is given by the sum of the errors found in the window (training error) and those found outside it (unseen instances). However, the error of trees built in the last iterations of a trial are closer to the resubstitution error, since the window gets bigger as the iterations are processed. The resubstitution error is optimistic, making the trees built in the first iterations of a trial be in disadvantage since the window is smaller and the number of unseen instances is greater. In order to make the assessment of the trees fairer, instead of the calculation of the training error, we created the possibility of using the estimated error (Algorithm 2, Line 11). The calculation of this error was proposed by [20] as a pessimistic estimate of the generalization error. This estimate is based on the binomial distribution and takes into consideration the class distribution in the leaf nodes. It is the same error used by the original algorithm to assess the best trees of each trial. Quinlan proposed the calculation of the estimated error following a recursive definition, in which the estimated error of a non-leaf node is given by the sum of the estimated error of each of its descendants; and the estimated error of a leaf node is calculated as follows: considering the leaf node covers M training instances, E of which incorrectly, Quinlan compared this situation with that of the observation of E events out of M trials. If M is seen as a sample, one could calculate a posteriori probability that would give an idea of the error probability of the leaf node. This probability has the form of a confidence interval. Once the confidence level is defined, which is one of the parameters included in the inducer, the confidence interval is given by the one of a binomial distribution. Since we want a pessimistic estimate of the generalization error of the tree, only the upper limit of the interval matters. Such approximation harms many of the concepts in Statistics, but it represents a heuristic that produces good estimates accord-ing to [20].

For the same reason presented in the last paragraph, a fourth alteration was implemented: the use of the weighted error in the assessment of the intermediary trees. This weighting is based on the size of the current window. The bigger the window, the more data the algorithm has for training, and the greater it is its chances to make good classifications. We implemented the possibility of “punishing” the trees built from bigger windows, approximating the conditions under which the trees are assessed. In this case the total error is multiplied by 
                        
                           1
                           +
                           |
                           window
                           |
                           /
                           N
                           ,
                        
                      where |window| is the size of the current window and N is the number of instances in the whole dataset (Algorithm 2, Line 18).

Accuracy and the area under the ROC curve (AUC) are two of the most commonly used metrics in the assessment of ML models. However, when one needs an idea of the interpretability degree of a model, or of how useful a model is, other measures have to be used. In the context of decision rules, many of these measures are already defined [35]. Nonetheless, we have found no such measures defined directly to DTs. What researchers typically do is to rewrite the tree as a set of rules, representing each path from the root to the leaf nodes in the form L → R, where L is a conjunction of tests in the attributes and R is the class label of the reached leaf. From that point, those decision rule measures can be used. Rules derived this way present some issues, including redundant tests. If this redundancy is eliminated, the resulting rules get to be dependent of each other, and the order they are applied becomes relevant. Pure decision rules, on the other hand, are normally generated independently of each other [36]. Our research group has been working on some syntactic metrics that could be applied directly to decision trees, such as cohesion, compactness, and cohesion–compactness. Other metrics, such as tree size, tree height, and window size are commonly known in the machine learning literature. All these metrics are presented in the following sections.

The size of the final tree produced by an inducer is simply the number of nodes in the tree . This measure is actually already provided by some systems, such as Weka. This is a trivial measure, but it provides us with some idea of one of the aspects related to the interpretability of the model: we assume that, the smaller the tree, the easier it is for the model to be understood by human specialists. In spite of being symbolic models, huge DTs are hard to interpret.

The height of a tree is defined as the number of steps between the root node and the deepest leaf node. This path allows only a step from a certain node to one of its direct descendants. This measure also provides an idea of how complex the model is. The taller the tree, the more attributes, and nodes are needed to explain the concept, which makes the tree more complex.

This measure is specific to the analysis of models produced by windowing, which may produce good models using only a subset of the original training dataset. The measure is defined as the number of instances present in the window used to generate the final classifier produced by the algorithm. Provided that the other performance measures are not significantly worse, the smaller the window that generated the model, the better the algorithm. In this sense, it is obvious for trees in Fig. 1
                        , it is not possible for us to calculate the size of the window by just looking at the final classifier; for that, we would have to analyze the algorithm execution.

Considering c > 1 as the number of different class labels, and f ≥ 1 as the number of leaf nodes in the tree, we define cohesion as:

                           
                              (1)
                              
                                 
                                    Cohesion
                                    =
                                    
                                       c
                                       
                                          f
                                          −
                                          1
                                          +
                                          c
                                       
                                    
                                 
                              
                           
                        
                     

As can be noted, cohesion values lie in the interval (0, 1]. Considering a tree has at least one leaf node (in the “worst” case, the tree will always predict the majority class), value 1 will be achieved when the model has exactly one leaf node, representing the simplest possible model. As the number of leaves grows, the value of cohesion approaches 0 and the tree becomes more complex because the concept needs more information to be explained (according to the model). A value close to 1 means the model is more cohesive in relation to the possible number of different class labels. The only especial case is the one cited above, in which the model is represented by just one leaf node. In spite of assuming value 1, it would not normally be considered a good model, since it found no relevant information that could be used to explain the concept being studied.
                     

This measure represents how compact the tree is in relation to the knowledge involved in the problem at hand. Considering t ≥ 0 as the number of different attributes used by the tree model, and a > 0 as the total number of attributes present in the dataset, we define compactness as follows:
                        
                        
                           
                              (2)
                              
                                 
                                    Compactness
                                    =
                                    1
                                    −
                                    
                                       t
                                       a
                                    
                                 
                              
                           
                        
                     

The value of compactness lies in the interval [0, 1]. Value 0 means that each attribute in the dataset has been used by the model at least once (
                           
                              t
                              =
                              a
                           
                        ); value 1 means no attribute has been used (
                           
                              t
                              =
                              0
                           
                        ). Compactness here is related to how many attributes the model considered relevant to describe the concept, in relation to the total number of attributes. A value close to 1 would indicate the knowledge is more compact (according to the model), being the model considered simpler than one whose compactness is close to 0, in which case more attributes were needed to represent the concept.

This measure is especially important in gene expression datasets, that typically have thousands of attributes, but only a few of them are really relevant. However, the fact that the tree has used just a few attributes, indicating the knowledge involved is compact, does not mean it has chosen the correct attributes (the most relevant to the problem).

One could say that cohesion and compactness are related. If there are only a few leaves in the tree, we would expect few attributes in the model and vice versa. That is, the greater the cohesion, the greater the compactness, and the other way around. One way of combining them is to take the geometric mean between 
                           cohesion
                         and 
                           compactness
                        . Geometrically, the resulting value would be the side size of the square with the same area as the rectangle whose sides are represented by the two components given above. The bigger the square, the simpler the concept could be considered. We define cohesion–compactness as:

                           
                              (3)
                              
                                 
                                    Cohesion-
                                    
                                    
                                    -Compactness
                                    =
                                    
                                       
                                          Cohesion
                                          ×
                                          Compactness
                                       
                                    
                                 
                              
                           
                        
                     

In Fig. 1, three examples of trees are shown, from which we will exemplify how to compute the proposed measures in Table 1.

In order to assess the alterations we had implemented, we designed an experiment that considered J48, and different versions of windowing. Such versions resulted from all possible combinations among the following options the algorithm started to offer: whether to use the confidence criterion, whether to use weighted errors, whether to replace the window errors by the estimated errors, and whether to prune the intermediary trees. All other parameters of the algorithm were kept with their default original values. This way, 16 different versions of windowing were evaluated (see Table 2). Whenever feasible, the J48 algorithm (unique tree induced without windowing) was also compared to the different windowing settings (in these cases, 17 different inducers participated in that experiment).

Forty-one real gene expression datasets were tested, some coming from microarray studies, others coming from SAGE studies. Their description can be found at the Appendix A.

For each inducer applied to each dataset, 10-fold cross-validation [37] was used. The performance measures taken into consideration were: accuracy, AUC, tree height, tree size, size of the window used to build the best classifier found, cohesion, compactness, cohesion/ compactness relation, and training time.

For the significance test, we used [38], a non-parametric technique based on ranks, and widely spread in the ML community. For the case of rejecting the null hypothesis (which states that differences in the data are obtained by chance), a post-hoc test is necessary to check in which classifier pairs the differences actually are [39]. Again we chose a non-parametric approach, and the p-Values found for the possible pairs were adjusted according to [40]. The tests considered all possible pairs of inducers, i.e., multiple comparisons with no control. The most important advantage of choosing a non-parametric approach is not making the assumption of any prior probability distribution for the variables being considered (ANOVA, for example, assumes a normal distribution).

The experiments reported here were performed in Weka [29]. Instead of C4.5 (C language implementation), we have used J48 (Java implementation), and since Weka does not provide any implementation of windowing, a Weka’s Java class was implemented as a meta-inducer, following its standards. The implementation has all the alterations we are proposing and allows the choice of any inducer available in Weka to be used as the base inducer (Algorithm 1, Line 5 as well as Algorithm 2, Line 6), provided that it works with classification problems. For our purposes, the base inducer was J48.

In fact, we have implemented a different version of J48, for two reasons: to make the model provide some performance measures we were proposing; to be able to build an unpruned tree that we could prune later without having to rebuild it from scratch (in Weka, one cannot prune a J48 tree without starting the process over again). Other than that, our reimplementation of J48 is identical to its original version. This inducer was also used with its default parameters.

The statistical tests were implemented and performed using The R Project for Statistical Computing.
                        2
                     
                     
                        2
                        
                           https://www.r-project.org/ .
                      The analysis presented here considered a significance level of 5%. In Table 2, we show the correspondence between the different configurations of windowing, and the notation adopted to refer to them. For those tables showing all pairwise comparisons using the Friedman and post-hoc test for a given variable (metric) in the next section, only the uppermost right triangle is shown in the matrix, because it is symmetric. The symbol ○ in a cell indicates no difference was found between the inducer at the respective row and the inducer at the respective column; △ (▽) indicates the inducer at the row was better (worse) than the one at the column, but not significantly; ▲ (▼) indicates the inducer at the row was significantly better (worse) than the one at the column.

@&#RESULTS AND DISCUSSION@&#

Considering the model
                         interpretability, the shorter the tree, the easier it is to syntactically understand it (of course that the fact the tree is shorter does not make it necessarily better, or even that it is somehow coherent with the concept being learned.) In the comparison we performed, J48 had one of the worst ranks and was considered as having produced significantly taller trees than many of the versions of windowing. The p-Value of the difference between J48 and WEWeC, for example, was at the order of 
                           
                              10
                              
                                 −
                                 8
                              
                           
                        . In Table 3, we show the comparison among the algorithms.

In this case, six of the windowing versions involving the confidence criterion occupied the first six positions, and the first four did not present significant differences among each other (WEWeC, WEC, WWeC and WC, in this order). The last position was WP, but it did not present any significant difference in relation to J48 and the original version of windowing). In Table 4, we show the average ranking of the test.

We can see that, in this case, the proposed alterations contributed to the improvement of the syntactic interpretability of the model, having produced, in general, shorter trees. This kind of contribution may be very important in gene expression studies, in which one needs to mine, among thousands of genes, only the ones actually associated with the problem.

For the tree height, we have also compared its values of standard deviation. No significant difference was found, and the p-Value of the Friedman test was almost 1.

The results for this measure were nearly identical to those obtained for the tree height, and the only difference was found in the order of the ranks of the algorithms which did not show significant differences. These results reinforce the contribution of the proposed alterations to the improvement of the model interpretability.

In Fig. 2
                        
                        , it is possible to see the boxplots for the measure tree size. The graph at the top of the figure shows the boxplot of the values of the measure itself. The four lowest boxes represent the four inducers with the lowest average ranks, as shown by the boxplot at the bottom of the figure. It is also possible to note that the differences among the values of the measure itself are lower than those of the average ranks. This situation is relatively common in the non-parametric analysis: even if there are not big differences among the observations of the variable being analyzed, if one or more inducers present values that are systematically greater or smaller for that variable, chances are those inducers will be considered significantly different than the others.

The comparison of the standard deviation values did not recognize any significant difference either, just like what happened in the tree height case.

For this
                        
                         comparison, J48 was taken out of the analysis for two reasons: (i) the algorithm does not work with the concept of window, because it always uses the whole training set; and (ii) it is relatively common that windowing finishes training before it adds all instances to the window. Even if we considered a “window” for J48 (e.g., a window of the size of the whole training set), the algorithm would be considered significantly worse in the majority of the times.

Again, the proposed alterations took effect, leaving the original version of windowing in the last positions. Versions WEWeC, WEC, WWeC and WC, in this order, were, just like in the case of the tree height and size, the four best ones, but did not show significant differences among each other. Once again, the combination among the confidence criterion, estimated error, and weighted error produced the better result, i.e., it produced more interpretable models at a lower price, using fewer instances to train the classifiers.

In Table 5, the comparison among all the algorithms can be visualized. It can be noted that many inducer pairs presented significant difference, but the pattern of those differences is very close to the one presented in the case of the tree height and size. In Table 6, the complete ranking of the test is shown.

We also
                         analyzed the metric CPU time
                        
                           3
                        
                        
                           3
                           CPU time is the amount of time taken by the CPU (Central Processing Unit) to process instructions, i.e., it does not take into consideration, for example, the time spent on waiting for input or output operations.
                         spent on training. Once again, J48 was excluded from the analysis. Despite the fact that windowing makes the time to train the model smaller in some cases, when it comes to gene expression datasets, the technique spends much more time on training the classifier. If J48 were included in the analysis, it would always be considered significantly faster than any of the windowing versions.
                        
                     

This analysis was meant to verify if any of the altered versions of windowing would be able to make the technique more computationally efficient. As can be noted in Table 7, the objective was achieved. The expectation of diminishing the computational time was mainly due to the fact that the alteration based on the confidence criterion could stop the trials earlier if the algorithm did not notice any sign of progress while adding instances to the window. This apparently happened, because almost all versions that used the confidence criterion occupied the first positions, as shown in Table 8.

Furthermore, from Table 8, one may notice something that, at first glance, could be considered incoherent: the first positions are occupied by versions involving the alteration related to pruning the intermediary trees. Nevertheless, even if tree pruning normally comes with a greater computational effort, it is important to the success of the window update based on the confidence criterion. As described earlier, the confidence of a DT is calculated from the class distribution of the leaf node that was reached by the instance. If pruning the intermediary trees is not done, the DT tends to overfit the data, producing class distributions with 0/1 values. If pruning is used, this effect is minimized and the confidence criterion becomes more effective.
                     

The original version (W) had one the worst performances, being considered significantly slower than many of the altered version (notably, WPEWeC, WPC, WPEC and WPWeC). Some versions considered good at the previous measures also had good performance at this measure, i.e., they produced more interpretable trees in a shorter time.

For cohesion, the pattern found in the analysis of the tree size got repeated, both in terms of the ranking positions and the significant differences found. One interesting difference was related to the standard deviation of cohesion, which was significant. The versions that had better results for cohesion itself presented a greater standard deviation, i.e., more instability (Table 9). In fact, in spite of the Friedman test having identified significant differences (p-Value of 
                           
                              3.91
                              ×
                              
                                 10
                                 
                                    −
                                    3
                                 
                              
                           
                        ), the post-hoc test was not able to identify in which pairs of inducers they occurred (at a significance level of 5%). If the significance level of the test is set to 10%, the differences are found in pairs WEC/WPWe, WEWeC/WPWe, WEC/WP and WEWeC/WP.

The results obtained for compactness and cohesion–compactness were coherent with those obtained for cohesion. The altered versions (notably, WEWeC, WEC, WWeC and WC) had a very good performance, being considered significantly more compact than J48 and the original version of windowing. This could indicate the best versions were able to extract the relevant knowledge in the gene expression studies considered, since they were cohesive (in the sense of presenting fewer leaf nodes per class values) and compact (in the sense of having found the relevant genes). Of course, the analysis of a specialist is essential to be really able to formulate such conclusion.

Unlike what happened to cohesion, there were found no significant differences in the standard deviation of compactness. For cohesion–compactness, the analysis of the standard deviation was practically identical to the standard deviation of cohesion, i.e., the best versions for the metric itself turned out to be the most unstable.

As for AUC, no significant difference was found, and the Friedman test calculated a p-Value of 0.29. No differences were found in the standard deviation of AUC either.

The analysis of accuracy showed significant differences (Table 10). This time, the versions considered the best at measures related to interpretability had significantly worse performance when it came to accuracy, even in relation to J48 and the original version of windowing. Nevertheless, the first positions in the ranking are occupied by some of the altered versions, even if they have not been considered significantly better than J48 and the original windowing (Table 11). The analysis of the standard deviation of accuracy also showed significant differences, although they have been restricted to the differences between the inducers at the first two positions and the last two positions of the ranking (Table 12). Again, the versions that produced the most interpretable models occupied the last positions.

In Fig. 3, we can see the boxplots for accuracy. The graph at the bottom of the figure shows the boxplot of the ranks obtained by the inducers in the various datasets. We can again see that the best inducers in terms of the interpretability of the model had a poorer performance in terms of accuracy.

In Table 13, a summary of all experiments is shown. Each cell of the table represents the performance, in terms of the average rank, of an inducer (row) on a measure (column). Such performance is defined by a gray level value: black, when the average rank of the inducer is below 5.2; dark gray, when it is between 5.2 and 8.5; light gray, when it is between 8.5 and 11.8; and even lighter gray, when it is above 11.8. The darker, the smaller the average rank, i.e., the better the inducer was. The gray level differences do not necessarily represent significantly statistical differences. As can be seen, there are some groups of inducers with similar performance: the inducers in the first four rows were quite similar considering all measures, having presented better results for accuracy and poorer results for measures related to the models syntactic complexity; from row five to row eleven, the inducers were clearly better at complexity-related measures, and almost all of them used the confidence criterion. The rest of the rows represent inducers with performance more and more poorer in terms of the complexity measures, but with a reasonable performance in terms of accuracy and AUC.

@&#CONCLUSION@&#

One of the conclusions that could be made is related to the trade-off performance × legibility of the model. Normally, the more complex a model, the less interpretable it is [2,41] and vice versa. As can be noted in the results, the altered versions of windowing considered more legible (mainly WEWeC, WEC, WWeC and WC) were the ones with the worst performance in terms of accuracy. The only version able to balance a little better that trade-off was WPEWeC, which involved all of the proposed alterations, because it obtained good results in the measures related to model interpretability and it was not considered significantly worse than any of the inducers in terms of accuracy and AUC.

Another important observation refers to the instability of the inducers that had good results for interpretability variables. Such inducers were considered more unstable by the tests when it came to cohesion, compactness, cohesion–compactness, and accuracy. Possibly, just like the trade-off performance × legibility, there may exist a trade-off stability × legibility. Models like Support Vector Machines are usually stable, but they are not easily interpreted.

Although significant differences were found for accuracy, that did not happen for AUC. This result reinforces the fact that there is not necessarily a relation (negative or positive) between accuracy and AUC. Since no differences were found in terms of AUC, but differences were found in terms of the interpretability variables, one can conclude that the proposed alterations have the potential to improve the legibility of the models without harming other performance measures.

In the related literature, very great attention has been given to performance measures such as AUC, leaving the interpretation of the resulting model at a lower level, even when analyzing symbolic classifiers. There are many different methods for constructing decision trees, although none of them is universally the best, since different application domains lead to different problems, requiring different solutions. It is then expected that a combination of methods, like the one proposed in this work, may yield better classification models in some cases.

@&#ACKNOWLEDGMENTS@&#

The authors would like to thank the following institutions for funding this research; Sao Paulo Research Foundation (FAPESP) Process 2009/04511-4, the National Research Council of Brazil (CNPq), the Amazon State Research Foundation (FAPEAM), the National Institutes of Science and Technology Program (INCT) through the ADAPTA Project (Centre for Studies of Adaptations of Aquatic Biota of the Amazon) Process 573976/2008-2.

The experiments reported in this manuscript used 41 datasets, all of which representing real gene expression data. Table A.14 shows a summary of the datasets, none of which having missing values for the class attribute. The instances whose class value appeared only once in a given dataset were excluded from it. Reason: windowing start with small windows and the gene expression datasets are typically already small. Since the technique adds instances to the window as it needs more information about a certain class value, there is no sense in keeping classes represented by only one instance. The datasets which had instances taken out were: ECML2004, GSE7898 and PROSTATE-WEL2001.

@&#REFERENCES@&#

