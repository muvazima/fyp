@&#MAIN-TITLE@&#Prediction of the hemoglobin level in hemodialysis patients using machine learning techniques

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Different prediction algorithms were used to predict Hb levels in CRF patients.


                        
                        
                           
                           Prediction errors in the validation cohorts of patients were around 0.6g/dl.


                        
                        
                           
                           Difficulty to obtain lower errors due to the measuring machine precision (0.2g/dl).


                        
                        
                           
                           Relevance analysis of features have been applied for each predictor.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Prediction

Hemoglobin

Chronic renal failure

Hemodialysis

Machine learning

@&#ABSTRACT@&#


               
               
                  Patients who suffer from chronic renal failure (CRF) tend to suffer from an associated anemia as well. Therefore, it is essential to know the hemoglobin (Hb) levels in these patients. The aim of this paper is to predict the hemoglobin (Hb) value using a database of European hemodialysis patients provided by Fresenius Medical Care (FMC) for improving the treatment of this kind of patients. For the prediction of Hb, both analytical measurements and medication dosage of patients suffering from chronic renal failure (CRF) are used. Two kinds of models were trained, global and local models. In the case of local models, clustering techniques based on hierarchical approaches and the adaptive resonance theory (ART) were used as a first step, and then, a different predictor was used for each obtained cluster. Different global models have been applied to the dataset such as Linear Models, Artificial Neural Networks (ANNs), Support Vector Machines (SVM) and Regression Trees among others. Also a relevance analysis has been carried out for each predictor model, thus finding those features that are most relevant for the given prediction.
               
            

@&#INTRODUCTION@&#

Patients who suffer from chronic renal failure (CRF) tend to suffer from an associated anemia, as well. Hemodialysis is the most common treatment for patients with end-stage renal disease (ESRD), an illness that has reached an important social and economic impact. Currently, erythropoietin (EPO) is the treatment of choice for this kind of anemia. The use of this drug has greatly reduced cardiovascular problems and the necessity of multiple transfusions. There are significant risks associated with erythropoietic stimulating factors (ESFs) such as thrombo embolisms and vascular problems [1], if hemoglobin (Hb) levels are too high or they increase too fast (they might be very oscillating from one month to another). Because of this, to predict the hemoglobin value from both analytical measurements and medication dosage is of great interest [2–4]. Thus, it is important to have a predictor of hemoglobin for next month because the prescription of EPO can control the level of Hb in a future month. For example, if it is known that a patient will present very high levels of Hb in the next month if a given dose of EPO is administered, then it can be corrected by a modified prescription of EPO that keeps the patient in appropriate levels. This problematic presents an important field of research since it is a very challenging problem [5–10].

The National Kidney Foundation Kidney Disease Outcomes Quality Initiative (NKF-K/DOQI) guidelines recommend that patients with chronic kidney disease (CKD) should maintain a target hemoglobin (Hb) concentration between 11 and 12g/dl; the upper limit is specified to 13g/dl since none of the trials have shown a benefit of higher Hb targets [11]. Effective anemia management is complex and expensive. Although the common procedure is to follow the guidelines published by government agencies and international organizations (mainly NKF-K/DOQI), patient's hemoglobin levels oscillate through the target range during the treatment and only approximately one-third (38%) are within the target range at any given time [12]. This behavior is not surprising given the economic and medical pressures to avoid falling below or exceeding specific levels, and due to the fact that the response to ESAs can change over time in the same patient or can be very different among different patients [13]. In the latter case, the so-called EPO-resistant patients are the most remarkable example. According to the literature, about 5–10% of patients have either a blunted or absent response to ESAs, despite high-dose therapy [14].

The goal of this work is to predict the hemoglobin level, for the next month from both analytical measurements and medication dosage (such as EPO), in patients undergoing hemodialysis using machine learning techniques. For this purpose, two main approaches have been proposed:
                        
                           1.
                           
                              Global models: This kind of models propose a unique solution (model) for the whole cohort of patients.


                              Local models: Local models propose different models for different patient profiles. The approach is based on first obtaining relevant profiles by means of clustering techniques, and after that, apply a different model for each profile. Therefore, models are focused on sets formed by similar patients; hence they are more homogeneous: local models tend to be simpler than global ones.

The organization of the paper is as follows. Section 2 points out some aspects of the data used in this work. Section 2.2 reviews the inclusion criteria and the processing of missing values. Section 3 presents the clustering approaches used as the first step in local Hb predictors. Section 4 presents the prediction models that were used both globally and locally. Section 5 discusses the achieved results. Finally, Section 6 gives some concluding remarks, and suggestions for future work.

The analysis presented in this paper is based on the data of an anemia database, which contains long-term records of the patients undergoing hemodialysis in the Fresenius Medical Care (FMC) clinics, collected with EuCliD system [15]. The Clinical Management System EuCliD (European Clinical Database) is an electronic system designed by FMC company used to manage dialysis clinics’ processes and to collect all relevant data related to dialysis clinical practice. EuCliD deployment started in 2004 with the first pilot clinic in Italy; currently the system is implemented in 626 clinics belonging to 25 countries all over the world. Using this system, all the data of patients treated in FMC clinics settled all over the world are stored in a database that is property of FMC company. In particular, this paper presents the results for Spanish and Italian clinics. Spain and Italy were selected by FMC as two good representatives of their clinics since there is a long experience of FMC in both countries. Moreover, it should also be emphasized that since Italy and Spain showed different characteristics, it also allowed to work on two different scenarios which are representative of the rest of countries.

For each patient from the data base, both analytical measurements and medication dosage are recorded at periodic time intervals. For the sake of reducing computational complexity, all the data from this huge database has been merged to a single Matlab numerical data matrix for each country under study, with one row (record) per single unit of data (either a blood test or a medication dosage), and as many rows as types of information (variables) are available on the database. Afterwards, this data is merged at hemoglobin sample intervals, as the main goal of the study is the modeling and prediction of the hemoglobin levels at the sampled instants Data merging. It is the process of arranging the records according to the intervals between hemoglobin samples into a new dataset. This tasks generates a new dataset merging all the records between each two consecutive hemoglobin samples into a single record. Due to the process of joining all the information present in the original database into a single table, each record (row of the data matrix) contains a single piece of information of a certain patient (it can be a drug prescription, a laboratory test result, etc.). Next step after data pre-processing is to arrange the different records of each patient according to the hemoglobin samples it contains. Taking the hemoglobin sample times as reference, we have joined all the records between Hb(t) and Hb(t+1), being t and t+1 the times corresponding with two consecutive hemoglobin samples monitored. Those records are related with the dose of Iron and EPO the patient receives during dialysis, and the results of the laboratory measurements realized to him. All this information is stored in a single record Hb(t) which will contain the accumulated dose of Iron and EPO, and the summary of all the laboratory measurements, carried out over the patient during this period. The idea on the prediction stage is to predict the hemoglobin until time t
                        +1 using information related to the state of the patient in time t plus the treatment received between t and t
                        +1. Fig. 1
                         shows graphically the merging process.

Merging process consists of finding all the different patients on the database, taking all the records that belong to each patient and sorting them according to the date of the record (laboratory test, drug administration, treatment, …). Then, hemoglobin values are used as reference for generating the new dataset. Next step is to extract all the records between two consecutive hemoglobin samples (the merged records subset). The process finally merges all these records into a single row and generates some additional variables. For instance, medication column is separated in two columns, one for EPO and another one for iron, accumulated dose and number or administration are also calculated and stored for model development. This task is carried out iteratively for all the variables within this subset of records.

When merging process is concluded, a new matrix is obtained. The number of rows of this matrix equals the number of hemoglobin values in the database, and the number of columns (number of variables) is 125, which will be reduced for training the models. Table 1
                         shows the number of records and patients for each country dataset.

In order to obtain useful models, cross validation method was carried out. One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). Since a particular lucky (unlucky) choice of the two subsets may be responsible of very good (bad) results in terms of the committed error, different training and test datasets were used to test that models’ performance. That is, multiple rounds of cross-validation are performed using different partitions to reduce variability, and the validation results are averaged over the rounds. For this reason, as stated above, many different training and test datasets were generated, in order to avoid the effects of the random distribution of records. After carrying out each different partition of the whole dataset into training and test data, each subset was standardized to zero mean and unit variance. Training datasets are formed by 66% of the records, and test datasets by the remaining 34%.

Several inclusion criteria were applied to remove all the inconsistent records. Table 2
                         indicates the ranges of accepted values. Patients with values outside those intervals were removed from the database. Variables not included in this table were not affected by the inclusion criteria.

An additional condition was related to the minimum number of patients’ records needed to be considered in the final dataset. This value was fixed to six, because when generating datasets for prediction the dynamics of the system is taken into account by including the values of some variables in the previous periods. Six was a reasonable figure for this purpose. Another condition was related to Age. Only patients older than 18 years were included in the study.

In addition to the inclusion criteria included in Table 2, another extra conditions were applied:
                           
                              •
                              Patients who presented an increase of weight during dialysis were removed, because obviously either no dialysis was carried out or a mistake in the data acquisition was committed.

Patients with a variation of hemoglobin higher than 7g/dl between two consecutive Hb samples were excluded.

After applying the conditions above mentioned, a series of routine actions were performed in order to solve some technical problems for the forecasting algorithms. An important problem deals with the number of hollows present in the data matrix associated with measured values in patients’ monitoring. In general, empty values were estimated from values in adjacent records or interpolated using mean or median values – depending on the type of variable. If a variable is empty in all the records of the patient, this patient was removed from the database.

The obvious heterogeneity shown in Chronic Renal Failure (CRF) patients in terms of their response to the anemia treatment suggests that clustering methods can help find different patient profiles. In this paper, two clustering approaches were applied, namely, Adaptive Resonance Theory (ART) [16] and hierarchical clustering [17].

Although it is out of the scope of this paper finding patient profiles is also useful to detect anomalous situations, e.g., high increase in Hb levels without any changes in EPO dosages as well as more usual situations like EPO-resistant patients or good responders; even though they are relatively usual situations, a prediction/treatment for EPO resistant patients or good responders must be obviously completely different.

Four different kinds of data have been taken into account to produce clusters for each country (Spain and Italy). Since the paramount interest is to profile patients according to their Hb response to EPO, not all the features were used in the clustering process
                        1
                     
                     
                        1
                        Only features related with EPO and Hb evolution have been taken into account
                     ; thus, the obtained clusters can be more easily interpreted analyzing the values of the prototypes. Therefore, the following kinds of data were used:
                        
                           1.
                           Current value of Hb and the four previous values of Hb (it was selected the four previous values of Hb, corresponding to the four previous months, because the lifespan of normal red blood cells is from 100 to 120 days).

The same but also including EPO dosages in the considered period of time (until four previous values of Hb); that will allow to detect not only performance in terms of the degree of anemia but also new profiles since relationships between Hb and EPO can show EPO-resistant profiles, patients who do respond well to the treatment, etc.

Current value of Hb and time derivatives
                                 2
                              
                              
                                 2
                                 Derivative of Hb level with respect to time.
                               of the differences between consecutive values of Hb in the four previous samples. The purpose of this approach was to find out whether working with Hb changes could make a difference providing new profiles not found when dealing with raw values of Hb.

The same but including EPO dosages in the considered period of time.

Local models were obtained by training the models detailed in next section only taking into account those records included in each cluster. That is, as many models as clusters were built. In general, the number of clusters for each country and data used was between five and eight. In hierarchical clustering, the number of clusters were obtained using two criteria. On one hand the experts’ opinion (the medical board suggested various choices about the number of clusters), and on the other hand the “lifetime” approach in the dendrogram. This is an intuitive approach, which consists on searching in the proximity dendrogram for clusters that have a large lifetime. The lifetime of a cluster is defined as the absolute value of the difference between the proximity level at which it is created and the proximity level at which it is absorbed into a larger cluster. It is worth mentioning that when using ART networks it is not necessary to choose the number of clusters in advance, but the network finds the number corresponding to the degree of similarity chosen as explained in Section 3.2. Therefore, the number of clusters is not extracted by intuition or by visualizing the dendrogram with this technique, so that the clustering obtained can represent better the data than when using the hierarchical clustering method at certain times since it is not biased with the subjectivity of the expert.

The membership of an input pattern was found by calculating the distance from itself to the clusters centroids. Once known which cluster the input pattern belonged to, the model corresponding with such cluster was used (model trained with data belonging to the cluster). As previously mentioned, it should be emphasized that the inputs of the clustering algorithm were not all patient records, but only those containing just five consecutive hemoglobin samples. This fact means that a particular patient can show different behaviors (that is, belong to different clusters) at different time intervals.

Hierarchical Clustering Algorithms (HCA) are characterized, among other things, because they do not generate a single clustering but produce a set of hierarchically structured clustering (one in each hierarchical step) [18]. In hierarchical clustering, m different partitions of the input data are generated into clusters, where m is the number of objects in the input data. One of these partitions corresponds to a single cluster made up of all m objects of the input data, while at the opposite extreme there is a partition corresponding to m clusters, each made up of just one object. Between these extremes there is a partition with 2 clusters, one with 3 clusters, and so on up to a partition with m
                        −1 clusters. The key characteristic of these partitions, which makes them hierarchical, is that the partition with r clusters can be used to produce the partition with r
                        −1 clusters by merging two clusters, and it can also be used to produce the partition with r
                        +1 clusters by splitting a cluster into two [18].

The Adaptive Resonance Theory (ART) proposes an approach to deal with the stability–plasticity dilemma [16]. ART operates as a two-stage process. Each time a pattern is presented, an appropriate cluster unit is chosen, and that cluster's weights are adjusted to let the cluster unit learn the pattern. The weights on a cluster unit are considered to be a prototype for the patterns assigned to that cluster. The second and crucial stage of the recognition process is to test whether the prototype forms an adequate representation of the input pattern. Once a good-enough winning prototype has been selected, the process is referred to a vigilance test. From this, either the prototype is updated to form a running average of the input vector, or a new prototype is initiated. ART networks allow the algorithm's designer to control the degree of similarity of patterns placed on the same cluster; once this choice is done, it is not necessary to choose the number of clusters in advance, but the network finds the number corresponding to the degree of similarity chosen.

The clusters obtained by hierarchical clustering and ART were used to model the problem locally, that is, the models explained in this section have been applied separately to each one of the clusters. Moreover, prediction models were also used as global models.

A multiple linear regression is defined, for a kth dimensional data sample x
                        
                           i
                        , y
                        
                           i
                        , (i
                        =1, 2…, n; being n the number of instances), by
                           
                              (1)
                              
                                 
                                    y
                                    i
                                 
                                 =
                                 
                                    b
                                    0
                                 
                                 +
                                 
                                    b
                                    1
                                 
                                 ·
                                 
                                    x
                                    
                                       i
                                       1
                                    
                                 
                                 +
                                 
                                    b
                                    2
                                 
                                 ·
                                 
                                    x
                                    
                                       i
                                       2
                                    
                                 
                                 +
                                 ⋯
                                 +
                                 
                                    b
                                    k
                                 
                                 ·
                                 
                                    x
                                    ik
                                 
                              
                           
                        or, equivalently, in more compact matrix terms,
                           
                              (2)
                              
                                 
                                    
                                       Y
                                    
                                 
                                 =
                                 
                                    
                                       X
                                    
                                 
                                 ·
                                 
                                    
                                       b
                                    
                                 
                                 +
                                 
                                    
                                       E
                                    
                                 
                              
                           
                        where for all the n observations considered, Y is a column vector with n rows containing the values of the response variable; X is a matrix with n rows and k
                        +1 columns containing for each column the values of the explanatory variables for the n observations, plus a column (to refer to the intercept) containing n values equal to 1; b is a vector with k
                        +1 rows containing all the model parameters to be estimated on the basis of the data (the intercept and the k slope coefficients relative to each explanatory variable); and E is a column vector of length n containing the error terms. It is necessary to estimate the vector of the parameters (b
                        0, b
                        1, …, b
                        
                           k
                        ) on the basis of the available data.

Tree models begin by producing a classification of observations into groups and then obtaining a score for each group [19,20]. Tree models are usually divided into Regression Trees, when the response variable is continuous, and classification trees, when the response variable is quantitative discrete or qualitative (categorical) [21]. Tree models can be defined as a recursive procedure, through which a set of n statistical units are progressively divided into groups, according to a division rule that aims to maximize a homogeneity or purity measure of the response variable in each of the obtained groups. At each step of the procedure, a division rule is specified by the choice of an explanatory variable to split and the choice of a splitting rule for the variable, which establishes how to partition the observations. The main result of a tree model is a final partition of the observations. To achieve this, it is necessary to specify stopping criteria for the division process. The output of the analysis is usually represented as a tree. This implies that the partition performed at a certain level is influenced by the previous choices. The two main aspects are the division criteria and the methods employed to reduce the dimension of the tree (pruning).

Bootstrap aggregation, or bagging, is a technique [22] that can be used with many classification and regression methods to reduce the variance associated with prediction, and thereby improve the prediction process. Bagging is the idea of collecting a random sample of observations into a bag (though the term itself is an abbreviation of bootstrap aggregation). Multiple bags are made up of randomly selected observations obtained from the original observations from the training dataset. Many bootstrap samples are drawn from the available data, some prediction method is applied to each bootstrap sample, and then the results are combined, by averaging for regression and simple voting for classification, to obtain the overall prediction, with the variance being reduced due to the averaging. In each of these sets, some examples are not chosen, and some are duplicated. On average, each set contains about 63% of the original examples in our experiments. Bagging works best with unstable learners, that is those that produce different generalization patterns with small changes to the training data. Averaging over a collection of fitted values can help compensate for over-fitting [23].

A Multilayer Perceptron, MLP, is an Artificial Neural Network ANN, formed by elementary processing units, the so-called neurons. A typical neuron model is shown in Fig. 2
                         
                        [24].

As it can be inferred from Fig. 2, a neuron without an activation function is equivalent to a multi-variant analysis. Therefore, a non-linear combination should be more powerful than a multi-variant analysis [25]. Neurons are arranged in layers to form an MLP. The first layer is known as input layer, and the last one is called output layer. All the other layers are called hidden layers [26]. This kind of arrangement enables the neuron outputs to be used as inputs to neurons of following layers (non-recurrent network) and/or previous layers (recurrent networks). Fig. 3
                         shows a typical MLP structure.

It should be taken into account the capability of this model, which is able to establish any relation between two datasets. This fact is mathematically proven by the Cybenko's Theorem [24].

Support Vector Machines (SVM) is another model that acts as a non-probabilistic binary linear classifier [27]. SVM works by constructing a hyperplane or set of hyperplanes in a high- or infinite-dimensional space, which can produce a linearly separable solution to a classification or regression problem. In the same manner as the non-linear SVM for classification approach, a non-linear mapping can be used to map the data into a high dimensional feature space where linear regression is performed [28]. The kernel approach is also employed to address the curse of dimensionality.

A committee is an ensemble method that consists in taking a combination of several models to form a new model. In the case of a linear combination, the committee learning algorithm tries to train a set of models {s
                        1, …, s
                        
                           P
                        } and choose coefficients {β
                        1, …, β
                        
                           P
                        } to combine them as
                           
                              (3)
                              
                                 y
                                 (
                                 
                                    
                                       
                                          x
                                       
                                    
                                    i
                                 
                                 )
                                 =
                                 
                                    ∑
                                    
                                       k
                                       =
                                       1
                                    
                                    P
                                 
                                 
                                    β
                                    k
                                 
                                 
                                    s
                                    k
                                 
                                 (
                                 
                                    
                                       
                                          x
                                       
                                    
                                    i
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          s
                                       
                                    
                                    i
                                    T
                                 
                                 
                                    
                                       β
                                    
                                 
                                 ,
                              
                           
                        where 
                           
                              
                                 
                                    s
                                 
                              
                              i
                           
                           =
                           
                              
                                 
                                    
                                       
                                          s
                                          i
                                       
                                       (
                                       
                                          
                                             
                                                x
                                             
                                          
                                          i
                                       
                                       )
                                       ,
                                       …
                                       ,
                                       
                                          s
                                          P
                                       
                                       (
                                       
                                          
                                             
                                                x
                                             
                                          
                                          i
                                       
                                       )
                                    
                                 
                              
                              T
                           
                         are the predictions of each committee member on x
                        
                           i
                        .

In this work, two methods have been used to compute the coefficients that combine the committee members. The first method uses least squares regression, and the second one uses quantile regression. The quantile regression [29] seeks to model the relationship between the input (x) and output (y) for different quantiles of the distribution of the error committed by the model. Whereas the method of least squares minimizes the sum of squared errors committed by the model.

@&#RESULTS@&#

This section summarizes the achieved results in order to compare the performance of the models previously explained.

This section presents an analysis of the most relevant features. Due to the high number of variables available in the datasets, it is probable that some of them may be irrelevant or redundant to predict Hb level. Adding irrelevant attributes to a dataset often confuses machine learning systems, thus becoming noise instead of useful information. Moreover, it entails worst generalization and a larger number of patterns is needed to fit the model. Therefore, the data must be preprocessed to select a subset of variables before apply any learning method.

Two techniques have been applied in order to discriminate the irrelevant features. On the one hand, statistically significant features in a linear model were found. This set of features was used for training the linear model. On the other hand, a sensitivity analysis was performed using a Multilayer Perceptron (MLP) model. This latter selection of features was then used for the rest of models presented in this paper. Table A.1 in Appendix A shows the results of both techniques for Spain and Italy datasets. Only a subset of the most relevant ones (30 variables) has been used for the model developing. For the sake of simplicity, only the most 10 relevant variables are presented in Table A.1 sorted by relevance (most relevant on the top). Table B.1 in Appendix B lists the meaning of the variables included in Table A.1.

Due to the many different approaches that have been tested, and in order to present the results in a compact format that makes easy to compare the performance of the different approaches, Tables 3 and 4
                         and summarize the performance of the prediction carried out by the best models for Italy and Spain, respectively. Results refer to the corresponding test sets. The name of each model (in the case of local models) consists of two terms separated by a low hyphen. The first concerns the used prediction model and the second the type of clustering used to the local prediction. When the term is Hb, only the current value of Hb and the four previous values of Hb are taken into account for the clustering (1st type in Section 3). When it is EPO, the same variables as in previous case are taken into account but also including EPO dosages in the considered period of time (until four previous values of Hb) (2nd type in Section 3). When the term is Hb_derivative, it is taken into account the current value of Hb and time derivatives (Derivative of Hb level with respect to time) of the differences between consecutive values of Hb in the four previous samples (3rd type in Section 3). Finally, when the term is Hb_EPO, the same variables as in the previous case are taken into account as well as the EPO dosages in the considered period of time (4th type in Section 3).

As shown in Tables 3 and 4, many different approaches have been tested, with different performances, but there is not a model that clearly outperforms the others. It should be noted that the error distribution of the quantiles Q25 and Q75 is between −0.5 and 0.5, so that MAE greater than 0.6 is due to a small number of patterns (outliers) with large error. Prediction errors in the test cohorts of patients were around 0.6g/dl. Moreover, it was though that local models would present a significant improvement, regarding the committed error, but finally this assumption was refuted. This could be because it has been exploited this dataset obtaining the maximum performance, hence, there is not a model that clearly outperforms the others.

@&#CONCLUSIONS@&#

This paper has presented a summary of the main results derived from the research on the database of European hemodialysis patients provided by Fresenius Medical Care (FMC). A set of many different prediction algorithms has been applied to the dataset. Prediction errors in the test cohorts of patients were around 0.6g/dl. It should be taken into account that the Hb measure presents a systematic error of up to 0.2g/dl, which can be accumulated in consecutive measures; in particular, between two consecutive hemoglobin samples, systematic error may be up to 0.4g/dl. Therefore, the predictions can be considered at least as acceptable, taking into account the difficulty of obtaining lower errors due to the precision of the measuring machine (0.2g/dl). Moreover, it should be noted that the error distribution of the quantiles Q25 and Q75 is between −0.5g/dl and 0.5g/dl,so that the MAE greater than 0.6g/dl is due to a small number of patterns (outliers) with large error.

Clustering techniques based on a hierarchical approaches and the Adaptive Resonance Theory (ART) have been used as the first step in local Hb predictors, in which a different predictor was used for each cluster.

Moreover, a relevance analysis has been carried out for each predictor model, thus finding those features that are most relevant for the given prediction.

None declared.


                     Table A.1
                     .


                     Table B.1
                      lists all the variables on the anemia database with their corresponding meanings.

@&#REFERENCES@&#

