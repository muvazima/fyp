@&#MAIN-TITLE@&#Approximate dynamic programming for the dispatch of military medical evacuation assets

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We consider the dispatching of aerial military medical evacuation (MEDEVAC) assets.


                        
                        
                           
                           A Markov decision process model of the MEDEVAC dispatching problem is formulated.


                        
                        
                           
                           We utilize approximate dynamic programming to obtain high quality dispatch policies.


                        
                        
                           
                           In a representative scenario, our policy improves system performance by 31%.


                        
                        
                           
                           Our work benefits military medical planners seeking to save lives on the battlefield.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Approximate dynamic programming

Emergency medical dispatch

Least squares temporal differences

Markov decision processes

Military medical evacuation (MEDEVAC)

@&#ABSTRACT@&#


               
               
                  Military medical planners must consider the dispatching of aerial military medical evacuation (MEDEVAC) assets when preparing for and executing major combat operations. The launch authority seeks to dispatch MEDEVAC assets such that prioritized battlefield casualties are transported quickly and efficiently to nearby medical treatment facilities. We formulate a Markov decision process (MDP) model to examine the MEDEVAC dispatching problem. The large size of the problem instance motivating this research suggests that conventional exact dynamic programming algorithms are inappropriate. As such, we employ approximate dynamic programming (ADP) techniques to obtain high quality dispatch policies relative to current practices. An approximate policy iteration algorithmic strategy is applied that utilizes least squares temporal differencing for policy evaluation. We construct a representative planning scenario based on contingency operations in northern Syria both to demonstrate the applicability of our MDP model and to examine the efficacy of our proposed ADP solution methodology. A designed computational experiment is conducted to determine how selected problem features and algorithmic features affect the quality of solutions attained by our ADP policies. Results indicate that the ADP policy outperforms the myopic policy (i.e., the default policy in practice) by up to nearly 31% with regard to a lifesaving performance metric, as compared for a baseline scenario. Moreover, the ADP policy provides decreased MEDEVAC response times and utilization rates. These results benefit military medical planners interested in the development and implementation of cogent MEDEVAC tactics, techniques, and procedures for application in combat situations with a high operations tempo.
               
            

@&#INTRODUCTION@&#

United States military medical planners must consider the dispatching of aerial military medical evacuation (MEDEVAC) assets when preparing for and executing major combat operations. MEDEVAC involves the rapid transport of prioritized battlefield casualties from the point of injury (POI) to the nearest appropriate medical treatment facility (MTF) by dedicated, standardized MEDEVAC platforms with onboard medical personnel providing care en route.

Aerial MEDEVAC operations involve the use of dedicated helicopters, specifically the UH-60 A/L Blackhawk. Helicopters are well suited for MEDEVAC operations because of their ability to travel faster, farther, and access terrain not accessible to ground vehicles. The ability to simultaneously treat and quickly transport casualties from the POI to an MTF greatly increases the probability of survival for casualties. Recent experience indicates that survivability of injuries on the battlefield is at a historic high; 90% of all casualties survive in post 9/11 combat operations, compared to 84% in Vietnam and 80% in World War II (Eastridge et al., 2012). This improvement is attributed primarily to the speed with which casualties are able to receive proper medical attention.

Military medical planners must consider many important aspects when designing a deployed military emergency medical service (EMS) system (Department of the Army, 2007). Such aspects include determining MEDEVAC unit locations, identifying a MEDEVAC dispatching policy, and considering the redeployment or relocation of MEDEVAC units after a mission starts or terminates. The determination of locations for MEDEVAC assets to support combat operations is influenced by the tactical situation. Location decisions require a balance between maximizing coverage and minimizing response time subject to resource, force protection, and logistical constraints. The decision of which MEDEVAC unit to launch (i.e., dispatch) to a given service call is an important aspect of the military EMS system and is the focus of this paper. The MEDEVAC launch authority (i.e., the dispatching authority) typically defaults to a myopic policy in which the nearest MEDEVAC unit is launched to a POI regardless of its categorized priority (among three possible tiers). Redeployment of MEDEVAC assets from an MTF directly to a new POI without returning to the MEDEVAC unit’s originating base is possible; however, refueling and medical resupply requirements, armed escort availability, communication challenges, and crew basing limitations often make MEDEVAC redeployment difficult, and it is not often performed within a theater of operations. For similar reasons, the temporary relocation of idle MEDEVAC assets to a different base is also not routinely performed.

In this paper, we consider the MEDEVAC dispatching problem in which a dispatching authority must decide how to dispatch MEDEVAC units to prioritized requests for service. We develop a Markov decision process (MDP) model of the MEDEVAC dispatching problem. The large size of the motivating problem instance yields a high-dimensional and uncountable state space, rendering classical dynamic programming methods inappropriate. As such, we apply approximate dynamic programming (ADP) techniques to obtain high quality dispatch policies. We develop an approximate policy iteration (API) algorithm that utilizes least-squares temporal difference (LSTD) learning for policy evaluation. We define a set of basis functions within a linear architecture to approximate the value function around the post-decision state. To demonstrate the applicability of our MDP model and to examine the efficacy of our proposed solution methodology, we construct a notional, representative planning scenario based on contingency operations in northern Syria. A designed computational experiment is conducted to determine how selected problem features and algorithmic features affect the quality of solutions attained by our ADP policies.

The unique military aspect of the MEDEVAC dispatching problem warrants discussion. The lack of unclassified historical, extensible data presents difficulties for those interested in examining military EMS systems. We cannot simply analyze computer-aided dispatch (CAD) data to obtain the necessary parameters for our MDP model. When seeking to inform decisions regarding the dispatching of MEDEVAC assets (e.g., in anticipation of a major combat operation), it is necessary to have notions of the anticipated mission, enemy disposition, terrain, weather, friendly forces disposition, logistical support availability, time constraints, and civil authority considerations. Within a military context, disposition denotes whether a unit is passively defending an area, actively patrolling it, or conducting some other deliberate operation. Calls for service result from the interaction of the opposing forces in hostile actions. Such actions are very fluid with respect to both location and intensity. Even when the sizes, locations, and dispositions of friendly forces are relatively stable, the locations and dispositions of adversary forces are dynamic and challenging to predict. We develop a simulation of the MEDEVAC process that implicitly considers such information. For example, deliberate selection of the casualty cluster centers within the Hawkes process (Hawkes, 1971) governing casualty event arrivals embeds the information concerning the interaction of friendly and adversary forces. The Hawkes process assumes that subsequent events are more likely to occur in close proximity to prior events, and so it works well when simulating the spatiotemporal aspects of both criminal and terrorist activity (Egesdal et al., 2010; White, Porter, & Mazerolle, 2013); moreover, Lewis, Mohler, Brantingham, and Bertozzi (2012) show that this process well represents casualty incidents within a country fighting an insurgency. We utilize the MEDEVAC process simulation within our ADP algorithm to obtain simulated trajectories of the system and to measure the performance of the MEDEVAC dispatch policies obtained from our ADP algorithm.

The remainder of this paper is organized as follows. Section 2 presents a review of pertinent literature concerning civilian and military EMS systems. Several ADP papers that inform the development of our model and solution methodology are also reviewed. Section 3 presents a description of the MEDEVAC dispatching problem. Section 4 describes the MDP model formulation of the MEDEVAC dispatching problem and presents our ADP approach. In Section 5 we demonstrate the applicability of our model and examine the efficacy of our proposed solution methodology. Section 6 provides conclusions and directions for future research.

@&#LITERATURE REVIEW@&#

Two streams of literature inform our work. The first stream of literature relates to emergency medical service (EMS) systems, with an emphasis on the dispatching of emergency response vehicles to calls for service. The second stream of literature involves selected works concerning approximate dynamic programming (ADP).

Past research endeavors examine both civilian and military EMS systems. Important aspects considered include determining server locations; defining the number and sizes of response districts (if a service area partitioning scheme is adopted), the number of servers per server location, and the server dispatch policy; possibly repositioning servers as a consequence of call initiation or service completion; and considering whether to focus on response time or patient survivability as the objective. Military medical planners also examine optimal placement of medical treatment facility (MTF) locations when designing a military medical evacuation (MEDEVAC) system, whereas hospital locations are typically given as fixed in civilian EMS studies. Various operations research methods are employed to examine EMS systems. These methods include, but are not limited to, discrete optimization, stochastic modeling, queueing, and simulation modeling.

Research from the 1970s and 1980s focuses primarily on civilian EMS systems and examines such aspects as the optimal placement (Fitzsimmons, 1973; Kolesar & Blum, 1973), relocation (Kolesar & Walker, 1974), and dispatch of emergency vehicles (Green & Kolesar, 1984; Ignall, Carter, & Rider, 1982; Larson, 1972; Swersey, 1982) to improve the performance of the EMS system. See Green and Kolesar (2004) for an account of early emergency service management papers. Few papers seek to improve the performance of the military EMS (i.e., MEDEVAC) system. In this paper, we focus on the dispatch of emergency response via aerial MEDEVAC to prioritized requests for service. Examining the dispatch policy of emergency response vehicles requires a dynamic and stochastic approach. Moreover, consideration of casualty priority classification is important. Many EMS system dispatch policies do not account for the priority level of the patient (Bandara, Mayorga, & McLay, 2014). This omission results in the nearest emergency response unit fulfilling the service requirement without regard to the void created in the system by that unit’s temporary absence. Known as a myopic policy, such procedures are shown to be inadequate by many researchers (Carter, Chaiken, & Ignall, 1972; Kuisma et al., 2004; Nicholl, Coleman, Parry, Turner, & Dixon, 1999); it follows that better informed priority dispatching can save more lives on the battlefield.

Another important aspect of the dispatching problem is the selection of the system optimality criterion (or performance measure). Selection of an appropriate EMS system performance measure is important since it governs employment of EMS system resources and ultimately determines patient survivability (McLay & Mayorga, 2010). EMS system performance is most often measured in terms of response time threshold (RTT), indicating the proportion of calls that are serviced within a fixed timeframe. A service call is considered covered if served by the stated time threshold. An often employed MEDEVAC system performance measure is the proportion of Priority I (Urgent) calls served in less than 60 minutes. While RTTs are easy to evaluate and easy to explain, one important criticism relates to how well the performance measure captures patient survival, the fundamental objective of the EMS system. For example, a system would receive credit for a MEDEVAC arriving at the MTF 59 minutes and 59 seconds after the call, but not for arriving at exactly 60 minutes. Yet, the medical literature (Fitch, 2005) suggests that there would be little difference in patient survival between two such cases. Erkut, Ingolfsson, and Erdoğan (2008) compare the utilization of RTTs to patient survival rates in an ambulance location problem; the authors recommend incorporating patient survivability explicitly and avoiding the use of RTTs, if possible. Unfortunately, as McLay and Mayorga (2010) note, estimating patient survivability is quite difficult. For our purposes, survival is defined in terms of discharge from the military medical system, which might occur several months later and after many transfers from field hospitals in the combat zone to larger, permanent facilities in the United States. It would be very difficult to retroactively determine MEDEVAC responses that resulted in survival.

Despite such challenges, Erkut et al. (2008) propose the use of a monotonically decreasing function over time for the probability of patient survival as a measure of performance. Subsequent work by Bandara, Mayorga, and McLay (2012), Mayorga, Bandara, and McLay (2013), Bandara et al. (2014), and Grannan, Bastian, and McLay (2015) similarly use survivability functions instead of the traditional RTT as a performance measure for their model. We also follow Erkut et al. (2008) by assuming a function that is monotonically decreasing in response time to model casualty outcome. A challenge with the use of a survivability function is the difficulty finding empirical evidence to support a particular functional form. Eastridge et al. (2012) provide extensive statistics about combat deaths, but the response times are not known, preventing the creation of a high confidence survivability function from their results. Although Feero, Hedges, Simmons, and Irwin (1995) examine EMS response times relative to trauma survivability, their work is limited to response times under eight minutes. MEDEVAC units typically need to travel significantly further than civilian EMS units, and so response times are notably longer. The MEDEVAC system seeks to respond within 60 minutes from notification to drop-off of the Priority I patient at an MTF (Garrett, 2013). The stated RTTs for each of the casualty priority levels (i.e., 60 minutes, 240 minutes, and 24 hours; Department of the Army, 2007) serves to inform the development of our casualty outcome functions.


                        Bandara et al. (2012) mention several studies in which the EMS system greatly improves patient survival probability if priority level is considered when deciding which emergency response vehicle to dispatch. The authors construct an MDP model that focuses on the priority level of an emergency call. Their study indicates an optimal policy in which the closest available (i.e., idle) unit is dispatched to the most urgent call and the next closest available unit to the less urgent call, regardless of the call order. While this result may seem intuitive, this dispatch policy quickly becomes complex. For example, it may be optimal to dispatch a more distant vehicle if the closer vehicle is more likely to receive a higher priority call. This policy essentially rations the closer vehicle in anticipation of a more urgent request. For problems with several more service zones and ambulances, the policy might not be as intuitive, although EMS systems may stand to benefit greatly from the employment of an optimal policy versus a myopic policy. Bandara et al. (2014) develop a simulation model to evaluate dispatch policies for EMS systems. The authors measure performance in terms of patient survival probability. Their results also indicate that dispatching the closest emergency response vehicle is not always optimal and that dispatching vehicles considering priority levels leads to an increase in the average survival probability of patients. The authors utilize their results to develop a heuristic that could be applied to the analysis of large-scale systems.


                        Mayorga et al. (2013) examine the dispatch policy within the EMS system wherein performance is measured in terms of the probability of patient survival. Prior to examining the dispatch policies, however, the authors obtain solutions for the number of districts and district location decisions by developing a construction heuristic. Their research provides more depth than previous studies by analyzing the dispatch policies for inter-district and intra-district situations. An intra-district situation occurs when a response vehicle services the call within its own district, whereas an inter-district situation occurs when all response vehicles within a district area are busy and the call must be serviced by a response vehicle from a different district. For the inter-district policy, either a myopic policy (i.e., the closest vehicle responds) or a heuristic policy (e.g., as developed in Bandara et al., 2012) is implemented. Whereas the myopic policy is the most widely applied policy in EMS systems, the heuristic policy considers the priority of the call as well as the workload of each crew. For such an implementation, a utilization factor is included to account for the workload of each crew. For the intra-district policy, two policies are considered. The first policy assumes that a sister emergency service (e.g., fire department or police department) will respond. The second policy uses the heuristic policy that Bandara et al. (2012) developed and allows a response vehicle from another district to cross boundary lines.


                        McLay and Mayorga (2013b) examine the dispatch policy within the EMS system while considering patient classification errors. McLay and Mayorga (2013b) differ from Mayorga et al. (2013) and Bandara et al. (2012) in that they consider patient classification errors and employ an optimality criterion based on RTT rather than patient survivability. They focus on the patients’ urgency level with an overall objective of maximizing the average long-run utility of the EMS system, rewarding the expected coverage of high-risk patients. McLay and Mayorga (2013a) examine the dispatch policy within a general server-to-customer service system (e.g., an EMS system) while considering the issue of balancing equity and efficiency. They develop a constrained variant of the Markov decision problem introduced by McLay and Mayorga (2013b) and formulate a linear programming model to solve the constrained problem. Their objective is to dispatch ambulances to patients to maximize average total reward subject to minimum standards of equity. They report results while considering four different notions of equity.

EMS system research exists that focuses on military MEDEVAC systems. Fulton, Lasdon, McDaniel, and Coppola (2010) and Bastian (2010) examine the allocation of MEDEVAC units in steady-state combat operations. Fulton et al. (2010) present a stochastic optimization model that locates deployable hospitals, reallocates hospital beds, and determines the location of emergency response vehicles (both aerial and ground MEDEVAC assets) prior to MEDEVAC system initialization. Their objective function minimizes total penalty-weighted expected service times. The penalty weight captures patient severity, derived from the historical data collection of patient injury severity scores from combat operations in the Iraq theater. Bastian (2010) describes a multi-criteria modeling approach that optimizes the emplacement of MEDEVAC assets. The author considers casualty demand coverage, MEDEVAC spare capacity, and site attack vulnerability. Keneally, Robbins, and Lunday (2014) develop an MDP model to examine MEDEVAC dispatch policies in the Afghanistan theater. The authors consider priority classifications and a reward function based on RTT, and they simulate a Hawkes process for casualty generation. They observe that myopic policies do not always lead to optimal EMS system performance. Grannan et al. (2015) examine location decisions in the Afghanistan theater. The authors propose a binary integer programming model to optimally locate two types of aerial MEDEVAC assets. The underlying queueing dynamics of their MEDEVAC system are captured using an approximate spatial queuing model, with the results serving to parameterize their integer program. The results obtained in the aforementioned research endeavors serve to inform decision-making by military medical planners.

The decisions concerning which MEDEVAC to dispatch to a request for service must be made sequentially over time and under uncertainty. Since dispatch decisions impact the capability of the MEDEVAC system to serve future requests, we must account for how current decisions affect the future state of the system. As such, we formulate a Markov decision process (MDP) model of the MEDEVAC dispatching problem. Unfortunately, due to the well-known curses of dimensionality, an optimal policy cannot be identified using classical exact dynamic programming algorithms. Instead, we employ an ADP solution methodology to solve the MEDEVAC dispatching problem. For an introduction to ADP from an operations research perspective we refer the reader to Powell (2009, 2011, 2012). Earlier contributions from an engineering controls perspective (Bertsekas & Tsitsiklis, 1996) and from a computer science (artificial intelligence) perspective (Sutton & Barto, 1998) also provide excellent overviews.

Two general algorithmic strategies exist for obtaining approximate solutions to our computational stochastic optimization problem: approximate value iteration (AVI) and approximate policy iteration (API). The interested reader is referred to Bertsekas (2011) for a detailed discussion concerning API. We utilize an API algorithmic strategy to obtain a policy that maps the system state (e.g., MEDEVAC locations and service status, casualty event locations and priorities) to an action (e.g., dispatching a specific MEDEVAC unit to service a new or queued call). Powell (2012) discusses four classes of policies: myopic cost function approximation, lookahead policies, policy function approximations, and policies based on value function approximations. We construct high-quality dispatch policies based on value function approximations. Our approximation strategy involves the design of an appropriate set of basis functions for use within a linear architecture. Moreover, we approximate the value function around the post-decision state. First introduced by Van Roy, Bertsekas, Lee, and Tsitsiklis (1997), the post-decision state allows for modification of Bellman’s equation to obtain an equivalent, deterministic expression. Use of the post-decision state addresses the curse of dimensionality with respect to the outcome space (Powell, 2011). Within the policy evaluation step of our API algorithm, we update the value function approximation for a fixed policy using least squares temporal differencing (LSTD). Introduced by Bradtke and Barto (1996), LSTD is a computationally efficient method for estimating the adjustable parameters when using a linear architecture with fixed basis functions to approximate the value function for a fixed policy. Lagoudakis and Parr (2003) extend the LSTD algorithm to include the consideration of state-action pairs. We implement a variant of the LSTD algorithm that utilizes value function approximations around the post-decision state, as recommended by Powell (2011) and demonstrated in Scott, Powell, and Moazeni (2014).

Two recent papers utilize ADP to examine ambulance repositioning (i.e., relocation or redeployment) and dispatching in civilian EMS systems. These two papers greatly informed the development of our paper and warrant discussion.


                        Maxwell, Restrepo, Henderson, and Topaloglu (2010) utilize an API algorithmic strategy to determine high quality ambulance redeployment decisions. At each system event occurrence, such as a new call arrival or an ambulance arrival at a hospital after transporting a patient, a decision is made to redeploy any idle ambulances to better cover the service area. The authors seek polices that minimize expected total discounted cost and define the single-period cost function in terms of an urgent call not being covered. That is, their system sustains a cost if the RTT requirement is not met. They demonstrate significant improvement in EMS response at two metropolitan cities, comparing their ADP redeployment policy to the currently practiced static, myopic policy. Our paper differs from Maxwell et al. (2010) in a number of ways. We approximate the value function around the post-decision state, whereas they use the pre-decision state. We employ temporal differencing in the policy evaluation step of our API algorithm, whereas they do not. We define a reward function based on casualty survival, whereas they define cost in terms of RTT. We focus on dispatching decisions, whereas they focus on redeployment decisions.


                        Schmid (2012) adopts an AVI algorithmic strategy to determine high quality relocation and dispatching decisions. At each system event occurrence, a decision is made to (1) relocate any idle ambulances to better cover the service area and (2) dispatch available ambulances to requests for service. Polices are sought that minimize expected total discounted cost, wherein the single-period cost function is defined in terms of response times. The author demonstrates improvement in EMS response for the city of Vienna, Austria, reporting that the ADP policy reduces average response time from 4.6 to 4.01 minutes, a 12.9% reduction. Our paper differs from Schmid (2012) in the application and corresponding model, as well as in the methodology we apply to solve it. We consider a military application of the dispatching problem that requires implicit consideration of escort vehicle (i.e., attack helicopters) availability. This military aspect of the problem is captured within our simulation in terms of the random dispatch time. In a civilian context, such a phenomenon would be akin to a requirement for police escort for some service calls. The most significant difference involves the algorithmic strategy; we employ API, whereas the author uses AVI. Although the author does approximate the value function around the post-decision state as we do in this research, we utilize a linear model with a designed set of basis functions to approximate the value function, whereas the author employs an aggregation scheme.

The aeromedical evacuation coordination center (AECC) is the primary decision-making authority within the military medical evacuation (MEDEVAC) system, responsible for monitoring and managing all activities related to the execution of aeromedical evacuation operations (Department of the Army, 2007). In a deployed military emergency medical service (EMS) system, the AECC acts as the MEDEVAC dispatching authority. When a casualty occurs and a ‘9-line’ MEDEVAC request is received, a decision must be made quickly regarding which MEDEVAC asset (if any) to dispatch. Delays in decision making affect casualty survivability. Thus, it is critical that the AECC employs a dispatching policy that results in the quick and efficient transport of high priority battlefield casualties from the point of injury (POI) to the nearest medical treatment facility (MTF). The MEDEVAC dispatching process is depicted in Fig. 1
                     .

A 9-line MEDEVAC request is transmitted over secure radio on a dedicated MEDEVAC frequency and contains a standard, prescribed amount of information. Such information includes the location of the POI, radio frequency and call sign of the requesting unit, number of patients by precedence, special equipment needed, casualty nationalities, and threat conditions at the POI. When making the MEDEVAC request, the on-site medic and unit leader apply a three category casualty triage rubric to govern evacuation precedence (Department of the Army, 2007):

                        
                           •
                           
                              Priority I, Urgent: Assigned to emergency cases that should be evacuated as soon as possible and within a maximum of 1 hour in order to save life, limb, or eyesight, to prevent complications of serious illness, or to avoid permanent disability.


                              Priority II, Priority: Assigned to sick and wounded personnel requiring prompt medical care. This precedence is used when the individual should be evacuated within 4 hours or when an individual’s medical condition could deteriorate to such a degree that he or she will become an Urgent precedence, or whose requirements for special treatment are not available locally, or who will suffer unnecessary pain or disability.


                              Priority III, Routine: Assigned to sick and wounded personnel requiring evacuation but whose condition is not expected to deteriorate significantly. The sick and wounded in this category should be evacuated within 24 hours.

MEDEVAC requests are typically transmitted from the POI, through intermediary headquarters, to the AECC at higher headquarters. The particular information flow depends on the communications equipment available to the requesting unit, the communication infrastructure within the command, and the command and control organization of the MEDEVAC system. The time at which the MEDEVAC request is received by the AECC is denoted by T
                     1. The AECC may or may not immediately assign a MEDEVAC unit
                        1
                     
                     
                        1
                        We note that, in U.S. military doctrine, a MEDEVAC unit is typically comprised of two MEDEVAC helicopters when executing missions due to a mutual support safety requirement.
                      to the request, depending on the location of the request, the precedence categories of the casualties, the status of the MEDEVAC units, and any pre-existing requests in the system. If a suitable idle MEDEVAC unit is available, it is directed at time T
                     2 to service the request. The AECC transmits the 9-line MEDEVAC request information to the assigned MEDEVAC unit via the command’s communication system. The time at which the MEDEVAC unit departs its station for the POI is denoted by T
                     3. The amount of time required between receipt of the 9-line MEDEVAC request at the AECC, T
                     1, and the MEDEVAC unit departure, T
                     3, is the dispatch time. The dispatch time comprises the process duration of determining which MEDEVAC unit to dispatch; whether an armed escort is required; which armed escort unit to dispatch, if required; notifying the MEDEVAC unit; and preparing the medical personnel, medical equipment, and the MEDEVAC helicopters for the mission.

The MEDEVAC unit arrives on the ground at the POI site at time T
                     4 and begins treating and loading casualties immediately. The time at which initial treatment and loading ends and the MEDEVAC unit departs the POI site is denoted by T
                     5. The destination MTF is selected in a deterministic manner based on the location of the POI. The MEDEVAC unit travels to the nearest MTF, arriving at time T
                     6. The casualties are then unloaded and receive subsequent treatment from the medical staff at the MTF. Once the casualties are unloaded, the MEDEVAC unit departs the MTF and travels back to its own station (or staging area). The time at which the MEDEVAC unit completes the unloading process and departs the MTF is denoted by T
                     7. The time at which the MEDEVAC unit arrives at its station is denoted by T
                     8. Upon arrival, the MEDEVAC unit’s mission is complete, and it becomes available for dispatch once again. Note that travel times from the MEDEVAC station to the POI site, from the POI site to an MTF, and from the MTF to the MEDEVAC station are expected to vary based on battlefield conditions (e.g., weather conditions, enemy disposition, air density due to flight altitude, and the amount of equipment being transported).

MEDEVAC units are not dispatched to POI sites directly from nearby MTFs immediately after unloading casualties from the prior mission, nor are they repositioned to other MEDEVAC stations; MEDEVAC units must return to their own stations. This restriction results from concerns about low fuel levels, onboard medical equipment configurations, crew bed down limitations, and other logistical issues. Predominantly, MEDEVAC units must return to their stations to refuel and, if necessary, reconfigure onboard medical equipment.

EMS systems typically refer to response time as the amount of time required to reach the patient after receiving an emergency call. According to McLay and Mayorga (2010), the rapid response to cardiac arrest situations is a primary focus of the (civilian) EMS system. This rationale exists because the EMS system is often evaluated on how quickly it responds to emergency cardiac arrest calls since there is effective treatment for cardiac arrests, and they are time-sensitive. Also, if the EMS system can respond quickly enough to a cardiac arrest call, it is more likely to be successful with similar life-or-death situations. Therefore, it is quite intuitive that the response time for a civilian EMS system is typically defined as the time between the receipt of the emergency call and the time the first emergency response vehicle arrives at the injury site.

However, the performance of the MEDEVAC system cannot be evaluated using the same measures as the EMS system since several additional factors are involved when medically evacuating a casualty from a battlefield. Not only can the load times, travel times, and unload times be much greater and vary much more, the primary cause of death on the battlefield is blood loss, not cardiac arrest. Garrett (2013) reports that 85% of soldiers killed in action (KIA) were a direct result of blood loss. Very recent improvements have been made in this area by equipping MEDEVAC units with in-flight blood transfusion capabilities, but not enough data have been generated to alter the MEDEVAC system’s evaluation measure at the time of this research (Malsby III et al., 2013). Thus, we consider it far more critical to stabilize and transport casualties to the nearest MTF (with an available blood transfusion capability) and into surgery rather than to simply reach the casualties quickly. Accordingly, we define the response time for a MEDEVAC unit as 
                        
                           
                              T
                              6
                           
                           −
                           
                              T
                              1
                           
                        
                     . Moreover, we define service time for a MEDEVAC unit as 
                        
                           
                              T
                              8
                           
                           −
                           
                              T
                              2
                           
                           ,
                        
                      the time expended servicing the call.

The overall objective of the MEDEVAC system is to dispatch MEDEVAC units so as to minimize the response times for all MEDEVAC requests, with a particular focus on the response times of the urgent and priority casualties. The decisions concerning which MEDEVAC to dispatch to a 9-line request for service must be made sequentially over time, under uncertainty. Casualty events and the subsequent MEDEVAC requests are not known a priori and become known to the MEDEVAC system only upon receipt. Once a MEDEVAC request is initiated, the assigned MEDEVAC unit must respond promptly, moving to the POI and transporting the casualties to the nearest MTF while providing medical care en route. The stochastic nature of the problem results from the uncertain manner in which casualty events manifest. Moreover, the dispatch time, travel times, and service times are stochastic. Information about dispatch, travel, and service times is available and can be leveraged to parameterize our models. However, as noted previously, the challenge for military medical planners is identifying a dispatch policy before the commencement of a major combat operation. In such a situation, no casualty event data exist. We employ a spatiotemporal point process, a Hawkes process (Hawkes, 1971), to model casualty event arrivals. Deliberate selection of the casualty event cluster centers within the Hawkes process embeds the military planners’ expert judgment concerning the future interaction of friendly and adversary forces that results in the occurrence of casualty events.

@&#METHODOLOGY@&#

This section describes the Markov decision process (MDP) model formulation of the medical evacuation (MEDEVAC) dispatching problem. The approximate dynamic programming (ADP) methodology utilized to obtain high quality solutions to the problem is also presented.

The objective of the MDP model is to determine which MEDEVAC unit to dispatch in response to a given 9-line MEDEVAC request in order to maximize expected total discounted reward over an infinite horizon. The reward function is monotonically decreasing in response time to model casualty outcome. We assume casualty events arrive sequentially over time and consider two of the three casualty event precedence categories: urgent and priority (Department of the Army, 2007). Because the focus of rapid aerial MEDEVAC support is on life-saving evacuations, casualty events having a routine category are assumed to be handled by ground MEDEVAC or casualty evacuation (i.e., evacuation provided by non-medical personnel and equipment). Hence, we exclude routine calls for service from our analysis.

A self-exciting spatiotemporal point process called a Hawkes process is utilized to generate casualty events over time. The Hawkes process is commonly applied in seismology, where it captures the notion that earthquakes can have aftershocks, and those aftershocks can have aftershocks, and so on (Hawkes & Adamopoulos, 1973). In our context, the occurrence of casualty events in a self-exciting point process causes other nearby casualty events to be more likely to occur due to the presumption of continued violent interactions between friendly and adversary forces. Using military planning guidelines for combat operations, we identify casualty event cluster centers that correspond both to locations at which friendly operations to secure the populace and transport supplies are certain and at which enemy attacks are likely to be mounted while reducing the risk of exposure by enemy forces. We proceed by generating a sequence of casualty event centers from a stationary Poisson process on the set of identified casualty event cluster centers with arrival rate λ. We then generate clusters of offspring casualty events around each center in the sequence. The collection of all generated events forms the Hawkes process. See Kroese and Botev (2014) for a full description of the generation procedure.

Should a casualty event (i.e., MEDEVAC request or call) arrive when all MEDEVAC units are busy, the request is placed in a queue to be serviced later. Incorporating a queue in the model allows the possibility of waiting before launching a MEDEVAC. The decision to wait can be advantageous if a low priority casualty event has occurred while many other MEDEVAC units are busy. It may be better to remain on standby until other MEDEVAC units become available in case a high priority casualty event occurs. All casualties are evacuated to the nearest medical treatment facility (MTF). After casualties are delivered to an MTF, the MEDEVAC unit returns to its originating station (or base).

We leverage Maxwell et al. (2010) when developing the state space of our MDP model. The system state is represented by the tuple 
                           
                              s
                              =
                              (
                              τ
                              ,
                              ϵ
                              ,
                              M
                              ,
                              Q
                              )
                              ,
                           
                         wherein τ corresponds to the current system time and ϵ corresponds to the current event. The two main system components are (1) the MEDEVAC status tuple 
                           
                              M
                              =
                              (
                              
                                 m
                                 1
                              
                              ,
                              
                                 m
                                 2
                              
                              ,
                              …
                              ,
                              
                                 m
                                 a
                              
                              )
                              ,
                           
                         wherein mi
                         contains information about the ith MEDEVAC unit and a represents the number of MEDEVAC units in the system, and (2) the call queue status tuple 
                           
                              Q
                              =
                              (
                              
                                 q
                                 1
                              
                              ,
                              
                                 q
                                 2
                              
                              ,
                              …
                              ,
                              
                                 q
                                 b
                              
                              )
                              ,
                           
                         wherein qj
                         contains information about the jth call in the queue and b represents the maximum number of calls allowed in the queue. The state of MEDEVAC unit i is given as a tuple 
                           
                              
                                 m
                                 i
                              
                              =
                              
                                 (
                                 
                                    σ
                                    i
                                 
                                 ,
                                 
                                    d
                                    i
                                 
                                 ,
                                 
                                    t
                                    i
                                 
                                 )
                              
                              ,
                           
                         wherein σi
                         is the status of the MEDEVAC unit, di
                         is the expected time at which the current mission is completed (i.e., casualties delivered to the nearest MTF), and ti
                         is the starting time of the MEDEVAC unit’s mission. After a MEDEVAC unit delivers its onboard casualties to an MTF, it returns to base where it then becomes available to launch. The status of the MEDEVAC unit σi
                         can be “idle”, “en route to call j”, “at call j POI”, “en route to MTF with call j casualties”, “at MTF”, or “returning to base”. If the MEDEVAC unit is not idle, ti
                         corresponds to the starting time of the movement; otherwise, ti
                         represents the time of the current event cycle. The state of call j in the queue is 
                           
                              
                                 q
                                 j
                              
                              =
                              
                                 (
                                 
                                    δ
                                    j
                                 
                                 ,
                                 
                                    l
                                    j
                                 
                                 ,
                                 
                                    ζ
                                    j
                                 
                                 ,
                                 
                                    η
                                    j
                                 
                                 )
                              
                              ,
                           
                         where δj
                         is the status of the call in the jth position, lj
                         is the location of the call, ζj
                         is the time the call arrived in the system, and ηj
                         is the priority of the call. The status of a call δj
                         can be “queued” or “assigned to MEDEVAC unit i.” Once a MEDEVAC unit is launched to service a call, the queue status is updated. The call is removed from the queue once the servicing MEDEVAC unit delivers the call’s casualties to the MTF.

Events are triggered by changes in the status of a MEDEVAC or an arrival of a call. We assume MEDEVAC dispatch decisions only occur when an event occurs. Although it is possible to reroute a MEDEVAC mid-flight, delay and confusion in communication can cause large problems and this practice is not typical in such operations.

To capture dispatching decisions we let 
                           
                              M
                              
                                 (
                                 s
                                 )
                              
                              =
                              
                                 {
                                 i
                                 :
                                 
                                    σ
                                    i
                                 
                                 =
                                 ``
                                 i
                                 d
                                 l
                                 e
                                 ""
                                 }
                              
                           
                         denote the set of MEDEVACs available for dispatching and 
                           
                              Q
                              
                                 (
                                 s
                                 )
                              
                              =
                              
                                 {
                                 j
                                 :
                                 
                                    δ
                                    j
                                 
                                 =
                                 ``
                                 q
                                 u
                                 e
                                 u
                                 e
                                 d
                                 ""
                                 }
                              
                           
                         denote the set of casualty events awaiting service when the system is in state s. Let 
                           
                              
                                 x
                                 
                                    i
                                    j
                                 
                              
                              
                                 (
                                 s
                                 )
                              
                              =
                              1
                           
                         if MEDEVAC unit 
                           
                              i
                              ∈
                              M
                              (
                              s
                              )
                           
                         is dispatched to call 
                           
                              j
                              ∈
                              Q
                              (
                              s
                              )
                           
                         when the system is in state s, and 0 otherwise. Further, let 
                           
                              x
                              
                                 (
                                 s
                                 )
                              
                              =
                              
                                 {
                                 
                                    x
                                    
                                       i
                                       j
                                    
                                 
                                 
                                    (
                                    s
                                    )
                                 
                                 :
                                 i
                                 ∈
                                 M
                                 
                                    (
                                    s
                                    )
                                 
                                 ,
                                 j
                                 ∈
                                 Q
                                 
                                    (
                                    s
                                    )
                                 
                                 }
                              
                           
                        . The set of feasible decisions can then be written as:

                           
                              (1)
                              
                                 
                                    X
                                    
                                       (
                                       s
                                       )
                                    
                                    =
                                    
                                       {
                                       x
                                       
                                          (
                                          s
                                          )
                                       
                                       ∈
                                       
                                          
                                             {
                                             0
                                             ,
                                             1
                                             }
                                          
                                          
                                             |
                                             M
                                             (
                                             s
                                             )
                                             |
                                             ×
                                             |
                                             Q
                                             (
                                             s
                                             )
                                             |
                                          
                                       
                                       :
                                       
                                          ∑
                                          
                                             i
                                             ∈
                                             M
                                             (
                                             s
                                             )
                                          
                                       
                                       
                                          ∑
                                          
                                             j
                                             ∈
                                             Q
                                             (
                                             s
                                             )
                                          
                                       
                                       
                                          x
                                          
                                             i
                                             j
                                          
                                       
                                       
                                          (
                                          s
                                          )
                                       
                                       ≤
                                       1
                                       }
                                    
                                    .
                                 
                              
                           
                        
                     

The trajectory of the system is given by 
                           
                              {
                              
                                 (
                                 
                                    s
                                    k
                                 
                                 ,
                                 
                                    x
                                    k
                                 
                                 )
                              
                              :
                              k
                              =
                              1
                              ,
                              2
                              ,
                              …
                              }
                           
                         where sk
                         is the state of the system and xk
                         is the decision after the kth event has occurred. The following recursion captures the dynamics of the system: 
                           
                              
                                 s
                                 
                                    k
                                    +
                                    1
                                 
                              
                              =
                              ψ
                              
                                 (
                                 
                                    s
                                    k
                                 
                                 ,
                                 
                                    x
                                    k
                                 
                                 ,
                                 ω
                                 
                                    (
                                    
                                       s
                                       k
                                    
                                    ,
                                    
                                       x
                                       k
                                    
                                    )
                                 
                                 )
                              
                              ,
                           
                         where ω(sk, xk
                        ) is a random element of an appropriate space representation of the stochastic process related to casualty event arrivals and MEDEVAC travel, load delays, and unload delays, and where ψ is the transfer function.

The reward function 
                           
                              r
                              (
                              
                                 s
                                 k
                              
                              ,
                              
                                 x
                                 k
                              
                              ,
                              
                                 s
                                 
                                    k
                                    +
                                    1
                                 
                              
                              )
                           
                         is defined as follows:

                           
                              (2)
                              
                                 
                                    
                                       
                                       
                                       
                                          
                                             r
                                             (
                                             
                                                s
                                                k
                                             
                                             ,
                                             
                                                x
                                                k
                                             
                                             ,
                                             
                                                s
                                                
                                                   k
                                                   +
                                                   1
                                                
                                             
                                             )
                                          
                                       
                                    
                                    
                                       
                                       
                                       
                                          
                                             =
                                             
                                                {
                                                
                                                   
                                                      
                                                         
                                                            
                                                               β
                                                               1
                                                            
                                                            
                                                               
                                                                  (
                                                                  0.99
                                                                  )
                                                               
                                                               
                                                                  
                                                                     
                                                                        t
                                                                        
                                                                           k
                                                                           +
                                                                           1
                                                                        
                                                                     
                                                                     −
                                                                     
                                                                        ζ
                                                                        j
                                                                     
                                                                  
                                                                  60
                                                               
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            if
                                                            
                                                            
                                                               η
                                                               j
                                                            
                                                            =
                                                            ``urgent
                                                            ""
                                                            
                                                            and
                                                            
                                                            ϵ
                                                            
                                                               (
                                                               
                                                                  s
                                                                  
                                                                     k
                                                                     +
                                                                     1
                                                                  
                                                               
                                                               )
                                                            
                                                            =
                                                            ``MEDEVAC
                                                            
                                                            i
                                                         
                                                      
                                                   
                                                   
                                                      
                                                      
                                                         
                                                            delivers
                                                            
                                                            call
                                                            
                                                            j
                                                            
                                                            at
                                                            
                                                            MTF""
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               β
                                                               2
                                                            
                                                            
                                                               
                                                                  (
                                                                  0.99
                                                                  )
                                                               
                                                               
                                                                  
                                                                     
                                                                        t
                                                                        
                                                                           k
                                                                           +
                                                                           1
                                                                        
                                                                     
                                                                     −
                                                                     
                                                                        ζ
                                                                        j
                                                                     
                                                                  
                                                                  240
                                                               
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            if
                                                            
                                                            
                                                               η
                                                               j
                                                            
                                                            =
                                                            ``priority
                                                            ""
                                                            
                                                            and
                                                            
                                                            ϵ
                                                            
                                                               (
                                                               
                                                                  s
                                                                  
                                                                     k
                                                                     +
                                                                     1
                                                                  
                                                               
                                                               )
                                                            
                                                            =
                                                            ``MEDEVAC
                                                            
                                                            i
                                                         
                                                      
                                                   
                                                   
                                                      
                                                      
                                                         
                                                            delivers
                                                            
                                                            call
                                                            
                                                            j
                                                            
                                                            at
                                                            
                                                            MTF""
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         0
                                                      
                                                      
                                                         otherwise.
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        The system receives rewards upon delivery of casualties to an MTF based on the time the call has been in the system. Time requirements are outlined by the Department of the Army (2007); the MEDEVAC system seeks to respond within 60 minutes from notification to MTF drop-off for urgent casualties and within 240 minutes from notification to MTF drop-off for priority casualties. The reward obtained by servicing higher priority casualties decays quickly; however, servicing higher priority casualties obtains higher reward and therefore are often served first. For lower priority casualties, the decay allows enough time to wait before being forced to launch immediately, which increases the flexibility of the system. The β parameters model the value trade-off between servicing urgent and priority casualty events. Note that no rewards are earned for routine calls. In the high operational tempo combat scenarios of interest in this paper, urgent and priority calls are assumed to occur with high frequency, and commanders will have less interest in servicing non-life threatening injuries using a limited fleet of aerial MEDEVAC helicopters.

A policy 
                           
                              π
                              (
                              s
                              )
                              ∈
                              X
                              (
                              s
                              )
                           
                         maps the state space to the action space, indicating the action to take when the system is in state s. When following policy π, the state trajectory of the system 
                           
                              {
                              
                                 s
                                 k
                                 π
                              
                              :
                              k
                              =
                              1
                              ,
                              2
                              ,
                              …
                              }
                           
                         evolves according to 
                           
                              
                                 s
                                 
                                    k
                                    +
                                    1
                                 
                                 π
                              
                              =
                              ψ
                              
                                 (
                                 
                                    s
                                    k
                                    π
                                 
                                 ,
                                 π
                                 
                                    (
                                    
                                       s
                                       k
                                       π
                                    
                                    )
                                 
                                 ,
                                 ω
                                 
                                    (
                                    
                                       s
                                       k
                                       π
                                    
                                    ,
                                    π
                                    
                                       (
                                       
                                          s
                                          k
                                          π
                                       
                                       )
                                    
                                    )
                                 
                                 )
                              
                           
                        . The expected total discounted reward earned by the system when following policy π and starting from initial state s is

                           
                              (3)
                              
                                 
                                    
                                       J
                                       π
                                    
                                    
                                       (
                                       s
                                       )
                                    
                                    =
                                    E
                                    
                                       [
                                       
                                          ∑
                                          
                                             k
                                             =
                                             1
                                          
                                          ∞
                                       
                                       
                                          γ
                                          
                                             k
                                             −
                                             1
                                          
                                       
                                       r
                                       
                                          (
                                          
                                             s
                                             k
                                             π
                                          
                                          ,
                                          π
                                          
                                             (
                                             
                                                s
                                                k
                                                π
                                             
                                             )
                                          
                                          ,
                                          
                                             s
                                             
                                                k
                                                +
                                                1
                                             
                                             π
                                          
                                          )
                                       
                                       
                                          |
                                       
                                       
                                          s
                                          1
                                          π
                                       
                                       =
                                       s
                                       ]
                                    
                                    ,
                                 
                              
                           
                        where γ ∈ [0, 1) is the discount factor. We adopt an event-based discounting scheme to model the notion that servicing recent calls is more important than servicing future calls. However, we also utilize a relatively high discount factor (e.g., 0.98) in our computational experiments to incentivize the system to position itself to efficiently respond to future calls for service and to not simply act to obtain the best immediate reward (e.g., by sending the nearest MEDEVAC no matter the state of the system) thereby sacrificing future rewards.

The optimal policy π
                        * maximizes the expected total discounted reward and satisfies the following optimality equation

                           
                              (4)
                              
                                 
                                    J
                                    
                                       (
                                       s
                                       )
                                    
                                    =
                                    
                                       max
                                       
                                          x
                                          ∈
                                          X
                                          (
                                          s
                                          )
                                       
                                    
                                    
                                       {
                                       E
                                       
                                          [
                                          r
                                          (
                                          s
                                          ,
                                          x
                                          ,
                                          ψ
                                          (
                                          s
                                          ,
                                          x
                                          ,
                                          ω
                                          (
                                          s
                                          ,
                                          x
                                          )
                                          )
                                          )
                                          +
                                          γ
                                          J
                                          (
                                          ψ
                                          (
                                          s
                                          ,
                                          x
                                          ,
                                          ω
                                          (
                                          s
                                          ,
                                          x
                                          )
                                          )
                                          )
                                          ]
                                       
                                       }
                                    
                                    .
                                 
                              
                           
                        Unfortunately, computing an optimal policy using Eq. (4) is difficult. Even if we discretize the state space to obtain a finite representation, the high dimensionality of the state space renders the problem of determining an optimal policy computationally intractable. We instead propose an ADP approach. Utilizing a value function approximation approach, high quality suboptimal policies are constructed and compared to a simple myopic policy, which is often employed in practice.

We employ an approximate policy iteration (API) algorithmic strategy to construct high-quality dispatch policies based on value function approximations. To obtain such policies we must determine approximate solutions to Eq. (4). We proceed by employing a modified version of the optimality equation that uses a post-decision state variable convention. The post-decision state 
                           
                              s
                              k
                              x
                           
                         refers to the state of the system after being in state sk
                         and taking action xij
                        (sk
                        ). The post-decision state variable provides tremendous computational advantages as its use eliminates the embedded expectation within the optimality equation. The value of being in pre-decision state sk
                         is denoted by Jk
                        (sk
                        ), and the value of being in post-decision state 
                           
                              s
                              k
                              x
                           
                         is denoted by 
                           
                              
                                 J
                                 k
                                 x
                              
                              
                                 (
                                 
                                    s
                                    k
                                    x
                                 
                                 )
                              
                           
                        . The relationship between Jk
                        (sk
                        ) and 
                           
                              
                                 J
                                 k
                                 x
                              
                              
                                 (
                                 
                                    s
                                    k
                                    x
                                 
                                 )
                              
                           
                         is then given by

                           
                              (5)
                              
                                 
                                    
                                       J
                                       k
                                       x
                                    
                                    
                                       (
                                       
                                          s
                                          k
                                          x
                                       
                                       )
                                    
                                    ≜
                                    E
                                    
                                       [
                                       
                                          
                                             J
                                             
                                                k
                                                +
                                                1
                                             
                                          
                                          
                                             (
                                             
                                                s
                                                
                                                   k
                                                   +
                                                   1
                                                
                                             
                                             )
                                          
                                          |
                                       
                                       
                                          s
                                          k
                                          x
                                       
                                       ]
                                    
                                    .
                                 
                              
                           
                        The optimality equation in terms of the post-decision state variable is

                           
                              (6)
                              
                                 
                                    
                                       J
                                       
                                          k
                                          −
                                          1
                                       
                                       x
                                    
                                    
                                       (
                                       
                                          s
                                          
                                             k
                                             −
                                             1
                                          
                                          x
                                       
                                       )
                                    
                                    =
                                    E
                                    
                                       [
                                       
                                          max
                                          
                                             x
                                             ∈
                                             X
                                             (
                                             s
                                             )
                                          
                                       
                                       
                                          (
                                          r
                                          
                                             (
                                             
                                                s
                                                k
                                             
                                             ,
                                             x
                                             ,
                                             
                                                s
                                                
                                                   k
                                                   +
                                                   1
                                                
                                             
                                             )
                                          
                                          +
                                          γ
                                          
                                             J
                                             k
                                             x
                                          
                                          
                                             (
                                             
                                                s
                                                k
                                                x
                                             
                                             )
                                          
                                          )
                                       
                                       |
                                       
                                          s
                                          
                                             k
                                             −
                                             1
                                          
                                          x
                                       
                                       ]
                                    
                                    .
                                 
                              
                           
                        Despite the dimensionality reduction attained by using the post-decision state variable, the optimality equation remains computationally intractable. We proceed by defining a fixed set of basis functions to approximate the post-decision state value function, 
                           
                              
                                 J
                                 k
                                 x
                              
                              
                                 (
                                 
                                    s
                                    k
                                    x
                                 
                                 )
                              
                           
                        . Let 
                           
                              
                                 ϕ
                                 f
                              
                              
                                 (
                                 
                                    s
                                    k
                                    x
                                 
                                 )
                              
                           
                         be a basis function, where 
                           
                              f
                              ∈
                              F
                           
                         is a feature and 
                           F
                         is the set of features. Identification of basis functions and features that are important to a particular problem can be difficult but is key to obtaining a quality approximation. Eq. (7) shows the linear approximation architecture we adopt. Let

                           
                              (7)
                              
                                 
                                    
                                       
                                          J
                                          ¯
                                       
                                       k
                                       x
                                    
                                    
                                       (
                                       
                                          s
                                          k
                                          x
                                       
                                       ;
                                       θ
                                       )
                                    
                                    ≜
                                    
                                       ∑
                                       
                                          f
                                          ∈
                                          F
                                       
                                    
                                    
                                       θ
                                       f
                                    
                                    
                                       ϕ
                                       f
                                    
                                    
                                       (
                                       
                                          s
                                          k
                                          x
                                       
                                       )
                                    
                                    =
                                    
                                       θ
                                       ⊤
                                    
                                    ϕ
                                    
                                       (
                                       
                                          s
                                          k
                                          x
                                       
                                       )
                                    
                                    ,
                                 
                              
                           
                        where 
                           
                              ϕ
                              (
                              
                                 s
                                 k
                                 x
                              
                              )
                           
                         is a column vector with elements 
                           
                              
                                 {
                                 
                                    ϕ
                                    f
                                 
                                 
                                    (
                                    
                                       s
                                       k
                                       x
                                    
                                    )
                                 
                                 }
                              
                              
                                 f
                                 ∈
                                 F
                              
                           
                         and θ is a column vector of basis function weights. By substituting the value function approximation (7) into the modified optimality Equation (6), we obtain the following expression for our value function approximation

                           
                              (8)
                              
                                 
                                    
                                       θ
                                       ⊤
                                    
                                    ϕ
                                    
                                       (
                                       
                                          s
                                          
                                             k
                                             −
                                             1
                                          
                                          x
                                       
                                       )
                                    
                                    =
                                    E
                                    
                                       [
                                       
                                          r
                                          
                                             (
                                             
                                                s
                                                k
                                             
                                             ,
                                             π
                                             
                                                (
                                                
                                                   s
                                                   k
                                                
                                                ;
                                                θ
                                                )
                                             
                                             ,
                                             
                                                s
                                                
                                                   k
                                                   +
                                                   1
                                                
                                             
                                             )
                                          
                                          +
                                          γ
                                          
                                             θ
                                             ⊤
                                          
                                          ϕ
                                          
                                             (
                                             
                                                s
                                                k
                                                x
                                             
                                             )
                                          
                                          
                                          |
                                       
                                       
                                       
                                          s
                                          
                                             k
                                             −
                                             1
                                          
                                          x
                                       
                                       ]
                                    
                                    ,
                                 
                              
                           
                        where, for a given weight vector θ, the decision is given by the policy function

                           
                              (9)
                              
                                 
                                    π
                                    
                                       (
                                       
                                          s
                                          k
                                       
                                       ;
                                       θ
                                       )
                                    
                                    =
                                    
                                       
                                          arg
                                          
                                          max
                                       
                                       
                                          x
                                          ∈
                                          X
                                          (
                                          
                                             s
                                             k
                                          
                                          )
                                       
                                    
                                    
                                       {
                                       R
                                       
                                          (
                                          
                                             s
                                             k
                                          
                                          ,
                                          x
                                          )
                                       
                                       +
                                       γ
                                       
                                          θ
                                          ⊤
                                       
                                       ϕ
                                       
                                          (
                                          
                                             s
                                             k
                                             x
                                          
                                          )
                                       
                                       }
                                    
                                    .
                                 
                              
                           
                        Note that we let R(sk, x) denote the expected reward 
                           
                              E
                              [
                              r
                              
                                 (
                                 
                                    s
                                    k
                                 
                                 ,
                                 x
                                 ,
                                 
                                    s
                                    
                                       k
                                       +
                                       1
                                    
                                 
                                 )
                              
                              |
                              
                                 s
                                 k
                              
                              ,
                              x
                              ]
                           
                        .

Selection of the set of basis functions is an important aspect of our approximation strategy. We proceed by discussing the development of the basis functions 
                           
                              {
                              
                                 ϕ
                                 f
                              
                              
                                 (
                                 ·
                                 )
                              
                              :
                              f
                              ∈
                              F
                              }
                           
                         utilized in our approximation of the post-decision state value function, Eq. (7). Basis functions can be very difficult to develop. Much trial and error went into the identification of effective basis functions for our application, and so we first discuss the logic underlying each one we utilize. The first basis function utilizes an indicator variable, denoting whether MEDEVAC unit i is available. Let

                           
                              (10)
                              
                                 
                                    
                                       ϕ
                                       
                                          1
                                          ,
                                          i
                                       
                                    
                                    =
                                    
                                       {
                                       
                                          
                                             
                                                1
                                             
                                             
                                                
                                                   if
                                                   
                                                   
                                                      σ
                                                      i
                                                   
                                                   =
                                                   ``
                                                   idle
                                                   ""
                                                
                                             
                                          
                                          
                                             
                                                0
                                             
                                             
                                                
                                                   otherwise
                                                   .
                                                
                                             
                                          
                                       
                                    
                                    i
                                    =
                                    1
                                    ,
                                    2
                                    ,
                                    …
                                    ,
                                    a
                                 
                              
                           
                        
                     

The second basis function captures the expected time until MEDEVAC unit i becomes available. Let

                           
                              (11)
                              
                                 
                                    
                                       ϕ
                                       
                                          2
                                          ,
                                          i
                                       
                                    
                                    =
                                    
                                       {
                                       
                                          
                                             
                                                
                                                   
                                                      d
                                                      i
                                                   
                                                   −
                                                   τ
                                                   +
                                                   
                                                      d
                                                      
                                                         i
                                                         ,
                                                         j
                                                      
                                                      r
                                                   
                                                
                                             
                                             
                                                
                                                   if
                                                   
                                                   
                                                      σ
                                                      i
                                                   
                                                   ≠
                                                   ``
                                                   idle
                                                   ""
                                                
                                             
                                          
                                          
                                             
                                                0
                                             
                                             
                                                
                                                   otherwise
                                                   ,
                                                
                                             
                                          
                                       
                                    
                                    i
                                    =
                                    1
                                    ,
                                    2
                                    ,
                                    …
                                    ,
                                    a
                                 
                              
                           
                        where 
                           
                              d
                              
                                 i
                                 ,
                                 j
                              
                              r
                           
                         represents the expected time for MEDEVAC unit i to return to base after delivering the casualties of its current call (call j) to the nearest MTF. This computation relies on lj
                        , the POI location of the call MEDEVAC unit i is currently serving, to identify the destination MTF and compute the expected travel time from the MTF to the MEDEVAC unit’s base. The expected time 
                           
                              d
                              
                                 i
                                 ,
                                 j
                              
                              r
                           
                         is added to the 
                           
                              
                                 d
                                 i
                              
                              −
                              τ
                           
                         term, the expected time taken for the MEDEVAC unit to complete its current mission.

The third basis function captures the expected time from the current event time τ until MEDEVAC unit i arrives at the nearest MTF with its assigned casualties. Let

                           
                              (12)
                              
                                 
                                    
                                       ϕ
                                       
                                          3
                                          ,
                                          i
                                       
                                    
                                    =
                                    
                                       {
                                       
                                          
                                             
                                                
                                                   
                                                      d
                                                      i
                                                   
                                                   −
                                                   τ
                                                
                                             
                                             
                                                
                                                   if
                                                   
                                                   
                                                      σ
                                                      i
                                                   
                                                   ≠
                                                   ``idle""
                                                
                                             
                                          
                                          
                                             
                                                0
                                             
                                             
                                                
                                                   otherwise
                                                   .
                                                
                                             
                                          
                                       
                                    
                                    i
                                    =
                                    1
                                    ,
                                    2
                                    ,
                                    …
                                    ,
                                    a
                                 
                              
                           
                        
                     

The next three basis functions capture information about the calls in the queue and the calls currently being served. Basis function ϕ
                        4, i, j
                         captures the expected total time, including all travel, load, and unload times, that call j will be in the system if it is served by MEDEVAC unit i. Let

                           
                              (13)
                              
                                 
                                    
                                       
                                       
                                       
                                          
                                             
                                                ϕ
                                                
                                                   4
                                                   ,
                                                   i
                                                   ,
                                                   j
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                       
                                          
                                             =
                                             
                                                {
                                                
                                                   
                                                      
                                                         
                                                            
                                                               ζ
                                                               j
                                                            
                                                            +
                                                            
                                                               d
                                                               i
                                                            
                                                            −
                                                            
                                                               t
                                                               i
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            if
                                                            
                                                            
                                                               σ
                                                               i
                                                            
                                                            =
                                                            ``enroute
                                                         
                                                      
                                                   
                                                   
                                                      
                                                      
                                                         
                                                            to
                                                            
                                                            call
                                                            
                                                            j
                                                            ""
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         0
                                                      
                                                      
                                                         
                                                            otherwise
                                                            .
                                                         
                                                      
                                                   
                                                
                                             
                                             i
                                             =
                                             1
                                             ,
                                             2
                                             ,
                                             …
                                             ,
                                             a
                                             ;
                                             
                                             
                                             j
                                             =
                                             1
                                             ,
                                             2
                                             ,
                                             …
                                             ,
                                             b
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Basis function ϕ
                        5, i, j
                         captures the priority of the call (call j) being served by MEDEVAC i. Let

                           
                              (14)
                              
                                 
                                    
                                       ϕ
                                       
                                          5
                                          ,
                                          i
                                          ,
                                          j
                                       
                                    
                                    =
                                    
                                       {
                                       
                                          
                                             
                                                
                                                   η
                                                   j
                                                
                                             
                                             
                                                
                                                   if
                                                   
                                                   
                                                      σ
                                                      i
                                                   
                                                   =
                                                   ``enroute
                                                
                                             
                                          
                                          
                                             
                                             
                                                
                                                   to
                                                   
                                                   call
                                                   
                                                   j
                                                   ""
                                                
                                             
                                          
                                          
                                             
                                                0
                                             
                                             
                                                
                                                   otherwise
                                                   .
                                                
                                             
                                          
                                       
                                    
                                    i
                                    =
                                    1
                                    ,
                                    2
                                    ,
                                    …
                                    ,
                                    a
                                    ;
                                    
                                    
                                    j
                                    =
                                    1
                                    ,
                                    2
                                    ,
                                    …
                                    ,
                                    b
                                 
                              
                           
                        
                     

Basis function ϕ
                        6, i, j
                         represents the expected time in system for call j if it is assigned to MEDEVAC i. Let

                           
                              (15)
                              
                                 
                                    
                                       ϕ
                                       
                                          6
                                          ,
                                          i
                                          ,
                                          j
                                       
                                    
                                    =
                                    
                                       d
                                       
                                          i
                                          ,
                                          j
                                       
                                       s
                                    
                                    +
                                    
                                       ϕ
                                       
                                          2
                                          ,
                                          i
                                       
                                    
                                    
                                    
                                    i
                                    =
                                    1
                                    ,
                                    2
                                    ,
                                    …
                                    ,
                                    a
                                    ;
                                    
                                    
                                    j
                                    =
                                    1
                                    ,
                                    2
                                    ,
                                    …
                                    ,
                                    b
                                    ,
                                 
                              
                           
                        where 
                           
                              d
                              
                                 i
                                 ,
                                 j
                              
                              s
                           
                         represents the expected MEDEVAC service time for MEDEVAC unit i to serve call j. We utilize polynomial functions comprised of these basis functions to approximate the post-decision state value function.

Having stipulated the policy function and the value function approximation upon which it is based, we proceed by discussing the manner in which the value function approximation is updated. We employ an API algorithmic strategy, the general structure of which is similar to exact policy iteration. Within the policy evaluation step of our API algorithm, we update the value function approximation for a fixed policy using least squares temporal differencing (LSTD). Introduced by Bradtke and Barto (1996), LSTD is a computationally efficient method for estimating the adjustable parameters (e.g., θ), when using a linear architecture with fixed basis functions to approximate the value function for a fixed policy. The API algorithm we employ is adapted in part from Scott et al. (2014) and is shown in Algorithm 1
                        .

The algorithm begins with an initial θ vector, representing an initial base policy. The performance evaluation of the current policy proceeds as follows. A post-decision state is randomly sampled, and the value 
                           
                              ϕ
                              (
                              
                                 s
                                 
                                    k
                                    −
                                    1
                                    ,
                                    h
                                 
                                 x
                              
                              )
                           
                         is recorded. Next, we simulate one event forward and determine the best decision as per Eq. (9), recording the associated expected reward R(s
                        
                           k, h
                        , x) and basis function evaluations of the post-decision state, 
                           
                              ϕ
                              (
                              
                                 s
                                 
                                    k
                                    ,
                                    h
                                 
                                 x
                              
                              )
                           
                        . A total of 
                           H
                         temporal difference sample realizations are collected, where 
                           
                              R
                              
                                 (
                                 
                                    s
                                    
                                       k
                                       ,
                                       h
                                    
                                 
                                 ,
                                 π
                                 
                                    (
                                    
                                       s
                                       
                                          k
                                          ,
                                          h
                                       
                                    
                                    ;
                                    θ
                                    )
                                 
                                 )
                              
                              +
                              γ
                              
                                 θ
                                 ⊤
                              
                              ϕ
                              
                                 (
                                 
                                    s
                                    
                                       k
                                       ,
                                       h
                                    
                                    x
                                 
                                 )
                              
                              −
                              
                                 θ
                                 ⊤
                              
                              ϕ
                              
                                 (
                                 
                                    s
                                    
                                       k
                                       −
                                       1
                                       ,
                                       h
                                    
                                    x
                                 
                                 )
                              
                           
                         is the hth temporal difference, given the parameter vector θ.

After obtaining the 
                           H
                         temporal difference sample realizations, we conduct the policy improvement steps of the API algorithm. We compute 
                           
                              
                                 θ
                                 ^
                              
                              ,
                           
                         a sample estimate of θ, by regressing the 
                           
                              h
                              =
                              1
                              ,
                              …
                              ,
                              H
                           
                         basis function evaluations of the post-decision states 
                           
                              ϕ
                              (
                              
                                 s
                                 
                                    k
                                    −
                                    1
                                    ,
                                    h
                                 
                                 x
                              
                              )
                           
                         and 
                           
                              ϕ
                              (
                              
                                 s
                                 
                                    k
                                    ,
                                    h
                                 
                                 x
                              
                              )
                           
                         against the reward R(s
                        
                           k, h
                        , x). We conduct a least squares regression so that the sum of the temporal differences over the 
                           H
                         inner loop simulations (which approximates the expectation) is equal to zero. To compactly state our update of θ, we denote basis function matrices and reward vectors as follows. Let

                           
                              
                                 
                                    
                                       
                                          
                                             Φ
                                             
                                                k
                                                −
                                                1
                                             
                                          
                                       
                                       
                                          ≜
                                       
                                       
                                          
                                             
                                                [
                                                
                                                   
                                                      
                                                         
                                                            ϕ
                                                            
                                                               
                                                                  (
                                                                  
                                                                     s
                                                                     
                                                                        k
                                                                        −
                                                                        1
                                                                        ,
                                                                        1
                                                                     
                                                                     x
                                                                  
                                                                  )
                                                               
                                                               ⊤
                                                            
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         ⋮
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            ϕ
                                                            
                                                               
                                                                  (
                                                                  
                                                                     s
                                                                     
                                                                        k
                                                                        −
                                                                        1
                                                                        ,
                                                                        H
                                                                     
                                                                     x
                                                                  
                                                                  )
                                                               
                                                               ⊤
                                                            
                                                         
                                                      
                                                   
                                                
                                                ]
                                             
                                             ,
                                             
                                             
                                                Φ
                                                k
                                             
                                             ≜
                                             
                                                [
                                                
                                                   
                                                      
                                                         
                                                            ϕ
                                                            
                                                               
                                                                  (
                                                                  
                                                                     s
                                                                     
                                                                        k
                                                                        ,
                                                                        1
                                                                     
                                                                     x
                                                                  
                                                                  )
                                                               
                                                               ⊤
                                                            
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         ⋮
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            ϕ
                                                            
                                                               
                                                                  (
                                                                  
                                                                     s
                                                                     
                                                                        k
                                                                        ,
                                                                        H
                                                                     
                                                                     x
                                                                  
                                                                  )
                                                               
                                                               ⊤
                                                            
                                                         
                                                      
                                                   
                                                
                                                ]
                                             
                                             ,
                                             
                                             
                                                R
                                                k
                                             
                                             ≜
                                             
                                                [
                                                
                                                   
                                                      
                                                         
                                                            R
                                                            (
                                                            
                                                               s
                                                               
                                                                  k
                                                                  ,
                                                                  1
                                                               
                                                            
                                                            )
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         ⋮
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            R
                                                            (
                                                            
                                                               s
                                                               
                                                                  k
                                                                  ,
                                                                  H
                                                               
                                                            
                                                            )
                                                         
                                                      
                                                   
                                                
                                                ]
                                             
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        where matrices 
                           
                              Φ
                              
                                 k
                                 −
                                 1
                              
                           
                         and Φk
                         consist of rows of basis function evaluations of the sampled post-decision states, and Rk
                         is the reward vector for the sampled events. The parameter vector 
                           
                              θ
                              ^
                           
                         is then computed as follows.

                           
                              (16)
                              
                                 
                                    
                                       θ
                                       ^
                                    
                                    =
                                    
                                       
                                          [
                                          
                                             
                                                (
                                                
                                                   Φ
                                                   
                                                      k
                                                      −
                                                      1
                                                   
                                                
                                                −
                                                γ
                                                
                                                   Φ
                                                   k
                                                
                                                )
                                             
                                             ⊤
                                          
                                          
                                             (
                                             
                                                Φ
                                                
                                                   k
                                                   −
                                                   1
                                                
                                             
                                             −
                                             γ
                                             
                                                Φ
                                                k
                                             
                                             )
                                          
                                          ]
                                       
                                       
                                          −
                                          1
                                       
                                    
                                    
                                       
                                          (
                                          
                                             Φ
                                             
                                                k
                                                −
                                                1
                                             
                                          
                                          −
                                          γ
                                          
                                             Φ
                                             k
                                          
                                          )
                                       
                                       ⊤
                                    
                                    
                                       R
                                       k
                                    
                                 
                              
                           
                        
                     

We employ a generalized harmonic stepsize rule to smooth in the new observation 
                           
                              θ
                              ^
                           
                         with the previous estimate θ. The stepsize rule is given by

                           
                              (17)
                              
                                 
                                    
                                       α
                                       g
                                    
                                    =
                                    
                                       κ
                                       
                                          κ
                                          +
                                          g
                                          −
                                          1
                                       
                                    
                                    .
                                 
                              
                           
                        The stepsize rule αg
                         greatly influences the rate of convergence of the algorithm and therefore impacts the attendant solutions. Increasing the stepsize parameter κ slows the rate at which the smoothing stepsize drops to zero. Selecting an appropriate value for κ requires understanding the rate of convergence of the application. Some problems allow convergence to good solutions fairly quickly (tens to hundreds of iterations) whereas others require much more effort (thousands of iterations). We observe that relatively lower κ-values work best in our application.

The update equation for θ is given by

                           
                              (18)
                              
                                 
                                    θ
                                    ←
                                    θ
                                    
                                       (
                                       1
                                       −
                                       
                                          α
                                          g
                                       
                                       )
                                    
                                    +
                                    
                                       θ
                                       ^
                                    
                                    
                                       (
                                       
                                          α
                                          g
                                       
                                       )
                                    
                                    .
                                 
                              
                           
                        On the right hand side of Eq. (18), θ is the previous estimate and is based on information from all previous outer loop iterations; 
                           
                              θ
                              ^
                           
                         is our estimate from the current outer loop iteration. As the number of iterations g increases, we place less emphasis on any single estimate and more emphasis on the estimate based on information from the first 
                           
                              g
                              −
                              1
                           
                         iterations.

Upon obtaining an updated (and smoothed) parameter vector θ, we have completed one policy improvement iteration of the algorithm. The parameters 
                           G
                         and 
                           H
                         are tunable, where 
                           G
                         is the number of policy improvement iterations completed and 
                           H
                         is the number of policy evaluation iterations completed.

This section presents the MEDEVAC process simulation model that supports the design and development of high quality MEDEVAC dispatch policies. The simulation model enables evaluation of policies identified by our ADP algorithm as well as the evaluation of the myopic policy typically utilized in practice by the aeromedical evacuation coordination center (AECC). For the myopic policy, MEDEVAC requests are served in decreasing order of priority with a first-in-first-out policy for like priorities. Note that we also employ the simulation model within the ADP algorithm itself when we randomly sample a post-decision state and simulate forward one system event.

The simulation model we develop encompasses the operations of a MEDEVAC system, incorporating information obtained from military doctrine (Department of the Army, 2007), interviews with US Army Blackhawk helicopter pilots (Fett, 2013; Flores, 2013), and our own personal experiences. In the representative scenario considered in the subsequent section, placement of the MTFs follows doctrine. Moreover, the deliberate selection of the casualty cluster centers within the Hawkes process governing casualty event arrivals embeds the information concerning the expected interaction of friendly and adversary forces.

We utilize the MEDEVAC process simulation within our ADP algorithm to obtain simulated trajectories of the system and to measure the performance of the MEDEVAC dispatch policies obtained from our ADP algorithm. The flow of control for the MEDEVAC process simulation model is shown in Fig. 2
                        . We employ an event-scheduling approach to simulation modeling, using a next-event time-advance mechanism.

In the initialization routine, the simulation is initiated by setting the simulation clock to zero, setting the system state to all MEDEVAC units sitting “idle” at their own base and an empty call queue, and setting statistical counters to zero. Statistical counters refer to the variables used to store system performance information such as accumulated rewards, casualty event response times, and MEDEVAC utilization (busy) rates. The event list is initiated with a casualty event. A Hawkes process with arrival rate λ governs casualty event generation. The Hawkes process models situations where subsequent events are likely to occur in close proximity to the prior event. In the timing routine, the next event (i.e., event k) of type ϵ, is selected from the event list and the simulation clock is advanced to the appropriate simulation time. In the event type ϵ routine, a dispatch decision is made according to the stipulated MEDEVAC dispatch policy. The system state and statistical counters are updated accordingly. We generate future events and add them to the event list. To obtain these future events, random variates capturing the MEDEVAC mission travel, load, and unload times are generated from probability distributions depicting the real-world MEDEVAC processes. We then check to see if the simulation is over, terminating the simulation at the 24-hour mark. If the simulation continues, we return to the timing routine to determine which event on the event list occurs next and the simulation proceeds.

In this section, we demonstrate the applicability of our model to a problem of interest to the military medical planning community and examine the efficacy of our proposed solution methodology. We present a representative military medical evacuation (MEDEVAC) planning scenario as the basis for our analysis. We examine different features of the MEDEVAC dispatching problem and of our approximate dynamic programming (ADP) algorithm to gain insight regarding the performance of our overall proposed approach. In particular, a computational experiment is designed and conducted to obtain insights regarding solution quality and computational effort. Moreover, we perform a series of sensitivity analyses to focus our study on specific problem features and algorithmic features of interest. For the computational experiments, we utilize a dual Intel Xeon E5-2650v2 workstation having 192 GB of RAM and MATLAB’s parallel computing toolbox.

We consider a notional planning scenario in which a coalition of allied countries performs peacekeeping operations in response to aggression by Islamic State militants in northern Syria. The locations for MEDEVAC bases are likely key military tactical sites. Casualty event cluster centers for use within the Hawkes process are selected and weighted by projected enemy locations. Fig. 3
                         shows the 18 casualty cluster centers, five MEDEVAC stations, and two medical treatment facilities (MTFs). Each MTF has a MEDEVAC station co-located with it.

We assume a high operations tempo with a baseline casualty event arrival rate of 
                           
                              λ
                              =
                              
                                 1
                                 60
                              
                              ,
                           
                         representing an average casualty event arrival rate of one event per hour. Military medical planners in cooperation with military intelligence and operational planners should be able to stipulate a reasonable arrival rate in advance of a planned combat operation based on the size, equipment, and disposition of friendly and adversary forces. Any MEDEVAC is allowed to service any casualty event, and casualty events need not be served as soon as they arrive. In this scenario, we assume urgent and priority class casualty events arrive in equal proportion. Routine events are not considered due to the high operations tempo. In such scenarios, routine events would be serviced by non-MEDEVAC or ground MEDEVAC assets. The reward function utilizes 
                           
                              
                                 β
                                 1
                              
                              =
                              10
                           
                         and 
                           
                              
                                 β
                                 2
                              
                              =
                              2
                           
                         which rewards the servicing of urgent casualty events much more than priority casualty events. We further assume the UH-60 A/L Blackhawk acts as the dedicated aeromedical MEDEVAC asset.

@&#EXPERIMENTAL DESIGN@&#

In our computational experiments, we measure performance of an ADP policy in terms of the percent increase in total discounted reward over that obtained by use of the default myopic policy. When using the myopic policy, the system dispatches the closest idle MEDEVAC unit. An important problem feature of interest is the casualty event arrival rate λ. The algorithmic features of interest include the number of policy improvement iterations 
                           G
                         and the number of policy evaluation iterations 
                           H
                        . In our experimental design we utilize a third-order polynomial of the basis functions to approximate the post-decision state value function. We select algorithmic factor levels based on our initial experiences with implementing the algorithm. We observe that computational effort beyond 
                           
                              G
                              =
                              20
                           
                         policy iterations obtains little improvement. Similarly, after 
                           
                              H
                              =
                              20
                              ,
                              000
                           
                         temporal difference samples, we observe little improvement in solution quality.

During our initial investigations, we also explored the issue of whether to employ smoothing for the θ parameter. In our experiments, smoothing outperforms non-smoothing. Fig. 4
                         illustrates a typical observed result. Indeed, we observed such a result across all experimental levels and for all runs. As such, we utilize an aggressive smoothing function (i.e., 
                           
                              κ
                              =
                              1
                           
                        ) when implementing our algorithm and do not report further results concerning smoothing.

Informed by our initial investigations, we design a 33 full factorial computational experiment to examine solution quality and computation time at different levels for factors λ, 
                           
                              G
                              ,
                           
                         and 
                           H
                        . Table 1
                         shows the factor levels. The purpose of the experiment is to inform the selection of appropriate algorithm parameter values for 
                           G
                         and 
                           H
                         for subsequent sensitivity analysis. Moreover, we seek general insight regarding the performance of the ADP algorithm for MEDEVAC dispatching problem instances with decreased and increased intensity in terms of the casualty event arrival rate, λ.

@&#EXPERIMENTAL RESULTS@&#


                        Table 2
                         reports the results from the experimental design. Starting from the left, the first three columns indicate the factor levels, the fourth column indicates the 95% confidence interval for ADP algorithm performance, expressed in terms of percentage improvement over the myopic policy (with respect to total discounted reward), the fifth and sixth columns report the mean response times (i.e., length of time from a 9-line call submission until casualties are dropped off at an MTF) experienced by urgent and priority calls, respectively, the seventh column reports MEDEVAC unit busy rates, and the eighth column reports computation times.

We highlight the best performing algorithmic features with respect to improvement for event arrival rates 
                           
                              λ
                              =
                              
                                 1
                                 30
                              
                           
                         and 
                           
                              λ
                              =
                              
                                 1
                                 60
                              
                           
                        . For a casualty event arrival rate of 
                           
                              λ
                              =
                              
                                 1
                                 120
                              
                              ,
                           
                         the ADP policy generally performs the same as the myopic policy. In a sense, this is an intuitive result. As the likelihood of an event arrival decreases, it is less risky to simply send the nearest MEDEVAC unit to service a call, regardless of its priority category. As such, the myopic policy is best, and the ADP policy will not outperform it. As the casualty event arrival rate λ increases, the ADP policy increasingly outperforms the myopic policy across all investigated algorithmic experimental factor levels. The ADP policy successfully captures important nuances of the problem, whereby MEDEVAC rationing occurs in anticipation of the arrival of urgency calls. A general trend observed for the first two casualty event arrival rates is that 
                           
                              G
                              =
                              10
                           
                         policy improvement loops and at least 
                           
                              H
                              =
                              10
                              ,
                              000
                           
                         policy evaluation loops achieve the best performance. We observe that computation time scales closely with the product 
                           
                              G
                              ·
                              H
                              ,
                           
                         as expected.

The experimental results indicate that setting 
                           
                              G
                              =
                              10
                           
                         and 
                           
                              H
                              =
                              20
                              ,
                              000
                           
                         is appropriate for subsequent investigations. We proceed by investigating our baseline case more thoroughly.

We utilize the problem and algorithmic parameters shown in Table 3
                         when investigating the baseline scenario. The medial casualty event arrival rate of 
                           
                              λ
                              =
                              
                                 1
                                 60
                              
                           
                         is selected as the base case arrival rate, as it models a high intensity conflict scenario of interest to military medical planners. The weights 
                           
                              
                                 β
                                 1
                              
                              =
                              10
                           
                         and 
                           
                              
                                 β
                                 2
                              
                              =
                              2
                           
                         capture the medical planners’ strong preference for servicing urgent casualty events over priority casualty events. We select 
                           
                              G
                              =
                              10
                           
                         and 
                           
                              H
                              =
                              20
                              ,
                              000
                           
                         as they are the best performing algorithmic parameters for 
                           
                              λ
                              =
                              
                                 1
                                 60
                              
                           
                        .


                        Table 4
                         shows the performance of the ADP policy against the myopic policy for the baseline scenario. We consider basis function polynomials up to third order. We stopped at the third order because fourth-order polynomials caused singularity issues when performing matrix operations to compute θ, indicating a linear dependency issue among the basis function vectors. Utilizing a third-order polynomial of basis functions to approximate the value of a post-decision state obtains the best ADP policy performance, outperforming the myopic policy by nearly 31%. The mean casualty event response times for the two priority classification levels and the mean MEDEVAC busy times (which include time spent traveling back to base even if they are not actively serving a casualty event) also outperform the myopic policy. The ADP policy focuses on servicing urgent casualty events first, as noted by shorter wait times; moreover, MEDEVACs are being utilized more efficiently as shown by the lower average busy percentage.

A discussion concerning the urgent and priority response times is warranted. Even when implementing the best ADP policy, our simulation results indicate a mean response time of 154.0 minutes for urgent evacuations. While the standard stipulated by the United States Secretary of Defense is 60 minutes, we note that the North Atlantic Treaty Organization (NATO) standard is 90 minutes (Cordell, Cooney, & Beijer, 2008). Moreover, the Department of the Navy adheres to a standard of 120 minutes, as indicated in the field medical service technician student manual definitions (Department of the Navy, 2013). Regardless of the desired goal, if the high dispatching response times are unacceptable to military medical planners and their commanding officers, then the placement of additional MTFs and/or more MEDEVAC units should be considered. Conversely, medical asset resource allocation and placement decisions, determined prior to the dispatching problem analysis, may have to be revisited. Analysis of the θ vector can inform such follow-up decisions.

Examination of the basis function θ coefficients from the best performing third-order ADP policy provides the following insights. The basis function ϕ
                        5, which captures the interaction between MEDEVAC unit i and the priority of the casualty event it is serving, has the largest impact on the policy. This result is indicated by the relatively larger positive values of the θ coefficients associated with ϕ
                        5, as compared to the θ coefficients of the other basis functions. The number and proximity of casualty event cluster centers to a MEDEVAC base show an increase in magnitude for their respective basis function θ coefficients. Moreover, MEDEVAC units co-located with the MTF show a similar increase in magnitude, despite being further away from large groupings of casualty event cluster centers. MEDEVAC units with low θ coefficient magnitudes (for their associated basis functions) may be candidates for permanent relocation, should medical planners consider such an issue. Such MEDEVAC units contribute very little to the value of being in a particular system state and may be more productive if stationed at a different location. MEDEVAC units located at more productive sites have higher θ-values (for their associated basis functions) since operating from their location (and attendant ability to service nearby 9-line calls) provides value to the system (as indicated by positive, higher θ-values). Interestingly, ϕ
                        1 shows low statistical significance and also has small magnitudes for its coefficients for all MEDEVACs. This occurs because the ADP policy does not seek to penalize MEDEVACs for being idle, which could force them to otherwise launch in response to low priority and/or distant casualty events. Examination of the superlative ADP policy indicates that it is best to launch MEDEVACs which are close to casualty events and co-located with hospitals only on urgent casualty events and to hold these units in reserve when lower priority casualty events occur.

The benefits of the ADP policy become more evident upon considering two vignettes (i.e., possible system states) in which the myopic and the ADP policies differ and recognizing the advantage afforded by the ADP in those vignettes. Consider the following reserve vignette in which a priority-level casualty event occurs in a high-demand area, after which an urgent-level event occurs in the same area. Initially, all MEDEVACs are idle. Under the myopic policy, the closest MEDEVAC is dispatched in response to the first (priority) event. Subsequently, the next-closest (yet, somewhat distant) MEDEVAC is dispatched in response to the second (urgent) event. The system receives a lower reward when responding to the urgent event due to the longer response time of the next-closest MEDEVAC. Under the ADP policy, the next-closest MEDEVAC is dispatched in response to the first (priority) event as dispatching the closest (in-area) MEDEVAC would place the system in a disadvantageous position, as indicated by the value function approximation. The closest MEDEVAC is dispatched in response to the second (urgent) event. The system receives a higher reward when responding to the urgent event due to the closest MEDEVAC’s shorter response time. In dispatching the next-closest MEDEVAC to the first (priority) event, the closest MEDEVAC is essentially held in reserve in anticipation of servicing the likely imminent arrival of an urgent casualty event. When the urgent casualty event then occurs, the system is well positioned to dispatch the closest MEDEVAC and earn a higher reward.

Consider the following wait vignette in which a casualty event occurs in a high-demand area, after which a second casualty event occurs in a distant high-demand area. Initially, all MEDEVACs are busy with the exception of the MEDEVAC in the distant high-demand area. Moreover, the closest MEDEVAC (in the first event’s area) is about to complete service, prior to the arrival of the second event. Under the myopic policy, the distant MEDEVAC is dispatched in response to first event. After the closest MEDEVAC completes its service, it is dispatched in response to the second event. Both calls must wait for a distant MEDAVAC, resulting in longer than necessary response times and attendant low rewards. Indeed, the system realizes a low reward with a high utilization rate. Under the ADP policy, the dispatching authority waits and queues the first call because the decision to dispatch the distant MEDEVAC in response to the first event would place the system in a disadvantageous position, as indicated by the value function approximation. The ADP policy accounts for the fact that the closest MEDEVAC is about to complete its current call and will then become available. After waiting, the closest MEDEVAC completes service and is then immediately dispatched to service the queued first call. The distant MEDEVAC initially remains idle and is dispatched in response to the second event. The system realizes a higher reward with a lower utilization rate, as compared to the myopic policy.

We next examine the performance of the ADP policy for different casualty event arrival rates by adjusting λ. Fig. 5
                         shows the performance of the ADP policy against the myopic policy. As λ decreases (to the right in Fig. 5), the frequency with which casualties arrive decreases and the ADP policy no longer outperforms the myopic policy. As casualty events arrive at a slower rate, the reward obtained by holding MEDEVAC aircraft in reserve is diminished. At a casualty event arrival rate of one every two hours (i.e., 
                           
                              λ
                              =
                              
                                 1
                                 120
                              
                           
                        ), we observe a reduction in performance of the ADP compared to the myopic policy, perhaps showing the limitation of the set of basis functions we have defined and their resulting value function approximations. Alternatively, as mentioned previously, as λ approaches zero, the difference in performance between the optimal and myopic policies becomes negligible; for such low intensity operations we would not expect to see a difference in performance between the ADP policy and the myopic policy.

The impact of changing the proportion of urgent casualty events is shown in Fig. 6
                        . The ADP policy outperforms the myopic policy for all proportions, increasingly so when there are more frequent urgent casualty events occurring. The ADP policy efficiently manages resources by not immediately sending the nearest available MEDEVAC, but rather waiting and/or dispatching a MEDEVAC unit that is further away from the casualty event and that is also not close to a high rate casualty cluster area.

Our final investigation considers the impact of replacing the UH-60 A/L Blackhawk with an improved aeromedical aircraft. There are experimental rotary wing aircraft which could potentially be put into service and which can travel significantly faster than the UH-60 A/L Blackhawk. To examine the impact of these aircraft on MEDEVAC system performance, we adjust the average speed at which the MEDEVAC aircraft can travel. Other random variables modeling the MEDEVAC process remain the same. Table 5
                         shows the results obtained by increasing average airspeed, where airspeed is indicated as a percentage increase over the airspeed obtained by the current dedicated aeromedical asset, the UH-60 A/L Blackhawk.

It is reasonable to assume that new rotary wing aircraft designs can achieve increased average airspeeds of 25%–50% compared to the currently fielded UH-60 A/L Blackhawk, which uses a power plant designed prior to 1989 (Federation of American Scientists, 2015). This increase in speed has a significant impact on the overall performance for both ADP and myopic policies. When implementing the ADP policy, an increase in airspeed of 25% results in mean response times of 116.8 and 140.3 minutes for urgent and priority casualty events, respectively. The reduction in mean response time from 235.4 to 140.3 minutes is quite substantial for the priority class. Although we observe diminishing returns for the ADP compared to the myopic policy as airspeed increases (i.e., increased airspeed helps the myopic policy more than it helps the ADP policy), the MEDEVAC dispatching authority still benefits greatly from high quality dispatching solutions provided by the ADP algorithm.

When we examine computational effort more closely, we find the ADP solution only requires about 150 seconds run time for the baseline scenario. The largest computational effort comes from running the simulation to obtain a reasonable measure of performance. For each event in the simulation, the best action must be determined, as indicated by the policy function (Eq. (9)). In addition to these computations, we require 500 runs to achieve our desired level of confidence. The computation times for the simulation using the myopic and third-order ADP policies are shown in Table 6
                        .

The computation time required for computing the decision using Eq. (9) is about 25 times that of the myopic policy. Despite this burden, it is still possible to compute the ADP policy and run the simulation in under 20 minutes. Moreover, once a satisfactory ADP policy is obtained, MEDEVAC dispatch decisions can be computed nearly instantaneously. These results are promising, as our proffered approach could readily be adapted and applied in current operational planning situations to yield timely results and inform military medical planning decisions.

@&#CONCLUSIONS@&#

This paper examines the medical evacuation (MEDEVAC) dispatching problem. The intent of the research is to determine policies that improve the performance of a deployed military emergency medical service system and ultimately to increase the survivability of battlefield casualties. Development of a Markov decision process (MDP) model of the MEDEVAC dispatching problem enables examination of many different military medical planning scenarios. Solving the MDP requires the design, development, and implementation of an approximate dynamic programming (ADP) algorithm. We develop an approximate policy iteration algorithm that utilizes least squares temporal difference learning for policy evaluation. We define a set of basis functions within a linear approximation architecture to approximate the value function around the post-decision state. To demonstrate the applicability of our MDP model and to examine the efficacy of the policies produced by our ADP algorithm, we construct a notional, representative military medical planning scenario based on contingency operations in northern Syria.

The ADP policy is able to increase overall expected total discounted reward by nearly 31% compared to the myopic policy in our baseline scenario. Moreover, MEDEVAC busy time was decreased by over 8%, indicating a more efficient utilization of MEDEVAC aircraft. Examination of basis function coefficients determined by our ADP algorithm revealed MEDEVAC aircraft in close proximity to higher likelihood casualty clusters are more valuable than aircraft based further away. This is an intuitive result. These higher value MEDEVAC units should likely not be dispatched for low priority casualty events when the lower value MEDEVACs may be dispatched instead. The ADP approximation architecture is able to capture the overall time it would take for any MEDEVAC unit to service any casualty event. This is an important result, as a MEDEVAC unit may become available that could service a casualty event faster than an idle MEDEVAC that is further away. The average cruise speed of the MEDEVAC aircraft greatly impacts system performance. Results indicate that a 25% increase in speed increases expected total discounted reward by 50%. Even with such increased aircraft speed greatly improving myopic policy performance, the ADP policy still outperforms the myopic policy.

The research presented in this paper is of interest to military medical planners and relevant combatant command dispatch authorities. Military medical planners can apply this model and solution approach to compare policies for planning scenarios with fixed medical treatment facility (MTF) and MEDEVAC station locations. Moreover, planners may utilize the model to evaluate different potential MTF and MEDEVAC station allocation and location schemes to maximize overall MEDEVAC system performance. Military staffs can leverage current military intelligence and operational experiences to identify areas in which casualties are likely to occur. Once these areas are identified, planners can make informed decisions about the value of each MEDEVAC unit and ultimately achieve greater utilization of resources. Our results indicate the criticality of the MEDEVAC platform’s travel speed. Military medical planners and acquisition specialists (i.e., those personnel responsible for implementing new technology into the military) can leverage this model to examine the impact of speed and capacity. This comparison informs the future design and development of a replacement for the UH-60 A/L. Perhaps a mix of large capacity UH-60 A/L’s and a new lower-capacity, high-speed design would improve overall casualty survivability.

One limiting assumption made in this paper is that MEDEVAC units must return to their own station to refuel and re-equip after delivering patients to the MTF prior to servicing a queued call. Some bases may have both an MTF and a co-located MEDEVAC station. On such bases, it is reasonable for MEDEVACs based elsewhere to refuel after delivering patients to the MTF and then immediately proceed to service a queued call. Our restriction for MEDEVACs to return to their own bases to refuel before servicing a subsequent demand represents a simplifying assumption for this initial effort. Modifying our problem formulation and the corresponding MDP model to allow the refueling and dispatch of MEDEVACs immediately after patient delivery at an MTF with a co-located helicopter base is intended as a future extension to this work.

Other extensions to our approach include the explicit modeling of the number of casualties at the casualty event as well as the capacity of the MEDEVAC platform. One could then investigate the trade-offs associated with employing a smaller capacity, but faster MEDEVAC platform versus the currently fielded larger capacity, but slower MEDEVAC platforms. Two important aspects pertaining to the overall design of a deployed MEDEVAC system also remain of interest: determining the number of MEDEVAC units and selecting the home stations for each unit. Examining the impact of these two issues on the best dispatching policy and demonstrating the advantage of the ADP policy over the myopic policy when making such decisions would be a valuable future contribution.

Implementing an ADP policy into active dispatch operations at the aeromedical evacuation coordination center (AECC) is a difficult proposition. The myopic policy is often used in practice because it is simple to implement and performs fairly well as long as casualty events arrive less frequently. In the representative scenario we analyzed, the ADP policy and the myopic policy performed equally well when casualty events arrived every two hours. However, when casualty events arrived more frequently, the myopic policy underperformed. The important point for the military medical planning community is recognizing that a myopic policy is not always best and understanding that there are operations research techniques available that may improve MEDEVAC system performance. Yet, important challenges remain. Although this research demonstrates the potential improvement of an ADP policy over a myopic policy, it is subject to challenges concerning changing battlefield conditions. Indeed, although a MEDEVAC dispatching decision can be computed nearly instantaneously using a particular ADP policy, if the assumptions embedded within the MDP model change (e.g., with respect to the casualty event arrival process or the locations and numbers of MEDEVAC units and MTFs), then a new ADP policy must be determined. Recognizing that a new ADP policy must be determined and then actually conducting an analysis to determine the new ADP policy would pose a great challenge for AECC personnel. Further investigation on this matter will be examined in a sequel to this research.

@&#ACKNOWLEDGMENTS@&#

The views expressed in this paper are those of the authors and do not reflect the official policy or position of the United States Army, United States Air Force, Department of Defense, or the United States Government. The authors are grateful to the United States Army Medical Evacuation Proponency Directorate for its encouragement and feedback on this line of research. The authors would like to thank the coordinating editor and two anonymous referees for their insightful comments and suggestions that resulted in an improved manuscript.

@&#REFERENCES@&#

