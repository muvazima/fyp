@&#MAIN-TITLE@&#Detection of temporal lobe epilepsy using support vector machines in multi-parametric quantitative MR imaging

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Measuring regional asymmetry is fundamental for optimal classification results.


                        
                        
                           
                           DTI derived measures seem to be more informative than T2 maps for classification of TLE patients.


                        
                        
                           
                           Best classification accuracy for left TLE was 100% and for right TLE was 88.9%.


                        
                        
                           
                           Most of the discriminative features belong to the temporal lobes.


                        
                        
                           
                           The right TLE group is difficult to distinguish from controls. Possible factors are pathology heterogeneity and a limited sample size.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

MRI

DESPOT

DTI

FA

MD

Quantitative imaging

Feature selection

MRMR

ANOVA

Support vector machines

Machine learning

SVM

ROI

PCA

TLE

Epilepsy

@&#ABSTRACT@&#


               
               
                  The detection of MRI abnormalities that can be associated to seizures in the study of temporal lobe epilepsy (TLE) is a challenging task. In many cases, patients with a record of epileptic activity do not present any discernible MRI findings. In this domain, we propose a method that combines quantitative relaxometry and diffusion tensor imaging (DTI) with support vector machines (SVM) aiming to improve TLE detection. The main contribution of this work is two-fold: on one hand, the feature selection process, principal component analysis (PCA) transformations of the feature space, and SVM parameterization are analyzed as factors constituting a classification model and influencing its quality. On the other hand, several of these classification models are studied to determine the optimal strategy for the identification of TLE patients using data collected from multi-parametric quantitative MRI.
                  A total of 17 TLE patients and 19 control volunteers were analyzed. Four images were considered for each subject (T1 map, T2 map, fractional anisotropy, and mean diffusivity) generating 936 regions of interest per subject, then 8 different classification models were studied, each one comprised by a distinct set of factors. Subjects were correctly classified with an accuracy of 88.9%. Further analysis revealed that the heterogeneous nature of the disease impeded an optimal outcome. After dividing patients into cohesive groups (9 left-sided seizure onset, 8 right-sided seizure onset) perfect classification for the left group was achieved (100% accuracy) whereas the accuracy for the right group remained the same (88.9%).
                  We conclude that a linear SVM combined with an ANOVA-based feature selection+PCA method is a good alternative in scenarios like ours where feature spaces are high dimensional, and the sample size is limited. The good accuracy results and the localization of the respective features in the temporal lobe suggest that a multi-parametric quantitative MRI, ROI-based, SVM classification could be used for the identification of TLE patients. This method has the potential to improve the diagnostic assessment, especially for patients who do not have any obvious lesions in standard radiological examinations.
               
            

@&#INTRODUCTION@&#

Magnetic resonance imaging (MRI) is a powerful tool for the evaluation of patients with brain disorders and neurological diseases. For those with temporal lobe epilepsy (TLE), the most common type of epilepsy in adults [1], it is the entry point to a clinical workflow that may conclude with temporal lobe surgery and an improved quality of life. Finding evidence of seizures on MRI is a clear diagnostic element for TLE, however this task is not easy given that epileptogenic lesions are often small and can be missed, they can be uncertain due to subtle intensity changes, or only perceptible after image post-processing. Furthermore, due to the multi-factorial nature of the disease, the localization and type of the lesions can vary from patient to patient.

While the visual inspection is a common radiological procedure for the diagnosis of TLE, it has been shown that the detection of brain pathologies associated with TLE can be improved with computer-assisted, automatic multi-parametric MRI analysis. For example, the detection of changes in the shape, volume and intensity of the hippocampus has been studied using structural T1- and T2- weighted images [2–5]; White matter abnormalities have been detected with diffusion tensor imaging (DTI) in TLE patients [6–8], and DTI has been employed concurrently with functional MRI (fMRI) to perform language lateralization of TLE patients [9].

A support vector machine (SVM) is a classifier that uses a priori knowledge in the form of group labels (supervised learning) and produces a decision boundary that can be used to determine the label of new examples [10,11]. Recent studies have examined the possibility of improving TLE detection using SVMs on MRI data. For example Focke et al. [12] show correct patient lateralization (left vs. right seizure onset) using SVMs on T1-weighted and DTI data. In addition to lateralization, Keihaninejad et al. [13], demonstrate the identification of TLE cases with hippocampal atrophy from cases without it using SVM on regional volumes obtained from T1-weighted MRI.

In this context, the goal of the current study is to explore TLE detection using multi-parametric quantitative MRI and support vector machines. For that purpose, quantitative maps of T1, T2, as well as fractional anisotropy (FA), and mean diffusivity (MD) are estimated for every participating subject. Quantitative MRI measures biophysical tissue properties and it has the potential to be more sensitive to TLE detection than T1- and T2-weighted images. Additionally, quantitative measurements are independent of experimental settings and thus comparable between different scanners, institutions and over different points in time [14].

@&#METHODS@&#

@&#OVERVIEW@&#

All subjects in this study underwent an imaging protocol approved by the Office of Research Ethics of Western University (Canada). The imaging protocol comprised DESPOT1, DESPOT2 [15,16] and DTI sequences, resulting image data being processed to obtain four different quantitative maps: T1, T2, fractional anisotropy (FA) and mean diffusivity (MD). Anatomical atlas-based labeling was used to define ROIs on each one of the quantitative maps and subsequently to measure and extract regional features.

Given that the number of features exceeded the number of subjects (936 features, 36 subjects), two different feature selection methods were explored to discard irrelevant or uninformative features. Then, the feature space was further reduced using principal component analysis (PCA).

An SVM was trained/tested on the filtered feature space using a leave-one-out cross-validation strategy (LOOCV), where the SVM was tasked with predicting the label (patient or control) for the omitted subject (Fig. 1
                        ). Once all subjects were evaluated, sensitivity, specificity and classification accuracy were measured and reported. This procedure was repeated for each classification model obtained by the combination of the following elements:
                           
                              •
                              the image that originates the features (T1, T2, FA, MD, or all combined)

the method to select features

the cardinality of the requested feature set [K]

the use of PCA to reduce the feature space

the type of SVM

A detailed comparison among classification models is reported in the Results section along the best classification scenarios. Specific recommendations regarding the optimal model are given in the Discussion section. In addition, an analysis of the elements constituting a classification model and their influence in the classifiers performance is also discussed. Finally, the features relevant for classification are analyzed and their clinical significance is considered.

Thirty-six individuals participated in this study, 19 of whom were control volunteers (age 32±10, 12 male, 7 female) and 17 TLE patients (age 35±10, 8 male, 9 female). All the patients had lateralizable seizures (confirmed by EEG) and all of them were eligible for temporal lobectomy (9 left, 8 right). Preoperative MRI and post-surgical pathology confirmed the presence of Mesial Temporal Sclerosis (MTS) in 8 of them.

Subjects were scanned (presurgically in the case of patients) using a 3T MR scanner (GE Discovery MR750) with whole brain, 1mm isotropic DESPOT1-HiFi and DESPOT2-FM,T1 and T2 mapping sequences [15,16] respectively, optimized for imaging at 3T [17]. Two SPGR images were acquired (flip angles of 4° and 18°) along with an inversion-prepared spoiled gradient recalled acquisition in the steady state (SPGR) to calculate a quantitative T1 map. Five balanced steady state free-precession (bSSFP) images were acquired with phase cycling (flip angles of 15°, 35°, 60°) to estimate a quantitative T2 map using the DESPOT2-FM procedure [18]. All images were co-registered to the 18° flip angle SPGR image using the FLIRT registration tool [19] from the FSL software (http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/) to account for motion between scans prior to the computation of the T1 and T2 maps.

A DTI sequence was also acquired with the following parameters: 2.4mm isotropic, 41 directions, b-value =1000, 4 non-weighted (b
                        =0) volumes. Non-linear distortions were corrected by deformable registration of the average of the b
                        =0 volumes to the undistorted T1 map, using a diffeomorphic registration method [20,21]. Eddy current correction and diffusion tensor estimation was performed using FSL's diffusion toolbox FDT. Maps of fractional anisotropy (FA) and mean diffusivity (MD) were transformed and re-sampled to the coordinate system defined by the 1mm isotropic T1 map. Synthetic T1-weighted images, with inherent bias-field correction, were generated from the T1 maps [22] and used in place of directly acquired T1-weighted images for subsequent segmentation.

This preprocessing stage yielded four co-registered quantitative maps (T1, T2, FA and MD) for each subject.

Volumetric segmentation of the synthetic T1-weighted images was performed with Freesurfer (http://surfer.nmr.mgh.harvard.edu/. This processing included removal of non-brain tissue using a hybrid watershed/surface deformation procedure, automated Talairach transformation, segmentation of the subcortical white matter and deep gray matter volumetric structures (including hippocampus, amygdala, caudate, putamen, ventricles). Once the cortical models were completed, the cerebral cortex was parcellated into regions based on gyral and sulcal structures [23]. This segmentation produced bilateral regions of interest for every subject in subject image space. Total of 35 cortical, 34 white matter and 9 subcortical regions (78 in all) were identified per hemisphere. The segmented ROIs were used to extract features from the quantitative maps (T1, T2, FA, and MD) for each subject (Fig. 3 
                        ).

Two types of features were extracted: The ROI mean intensity, and the asymmetry between correspondent left and right ROIs (i.e. left and right hippocampus). The asymmetry was expressed as the non-parametric two-sample Kolmogorov–Smirnov statistical score between left–right region pairs. For each subject, a vector of 936 features was created, accounting for 624 mean intensity features (156 ROIs, 4 image sources) and 312 intensity difference features (78 left–right region pairs, 4 image sources). Each feature vector was labeled according to the respective subject ground truth (i.e. patient or control). Prior to using these feature vectors as input for the SVM, irrelevant features were discarded.

Linear SVMs are used in scenarios where the features that discriminate between subject groups are linearly separable. Linear SVMs estimate the hyperplane or decision boundary that provides the largest margin (separation) between the two groups [11]. In contrast, non-linear SVMs such as radial basis function SVMs provide a non-linear boundary using the kernel trick as described in [10] which effectively transforms the non-linear space into a space that has a higher number of dimensions but where the features are linearly separable. The scikit-learn library [24] was used to implement the SVM classifiers in this work. Both linear and non-linear support vector machines were used for patient detection (Fig. 2
                        ).

Each subject was assigned one of two possible labels: patient or control. The SVM was trained using feature vectors to distinguish between these two groups. A leave-one-out cross-validation approach (LOOCV) was used for training/testing. One subject was left out from the training stage while the remainder were used for training (i.e. to compute the decision boundary). Then, the classifier was asked to predict the correct label for the excluded subject. This procedure iterated until all subjects had been classified. Afterwards, the sensitivity, specificity and accuracy were calculated as follows:


                        
                           
                              (1)
                              
                                 Sensitivity
                                 =
                                 
                                    tp
                                    
                                       tp
                                       +
                                       fc
                                    
                                 
                              
                           
                        
                        
                           
                              (2)
                              
                                 Specificity
                                 =
                                 
                                    tc
                                    
                                       fp
                                       +
                                       tc
                                    
                                 
                              
                           
                        
                        
                           
                              (3)
                              
                                 Accuracy
                                 =
                                 
                                    
                                       tp
                                       +
                                       tc
                                    
                                    
                                       p
                                       +
                                       c
                                    
                                 
                              
                           
                        where tp = correctly identified patients, tc = correctly identified controls, fp = controls misclassified as patients, fc = patients misclassified as controls, p = total number of patients, c = total number of controls.

As shown in the Results section, patients were subsequently split into those with left temporal lobe seizures and those with right temporal lobe seizures (L-TLE and R-TLE respectively). The same training/testing procedure was repeated to assess the detectability of each patient subgroup.

The outcome of an SVM is influenced by the feature space over which it operates (extrinsic element) as well as by the type of SVM (intrinsic element). In this work, we define a classification model as the combination of extrinsic and intrinsic elements selected for the operation of the SVM.

Each model is built in three steps as shown in Fig. 1: One of two possible feature selection methods (correlation-based or ANOVA-based); the option or not of performing dimensionality reduction with PCA; and the type of SVM (linear or non-linear). Eight different classification models were evaluated:
                           
                              •
                              
                                 Correlation-svm-linear: Features are selected with a correlation-based method and a linear SVM runs on this space.


                                 Correlation-pca-svm-linear: After the correlation-based feature selection, the dimension of the resulting feature space is reduced using PCA. A linear SVM is used for classification.


                                 Correlation-svm-rbf: Features are selected with a correlation-based method and the SVM used is non-linear (radial basis functions).


                                 Correlation-pca-svm-rbf: The dimension of the feature space obtained with correlation-based selection is simplified using PCA. The employed SVM is non-linear.


                                 Anova-svm-linear: Features are selected using an ANOVA-based approach. The SVM used is linear.


                                 Anova-pca-svm-linear: Features are selected using an ANOVA-based approach. The resulting feature space is transformed using PCA. The SVM used is linear.


                                 Anova-svm-rbf: Features are selected using an ANOVA-based approach. The SVM is non-linear.


                                 Anova-pca-svm-rbf: Similar to the previous case, but the resulting feature space is reduced with PCA.

The following sections describe the steps taken to build a classification model, while the Results section compares models based on their performance. Finally, the best and worst models considering feature selection, PCA, cardinality and computational performance elements are discussed and an analysis of relevant, stable features and their clinical significance is presented.

Feature selection algorithms identify features that are pivotal for classification [25], and optimize the accuracy of machine learning algorithms (such as SVMs), while avoiding overfitting of the data [26].

In theory, feature selection should be performed on data that are independent of both the training and testing sets [27, p. 222]. In practice, this is not always feasible, particularly when the number of features is much larger than the number of subjects, as is the case in our work. To address this issue, our feature selection step subsamples the training set, creating leave-one-subject-out folds and then scores features on each resulting fold. After this, a voting algorithm examines the agreement among the folds, and decides which features belong to the final feature set. This approach reuses the data efficiently and reduces the outlier effect (performance reduction attributable to a subject).

The training set was subsampled by leaving one subject out every time, creating M folds for a training set of size M (Fig. 4
                           ). On each fold, one of two possible feature selection methods (correlation or ANOVA-based) was evaluated. Each method began by assigning a score to each of the 936 features for each subject in the current fold. Then, using the cardinality parameter K, set by the user, the respective method selected the top 2K features based on their scores. Finally, after all folds were evaluated, the voting algorithm decided on the final K features by reviewing the agreement among the folds.

Correlation-based feature selection analyzes the linear dependency between features and classification groups. Features that have a high correlation with the group labels (relevance) are good candidates for the prediction of the subject's group [25]. Similarly to previous work, our approach accounts for inter-feature correlation. Features with high inter-feature correlation (redundancy) are penalized producing a set where features tend to be linearly independent of each other [28–30].

The correlation-based feature selection, approached in three steps: relevance evaluation, redundancy evaluation, and final scoring, is applied to each subsampled training fold.

First, feature relevance is evaluated using a Pearson correlation coefficient. Let N(=
                           M
                           −1) be the total number of subjects in the current fold; f
                           
                              i,s
                            the value of the feature i evaluated in the subject s; and t
                           
                              s
                            the binary group label (−1, +1) for subject s (for binary classification). Then, the relevance for feature i is obtained by:


                           
                              
                                 (4)
                                 
                                    ρ
                                    
                                       (
                                       i
                                       )
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                s
                                                =
                                                1
                                             
                                             N
                                          
                                          (
                                          
                                             f
                                             
                                                i
                                                ,
                                                s
                                             
                                          
                                          −
                                          
                                             
                                                
                                                   f
                                                   i
                                                
                                             
                                             ¯
                                          
                                          )
                                          (
                                          
                                             t
                                             s
                                          
                                          −
                                          
                                             t
                                             ¯
                                          
                                          )
                                       
                                       
                                          
                                             
                                                
                                                   ∑
                                                   
                                                      s
                                                      =
                                                      1
                                                   
                                                   N
                                                
                                                
                                                   
                                                      (
                                                      
                                                         f
                                                         
                                                            i
                                                            ,
                                                            s
                                                         
                                                      
                                                      −
                                                      
                                                         
                                                            
                                                               f
                                                               i
                                                            
                                                         
                                                         ¯
                                                      
                                                      )
                                                   
                                                   2
                                                
                                                
                                                   ∑
                                                   
                                                      s
                                                      =
                                                      1
                                                   
                                                   N
                                                
                                                
                                                   
                                                      (
                                                      
                                                         t
                                                         s
                                                      
                                                      −
                                                      
                                                         t
                                                         ¯
                                                      
                                                      )
                                                   
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 
                                    
                                       f
                                       i
                                    
                                 
                                 ¯
                              
                            and 
                              
                                 t
                                 ¯
                              
                            are the feature and the label averages respectively, evaluated using all the subjects in the current fold. After this, the top 2K features are selected.

In the second step, feature redundancy is calculated as the average of the correlation between each feature and the remaining 2K
                           −1 features selected on the first step (inter-feature correlation). Features that have a high redundancy do no provide additional information due to high collinearity, and can be discarded. Let i and j be two different features (i
                           ≠
                           j), the inter-feature correlation, in the current fold, is calculated as it follows:
                              
                                 (5)
                                 
                                    δ
                                    
                                       (
                                       i
                                       ,
                                       j
                                       )
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                s
                                                =
                                                1
                                             
                                             N
                                          
                                          (
                                          
                                             f
                                             
                                                i
                                                ,
                                                s
                                             
                                          
                                          −
                                          
                                             
                                                
                                                   f
                                                   i
                                                
                                             
                                             ¯
                                          
                                          )
                                          (
                                          
                                             f
                                             
                                                j
                                                ,
                                                s
                                             
                                          
                                          −
                                          
                                             
                                                
                                                   f
                                                   j
                                                
                                             
                                             ¯
                                          
                                          )
                                       
                                       
                                          
                                             
                                                
                                                   ∑
                                                   
                                                      s
                                                      =
                                                      1
                                                   
                                                   N
                                                
                                                
                                                   
                                                      (
                                                      
                                                         f
                                                         
                                                            i
                                                            ,
                                                            s
                                                         
                                                      
                                                      −
                                                      
                                                         
                                                            
                                                               f
                                                               i
                                                            
                                                         
                                                         ¯
                                                      
                                                      )
                                                   
                                                   2
                                                
                                                
                                                   ∑
                                                   
                                                      s
                                                      =
                                                      1
                                                   
                                                   N
                                                
                                                
                                                   
                                                      (
                                                      
                                                         f
                                                         
                                                            j
                                                            ,
                                                            s
                                                         
                                                      
                                                      −
                                                      
                                                         
                                                            
                                                               f
                                                               j
                                                            
                                                         
                                                         ¯
                                                      
                                                      )
                                                   
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           Then, the redundancy for feature i in the current fold, is calculated as:


                           
                              
                                 (6)
                                 
                                    δ
                                    
                                       (
                                       i
                                       )
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                                ;
                                                j
                                                ≠
                                                i
                                             
                                             
                                                2
                                                K
                                             
                                          
                                          δ
                                          
                                             (
                                             i
                                             ,
                                             j
                                             )
                                          
                                       
                                       
                                          2
                                          K
                                          −
                                          1
                                       
                                    
                                 
                              
                           The third step computes the score, where for each feature in the 2K set, the feature-label correlation (relevance) is divided by the respective average inter-feature correlation (redundancy). The scoring function S for feature i in the current fold, was then defined as:
                              
                                 (7)
                                 
                                    S
                                    (
                                    i
                                    )
                                    =
                                    
                                       
                                          ρ
                                          
                                             (
                                             i
                                          
                                          )
                                       
                                       
                                          δ
                                          
                                             (
                                             i
                                             )
                                          
                                       
                                    
                                 
                              
                           
                        

Finally, the 2K scored features are selected as the feature set for the current fold.

To the best of our knowledge, there is very little literature on the use of ANOVA as a feature selection method for classification of structural brain images. The use of ANOVA for feature selection in neuroimaging, has mainly focused on classification of fMRI datasets [31–33]. Nonetheless, ANOVA has be used in others areas of science to attenuate the curse-of-dimensionality 
                           [26] by discarding variables with poor statistical significance thereby reducing the size of the feature space. We explored this approach as an alternative to the more commonly used correlation-based method. Similar to the latter, the ANOVA approach is supervised feature selection method where the a priori knowledge of the subject groups (ground truth) is used in the evaluation of the features. Unlike the correlation-based method introduced earlier, the ANOVA-based method does not penalize redundant features.

This method proceeds as follows: The subjects in the current training set fold (as defined above) are first divided into groups according to the classification labels of the given experiment (i.e. patients vs. controls). Then, a one-way ANOVA test is performed for each feature between the groups. The null hypothesis for the ANOVA test is that the mean value for each feature is the same between the groups. If there is evidence that a feature mean is significantly different between groups, then that feature becomes a good candidate for the prediction of the group. Finally, a scoring function is assigned to quantify the relevance of each feature according to the result of its ANOVA test.

For any feature i, with 0<
                           i
                           ≤
                           T where T is 936, the total number of features, (K
                           <
                           T), the ANOVA-based score S for feature i in the current fold is defined as:
                              
                                 (8)
                                 
                                    S
                                    (
                                    i
                                    )
                                    =
                                    1
                                    −
                                    
                                       α
                                       i
                                    
                                 
                              
                           where α
                           
                              i
                            is the p-value resulting of the correspondent F-test for feature i. In other words, significant features have low p-value and a correspondingly high score. Features with the top 2K scores are selected to produce the feature set for the current fold.

For both the correlation and ANOVA-based methods, the final feature set is built on a feature-by-feature basis examining the scores obtained by each feature among the participating folds. For any given feature i, the vote collected from the fold q is given by:


                           
                              
                                 (9)
                                 
                                    V
                                    (
                                    i
                                    ,
                                    q
                                    )
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      S
                                                      (
                                                      i
                                                      )
                                                   
                                                   
                                                      when
                                                         
                                                      i
                                                      ∈
                                                      q
                                                   
                                                
                                                
                                                   
                                                      0
                                                   
                                                   
                                                      when
                                                         
                                                      i
                                                      ∉
                                                      q
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           Thus the vote is 0 if the feature i was not scored in fold q, and is S(i), the scoring function, otherwise. The agreement is reached by:
                              
                                 (10)
                                 
                                    A
                                    (
                                    i
                                    )
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                q
                                                =
                                                1
                                             
                                             M
                                          
                                          V
                                          (
                                          i
                                          ,
                                          q
                                          )
                                       
                                       M
                                    
                                 
                              
                           The agreement function A(i) becomes a score average when feature i has been scored across all folds. Otherwise, infrequent features are penalized even if they have scored highly in a given particular fold. The purpose of this penalization is to enforce feature stability, producing feature sets that are robust.

Given that each voting fold scores 2K features, after the agreement function has been applied, only features in the top K agreement values are retained. These features become the final feature set that is used for classification.

Dimensionality reduction of high-dimensional feature spaces has been used in neuroimaging to reduce computational complexity, by projecting the high dimensional data onto a space of smaller dimensionality without loss of information [34]. Techniques such as principal component analysis (PCA) [35] have been applied directly to unfiltered highly-dimensional feature spaces in classification problems. However it has been shown that the resulting condensed space may carry over noise from original features, negatively affecting classification performance [36].

As an alternative, we present a two-step approach, where the feature space is filtered using the proposed feature selection stage, followed by the application of PCA to the resulting space. Applying PCA to the feature space creates a new space where each dimension is uncorrelated. This could reduce noise and improve classification performance on this space [37].

There is, however, a trade-off when using PCA. On one hand, PCA is an unsupervised method. This means that class labels are not required. On the other hand, the number of dimensions or principal components to which the feature space is reduced to, is a parameter that must be set by the user. To take advantage of the PCA technique and tackle the selection of the number of principal components, this parameter was included in the optimization step described below.

A linear SVM has only one parameter C which determines the level of allowed regularization computing the decision boundary. A low C creates a decision boundary with a large margin between the two classes, while a high C attempts to classify all the data points correctly producing a narrow margin. In contrast, a non-linear, radial basis function SVM has two parameters: C, the regularization parameter and γ which determines the weight of individual observations in the resulting decision boundary. A low γ produces higher weighting while a high γ produces lower weighting (Fig. 5
                        ).

Clearly, the choice of SVM brings along parameters that need to be optimized. Parameter selection can either boost or hinder the generalization ability of the SVM. To determine the optimal parameters, a cross-validated grid search was employed on the parameter space as follows: The training set was divided into evaluation and validation partitions using cross-validation (CV) and an SVM was trained using parameters taken from the parameter grid onto the evaluation partition. The validation partition was then used to measure the accuracy of the classifier. This process was repeated for all possible training and validation partitions given by CV to obtain an average measurement of classification accuracy. This CV process was repeated for all possible parameter sets in the parameter grid. Finally the parameter set that produced the best average accuracy was selected as the optimal choice to configure the SVM (Fig. 6
                        ).

If the model being evaluated included the PCA reduction step, a range for the number of principal components was included into the parameter grid. In that case, parameter optimization occurred on the PCA-reduced space.

@&#RESULTS@&#

Each of the eight classification models was evaluated on the full range of possible K values (from 2 to 936 features for the universal feature space (T1+T2+FA+MD), and from 2 to 234 for individual image subspaces). Given that K is the only parameter that needs to be set by the user, an adequate range for the choice of K is addressed in the Discussion section.

A maximum accuracy of 88.9% for automatic classification between patients and controls was obtained by an anova-pca-svm-linear model using 10 features from the T1 image only as shown in Table 1
                        . This model misclassified 3 patients, all of whom were diagnosed with R-TLE, and one control volunteer. In general the classification models that employed features from the T1 image, obtained 81% average accuracy, followed by the MD models with an average accuracy of 75%, T2 models with an average accuracy of 74% and in last place FA models with an accuracy of 67%.


                        Figs. 7 and 8
                        
                         (I) show regions selected by the best classification model that were common to all subjects. Fig. 7 shows left/right mean intensity features while Fig. 8 shows regions selected based on asymmetry. Thus, the middle-temporal cortex, the right parahippocampal white matter and the left entorhinal white matter are selected based on their mean intensity. In contrast, the inferior-temporal, middle-temporal and parahippocampal white matter regions are selected based on their asymmetry. These findings are consistent with previous studies where changes in temporal white matter regions have been associated to TLE [6,8]. Although feature selection is not restricted to the temporal lobe, it is relevant to notice that all the regions shown in the overlay are temporal lobe regions.

We hypothesized that the heterogeneity of the patient group (L-TLE, R-TLE) was a relevant factor that influenced the accuracy of the patient identification process. Its effect can be seen by performing a dimensionality reduction transformation on the feature space (Fig. 9
                        ). While the control group forms a cluster, the patient groups are sparse. This configuration was consistently reproducible for feature spaces of different sizes and for different data projection techniques including PCA, Isomaps and Multidimensional Scaling. These projections demonstrated that the L-TLE group tends to be linearly separable from the control group, while the R-TLE group intersected with the control group. This configuration could explain why perfect classification is achievable between controls and L-TLE, while classification between controls and R-TLE appeared to be more challenging. To address the heterogeneity matter, classification accuracy was examined using more homogeneous patient subgroups: L-TLE vs. controls (Experiment II) and R-TLE vs. controls (Experiment III).

Perfect classification (100% accuracy) was attainable for L-TLE patients (Table 2
                        ). This result was reported by the correlation-pca-svm-rbf model with MD features only (K
                        =7). However, the correlation-pca-svm-linear and anova-pca-svm-linear models also obtained good classification accuracy (96.4%) using MD features with low feature set cardinalities. In general, linear models were not outperformed by non-linear models. Overall, PCA improved classification accuracy.

When looking at individual feature subspaces (columns on Table 2), the best accuracies across models were obtained by the MD and T1 subspaces where the average accuracy was 90% for T1, and 94% for MD. The average classification accuracy across models dropped in the FA and T2 subspaces to 84% and 73% respectively. In general, classifiers had an excellent performance on the universal feature space where the average accuracy was 90%.


                        Figs. 7 and 8(II) show the regions selected by the best classification method that were common to all subjects. In general, the asymmetry between left and right temporal lobe white matter regions, and the mean intensity of left temporal cortical regions are determined to be relevant. Specifically, the left hippocampus, the left middle-temporal cortex and the left entorhinal white matter regions are selected based on the mean intensity, whereas the enthorhinal cortex, the superiotemporal white matter and the temporal pole are selected based on their asymmetry. All of these regions clinically correlate to pathologies in L-TLE patients such as mesial temporal sclerosis (MTS) and focal cortical dysplasia (FCD).

As presumed by the PCA projection, the classification between R-TLE patients and controls was a difficult problem. This was reflected in classification accuracy average of 74% across the different models as well as the high cardinality reported by each model (Table 3
                        ).

A possible explanation for these results is that the feature space does not have discriminative features to distinguish the R-TLE patients from controls (evidenced by the PCA projection). Hence, reliable decision boundaries cannot be obtained. There is some evidence supporting the proximity of R-TLE patients and controls in this type of images. For example, Zhong Xue et al. [6] have found fewer regions to distinguish R-TLE patients from controls than L-TLE from controls. Similarly, Ahmadi et al. [7] have reported fewer and less extensive gray matter change in R-TLE than in L-TLE patients with respect to controls using voxel-based morphometry.

The average accuracy for T1 and T2 subspaces across models was 74 and 73% respectively. For FA and MD, it was 76 and 74%. Similarly to the classification of L-TLE patients, the best accuracy on individual image spaces was obtained on the MD space. The highest accuracy on the universal feature space was 88.9% and it was obtained using a correlation-svm-linear model with 141 features. Intuitively, models with such a high number of features are less useful to the researcher than simpler models. One of such alternatives is presented by the correlation-pca-svm-linear model on T1 with K
                        =29 and an accuracy of 81.5%.


                        Figs. 7 and 8(III) show the regions that were common to all subjects, reported by the aforementioned simpler T1 model. Regions selected by virtue of their mean intensity are show in Fig. 7. These regions are the right inferiotemporal cortex, the right entorhinal and parahippocampal white matter regions as well as the right white matter in temporal pole. Neither extra-temporal nor left-temporal regions were chosen.

When looking at regions selected based on asymmetry, Fig. 8 reveals numerous features most of them corresponding to the temporal lobes, including the inferiotemporal cortex, the parahippocampal white matter and in general the temporal lobe white matter. The difficulty of the classification between R-TLE and controls explains why the optimal results are obtained by the inclusion extra-temporal features. To validate this assumption we restricted the method to select only temporal lobe features and we found that the classification accuracy reduced from 81.5 to 70.4%.

After obtaining the classification results, we performed an analysis of the two feature selection strategies. The two methods were compared measuring their stability and similarity. To measure the stability of each method we calculated the average Tanimoto distance among feature sets [38,39] in the outer leave-one-out cross-validation loop (training/testing loop). Fig. 10
                         shows the results. As expected, stability increases asymptotically towards 100% as the number of requested features approaches the number of available features. The [2< K< 100] range of cardinalities is most interesting as stability varies between 40 and 70% (right column in the figure). This range gives a better estimation of stability as the methods are asked to retrieve at most 100 features out of 936 for each iteration of the cross-validation loop. In comparison, in the absence of any heuristic, the likelihood of selecting 100 features out of 936 for the same 36 subjects is approximately 10−36. As shown in the figure, the most stable features (> 80%) were obtained in Experiment II (L-TLE) and the least stable features (< 70%) in Experiment III (R-TLE). In all three experiments it was observed that classification accuracy was proportional to feature stability.

To assess similarity between methods, in a given training fold, the Tanimoto distance between the ANOVA-generated feature set and the correlation-generated feature set was measured. This was repeated for all the training folds generated by the LOOCV procedure. Then, the average Tanimoto distance was reported. Fig. 11
                         shows how similarity varies according to cardinality. For very small cardinalities (K
                        <20) the two methods (correlation, ANOVA) generate very similar features. In the [20< K<100] range (right column in the figure) the similarity drops to 50%. This is desirable as it shows that the two methods are sufficiently distinct in the cardinality range that contains both the best classification results and the inherent dimensionality of the problem (see the Discussion section). Conceptually, the dissimilarity between the two methods can be explained by the redundancy evaluation step in the correlation method. However, as the cardinality approaches the number of available features, the correlation method runs out of features to discard and the similarity between methods increases.

Feature relevance was evaluated by analyzing how frequently a feature was selected when the classifier was successful. The level of success was defined by setting a minimum accuracy threshold, and the analysis was restricted to those cardinalities where the classifier performance was higher than the threshold. For each cardinality the feature frequency was measured on the external LOOCV (training/testing) analyzing the features obtained from each training fold. Then, a ranking was obtained by averaging frequencies across the selected cardinalities. The ROI pointed at by the top ranked relevant features are analyzed in the Discussion section.

Alternatively, we considered using the coefficient assigned to features by the linear classifiers as a measurement of their relevance. However this approach would not reflect the importance of the features in non-linear classifiers or in classifiers that use PCA. We think that our approach is more comprehensive as it identifies features that are relevant and stable across cardinalities regardless of the type of classification model..

We quantified the effect of the small sample size in the performance of the classifier by evaluating the SVM reliability index (SRI) 
                        [40]. Let w be the vector defining the decision boundary (vector normal to the decision plane), this vector is obtained as the solution of the respective convex optimization problem. Let w
                        * be the alternative convex optimization solution after randomly removing some data points. If we have enough data for training then we can randomly remove some and what is left will result in w
                        * ≈ w. If we do not have enough data, the random removal of training data will result in a very different decision boundary and w
                        * ≠ w. This is quantified by the SRI as follows:


                        
                           
                              (11)
                              
                                 SRI
                                 (
                                 
                                    
                                       
                                          w
                                       
                                    
                                    *
                                 
                                 ,
                                 
                                    
                                       w
                                    
                                 
                                 )
                                 =
                                 |
                                 r
                                 (
                                 
                                    
                                       
                                          w
                                       
                                    
                                    *
                                 
                                 ,
                                 
                                    
                                       w
                                    
                                 
                                 )
                                 |
                              
                           
                        which is the absolute value of the Pearson product-moment correlation coefficient between w
                        * and w. The SRI was evaluated for each experiment by selecting a training set to estimate w, then the training set was randomly subsampled 10 times to obtain w
                        * estimates. Each SRI result was averaged. This process was repeated for all training sets.

It is expected that the decision boundary remains stable when data points that do not weight in its calculation are removed. However, when support vectors are eliminated, this causes the boundary to be redefined and consequently the SRI decreases. Our results show that the decision boundary is fairly sensitive to the training set size (Fig. 15). However, in all three experiments we obtained SRI measures above 80% (arbitrarily set).

It is relevant to notice that the stability of the boundary is independent from performance: if two classes are sufficiently distinct (large margin), then they can be linearly separable by many possible boundaries. From the PCA projections and the experimental results, we believe this is the case for the L-TLE group vs. controls.

@&#DISCUSSION@&#


                        Table 4
                         summarizes the ROIs associated to relevant features, and Fig. 14 summarizes this information graphically grouping by lobe, type of feature, and quantitative MRI parameter. In all three experiments most of these region belong to the temporal lobe (a total of 71 features in the table). Among all, the middle-temporal, superior-temporal, the temporal pole, and hippocampal ROIs are common to all three classification experiments revealing the importance of these regions for the identification of patients. The selection of the hippocampus is to be expected, since it is the presumed focus in mesial temporal sclerosis (MTS), the most common pathology in this group of patients [41]. Relevant features from the neocortex and adjacent white matter could relate either to changes due to seizure propagation, such as gliosis or neuronal loss, or could be related to the presence of epileptogenic lesions in the neocortex, considering that the patient group also included subjects with focal cortical dysplasia (FCD). The inclusion of some extra-temporal regions in the set of relevant features (in the frontal lobe and parietal lobes, as well as the occipital lobe) is also remarkable, however, here the features likely relate to white matter abnormalities related to seizure propagation, and not extra-temporal epileptogenic lesions, since extra-temporal onset was not observed clinically in these patients.

We also analyzed relevant features based on the type of feature. We found that asymmetry features (ks) were chosen more often than mean intensity features from each hemisphere, highlighting the benefit of sensitive examination of intensity distributions between bilateral regions. In patients with unilateral TLE was expected that asymmetry features were highly relevant since seizure onset is restricted to one hemisphere. However, asymmetry features may also be sensitive to compensatory mechanisms occurring in the contralateral hemisphere and thus may not be specific to seizure-related abnormalities. When examining individual image spaces, the classifiers performed optimally in the T1 and MD subspaces, and generally good in FA and T2. Also, classifiers on the universal space obtained accuracies that were better or close to the best individual image subspace. We also see that in Experiments I and II, MD features were chosen most, followed by FA. The lateralization ability of diffusion metrics has been shown before [6–8], including in our previous work [42], where DTI and relaxometry quantitative imaging parameters were compared in the temporal lobe. This work extends this previous findings showing how these quantitative imaging parameters across the entire brain can be used to classify patient groups.

The proposed method requires that the feature set cardinality K is set a priori. To determine an adequate range for K we plotted K against classification accuracy. For this analysis the best model for each experiment was selected. Due to the heuristic of the feature selection methods described in this paper, relevant features are selected first, then, as expected, the accuracy degrades as noisy/non-relevant features are added. Fig. 12
                         shows that above 200 features the average classification approaches the line of random chance. The classifiers present an acceptable behavior with several peaks above the 80% classification rate for K< 50,. Given that these plots are taken on the best models we used K
                        =50 as an upper bound for the selection of K.

Independently, a cross-validated L1-penalized logistic regression model was used to estimate K for Experiments I–III. The number of non-zero coefficients in the regularized regression (regularization factor = 0.1) corresponds to the estimated size of the feature set K. The results are presented in Table 5
                        . In all cases the estimated K is lower than the upper bound previously obtained by simple inspection (K
                        <50). Also, it is important to notice that the estimated K (as shown in Table 5) is close to the number of observations (36 subjects).

Considering classification accuracy, correlation-based models performed slightly better than those based upon ANOVA. This could be explained by the fact that the ANOVA-based method does not eliminate superfluous (collinear) features. Though in theory feature redundancy should not affect classification accuracy, the presence of redundant features in the feature set could hinder the algorithm from finding an optimal set before the desired cardinality is reached.

In terms of the type of SVM, we found that linear classifiers exhibited slightly better performance than non-linear ones in our dataset. However this could be attributable to the size of our sample, which may not be large enough to adequately estimate non-linear SVM parameters.

In terms of computation time the ANOVA-based classifiers outperformed the correlation-based classifiers for large cardinalities (Fig. 13
                        )
                        . This estimation was performed on a machine with 4 CPU cores (Intel Core I7-2600 CPU @ 3.4GHZ) running Ubuntu Linux 12.04 with 16GB of RAM. The ANOVA-based method can evaluate several features simultaneously and it does not incur in the computational cost of the feature redundancy evaluation. Additionally, the PCA transformation and the parameter optimization for non-linear classifier increased significantly the time required to fully classify a dataset.

With these considerations in mind, a good compromise between classification accuracy and computation time can be achieved by using an anova-pca-linear model. A linear SVM requires less parameter tuning than a non-linear SVM and classification accuracy did not degrade as rapidly as it did for non-linear classifiers as the cardinality increased. The ANOVA approach is more scalable than the correlation-based method in terms of computational cost when evaluating large feature spaces. In addition, the PCA transformation has the effect of decorrelating the resulting feature space which explains why the classification accuracy is similar to those classifiers using the correlation-based method where redundant features are discarded.

Our results were comparable and in some cases better than to those obtained by Focke et al [12]. For example, the classification accuracy for L-TLE described by their method varied between 93 and 95%, while our method obtained perfect identification. In contrast, their accuracy for R-TLE detection was 97% while ours was 88.9%. We believe that the size of the patient group is a key factor to obtain higher accuracies in the R-TLE group. Their patient group was more than twice the size than ours (38 vs. 17). In addition, we did not perform any Morphometry-based features (such as gray matter and white matter probability maps); these features provided the best performance for their R-TLE group. The purpose of this paper was to explore classification using quantitative imaging (T1 maps, T2 maps, DTI). We plan to investigate the added benefit of adding Morphometry to these features as we believe that this could improve our results. Otherwise, in agreement with these findings, we observed that MD data is more useful for discrimination of TLE patients than FA data, and we also observed poor classification results when looking at T2 data only as they did. Similarly, Keihaninejad et al. [13] stated an accuracy of 86% for the identification of TLE patients where ours is 88.9%, with the caveat that this number refers to MR-negative patients (MRI without clinical findings), therefore more difficult to classify.

It is well known that SVM performance is highly dependent on the quality of the training set. This is a common concern when dealing with biological data characterized by a large number of features and a small number of observations like ours. We have followed several steps to address this limitation and mitigate its effect.

On the one hand, it is clear that the number of features has a direct impact on classification error [43]. Feature sets with cardinalities above the intrinsic dimension of the problem can lead to overfitting effects because the classifier can produce decision boundaries that follow the sample points too closely [44]. This effect is evident in our dataset. Fig. 12 shows that the generalization ability of the classifier suffers as the cardinality of the feature set increases. A simple method to estimate the ideal cardinality is provided (4.2) and the experiments corroborated optimal results in regions around this estimation.

On the other hand, a small sample size makes necessary the systematic use of cross-validation to avoid overfitting. It is well known that cross-validation reduces the error bias at the expense of increasing the error variance. We believe that we can obtain a better bias-variance trade-off by acquiring more data and reducing the amount of cross-validation. Also, a larger dataset can contribute to SVM parameter optimization, boosting the comparison between linear and non-linear models.

Another limitation in our work is the sensitivity for the detection of R-TLE patients. A larger dataset could potentially improve feature selection leading to better classification rates. Nonetheless, there is evidence suggesting that the R-TLE group is very heterogeneous in terms of MRI abnormalities [45,46] and the low sensitivity of our results could be associated with that heterogeneity. To validate this, a larger dataset would allow the R-TLE group to be subdivided into specific pathologies (i.e. MTS, non-MTS, FCD) for classification analyses.

@&#CONCLUSION@&#

This paper describes a novel approach for the detection of TLE patients using feature selection and support vector machine methods. The novelty of this work consists of the use of multi-parametric quantitative imaging, the definition of a measurement of regional asymmetry using a non-parametric statistical test and the evaluation and validation of the optimal cardinality of the problem. The main contribution of this paper comprises the evaluation of key factors influencing classification performance, namely the feature selection method, the possibility of a further dimensionality reduction step with the use of PCA, and the type of support vector machine for this type of data. Then, relevant features are identified and their clinical significance is addressed.

Our results demonstrate that the identification of TLE subjects based on quantitative MR images is possible. In particular, DTI derived features seemed to be more effective than T2 features, which in general performed sub-optimally. In all experiments, good accuracies were attainable. However, the identification of R-TLE patients proved to be a difficult problem and in this case, the multi-parametric approach proved to be slightly better (88.9% accuracy) than the classification on individual feature spaces. The subsequent feature analysis confirmed that the key ROIs for patient identification do indeed belong to the temporal lobe. Their relevance in TLE has been indicated by clinical findings and similar research studies. These results reflect the sensitivity of quantitative imaging and the utility of the presented method towards the detection of TLE.

Classification models including PCA transformation after feature selection in general provided better results than the non-PCA models. Among the 8 classification models evaluated, the anova-pca-linear model demonstrated the best balance between classification performance and computation time, both key elements in the analysis large datasets. Although SVM behaves coherently in small sample scenarios, a larger patient sample would allow the amount of internal cross-validation and sub-sampling to be reduced. In machine learning terms, this could lead to a better balance between bias and variance.

The authors do not have any conflict of interest to disclose.

@&#ACKNOWLEDGMENTS@&#

This work is supported by The Canadian Institutes for Health Research, Grant MOP 184807, Canadian Foundation for Innovation, Leading Edge Fund 
                  20994, and the CIHR Post-Doctoral Research Fellowship 
                  276108 (A. R. Khan).

@&#REFERENCES@&#

