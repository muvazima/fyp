@&#MAIN-TITLE@&#Predicting associated statutes for legal problems

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           In the legal domain, this method for statute prediction is a new research topic.


                        
                        
                           
                           We predict relevant statutes for the problem described by everyday vocabulary.


                        
                        
                           
                           The gap between lay terms and legal terms was remedied without using a synopsis.


                        
                        
                           
                           Employing the Normalized Google Distance, SVM and Apriori algorithms into TPP.


                        
                        
                           
                           The result shows the performance of TPP is accurately and effectively.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Text mining

Statute

Criminal judgment

Normalized Google Distance (NGD)

Support vector machines (SVM)

Apriori algorithm

@&#ABSTRACT@&#


               
               
                  Applying text mining techniques to legal issues has been an emerging research topic in recent years. Although a few previous studies focused on assisting professionals in the retrieval of related legal documents, to our knowledge, no previous studies could provide relevant statutes to the general public using problem statements. In this work, we design a text mining based method, the three-phase prediction (TPP) algorithm, which allows the general public to use everyday vocabulary to describe their problems and find pertinent statutes for their cases. The experimental results indicate that our approach can help the general public, who are not familiar with professional legal terms, to acquire relevant statutes more accurately and effectively.
               
            

@&#INTRODUCTION@&#

The law represents social norms, which protect civilian rights and maintain social order. To achieve these objectives, the legislature enacts legal provisions preserving people’s rights to life, property, and so on. As an increasing number of individuals’ rights have been violated, more and more litigation has occurred.

When people have legal issues, it is critical to know which statutes are involved. Usually, because of a lack of legal knowledge, they may seek help from legal experts, such as attorneys, or from automatic systems. Since legal consultation is very costly, automatic systems are a much more affordable form of legal support. They involve utilizing search engines on the Internet or searching legal databases, such as Westlaw International (2013) and LexisNexis (2013). Although these automatic systems provide query methods, users cannot obtain pertinent statutes through simple case statements. This motivated us to propose a new approach that will help laypeople obtain relevant statutes by simply stating their problem or case using daily vocabulary, and without the help of legal experts. In Fig. 1
                     , we show the proposed statute retrieval approach.

The purpose of this research is to provide a statute retrieval method that will help people deal with their legal problems more effectively. It can be helpful to at least two types of people. First, for specialists, this method can reduce workloads and be used as a reference when dealing with legal cases. As the sheer number of legal cases increase, legal experts need to expend more time and energy in their work. This research provides an aid for efficiently processing cases. Second, for laypeople, this approach can reduce searching and consultation needs. When laypeople have legal issues, it is difficult for them to acquire related statutes though existing automatic systems because of their insufficient knowledge of professional legal terms. This results in incorrect search results and lengthens the searching process. With the help of our approach, this insufficient knowledge problem can be alleviated.

In recent years, text mining research has gotten more and more attention. Basically, text mining is the procedure of uncovering salient features and information from textual data. Since most human knowledge is stored in text, abundant text mining applications and methods have been developed. Examples of these text mining applications include patent retrieval (Chen & Chiu, 2011; Tikk, Biró, & Törcsvári, 2007), e-mail security (Bergholz et al., 2010; Wei, Chen, & Cheng, 2008), news categorization (Calvo, 2001; Zheng, Milios, & Watters, 2002), authorship identification (Stamatatos, 2009; Zheng, Li, Chen, & Huang, 2006), scientific document retrieval (Kaur, Yusof, Boursier, & Ogier, 2010), document sentiment analysis (Li & Wu, 2010; Schumaker, Zhang, Huang, & Chen, 2012), document summarization (Goldstein, Mittal, Carbonell, & Kantrowitz, 2000; Li, Du, & Shen, 2013; Wang, Zhu, Li, & Gong, 2009), online advertisement recommendations (Thomaidou & Vazirgiannis, 2011; Wang, Wang, Duan, Tian, & Lu, 2011), search engines (Kawai, Jatowt, Tanaka, Kunieda, & Yamada, 2011; Yin, 2007), etc.

Text mining has been applied in various areas. Although a few past studies applied text mining techniques to the legal domain (Chen & Chi, 2010; Chou & Hsing, 2010; Conrad & Schilder, 2007; Moens, 2001), all of them focused only on helping professional users retrieve or classify legal documents. None of them considered how to help laypeople retrieve relevant statutes from a case statement using daily customary terms. Therefore, this research aims to develop a framework for statute prediction that will remedy this problem. This framework is built with judgments and statutes. Judgments are included in the framework because they contain the facts of the crime and the cited statutes from the judge’s adjudication. From them, we can find the connections between the problem and the cited statutes. In turn, these connections help us to determine the most relevant statutes with respect to the user’s problem.

The prediction method process is shown in Figs. 2 and 3
                     
                     , Batch and Online, respectively. In the Batch process, three outputs are generated to be used in the Online process. The first output is a classification model that classifies cases to statutes, and is produced by adopting a SVM (support vector machine) classifier. In the second output, all statutes are represented as statute vectors. The last output is a set of association rules, which show what statutes frequently occur together, and is generated from the training collection of judgments. In the Online process, the classification model is adopted to acquire the prediction of top k
                     1 statutes for the user query. Then, the NGD (Normalized Google Distance) method (Cilibrasi & Vitanyi, 2007; Evangelista & Kjos-Hanssen, 2006) is used to perform terms transformation between the statutes and user query, and the top k
                     2 most similar statutes are selected. Finally, by applying associative statute rules to the top k
                     2 statutes, the statute weight computation metric is defined, so as to obtain the most relevant statutes for the user query.

The advantages of our approach are that (1) ordinary users can express their cases using daily vocabulary, (2) a bridge is created between laypeople and legal statutes, and (3) the most pertinent statutes are recommended to users. This work acquires relevant statutes by developing a three-staged algorithm. In the first stage, we utilize the multi-label SVM text classifier to classify the cases into k
                     1 statutes. Then, from these k
                     1 statutes, the second stage selects the most similar k
                     2 statutes, with respect to the user query, by employing the semantic relatedness measure (i.e., Normalized Google Distance method). Finally, from these k
                     2 statutes, we select the final set of statutes by considering the associations among statutes. The contributions of this research are as follows.
                        
                           1.
                           It is motivated by a real-world societal needs arose from general public. To our knowledge, this innovative approach to statute prediction in the legal domain has not been attempted in any previous research.

It proposes an innovative approach, called TPP (Three Phase Prediction), to predict relevant statutes for the problem described by user. The core of the approach is to remedy the gap between lay terms and legal terms without using a synopsis.

An evaluation metric (i.e., coverage) was adopted to confirm the performance of the proposed TPP approach. Experimental results show that it performs accurately and effectively.

The rest of this paper is organized as follows. The relevant literature is first reviewed and described in Section 2. The research design is then introduced in Section 3. In Section 4, we discuss the experiment results and evaluations. Finally, conclusions and future directions are presented in Section 5.

@&#LITERATURE REVIEW@&#

@&#BACKGROUND@&#

The legal system in Taiwan is based upon the written law, which is enacted by the legislature or the congress, and who also develop a variety of codes and regulations. In court cases, the judge’s sentence must be in accordance with the stipulations and statutes of the law. Furthermore, to determine a clear sentence, the judge must interpret the statutes and the factual circumstances of the case. The statutes and judgments play an important role in the judge’s verdict in governing the society.

A judgment is the final decision by a court in a lawsuit to resolve the controversial issues and terminate the lawsuit. The court may also make a range of court orders, such as imposing a sentence upon a guilty defendant in a criminal matter, or providing a remedy for the plaintiff in a civil matter. In addition, a judgment also signifies the end of the court’s jurisdiction in the case. The form of the judgment is generally revealed in a combination of segments; some are necessary, and others are optional, arranged in a fixed or partially fixed order. A judgment consists of the file number, the accused, the counsel, the cause of action, the main body of a court verdict, the facts and reasons, the cited statutes, the date of judgment, and the judge.

In our study, we utilize a collection of judgments as training documents. Two relevant parts of a judgment are employed for processing, the facts and the cited statutes. Representative keywords for a judgment are extracted from the facts, while the cited statutes can be seen as a classification label for the judgment. Both the keywords and the cited statutes are used to train a text classifier. Additionally, a statute’s contents and its relationship are two important traits needed to accurately acquire relevant statutes.

Text mining is an analysis process that discovers hidden features and extracts sensitive information from the sheer volume of documents for further processing. It uses techniques from information retrieval, information extraction (IR), as well as natural language processing (NLP), and connects them using data mining, machine learning, and statistics.

In general, an IR system is composed of three components: Documents, Queries and Matching/Ranking functions. Most prior studies are based on the idea that text documents can be represented as vectors in multi-dimensional space, called the Vector Space Model (VSM) (Salton, Wong, & Yang, 1975). In VSM, documents and queries are represented as vectors of terms and its accuracy and performance relies on the selected vector base. To determine the matching function, the similarity between queries and documents can be computed using the distance or correlation between the corresponding vectors (Baeza-Yates & Ribeiro-Neto, 1999; Hotho, Nürnberger, & Paaß, 2005; Salton et al., 1975). Some examples of these methods include Euclidean distance, cosine measure, and Pearson coefficient. According to Trappey and Trappey (2008), several weighting schemes have been proposed to compute the terms weights, such as TF (Baeza-Yates & Ribeiro-Neto, 1999), TF-IDF (Salton, Allan, & Buckley, 1994; Salton & McGill, 1983), entropy (Hotho et al., 2005; Lochbaum & Streeter, 1989), and chi-squares (χ
                        2) (Feldman & Sanger, 2007; Li, Luo, & Chung, 2008). Among these schemes, TF-IDF is a welcome term weight scheme introduced in VSM. Various evaluation studies (Salton, 1989; Salton & Buckley, 1988) have shown that VSM was one of the most successful models and most existing information retrieval systems were designed based on it.

There are multiple IR systems that have been developed for utilizing and facilitating in different fields. Every IR system has its own traits, architecture and limitations, but contributes to specific or open domains. In the late 1960s, the SMART information retrieval system was implemented and presented many important concepts in research, including thevector space model, relevance feedback, and Rocchio classification (Buckley, 1985). Over the past few years, a wide variety of IR systems were proposed, including search engines (e.g., Google and Yahoo! Search), digital library (e.g., Digital Public Library of America), and information filtering (e.g., Spam filter), etc. In particular, a flexible and functional API Lucene (lucene.apache.org) was introduced, which is a Java-based indexing and searching engine library and a technology appropriate for applications that require full-text search. It provides several features with many query types (e.g., Phrase queries, wildcard queries and more), fielded searching, multiple-index searching with merged results and two ranking models: VSM and BM25 (Robertson & Zaragoza, 2009).

The main concern of Question–answering (QA) system is that it can automatically answer accurate questions posed by humans in anatural expression of queries. Typically, QA systems can be divided into two categories: open-domain and domain-specific. An open-domain QA system aims at returning an answer to a user’s question with short texts rather than a list of relevant documents. START (start.csail.mit.edu) is one of the earliest web-based systems that can be publicly-accessible since 1993 (Katz, 1997) and it plays a notable role in the evolution of QA systems. In 1999, Text REtrieval Conference (TREC) commenced to supply a standard QA evaluation track from a large collection of documents. A famous open-domain QA system, IBM’s Watson system (Ferrucci et al., 2010), is developed by the IBM DeepQA research team. The main design idea behind Watson is that it synthesized information retrieval, natural language processing, knowledge representation and reasoning, machine learning, and computer–human interfaces. The information resources include encyclopedias, dictionaries, thesauri, newswire articles, and literary works, etc. Besides, the sources are identified and collected in content-acquisition process, which include databases, taxonomies, and ontologies, such as dbPedia (dbpedia.org), WordNet (Miller, 1995), and the Yago (www.mpi-inf.mpg.de/yago-naga/yago/) ontology. Although Watson provides accurate answers to questions, it is not available to the public.

For domain-specific question answering, there have been fewer recent works developed, like a medical domain QA system, MedQA (Lee et al., 2006), which employs supervised machine learning approach to perform question classification based on an evidence taxonomy built up by physicians. In engineering education domain, Diekema, Yilmazel, and Liddy (2004) developed the Knowledge Acquistion and Access System (KAAS) QA system using a user-oriented approach in a collaborative learning environment. L’opez-Moreno et al. (2007) proposed an analysis study of the problems and challenges of QA systems in academic domain. Despite those promising QA systems proposed recently, however, to our best knowledge, there is still no well-built QA system in legal domain.

Text mining has been successfully applied in many diverse areas, with each of the applications exhibiting specific traits. When text appears in new document types, it usually leads to novel text mining applications. For example, text mining methods can be used in news texts to determine the news category (Calvo, 2001; Zheng et al., 2002). For patent documents, patent retrieval methods can be developed according to text similarities (Chen & Chiu, 2011; Tikk et al., 2007). In e-mail, we can determine if the mail is spam by analyzing its text (Bergholz et al., 2010; Wei et al., 2008). For scientific documents, intelligent search methods can be designed by comparing text similarities and other attributes (Kaur et al., 2010). In a commentary, text mining methods can help discover the main themes, the authors’ attitudes, and sentiments (Hsu, Chen, Lin, Hsieh, & Shih, 2012; Li & Wu, 2010; Reyes, Rosso, & Buscaldi, 2012; Schumaker et al., 2012).

Additionally, text mining methods can be used to guess the identities of anonymous authors (Stamatatos, 2009; Zheng et al., 2006). They can also be used to automatically generate an abstract or summary of long documents based on sentence features (Goldstein et al., 2000; Li et al., 2013; Wang et al., 2009). Furthermore, commercial web sites can use text mining methods to recommend appropriate advertisements (Thomaidou & Vazirgiannis, 2011; Wang et al., 2011). As one of the most important contributions, the modern search engine was developed by utilizing text mining techniques (Kawai et al., 2011; Yin, 2007). In the last few years, QA has received more attention. Community-based QA (CQA) is an emerged focus of prior studies for information seeking online, and it retrieves the most appropriate answers by segmenting multi-sentence user questions (Wang, Ming, Hu, & Tat-Seng Chua, 2010). For question retrieval, the analysis of many questions was not only based on factual knowledge, but incorporated sentiment analysis of user intent (Chen, Zhang, & Levene, 2013). Multimedia content analysis approach was proposed to help text QA acquire relevant answers by adopting multimedia information such as image and video (Nie, Wang, Zha, Li, & Chua, 2011). It is impossible to list every text mining application, but the above discussion should illustrate the impact text mining has had in countless situations.

Text mining in the legal domain has been an emerging research topic in recent years. So far, only several studies have been done on this topic. Moens (2001) gave an overview of text mining methods and discussed their potential to help improve legal document retrieval. Besides, Moens (2005) also proposed several XML retrieval models which exploit the structured and unstructured legislative document information to a query. EgoIR (Gomez-Perez, Ortiz-Rodriguez, & Villazon-Terrazas, 2007) is a system that provides an efficient way to retrieve legal information from e-Government documents based on Legal Ontology. Conrad and Schilder (2007) presented an opinion mining application on legal web blogs evaluated by sentiment analysis. In Chen and Chi (2010), their aim was to retrieve the most similar historical judgments for the prosecutor using the police’s criminal investigation documents. Chou and Hsing (2010) developed a legal document classification, clustering, and search methodology based on neural network technology, helping law enforcement to manage criminal written judgments. Chen, Liu and Ho (2013) introduced an approach to assist the general public in retrieving the most relevant judgments using ordinary terminology or statements as queries.

All of the above studies were designed to support users in retrieving or managing related legal documents. They did not, however, consider providing relevant statutes with respect to legal problems. This deficiency motivated us to propose a statute prediction system that is custom-designed for the general public.

Since the criminal code includes many kinds of crime, and the judgment for a specific offense is usually dependent on the articles and the judge’s perspective, there has been no absolute standard for court decisions. Therefore, designing automatic methods to determine the exact statutes for a target judgment has been extremely difficult. Intuitively, a legal document can be regarded as a text document. However, several differences exist between legal documents and normal documents.

As Table 1
                      illustrates, there are four major differences. These four unique characteristics of legal judgments suggest a need to develop a brand new approach that can address such differences.
                        
                           (1)
                           The terms used by laypeople differ from the ones that appear in legal documents, such as judgments and statutes. Problems arise when we search for relevant statutes using laypeople’s queries due to keyword differences.

In statute prediction, a statute is regarded as a label in the classification model. Since each judgment has at least one cited statute, judgments have multi-labels.

In traditional document classification, a label is a tag without content. In statute prediction, however, a statute is a paragraph of text or a sentence. In other words, a label is no longer just a tag, but content-rich text.

In traditional document classification, labels are independent of each other. In judgments, however, some statutes are more likely to appear together than others. Association relations may exist between statutes.

In order to more accurately determine statutes, our method must take into account these four characteristics. Therefore, we propose a three-phase prediction (TPP) approach that can be used to improve the accuracy of traditional text classifiers for judgments. There are two collections of criminal judgments, a training collection and a test collection, where the TPP approach can perform automated statute prediction. Before classifying the judgments, a text preprocessing procedure is performed on all documents in the training collection. In the preprocessing procedure, we conduct CKIP (2013) for segmentation, Chinese stop word elimination, and POS filtering operations. Most studies have traditionally selected nouns to represent documents. In criminal case documents, however, other POS tags are also meaningful such as adjectives (共同, together), nouns (罪犯, criminal), and verbs (破壞, damage). Therefore, we revised the traditional POS filtering rule to keep nouns, verbs, and adjectives as candidate terms.

For simplicity, we introduce our method based on the three phases of the online process; we do not discuss the batch process separately. However, whenever needed in discussing the online process, we will explain the procedures from the batch process.

The TPP approach is depicted in Fig. 4
                     . The design framework is separated into three phases: (1) select the top k
                     1 statutes, (2) select the top k
                     2 statutes, and (3) select the final predicted statutes. Fig. 4 shows which characteristics are addressed in each phase. These three phases are described and explained in detail below.

The SVM algorithm is a well-known multi-label classification algorithm. It has several specific advantages in processing nonlinear and high dimensional model identification problems, and small data samples. When classifying, it also minimizes ranking loss and generates a better classification model for prediction (Tsoumakas, Katakis, & Vlahavas, 2010).

In this phase, the SVM classifier is applied to the training collection of judgments to generate a classification model. This classification model is used to predict statutes for the input query. Fig. 5
                         shows the process of classification model generation. After preprocessing, feature selection is necessary to reduce the document dimensions; then, the Vector Space Document (VSD) model is used to represent the documents.

This research attempts to generate representative vectors for each criminal case in the gathered document set. Throughout this paper, each document (i.e., criminal judgment) is denoted by term set {t
                        1, t
                        2,
                        …
                        ,
                        tm
                        }, where ti
                         is a term and m is the number of terms that occur in the document. The relevance of each ti
                         in document dj
                         is given a weight, and we use the following weighting schemes for assigning weights to terms: tf(ti
                        
                        ,
                        
                           j
                        )
                        ×
                        
                        idf(ti
                        ), w(ti
                        ,
                        dj
                        ).
                           
                              
                                 
                                    
                                       
                                       
                                          
                                             tf
                                             (
                                             
                                                
                                                   t
                                                
                                                
                                                   i
                                                   ,
                                                   j
                                                
                                             
                                             )
                                             =
                                             freq
                                             (
                                             
                                                
                                                   t
                                                
                                                
                                                   i
                                                   ,
                                                   j
                                                
                                             
                                             )
                                             ⇒
                                             
                                             the normalized frequency of term
                                             
                                             
                                                
                                                   t
                                                
                                                
                                                   i
                                                
                                             
                                             
                                             in document
                                             
                                             
                                                
                                                   d
                                                
                                                
                                                   j
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             idf
                                             (
                                             
                                                
                                                   t
                                                
                                                
                                                   i
                                                
                                             
                                             )
                                             =
                                             log
                                             (
                                             n
                                             /
                                             
                                                
                                                   n
                                                
                                                
                                                   i
                                                
                                             
                                             )
                                             ⇒
                                             n
                                             :
                                             
                                             number of documents;
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             
                                             
                                             
                                             
                                             
                                             
                                                
                                                   n
                                                
                                                
                                                   i
                                                
                                             
                                             :
                                             
                                             frequency of term
                                             
                                             
                                                
                                                   t
                                                
                                                
                                                   i
                                                
                                             
                                             
                                             in document collection
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             w
                                             (
                                             
                                                
                                                   t
                                                
                                                
                                                   i
                                                
                                             
                                             ,
                                             
                                                
                                                   d
                                                
                                                
                                                   j
                                                
                                             
                                             )
                                             =
                                             tf
                                             (
                                             
                                                
                                                   t
                                                
                                                
                                                   i
                                                
                                             
                                             ,
                                             j
                                             )
                                             ×
                                             idf
                                             (
                                             
                                                
                                                   t
                                                
                                                
                                                   i
                                                
                                             
                                             )
                                             ⇒
                                             the weight of term
                                             
                                             
                                                
                                                   t
                                                
                                                
                                                   i
                                                
                                             
                                             
                                             in document
                                             
                                             
                                                
                                                   d
                                                
                                                
                                                   j
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Since not all features in a document collection are helpful for classification, features that are less discriminative must be eliminated in the classification process. Feature selection is a necessary step in reducing document dimensions. Dimensionality reduction has been widely explored in single-label data, but these feature selection methods (Liu & Yu, 2005; Ribeiro, Neto, & Prudêncio, 2008; Rogati & Yang, 2002) cannot be directly applied to multi-label data. Therefore, a multi-label entropy method (Clare & King, 2001) is employed, which calculates the entropy of each term ti
                         across statutes as follows:
                           
                              
                                 Entropy
                                 (
                                 
                                    
                                       t
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 =
                                 -
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          k
                                          =
                                          1
                                       
                                       
                                          r
                                       
                                    
                                 
                                 (
                                 (
                                 p
                                 (
                                 
                                    
                                       S
                                    
                                    
                                       k
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                t
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                 
                                 )
                                 log
                                 p
                                 (
                                 
                                    
                                       S
                                    
                                    
                                       k
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                t
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                 
                                 )
                                 )
                                 +
                                 (
                                 q
                                 (
                                 
                                    
                                       S
                                    
                                    
                                       k
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                t
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                 
                                 )
                                 log
                                 q
                                 (
                                 
                                    
                                       S
                                    
                                    
                                       k
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                t
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                 
                                 )
                                 )
                                 )
                              
                           
                        
                        
                           
                              p
                              (
                              
                                 
                                    S
                                 
                                 
                                    k
                                 
                              
                              |
                              
                                 
                                    t
                                 
                                 
                                    i
                                 
                              
                              )
                           
                         denotes the probability of statute Sk
                         given that term ti
                         appears; 
                           
                              q
                              (
                              
                                 
                                    S
                                 
                                 
                                    k
                                 
                              
                              |
                              
                                 
                                    t
                                 
                                 
                                    i
                                 
                              
                              )
                              =
                              1
                              -
                              p
                              (
                              
                                 
                                    S
                                 
                                 
                                    k
                                 
                              
                              |
                              
                                 
                                    t
                                 
                                 
                                    i
                                 
                              
                              )
                           
                         denotes the probability of statute Sk
                         given that term ti
                         is absent. The smaller the term entropy value, the greater its distinguishing ability. This is because if a given term ti
                         has a small entropy value, then term ti
                         is concentrated in fewer specific statutes, instead of being widely distributed over numerous statutes.

After feature selection, the training judgments are transformed into vector space documents (VSDs) according to TF-IDF. Additionally, each VSD is assigned a set of labels, where each label is a statute cited in the judgment. Let BASE denote the set of judgment terms, i.e., all terms selected from the training judgments, used as the vector base in Phase 1. Then, these VSDs, as well as their labels, are analyzed by the SVM algorithm and the classification model is generated. After the classification model is built, the SVM prediction is applied to the user query, as shown in Fig. 6
                        , and the probabilities of all statutes for the user query are produced. All the statutes are sorted by probabilities, and the top k
                        1 statutes are selected as the output.

To apply the classification model, in this phase, the terms in the user query must be transformed into the judgment terms in BASE. There are a couple difficulties, however, involved in this process. First, a query term may be related to multiple judgment terms, each of which is associated with a different degree of strength. Secondly, it is difficult, if not impossible, to set up a dictionary or an ontology model that can store all these mapping relations.

To overcome these difficulties, Normalized Google Distance (NGD) is employed in this phase. It utilizes a well-known search engine, Google, which can return aggregate page-count estimates for a query or keyword. Normalized Google Distance (Cilibrasi & Vitanyi, 2007; Evangelista & Kjos-Hanssen, 2006) relies on the number of web pages found using a Google search engine that contains a given word; the page count is used to correlate one word or phrase with another word’s meaning. In our study, we applied the NGD method to terms transformation.


                        Fig. 7
                         demonstrates the process of query terms transformation. The preprocessing procedure is used to segment a user query into multiple terms. In order to transform query terms into a judgment term set (i.e., BASE), we utilize the NGD formula, which is defined as follows:
                           
                              
                                 NGD
                                 (
                                 
                                    
                                       t
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       u
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          max
                                       
                                       {
                                       log
                                       f
                                       (
                                       
                                          
                                             t
                                          
                                          
                                             i
                                          
                                       
                                       )
                                       ,
                                       log
                                       f
                                       (
                                       
                                          
                                             u
                                          
                                          
                                             j
                                          
                                       
                                       )
                                       }
                                       -
                                       log
                                       f
                                       (
                                       
                                          
                                             t
                                          
                                          
                                             i
                                          
                                       
                                       ,
                                       
                                          
                                             u
                                          
                                          
                                             j
                                          
                                       
                                       )
                                    
                                    
                                       log
                                       M
                                       -
                                       
                                          min
                                       
                                       {
                                       log
                                       f
                                       (
                                       
                                          
                                             t
                                          
                                          
                                             i
                                          
                                       
                                       )
                                       ,
                                       log
                                       f
                                       (
                                       
                                          
                                             u
                                          
                                          
                                             j
                                          
                                       
                                       )
                                       }
                                    
                                 
                              
                           
                        
                        M denotes the total number of web pages indexed by Google Search, f(ti
                        ) denotes the number of web pages containing term ti
                        , f(uj
                        ) denotes the number of web pages containing judgment term uj
                        , and f(ti
                        ,
                        uj
                        ) denotes the number of web pages containing both term ti
                         and judgment term uj
                        . The value of the NGD function indicates the degree of similarity between term ti
                         and judgment term uj
                         using a zero to one scale. A distance value of zero indicates that two terms are practically the same; two independent terms have a distance value of one.

Furthermore, we can associate a query term ti
                         with k judgment terms u
                        1, u
                        2,
                        …
                        ,
                        uk
                         using different degrees. The vector gi
                        
                        =(gi
                        
                        ,1,…,
                        gi
                        
                        ,
                        
                           k
                        ) is a k-dimensional vector, where gi
                        
                        ,
                        
                           j
                         represents the similarity between term ti
                         and judgment term uj
                        . The definition of gij
                         is as follows:
                           
                              
                                 
                                    
                                       g
                                    
                                    
                                       i
                                       ,
                                       j
                                    
                                 
                                 =
                                 1
                                 -
                                 NGD
                                 (
                                 
                                    
                                       t
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       u
                                    
                                    
                                       j
                                    
                                 
                                 )
                              
                           
                        The higher the value of gi
                        
                        ,
                        
                           j
                        , the greater the similarity between term ti
                         and judgment term uj
                        . Based on the computed values, the following method is proposed to transform a query term into multiple judgment terms. This method is divided into four steps.


                        Step1: Calculate the similarity value gi
                        
                        ,
                        
                           j
                         between query terms ti
                         and each of the k judgment terms uj
                        .


                        Step2: Let pcti
                        
                        ,
                        
                           j
                         be the proportion of gi
                        
                        ,
                        
                           j
                         and the sum of gi
                        
                        ,
                        
                           j
                         over all judgment terms. Additionally, let lwi
                        
                        ,
                        
                           q
                         be the weight of term ti
                         in query q. The formulas of pcti
                        
                        ,
                        
                           j
                         and lwi
                        
                        ,
                        
                           q
                         are defined as follows:
                           
                              
                                 
                                    
                                       pct
                                    
                                    
                                       i
                                       ,
                                       j
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             g
                                          
                                          
                                             i
                                             ,
                                             j
                                          
                                       
                                    
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             g
                                          
                                          
                                             i
                                             ,
                                             j
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    
                                       lw
                                    
                                    
                                       i
                                       ,
                                       q
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             freq
                                          
                                          
                                             i
                                             ,
                                             q
                                          
                                       
                                    
                                    
                                       
                                          
                                             max
                                          
                                          
                                             l
                                          
                                       
                                       
                                          
                                             freq
                                          
                                          
                                             l
                                             ,
                                             q
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    max
                                 
                                 
                                    l
                                 
                              
                              
                                 
                                    freq
                                 
                                 
                                    l
                                    ,
                                    q
                                 
                              
                           
                         is computed over all terms that occur within query q.


                        Step3: Distribute the weight of lwi
                        
                        ,
                        
                           q
                         to k judgment terms uj
                         according to pcti
                        
                        ,
                        
                           j
                        
                        , which is defined as twi
                        
                        ,
                        
                           j
                        .
                           
                              
                                 
                                    
                                       tw
                                    
                                    
                                       i
                                       ,
                                       j
                                    
                                 
                                 =
                                 
                                    
                                       lw
                                    
                                    
                                       i
                                       ,
                                       q
                                    
                                 
                                 ×
                                 
                                    
                                       pct
                                    
                                    
                                       i
                                       ,
                                       j
                                    
                                 
                              
                           
                        
                     


                        Step4: Sum up the weight twi
                        
                        ,
                        
                           j
                         of each term.


                        Figs. 8 and 9
                        
                         show an example of how to distribute the weight of a query term to judgment terms and how to add together the weights of each judgment term. Assume that the similarity values of query term T
                        1 to terms U
                        1, U
                        3, and U
                        4 are 0.7, 0.2, and 0.4, respectively, while those of T
                        2 to terms U
                        2, U
                        3, and U
                        4 are 0.8, 0.5, and 0.3, respectively. In step 2, we found that the percentages of U
                        1, U
                        3, and U
                        4 with respect to T
                        1 are 0.54, 0.15, and 0.31, respectively, while those of U
                        2, U
                        3, and U
                        4 to T
                        2 are 0.5, 0.31, and 0.19, respectively. Therefore, the weights of term T
                        1 distributed to terms U
                        1, U
                        3, and U
                        4 are 0.54, 0.15, and 0.31, respectively, while the weights of term T
                        2 distributed to terms U
                        2, U
                        3, and U
                        4 are 0.3, 0.186, and 0.114, respectively. Finally, the weights of judgment terms U
                        1, U
                        2, U
                        3, and U
                        4 are 0.54, 0.3, 0.336, and 0.424, respectively.

Using the statute probabilities from the first phase, we identified the top k
                        1 statutes for further selection. A statute is a content-rich text that consists of prerequisites and statute terms. In order to calculate the weights of statute terms for the top k
                        1 statutes, the following formula, called the TF-ISF (term frequency-inverse statute frequency) formula, is used. The relevance of each statute term li
                         in statute sj
                         is measured by a weight, using the following weighting schemes for assigning weights to terms: TF(li
                        
                        ,
                        
                           j
                        )×
                        ISF(li
                        ), w(li
                        ,
                        sj
                        ).
                           
                              
                                 
                                    
                                       
                                       
                                          
                                             TF
                                             (
                                             
                                                
                                                   l
                                                
                                                
                                                   i
                                                   ,
                                                   j
                                                
                                             
                                             )
                                             =
                                             freq
                                             (
                                             
                                                
                                                   l
                                                
                                                
                                                   i
                                                   ,
                                                   j
                                                
                                             
                                             )
                                             ⇒
                                             the normalized frequency of statute term
                                             
                                             
                                                
                                                   l
                                                
                                                
                                                   i
                                                
                                             
                                             
                                             in statute
                                             
                                             
                                                
                                                   s
                                                
                                                
                                                   j
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             ISF
                                             (
                                             
                                                
                                                   l
                                                
                                                
                                                   i
                                                
                                             
                                             )
                                             =
                                             
                                                
                                                   log
                                                
                                                
                                                   2
                                                
                                             
                                             (
                                             N
                                             /
                                             ni
                                             )
                                             ⇒
                                             N
                                             :
                                             
                                             number of statutes;
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             
                                             
                                             
                                             
                                             
                                             
                                             
                                                
                                                   n
                                                
                                                
                                                   i
                                                
                                             
                                             :
                                             
                                             number of statutes containing
                                             
                                             
                                                
                                                   l
                                                
                                                
                                                   i
                                                
                                             
                                             
                                             in statute collection
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             w
                                             (
                                             
                                                
                                                   l
                                                
                                                
                                                   i
                                                
                                             
                                             ,
                                             
                                                
                                                   s
                                                
                                                
                                                   j
                                                
                                             
                                             )
                                             =
                                             TF
                                             (
                                             
                                                
                                                   l
                                                
                                                
                                                   i
                                                   ,
                                                   j
                                                
                                             
                                             )
                                             ×
                                             ISF
                                             (
                                             
                                                
                                                   l
                                                
                                                
                                                   i
                                                
                                             
                                             )
                                             ⇒
                                             the weight of statute term
                                             
                                             
                                                
                                                   l
                                                
                                                
                                                   i
                                                
                                             
                                             
                                             in statute
                                             
                                             
                                                
                                                   s
                                                
                                                
                                                   j
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

In this phase, the main objective is to transform the terms in the user query into the statute terms in the statute. However, we encounter the same difficulties as when transforming query terms into judgment terms. Therefore, we applied the same NGD transformation method as described in Phase 1 to solve these problems.


                        Fig. 10
                         illustrates the process of transforming query terms into statute terms. First, the preprocessing procedure is used to segment a user query into multiple terms. Then, in order to transform the terms, we utilize the NGD method.

Similarly, by applying the transformation method described in Phase 1, we can transform a vector of r terms in a user query into a list of statute terms and retrieve the most similar k
                        2 statutes from the top k
                        1 statutes. “Closeness” is defined in terms of the cosine similarity metric, which is used to compute the similarity values between a user query and those statutes. Cosine similarity function measures how close user query q is to statute sj
                         on a scale from zero to one. A value of zero indicates complete irrelevance. A higher similarity value implies greater similarity between the user query and those statutes. In this way, we can obtain the top k
                        2 ranking statutes. The cosine similarity function formula is defined as the following equation:
                           
                              
                                 
                                    
                                       Sim
                                    
                                    
                                       cos
                                       ine
                                    
                                 
                                 (
                                 
                                    
                                       s
                                    
                                    
                                       j
                                    
                                 
                                 ,
                                 q
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   s
                                                
                                                
                                                   →
                                                
                                             
                                          
                                          
                                             j
                                          
                                       
                                       ·
                                       
                                          
                                             q
                                          
                                          
                                             →
                                          
                                       
                                    
                                    
                                       |
                                       
                                          
                                             
                                                
                                                   s
                                                
                                                
                                                   →
                                                
                                             
                                          
                                          
                                             j
                                          
                                       
                                       |
                                       ×
                                       |
                                       
                                          
                                             q
                                          
                                          
                                             →
                                          
                                       
                                       |
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             n
                                          
                                       
                                       
                                          
                                             w
                                          
                                          
                                             i
                                             ,
                                             j
                                          
                                       
                                       ×
                                       
                                          
                                             w
                                          
                                          
                                             i
                                             ,
                                             q
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   n
                                                
                                             
                                             
                                                
                                                   w
                                                
                                                
                                                   i
                                                   ,
                                                   j
                                                
                                                
                                                   2
                                                
                                             
                                          
                                       
                                       ×
                                       
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   n
                                                
                                             
                                             
                                                
                                                   w
                                                
                                                
                                                   i
                                                   ,
                                                   q
                                                
                                                
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              0
                              ⩽
                              
                                 
                                    Sim
                                 
                                 
                                    cos
                                    ine
                                 
                              
                              (
                              
                                 
                                    s
                                 
                                 
                                    j
                                 
                              
                              ,
                              q
                              )
                              ⩽
                              1
                           
                        .

After Phase 2, the top k
                        2 statutes are acquired. The Apriori association algorithm finds further association rules among the statutes in order to retrieve more precise statutes. The main purpose is to discover relationships between statutes (i.e., one statute is cited with another statute). This phase is divided into two steps: (1) mine associative statute rules and (2) determine the final predicted statutes.

When mining associative statute rules, a set of association rules is generated from the training collection. The Apriori mining algorithm is employed to discover associative statute rules. Let S
                        ={s
                        1,
                        s
                        2,…,
                        sr
                        } be the statute set in all training data, where variable r is the total number of statutes in the statute set S. The definition of the associative statute rule is as follows:
                           
                              
                                 {
                                 
                                    
                                       s
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                s
                                             
                                             
                                                i
                                             
                                          
                                          ∈
                                          S
                                       
                                    
                                 
                                 }
                                 →
                                 {
                                 
                                    
                                       s
                                    
                                    
                                       j
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                s
                                             
                                             
                                                j
                                             
                                          
                                          ∈
                                          S
                                       
                                    
                                 
                                 }
                                 [
                                 confidence
                                 
                                 %
                                 ]
                              
                           
                        
                        si
                         is one statute, sj
                         is another statute, and 
                           
                              
                                 
                                    s
                                 
                                 
                                    i
                                 
                              
                              ∩
                              
                                 
                                    s
                                 
                                 
                                    j
                                 
                              
                              =
                              ∅
                           
                        . For example, {s
                        1}→{s
                        2} [70%] means if statute s
                        1 is cited in the judgment, then we are 70% confident that s
                        2 is also cited.

A set of associative statute rules can be found using the Apriori mining algorithm. Let wi
                         be the weight of statute si
                        . We set wi
                         as the similarity of si
                        , with respect to the query, obtained in Phase 2. To find the final predicted statutes among the top k
                        2 statutes, the following formula, called the SFW (statute final weight) formula, is used:
                           
                              
                                 
                                    
                                       SFW
                                    
                                    
                                       j
                                    
                                 
                                 =
                                 
                                    
                                       w
                                    
                                    
                                       j
                                    
                                 
                                 +
                                 log
                                 2
                                 M
                                 ×
                                 
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             M
                                          
                                       
                                       
                                          
                                             w
                                          
                                          
                                             i
                                          
                                       
                                       ×
                                       conf
                                       (
                                       i
                                       ,
                                       j
                                       )
                                    
                                    
                                       M
                                    
                                 
                              
                           
                        
                        M denotes the number of statute rules with consequent sj
                        , and conf(i,j) denotes the confidence of an associative statute rule 
                           
                              
                                 
                                    s
                                 
                                 
                                    i
                                 
                              
                              →
                              
                                 
                                    s
                                 
                                 
                                    j
                                 
                              
                           
                        .

In accordance with the SFW formula, the SFW values of statutes are computed and ranked. Then, the top candidate predicted statutes can be selected as the final predicted statutes. Suppose we want to obtain the SFW value of a statute, as shown in Fig. 11
                        . Assume that the weight values of s
                        1, s
                        2, s
                        3, s
                        4, and s
                        5 are 0.6, 0.5, 0.3, 0.4, and 0.7, respectively. Also assume that the confidence of the association rules of s
                        1, s
                        3, s
                        4, and s
                        5 with respect to s
                        2 are 65%, 85%, 75%, and 70%, respectively. The final weights of s
                        1, s
                        3, s
                        4, and s
                        5 with respect to s
                        2 are 0.39, 0.255, 0.3, and 0.49, respectively. In this way, the SFW value of s
                        2 (0.823984) is acquired through the SFW formula.

To evaluate our TPP approach, we conducted experiments on the Chinese criminal judgments stored in the Law and Regulations Retrieving System (Judicial Yuan, 2013). For each judgment, we collected data related to the fact and the cited statutes fields. The fact field describes the criminal facts and processes of the defendants, while the cited statutes are required at the time of the judge’s sentence. The experiment data used in our research were gathered from the ten most common types of crime in 2012, as reported by the Judicial Yuan. In total, the data set contains 1518 different criminal judgments and took about 3months to gather manually. Table 2
                         shows the distribution of criminal judgments over the ten types of crimes.

To test the performance of the TPP approach, we selected 70 examples from metropolitan civil news stories (Udn News Net, 2013) as queries. Udn News Net was selected for the query data because it is a well-known news website in Taiwan, containing various types of news stories related to people’s everyday lives and social events. We sent these 70 queries to legal professionals, including two lawyers and two prosecutors. We expected them to provide at least two and no more than three associated statutes for each query. Based on their suggestions, 20 queries were eliminated because of an insufficient number of recommended related statutes. Most queries contain no more than three pertinent statutes. As a result, in our experiment, 50 queries were used for testing. The length of these 50 query examples varied from 41 to 321 words. The following is an example of a query:

Query: 新竹警方查獲 KTV 羅姓女店員暗中抄下多名客人信用卡號和卡片背面認證碼到網路購物盜刷。 (HsinChu Police seized a female KTV clerk named Luo, who secretly wrote down guests’ credit card numbers and authentication codes to conduct misappropriated online shopping.) From this query, we obtain the following useful terms:

Terms: 查獲 (seize), 暗中 (secretly), 信用卡 (credit card), 網路購物 (online shopping), 盜刷 (misappropriated).

After all the query and training data were collected, text preprocessing tasks were executed, including word segmentation, stop-word elimination, and POS tagging. These outcomes were then treated as the input data for the next step.

Experiments were conducted to examine the performance of the TPP approach. First, preprocessing was executed for the document set. A stop word list containing 7321 words was developed to remove useless words, and 8306 terms were extracted from the document set. The TF-IDF weighting schemes were then used to determine the feature weights. For feature selection, all feature terms in the term list were screened using the multi-label entropy method (Clare & King, 2001). After feature selection, the selected feature sets consisted of 507 features. In the next step, all document vectors were analyzed using the SVM algorithm and the classification model was generated. We chose libSVM (Chang & Lin, 2001), an open source multi-label SVM package, to execute the SVM learning and prediction processes.

To select optimal k
                        1 and k
                        2 values, we used 50 query examples as testing documents. Since the TPP approach has three phases, we must evaluate each phase’s performance. Performance was evaluated according to the statutes’ rankings, which our experts recommended as the output result. In the first phase, the best ranked and the worst ranked experts’ statutes were 2 and 28, respectively. In other words, in the best case, the experts’ statute was ranked second, while in the worst case, it was ranked 28th. Accordingly, we set k
                        1
                        =28. Next, in the second phase, the top 28 ranked statutes for each query were chosen for processing. The statute terms were extracted from these top 28 statutes for each query, such as 放火(set fire), 致人於死(resulted in death), and 財產(property). Every query term was transformed into a different number of statute terms using the NGD transformation method. The TF-ISF weighting schemes were then used to determine the statute term weights. After calculating the similarities between the queries and the statute vectors, we found that the best ranked and the worst ranked experts’ statutes were 2 and 16, respectively. Therefore, we set k
                        2
                        =16. Finally, the third phase’s results are determined according to the selection of the top 16 statutes for each query, and applying the associative statute rules discovered by using the Apriori algorithm included in the Weka Data Mining Package (Witten & Frank, 2011). In total, 1082 rules were produced. Then, we applied the SFW formula to acquire the final predicted statutes. The best and worst ranking statutes among these 50 queries were 1 and 13, respectively. By aggregating the results from these three phases, we see that the associated statutes recommended by legal professionals are covered by selecting the top 13 statutes among all queries with k
                        1
                        =28 and k
                        2
                        =16. We also found that the performance continuously improved from the first phase to the third phase.

To see how k
                        1 and k
                        2 influence the performance of our TPP approach, we had to test various combinations of k
                        1 and k
                        2. We set k
                        1
                        =28 and k
                        2
                        =16 (denoted by TPPBASE) as the baseline combination. Table 3
                         illustrates the combinations of k
                        1 and k
                        2 that we compared with TPPBASE.

In general, the most popular metrics to evaluate the performance in information retrieval are precision and recall. Due to the following reason, our experiments only apply the concept of recall, called coverage in this paper, to evaluate the result. For each query, the number of suitable statutes as selected by the legal professionals is usually not more than three. Since the answer set for each query is very small, the precision rates, which are the number of statutes recommended by experts divided by the number of output statutes, are very low and difficult to articulate their relative performance. Therefore, this work uses coverage to measure the performance. Basically, the coverage is the percentage of experts’ recommended statutes that are included within the top N statutes. The following equation is used to compute the coverage:
                           
                              
                                 Coverage
                                 =
                                 
                                    
                                       number of recommended statutes within the top
                                       
                                       N
                                       
                                       statutes
                                    
                                    
                                       total number of statutes recommended legal professionals
                                    
                                 
                              
                           
                        
                        N denotes the threshold on the number of output statutes.


                           Fig. 12
                            illustrates the coverage of TPPBASE. As can be seen from the graph, the third phase outperforms the first two phases. In the beginning, within the top 3 statutes, the coverage was at 15.9% in the first phase, 34.1% in the second phase, and 50% in the third phase. When N was set to 13, the third phase reached the maximum 100%, while the first phase only reached 63.6%. Coverage reached 100% for all phases, however, when N
                           =28.

In Figs. 13–15
                           
                           
                           , we present the coverage values for combinations with k
                           1
                           =40, 30, and 20, respectively. In general, performance improves when the value of k
                           1 decreases. Fig. 13 clearly shows that by fixing k
                           1
                           =40 and varying the value of k
                           2, a smaller k
                           2 value results in a higher coverage rate.

Similarly, Fig. 14 reveals that by fixing k
                           1
                           =30 and varying the value of k
                           2, a smaller k
                           2 value results in a higher coverage rate. With k
                           2
                           =15, however, it could only reach a peak of 97.7% coverage in the third phase. This is because there was one recommended statute ranked behind the top 15 in the two queries. Consequently, the coverage could not be further improved. Finally, as shown in Fig. 15, we checked the coverage of testing queries by fixing k
                           1
                           =20 and varying the value of k
                           2. As expected, the smaller the value of k
                           2, the greater the coverage in the third phase. This trend changes after N
                           
                           >
                           8, however, when the coverage for a large k
                           2 becomes greater than that of a small k
                           2. The reason is because when N is large, it is easy for the third phase to find recommended statutes from the input statute set. Thus, to improve coverage, we must make the input statute set of the third phase as complete as possible. Since a larger k
                           2 means a larger statute set for phase 3, a larger k
                           2 will result in better coverage than a smaller k
                           2 when N is large.

In this experiment, we tested many combinations of k
                           1 and k
                           2 values to find the optimal combination. Table 4
                            summarizes the results of k
                           1 and k
                           2 using the TPP approach. After comparing these combinations, we find that k
                           1
                           =28 and k
                           2
                           =16 results in the most complete information, since it reaches 100% coverage ratio for the top 13 statutes. On the other hand, although other combinations cannot reach 100% coverage, they are more compact and can reduce user workload when applying the suggested statutes. One of these combinations is (k
                           1
                           =30, k
                           2
                           =15) with N
                           =11, which has a coverage of 97%.

To assess the performance of our custom-designed approach, the comparison methods used a classic TF-IDF scheme to build vectors for testing documents. In accordance with three phases of TPP, we built two groups of algorithms for comparison, where the second group contains NGD transformation while the first group does not. In each group, there are three algorithms, corresponding to executing phase 1, phase 1+phase 2 and phase 1+phase 2+phase 3, respectively. When only executing phase 1, it correspond to executing SVM algorithm only. When executing phase 1+phase 2, it correspond to first executing SVM algorithm and then executing N-nearest neighbor algorithm. When executing all phases, it correspond to first executing SVM algorithm, then executing N-nearest neighbor algorithm and finally executing association mining algorithm. For ease of references, these six algorithms are denoted as: (1) TF-IDF without NGD: Phase 1 (denoted by F1), Phase 1+Phase 2 (denoted by F2) and Phase 1+Phase 2+Phase 3 (denoted by F3); and (2) With NGD: Phase 1 (denoted by F4), Phase 1+Phase 2 (denoted by F5) and Phase 1+Phase 2+Phase 3 (our TPP approach, denoted by F6). In addition, from Table 4, we chose the top 3 combinations of k
                           1 and k
                           2 to evaluate these six algorithms. Table 5
                            shows the performances of various algorithms on statute prediction. Compared with each algorithm in different combination of k
                           1 and k
                           2 for the top 3 statutes, the coverage values of F6 (i.e., TPP) were 52.3, 49.2 and 50.8 respectively. It is obvious that F6 outperforms other algorithms in different combination of k
                           1 and k
                           2. Similarly, when N is between 5 and 10, F6 is also the best of these algorithms. From the results in Table 5, we see that our proposed method is better than all the comparative methods.

Besides, we evaluated the performance of TPP method by comparing results using three state of the art retrieval functions: Cosine similarity, Pearson correlation coefficient and Spearman’s correlation coefficient. Table 6
                            summarizes the results of different methods. Compared to the three state of the art functions, our proposed method TPP contributes to the highest coverage rate when N is between 3 and 10. As we can see, the performance of the TPP method surpasses the other three methods. Therefore, we can conclude that the TPP approach provides an innovative method of statute prediction.

With the increasing amount of social interaction in people’s daily lives, understanding related laws and regulations has become more and more important. Due to a shortage in knowledge and legal background, the general public has found it difficult to navigate automatic law consulting systems. To remedy this gap, this paper proposed an innovative method named TPP (Three Phase Prediction) to assist the general public in the automatic retrieval of associated statutes. The training data in the experiments were historical criminal judgments and statutes taken from the laws and regulations retrieval system of the Judicial Yuan in Taiwan. We selected 13 news stories from metropolitan civil news to use as queries. In addition, we proposed an evaluation with various combinations to choose proper k
                     1 and k
                     2 values for the TPP algorithm. The experimental results demonstrated that TPP can predict pertinent statutes with a coverage rate of 52.3% for the top 3 statutes, 61.4% for the top 5 statutes, and 74.2% for the top 8 statutes. The three phase approach provides a solid framework to develop efficient statute prediction algorithms. This paper marks just the beginning of this research line.

Future works can improve upon the performance of the current study by using data or text mining techniques to increase the accuracy of the first or second phases. They can also work to improve the performance of the third phase, which finds an accurate target from a number of candidates. Moreover, we plan to further extend the system so that it will also provide possible adjudications for the user’s legal problem. For another interesting issue, since similar factual scenarios might appear in many different areas of law, the mediation between factual specifics and a particular area of the law will be a potential future research topic. Finally, the two parameters k
                     1 and k
                     2 are query independent. That is, their values are determined before the query is issued. If their values can be determined dynamically in retrieval time, the retrieval performance will be improved. To this end, our current solution approach must be extended so that it can decide the best values of k
                     1 and k
                     2 based on the characteristics/features of the query before performing the current TPP method.

We use the following query as our example to present our TPP method. Suppose we have ten statues, and we set k
                     1 to 7 in Phase 1, k
                     2 to 5 in Phase 2 and finally retrieve Top 3 statutes in Phase 3. Fig. A1
                      shows the procedures of how TPP approach works in this query.

Query: 新竹警方查獲 KTV 羅姓女店員暗中抄下多名客人信用卡號和卡片背面認證碼到網路購物盜刷。 (HsinChu Police seized a female KTV clerk named Luo, who secretly wrote down guests’ credit card numbers and authentication codes to conduct misappropriated online shopping).

From this query, we obtain five query terms, including 查獲 (seize), 暗中 (secretly), 信用卡 (credit card), 網路購物 (online shopping), and 盜刷 (misappropriated). Let t
                     1 denote 查獲, t
                     2 denote 暗中, t
                     3 denote 信用卡, t
                     4 denote 網路購物, and t
                     5 denote 盜刷.

In this phase, first of all, we need to do transformation between five query terms and the judgment term set (i.e., BASE), which is selected from the training judgments. Suppose there are ten judgment terms u
                        1, u
                        2,
                        …
                        ,
                        u
                        10 in BASE. As defined in Section 3.1, gi
                        
                        ,
                        
                           j
                         is the similarity between query term ti
                         and judgment term uj
                        . In Table A.1
                        , we show the similarities between query terms and judgment terms. Also, assume that the weights of query terms t
                        1, 
                        t
                        2, t
                        3, t
                        4, and t
                        5 are all equal to 1. To transform a query term into multiple judgment terms, there are four steps to perform as demonstrated like Figs. 8 and 9 listed in Section 3.1. After transformation, the weights of judgment terms u
                        1, u
                        2,
                        …
                        ,
                        u
                        10 are 0.6, 0.4, 0.2, 0.1, 0.55, 0.8, 0.8, 0.2, 0.65, and 0.45 respectively.

Next, through the SVM classification model, the probabilities of ten statutes can be determined. Assume that the probabilities of 10 statutes S
                        1, S
                        2, 
                        S
                        3, S
                        4, S
                        5, S
                        6, S
                        7, S
                        8, S
                        9, and S
                        10, which are produced by SVM model, are 0.85, 0.8, 0.75, 0.7, 0.65, 0.6, 0.55, 0.5, 0.45, and 0.4 respectively.

From Phase 1, in accordance with the probabilities, the Top 7 statutes’ sequence is S
                        1, S
                        2
                        
                        …
                        ,
                        S
                        7. Assume that there are 12 statute terms l
                        1, l
                        2,
                        …
                        ,
                        l
                        12 generated from Top 7 statutes. Likewise, query terms can be mapped into the 12 statute terms of Top 7 statutes using the transformation method described in Section 3.2. After transformation, query vector is built by the weights of statute terms in the statute. Afterward, we use the cosine similarity formula to compute the similarity between query vector and statute vectors. Suppose in Table A.2
                        , there are 7 statute vectors and one query vector. The cosine similarity value between query vector and Top 7 statute vectors of S
                        1, S
                        2, S
                        3, S
                        4, S
                        5, S
                        6, and S
                        7 are 0.605228, 0.771517, 0.62361, 0.377964, 0.600538, 0.251976, and 0.679921 respectively. Consequently, we found that the new order of statutes is S
                        2, S
                        7, S
                        3, S
                        1, S
                        5, S
                        4, and S
                        6.

Since k
                        2 is 5, the output statutes of Phase 2 include S
                        2, S
                        7, S
                        3, S
                        1, S
                        5. To acquire more precise relevant statutes, this phase employed the Apriori association algorithm to achieve this goal. The definition of SFW (Statute Final Weight) formula, which is described in Section 3.3, is applied to determine the final sequence of statutes. Assume that we have a confidence matrix, shown in Table A.3
                        , which presents the confidence of the associative statute rule si
                        
                        →
                        sj
                        . After computation, the SFW values of Top 5 statutes S
                        2, S
                        7, S
                        3, S
                        1, and S
                        5 are 0.887866, 1.074924, 0.898879, 0.923447, and 1.019136 respectively. From the result, we can see that the sequence of the final Top 3 statutes is S
                        2, S
                        7, and S
                        5.

@&#REFERENCES@&#

