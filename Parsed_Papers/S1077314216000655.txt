@&#MAIN-TITLE@&#Semantic video labeling by developmental visual agents

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Developmental Visual Agents are life-long learning systems for video understanding.


                        
                        
                           
                           The proposed agents continuously process videos and receive supervisions by humans.


                        
                        
                           
                           The architecture goes from unsupervised feature extraction up to the symbolic level.


                        
                        
                           
                           Motion-based coherence constraints allow to exploit even few supervisions per class.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Learning from constraints

Life-long learning

Scene understanding

Motion estimation

Deep learning

@&#ABSTRACT@&#


               
               
                  In the recent years, computer vision has been undergoing a period of great development, testified by the many successful applications that are currently available in a variety of industrial products. Yet, when we come to the most challenging and foundational problem of building autonomous agents capable of performing scene understanding in unrestricted videos, there is still a lot to be done. In this paper we focus on semantic labeling of video streams, in which a set of semantic classes must be predicted for each pixel of the video. We propose to attack the problem from bottom to top, by introducing Developmental Visual Agents (DVAs) as general purpose visual systems that can progressively acquire visual skills from video data and experience, by continuously interacting with the environment and following lifelong learning principles. DVAs gradually develop a hierarchy of architectural stages, from unsupervised feature extraction to the symbolic level, where supervisions are provided by external users, pixel-wise. Differently from classic machine learning algorithms applied to computer vision, which typically employ huge datasets of fully labeled images to perform recognition tasks, DVAs can exploit even a few supervisions per semantic category, by enforcing coherence constraints based on motion estimation. Experiments on different vision tasks, performed on a variety of heterogeneous visual worlds, confirm the great potential of the proposed approach.
               
            

@&#INTRODUCTION@&#

Computer vision systems have nowadays shown outstanding results in several specific tasks that range from object recognition, detection, localization, to segmentation and tracking. The whole research field of computer vision is facing a great development, and related technologies can be found today in a variety of low-cost commercial devices such as cameras, tablets, and smartphones, as well as in highly advanced systems, as in the case of autonomous vehicles, augmented reality environments, medical diagnosis assistants, video surveillance controllers.

Despite this notable achievements, the general problem of constructing an automatic agent capable of performing visual scene understanding in unrestricted domains is far from being solved. As a matter of fact, the basic task of video semantic labeling (or scene parsing), which consists in assigning a semantic label to each pixel of a given video stream, has mostly been carried out only at the frame level, as the outcome of well-established pattern recognition methods working on images [1–5]. Conversely, we maintain that there are strong arguments to start exploring the more challenging problem of semantic labeling in unrestricted video streams, by developing automatic visual systems which can continuously interact with the environment and improve their skills, in a lifelong process which in principle never ends. Rather than exploiting huge amounts of labeled examples at once [6], which could be extremely costly to obtain in case of videos, we argue that these agents should be capable of using even only a few pixel-wise supervisions per semantic category, but exploiting the intrinsic information coming from motion in order to virtually extend supervisions, as well as to enforce coherence in predictions. Roughly speaking, once a pixel has been labeled, the constraint of motion coherent labeling virtually offers tons of other supervisions, that are essentially ignored in most machine learning approaches working on big databases of labeled images. This process resembles the visual interaction experienced in their own life by humans, who progressively acquire knowledge and competence, and can perform scene understanding after receiving just a few supervisions.

Following this idea, in this paper we introduce Developmental Visual Agents. DVAs continuously develop their visual skills by processing videos coming from any kind of source, and by interacting with users, from which they can ask for and receive supervisions. These agents implement a lifelong learning mechanism which proceeds asynchronously with respect to the processing of the input stream and the acquisition of external information. We aim at devising these systems from bottom to top, starting from the low-level feature extraction process, up to the symbolic layer where interaction with users occurs, and scene parsing predictions are shown. On top of the representation gained by motion coherence, the mapping to linguistic descriptions is dramatically simplified.

In this scenario, the learning framework indeed plays a crucial role. The work described in this paper is rooted on the theory of learning from constraints 
                     [7] that allows us to model the interaction of intelligent agents with the environment by means of constraints on the tasks to be learned. The notion of constraint is very well-suited to express both visual and linguistic granules of knowledge. In the simplest case, a visual constraint is just a way of expressing the supervision on a labeled pixel, but the same formalism is used to express motion coherence, as well as complex dependencies on real-valued functions. In principle one could also include abstract logic formalisms, such as First-Order-Logic (FOL) formulae [8,9].

The main contributions of the paper can be summarized as follows:

                        
                           •
                           DVAs are introduced as general-purpose visual agents for semantic video labeling in unrestricted domains, in a complete bottom-to-top pipeline from feature extraction up to the symbolic layers;

A lifelong learning paradigm for on-line video processing is defined, which exploits motion and temporal coherence throughout the life of the agent;

An incremental development of the system is proposed, so that DVAs continuously interact with external users, who provide new supervisions as the learning process asynchronously proceeds;

In order to perform real-time responses, motion coherence is widely exploited, and time budgets are introduced for handling and processing the input video stream.

A few ideas at the basis of the DVA architecture and the acronym “DVA” have been introduced in our previous works [10–12], yet they have been limited to a preliminary feature extraction process only, and have never been applied to video semantic labeling. The focus of this paper is on semantic predictions based on visual patterns and motion dynamics. In order to move closer to a real understanding of the scene, higher level reasoning mechanisms should be added to relate the predictions on the frame pixels. While these mechanisms are out of the scope of this work, the selected grounding theory of learning from constraints 
                     [7] is generic enough to naturally embed new types of knowledge into the DVA architecture.

The next Section of the paper will shortly describe the whole architecture of a DVA, while the subsequent Sections 3 to 4 will describe in detail the computational blocks regarding feature extraction and the symbolic levels, respectively. Section 5 will relate the DVA paradigm to existing works in the literature, while in Section 6 several experiments will show the performance of the proposed approach.

The software for running experiments with DVAs can be downloaded at the website of the project:


                     
http://dva.diism.unisi.it

                  

together with several videos and other supplementary material which illustrate the behavior of DVAs in different scenarios.

A DVA is a system designed to perform semantic labeling in unrestricted domains, by living in its own environment and by continuously processing videos, following a lifelong learning paradigm. The agent is devised so as to implement all the levels of a truly on-line vision system, starting from feature extraction up to the symbolic layers where interaction with users occurs, and predictions on semantic categories are attached to visual patterns. The system architecture is depicted in Fig. 1
                     .

Given a video stream 
                        
                           V
                           ,
                        
                      we indicate with 
                        
                           V
                           t
                        
                      the video frame at time t. The first element of the DVA pipeline consists of computational blocks hierarchically organized into multiple layers, that will be described in Section 3. In each layer, a set of features are progressively learned and extracted from each pixel x of 
                        
                           V
                           t
                        
                     . The features of the 
                        
                           ℓ
                           +
                           1
                        
                     th layer are built upon the ones extracted at layer ℓ, and they pretty much resemble the responses to convolutional filters, with a local support that is limited to a small area around x, also referred to as receptive field 
                     [13], indicated with a small grid in Fig. 1. Filter responses on x are encoded and collected into a feature vector (a group of colored boxes in Fig. 1), and the response to the same feature for all pixels is referred to as feature map (Fig. 1).

DVAs parametrize receptive fields by considering also their transformed instances under the class of affine transformations, paired by a criterion that allows us to get an affine invariant representation of the data covered by the field. This choice allows DVAs to compactly represent such data by a fixed-length vector called receptive input (Section 3.1). As the on-line video processing advances, motion estimation is yielded by matching receptive inputs among consecutive frames (Section 3.2). Given the receptive inputs of 
                        V
                      observed up to time t, a set of features are learned in an unsupervised setting, following information theoretical principles of Minimal Entropy Encoding [14] (Section 3.3). Features inherit motion coherence by the aforementioned matching scheme, and, within a given layer, they can be grouped to encode different properties of the input. Before being fed as input to the next layer, features are projected onto a space of lower dimensionality, by estimating the principal components over a time window with the NIPALS (Non-linear Iterative Partial Least Squares) algorithm [15]. The deep structure allows higher layers to virtually cover larger areas of the input frame (Section 3.4).

The second portion of the DVA pipeline of Fig. 1 will be described in Section 4, and it begins with an aggregation process to partition the input frame 
                        
                           V
                           t
                        
                      into homogeneous regions, sometimes also named super-pixels. To this aim, we extend the graph-based region-growing algorithm of [16], enforcing motion coherence between consecutive frames into the aggregation procedure (Section 4.1). The partitioning obtained for frame 
                        
                           V
                           t
                        
                      contains a set of regions that can be described in terms of the features extracted on the pixels belonging to them. In particular, DVAs average the features among the pixels of each region, to build a unique feature histogram. During the agent’s life, descriptors are progressively accumulated as vertices (nodes) of a graph, named Developmental Object Graph (DOG) (see Fig. 1). Two vertices in the DOG can be linked by an edge if either they are spatially similar, or if the agent collected evidence that motion estimation is relating them (Section 4.2).

Descriptors stored in the DOG correspond to visual patterns, and supervisions can be attached to them by external user interaction. In particular, users can provide two kinds of supervisions: (i) strong, which associates one or more tags to a specific coordinate x in a given frame, and therefore to a specific region and node within the DOG; (ii) weak, which only provides the information on the presence of a certain object class within the frame, yet without precisely indicating its position (see Fig. 2
                     ). The use of weak supervisions is motivated by real-time user interaction, for example in scenarios where users can only provide spoken supervisions through a microphone: clearly, it is reasonable to employ this kind of labeling only after the concepts have already been learned and developed for some time. DVAs are also designed to ask for supervisions on their own initiative, on those DOG nodes which are most frequently observed and for which a label is either not assigned nor predicted (e.g., unknown objects that are frequently seen and that differ from what the agent is able to recognize).

The last computational block of the DVA architecture of Fig. 1 involves the symbolic decision mechanism. DVAs learn a set of ω classifiers on the space of DOG nodes, where ω is the number of classes for which an external supervision was received up to the considered time instant t. The jth classifier models the class membership of a given node to the jth symbolic class, where 
                        
                           j
                           ∈
                           {
                           1
                           ,
                           …
                           ,
                           ω
                           }
                        
                     . Since pixels are associated to regions, and regions to nodes, we get ω pixel-wise predictions on 
                        
                           V
                           t
                        
                     . Learning is semi-supervised, since it is based on the few supervisions received up to t, and on the relationships among all the nodes (supervised and not-supervised), represented by the previously introduced edges of the DOG graph (Section 4.3).

Both the just mentioned symbolic classifiers and the previously introduced multilayer hierarchical features are grounded on the theory of learning from constraints 
                     [7]. Such theory defines a generic learning framework that is centered around the parsimony principle and on the concept of constraint to represent any kind of knowledge on the learning environment, on the learning tasks and on their interactions. The classical regularization framework of kernel machines is naturally extended to the case in which the agents interact with a richer environment, heading to more sophisticated tools called Support Constraint Machines (SCMs). In the case of DVAs, the hierarchical features are constrained to maximize the mutual information between their output values and the receptive inputs, while the symbolic classifiers are subject to supervision constraints on a few nodes, and to motion and spatial coherence constraints between pairs of nodes connected by a DOG edge. The constraints enrich their expressiveness with the progressive exposition to the video so as to follow a truly lifelong learning paradigm. As a matter of fact, as time passes, new receptive inputs are observed, new supervision constraints are introduced by user interaction, and new motion-based relationships are discovered.

DVAs are designed so as to continuously interact with the environment, while making predictions at any time. However, there are two major “budgets” that DVAs must take into account: memory budget and time budget. The memory budget is due to the limited storage capability of the host machine, that cannot memorize all the observed receptive inputs (to learn hierarchical features), nor all the region descriptors, i.e., DOG nodes (to learn the symbolic classifiers). DVAs introduce internal storage of tunable sizes, and metrics to compare receptive inputs or region descriptors, so that the ones computed on the current frame 
                        
                           V
                           t
                        
                      are associated to the most similar entries in the current storage. A predefined sampling resolution allows the agents to memorize only those elements that are not too similar to the already stored ones, exploiting popular tools in metric spaces that are called ϵ-nets [17]. A removal policy is provided for those data that are not observed by a certain amount of time. Differently, the time budget defines the maximum time to spend on a frame, and it is needed to perform predictions close to real-time. The metric-based comparisons are made fast by exploiting motion coherence, and they can be easily early stopped if the required time budget is overtaken. On the other hand, extracting a large set of features, or computing predictions for a large set of symbolic classes can quickly become cumbersome and time consuming. For this reason, we assume that DVAs operate in a transductive environment, so that features are extracted only from the stored receptive inputs, and semantic predictions are performed only on the stored DOG nodes. The lifelong learning procedure operates so as to cache feature values and the outputs of the symbolic classifiers as the optimization asynchronously evolves: in this way the agent will be able to continuously make predictions by simple lookup operations on the cached data, while learning can proceed in background with slower dynamics (Sections 3.3, 4).

The first computational block in the DVA architecture (Fig. 1) is the feature extraction module. Sections 3.1–3.3 will consider the case of a single layer extractor that, given an input frame 
                        
                           
                              V
                              t
                           
                           ,
                        
                      extracts a vector of d features for each pixel. The vector components are the responses of a small area around x (the so called receptive field 
                     [13]) to a bank of d filters that are learned in an unsupervised manner. Equivalently, this can be seen as applying a set of convolutional filters to 
                        
                           
                              V
                              t
                           
                           ,
                        
                      as in any feature learning scheme based on convolutional networks [18]. The extension to a multilayer hierarchy of extractors will be discussed in Section 3.4.

We model a receptive field of x with 
                           N
                         Gaussians gk
                        , 
                           
                              k
                              =
                              1
                              ,
                              …
                              ,
                              N
                              ,
                           
                         centered nearby the pixel at coordinates 
                           
                              
                                 x
                                 k
                              
                              +
                              x
                              ,
                           
                         and with variance η
                        2. The number of Gaussians, their position, and their variances are design choices, and in this work we assume the means to be located on a square grid 
                           
                              
                                 N
                              
                              ×
                              
                                 N
                              
                           
                         of unitary edge, centered in x. We define receptive input of x the vector

                           
                              
                                 
                                    ξ
                                    
                                       (
                                       x
                                       ,
                                       
                                          V
                                          t
                                       
                                       )
                                    
                                    :
                                    =
                                    
                                       
                                          [
                                          
                                             ξ
                                             1
                                          
                                          
                                             (
                                             x
                                             ,
                                             
                                                V
                                                t
                                             
                                             )
                                          
                                          ,
                                          …
                                          ,
                                          
                                             ξ
                                             N
                                          
                                          
                                             (
                                             x
                                             ,
                                             
                                                V
                                                t
                                             
                                             )
                                          
                                          ]
                                       
                                       ′
                                    
                                    ,
                                 
                              
                           
                        where the kth component is the outcome of convolving the kth Gaussian with the input frame,

                           
                              
                                 
                                    
                                       ξ
                                       k
                                    
                                    
                                       (
                                       x
                                       ,
                                       
                                          V
                                          t
                                       
                                       )
                                    
                                    :
                                    =
                                    
                                       g
                                       k
                                    
                                    ⊗
                                    
                                       V
                                       t
                                    
                                    ∝
                                    
                                       ∫
                                       
                                          V
                                          t
                                       
                                    
                                    
                                       e
                                       
                                          −
                                          
                                             
                                                
                                                   ∥
                                                
                                                
                                                   x
                                                   k
                                                
                                                
                                                   
                                                      +
                                                      x
                                                      −
                                                      y
                                                      ∥
                                                   
                                                   2
                                                
                                             
                                             
                                                2
                                                
                                                   η
                                                   2
                                                
                                             
                                          
                                       
                                    
                                    
                                       V
                                       t
                                    
                                    
                                       (
                                       y
                                       )
                                    
                                    d
                                    y
                                    
                                    .
                                 
                              
                           
                        In other words, the receptive input 
                           
                              ξ
                              (
                              x
                              ,
                              
                                 V
                                 t
                              
                              )
                           
                         is a filtered representation of the neighborhood of x, with a degree of image detail that depends on the number of Gaussians 
                           N
                         and on their variance η
                        2.

We can extend the notion of receptive field by allowing it to scale, rotate, or, more generally, to incur in affine transformations. Any 2D affine map A can be rewritten as the composition of three 2D transformations and a scale parameter, 
                           
                              A
                              =
                              σ
                              
                                 R
                                 
                                    φ
                                    1
                                 
                              
                              
                                 U
                                 
                                    φ
                                    2
                                 
                              
                              
                                 R
                                 
                                    φ
                                    3
                                 
                              
                              ,
                           
                         where σ > 0 and R
                        φ, U
                        ϑ are

                           
                              
                                 
                                    
                                       R
                                       φ
                                    
                                    =
                                    
                                       [
                                       
                                          
                                             
                                                
                                                   cos
                                                   φ
                                                
                                             
                                             
                                                
                                                   −
                                                   sin
                                                   φ
                                                
                                             
                                          
                                          
                                             
                                                
                                                   sin
                                                   φ
                                                
                                             
                                             
                                                
                                                   cos
                                                   φ
                                                
                                             
                                          
                                       
                                       ]
                                    
                                    ,
                                    
                                    
                                       U
                                       ϑ
                                    
                                    =
                                    
                                       [
                                       
                                          
                                             
                                                
                                                   
                                                      1
                                                      
                                                         cos
                                                         ϑ
                                                      
                                                   
                                                
                                             
                                             
                                                0
                                             
                                          
                                          
                                             
                                                0
                                             
                                             
                                                1
                                             
                                          
                                       
                                       ]
                                    
                                    ,
                                 
                              
                           
                        with φ1 ∈ [0, 2π], 
                           
                              
                                 φ
                                 2
                              
                              ∈
                              
                                 [
                                 0
                                 ,
                                 
                                    π
                                    2
                                 
                                 )
                              
                              ,
                           
                         and φ3 ∈ [0, π) [19]. These continuous intervals are discretized into grids Φ
                        1, Φ
                        2, Φ
                        3, and, similarly, we collect in Σ a set of discrete samples of σ (starting from 
                           
                              σ
                              =
                              1
                           
                        ). The domain 
                           
                              T
                              =
                              Σ
                              ×
                              
                                 Φ
                                 1
                              
                              ×
                              
                                 Φ
                                 2
                              
                              ×
                              
                                 Φ
                                 3
                              
                           
                         collects all the possible transformation tuples (or simply tuples).

Following this new notion of receptive field, and given a tuple 
                           
                              T
                              ∈
                              T
                              ,
                           
                         we can redefine the receptive input to depend on T as well, introducing 
                           
                              ξ
                              (
                              x
                              ,
                              T
                              ,
                              
                                 V
                                 t
                              
                              )
                              ,
                           
                         whose kth component is

                           
                              (1)
                              
                                 
                                    
                                       ξ
                                       k
                                    
                                    
                                       (
                                       x
                                       ,
                                       T
                                       ,
                                       
                                          V
                                          t
                                       
                                       )
                                    
                                    ∝
                                    
                                       ∫
                                       
                                          V
                                          t
                                       
                                    
                                    
                                       e
                                       
                                          −
                                          
                                             
                                                
                                                   ∥
                                                   σ
                                                
                                                
                                                   R
                                                   
                                                      φ
                                                      1
                                                   
                                                
                                                
                                                   U
                                                   
                                                      φ
                                                      2
                                                   
                                                
                                                
                                                   R
                                                   
                                                      φ
                                                      3
                                                   
                                                
                                                
                                                   x
                                                   k
                                                
                                                
                                                   
                                                      +
                                                      x
                                                      −
                                                      y
                                                      ∥
                                                   
                                                   2
                                                
                                             
                                             
                                                2
                                                
                                                   σ
                                                   2
                                                
                                                
                                                   η
                                                   2
                                                
                                             
                                          
                                       
                                    
                                    
                                       V
                                       t
                                    
                                    
                                       (
                                       y
                                       )
                                    
                                    d
                                    y
                                    
                                    .
                                 
                              
                           
                        Notice that the value of σ affects both the width of the Gaussians and their centers, and that only the transformation parameter σ affects the “shape” of the Gaussian bells. For this reason, computing the receptive input for all the pixels x and for all the transformations in 
                           T
                         only requires to perform |Σ| convolutions per-pixel, independently of the number of centers 
                           N
                         and on the size of the grids Φ
                        1, Φ
                        2, Φ
                        3.
                           1
                        
                        
                           1
                           The non-uniform scaling of 
                                 
                                    U
                                    
                                       φ
                                       2
                                    
                                 
                               should generate anisotropic Gaussians (see [19]), that we do not consider here both for simplicity and to reduce the computational burden.
                         The receptive input can include invariance to local changes in brightness and contrast, that we model by normalizing 
                           
                              ξ
                              (
                              x
                              ,
                              T
                              ,
                              
                                 V
                                 t
                              
                              )
                           
                         to zero-mean and unitary L
                        2 norm.

For each pixel x of 
                           
                              V
                              t
                           
                         we can compute 
                           
                              |
                              T
                              |
                           
                         receptive inputs, by varying the transformation tuple 
                           
                              T
                              ∈
                              T
                           
                        . That is, T can be considered as a hidden variable depending on pixel x of frame 
                           
                              V
                              t
                           
                        . DVAs associate a unique receptive input to each pixel, by determining the most appropriate tuple 
                           
                              T
                              
                                 x
                                 t
                              
                           
                        . The selection criterion is based on similarity matching with respect to the video processed up to the current time instant. We aim at “warping” the receptive field by the tuple 
                           
                              T
                              
                                 x
                                 t
                              
                           
                         for which the receptive input 
                           
                              ξ
                              (
                              x
                              ,
                              
                                 T
                                 
                                    x
                                    t
                                 
                              
                              ,
                              
                                 V
                                 t
                              
                              )
                           
                         is more “similar” to one of the other receptive inputs that DVA observed up to now, indicated with 
                           
                              ξ
                              
                                 x
                                 t
                              
                           
                        . Let Q be the collection of receptive inputs up to time t, and dist( ·, ·) a metric on Q
                        
                           2
                        
                        
                           2
                           We do not explicitly indicate the dependance of Q on the frame and pixel indices to keep the notation simpler.
                        . Given 
                           
                              x
                              ∈
                              
                                 V
                                 t
                              
                           
                         we solve

                           
                              (2)
                              
                                 
                                    
                                       (
                                       
                                          T
                                          
                                             x
                                             t
                                          
                                       
                                       ,
                                       
                                       
                                          ξ
                                          
                                             x
                                             t
                                          
                                       
                                       )
                                    
                                    =
                                    
                                       argmin
                                       
                                          
                                          T
                                          ∈
                                          T
                                          ,
                                          
                                          ξ
                                          ∈
                                          Q
                                       
                                    
                                    
                                    d
                                    i
                                    s
                                    t
                                    
                                       (
                                       ξ
                                       ,
                                       ξ
                                       
                                          (
                                          x
                                          ,
                                          T
                                          ,
                                          
                                             V
                                             t
                                          
                                          )
                                       
                                       )
                                    
                                 
                              
                           
                        to determine the optimal transformation tuple 
                           
                              
                                 T
                                 
                                    x
                                    t
                                 
                              
                              ,
                           
                         and to associate x to its nearest neighbor in Q (that is 
                           
                              ξ
                              
                                 x
                                 t
                              
                           
                        ), as depicted in Fig. 3
                         (full search). Then, the new receptive input 
                           
                              ξ
                              (
                              x
                              ,
                              
                                 T
                                 
                                    x
                                    t
                                 
                              
                              ,
                              
                                 V
                                 t
                              
                              )
                           
                         is added to Q, and we move to the next pixel, repeating the procedure.

It is clearly unfeasible to store in Q all the receptive inputs of every pixel of the video. On the other hand, since Eq. (2) aims at moving a new input closer to one of those already on Q, we introduce a tolerance ϵ to avoid storing near-duplicates. We add 
                           
                              ξ
                              (
                              x
                              ,
                              
                                 T
                                 
                                    x
                                    t
                                 
                              
                              ,
                              
                                 V
                                 t
                              
                              )
                           
                         to Q only if its distance from 
                           
                              ξ
                              
                                 x
                                 t
                              
                           
                         is larger than ϵ. Thus, ϵ determines the sampling resolution. The criterion of Eq. (2) is also well-suited for creating dense regions of similar receptive inputs in Q, which is what we are going to exploit to generate compact codes using density-based encoding schemes (as we will describe in Section 3.3).

The data in Q are distributed on a 
                           
                              (
                              N
                              −
                              2
                              )
                           
                        -sphere of radius 1, because of the L
                        2 normalization and the mean subtraction. For this reason, when dist(·, ·) is chosen as the Euclidean distance, a similarity measure based on the inner product ⟨·, ·⟩ can be equivalently employed to compare receptive inputs.
                           3
                        
                        
                           3
                           Due to the L
                              2 normalization, dist( ·, ·) depends only on the dot product of its arguments, 
                                 
                                    d
                                    i
                                    s
                                    t
                                    
                                       (
                                       
                                          ξ
                                          i
                                       
                                       ,
                                       
                                          ξ
                                          j
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          
                                             ∥
                                          
                                          
                                             ξ
                                             i
                                          
                                          
                                             
                                                ∥
                                             
                                             2
                                          
                                          +
                                          
                                             
                                                ∥
                                                
                                                   ξ
                                                   j
                                                
                                                ∥
                                             
                                             2
                                          
                                          −
                                          2
                                          
                                             〈
                                             
                                                ξ
                                                i
                                             
                                             ,
                                             
                                                ξ
                                                j
                                             
                                             〉
                                          
                                       
                                    
                                    =
                                    
                                       
                                          2
                                          −
                                          2
                                          〈
                                          
                                             ξ
                                             i
                                          
                                          ,
                                          
                                             ξ
                                             j
                                          
                                          〉
                                       
                                    
                                 
                              .
                         The set Q is an ϵ-net of the subspace of 
                           
                              
                                 I
                                 
                                 
                                 R
                              
                              N
                           
                         that contains all the observed receptive inputs. Such nets are standard tools in metric spaces, and they are frequently exploited in searching problems because of their properties [12]. For instance, it can be easily shown that there exists a finite set Q for any processed video stream. Finding a solution to Eq. (2) in an ϵ-net, for all pixels in a given frame, can be speeded up by a pivot-based mechanism [12], that avoids searching over the entire set 
                           
                              T
                              ×
                              Q
                           
                        .

While 
                           
                              Q
                              =
                              ∅
                           
                         at the beginning of the agent’s life, it is progressively populated as long as time passes. The transformation 
                           
                              T
                              
                                 x
                                 t
                              
                           
                         of the first pixel of the video is clearly not-unique, thus we arbitrarily set it to [1, 0, 0, 0] and add the receptive input to Q. More generally, the creation of Q turns out to be strongly based on the very early stages of life, in which the population in Q is very small. This does not seem to be an appropriate developmental mechanism in principle, so we propose using a blurring scheme such that Q ends up into a nearly stable configuration only after a certain visual developmental time. We initially set the variance factor η of the Gaussian filters of Eq. (1) to a large value, and progressively decrease it with an exponential decay that depends on t. This mechanism produces initial frames (input layers) strongly blurred, so that only a few receptive inputs are added to Q for each frame (since they are almost all near-duplicates, due to the blurring of the image). As η is decreased, the number of items in Q grows until a stable configuration or a maximum amount of budget memory are reached. This resembles somehow curriculum learning [20], where examples are presented to learning systems following an increasing degree of complexity. When the memory budget for Q is set, we propose using a removal policy of those elements that are less frequently solutions of Eq. (2).

Despite the use of efficient schemes to find a solution to Eq. (2) for all pixels in a given frame, the procedure still quickly becomes computationally intractable when the resolution of the video and the number of transformation tuples 
                           T
                         get larger. Strict time requirements in real-time settings can be met by partitioning the search space into mini-batches, and by accepting sub-optimal solutions within a pre-defined time budget. However, the quality of the result can be unsatisfactory for practical usages.

While 
                           
                              (
                              
                                 T
                                 
                                    x
                                    t
                                 
                              
                              ,
                              
                                 ξ
                                 
                                    x
                                    t
                                 
                              
                              )
                           
                         in Eq. (2) indicates the optimal solution of the problem, here we relax the notation by using the same symbols to also refer to potentially suboptimal solutions. The quantity 
                           
                              D
                              
                                 x
                                 t
                              
                           
                         is the value of the objective function of Eq. (2) computed for the given 
                           
                              (
                              
                                 T
                                 
                                    x
                                    t
                                 
                              
                              ,
                              
                                 ξ
                                 
                                    x
                                    t
                                 
                              
                              )
                           
                        .

We exploit the inherent coherence of video sequences to define a heuristic technique which performs quick local searches and that can provide good approximations of the problem stated by Eq. (2), while greatly speeding up the computation. The key idea is that the scene smoothly changes in subsequent frames and, therefore, we can “track” receptive inputs between 
                           
                              V
                              
                                 t
                                 −
                                 1
                              
                           
                         and 
                           
                              
                                 V
                                 t
                              
                              ,
                           
                         also yielding a motion estimation for the pixels of the frame. We use the already computed pairs 
                           
                              (
                              
                                 T
                                 
                                    x
                                    
                                       t
                                       −
                                       1
                                    
                                 
                              
                              ,
                              
                                 ξ
                                 
                                    x
                                    
                                       t
                                       −
                                       1
                                    
                                 
                              
                              )
                           
                         and the distances 
                           
                              D
                              
                                 x
                                 
                                    t
                                    −
                                    1
                                 
                              
                           
                         to compute the new pairs 
                           
                              (
                              
                                 T
                                 
                                    x
                                    t
                                 
                              
                              ,
                              
                                 ξ
                                 
                                    x
                                    t
                                 
                              
                              )
                           
                         and the new 
                           
                              
                                 D
                                 
                                    x
                                    t
                                 
                              
                              ,
                           
                         following [12]. For each pixel location x of 
                           
                              
                                 V
                                 t
                              
                              ,
                           
                         we look for a location y in 
                           
                              V
                              
                                 t
                                 −
                                 1
                              
                           
                         that can be associated to x, such that: (1) the coordinates y belong to a neighborhood of x, since the smoothness assumption implies a small translation between 
                           
                              V
                              
                                 t
                                 −
                                 1
                              
                           
                         and 
                           
                              V
                              t
                           
                        ; (2) the tuple 
                           
                              T
                              
                                 x
                                 t
                              
                           
                         is “close” to 
                           
                              
                                 T
                                 
                                    y
                                    
                                       t
                                       −
                                       1
                                    
                                 
                              
                              ,
                           
                         since the smoothness assumption also implies small changes in the scene; (3) the receptive input computed at x in frame 
                           
                              V
                              t
                           
                         must be similar to the one that was computed at y in frame 
                           
                              V
                              
                                 t
                                 −
                                 1
                              
                           
                        . The last requirement can be reformulated by constraining the distance between the receptive input computed at x in frame 
                           
                              V
                              t
                           
                         and 
                           
                              ξ
                              
                                 y
                                 
                                    t
                                    −
                                    1
                                 
                              
                           
                         to be similar to 
                           
                              D
                              
                                 y
                                 
                                    t
                                    −
                                    1
                                 
                              
                           
                        .

In detail, we indicate with 
                           
                              
                                 T
                                 ˜
                              
                              
                                 y
                                 
                                    t
                                    −
                                    1
                                 
                              
                           
                         the set of tuples that are equal or close
                           4
                        
                        
                           4
                           We considered 9 cases for 
                                 
                                    
                                       
                                          T
                                          ˜
                                       
                                       
                                          y
                                          
                                             t
                                             −
                                             1
                                          
                                       
                                    
                                    ,
                                 
                               by increasing/decreasing the 4 transformation parameters of 
                                 
                                    T
                                    
                                       y
                                       
                                          t
                                          −
                                          1
                                       
                                    
                                 
                               one at a time, or none of them.
                         to 
                           
                              T
                              
                                 y
                                 
                                    t
                                    −
                                    1
                                 
                              
                           
                        . Given the pixel x of the current frame and a search radius r, we scan the pairs 
                           
                              (
                              
                                 T
                                 
                                    y
                                    
                                       t
                                       −
                                       1
                                    
                                 
                              
                              ,
                              
                                 ξ
                                 
                                    y
                                    
                                       t
                                       −
                                       1
                                    
                                 
                              
                              )
                           
                         with y satisfying 
                           
                              |
                              x
                              −
                              y
                              |
                              =
                              r
                           
                        . For iteratively increasing values of r (starting with 
                           
                              r
                              =
                              0
                           
                        ), we look for a pixel y such that

                           
                              
                                 
                                    d
                                    i
                                    s
                                    t
                                    
                                       (
                                       ξ
                                       
                                          (
                                          x
                                          ,
                                          
                                             
                                                T
                                                ˜
                                             
                                             
                                                y
                                                
                                                   t
                                                   −
                                                   1
                                                
                                             
                                          
                                          ,
                                          
                                             V
                                             t
                                          
                                          )
                                       
                                       ,
                                       
                                          ξ
                                          
                                             y
                                             
                                                t
                                                −
                                                1
                                             
                                          
                                       
                                       )
                                    
                                    ≤
                                    ζ
                                    ·
                                    
                                       D
                                       
                                          y
                                          
                                             t
                                             −
                                             1
                                          
                                       
                                    
                                    
                                    ,
                                 
                              
                           
                        where ζ ≥ 1, considering all the tuples in 
                           
                              
                                 
                                    T
                                    ˜
                                 
                                 
                                    y
                                    
                                       t
                                       −
                                       1
                                    
                                 
                              
                              ∈
                              
                                 
                                    T
                                    ˜
                                 
                                 
                                    y
                                    
                                       t
                                       −
                                       1
                                    
                                 
                              
                           
                        . When the maximum radius is reached without finding a solution, then full searches are performed, as described in Section 3.1 (see also Fig. 3). Otherwise, we have 
                           
                              
                                 T
                                 
                                    x
                                    t
                                 
                              
                              =
                              
                                 
                                    T
                                    ˜
                                 
                                 
                                    y
                                    
                                       t
                                       −
                                       1
                                    
                                 
                              
                              ,
                           
                        
                        
                           
                              
                                 ξ
                                 
                                    x
                                    t
                                 
                              
                              =
                              
                                 ξ
                                 
                                    y
                                    
                                       t
                                       −
                                       1
                                    
                                 
                              
                              ,
                           
                         and we set

                           
                              
                                 
                                    
                                       D
                                       
                                          x
                                          t
                                       
                                    
                                    =
                                    min
                                    
                                       (
                                       
                                          D
                                          
                                             y
                                             
                                                t
                                                −
                                                1
                                             
                                          
                                       
                                       ,
                                       d
                                       i
                                       s
                                       t
                                       
                                          (
                                          ξ
                                          
                                             (
                                             x
                                             ,
                                             
                                                
                                                   T
                                                   ˜
                                                
                                                
                                                   y
                                                   
                                                      t
                                                      −
                                                      1
                                                   
                                                
                                             
                                             ,
                                             
                                                V
                                                t
                                             
                                             )
                                          
                                          ,
                                          
                                             ξ
                                             
                                                y
                                                
                                                   t
                                                   −
                                                   1
                                                
                                             
                                          
                                          )
                                       
                                       )
                                    
                                    ,
                                 
                              
                           
                        to avoid progressive increments of the estimated 
                           
                              D
                              
                                 x
                                 t
                              
                           
                        . Finally, the found association 
                           
                              y
                              ∈
                              
                                 V
                                 
                                    t
                                    −
                                    1
                                 
                              
                              →
                              x
                              ∈
                              
                                 V
                                 t
                              
                           
                         is an instance of motion estimation between x and y, that will be exploited also in the last stage of the DVA pipeline of Fig. 1.

So far, we have described how DVAs determine, for each pixel in the video stream, the transformation parameters (
                           
                              T
                              
                                 x
                                 t
                              
                           
                        ) and the “best” representative (
                           
                              ξ
                              
                                 x
                                 t
                              
                           
                        ) in Q. In order to extract and encode the set of d features from the stream 
                           V
                         up to time t, it is natural to exploit the data in Q, as it is a subsampling of the receptive inputs processed up to t. Once we have extracted features from Q, we can propagate them to each pixel of the video, using the aforementioned association “pixel → representative in Q”.

The goal is to associate a code out of d symbols, 
                           
                              p
                              
                                 (
                                 
                                    ξ
                                    i
                                 
                                 )
                              
                              =
                              
                                 [
                                 
                                    p
                                    1
                                 
                                 
                                    (
                                    
                                       ξ
                                       i
                                    
                                    )
                                 
                                 ,
                                 …
                                 ,
                                 
                                    p
                                    d
                                 
                                 
                                    (
                                    
                                       ξ
                                       i
                                    
                                    )
                                 
                                 ]
                              
                              ,
                           
                         to each ξi
                         ∈ Q, such that similar ξi
                        ’s will share similar codes. The vector p(ξi
                        ) is the feature vector extracted from ξi
                         (Fig. 1). The set Q is built using a criterion that favors the creation of dense regions (Section 3.1), so that the notion of similarity can be translated into the idea of assigning a similar code to all data grouped into dense regions, and placing separation surfaces between different codes in low-density regions. Minimal Entropy Encoders (MEEs) [14] implement this criterion in the context of kernel machines, showing good performances in clustering (encoding) tasks. MEEs learn a set of d functions 
                           
                              f
                              
                                 (
                                 
                                    ξ
                                    i
                                 
                                 )
                              
                              =
                              
                                 [
                                 
                                    f
                                    1
                                 
                                 
                                    (
                                    
                                       ξ
                                       i
                                    
                                    )
                                 
                                 ,
                                 …
                                 ,
                                 
                                    f
                                    d
                                 
                                 
                                    (
                                    
                                       ξ
                                       i
                                    
                                    )
                                 
                                 ]
                              
                              ,
                           
                         that are placed in a probabilistic relationship by the softmax function,

                           
                              (3)
                              
                                 
                                    
                                       p
                                       j
                                    
                                    
                                       (
                                       
                                          ξ
                                          i
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          e
                                          
                                             
                                                f
                                                j
                                             
                                             
                                                (
                                                
                                                   ξ
                                                   i
                                                
                                                )
                                             
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                k
                                                =
                                                1
                                             
                                             d
                                          
                                          
                                             e
                                             
                                                
                                                   f
                                                   k
                                                
                                                
                                                   (
                                                   
                                                      ξ
                                                      i
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                    
                                    
                                    .
                                 
                              
                           
                        The feature vector 
                           
                              p
                              
                                 (
                                 
                                    ξ
                                    i
                                 
                                 )
                              
                              =
                              
                                 [
                                 
                                    p
                                    1
                                 
                                 
                                    (
                                    
                                       ξ
                                       i
                                    
                                    )
                                 
                                 ,
                                 …
                                 ,
                                 
                                    p
                                    d
                                 
                                 
                                    (
                                    
                                       ξ
                                       i
                                    
                                    )
                                 
                                 ]
                              
                           
                         is a collection of probabilistic scores (it sums to 1), enforcing the functions in f(ξi
                        ) to compete during their development. The rationale behind MEEs is that of getting close-to-one-hot configurations of the probabilities p(ξi
                        ) for all the ξi
                         ∈ Q (i.e., only one feature probability is “high” in the feature vector) and of exploiting the whole available set of features (i.e., all the d features are used in Q, on average), by means of functions f(·) that are smooth on Q (so that the separation between different one-hot configurations will be placed in low density regions).

MEEs are instances of the theory of learning from constraints 
                        [7]. Under such a framework, MEEs can be seen as Support Constraint Machines that minimize a parsimony principle (the norm of f(·)) under a (soft) constraint that enforces the maximization of the mutual information (MI) between the space of receptive inputs (X) and the space of the output features (Y). Formally, their objective is

                           
                              (4)
                              
                                 
                                    
                                       
                                       
                                       
                                          
                                             
                                                min
                                                f
                                             
                                             −
                                             M
                                             I
                                             
                                                (
                                                Y
                                                
                                                   (
                                                   f
                                                   )
                                                
                                                ,
                                                X
                                                )
                                             
                                             +
                                             λ
                                             
                                                ∥
                                                f
                                                ∥
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                       
                                          
                                             =
                                             
                                             
                                                min
                                                f
                                             
                                             H
                                             
                                                (
                                                Y
                                                
                                                   (
                                                   f
                                                   )
                                                
                                                |
                                                X
                                                )
                                             
                                             −
                                             H
                                             
                                                (
                                                Y
                                                
                                                   (
                                                   f
                                                   )
                                                
                                                )
                                             
                                             +
                                             λ
                                             
                                                ∥
                                                f
                                                ∥
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where H(·) and H(· | ·) are the entropy and the conditional entropy, respectively. In the case of DVAs, the entropy functions can be computed with X being the discrete sample Q, and Y(f) the space of output features whose activation probabilities are given by Eq. (3).

When the parsimony principle is casted into the Reproducing Kernel Hilbert Space (RKHS) of a given kernel function K( ·, ·), the solution becomes a kernel expansion 
                           
                              
                                 f
                                 j
                              
                              
                                 (
                                 ξ
                                 )
                              
                              =
                              
                                 ∑
                                 
                                    k
                                    =
                                    1
                                 
                                 
                                    |
                                    Q
                                    |
                                 
                              
                              
                                 m
                                 
                                    k
                                    j
                                 
                              
                              K
                              
                                 (
                                 
                                    ξ
                                    k
                                 
                                 ,
                                 ξ
                                 )
                              
                              ,
                           
                         with ξk
                         ∈ Q. In the case of a linear kernel function ⟨·, ·⟩, we get 
                           
                              
                                 f
                                 j
                              
                              
                                 (
                                 ξ
                                 )
                              
                              =
                              
                                 〈
                                 
                                    u
                                    j
                                 
                                 ,
                                 ξ
                                 〉
                              
                           
                        . Then, the problem of Eq. (4) can be solved with respect to the variables mj
                         or uj
                        
                        
                           
                              (
                              j
                              =
                              1
                              ,
                              …
                              ,
                              d
                              )
                              ,
                           
                         respectively [7,14].
                           5
                        
                        
                           5
                           In the DVA implementation, the dth feature is not learned by MEEs, but automatically triggered in presence of almost constant receptive inputs (standard deviation under a given threshold). These inputs are not added to Q.
                         In the linear case, each fj
                        (·) can be seen as the response to a convolution (correlation) operation between a local image portion (ξi
                        ) and a linear filter (uj
                        ). In the non-linear case, fj
                        (·) models a more complicated non-linear relationship between the input and the filter response.

Since DVAs implement an instance of transductive learning (as described in Section 1), the learning procedure and the prediction of the feature vectors are performed on the same domain, Q. DVAs are able to extract features for all the pixels by a simple lookup operation, exploiting the association “pixel → representative on Q → feature vector”, that is particularly well suited in the case of non-linear kernels, in which the kernel expansion may involve several terms. Moreover, since the described motion estimation scheme enforces coherence on receptive inputs among consecutive frames, feature vectors inherit such coherence.

Considering the feature extraction and the invariance mechanisms of Section 3.1 as a whole, an important property must be remarked. There are no repeated instances of the same input under different affine transformations in the set Q, by construction. As a result, the MEE will not learn multiple features that correspond to the same input under affine transformations. For example, suppose that the jth feature responds to an edge-like pattern. When a DVA processes edges at different orientations, it will always encode them using the jth feature. The representation capabilities of each feature in the learning algorithm is enhanced by the invariance mechanism, without increasing the computational burden of the learning scheme. On the other hand, we lose any transformation-related information (e.g., the orientation of the edge in the previous example, or, more generally, 
                           
                              T
                              
                                 x
                                 t
                              
                           
                        ). In some real-world recognition tasks, it might be important to encode also the way the pattern is geometrically oriented into the feature vector. For this reason, a transformation unrolling mechanism can be activated in DVAs, in which the feature vector is augmented to include transformation related data. Each feature is represented 
                           
                              |
                              T
                              |
                           
                         times in the augmented vector. Only the representation corresponding to the index of the tuple 
                           
                              T
                              
                                 x
                                 t
                              
                           
                         will be set to the original value of the feature (the other repetitions will be set to zero). The size of the augmented feature vector will be 
                           
                              d
                              ×
                              |
                              T
                              |
                           
                        .

The feature extraction scheme described up to this point allows DVAs to associate a vector of d features to each pixel x of the frame 
                           
                              V
                              t
                           
                        . We use the symbol 
                           
                              p
                              (
                              x
                              ,
                              
                                 V
                                 t
                              
                              )
                           
                         to indicate such vector, overloading the previously used notation (to avoid making an explicit reference to the receptive inputs, keeping the notation simpler). DVAs process the input frame 
                           
                              V
                              t
                           
                         at resolution w × h and produces 
                           
                              (
                              w
                              −
                              b
                              )
                              ×
                              (
                              h
                              −
                              b
                              )
                           
                         feature vectors of length d, where the quantity b is the number of pixels that we have to discard due to border effects (we cannot compute receptive fields on image border, due to the finiteness of the image). We can represent such data with a tensor sized 
                           
                              (
                              w
                              −
                              b
                              )
                              ×
                              (
                              h
                              −
                              b
                              )
                              ×
                              d
                           
                        . The input frame can be a grayscale image, as well as an RGB representation, a concatenation of RGB and grayscale, or other combinations. More generally, the input can be a w × h × q tensor (e.g., 
                           
                              q
                              =
                              1
                           
                         for grayscale images, 
                           
                              q
                              =
                              3
                           
                         for RGB data). The proposed feature extraction by means of receptive inputs allows DVAs to handle generic tensorial data without affecting the described computations. The centers of the 
                           N
                         Gaussians are simply replicated for the q input channels, leading to receptive inputs with 
                           
                              q
                              ·
                              N
                           
                         components. We can build richer features by replicating the feature extractor into two possibile “directions”: vertical and horizontal, respectively. Fig. 4
                         illustrates an example with the notation what we are going to introduce in the rest of this Section.

In the vertical case, we build a hierarchy of extractors by stacking them into L layers, so that the features learned/extracted in layer ℓ are the input of the features that will be learned/extracted in layer 
                           
                              ℓ
                              +
                              1
                           
                        . Formally, we indicate with w
                        ℓ × h
                        ℓ × q
                        ℓ the size of the input for layer ℓ, while b
                        ℓ is the border to be discarded on such layer. The output tensor will be sized 
                           
                              
                                 (
                                 
                                    w
                                    ℓ
                                 
                                 −
                                 
                                    b
                                    ℓ
                                 
                                 )
                              
                              ×
                              
                                 (
                                 
                                    h
                                    ℓ
                                 
                                 −
                                 
                                    b
                                    ℓ
                                 
                                 )
                              
                              ×
                              
                                 d
                                 ℓ
                              
                              ,
                           
                         where d
                        ℓ is the number of features of layer ℓ. The higher level features are the outcome of a composition of feature extractors, thus modeling more complicated non-linear functions (due to the softmax operator), as commonly done in deep learning algorithms. Moreover, as long as we move toward the higher layers, the receptive-field-based feature extraction virtually involves larger image portions around each pixel.

In principle, due to the stacking mechanisms, the dimensions of the input tensor of layer ℓ are 
                           
                              
                                 w
                                 ℓ
                              
                              =
                              
                                 (
                                 
                                    w
                                    
                                       ℓ
                                       −
                                       1
                                    
                                 
                                 −
                                 
                                    b
                                    
                                       ℓ
                                       −
                                       1
                                    
                                 
                                 )
                              
                              ,
                           
                        
                        
                           
                              
                                 h
                                 ℓ
                              
                              =
                              
                                 (
                                 
                                    h
                                    
                                       ℓ
                                       −
                                       1
                                    
                                 
                                 −
                                 
                                    b
                                    
                                       ℓ
                                       −
                                       1
                                    
                                 
                                 )
                              
                              ,
                           
                         and 
                           
                              
                                 q
                                 ℓ
                              
                              =
                              
                                 d
                                 
                                    ℓ
                                    −
                                    1
                                 
                              
                           
                        . A main issue of this bare stacking is that computing receptive inputs for layers ℓ > 1 can be very cumbersome, due to the size of the input tensor that depends on the number of features extracted from the layer below (i.e., 
                           
                              
                                 q
                                 ℓ
                              
                              =
                              
                                 d
                                 
                                    ℓ
                                    −
                                    1
                                 
                              
                           
                        ). While in image classification it is quite common to pool and subsample the tensor to reduce its size in the first two dimensions, in the case of DVAs we do not subsample it, to keep a pixel-wise feature extraction that will be needed for the final goal of semantic labeling. However, some of the 
                           
                              d
                              
                                 ℓ
                                 −
                                 1
                              
                           
                         features may be correlated, or some of them could be triggered very rarely or never. For those reasons, it can be useful to project the 
                           
                              d
                              
                                 ℓ
                                 −
                                 1
                              
                           
                         features into a smaller dimensional space before feeding it to the next layer, so that 
                           
                              
                                 q
                                 ℓ
                              
                              <
                              
                                 d
                                 
                                    ℓ
                                    −
                                    1
                                 
                              
                           
                        . DVAs apply stochastic iterations of the NIPALS (non-linear iterative partial least squares) algorithm [15], to roughly compute q
                        ℓ principal components over a set of feature vectors (sized 
                           
                              
                                 d
                                 
                                    ℓ
                                    −
                                    1
                                 
                              
                              >
                              
                                 q
                                 ℓ
                              
                           
                        ) that are randomly taken during video processing in a given time window.

At each layer of the deep net, the feature learning is based on the same principles and algorithms. However, we use developmental stages based on learning layers separately, so as upper layers activate the learning process only when the features of the lower layers have already been learned, which can be measured by the stability of the MEE objective function, and of the Q set.

Feature extractors can be replicated also horizontally. In layer ℓ we can implement C
                        ℓ feature extractors, each of them independently contributing to learn a subset of the d
                        ℓ features. We indicate the number of features on each subset with 
                           
                              
                                 d
                                 c
                                 ℓ
                              
                              ,
                           
                         with 
                           
                              c
                              =
                              1
                              ,
                              …
                              ,
                              
                                 C
                                 ℓ
                              
                           
                         (we generally select them to be roughly of the same size). Each group of 
                           
                              d
                              c
                              ℓ
                           
                         features is what we refer to as a category of features. Different categories are characterized by the different portions of the input taken into account for their computation. In other words, each of the C
                        ℓ “horizontal” feature extractors processes a subset of the input tensor for layer ℓ. In particular, the third dimension, q
                        ℓ, is partitioned into C
                        ℓ disjoint groups, generating C
                        ℓ input tensors. In the case of the first layer, each category can operate on a different input channel or on different combinations of the channels.

We indicate with 
                           
                              
                                 p
                                 c
                                 ℓ
                              
                              
                                 (
                                 x
                                 ,
                                 
                                    V
                                    t
                                 
                                 )
                              
                           
                         the feature vector, sized 
                           
                              
                                 d
                                 c
                                 ℓ
                              
                              ,
                           
                         extracted on the cth category of the ℓth layer. The pixel-based features that are developed in the whole architecture are used for the construction of higher-level representations that are involved in the prediction of symbolic functions.

In order to build high-level symbolic classifiers for semantic labeling, we first aggregate homogenous regions (superpixels) of 
                        
                           
                              V
                              t
                           
                           ,
                        
                      as it will be described in Section 4.1. This reduces the computational burden of pixel-based processing, but it requires to move from the pixel-based descriptors 
                        
                           
                              p
                              c
                              ℓ
                           
                           
                              (
                              x
                              ,
                              
                                 V
                                 t
                              
                              )
                           
                        
                      to region-based descriptors 
                        
                           
                              s
                              
                                 z
                                 t
                              
                           
                           ,
                        
                      where zt
                      is the index of a region of 
                        
                           V
                           t
                        
                     . Section 4.2 will show how these region-based descriptors can be stored into a Development Object Graph, and Section 4.3 will describe how a classifier can be built over such graph, yielding to semantic labeling.

The aggregation procedure generates R regions
                           6
                        
                        
                           6
                           In the following description we drop the dependence of the region variables on time t to keep the notation simple.
                         (superpixels) rz
                        , 
                           
                              z
                              =
                              1
                              ,
                              …
                              ,
                              R
                           
                         for a given frame, where each rz
                         collects the coordinates of the pixels belonging to the zth region. We extended the algorithm in [16], a simple graph-based segmentation algorithm, that has already been exploited to perform segmentation over time (e.g., see [1]). The algorithm progressively merges pixels according to a dissimilarity function based both on color similarity (as in [16]) and on motion coherence. Other approaches, such as the recent streaming hierarchical video segmentation [21] (still a graph-based approach), could be used as well. Differently from that work, we do not make explicit use of voxels in the aggregation stage, while we introduce motion information from the optical flow in the pixel-wise similarity measure, getting an instance of video segmentation.
                           7
                        
                        
                           7
                           Our approach also generalizes the case of convolutions over time, leading to 3D voxels in the feature extraction stage.
                        
                     

Our algorithm first computes a dissimilarity score between the RGB triplets of neighboring pixels.
                           8
                        
                        
                           8
                           We also tried L*a*b, but we did not observe significant difference in the overall performance of the system.
                         Then, such measure is decreased (increased) for those pixels whose estimated motion directions are (are not) coherent, by a customable factor. In this way, neighboring pixels which are locally moving in the same direction will more likely belong to the same region. The dissimilarity is also decreased for pixels belonging to the same static region (i.e., a region whose pixels are estimated to not move) in the previous and current frames. The obtained regions correspond to visual patterns that the user can tag with supervisions, and they can be described in terms of the features extracted on the pixels belonging to them. DVAs compute the average feature vector to build a feature histogram on each region, repeating the process for all the layers and categories, finally stacking the results to generate the region descriptor sz
                        . We also append to sz
                         the color histogram of rz
                        , considering 4 equally spaced bins for each channel of the considered color space. Finally, we normalize sz
                         to sum to 1, giving the same weight to the feature-based portion of sz
                         and to the color-based one (more generally, the weight of each portion is a customizable parameter). The length of sz
                         is then 
                           
                              
                                 ∑
                                 
                                    ℓ
                                    =
                                    1
                                 
                                 L
                              
                              
                                 ∑
                                 
                                    c
                                    =
                                    1
                                 
                                 
                                    C
                                    ℓ
                                 
                              
                              
                                 d
                                 c
                                 ℓ
                              
                              +
                              
                                 4
                                 3
                              
                           
                        .

Region descriptors and their relations are stored as vertices (or “nodes”) and edges of a graph, that we refer to as Developmental Object Graph (DOG). Following the same approach described for receptive inputs and the set Q in Section 3, nodes are stored into the set V, and each region descriptor 
                           
                              s
                              
                                 z
                                 t
                              
                           
                         is either mapped to its nearest neighbor vj
                         ∈ V (if their distance is below a sampling resolution τ), or it is added to V (otherwise). In the first case, we say that 
                           
                              s
                              
                                 z
                                 t
                              
                           
                         
                        hits node vj
                        . We exploit the χ
                        2 distance, since it is well suited for comparing histograms. Also in this case the search procedure can be efficiently performed by partitioning the search space, and by tolerating sub-optimal mappings. A pre-defined time budget is defined, and we return the best response within such time constraint.

We can also use region-based motion coherence to strongly reduce the number of full searches required to retrieve the exact (or good approximation of the) nearest neighbors on V for the region descriptors of 
                           
                              V
                              t
                           
                        . We partition the image into M × M rectangular areas, and we associate each region to the area containing its barycenter. We are given the mappings between region descriptors and their closest DOG nodes at frame 
                           
                              V
                              
                                 t
                                 −
                                 1
                              
                           
                        . For each region 
                           
                              r
                              
                                 z
                                 t
                              
                           
                         in frame 
                           
                              
                                 V
                                 t
                              
                              ,
                           
                         belonging to a given M × M area, we compute the descriptor 
                           
                              s
                              
                                 z
                                 t
                              
                           
                        . Then, we search the same area in 
                           
                              V
                              
                                 t
                                 −
                                 1
                              
                           
                         (and the neighboring areas) for a region 
                           
                              r
                              
                                 h
                                 
                                    t
                                    −
                                    1
                                 
                              
                           
                         whose descriptor 
                           
                              s
                              
                                 h
                                 
                                    t
                                    −
                                    1
                                 
                              
                           
                         is similar to 
                           
                              s
                              
                                 z
                                 t
                              
                           
                        . If such a region is found, then 
                           
                              s
                              
                                 z
                                 t
                              
                           
                         
                        hits the same DOG node vj
                         that was associated to 
                           
                              r
                              
                                 h
                                 
                                    t
                                    −
                                    1
                                 
                              
                           
                         in the previous frame (see Fig. 5
                        ). In this case, 
                           
                              r
                              
                                 z
                                 t
                              
                           
                         is associated to vj
                         without the need to perform a full search to retrieve its nearest neighbor in V (as in the case of Section 3, this might be a suboptimal solution). A memory budget is also employed to limit the maximum number of stored DOG nodes, and a dedicated policy is used to decide which nodes can be removed during the life of the agent.

DOG edges are of two different types, spatial and motion-based, and their weights are named 
                           
                              w
                              
                                 i
                                 j
                              
                              s
                           
                         and 
                           
                              
                                 w
                                 
                                    i
                                    j
                                 
                                 m
                              
                              ,
                           
                         respectively. Spatial connections are built by assuming that close descriptors represent similar visual patterns. Only nodes that are closer than a predefined threshold γs
                         > τ are connected, leading to a sparse set of edges. Weights are computed by the χ
                        2 Gaussian kernel, as 
                           
                              
                                 w
                                 
                                    i
                                    j
                                 
                                 s
                              
                              =
                              exp
                              
                                 (
                                 −
                                 
                                    
                                       χ
                                       2
                                    
                                    
                                       (
                                       
                                          v
                                          i
                                       
                                       ,
                                       
                                          v
                                          j
                                       
                                       )
                                    
                                 
                                 /
                                 
                                    2
                                    
                                       σ
                                       
                                          τ
                                       
                                       2
                                    
                                 
                                 )
                              
                           
                        .

It is worth remarking that nodes that represent regions with similar appearance may not be actually spatially close, due to slight variations in lighting conditions, occlusions, or due to the suboptimal solutions of the receptive input matching process described in Section 3. The motion between frames 
                           
                              V
                              
                                 t
                                 −
                                 1
                              
                           
                         and 
                           
                              V
                              t
                           
                         can be used to overcome this issue, and, for this reason, we introduce links between nodes that are estimated to be the source and the destination of a motion flow. The weights are initialized as 
                           
                              
                                 w
                                 
                                    i
                                    j
                                 
                                 m
                              
                              =
                              0
                           
                         at 
                           
                              t
                              =
                              0
                              ,
                           
                         for each pair (i, j), and then they are estimated by a two-step process. First we compute the likelihood Pt
                        (va, vb
                        ) that two DOG nodes va, vb
                         ∈ V are related in two consecutive frames 
                           
                              V
                              t
                           
                         and 
                           
                              V
                              
                                 t
                                 −
                                 1
                              
                           
                         due to the estimated motion. Then the weight 
                           
                              w
                              
                                 a
                                 ,
                                 b
                              
                              m
                           
                         of the edge between the two corresponding DOG nodes is updated. Pt
                        (va, vb
                        ) is computed by considering the motion vectors that connect each pixel in 
                           
                              V
                              t
                           
                         to another pixel of 
                           
                              V
                              
                                 t
                                 −
                                 1
                              
                           
                         (Section 3.2). For each pair of connected pixels, one belonging to region 
                           
                              
                                 r
                                 
                                    z
                                    t
                                 
                              
                              ⊂
                              
                                 V
                                 t
                              
                           
                         and the other to 
                           
                              
                                 r
                                 
                                    h
                                    
                                       t
                                       −
                                       1
                                    
                                 
                              
                              ⊂
                              
                                 V
                                 
                                    t
                                    −
                                    1
                                 
                              
                              ,
                           
                         we consider the DOG nodes va
                         and vb
                         to which 
                           
                              r
                              
                                 z
                                 t
                              
                           
                         and 
                           
                              r
                              
                                 h
                                 
                                    t
                                    −
                                    1
                                 
                              
                           
                         are respectively associated. The observed event gives an evidence of the link between va
                         and vb
                        , and, hence, the frequency count for Pt
                        (va, vb
                        ) is increased by a vote, scaled by 
                           
                              
                                 |
                              
                              
                                 r
                                 
                                    z
                                    t
                                 
                              
                              
                                 |
                              
                           
                         to avoid penalizing smaller regions. Moreover, in the computation we consider only votes involving regions of comparable size, i.e. 
                           
                              
                                 |
                                 
                                    r
                                    
                                       z
                                       t
                                    
                                 
                                 |
                              
                              /
                              
                                 |
                                 
                                    r
                                    
                                       h
                                       
                                          t
                                          −
                                          1
                                       
                                    
                                 
                                 |
                              
                              ∈
                              
                                 [
                                 0.8
                                 ,
                                 1.25
                                 ]
                              
                           
                         Finally, since a DOG node v corresponds to all the region descriptors that hit it, the total votes accumulated for the edge between va
                         and vb
                         are also scaled by the number of distinct regions of 
                           
                              V
                              t
                           
                         that contributed to the votes. Similarly to the spatial case, a sparse connectivity is favored by pruning the estimates below a given threshold γm
                        , in order to avoid adding weak connections due to noisy motion predictions. Weights are computed by averaging through time the computed probabilities: as 
                           
                              
                                 w
                                 
                                    a
                                    ,
                                    b
                                 
                                 m
                              
                              =
                              
                                 1
                                 
                                    t
                                    +
                                    1
                                 
                              
                              
                                 ∑
                                 
                                    u
                                    =
                                    0
                                 
                                 t
                              
                              
                                 P
                                 u
                              
                              
                                 (
                                 
                                    v
                                    a
                                 
                                 ,
                                 
                                    v
                                    b
                                 
                                 )
                              
                           
                        . This step can be done with an incremental update that does not require to store the likelihood estimates for all the time steps.

So far, we described how a pixel x is univocally associated to a region of the current frame, whose descriptor is associated to a DOG node, with nodes related by spatial and motion-based connections. External supervisions can also be exploited and handled within this framework, by learning a set of classifiers predicting the labels (tags) associated to each DOG node → to each region of 
                           
                              V
                              t
                           
                         → to each pixel of 
                           
                              
                                 V
                                 t
                              
                              ,
                           
                         that is the final goal of semantic labeling.

While the video stream is processed, humans can interact with DVAs providing custom strong supervisions for a selected coordinate x of the frame they are observing (Section 2). Classes are not known in advance, and users can provide both positive (
                           
                              +
                              1
                           
                        ) and negative (
                           
                              −
                              1
                           
                        ) supervisions. Let ω be the number of classes for which the DVA received at least one supervision up to time t (
                           
                              ω
                              =
                              0
                           
                         at the beginning of the agent’s life). The DVA learns a set of ω classifiers on the space of DOG nodes V,

                           
                              
                                 
                                    l
                                    
                                       (
                                       
                                          v
                                          i
                                       
                                       )
                                    
                                    =
                                    
                                       [
                                       
                                          l
                                          1
                                       
                                       
                                          (
                                          
                                             v
                                             i
                                          
                                          )
                                       
                                       ,
                                       …
                                       ,
                                       
                                          l
                                          ω
                                       
                                       
                                          (
                                          
                                             v
                                             i
                                          
                                          )
                                       
                                       ]
                                    
                                    ,
                                    
                                    
                                       v
                                       i
                                    
                                    ∈
                                    V
                                    ,
                                 
                              
                           
                        following the theory of learning from constraints 
                        [7], as already done in the case of MEEs (Section 3.3). In particular, we look for smooth functions l(·), that are required to fulfill a set of constraints in a soft manner. Smoothness is modeled by minimizing the norm ‖l‖, while the constraints are of two types: supervision constraints 
                           
                              
                                 μ
                                 
                                    S
                                 
                                 
                                    (
                                    1
                                    )
                                 
                              
                              ,
                           
                         and coherence constraints 
                           
                              μ
                              
                                 M
                              
                              
                                 (
                                 2
                                 )
                              
                           
                        . Formally, we solve

                           
                              (5)
                              
                                 
                                    
                                       min
                                       l
                                    
                                    
                                       {
                                       
                                          μ
                                          
                                             S
                                          
                                          
                                             (
                                             1
                                             )
                                          
                                       
                                       
                                          (
                                          l
                                          )
                                       
                                       +
                                       
                                          μ
                                          
                                             M
                                          
                                          
                                             (
                                             2
                                             )
                                          
                                       
                                       
                                          (
                                          l
                                          )
                                       
                                       +
                                       
                                          λ
                                          R
                                       
                                       
                                          ∥
                                          l
                                          ∥
                                       
                                       }
                                    
                                    
                                    ,
                                 
                              
                           
                        where λR
                         > 0 is a scalar weight. We remark that the aforementioned theory does not make any restriction on the source of the involved constraints, so that we plan to add new constrains in future work (relationships among different classes, hierarchical organization, first order logic clauses [8,9]).

The supervision constraints enforce the approximation of labels 
                           
                              
                                 y
                                 
                                    i
                                    ,
                                    k
                                 
                              
                              ∈
                              
                                 {
                                 −
                                 1
                                 ,
                                 +
                                 1
                                 }
                              
                           
                         on some DOG nodes vi
                         ∈ V and for some functions lk
                        . For each lk
                        , the supervised nodes are collected into the set 
                           
                              
                                 S
                                 k
                              
                              =
                              
                                 {
                                 
                                    (
                                    
                                       v
                                       i
                                    
                                    ,
                                    
                                       y
                                       
                                          i
                                          ,
                                          k
                                       
                                    
                                    )
                                 
                                 ,
                                 
                                 i
                                 =
                                 1
                                 ,
                                 …
                                 ,
                                 
                                    n
                                    k
                                 
                                 }
                              
                              ,
                           
                         and

                           
                              (6)
                              
                                 
                                    
                                       μ
                                       
                                          S
                                       
                                       
                                          (
                                          1
                                          )
                                       
                                    
                                    
                                       (
                                       l
                                       )
                                    
                                    =
                                    
                                       ∑
                                       
                                          k
                                          =
                                          1
                                       
                                       ω
                                    
                                    
                                       ∑
                                       
                                          
                                             (
                                             
                                                v
                                                i
                                             
                                             ,
                                             
                                                y
                                                
                                                   i
                                                   ,
                                                   k
                                                
                                             
                                             )
                                          
                                          ∈
                                          
                                             S
                                             k
                                          
                                       
                                    
                                    
                                       β
                                       
                                          i
                                          k
                                       
                                    
                                    max
                                    
                                       
                                          (
                                          0
                                          ,
                                          1
                                          −
                                          
                                             y
                                             
                                                i
                                                ,
                                                k
                                             
                                          
                                          
                                             l
                                             k
                                          
                                          
                                             (
                                             
                                                v
                                                i
                                             
                                             )
                                          
                                          )
                                       
                                       2
                                    
                                    
                                    .
                                 
                              
                           
                        The scalar βik
                         > 0 is the belief 
                        [7] of each point-wise constraint. When a new supervision for class k is given on node vi
                        , its belief is set to a fixed value. Then, βik
                         is increased if the user provides the same supervision multiple times or decreased in case of mismatching supervisions, and it is normalized to keep 
                           
                              
                                 ∑
                                 i
                              
                              
                                 β
                                 
                                    i
                                    k
                                 
                              
                              =
                              1
                           
                        . This allows the agent to better focus on those supervisions that have been frequently provided, and to give less weight to noisy and incoherent labels. When the user provides a supervision for class k without indicating the exact position to which it should be attached (weak supervision, Section 2), DVAs uniformly include it into Eq. (6) if they determine that there exists a node associated to the current frame for which lk
                        (·) is above a predefined threshold. DVAs can also ask for supervision on those nodes that are hit more frequently and for which l(·) is small, meaning that no confident predictions are performed on them. When a frame that includes a region associated to one of these nodes is processed, it is saved and shown to the user, whose reply is treated as strong supervision.

The coherence constraints enforce a smooth prediction over connected vertices of the DOG,

                           
                              (7)
                              
                                 
                                    
                                       
                                          
                                             
                                                μ
                                                
                                                   M
                                                
                                                
                                                   (
                                                   2
                                                   )
                                                
                                             
                                             
                                                (
                                                l
                                                )
                                             
                                             =
                                             
                                                ∑
                                                
                                                   k
                                                   =
                                                   1
                                                
                                                ω
                                             
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   |
                                                   V
                                                   |
                                                
                                             
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   i
                                                   +
                                                   1
                                                
                                                
                                                   |
                                                   V
                                                   |
                                                
                                             
                                             
                                                w
                                                
                                                   i
                                                   j
                                                
                                             
                                             
                                                
                                                   (
                                                   
                                                      l
                                                      k
                                                   
                                                   
                                                      (
                                                      
                                                         v
                                                         i
                                                      
                                                      )
                                                   
                                                   −
                                                   
                                                      l
                                                      k
                                                   
                                                   
                                                      (
                                                      
                                                         v
                                                         j
                                                      
                                                      )
                                                   
                                                   )
                                                
                                                2
                                             
                                             
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        leading to an instance of manifold regularization [22]. The task is semi-supervised, since such constraints operate on all DOG nodes (both supervised and unsupervised). In this case, the belief of each point-wise constraint (wij
                        ) is a linear combination of the aforementioned edge weights 
                           
                              w
                              
                                 i
                                 j
                              
                              s
                           
                         and 
                           
                              
                                 w
                                 
                                    i
                                    j
                                 
                                 m
                              
                              ,
                           
                        
                        
                           
                              (8)
                              
                                 
                                    
                                       w
                                       
                                          i
                                          j
                                       
                                    
                                    =
                                    
                                       λ
                                       M
                                    
                                    
                                       (
                                       
                                          α
                                          M
                                       
                                       ·
                                       
                                          w
                                          
                                             i
                                             j
                                          
                                          s
                                       
                                       +
                                       
                                          (
                                          1
                                          −
                                          
                                             α
                                             M
                                          
                                          )
                                       
                                       ·
                                       
                                          w
                                          
                                             i
                                             j
                                          
                                          m
                                       
                                       )
                                    
                                    
                                    .
                                 
                              
                           
                        Here 
                           
                              
                                 λ
                                 M
                              
                              >
                              0
                           
                         defines the global weight of the coherence constraints while 
                           
                              
                                 α
                                 M
                              
                              ∈
                              
                                 [
                                 0
                                 ,
                                 1
                                 ]
                              
                           
                         can be used to tune the strength of the spatial-based connections w.r.t. the motion-based ones.

When casted into the framework of RKHS, the solution of Eq. (5) is a kernel expansion in the DOG nodes, 
                           
                              
                                 l
                                 k
                              
                              
                                 (
                                 v
                                 )
                              
                              =
                              
                                 ∑
                                 
                                    i
                                    =
                                    1
                                 
                                 
                                    |
                                    V
                                    |
                                 
                              
                              
                                 o
                                 
                                    i
                                    k
                                 
                              
                              K
                              
                                 (
                                 
                                    v
                                    i
                                 
                                 ,
                                 v
                                 )
                              
                              ,
                           
                         and the learning problem can be solved with respect to the variables oik
                        . We also assume that the agent is biased towards negative predictions, adding a fixed bias term equal to 
                           
                              −
                              1
                           
                         to each lk
                        (·). This choice, paired with a kernel with local support, allows DVAs to learn from positive examples only. For this reason, we selected the χ
                        2 exponential kernel [23].

Coherently to the feature extraction of Section 3.3, DVAs implement an instance of transductive learning also in the case of the class-predictions l(·) over the set V. The motivations are exactly the same: the learning can proceed in background, asynchronously with respect to the video stream, and the values of l(·) on V can be buffered. The predictions on each pixel can be performed by a simple lookup operation, exploiting the association “pixel → region → node ∈ V → buffered predictions”. The set of DOG nodes V progressively grows as the video stream is processed, up to a predefined maximum size (due to the selected memory budget). As V reaches the maximum allowed size, the adopted removal policy selects those nodes with a small number of hits, and that are neither involved by any supervision constraint, nor been recently hit by any descriptor.

@&#RELATED WORK@&#

DVAs involve a pipeline of several novel computational blocks that share many aspects with a number of proposals in the literature. In this Section we clarify the relationships of the DVA internals with existing approaches, starting from the early stages of the pipeline, up to the symbolic level.

The concept of “receptive field”, can be traced back to the studies of Hubel and Wiesel ([13]) and to the popular “Neocognitron” computer vision system [24]. Convolutional Neural Networks (CNNs) [18] have widely embraced the idea of receptive field, leading to very good results in object recognition (such as on the ImageNet data [6,25]), as well as in localization and detection [26]. DVA features share many properties with those of CNNs, namely the multilayer architecture and the learned convolutional filter banks. Differently from the massively-supervised nature of classical CNNs, DVAs features are learnt in an unsupervised manner, while progressively acquiring an incremental amount of samples from a video stream. DVAs are conceived for real-world scenarios in which only a few (symbolic) supervisions are available, not enough to fully train a CNN. Recently, some attempts of transferring the internal representation of a pre-trained CNN to other tasks were studied [27,28], showing interesting results but also some issues. DVA features are instead progressively learnt, being strongly embedded in a motion estimation/coherence scheme, and thus making transfer learning of pre-trained CNNs less practical.

Unsupervised learning of visual features has been the subject of several studies [29–40]. These approaches share a similar structure for the feature extraction mechanism (i.e., convolutional filter banks), while the objective of the learning problem is rooted on different schemes, such as reconstruction error [30], the K-Means algorithm [31,35], simulated fixation [32], non-negativity constraints [33], artificial data jittering [34], depth information and pooling [36], invariance to groups of transformations of the input [37], sparsity [29,38], approximating convolutional kernels [40], ensemble models [39]. These approaches are generally applied to image classification tasks, in which a global descriptor for the whole image is computed and fed to a supervised classifier. More generally, the main problem of learning “how to represent the data” is comprehensively presented in [41], which also contains a review of several related approaches. DVAs are based on the idea of directly maximizing the mutual information between the input data and the encoding produced by the learned features. Clearly, this task is strongly related to that exploited by auto-encoders and, more generally, by every approach that tries to preserve the information of the input into the output codes (e.g., see [42]). Here we directly optimize the mutual information within the objective function, following our recent work with Minimal Entropy Encoders [14], but other similar approaches could in principle be used. Differently from most of the existing works, we do not distinguish the two steps of learning the features and learning the encoding [43], that are both embedded into the probabilistic representation required to compute the mutual-information-based objective.

The notion of invariance in feature extraction has been the subject of many analyses on biologically inspired models [44–46], and invariances to geometric transformations are also strongly exploited in hand-designed low level features, such as SIFT [47] and HOG [48]. Some of the previously described learning approaches try to handle invariances to geometric transformations of the input data in the learning process or in the feature extraction pipeline [34,35,37,49]. DVAs share analogies with the ideas described in [46], in which an invariant signature can be associated to each input (patch). On the other hand, we do not store all the instances of transformed receptive inputs, while we rely on a search procedure that, when paired with motion estimation, can guide the feature extraction mechanism to follow the trajectories of the visual patterns along the video stream. As a matter of fact, a key difference with most of the existing works is that DVAs are designed to handle videos, mixing motion coherence and invariant feature extraction in a unique manner.

Moving toward the higher (symbolic) levels of DVAs, the extracted features are grouped and fed to a classifier, which aims at assigning a tag to each pixel in an image. For this reason, our work is related to the principles inspiring Scene Parsing/Labeling approaches. Recent works have shown successful results on classical benchmarks [2,3,50,51], although they seem to be very expensive in terms of computational resources. Most of them are based on the Markov random field (conditional random field) framework. While this choice seems quite natural to handle spatial relationships and global properties of the image, it generally relies on fully labeled (pixel-wise) images, and in the case of DVAs we consider the setting in which only a few sparse supervisions are available. Other successful ideas are based on random forests [4,52]. Fully supervised convolutional architectures were exploited for Scene Parsing in [1], following a computational pipeline that is similar to the one of DVAs. A similar approach was extended toward RGBD data, including motion-coherent segmentation [5]. Differently from those works, DVAs iteratively develop following a partially supervised setting, and they exploit motion-based coherence to reduce the computational burden of the pixel-tagging process.

DVAs interact with one or more supervisors, progressively learning new classes, improving their predictions, and, to a minor extent, asking for new supervisions. The field of Active Learning is in principle related to the proposed approach, as well as those works in which the visual recognition loop is interleaved by partial human supervisions [53,54]. Online Learning is also clearly related to our learning infrastructure (see, e.g., [55,56], as examples of computer vision approaches based on online/incremental learning). Since DVAs process a continuous video stream, we mention that there exist some experiences in never ending learning, mostly on still images [57].

Finally, there are several works which explicitly use motion and motion constraints to train visual object recognizers in a weakly-supervised or unsupervised setting [58–61]. DVAs rely on motion both in the feature extraction procedure, and in high level predictions. Moreover, motion estimation is not based on external optical flow estimates [62], but on a local matching scheme that is based on our previous experience [12] in which we studied motion in the context of long-term tracking. Since motion estimation is also used to enforce super-pixel coherence in DVAs, we also mention those works that learn object segmentation in videos [63,64].

@&#EXPERIMENTS@&#

In this Section, we present the experimental results to evaluate several aspects of the DVA architecture, from feature extraction up to the symbolic level of semantic labeling. Experiments were carried out on a variety of different videos ranging from artificial worlds and cartoons to real-world scenes, to show the flexibility of learning in unrestricted visual environments. The website of the project (http://dva.diism.unisi.it) hosts supplementary material with video sequences illustrating several case studies. The DVA software package can also be downloaded. In order to illustrate the characteristics of some modules of the DVA architecture, we start by introducing a variety of basic experiments in Sections 6.1 and 6.2 to analyze the process of feature development and the impact of motion estimation on aggregation and labeling. Then, we also present some results on image benchmarks in Section 6.3, although we remark that dealing with collections of images is not the application domain for which our systems have been developed. Finally, Section 6.4 presents results on two scene labeling benchmarks, whereas Section 6.5 describes experiments in a lifelong learning setting.

We evaluated the impact of the invariances in building the set Q from which the low-level features are learned. We used real-world video sequences from the Hollywood Dataset HOHA2 [65]. A shallow DVA (1 layer) was run on three videos that were rescaled to 320 × 240, and converted to grayscale. We selected an architecture with 
                           
                              N
                              =
                              5
                              ×
                              5
                           
                         receptive fields, 
                           
                              ϵ
                              =
                              0.7
                              ,
                           
                         and we repeated the experiment by activating invariances to different classes of geometric transformations (with 
                           
                              
                                 |
                              
                              
                                 Φ
                                 1
                              
                              
                                 |
                                 =
                                 16
                                 ,
                              
                           
                        
                        
                           
                              
                                 |
                              
                              
                                 Φ
                                 2
                              
                              
                                 |
                                 =
                                 3
                                 ,
                              
                           
                         |Φ
                        3| ≤ 6, 
                           
                              |
                              Σ
                              |
                              =
                              3
                              ,
                           
                         see Section 3). Fig. 6
                         highlights the crucial impact of using invariances for reducing |Q|. We set a memory budget that allowed DVA to store up to 6,000 ξ’s into Q. When full affine invariance is activated, there is a significant reduction of |Q|, thus simplifying the feature learning procedure. When considering the case with no-invariances, we reached the budget-limit earlier that in the case of scale-invariance-only. A deeper DVA (3 layers) processed the same sequences in order to learn 
                           
                              
                                 d
                                 ℓ
                              
                              =
                              20
                           
                         features per layer, 
                           
                              ℓ
                              =
                              1
                              ,
                              2
                              ,
                              3
                           
                         (one-category, 
                           
                              
                                 C
                                 ℓ
                              
                              =
                              1
                           
                        ). The same architecture was also used in processing a cartoon clip with different resolution. Fig. 7
                         shows the feature maps on four sample frames. Each pixel is depicted with the color that corresponds to the winning feature, i.e., the color of x is indexed by 
                           
                              arg
                              
                                 max
                                 j
                              
                              
                                 {
                                 
                                    p
                                    
                                       j
                                    
                                    ℓ
                                 
                                 
                                    (
                                    x
                                    ,
                                    ·
                                    )
                                 
                                 ,
                                 
                                 j
                                 =
                                 1
                                 ,
                                 …
                                 ,
                                 
                                    d
                                    ℓ
                                 
                                 }
                              
                           
                        . While features of the lowest layer clearly follow the details of the input, higher layers develop functions that capture more abstract visual patterns. For example, bright red pixels basically indicate the feature associated with constant receptive inputs, which become less evident in the upper layers, as the hierarchical application of the receptive fields virtually captures larger portions of the input frame, thus reducing the probability of constant patterns. Notice that feature orientation, scale, and other transformation-related properties are defined by the search procedure of Section 3. Hence, for each pixel, these transformations can also be recovered and eventually unrolled to augment the feature vector (Section 3.3). For other details regarding the impact of the feature extractor parameters on the overall performance of DVA, we refer to our previous works [12,14]. We observed DVAs to be not very sensitive to some of the parameters, such as moving from 5 × 5 to 7 × 7 receptive fields, or slightly changing the MEE parameters.

Motion plays a crucial role at several levels of the DVA architecture. In Section 3.2 we have seen that handling invariances to geometric transformations allows DVAs to estimate the motion field. An example of motion estimation is given in Fig. 8
                        , where each pixel in the third column is colored with a different hue according to the angle associated to its velocity vector. In this context, the use of motion in feature extraction is crucial also to speed up the computation for solving Eq. (2). We used again three random clips from the HOHA2 dataset, and measured the average computational time required by a 1-layer DVA to process one frame at 320 × 240 resolution. The impact of motion is dramatic, as the required time drops from 5.2 seconds per frame to 0.3 seconds per frame on an Intel Core-i7 laptop. It is worth mentioning that time budget requirements can also be imposed in order to further speed up the computation and perform real-time frame processing.


                        Fig. 9
                         shows a pair of consecutive frames taken from a Pink Panther cartoon (left column), with the results obtained by the region-growing algorithm without (middle column) or with (bottom column) exploiting motion coherence. Regions having the same color are associated to the same DOG node, therefore sharing very similar descriptors (their distance being ≤ τ, see Section 4.2). This example shows that the role of motion is crucial in order to get coherent regions through time, and this greatly simplifies the subsequent recognition process. In the middle column we can observe that the body of the Pink Panther changes from blue to orange, while it is always light green when exploiting motion information (right column); the water carafe, the faucet, and the tiles in the central part of the frame are other examples highlighting this phenomenon.

In order to investigate the effect of motion constraints in the development of the symbolic functions for semantic labeling (Section 4.3), we created a visual environment of Lucida (antialiased) digits which move from left to right by generic roto-translations, also scaling up and down. Each digit (from “0” to “9”) follows the same trajectory. The visual environment consists of a (loop) collection of 1,378 frames with resolution of 220 × 180 pixels, with a playback speed of 10 frames per second. A DVA processed the visual environment while a human supervisor interacted with the agent by providing only 11 pixel-wise positive supervisions, i.e., 1 per-digit and 1 for the background. The descriptors of rotated/scaled instances of the same digit turned out to be similar, due to the invariance properties of the low-level features. Because of the simplicity of this visual environment, we selected a shallow DVA architecture, and we kept a real-time processing of the video: 5 × 5 receptive fields, with minimum scale σ equal to 5, and 50 output features. We compared three different settings to construct the symbolic functions: the first one is based on supervision constraints (SC) only, the second one adds spatial coherence constraints (SPC), the last one also motion coherence constraints (MC). We created a classification task in which the goal was to predict the digit in each frame. We evaluated a baseline linear SVM trained for frame classification, thus excluding the background class (which yet DVA can easily predict just like any other class). The input to SVM were the vectorized frames rescaled to 44 × 36. We also generated negative supervisions (not used by DVA) to train the SVM in a one-vs-all scheme. Table 1
                         reports the macro accuracy for the digit-classes (excluding class of digit “9”, that is not distinguishable from a rotated instance of digit “6”), where DVA had reached a stable configuration as time goes by. Clearly, the SVM classifier does not generalize in the digit visual environment, as it uses full-frame supervisions only. DVA instead produces very good predictions even with one supervision per class only. Spatial coherence constraints allow the DVA to better generalize the prediction on unlabeled DOG nodes, thus exploiting the underlying manifold of the descriptor space. However, it turns out that the classes “2” and “5” are confused, due to the invariance properties of the low-level features that yield spatially similar descriptors for these classes. When introducing motion-based constraints the DVA disentangles these ambiguities, since the motion flow enforces a stronger coherence over those subsets of nodes that are related to the same digit. Notice that the enforcement of motion constraints is not influenced by the direction; the movement on different trajectories (e.g., playing the video frames in reverse order) generates the same results of Table 1. Without supplying any additional supervision, the same agent is also able to generalize on different fonts: Fig. 10
                         shows the results on an analogous artificial dataset by using Comic Sans MS font instead of Lucida. Again, very similar results were obtained also when playing the video in reverse order. All video sequences are available at the website project.

In order to assess the representation capabilities of the features learned by a DVA, we consider an image recognition task. In principle DVAs are not designed to directly handle this task, since they are conceived to process video streams in a on–line setting, with motion information, and a few sparse supervisions. Nevertheless, we can detach the feature extraction module from the whole DVA architecture and use it to learn features from a batch of images. This allows a comparison with other works specialized in unsupervised feature learning for image classification. To this aim, we disabled the motion estimation/coherence module, as we are not dealing with a video stream, and we collected the templates in Q by randomly selecting them from the training data (for each layer/category).

The STL-10 benchmark [66] is commonly used in related approaches. STL-10 is composed of static images (96 × 96 pixels, RGB) divided into 10 classes. Training samples are split into 10 pre-defined training folds with 1000 training images each, while the test set is composed of 8000 images. We follow the same procedure of the competitors, in which features are learned and extracted (pixel-wise) from each image, that is then described using average pooling of the features (taken from all the layers) over its four quadrants. We applied a whitening transform to the data in Q, as suggested in [66] for patch collections. Then, a linear SVM is trained on the batch of training image descriptors.

We notice that the number of patches on which features are learned by related approaches are of the order of hundred of thousands or even millions (e.g., [66,36]). Setting |Q| to such size would not allow DVA to handle the invariance matching procedure in reasonable times (since, as stated above, we had to disable motion estimation). For this reason, we first disabled geometric invariances to compare with the other works, and then we considered invariances in a smaller-scale setting.

We trained multiple DVAs, with 1, 2 or 3 layers. The first layer is composed of 2 categories, processing the RGB and grayscale representations, respectively. Layer 2 and 3 are composed of 8 categories each. Receptive fields of size 
                           
                              N
                              =
                              5
                              ×
                              5
                           
                         were selected for layer 1, while 3 × 3 fields were used in the higher layers. A 3 × 3 average pooling operation is performed to the output of layer 1 (without overlap), while 2 × 2 average pooling is applied to the output of layer 2 (with overlap). The output of each layer is then projected to 16 dimensions before feeding it in to the next layer, using 100,000 samples for the NIPALS procedure (Section 3.4). The size of Q was set to 200,000 for the categories of the first layer and to 100,000 for the ones of layer 2 and 3. The parameter λ of the MEEs (Section 3.3) was set to 
                           
                              
                                 10
                                 
                                    −
                                    6
                                 
                              
                              ,
                           
                         and a linear kernel was used. Some works focus on the downscaled version of STL-10 (32 × 32), so that we consider both the original and the downscaled version of this data. Table 2
                         collects the results that we obtained. We also report the number of layers and the total number of features that are learned from data. At first, we can clearly observe how introducing multiple layers improves the classification accuracy. While this also means that we estimate a larger number of features, we remark that learning the same number of features in a single layer architecture was leading to worse performances, confirming the importance of building higher-level abstractions of the data. Secondly, DVA features outperform most of the competitors (sometimes with a strongly smaller d), and they get results close to the state-of-the-art. Both CNN Ensemble and ExCNN, that achieve the best performance, are based on artificially augmented data. CNN Ensemble combines five models over multi-scale data, with a training data also artificially augmented by mirroring and rotation. Similarly, ExCNN is fully based on artificially transformed training data, with transformations that involve both geometric and appearance related features.

In another experiment, we also considered a smaller-scale setup, that follows the same parameter choices of the previous one with the main exception of |Q| that was fixed to 2,000, for each category of each layer. In this case we reduced the number of features and introduced invariance to geometric transformations by considering 8 rotation angles and 2 scales. Receptive field size was fixed to 
                           
                              N
                              =
                              3
                              ×
                              3
                           
                         Gaussians, pooling data on 3 × 3 patches with 1 pixel of overlap. The output of layer 1 was projected to 8 dimensions. We compared a DVA in which invariances are not activated with two ones in which they are enabled. The first DVA does not unroll the learned features using the estimated transformation parameters (Section 3.3), while the second DVA does unroll the extracted features. Table 3
                         collects the results of this comparison, showing how keeping fixed the number of features and introducing geometric invariances improves the classification accuracy. A single layer DVA leads to even better performance than one with two layers in which invariances are not modeled. When feature unrolling is not used, we do not keep track of the transformation parameters of the features learned on each pixel, and we observe a strong performance decrease. As a matter of fact, we are discarding precious information for representing images. While the results in Table 3 are clearly inferior to the ones in Table 2, we remark that the experimental setting of Table 2 is specific for comparing with approaches focused on image classification, thus not suitable for video semantic labeling with on–line user interaction.

The main purpose of DVAs is to perform semantic labeling in generic video domains. Unfortunately, building data sets for this kind of application is an extremely time-consuming operation, and for this reason semantic labeling methods are typically evaluated on collections of images rather than on videos. In this Section, we will present both results on a few benchmarks for semantic labeling, and also a novel kind of crowdsourcing evaluation, based on the idea of observing the behavior of the agents on a variety of different videos. Results on these videos are available on the project website.

We first show results on the Cambridge-driving Labeled Video Database (CamVid) [69]. This benchmark consists of a collection of pixel-wise labeled videos captured by a vehicle, with ground truth labels involving several semantic classes. We reproduced the experimental setting employed by almost any related work in recent years [51,67], considering the 11 most frequent classes, and a total of 600 labeled frames, splitted into a training set of 367 frames and a test set of 233 frames. Note that this collection of frames actually consists of a few different short video sequences at 1 frame per second: in this sense, the contribution of motion estimation in the scene understanding process is extremely weak, and the problem could be more appropriately named as semantic labeling on images rather than on videos. For each ground truth region in each frame of the training set, a supervision was provided to DVA, by computing the medoid of the ground truth region and by attaching the supervision to the DVA-created region which contains the medoid pixel. By following this scheme, it is worth mentioning that, with respect to the existing work on the same dataset, DVAs are conceived for on-line interaction, and not for a massive processing of labeled frames. Therefore, we decided to use a fraction of the available supervisions. While almost all the other approaches exploit all the supervised pixels (about 28 millions), we used about 17,000 supervisions, that is more than three order of magnitude less. A variety of different methods were used on these data. The state-of-the-art is obtained by exploiting Markov Random Fields [67] or Conditional Random Fields [51]. In this paper, we do not compare DVAs against these approaches, because the analysis of a post-processing (or refinement) stage for DVA predictions based on spatial or semantic reasoning is beyond the scope of this work. We therefore compare only against those methods which exploit appearance features, motion and geometric cues. We refer to (i) [68], where bag-of-textons are used as appearance features, and motion and structure properties are estimated from cloud points, and to (ii) [67], where convolutional neural networks (CNN) are tested, either enforcing spatial coherence among superpixels (CNN-superpixels), or weighting the contributions of multilayer predictions with a single scale (CNN-MR fine) or multiple scales (CNN-multiscale). In order to incorporate the information regarding region positions within the frame, which is an important feature in this scenario, we simply estimated from the training set the a priori probability of each class given the pixel coordinates, and, for each region, we multiplied the score computed by DVA for each class with the prior probability of its centroid. Table 4
                         shows the results of our experiments. DVA performs better than CNN-superpixels and comparable to motion-structures cues, while slightly inferior to appearance cues. Consider that state-of-the-art results on this database achieve 62.5/83.8 for the average/global measurement, respectively, combining object detectors and CRFs [70]. Not surprisingly, given the general nature of our approach, more specific methods oriented to this task perform better than DVA on most classes: it is the case of the last three competitors in Table 4 that, beside exploiting a larger amount of supervisions, rely on combinations of multiple hypotheses specifically designed for the benchmark. We also remark that the motion component in this task is negligible, due to the nature of the data set, and thus DVA cannot fully exploit its potential.

A second benchmark for which we present quantitative results is a scene labeling image data set, called eTraining for Interpreting Images of Man-Made Scenes (eTRIMS)
                           9
                        
                        
                           9
                           
                              http://www.ipb.uni-bonn.de/projects/etrims_db/
                           
                        , representing building facades, where each pixel is labeled with one of eight semantic classes. The data set is very small and therefore challenging: it consists of 60 images, which are typically split in 40/20 training/test images, respectively. Results are averaged on five random splits, in order to compare against other methods in the literature. As for the CamVid database, even in the case of eTRIMS we employed a single supervision for each region in the ground truth, therefore exploiting 1,769 supervisions for all 60 images instead of 23,592,260 available for all the pixels in the data set (four orders of magnitude less). As for CamVid, we multiplied the score of the SCM for the a priori probability of the class given the position of the region. Table 5
                         shows a comparison with recent works [4,71] employing the same setting, which only use RGB inputs, most of the other works in the literature being based on specific facade recognition models. Despite the huge difference in the employed supervisions, DVA performs comparably with Random Forests (RF), Conditional Random Fields (CRF), Hierarchical Conditional Random Fields (HCRF), and SIFT combined with RF (SIFT+RF). Neural Decision Forests achieve slightly superior performance, as the complexity of the model grows (NDF-MLP employs multi-layer perceptrons to model the split functions, while NDF-MLPC-ℓ1 employs a 4-layer MLP with additional ℓ1 regularization for the weights), being specifically designed for a batch setting.

Finally, we present results on semantic labeling performed in a lifelong learning setting. First, we show an illustrative case study, in order to describe DVA operation in all its phases. A visual environment from the UNISI AI lab was constructed using a 2 min video stream acquired by a webcam. During the first portion of the video, a supervisor interacted with the DVA by providing 72 pixel-wise supervisions, out of which only 2 were negative. The supervisions covered four object classes (bottle, chair, journal, face). In the remaining portion of the video no further supervision is given. Fig. 11
                         collects examples of both user interactions and the most confident DVA predictions, highlighting only regions having the highest tag score above zero. The frame sequence is ordered following the real video timeline (left to right, top to bottom). The first two rows show samples taken from the first portion of the video, where red-framed pictures mark user supervisions, and the others illustrate DVA’s responses. For example, a wrong “bottle” is predicted over the black monitor in the third sample, which is corrected, later on, by a subsequent negative supervision: this is a typical mistake made by DVA, induced by a wrong generalization over DOG nodes. The last two rows refer to the video portion in which no supervisions were provided. The system is capable of generalizing predictions even in presence of small occlusions (chair, bottle), or in cases where objects appear in contexts that are different from the ones in which they were supervised (bottle, journal, face).

Following the same paradigm, we performed an experiment with the aim of comparing different DVA architectures on various video sequences. Five DVAs were developed on a set of videos taken from four different visual worlds: a Donald Duck cartoon, a Pink Panther cartoon, a set of (merged) clips from the movie “Get Shorty”, taken from the HOHA2 database [65] and the video sequence recorded at the UNISI AI-lab, described in the previous paragraphs. All the videos are processed at 240 × 180 resolution, 25 fps. A number of semantic classes ranging from 4 to 6 were defined for each world (see Table 6
                        ), and the DVAs were provided with very few positive-only supervisions for each class (from 5 up to 10). For each visual world, only the first portion of the video was used to feed supervisions ( ≈ 2 minutes), so that the remaining part could be employed to evaluate the generalization capability of each agent ( ≈ 1 min). In this setting, we propose an evaluation scheme based on crowdsourcing, where users can register on the website, observe the behavior of the agents, and rate them with a score ranging from 0 (worst) up to 5 (best) stars. We believe that this could be an interesting evaluation procedure for computer vision systems, as it opens the doors of a laboratory to external observers, who can truly observe the operation of the agents on a variety of different and heterogeneous scenarios. At the time of writing, 40 users coming from different AI/CV laboratories and Computer Science students were involved in the rating process. Examples of the outcome of the DVA predictions, as shown to the subscribed users, is depicted in Fig. 12
                        : each region was labeled with the most-confident class, highlighting only regions where the confidence of the predictor is greater than zero.

The five agents were diversified by some high level characteristics of their architecture, while sharing a common Base Architecture (BA) for the development of low-level feature extractors. The choice of these parameters was made so that strict time-budget constraints could be fulfilled (realtime processing on an ordinary multicore CPU). In particular, we employed 5 × 5 receptive fields, 1 layer/feature-category, spatial scales in {1, 1.5}, 8 in-plane rotation angles, 800 features. The first agent exploits such BA, designed to store up to 10,000 nodes on the DOG, including spatial and motion-based connections (and the corresponding constraints of Section 4.3). The other agents are built on a specific variation of BA, described in the second column of Table 7
                        . Therefore, the experiment has the goal to analyze the impact on the performance of some components of the DVA architecture, as it should be reflected by the rates given by the users during evaluation. The aspects considered are the impact of motion constraints (agent 2), the use of a larger DOG graph (agent 3), the number of supervisions (agent 4), and the duration of the “life” of the agent (agent 5). Table 7 reports the results obtained by this first crowdsourcing evaluation, averaged on all the rates obtained in all the four considered worlds (variances were very small—we do not report them). These results suggest that the role of motion is remarkably crucial to improve the quality of the visual agents (agent 1 vs. agent 2). The motion constraints virtually propagate supervisions over nodes connected by motion-estimation-based links in the DOG. This holds true not only for moving instances, but also for static objects that undergo small changes in appearance due to illumination or occlusions, as it can be appreciated in Fig. 13
                        , where the moving journal is correctly predicted, as well as other static objects of the scene. Moreover, while more supervisions (agent 4) seem to improve the quality (as expected), doubling the size of the DOG (agent 3) was badly rated. We explain this result by considering that the more densely sampled DOG probably requires a more appropriate parameter adjustment (e.g., the width of the kernel, the weight of the spatial/motion constraints) to better exploit its structure. Finally, processing longer sequences (agent 5) leads to significantly better DVAs, since the motion-based-links become more accurate as long as the agent “grows”, and the long–term node removal procedure can noise-filter more vertices.

Another set of results we present here concerns a long–term experiment inspired by lifelong learning principles. In this case, the so called agent DEVA, was developed by continuously processing
                           10
                        
                        
                           10
                           The agent actually processes a few minutes of videos each day, in order to allow us to analyze its performance.
                         the cartoon “The Aristocats” (©  The Walt Disney Company), and receiving every day new supervisions from a set of selected supervisors. The outcome of DEVA processing can be monitored online for each day in the life of the agent (http://dva.diism.unisi.it/demo_aristocats.html), in order to check how it is evolving, if it is improving, and which are the common mispredictions it makes.
                           11
                        
                        
                           11
                           Video processed at 320 × 240 pixels, 25 fps, 20,000 DOG nodes, low-levels analogous to crowdsourcing experiments.
                         Users can access the agent webpage by a web browser (even with mobile devices), select the day of life to observe, and visualize the agent predictions over the portion of video processed in that day. The agent sensitivity and the classes to visualize can be selected as well (Fig. 14
                        ). In Fig. 15
                         we report the result of an artificial test extrapolated from this experiment, in which DEVA was tested on the same video sequence at different life stages (1 vs. 5 days old). Despite some errors in the predictions (the white female cats, Duchess and Marie, and reddish cats, Toulouse and O’Malley, are extremely similar, easy to confuse) we can clearly appreciate how the agent is progressively acquiring better visual skills during its life. The goal of this experiment is twofold: (1) it allows us to publicly share the results of a “live” experiment; (2) it shows how DVA can handle and predict several classes for each image region, in a dynamic world in which the number of classes grows over time, and where existing classes receive new user interactions. The experiment was started on November 6th, 2014
                           12
                        
                        
                           12
                           ≈ 1, 000 supervisions during first 10 days of life, distributed among 15 semantic categories.
                         and stopped in February 2015, as we are we are currently planning to develop a system that allows registered users to submit supervisions and customize their own agent, and to extend this activity to new worlds (videos).

@&#CONCLUSIONS AND FUTURE WORK@&#

We introduced Developmental Visual Agents (DVAs) for semantic labeling in unrestricted videos. The key idea in the DVA architecture is to implement a lifelong learning paradigm, so that each agent can progressively develop its visual ability, by a continuous and never-ending interaction with the external environment. The approach is completely different from most of the current state-of-the-art systems in computer vision, which typically work on images, by relying on huge labeled data sets. DVAs, on the contrary, can exploit even a few supervisions per semantic category, by implementing motion coherence constraints that virtually spread supervision information. The experimental results presented in this paper show very promising results for this highly challenging task, describing a complete bottom-to-top experience.

The DVA architecture can be extended in different ways. At the symbolic level, several different solutions can be conceived in order to plug contextual information into the model. For example, we are currently investigating some mechanisms which exploit the co-occurrence between semantic categories, as well as between DOG nodes, to perform a sort of collective classification for scene parsing. Moreover, the proposed theoretical framework also allows to model generic logic constraints into the Support Constraints Machines, thus allowing to perform high level cognitive tasks such as spatial-semantic reasoning. Another intersting direction of research we are undertaking is to extend the low-level filters through the temporal dimension, in order to extract spatio-temporal features which, integrated in the framework of learning from constraints, may provide very powerful features for action recognition.

Conflict of Interest: The authors declare that they have no conflict of interest.

@&#ACKNOWLEDGMENTS@&#

We thank Marcello Pelillo, Paolo Frasconi, Fabio Roli, Yoshua Bengio, Alessandro Mecocci, Oswald Lanz, Samuel Rota Bulò, Luciano Serafini, Ivan Donadello, Alberto Del Bimbo, Federico Pernici, Salvatore Frandina, Alessandro Rossi and Vincenzo Scoca.

@&#REFERENCES@&#

