@&#MAIN-TITLE@&#A heuristic hierarchical scheme for academic search and retrieval

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We present PubSearch, a new system for academic search and retrieval.


                        
                        
                           
                           PubSearch is based on a cascade hierarchy of three heuristics.


                        
                        
                           
                           The heuristics include Term-Frequency, citation distribution and topics’ inter-relations.


                        
                        
                           
                           We compare PubSearch performance against ACM Portal on 58 user queries.


                        
                        
                           
                           The system outperforms ACM Portal in terms of ERR, NDCG & LEX metric by large margin.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Academic search

Search and retrieval

Heuristic document re-ranking

@&#ABSTRACT@&#


               
               
                  We present PubSearch, a hybrid heuristic scheme for re-ranking academic papers retrieved from standard digital libraries such as the ACM Portal. The scheme is based on the hierarchical combination of a custom implementation of the term frequency heuristic, a time-depreciated citation score and a graph-theoretic computed score that relates the paper’s index terms with each other. We designed and developed a meta-search engine that submits user queries to standard digital repositories of academic publications and re-ranks the repository results using the hierarchical heuristic scheme. We evaluate our proposed re-ranking scheme via user feedback against the results of ACM Portal on a total of 58 different user queries specified from 15 different users. The results show that our proposed scheme significantly outperforms ACM Portal in terms of retrieval precision as measured by most common metrics in Information Retrieval including Normalized Discounted Cumulative Gain (NDCG), Expected Reciprocal Rank (ERR) as well as a newly introduced lexicographic rule (LEX) of ranking search results. In particular, PubSearch outperforms ACM Portal by more than 77% in terms of ERR, by more than 11% in terms of NDCG, and by more than 907.5% in terms of LEX. We also re-rank the top-10 results of a subset of the original 58 user queries produced by Google Scholar, Microsoft Academic Search, and ArnetMiner; the results show that PubSearch compares very well against these search engines as well. The proposed scheme can be easily plugged in any existing search engine for retrieval of academic publications.
               
            

@&#INTRODUCTION@&#

Academic research has been revolutionized since the days of standard library-based research, mostly by the advancements of the internet and the world-wide web, and by the break-through advancements of companies such as Google. And yet, searching for papers on specific topics on standard digital libraries such as ACM or IEEE digital libraries often leads to top results that are not as satisfactory as the search results on more generic queries such as those depicted on Yahoo!’s “Trending Now” section. For example, searching for the terms “Web Information Retrieval” on ACM Portal (http://portal.acm.org) the top 100 results do not include the—required reading—paper on “The anatomy of a large-scale hyper textual web search engine” defining Google’s Page-Rank algorithm (Brin & Page, 1998). Similarly, on the IEEE Xplore’s web site, the query results fail to include the paper in the first one hundred results. The same query when submitted on Microsoft’s Academic Search (http://academic.research.microsoft.com) fails to return the landmark paper among the 266 results it returns even though its database contains the paper. Even on Google Scholar (http://scholar.google.com), when submitting the same query, the paper appears buried in the 27th position–in the third page of results–despite the fact that by the same system’s account, the paper has received 8000 citations to date!. Notice that the terms “Web” and “Information Retrieval” both appear as keywords in the afore-mentioned paper, yet none of the major academic search engines manages to return the article in a high position among its result set. As another example, when searching for “peer-to-peer protocol”, ACM Portal returns the landmark paper “Chord: a scalable peer-to-peer lookup protocol for internet applications” (Stoica et al., 2003) that has received more than 1900 citations in the 43rd position in the results list (on the third page of results). From the examples above, it becomes apparent that while web information retrieval in general has advanced to a very mature state, academic search has still some way to go before reaching maturity (Beel & Gipp, 2009; Walters, 2007).

In this paper, we propose a heuristic hierarchical scheme for re-ranking publications returned from standard digital libraries such ACM Digital Library and evaluate the scheme based on the feedback of users. The scheme relies on a combination of a custom implementation of the term frequency heuristic that takes into account the positioning of the query terms in the document and their relative distance in-between sentences, paragraphs and sections, the annual distribution of the paper’s citation count and the paper’s index terms from the ACM Document Classification System. In particular, index terms are grouped together in cliques according to the co-occurrence of pairs of terms in the literature (see Tang, Jin, and Zhang (2008) for a related approach to topic modeling); a paper’s index terms are then matched (as a set) against pre-computed cliques of terms that have been obtained by crawling offline a large sample of papers in the ACM Portal. This crawling process resulted in the formation of a set of graphs connecting index terms according to the authors and co-authors who work on these topics as well as the papers’ index terms (Section 2 provides all necessary details). The properties we use are ordered in a hierarchy. Each property is used to obtain an ordering of the results, which are then clustered together in buckets; within each bucket, the results contained in it are further re-ranked using the property that is immediately lower in the hierarchy of properties, and then further clustered together in finer-grain buckets. In the bottom-end of the hierarchy, the ordered documents within each bucket together with the order of the buckets forms a new ordering of the original results to the submitted query.

The remainder of this paper is organized as follows. A brief overview of related work is presented in Section 1.1. Section 2 describes the system architecture & design and applies all ranking algorithms we implemented. The design of our experimental test-bed is analyzed in Section 3. Experimental results are described in Section 4. Finally, in Section 5 we present our conclusions and plans for future work.

@&#RELATED WORK@&#

The main graph representation methods in academic search can be distinguished among two approaches based on: (i) link references of scientific papers and (ii) academic collaboration networks. With respect to approaches that are based on the link references of scientific papers, a new multi-faceted dataset named CiteData for evaluating personalized search performance (Harpale, Yang, Gopal, He, & Yue, 2010) is among the most relevant and recent works in the literature. CiteData is a collection of academic papers selected from CiteULike social tagging web-site’s database and filtered through CiteSeer’s database for cleaning meta-data regarding each paper. The dataset contains a rich link structure comprising of the references between papers—whose distribution is shown to obey a power law as expected—but even more, the dataset contains personalized queries and relevance feedback scores on the results of those queries obtained through various algorithms. The authors report that personalized search algorithms produce much better results than non-personalized algorithms for information retrieval in academic paper corpuses.

Graph-theoretic methods based on social or academic collaboration networks have been used in information retrieval many times before. For example, the Page-Rank algorithm has been used in citation analysis in (Ma, Guan, & Zhao, 2008) as well as, to identify information retrieval researchers’ “authority” in (Kirsch, Gnasa, & Cremers, 2006). Furthermore, graph and link analysis have been used to develop a graph database querying system for information retrieval in social networks (Martinez-Bazan et al., 2007). Graphs depicting scientific collaboration networks have been examined in structure to demonstrate collaboration patterns among different scientific fields including the number of publications that authors write, their co-author network, as well as the distance between scientists in the network among others (Newman, 2001, 2004).

Similarly, an analysis of the proximity of the members of social networks (represented as network vertices) can help estimate the likelihood of new interactions occurring among network members in the future by examining the network topology alone (Liben-Nowell & Kleinberg, 2007). The community structure property of networks in which the vertices of the network form strong groups consisted of nodes with only looser connections has also been examined in order to identify such groups and the boundaries that define them, a concept based on the concept of centrality indices (Girvan & Newman, 2002).

A number of journals from the fields of mathematics and neuroscience covering an 8-year period have been examined in order to identify the structure of the network of co-authors as well as the network’s evolution and topology (Barabsi et al., 2001). The method consisted of empirical measurements that attempt to characterize the specific network at different points in time as well as a model for capturing the network’s evolution in time in addition to numerical simulations. The combination of numerical and analytical results allowed the authors to identify the importance of internal links as far as scaling behaviour and topology of the network are concerned.

The work by Aljaber et al. (2009) identifies important topics covered by journal articles using citation information in combination with the original full-text in order to identify relevant synonymous and related vocabulary to determine the context of a particular publication. This publication representation scheme, when used by the clustering algorithm that is presented in their paper, shows an improvement over both full-text as well as link-based clustering.

Topic modeling integrated into the random walk framework for academic search has been shown to produce promising results and has been the basis of the academic search system ArnetMiner (http://arnetminer.org) (Tang et al., 2008). Relationships between documents in the context of their usage by specific users representing the relevance value of the document in a specific context rather than the document content can be identified by capturing data from user computer interface interactions (Campbell et al., 2007).

Odysci (http://www.odysci.com) is another Web portal application that provides among others, search functionality for scientific publications, currently in the fields of computer science, electrical engineering and related areas. Odysci aims to provide state-of-the-art search algorithms that will allow user to retrieve the most relevant publications in response to a user query.

Some of the aforementioned approaches use the constructed graphs in order to provide improved querying functionality based on knowledge originating from the graph structure and topology while other methods attempt to identify the presence of clusters in the graphs revealing patterns of collaboration. The application of such methods proves to be powerful for both information retrieval systems as well as for systems predicting future collaborations. Standard information retrieval techniques including term frequency are necessary but not sufficient technology for academic paper retrieval. Clustering algorithms prove to be also helpful in cases in order to determine the context of a particular publication by identifying relevant synonyms (or so-called searchonyms, see (Attar & Fraenkel, 1977)) and related vocabulary. It seems that the link structure of the academic papers literature as well as other (primal and derived) properties of the corpus should be used in order to enhance retrieval accuracy in an academic research search engine.

In our proposed system, PubSearch, we use a hierarchical combination of heuristics in order to be able to come up with an improved ranking scheme. At the top level of the hierarchy, we use a custom implementation of the term frequency algorithm that considers combinations of term occurrences in different parts of the publication in order to determine which publications are the most relevant ones with respect to a specific query (abbreviated as “TF heuristic”).

At a second level, our scheme introduces a depreciated citation count method (“DCC heuristic”) that examines the annual citation distribution of a particular publication and depreciates the annual citation count of the publication by an amount relative to the years lapsed since the particular year. The purpose of this heuristic is to promote most recent influential papers as well as to avoid a bias in favour of older publications. For example, a paper published a year ago that has received 15 citations is likely more important than a paper having received 50 citations that was published 10years ago.

Finally, for the third level in our hierarchy, we first developed a focused crawler that extracted the index terms from about 10,000 papers in ACM Portal (from more than 15,000 authors) and constructed two distinct types of graphs that connect the index terms so as to reveal how closely various connected index terms are; details of these graphs’ construction and their properties are discussed in Section 2.2. This information is used to provide an even further improvement to our ranking scheme by determining the degree that a paper is a good match to a particular user query (“MWC heuristic”), as we explain in the next two sub-sections.

The entire system architecture is shown in a Data Flow Diagram in Fig. 1
                        . Overall, the system consists of 7 different processes. Process P1 implements a focused crawler that crawls the ACM Portal for publications in diverse fields of Computer Science and Engineering, in order to select information about the relationships between authors/co-authors and the topics (identified by the index terms) they work on.

This information is analyzed in process P2 (“Analysis of topic associations and connections among authors and co-authors”) and produces a set of edge-weighted graphs that connect index terms with each other. The process P3 (“Construction of maximal weighted cliques”) computes fully-connected subsets of nodes. The subsets form cliques that are an indirect measure of the likelihood that a researcher working in an area described by a subset of the index terms in a clique might also be interested in the other index terms in the same clique. All these cliques can be visualized via the components developed for the implementation of process P7 (“Interactive graph visualizations”) using the Prefuse Information Visualization Toolkit (Heer, Card, & Landay, 2005).

Processes P4–P6 form the heart of the prototype search engine we have developed, which includes a web-based application allowing the user (after registering to the site) to submit their queries. Each user query is then submitted to the ACM Portal and the prototype re-ranks the top 10 ACM Portal results, and then returns the new top ten results to the user. It is important to mention that in the testing and evaluation phase of the system, the results were returned to the user randomly re-ordered, along with a user feedback form via which the system got relevance judgement scores from the user, as explained in Section 4.

As mentioned already, by crawling the ACM Portal web site, we collected an initial corpus of nearly 10,000 publications that are indexed according to the ACM Classification Scheme. Initially a list of authors (with a number of highly cited publications in Computer Science) was provided in order for the application to start collecting all required information that then allowed to run the algorithm that constructed the maximal weighted clique graphs. The following process (schematically shown in Fig. 2
                        ) continues until there are no more authors to process:
                           
                              1.
                              The application fetches from persistence the next author that has not been yet processed.

A search query is constructed and submitted via Google Scholar in order to fetch all publications of the current author. We are querying Google Scholar because—to the best of our knowledge—it has the richest coverage of scientific bibliography and consequently, it has the best estimates of papers’ citation counts.

The application parses all Google Scholar results and selects only those corresponding to ACM Portal pages.

For each of the ACM Portal pages selected in the previous step, the application checks whether the page has already been processed before; If the ACM Portal page has not been already processed then the application fetches and parses the page.

The application extracts and stores in persistence the following publication information:

index terms,

names of all authors,

date of publication,

citation count,

all ACM Portal publications citing the current publication; for each of the citing publications the application performs the step 4. From the current step, the application extracts only the names of authors of those publications.

The author names extracted in the previous step that have not been previously encountered are now stored in persistence and will be processed at some point during the next crawling cycles.

The application sets processed flag to true for the current author.

GOTO step 1.

We constructed two types of graphs, each of different semantic strength. The first type of graph corresponds to the most direct relationship between index terms, namely that of index terms being present in the same publication. So, in a Type I graph, two index terms t
                        1 and t
                        2 are connected by an edge (t
                        1, t
                        2) with weight w if and only if there are exactly w papers in the collection indexed under both index terms t
                        1 and t
                        2. In a Type II graph, two index terms t
                        1 and t
                        2 are connected by an edge (t
                        1, t
                        2) with weight w if and only if there are w distinct authors that have published at least one paper where t
                        1 appears but not t
                        2 and also at least one paper where t
                        2 appears but not t
                        1. The motivation for introducing Type II graphs is the identification of topical associations of “looser type” (compared to graphs of Type I) between different topics of interest. Specifically we aim to identify the number of times that each topic pair appears in different publications of the same author. The idea behind such an association type is that there might in fact be a relation between two seemingly unrelated topics when both topics appear in the general field of scientific interest of a considerable number of authors/papers. For example if a number of authors publish a number of papers both in the fields of cloud computing (or network architectures in general) and concurrent programming, there might be an indirect, yet significant, association among those two fields of interest and this association is the one that a Type II graph aims to address.

We mine heavily-connected clusters in these graphs by computing all maximal weighted cliques in these graphs. Computing all cliques in a graph is of course (Garey & Johnson, 1979) an intractable problem in general (both in time and in space complexity), but in our case, the graphs are of limited size with only up to 300 nodes in each of the graphs, and each graph has only up to 13 node degree. We further reduce the problem complexity by considering edges whose weight exceeds a certain user-defined threshold w
                        0 (by default set to 5). Given these restrictions, the standard Bron-Kerbosch algorithm with pivoting (Bron & Kerbosch, 1973) applied to the restricted graph containing only those edges whose weight exceeds w
                        0 computes all maximally weighted cliques for all graphs in our databases in less than 1min of CPU time on a standard commodity workstation (these graphs can be interactively visualized via a web-based application we have implemented as part of process P7 in Fig. 1, by visiting http://hermes.ait.gr/scholarGraph/index).

The heuristic hierarchy we use for re-ranking search results for a given query is schematically shown in Fig. 3
                        .

At the top-level of our hierarchical heuristic algorithm, we use a custom implementation of the term frequency heuristic. Term frequency (TF) is used as the primary heuristic in our scheme in order to identify the most relevant publications as far as pure content is concerned (for a detailed description of the now standard TF-IDF scheme see for example Manning, Raghavan, and Schutze (2009), or Jackson and Moulinier (2002)). When designing the term frequency heuristic we have taken into consideration the fact that calculating the frequency of all terms individually does not provide an accurate measure for the relevance of a specific publication with respect to a specific query.

In order to overcome this limitation, our implementation identifies the number of occurrences of all combinations of the query terms appearing in close proximity in different sections of each publication. After experimenting with different implementations of the term frequency heuristic, the experiment results showed that this approach performs significantly better in identifying relevant documents than the classical case of the sum of all individual term frequencies.

Our implementation assigns different weights to term occurrences appearing in different sections of the publication. (See Amolochitis, Christou, and Tan (2012) for results from an initial implementation that utilized the standard TF heuristic as described in most textbooks on Information Retrieval.) Term occurrences in the title are more significant than term occurrences in the abstract and similarly, term occurrences in the abstract are more significant than term occurrences in the publication body. Additionally we take into consideration the proximity level of the term occurrences, meaning the distance among encountered terms in different segments of the publication. By proximity level we denote the distance among encountered terms in different segments of the publication and for simplicity we have two proximity levels; sentence and paragraph. Furthermore we distinguish the following two types of term occurrence completeness, complete and partial. A complete term occurrence is when all query terms appear together in the same structural unit (e.g. a sentence) and similarly, a partial occurrence is when a strict but non-empty subset of query terms appears together in the same structural unit. The significance of a specific term occurrence is based on its completeness as well as the proximity level; complete term occurrences are more significant than partial ones and similarly term occurrences at sentence level are more significant than term occurrences at paragraph level.

Before discussing the details of our custom TF scheme, a word is in order to justify the omission of the “Inverse Document Frequency” (IDF) part from our scheme. The reason for omitting IDF is that we cannot maintain a full database of academic publications such as the ACM Digital Library (as we do not have any legal agreements with ACM) but instead fetch the results another engine provides (e.g. ACM Portal) and simply work with those results. It would be expected then that computing the IDF score for only the limited result-set that another engine returns, would not improve the results of our proposed scheme, and initial experiments with the TF scheme proved this intuition is correct.

We now return to the formal description of our custom TF scheme. Let Q
                        ={T
                        1,…,
                        Tn
                        } be the set of all terms in the original query, and let 
                           
                              O
                              (
                              u
                              )
                              ⊆
                              Q
                           
                         be the subset of terms in Q appearing together in a document structural unit u (e.g. a sentence, or a paragraph in the document). We define the term occurrence score for the unit u to be s(u)=|O(u)|/|Q|. In case of a “complete occurrence” (meaning all query terms appear in the unit) clearly, s(u)=1 since O
                        =
                        Q. Method calcTermOccurenceScore(O,Q) implements this formula.

Now, let T denote the set of all sections of a paper, P the set of all paragraphs in a section and S the set of all sentences in a paragraph. The method splitSectionIntoParagraphs(Section) splits the specified section into a set of paragraphs. Similarly splitParagraphIntoSentences(Paragraph) splits the specified paragraph into a set of sentences. The method findAllUniqueTermOccurInSentence(Sentence, Q) returns the subset of all terms in Q that appear in the specified Sentence. Similarly findAllUniqueTermOccurInAllSentences(S,Q) returns the set of all terms in Q that appear in at least one sentence member of S. The method noCompleteMatchExists(S) evaluates to true if and only if there exists no sentence in S that contains all the query terms in Q.

We have also introduced a set of weight values to apply a different significance to different term occurrence types appearing: (i) in different publication sections: tWeight represents the term occurrence weight at different publication sections (title, abstract, body), and (ii) in different proximity levels: sWeight represents the term occurrence weight at sentence level, whereas pWeight represents the term occurrence weight at paragraph level. The method determineSectionWeight(t) determines the type of the specified section (title, abstract or body) and returns a different weight score that should be applied in each case. All weight values have been determined empirically after experimenting with different weight value ranges. Overall, our term-frequency heuristic is implemented as follows:

Algorithm calculateTF(Publication d, Query q)
                           
                              
                                 
                                 
                                    
                                       1. Let S←{}, T←{}, P←{}, O←{}, tf←0
                                    
                                    
                                       2. Set T←splitPublicationIntoSections(d)
                                    
                                    
                                       3. foreach section t in T do
                                    
                                    
                                       
                                          a. Let sectionScore←0
                                    
                                    
                                       
                                          b. Set P←splitSectionIntoParagraphs(t)
                                    
                                    
                                       
                                          c. Let scoreInSegment←0
                                    
                                    
                                       
                                          d. foreach paragraph p in P do
                                    
                                    
                                       
                                          
                                          i. Set S←splitParagraphIntoSentences(p)
                                    
                                    
                                       
                                          
                                          ii. Let sentenceScore←0
                                    
                                    
                                       
                                          
                                          iii. foreach sentence s in S do
                                    
                                    
                                       
                                          
                                          
                                          1. Set O←findAllUniqueTermOccurInSentence(s)
                                    
                                    
                                       
                                          
                                          
                                          2. Let sScore←calcTermOccurrenceScore(O, Q)
                                    
                                    
                                       
                                          
                                          
                                          3. Set sentenceScore←sentenceScore + sScore
                                       
                                    
                                    
                                       
                                          
                                          iv. endfor
                                    
                                    
                                       
                                          
                                          v. Set sentenceScore←sentenceScore · sWeight
                                       
                                    
                                    
                                       
                                          
                                          vi. Let paragraphScore←0
                                    
                                    
                                       
                                          
                                          vii. Let partialMatch← noCompleteMatchExists(S)
                                    
                                    
                                       
                                          
                                          viii. if (partialMatch === true) then
                                    
                                    
                                       
                                          
                                          
                                          1. Set O←findAllUniqueTermOccurrInAllSentences(S, Q)
                                    
                                    
                                       
                                          
                                          
                                          2. Set paragraphScore←calcTermOccurrenceScore(O, Q)
                                    
                                    
                                       
                                          
                                          ix. else Set paragraphScore←1
                                    
                                    
                                       
                                          
                                          x. endif
                                    
                                    
                                       
                                          
                                          xi. Set paragraphScore ← paragraphScore · pWeight
                                       
                                    
                                    
                                       
                                          
                                          xii. Set scoreInSegment←sentenceScore + paragraphScore
                                       
                                    
                                    
                                       
                                          e. endfor
                                    
                                    
                                       
                                          f. Let tWeight←determineSectionWeight(t)
                                    
                                    
                                       
                                          g. Set sectionScore← tWeight · scoreInSegment
                                       
                                    
                                    
                                       
                                          h. Set tf←tf + sectionScore
                                       
                                    
                                    
                                       4. endfor
                                    
                                    
                                       5. return tf
                                       
                                    
                                    
                                       6. end
                                    
                                 
                              
                           
                        
                     

After calculating the total query term frequency for each publication, the algorithm groups all publications with similar term frequency scores into buckets of specified range. This grouping of the publications allows bringing together publications with similar term frequency scores in order to apply further heuristics to determine an improved ranking scheme. Results placed in higher range term frequency buckets are promoted at the expense of publications placed in lower term frequency buckets.

At the second level of our hierarchical ranking scheme, the results within each bucket created in the previous step are ordered according to a depreciated citation count score. Specifically we analyze the annual citation distribution of a particular publication examining the number of citations that a paper has received within a specific year. We analyze all citations of a particular paper via Google Scholar and for each citing publication we consider the date of publication. After all citing publications are examined we create a distribution of the total citation count that the cited publication received annually. Our formula then depreciates each annual citation count based on the years lapsed since the publication date. After all annual depreciation scores are calculated then the scores are summed and produce a total depreciation count score for a particular publication obeying the formulae
                           
                              (1)
                              
                                 
                                    
                                       
                                       
                                          
                                             
                                                
                                                   c
                                                
                                                
                                                   p
                                                
                                             
                                             =
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      j
                                                      =
                                                      y
                                                      (
                                                      p
                                                      )
                                                   
                                                   
                                                      n
                                                   
                                                
                                             
                                             
                                                
                                                   n
                                                
                                                
                                                   j
                                                   ,
                                                   p
                                                
                                             
                                             
                                                
                                                   d
                                                
                                                
                                                   j
                                                   ,
                                                   p
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             
                                                
                                                   d
                                                
                                                
                                                   j
                                                   ,
                                                   p
                                                
                                             
                                             =
                                             1
                                             -
                                             
                                                
                                                   1
                                                   +
                                                   tanh
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  n
                                                                  -
                                                                  j
                                                                  -
                                                                  10
                                                               
                                                               
                                                                  4
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where cp
                         is the total (time-depreciated) citation-based score for paper p, nj
                        
                        ,
                        
                           p
                         is the total number of citations that the paper has received in a particular year j, n is the current year, dj
                        
                        ,
                        
                           p
                         is the depreciation factor for the particular year j and y(p) is the publication year of the paper p.

As already mentioned, our intention is to identify recent publications with high impact in their respective fields and promote them in the ranking order at the expense of older publications that might have a higher citation count but a considerable number of years have passed since the date of publication. In order to achieve this we determine the significance of a publication’s citation count as a function of the number of citations received depreciated by the years lapsed since its publication date. Once publications have been sorted in decreasing order of the criterion cp
                        , we further partition them into second-level buckets of like-score publications.

Within each bucket of the second-level heuristic, we further order the results by examining each publication’s index terms and calculate their degree of matching with all topical maximal weighted cliques, the off-line computation of which has already been described in Section 2.2. Additionally we assign specific weight values to the calculated cliques based on certain different characteristics such as the types of associations they represent and the time period they belong to. The system calculates for each publication a total clique matching score which corresponds to the sum of matching score of the publication’s index terms with all maximal weighted cliques. The calculation details are as follows.

Let C be the set of all cliques to examine. Let ci
                         denote the total number of index terms in clique i. Let d denote the total number of index terms of publication p and pi
                         denote the total number of index terms of publication p that belong to clique i; for each clique to examine, the system calculates the matching degree of all publication index terms with those of the clique. In cases of a perfect match (meaning that all index terms of i appear as index terms of p) in order to avoid bias towards publications with a big number of index terms against cliques with a small number of index terms we calculate the percentage match mi
                        
                        =
                        ci
                        /d. For all remaining cases (non-perfect match) the percentage matching is calculated as mi
                        
                        =min(ci
                        /d,
                        pi
                        /ci
                        ). If mi
                        
                        >
                        t where t is a configurable threshold for the accepted matching level (empirically set at t
                        =0.75) the process continues, else the system stops processing the current clique and moves to the next one. In case that the matching level is above t the system calculates a weight score representing the overall value of the association of p with ci
                         as the product wp
                        
                        ,
                        
                           i
                        
                        =
                        wi
                        
                        ×
                        mi
                        
                        ×
                        esi
                        
                        ×
                        aci
                         where wi
                         is the weight score of the examined maximal weighted clique i, and aci
                         is a score related to the association type that the current graph that the current clique belongs to represents (aci
                        
                        =1.0 for association Type I, aci
                        
                        =0.6 for Type II). Finally, esi
                         is an exponential smoothing factor that depreciates cliques of graphs covering older periods in order to promote more recent ones. Since each type of graph has a different significance, we consider recent graphs of stronger association types as more significant and thus we assign greater value to maximal weighted cliques of such graphs.

The algorithm calculates for each publication a total clique matching score Sp
                         which corresponds to the sum of matching score of the publication’s index terms with all maximal weighted cliques and determines the final ranking of the results accordingly:
                           
                              (2)
                              
                                 
                                    
                                       S
                                    
                                    
                                       p
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          ∈
                                          C
                                       
                                    
                                 
                                 
                                    
                                       w
                                    
                                    
                                       p
                                       ,
                                       i
                                    
                                 
                              
                           
                        
                     

The total clique matching score determines the order of the results within the current second level bucket and eventually determines the final ranking of the results.

As previously mentioned we have developed a meta-search engine application in order to evaluate our ranking algorithm. Registered users can submit a number of queries via our meta-search engine’s user interface. The search interface allows users to use quotes for specifying exact sequence of terms in cases that it is applicable for improving query accuracy for both PubSearch and ACM Portal.

For each query in the processing queue, our system queries ACM Portal using the exact query phrase submitted by the user and crawls ACM Portal’s result page in order to extract the top 10 search results. The top ten search results as well as the default ranking order provided by ACM Portal are stored. For each of the returned results, our system automatically crawls each publication’s summary page in order to extract all required information. Additionally for each of the returned results the system queries Google Scholar to extract the total number of citations and find a downloadable copy of the full publication text if possible. The publication full-text has been obtained for approximately 83% of the search results. There are several cases of results where the actual text is not available, or exists only as image scans in PDF format which require special OCR software to be parsed—a feature we have not implemented. For the cases that the full-text is not available our system takes into consideration only all available content from the result summary page, including the title and abstract (the index terms are available from the result summary page).

When all available publication information is gathered, the system executes our own ranking algorithm with the goal of improving the default rank by re-ranking the default top ten results provided by ACM Portal. The rank order generated by our algorithm is stored in the database and when the process is complete the query status is updated and the user is notified in order to provide feedback. The user is presented with the default top ten results produced by ACM Portal in a random order and is asked to provide feedback based on the relevance of each search result with respect to the user’s preference and overall information need. The provided relevance feedback score for each result is used for evaluating the overall feedback score of both ACM Portal as well as our own algorithm, since both systems attempt to process the same set of results. We use a 1–5 feedback score scheme where 1 corresponds to “least relevant” and 5 corresponds “most relevant”.

In order to compare an IR system’s ranking performance, we use two commonly encountered metrics: (i) Normalized Discounted Cumulative Gain (NDCG) and (ii) Expected Reciprocal Rank (ERR). We also introduce a new metric, the lexicographic ordering metric (LEX), that can be considered a more extreme version of the ERR metric.

Normalized Discounted Cumulative Gain (Järvelin & Kekäläinen, 2000) is a metric commonly used for evaluating ranking algorithms in cases where graded relevance judgments exist. Discounted Cumulative Gain (DCG) measures the usefulness of a document based on its rank position. DCG is calculated as follows:
                        
                           (3)
                           
                              
                                 
                                    DCG
                                 
                                 
                                    p
                                 
                              
                              =
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       p
                                    
                                 
                              
                              
                                 
                                    
                                       
                                          2
                                       
                                       
                                          f
                                          (
                                          
                                             
                                                p
                                             
                                             
                                                i
                                             
                                          
                                          )
                                       
                                    
                                    -
                                    1
                                 
                                 
                                    
                                       
                                          log
                                       
                                       
                                          2
                                       
                                    
                                    (
                                    1
                                    +
                                    i
                                    )
                                 
                              
                           
                        
                     where f(pi
                     ) is the relevance judgment (user relevance feedback) of the result at position
                     i. The DCG score is then normalized by dividing it with its ideal score which is the DCG score for the sorted (on descending order) result list according to the relevance scores:
                        
                           (4)
                           
                              n
                              
                                 
                                    DCG
                                 
                                 
                                    p
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          DCG
                                       
                                       
                                          p
                                       
                                    
                                 
                                 
                                    
                                       
                                          IDCG
                                       
                                       
                                          p
                                       
                                    
                                 
                              
                           
                        
                     
                  

The term IDCG
                        p
                      (acronym for “Ideal DCG till position p”) is the DCG
                        p
                      value of the result list ordered in descending order of relevance feedback, so that in a perfect ranking algorithm nDCG
                        p
                      will always equal 1.0 for all positions of the list.

Expected Reciprocal Rank (Chapelle, Metlzer, et al., 2009) is a metric that attempts to compute the expectation of the inverse of the rank position in which the user locates the document they need (so that when for example ERR≃0.2 the required document should be found near the 5th position in the list of search results), assuming that after the user locates the document they need, they stop looking further down the list of results. ERR is defined as follows:
                        
                           (5)
                           
                              ERR
                              (
                              q
                              )
                              =
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       r
                                       =
                                       1
                                    
                                    
                                       n
                                    
                                 
                              
                              
                                 
                                    
                                       
                                          R
                                       
                                       
                                          r
                                       
                                    
                                 
                                 
                                    r
                                 
                              
                              
                                 
                                    
                                       ∏
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       r
                                       -
                                       1
                                    
                                 
                              
                              (
                              1
                              -
                              
                                 
                                    R
                                 
                                 
                                    i
                                 
                              
                              )
                              
                              
                                 
                                    R
                                 
                                 
                                    i
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          2
                                       
                                       
                                          f
                                          (
                                          
                                             
                                                p
                                             
                                             
                                                i
                                             
                                          
                                          )
                                          -
                                          1
                                       
                                    
                                    -
                                    1
                                 
                                 
                                    
                                       
                                          2
                                       
                                       
                                          
                                             
                                                f
                                             
                                             
                                                max
                                             
                                          
                                       
                                    
                                 
                              
                              
                              i
                              =
                              1
                              ,
                              …
                              ,
                              n
                           
                        
                     where f
                     max is the maximum possible user relevance feedback score (in our case, 5).

Besides the common NDCG and ERR metrics, we also calculate a total feedback score LEX(q) for the (re-) ranked results of any particular query q by following a lexicographic ordering approach to produce a weighted sum of all independent feedback result scores:
                        
                           (6)
                           
                              LEX
                              (
                              q
                              )
                              =
                              
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          n
                                       
                                    
                                    
                                       
                                          a
                                       
                                       
                                          i
                                       
                                    
                                    
                                       
                                          f
                                       
                                       
                                          norm
                                       
                                    
                                    (
                                    
                                       
                                          p
                                       
                                       
                                          i
                                       
                                    
                                    )
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          n
                                       
                                    
                                    
                                       
                                          a
                                       
                                       
                                          i
                                       
                                    
                                 
                              
                           
                        
                     where n is the number of results, δf
                     
                     =(f
                     max
                     −1)−1, 
                        
                           a
                           =
                           
                              
                                 
                                    
                                       δ
                                    
                                    
                                       f
                                    
                                 
                              
                              
                                 1
                                 +
                                 
                                    
                                       δ
                                    
                                    
                                       f
                                    
                                 
                              
                           
                        
                      and 
                        
                           
                              
                                 f
                              
                              
                                 norm
                              
                           
                           (
                           
                              
                                 p
                              
                              
                                 i
                              
                           
                           )
                           =
                           
                              
                                 f
                                 (
                                 
                                    
                                       p
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 -
                                 1
                              
                              
                                 
                                    
                                       f
                                    
                                    
                                       max
                                    
                                 
                                 -
                                 1
                              
                           
                        
                      is the normalized relevance feedback provided by the user for the publication pi
                      with values in the set {0,
                     δf
                     ,2δf
                     ,…,1}. In our case, δf
                     
                     =0.25, a
                     =0.2. In this way, in any two rankings of some results list produced by two different schemes, the scheme that assigns a higher score for the highest ranked publication always receives a better overall score LEX(q) regardless of how good or bad the publications in lower positions score. To see why this is so, ignoring the normalizing denominator constant in (6), and without loss of generality, we must simply show that if two result-lists (r
                     1,1,…,
                     r
                     1,
                     
                        n
                     ) and (r
                     2,1,…,
                     r
                     2,
                     
                        n
                     ) for the same query q get normalized feedback scores (fnorm
                     (ri
                     
                     ,1),…,
                     fnorm
                     (ri
                     
                     ,
                     
                        n
                     )),
                     i
                     =1,2 and fnorm
                     (r
                     1,1)>
                     fnorm
                     (r
                     2,1), then the LEX score of the first result list will always be greater than the LEX score of the second result list. Given that if two normalized feedback scores are different, their absolute difference will be at least equal to δf
                     , and at most equal to 1, we need to show that
                        
                           (7)
                           
                              
                                 
                                    δ
                                 
                                 
                                    f
                                 
                              
                              a
                              >
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       2
                                    
                                    
                                       n
                                    
                                 
                              
                              
                                 
                                    a
                                 
                                 
                                    i
                                 
                              
                              [
                              
                                 
                                    f
                                 
                                 
                                    norm
                                 
                              
                              (
                              
                                 
                                    r
                                 
                                 
                                    2
                                    ,
                                    i
                                 
                              
                              )
                              -
                              
                                 
                                    f
                                 
                                 
                                    norm
                                 
                              
                              (
                              
                                 
                                    r
                                 
                                 
                                    1
                                    ,
                                    i
                                 
                              
                              )
                              ]
                           
                        
                     for all possible values of the quantities fnorm
                     (r
                     1,
                     
                        i
                     ),
                     fnorm
                     (r
                     2,
                     
                        i
                     ),
                     i
                     =2,…,
                     n. Taking into account that 
                        
                           
                              
                                 f
                              
                              
                                 norm
                              
                           
                           (
                           
                              
                                 r
                              
                              
                                 2
                                 ,
                                 i
                              
                           
                           )
                           -
                           
                              
                                 f
                              
                              
                                 norm
                              
                           
                           (
                           
                              
                                 r
                              
                              
                                 1
                                 ,
                                 i
                              
                           
                           )
                           ⩽
                           1
                           
                           ∀
                           i
                           =
                           2
                           ,
                           …
                           ,
                           n
                        
                     , if the value a is such so that 
                        
                           
                              
                                 δ
                              
                              
                                 f
                              
                           
                           a
                           >
                           
                              
                                 ∑
                              
                              
                                 i
                                 =
                                 2
                              
                              
                                 n
                              
                           
                           
                              
                                 a
                              
                              
                                 i
                              
                           
                           =
                           
                              
                                 
                                    
                                       a
                                    
                                    
                                       n
                                       +
                                       1
                                    
                                 
                                 -
                                 
                                    
                                       a
                                    
                                    
                                       2
                                    
                                 
                              
                              
                                 a
                                 -
                                 1
                              
                           
                        
                      then the required inequality (7) will hold for all possible values of the quantities fnorm
                     (r
                     1,
                     
                        i
                     ),
                     fnorm
                     (r
                     2,
                     
                        i
                     ),
                     i
                     =2,…,
                     n. But the last inequality can be written as 
                        
                           
                              
                                 δ
                              
                              
                                 f
                              
                           
                           >
                           
                              
                                 a
                                 (
                                 1
                                 -
                                 
                                    
                                       a
                                    
                                    
                                       n
                                       -
                                       1
                                    
                                 
                                 )
                              
                              
                                 1
                                 -
                                 a
                              
                           
                        
                      and it will always hold if 
                        
                           
                              
                                 δ
                              
                              
                                 f
                              
                           
                           ⩾
                           
                              
                                 a
                              
                              
                                 1
                                 -
                                 a
                              
                           
                        
                      (since an
                     
                     −1
                     ∊(0,1)) so by choosing 
                        
                           
                              
                                 δ
                              
                              
                                 f
                              
                           
                           =
                           
                              
                                 a
                              
                              
                                 1
                                 -
                                 a
                              
                           
                           ⇔
                           a
                           =
                           
                              
                                 
                                    
                                       δ
                                    
                                    
                                       f
                                    
                                 
                              
                              
                                 1
                                 +
                                 
                                    
                                       δ
                                    
                                    
                                       f
                                    
                                 
                              
                           
                           =
                           0.2
                        
                      the lexicographic ordering property always holds regardless of the result list size or feedback values. Clearly, it always holds that LEX(q)∊[0,1], with the value 1 being assigned to a result list where all papers were assigned the value f
                     max whereas if the user assigns the lowest possible score (1) for all papers in the results list, the LEX score for the query will be zero.

Notice some further useful properties of the LEX score: the expected value of the LEX score of a list (of an arbitrary number of search results) containing values randomly selected from the range {f
                     min,…,
                     f
                     max} (where f
                     min is the lowest possible relevance judgment score, in our case 1) is 0.5. More importantly, LEX, being a weighted linear combination of the normalized relevance judgments of an ordered list of search results that assigns decreasing value weights to the list items, will be above the average of the normalized relevance judgment scores 
                        
                           
                              
                                 
                                    
                                       f
                                    
                                    
                                       ¯
                                    
                                 
                              
                              
                                 norm
                              
                           
                        
                      if the search engine promotes higher scoring items to the top of the list, and vice versa, the LEX score will be below the value 
                        
                           
                              
                                 
                                    
                                       f
                                    
                                    
                                       ¯
                                    
                                 
                              
                              
                                 norm
                              
                           
                        
                      if the search engine fails to promote the better results towards the top of the list, thus providing a statistical decision criterion (
                        
                           E
                           [
                           LEX
                           ]
                           >
                           E
                           [
                           
                              
                                 
                                    
                                       f
                                    
                                    
                                       ¯
                                    
                                 
                              
                              
                                 norm
                              
                           
                           ]
                        
                     ) on whether a search engine actually “tends” to order its search results in descending order of user relevance judgment. To be more precise, if the search engine produces ordered results lists with the property that the relevance judgment scores X
                     1,…,
                     Xn
                      for the results can be considered as random variables satisfying the stochastic order Xn
                     
                     ⩽
                     
                        stXn
                     
                     −1
                     ⩽
                     
                        st
                     …⩽
                     
                        stX
                     1 (Shaked & Shanthikumar, 2007), then 
                        
                           E
                           [
                           LEX
                           (
                           
                              
                                 X
                              
                              
                                 1
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 X
                              
                              
                                 n
                              
                           
                           )
                           ]
                           >
                           E
                           [
                           
                              
                                 
                                    
                                       X
                                    
                                    
                                       ¯
                                    
                                 
                              
                              
                                 norm
                              
                           
                           ]
                        
                     . We have verified the validity of this criterion through extensive simulation as well. We suspect that the stronger condition 
                        
                           
                              
                                 
                                    
                                       X
                                    
                                    
                                       ¯
                                    
                                 
                              
                              
                                 norm
                              
                           
                           
                              
                                 ⩽
                              
                              
                                 st
                              
                           
                           LEX
                           (
                           
                              
                                 X
                              
                              
                                 1
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 X
                              
                              
                                 n
                              
                           
                           )
                        
                      holds, but we do not have a proof of this conjecture.

The LEX scoring scheme can be considered as a more extreme version of the ERR and NDGC metrics and is inspired from the fact that people always place much more importance to the top results (and usually judge the whole list of results by the quality of the top 2–3 results) that are returned from any search engine than on lower ranked results—see also the related work by Breese, Heckerman, and Kadie (1998) on the R-Score metric on ranking measures. This is probably due to the very strong faith of users in the ability of search engines to rank results correctly and place the most relevant results on top, a faith that (if it exists) apparently does not have solid grounding with regards to academic search engines—at least, not yet.

@&#EXPERIMENTAL RESULTS@&#

In an initial training phase, the results of a limited set of relevance feedback scores from a limited base of five volunteer users were used in order to optimize the bucket ranges of our heuristic hierarchical ranking scheme as well as the values for the parameters tWeight, pWeight, and sWeight for the proposed TF-scheme. The bucket ranges are as follows:
                        
                           •
                           For the TF-heuristic, we always compute exactly 10 buckets by first computing the proposed TF metric for each publication and then we normalize the calculated scores in the range [0,1] in a linear transformation that assigns the score 1 to the publication with the max. calculated TF score, and then we “bucketize” the publications in the 10 intervals [0,0.1], (0.1,0.2],…(0.9,1].

For the 2nd level-heuristic, the bucket range is set to 5.20.

Values for the other parameters are set as follows: sWeight
                     =15.25, pWeight
                     =4.10, and tWeighttitle
                     
                     =125.50, tWeightabstract
                     
                     =45.25, tWeightbody
                     
                     =5.30. These values were obtained through the application of an optimization procedure based on standard meta-heuristics (namely a GA process implemented in the Open-Source popt4jlib library developed by one of the authors, obtainable through http://www.ait.edu.gr/ait_web_site/faculty/ichr/popt4jlib.zip) with the objective function being the average LEX score of the training-set queries.

Given these parameters, we proceeded into testing the system by processing 58 new queries that were submitted by 15 different users (other than the authors of the paper) specializing in different areas of computer science and electrical & computer engineering. The users were selected based on their expertise in different areas of computer science and electrical engineering and they are researchers of different levels from the authors’ universities. Each of our test users submitted a number of queries and provided feedback for all produced query results without knowing which algorithm produced each ranking. We used the three metrics mentioned before (NDCG, ERR, and LEX) to evaluate the quality of our ranking algorithm.

Our ranking approach, PubSearch, compares very well with ACM Portal, and in fact outperforms ACM Portal in most query evaluations as the tests reveal using all three metrics. We illustrate the performance of each system in Table 1
                        .


                        Table 2
                         shows the average score of each system using the three different metrics. PubSearch performs much better than ACM Portal in most of the 58 queries used to evaluate our system under all metrics.

On average, the percentage gap of performance between PubSearch and ACM Portal in terms of LEX metric is 907.5%(!), in terms of NDCG is 11.94%, and in terms of ERR, the average gap is 77.5%. The large average gap in the LEX metric is due to the fact that for some queries, ACM Portal produces a LEX score close to zero, whereas PubSearch re-orders the results so that it produces a LEX score close to 1, leading to huge percentage deviations for such queries.

Even though it is clear to the naked eye, statistical analysis using the t-test, the sign test and the signed rank test all show that the performance difference between the two systems is statistically significant at the 95% confidence level for all performance metrics. In Table 10 (Appendix) we present an analytical comparison of the evaluation scores of the two systems using the three metrics.

To highlight the difference of the ranking orders produced by the two systems, consider query#1 (‘query privacy “sensor networks”’): The ACM Portal results list was given the following relevance judgement by the user: 1,1,2,3,2,1,1,4,3,5. PubSearch re-orders the ACM Portal results in a sequence that corresponds to the following relevance judgement: 5,4,3,3,1,2,2,1,1,1. PubSearch produces the best possible ordering of the given search results (with the exception of the document in 5th position that should have been placed in 7th position). Similarly, consider query #46 (‘resource management grid computing’): ACM Portal orders its top ten results in a sequence that received the following scores: 1,1,3,3,4,4,4,5,1,3. PubSearch on the other hand re-orders the list of results so that the sequence’s scores appear as follows: 5,4,3,4,4,3,1,1,3,1, which is a much improved ordering than ACM Portal.

We have compared the performance of our hierarchical heuristic scheme using our custom implementation of the TF heuristic against using the traditional Boolean method embedded in our 3-level heuristic hierarchy. Using the traditional method results in a LEX average score of 0.499, an average NDCG score of 0.93, and an average ERR score of 0.514, resulting in an average LEX percentage gap of 89.9%, an average NDCG percentage gap of 5.42%, and an average ERR percentage gap of 43.3%, all statistically significant. The above results clearly show that our custom TF method significantly outperforms the traditional TF heuristic, even when embedded within our hierarchical scheme.

Statistical analysis using the t-test, the sign-test and the signed-rank test all show that the effect of the third heuristic in the hierarchy is significant. The gap is small but statistically significant so all heuristics in the hierarchy are needed to obtain the best possible score for all metrics considered.

In Fig. 4
                         we show the performance of our proposed heuristic configuration when comparing it with different hierarchies of heuristics. Each chart presents the average performance of each heuristic configuration under a different metric. Note that we specify the different heuristic hierarchies by separating each heuristic in a hierarchy with a slash (“/”) character. For each hierarchy, each left-side heuristic argument is higher in the suggested hierarchy than its right-side argument. We consider the following configurations: (1) TF/DCC/MWC (the proposed scheme), (2) TF/DCC, (3) TF, (4) DCC, (5) MWC, and finally, (6) TF/MWC.

It can be seen from the figure that our proposed configuration is the best performing configuration in terms of all metrics considered.

The percentage difference between the proposed full PubSearch configuration (TF/DCC/MWC) and applying the proposed TF heuristic alone is 3.26% for the LEX metric, 1.54% for the NDCG metric, and 5.96% for the ERR metric. Furthermore, statistical analysis using the t-test, sign test, and signed-rank test show that the differences between TF/DCC/MWC and TF heuristic alone are statistically significant for all the metrics considered at the 95% confidence level.

To make the comparison with TF-IDF methods clearer, we also compare PubSearch against the standard Okapi BM25 weighting scheme (Sparck Jones, Walker, & Robertson, 2000). However, in our comparison, since as we have already mentioned we do not maintain an academic paper database but instead simply re-rank the results returned by other engines, when computing the BM25 score for a result list, we assume that the entire database consists of the returned results of the base engine only (ACM Portal’s top ten results for a given query). As expected, the results from BM25 are quite inferior to those obtained by PubSearch. The results are summarized in Table 3
                        , where we show the average score obtained for the 58 queries on each metric for each of the two systems.

The average percentage difference between PubSearch and Okapi BM25 in terms of the LEX metric is 1898% (due to BM25 producing a LEX score of less than 0.004 for some queries while for the same query PubSearch producing scores of more than 0.7) in terms of the NDCG metric is 20.9% and in terms of the ERR metric it reaches 144%. Statistical analysis (though not really needed) in terms of t-test, sign test and signed rank test shows these differences to be very significant. This result is not surprising as BM25 is a generic non-binary information retrieval model that has no specific domain knowledge about academic publications.

We also compare our approach against linearly combining the three heuristics (TF, DCC, and MWC) via standard Linear Regression. To obtain the linear regression coefficients, we apply standard Linear Regression using the results of the same training set we used to determine the bucket sizes for our proposed hierarchical scheme. Having obtained those coefficients, we then obtain a score for each publication returned in a search results list, which is then used to sort the initial list in descending order of the new score. We tested the linear regression method on the same set of 58 queries in our test-set: the results are shown in Table 4
                            below. PubSearch outperforms the linear combination of the heuristics, and the t-test, and sign-test reveal that the differences are statistically significant at the 95% confidence level, for all metrics considered.
                        

We also compare our proposed approach against a “fusion” scheme (Kuncheva, 2004) where for each heuristic h (that can be TF, DCC, or MWC) we compute a score Ah
                            that represents an “inverse accuracy” score of the heuristic in obtaining the best possible sequence of a query’s search results (measured against the training set query data).

This score Ah
                            is computed as follows: assume the search results for a query q ranked in descending order of relevance feedback by the user are as follows: dq
                           
                           ,1,…,
                           dq
                           
                           ,
                           
                              n
                            having relevance feedback scores fq
                           
                           ,1
                           ⩾
                           fq
                           
                           ,2
                           ⩾⋯⩾
                           fq
                           
                           ,
                           
                              n
                           . Now, assume the heuristic h scores the documents so that they are ranked according to the following order: 
                              
                                 
                                    
                                       d
                                    
                                    
                                       q
                                       ,
                                       
                                          
                                             h
                                          
                                          
                                             1
                                          
                                       
                                    
                                 
                                 ,
                                 
                                    
                                       d
                                    
                                    
                                       q
                                       ,
                                       
                                          
                                             h
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    
                                       d
                                    
                                    
                                       q
                                       ,
                                       
                                          
                                             h
                                          
                                          
                                             n
                                          
                                       
                                    
                                 
                              
                           . Define the quantity gq
                           
                           ,
                           
                              i
                            as follows:
                              
                                 
                                    
                                       
                                          g
                                       
                                       
                                          q
                                          ,
                                          i
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      0
                                                   
                                                   
                                                      if
                                                      
                                                      
                                                         
                                                            f
                                                         
                                                         
                                                            q
                                                            ,
                                                            i
                                                         
                                                      
                                                      =
                                                      
                                                         
                                                            f
                                                         
                                                         
                                                            q
                                                            ,
                                                            
                                                               
                                                                  h
                                                               
                                                               
                                                                  i
                                                               
                                                            
                                                         
                                                      
                                                      
                                                         min
                                                      
                                                      {
                                                      |
                                                      i
                                                      -
                                                      
                                                         
                                                            j
                                                         
                                                         
                                                            ¯
                                                         
                                                      
                                                      |
                                                      ,
                                                      |
                                                      i
                                                      -
                                                      
                                                         
                                                            j
                                                         
                                                         
                                                            ̲
                                                         
                                                      
                                                      |
                                                      }
                                                      :
                                                      
                                                         
                                                            j
                                                         
                                                         
                                                            ¯
                                                         
                                                      
                                                      =
                                                      
                                                         max
                                                      
                                                      {
                                                      j
                                                      |
                                                      
                                                         
                                                            f
                                                         
                                                         
                                                            q
                                                            ,
                                                            j
                                                         
                                                      
                                                      =
                                                      
                                                         
                                                            f
                                                         
                                                         
                                                            q
                                                            ,
                                                            
                                                               
                                                                  h
                                                               
                                                               
                                                                  i
                                                               
                                                            
                                                         
                                                      
                                                      }
                                                      ,
                                                      =
                                                      
                                                         
                                                            j
                                                         
                                                         
                                                            ̲
                                                         
                                                      
                                                      
                                                         min
                                                      
                                                      {
                                                      j
                                                      |
                                                      
                                                         
                                                            f
                                                         
                                                         
                                                            q
                                                            ,
                                                            j
                                                         
                                                      
                                                      =
                                                      
                                                         
                                                            f
                                                         
                                                         
                                                            q
                                                            ,
                                                            
                                                               
                                                                  h
                                                               
                                                               
                                                                  i
                                                               
                                                            
                                                         
                                                      
                                                      }
                                                      else
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           We define 
                              
                                 
                                    
                                       A
                                    
                                    
                                       q
                                       ,
                                       h
                                    
                                 
                                 =
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       n
                                    
                                 
                                 
                                    
                                       g
                                    
                                    
                                       q
                                       ,
                                       i
                                    
                                 
                              
                           . Clearly, Aq
                           
                           ,
                           
                              h
                           
                           ⩾0, and Aq
                           
                           ,
                           
                              h
                           
                           =0 if and only if the heuristic h obtains a perfect sorting of the result set of query q (as indicated by the user relevance judgements). The measured inverse accuracy of a heuristic h on the training set Qtrain
                            is then defined as 
                              
                                 
                                    
                                       A
                                    
                                    
                                       h
                                    
                                 
                                 =
                                 
                                    
                                       ∑
                                    
                                    
                                       q
                                       ∈
                                       
                                          
                                             Q
                                          
                                          
                                             train
                                          
                                       
                                    
                                 
                                 
                                    
                                       A
                                    
                                    
                                       q
                                       ,
                                       h
                                    
                                 
                              
                           . We compare PubSearch against an ensemble of the three heuristics from the set H
                           ={TF,
                           DCC,
                           MWC} that works as follows: each heuristic in the set of heuristics produces a re-ranked order of results 
                              
                                 
                                    
                                       d
                                    
                                    
                                       q
                                       ,
                                       
                                          
                                             h
                                          
                                          
                                             1
                                          
                                       
                                    
                                 
                                 ,
                                 
                                    
                                       d
                                    
                                    
                                       q
                                       ,
                                       
                                          
                                             h
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    
                                       d
                                    
                                    
                                       q
                                       ,
                                       
                                          
                                             h
                                          
                                          
                                             n
                                          
                                       
                                    
                                 
                              
                           . The final ensemble result is the list of results sorted in ascending order of the combined value
                              
                                 (8)
                                 
                                    
                                       
                                          r
                                       
                                       
                                          d
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         ∑
                                                      
                                                      
                                                         h
                                                         ∈
                                                         H
                                                      
                                                   
                                                
                                                
                                                   
                                                      (
                                                      
                                                         
                                                            A
                                                         
                                                         
                                                            h
                                                         
                                                      
                                                      +
                                                      1
                                                      )
                                                   
                                                   
                                                      -
                                                      1
                                                   
                                                
                                             
                                          
                                       
                                       
                                          -
                                          1
                                       
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             h
                                             ∈
                                             H
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                r
                                             
                                             
                                                d
                                                ,
                                                h
                                             
                                          
                                       
                                       
                                          (
                                          
                                             
                                                A
                                             
                                             
                                                h
                                             
                                          
                                          +
                                          1
                                          )
                                       
                                    
                                 
                              
                           of each document in the result list where rd
                           
                           ,
                           
                              h
                            is the position of document d in the result list according to heuristic h. The ensemble fusion results are comparable with ACM Portal on the NDCG and ERR metrics (0.5% better in terms of NDCG, and 8.1% better in terms of ERR metric); the ensemble fusion also does a much better job than ACM Portal in terms of the LEX metric (442% better). Still, the ensemble fusion results do not compare well with PubSearch as can be seen in Table 5
                           .

PubSearch is on average more than 472% better than the fusion heuristic described above in terms of LEX metric, more than 12% better in terms of the NDCG metric, and more than 70% better than the fusion heuristic in terms of the ERR metric.

We performed a head-to-head comparison between PubSearch and the three state-of-the-art academic search engines:
                           
                              1.
                              Google Scholar (http://scholar.google.com).

Microsoft Academic Search (http://academic.research.microsoft.com).

ArnetMiner (http://arnetminer.org).

The comparison was made on a sizeable subset of our original query set of 58 user queries shown in Table 10, comprising a total of 20 user queries, augmented by 4 new user queries, for a total of 24 user queries. The four new user queries were Q
                        59
                        =‘Page Rank clustering’, Q
                        60
                        =‘social network information retrieval’, Q
                        61
                        =‘unsupervised learning’, and Q
                        62
                        =‘web mining’ respectively. The 20 queries from the original query set were #[34–47], #49, and #[54–58].

Each query was given to each of the above mentioned search engines, and the top-10 results (from each of the above search engines) were then presented to the users for relevance feedback in random order. The summary of results produced by each search engine, as well as the re-ranked results produced by PubSearch when given the same list of results are shown in Tables 6–8
                        
                        
                         respectively.

The average percentage difference between PubSearch and Microsoft Academic Search is 15% for the LEX metric, 2.6% for the ERR metric, and 11.7% for the NDCG metric. Statistical analysis shows that for the ERR and LEX metric, the differences are significant at the 95% confidence level according to the t-test and signed-rank test but not according to the sign test. The results are shown in Fig. 5
                        .


                        The average percentage difference between PubSearch and Google Scholar in terms of LEX is 39%, in terms of ERR metric is 4.6% and in terms of NDCG is 13.4%. The results of applying the t-test, the signed-rank test as well as the sign test on the ERR metric shows that the improvement is statistically significant at the 95% confidence level.

However the same does not apply for the other two metrics, although the t-test shows that the results for the LEX metric are also statistically significant at the 93% confidence level. A visualization of the results is shown in Fig. 6
                        .


                        The average percentage improvement of PubSearch over the ArnetMiner results in terms of LEX score is 19.9%, in terms of ERR is 4.1%, and in terms of NDCG metric is 12.6%. The results of all statistical tests are statistically significant for the LEX and ERR metrics, but only the sign test shows statistical significance for the NDCG metric at the 95% confidence level. Fig. 7
                         visualizes these results.

In the introduction, we mentioned that ACM Portal fails to return the “Chord: a scalable peer-to-peer lookup protocol for internet applications” paper in the top 20 results of the query “peer-to-peer protocol”. PubSearch on the other hand, when given the top 50 results of ACM Portal for the same query, re-ranks them and returns the mentioned paper in the top position.

For the query “Web Information Retrieval”, because ACM Portal fails to return “The anatomy of a large-scale hyper textual web search engine” paper within the first 5000 search results, we manually added the “Google” paper into the top 50 results from ACM Portal and asked PubSearch to re-rank the new list of 51 search results. The Page-Rank paper comes out in 5th place, immediately below the following papers: (1) “Contextual relevance feedback in web information retrieval” (Limbu, Connor, Pears, & MacDonell, 2006), (2) “Concept unification of terms in different languages via web mining for information retrieval” (Li, Chen, Myaeng, Jin, & Kang, 2009), (3) “An architecture for personal semantic web information retrieval system” (Yu, Mine, & Amamiya, 2005), and (4) “An algebraic multi-grid solution of large hierarchical Markovian models arising in web information retrieval” (Krieger, 2011). The papers appearing above the Page-Rank paper all share the following characteristics: (i) they have all terms of the query appearing in the title, and (ii) they are more recent papers. Because of this, our custom implementation of the TF heuristic promotes the other papers high in the result list so that the Google paper ends up in the 2nd TF bucket, and then, its citation count alone cannot promote it higher than the 5th position. Still, PubSearch manages to promote the Google paper in the top 5 results which is much better than the other academic search engines we experimented with.

To further enhance our confidence in the ability of PubSearch to promote “good” publications—for a particular user information need—that happen to appear much lower than the top ten positions in the results list of ACM Portal, we ran another experiment where the users ranked the top 25 results of 20 queries. The system shows again very significant performance improvement against ACM Portal in all metrics considered, and in fact, it significantly improves its performance gap over ACM Portal in terms of both the NDCG and the ERR metrics, when compared with the previous case of re-ranking only the top-10 results.

The results are shown in Table 9
                        . The percentage improvement of PubSearch over ACM Portal on average in terms of the LEX metric is 500.9%, in terms of the ERR metric is 136%, and in terms of the NDCG metric is 18.9%. All the results are statistically significant at the 95% confidence level. The very high gap in terms of LEX score for the case of the top-25 results is exactly due to the fact that good publications that are a good match for the user’s information needs are actually promoted from the bottom of the list of the top-25 ACM Portal results to the top positions. The LEX score is therefore also a useful indicator when investigating the ability of ranking schemes to promote otherwise “buried” publications high in the result list as it amplifies this effect to the maximum extent, and one could easily argue that it is more powerful than the per-query R-Score metric introduced in Breese et al. (1998).

The run-time overhead our initial prototype requires to perform the re-ranking of the search results given a query and a set of results from another engine (i.e. ACM Portal, or Google Scholar or Microsoft Academic Search, or ArnetMiner) is in the order of less than three (3) seconds per document. This run-time applies for a commodity hardware workstation. However, the computation of the TF-score (by far the most compute intensive process in the whole system) for each document is independent of the other documents in the result list, and therefore can be done in parallel so that the total computation time for a full list of search results will still be in the order of seconds in a server farm. Furthermore, our prototype is not in any way optimized for speed at this point. We are in the process of optimizing the response time of the system to reduce the run-time processing requirements per document by one order of magnitude to make the system commercially feasible.

The results of t-test, sign-test and signed-rank test, all indicate thatPubSearchoutperforms ACM Portal by alarge and statistically significant margin in terms of all metrics considered, namely lexicographic ordering LEX, NDCG and ERR metrics. In terms of lexicographic ordering, the average improvement is in the order of 907.5%, in terms of NDCG the average improvement is almost 12% and in terms of the ERR metric the average improvement is more than 77%. Similarly, comparing PubSearch against the standard Okapi BM25 scheme shows that PubSearch offers very significant advantages for ranking academic search results.

Even when comparing PubSearch against the current state-of-the-art academic search engines Google Scholar, Microsoft Academic Search, and ArnetMiner, the comparisons show that PubSearch outperforms these other engines in all metrics considered, and in the vast majority of cases, by statistically significant margins.

We have introduced a new metric for ordered lists of academic search results, called LEX, with the nice properties that (i) when comparing two lists, the list with the higher LEX score is guaranteed to have a higher relevance judgment score for the item in the highest position having a different score in the two lists, (ii) the LEX score is always in [0,1], (iii) random-valued relevance judgments in the ordered list have an expected LEX score of 0.5, and more importantly (iv) a search engine that orders search results in user relevance order in the usual stochastic order sense, will produce expected LEX scores above the average normalized relevance judgments score.

Without detailed knowledge of the ranking system behind ACM Portal or the other academic search engines we compared our system with, we postulate that the main reasons for the better quality of our ranking scheme is in the custom implementation of the term frequency heuristic we have developed that takes into account the position of the various terms of a query in the document and the relative distance between the terms, as well as in the chosen architecture itself: the custom implementation of the term frequency score roughly determines whether a publication is relevant to a particular query, the time-depreciated citation distribution is a good indicator of the overall current value of a paper, and finally the clique score criterion accords extra value to (otherwise similarly ranked) papers that are classified in subjects that are strongly linked together as evidenced by the cliques formed in the Type I & II graphs that connect index terms together in a relatively small but typical publication base crawled for this purpose.

We have shown through extensive experimentation that the proposed configuration outperforms all the other configurations we have experimented with, as well as linear regression or the popular ensemble fusion approach using linear weights (that is present in many if not most modern classifier, clustering, and recommender systems designed today, e.g. Christou, Gkekas, & Kyrikou, 2012).

We are already in the process of fine-tuning and optimizing our initial prototype (mostly in terms of response time) to turn it into industrial-strength software; we are also in the process of developing a scheme that automatically classifies publications according to the ACM classification scheme, which, besides the obvious advantage of letting us use the full PubSearch system against any publication it clearly would have many other independent uses.

In this Appendix we present the table containing extended information about analytical experiment comparison results, and in particular, containing the queries used in our evaluation test-set (see Table 10
                     ).

@&#REFERENCES@&#

