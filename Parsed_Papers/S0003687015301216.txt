@&#MAIN-TITLE@&#Evaluating the ability of novices to identify and quantify physical demand elements following an introductory education session: A pilot study

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Students attended a 3-h introductory workshop on physical demands description.


                        
                        
                           
                           Participants were asked to identify and quantify physical demands in three case examples.


                        
                        
                           
                           Participants identified 80% of the physically demanding elements.


                        
                        
                           
                           Participants quantified key measures with more the 10% error from the criterion.


                        
                        
                           
                           Novices' have some limitations in their ability to accurately conduct PDDs.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Physical demands description

Job demands

Observation

@&#ABSTRACT@&#


               
               
                  A Physical Demands Description (PDD) is a resource that describes the physical demands of a job in a systematic way. PDD data are commonly used to make legal, medical, and monetary decisions related to work. Despite the fundamental importance of a PDD, data are often gathered by novice or early career ergonomists, where we have limited knowledge regarding their proficiency in performing PDDs. The purpose of this pilot study was to evaluate novices' proficiency in identifying and quantifying physical demands elements embedded within three job simulations, following a formal PDD education session. The education session was based on the revised Occupational Health Clinics for Ontario Workers (OHCOW, 2014) PDD Handbook. Participants were able to identify physical demands elements with an average success rate of 80%, but were often unable to accurately quantify measures related to each element within a prescribed error threshold of 10%. These data suggest that practitioners should exercise caution when sending novice ergonomists out on their own to complete PDDs.
               
            

@&#INTRODUCTION@&#

A Physical Demands Description (PDD) describes the physical demanding elements of a job. A description of a jobs physically demanding elements is important to different users for different reasons. Within the hiring department PDD information can be used to help clearly describe a job and its requirements to a prospective applicant (Hogan and Bernacki, 1981). In the event that a worker has become injured, claims adjudicators may use PDD information to decide if the claimed injury is plausible or consistent with the physical demands of the work (Jones et al., 2005). In a return-to-work context professionals rely on PDD information to plan out and progress rehabilitation to restore an injured workers' capability such that they can again meet the physical demands of their job (Isernhagen, 2006). However, despite the importance of PDD information in the decision making process regarding hiring, injury compensation, or return-to-work, there is limited consistency in how PDD data are gathered and reported.

Describing the physical demands of a job is the central reason for the PDD. However, when scanning PDD templates available online from Canadian health and safety related organizations (e.g. WCB Alberta, WSIB, Workplace Safety North, Workplace Safety and Prevention Services, etc.), it is clear that we lack consensus on which demands to identify, what measures to include to describe each physical demand and how to report physical demands information. While the lack of standardization is a concern, particularly for those tasked with trying to extract information from a PDD for the purpose of adjudicating over an injury claim or in planning a return-to-work, perhaps the greater concern is “who is gathering PDD information and are they doing it well?”

Accurately gathering PDD data requires expertise and training to ensure that physical demands are correctly identified and accurately quantified. Ergonomists are often tasked with completing PDDs and are well educated and experienced; where the majority of practitioners reportedly hold a Master's or Doctoral degree with over ten years of ergonomic work experience (Dempsey et al., 2005). However, anecdotally, health and safety professionals note that the established ergonomist is not completing the PDD; rather, it has been delegated to a novice ergonomist or trainee. Moreover, PDDs are among the most frequently completed analysis by Joint Health and Safety Committee (JHSC) members (Pascual and Naqvi, 2008), where JHSC members may have limited education or experience in the measurement and assessment of physical demands. Since there are no entry-to-practice standards regarding PDD competency, it is likely that most novice ergonomists or JHSC members have only achieved a basic level of training on the PDD process, perhaps in the form of a formal class lecture or two, or by attending a PDD workshop. Therefore it is important to evaluate novices' proficiency in completing a PDD.

It is expected that novice's will demonstrate some limitations in their proficiency. With specific reference to observationally-based ergonomics tools, Stanton and Young (2003) found that novice's demonstrate acceptable intra-observer reliability; but, exhibit poor inter-observer variability. Poor inter-observer variability may suggest that novice's do not collectively key in on the same information when observing work. While not-yet tested in the context of ergonomics, novices may lack the perceptual skills required to identify and extract only relevant information when viewing complex dynamic visual stimuli (Jarodzka et al., 2010). Indeed, enhanced perceptual proficiency with experience and training have been demonstrated in other occupations including diagnostics in x-ray images (Lesgold et al., 1988) and weather map analysis in meteorology (Canham and Hegarty, 2010). Since novice ergonomists or JHSC personnel may not have developed the perceptual proficiency of a seasoned ergonomist, it is important to evaluate the proficiency of novices to identify and measure physical demand elements accurately.

The purpose of this study was to conduct a preliminary evaluation of the ability of novices to identify and measure physical demand elements following a PDD education session. It was hypothesized that participants would not be perfect in their ability to correctly identify physical demand elements; however, they would correctly identify physical demand elements with a success rate of at least 80%. It was also hypothesized that participants would be able to accurately quantify demands with an absolute percentage error (APE) threshold of less than 10% relative to criterion measurements obtained by two ergonomic professionals.

@&#METHODS@&#

Ten university aged students (3 males, 7 females) volunteered to participant in this study. They were recruited from an undergraduate occupational biomechanics and physical ergonomics class, where they took part in a 3-h education session on PDDs (described below) early in the semester. Prior to the class they had no prior knowledge or experience with PDDs. This project was approved by the University's Research Ethics Board and all participants provided informed consent.

A one-shot case study experimental design was employed as a pilot investigation to evaluate the ability of novices to identify and quantify physical demand elements following an introductory level PDD education session. This model was employed to facilitate the research team in recruiting undergraduate students directly from an undergraduate-level ergonomics class where PDDs were taught as part of the normal curriculum.

The education session was based on the Occupational Health Clinics for Ontario Workers (OHCOW) Revised PDD Handbook (OHCOW, 2014). This foundational resource was selected as it represented the most current, publically available document describing the PDD process as a series of easy to follow steps. During the session participants were presented with information and activities corresponding to the three steps of a PDD (Fig. 1
                        ). While working through each step instructors provided drill-down opportunities to discuss pertinent details in great depth, such as: considerations when scheduling a data collection, how to take measurements in the field, how to identify physical demand elements, etc. During drill-down activities course instructors provided specific feedback and reminders to students as they completed associated activities. While all three high-level steps are fundamental to conducting a proper PDD, the education session focused primarily on step two (observation & data collection) as it represents the actual data gathering portion of the process. At the conclusion of the education session students were invited to participate in this study, where we scheduled them into the laboratory for testing one-week after completing the in-class training.

During the testing phase, participants independently observed three job simulations: two video-based examples, and one live example. Video 1 portrayed a road construction labourer, where the worker used a shovel and rake to spread and level asphalt (Fig. 2
                         – left pane). The video was approximately one minute in length, during which time the following physical demand elements were performed: Push, Pull, Reach, Grip, Stand, Walk, Balance, and Vision. Video 2 portrayed an automotive quality control tester, where the worker manually inspected the worthiness of a car door by exerting forces using a series of different techniques (Fig. 2 – centre pane). The video was approximately one minute in length, during which time the following physical demand elements were performed: Push, Pull, Crouch, Grip, Stand, Walk, Feel, and Vision. The live job simulation was performed by an actor mimicking a manual materials handling task of stocking shelves (Fig. 2 – right pane). The task was approximately 3 min in length, during which time the following physical demand elements were performed: Lift/Lower, Push, Pull, Grip, Crouch, Stand, Walk, and Vision. Participants were instructed to identify and list all of the physical demand elements observed across the three job simulations.

Within other PDD models, specific elements are often grouped under broader headings such as “Strength”, “Mobility”, “Hand-Activity”, and, “Sensory” (WSPS, 2011). Physical demand elements existing in the three job simulations used in this study were divided among these groupings (Strength – Lift/Lower, Push, and Pull; Mobility – Stand, Walk, Reach, Crouch, and Balance; Hand-Activity – Grip; and Sensory – Feel and Vision). Beyond proficiency in listing all physical demands existing across the job simulations, we aimed to conduct a descriptive analysis to identify if novices may be more, or less accurate at identifying elements within these overarching groupings, where we believed that the identification of sensory- or mobility-based demands may require greater perceptual proficiency, and thus be more challenging for novices to identify.

During the live job simulation, participants were also asked to quantify requisite dimensions for each physical demand element identified. To reduce the potential for inter-rater variability due to actual differences in the actor's performance of the task, the actor followed a clearly defined script when performing the live job simulation. This ensured that each participant observed the same physical demand elements in the same sequence. Special attention was given to the performance of the push and pull demands. These tasks were performed by initiating cart movement at a gradual and controlled pace. We did not want the actors technique (push quick or slow) to affect individual rater PDDs. All participants were provided with tools commonly used by ergonomists including a: measuring tape, force gauge, stopwatch, pencil, and paper. Using the OHCOW PDD template (OHCOW, 2014), participants were instructed to record and measure relevant dimension for each physical demand element that they identified (e.g., a Reach requires the evaluator to quantify the dimensions of: Frequency: repetitions per minute, Height: centimetres, Distance: centimetres, and Hand(s) Used: left, right, or both). Participants were not given a time limit to complete the quantification task.

A criterion method was used to evaluate each participant's performance on physical demand element identification and quantification. Criterion measures were established by a team of subject matter experts (authors BC and CV) who reviewed each activity and came to consensus on the list of physical demands associated with simulation. This list was used as the benchmark for evaluating participants' identification accuracy. During the identification tasks, participants' list of physical demand elements were compared independently against the three simulation criterion lists, where one point was awarded for each element correctly identified. Points were given for correct responses only. No points (negative or otherwise) were given for missed or incorrectly identified elements. A threshold of 80% accuracy was used to classify the participant as successful or un-successful regarding their ability to identify physical demand elements. The 80% threshold is commonly used as a benchmark to indicate success in education-based training within various health and safety associations such as the Heavy Construction Safety Association of Saskatchewan – Safety Program Certificate of Recognition (HCSAS, 2010), and the National Association of Safety Professionals – Workplace Ergonomics Technician (NASP, n.d.).

During the live simulation component participants were also challenged to quantify requisite dimensions for each of the physical demand elements that they identified. The criterion measures were established for fixed values; such as object weights, shelve heights, and walked distance, through direct measurement according to the prescribed methods as taught in the education session. Criterion measures were established for measures which are more prone to deviation (push and pull forces) by averaging several trial measurements (across two different raters – authors BC and SF) while affirming that variation in values were less than 5%. While these measures are also subject to error; it was believed that these expert derived measures would be closer to the true value based on their combined experience and the ability to average measures across expert raters. For each measurement, participant's absolute percentage error (APE) was calculated by comparing participant's measurements (Reported Value) with the criterion measurements established by the subject matter expert team (Criterion Value), as illustrated below:
                           
                              
                                 
                                    A
                                    P
                                    E
                                    =
                                    
                                       |
                                       
                                          
                                             (
                                             
                                                
                                                   
                                                      R
                                                      e
                                                      p
                                                      o
                                                      r
                                                      t
                                                      e
                                                      d
                                                      
                                                      V
                                                      a
                                                      l
                                                      u
                                                      e
                                                      −
                                                      C
                                                      r
                                                      i
                                                      t
                                                      e
                                                      r
                                                      i
                                                      o
                                                      n
                                                      
                                                      V
                                                      a
                                                      l
                                                      u
                                                      e
                                                   
                                                   
                                                      C
                                                      r
                                                      i
                                                      t
                                                      e
                                                      r
                                                      i
                                                      o
                                                      n
                                                      
                                                      V
                                                      a
                                                      l
                                                      u
                                                      e
                                                   
                                                
                                             
                                             )
                                          
                                       
                                       |
                                    
                                    ×
                                    100
                                    %
                                 
                              
                           
                        
                     

Presently, there is no consensus regarding the level of accuracy required for a PDD. As such we considered an error threshold of 10% APE to be reasonable boundary. The threshold was intentionally selected to be more conservative than that used for physical demand identification, as the quantification of physical demands elements requires a higher degree of skill and expertise due to the complexity of hands-on measurements. APE was calculated for all measured data (i.e., height, distance, force). When physical demand elements were further described with nominal variables (i.e., grip type, hand(s) used) rather than specific measurements, APE was not calculated and these data were not analysed further.

Data were transcribed to reflect the frequency of correct physical demand element identifications and quantified values of relative error in measurement of physical demand elements using Microsoft Excel (Redmond, WA).

Data were compared using IBM SPSS Statistical Software (Armonk, NY, USA). One-tailed independent samples t-tests were used to determine if participants, on average, correctly identified at least 80% of the physical demand elements present in each of the job simulations. To determine the effect size for identification task findings, Cohen's d was calculated (Cohen, 1988) where a d value of .20 represents a small effect, .50 a medium effect, and .80 a large effect (Cohen, 1992). When considering the ability to accurately quantify dimensions associated with identified demands, a one-tailed Wilcoxon rank-sum test, a non-parametric alternative to the independent samples t-test, was used. Median APE values were compared to the 10% error threshold value to determine if the groups' measurement accuracy was 10% or less. A non-parametric test was chosen as measurement data were not normally distributed. For both identification and quantification tasks, a p-value <.05 was chosen to detect significant differences.

@&#RESULTS@&#

Participants were able to accurately identify physical demand elements with a success rate of at least 80% in all three job simulations. No differences were found between physical demand element identification and the threshold for Video 1 (M = 80.0, SD = 17.8); Video 2 (M = 90.0, SD = 9.8), and Live (M = 82.5, SD = 12.0) job simulation tasks (Fig. 3
                        ).

Group means at or above the threshold indicate that participants, on average, met the defined proficiency criterion of 80% when identifying physical demand elements. However individual participants varied in their identification accuracy. Three participants failed to identify at least 80% of physical demand elements that occurred within each video, where scores ranged from 50% to 75%.

Participants were more successful at identifying Strength and Hand-Activity related demands, relative to types of physical demand elements (Fig. 4
                        ). Conversely, Sensory and Mobility demands were missed most often. While this comparison is strictly descriptive in nature, these data suggest that novice observers may not identify all categories of physical demand elements with the same accuracy.

Dimensions of each physical demand element were quantified during the live job simulation of a shelf stocking task. The job exposed the worker/actor to eight physical demand elements: Lift/Lower, Push, Grip, Crouch, Pull, Walk, Stand, and Vision, where each contained several dimensions for quantification (OHCOW, 2014). For this analysis, only variables and dimensions which were measurable on a ratio scale (non-ordinal data) were included. Table 1
                         reflects data compared using the non-parametric Wilcoxon rank-sum test that compares participants' median APE values to the 10% criterion error threshold.

Based on the median APE, participants were not able to accurately measure the horizontal reach distance within the pre-defined 10% error threshold (Table 1). Group median values were not statistically different from the 10% criterion APE for weight, height, and frequency measures. The largest maximum error was recorded for the weight measurement (152% APE) while frequency was the measurement most commonly quantified within the 10% error threshold (eight participants).

The median APE was significantly greater than the error threshold for the measures of push force and push distance (Table 1). Group median values were not statistically different from the 10% criterion APE for height and frequency. The largest maximum error was recorded for the force measurement (308% APE) while height was the measurement most commonly quantified within the 10% error threshold (nine participants).

The median APE for grip height was significantly higher than the error threshold (Table 1); however, there was no significant difference associated with grip frequency. The largest maximum error was recorded for frequency (83% APE), which was also the measurement most commonly quantified within the 10% error threshold (eight participants).

The median APE for pull force significantly exceeded the error threshold criterion (Table 1). Group median values were not statistically different from the 10% criterion APE for height, distance, and frequency. The largest maximum error was recorded for the force measurement (950% APE), while height was the measurement most commonly quantified within the 10% error threshold (nine participants).

The median APE in the walked distance measure was significantly higher than threshold (Table 1). All seven participants whom correctly identified the walking demand measured values with APE above threshold, where values ranged from 19% to 67% APE.

@&#DISCUSSION@&#

Physical demand element identification is an important step when gathering data in the PDD process. We hypothesized that participants would be able to identify physical demand elements in multiple job simulations at or above our pre-defined criterion of 80% following completion of the PDD education session. The data support our hypothesis where, on average, participants correctly identified 80% of the physical demand elements. King et al. (1997) reported a group mean proficiency score of 79% when novices were tasked with addressing aspects of ergonomics and job redesign, following an education session; while Robertson et al. (2009) reported scores between ∼80% and 90% in novice's ability to conduct office ergonomics evaluations, following an education session. While these collective results do not address the efficacy of the teaching methods, they do indicate that novices with some foundational training, seem to achieve approximately 80% proficiency in conducting the associated ergonomics related activities. Now that we have a clearer picture of the normal level of proficiency demonstrated by a novice following an introductory education session, we can investigate how proficiency might be improved with experience, and/or revised training and education; however, those considerations remain as future research opportunities.

Participants often correctly identified strength and hand activity-based elements but often struggled in identifying mobility and sensory based elements. While this trend could be a result of limited sample size, the result is similar to previous research. A study by Silverstein et al. (1991) indicated that trainee's accuracy in identifying and classifying specific postures, when using pencil and paper-based postural observation tools, varied between ∼40 and 90% following ergonomic training. This is similar to our results, albeit we asked participants to identify mobility elements, which encompass a broader set of movement-based actions, without any prompts. The vantage point of observers could also affect which elements were observed and reported, based on where they were standing relative to the live worker or the angle from which the video was captured from the two recordings. Concurrent with existing knowledge regarding perceptual skills (Jarodzka et al., 2010), it may be that novices were simply not able to precisely identify and extract out this specific information from the complex visual information provided. While our preliminary explanation requires further research, the results of this study suggest that novices may be limited in their ability to correctly identify non-strength or hand-activity based demands. Senior consultants should exercise caution when delegating PDDs to novices, particularly where those jobs may require a lot of mobility or sensory based demands. Future research should investigate the efficacy of training approaches that differentially emphasize the focus on mobility and sensory based physical demand elements.

Physical demand element quantification is also a critical aspect of developing an accurate PDD. The second hypothesis stated that participants would be able to measure physical demand elements within an absolute percent error (APE) threshold of 10%. While select measures were quantified within the APE threshold, others were not. Therefore, the data do not fully support the second hypothesis. One major implication of inaccurate PDD data relates to its use in the return to work process for an injured worker. Health care practitioners often perform job function matching, comparing the demands of the job to the abilities of the worker (Genovese and Galper, 2009). This process allows them to determine which tasks can be performed safely and which require modification by ergonomic tools, job design, or rotational work schedule (Isernhagen, 2006). For safe and effective job matching, job demand quantities must be accurate. Inaccurate data could impair the process of job matching by pairing a vulnerable worker with job demands that may actually exceed their physical capacity if the demands listed in the PDD were under-estimated, or, by excluding a viable worker from a role if the demands were over-estimated. Our findings reporting large APEs in many measures coupled with a need for accurate demands data emphasize the importance of increasing training and practice opportunities with respect to measurement in the workplace. Further, these results suggest that PDD data should be interpreted with caution if the report was prepared by a novice.

Measuring force requirements was particularly challenging for many participants. When considering the weight lifted, the push force and the pull force (where median APE was significantly higher than the criterion for push and pull forces), more than half of the participants measured each value with an APE above threshold, and the maximum APE values exceeded 150%. It is possible that these errors may pertain to the participant's familiarization with the use of a force gauge, resulting in inaccurate measurements. Further, it seems like the limited familiarization presented more challenge to the participants when trying to measure dynamic activities such as pushing or pulling (34% and 68% median APE, respectively), relative to measuring the static load weight (16% APE). As one possible explanation, Bao et al. (2009) suggest that variations in force production using a force gauge can be attributed to individual differences in hand and wrist postures used. While hand and wrist postures during force measurement were not monitored in this study, it is feasible that the variation may, in part, be explained by this factor. However, given the magnitude of APE, it seemed as though the large errors were a result of measuring incorrectly. Anecdotally, some participants measured push and pull quantities with the incorrect number of objects on the cart, which would directly propagate error in the measurement. Further, some participant applied forces, through the force gauge, to the cart at faster rates, where the peak measured force would be indicative of how hard they pushed/pulled, rather than how much force was required to initiate (or maintain) movement of the cart. Lastly, some participants recorded only a single measurement, while others reported averaged values from multiple measures. This also adds to explain the variability and magnitudes of APE observed. Where previous research on force gauge use has focused primarily on force matching (Casey et al., 2002; Bao and Silverstein, 2005; Koppelaar and Wells, 2005), it may be useful to extend these research efforts to help improve our understanding of common force gauge usage mishaps among relative novice PDD evaluators.

Height, horizontal reach, and walked distances are commonly measured with a measuring tape. When considering vertical height, participants tended to measure handle height (push and pull) with the lowest APE, where APE increased when measuring lifting height, and increased significantly above the 10% criterion when considering grip height. Similar to the force measurement results, participants seemed to be most accurate when measures were relatively static within the plane of interest; such as when the worker maintained their vertical hand position during the push/pull activity. However, measures were less accurate when the vertical hand positioned varied such as when gripping and lifting boxes. Differentiating between grip height (measured as the peak height of the grip centre from the floor) and lift height (load centre of gravity at end of lift and origin of lift) seemed to pose a challenge, where grip height seemed harder to measure accurately for our participants. Despite instructions (within the education) regarding definitions for these distances, synonymous with those described by NIOSH (1981), participants seemed to struggle to recall these definition points of reference for making standard measures. These errors seemed to propagate when the measure also included a lot of movement by the worker, within the plane of interest. Lastly, for reasons unbeknownst to the researchers, walking distance was often estimated based on an average stride length, or a guess.

This pilot study provides important information to those responsible for providing PDD training to novices. Firstly, these data indicate that novice observers may be able to correctly identify physical demands elements accurately following a target PDD training. However, it is possible that training may be improved by over emphasizing trainees' attention to sensory and mobility related elements. For this study, participants received equal amounts of time reviewing each sub-category of physical demand elements, although these results indicate that their performance in identifying each class of element was not equivalent. Secondly, when asked to quantify aspects of each element, some novice observers were inaccurate by large margins. To improve on the accuracy of physical demand element quantification, PDD educators may want to place a greater emphasis on measurement tool use where additional practice may be necessary. Functional capacity testing relies on objective job specific information for a stronger match, and for this reason, robust measurement of job elements is crucial (Isernhagen, 2006).

The individual differences in the ability to identify physical demand elements and, more so in the ability to measure aspects of those demands, raises a concern. While on average, the group identified elements with a success rate of 80%, and measured with an absolute percentage error of 10% or less most of the time, not all individual participants met the standard. Further, considering that our participant pool consisted of kinesiology students, with some existing knowledge of anatomy and biomechanics, these results may represent a “best case” scenario. Therefore, it may be prudent to suggest that a PDD should be performed by an experienced ergonomist, or at least by more than one observer when being completed by novices, as an averaged value obtained from multiple observers may help control for this error. This study only challenged observers to identify and quantify short, cyclic tasks, whereas it could be expected that error in measurement and misidentification could increase when conducting a PDD for a more complex job where tasks are non-cyclical or inconsistent, such as construction work (Buchholz et al., 1996). Important work-related decisions are grounded on PDD information so we must be confident in the quality of that data. Revisiting training and evaluation methods will be beneficial for PDD educators and practitioners moving forward to ensure the highest quality of data collection.

These data clearly indicate that novices' experience limitations in accurately identifying and quantifying PDD elements. While every expert must begin somewhere, these data question the potential impact of relying on novices' or non-ergonomists, such as JHSC members to independently complete PDDs. When these potential training or expertise related inaccuracies are overlaid with the current lack of standardization in PDD assessment and reporting it is clear that changes are needed to ensure that PDDs provide valuable, accurate information to support disability prevention and management. Considering these growing concerns, as a profession, we would benefit greatly from leadership in the development of standards of practice for the collection and reporting of PDDs. While these standards may not directly affect who is conducting PDDs, as noted by Stanton and Young (2003), there is no substitute for ergonomics knowledge and expertise.

@&#LIMITATIONS@&#

These data are intended to provide an introductory exploration into the issue of measurement error among novices when gathering data as required to complete a PDD. Results should be considered in the context of the following (de)limitations. All participants attended a three-hour education session on PDD prior to completing the required PDD activities. While an investigation into the efficacy of training was beyond the scope of this project, the results may be affected if participants were provided with more training and practice. However, the intent of this pilot study was to assess proficiency of novices, after assuring that each had at least a minimum level of foundational training in the form of a three-hour targeted PDD workshop, consistent with the time typically allotted to PDD training in undergraduate ergonomics courses and/or JHSC hazard assessment courses (Robins and Klitzman, 1988). Additionally, participants in this study were asked to apply their news skills one week after learning a novel subject without intermediate practice. The observed errors in measurement technique and amplitude may be attributed to recall bias associated with the time gap between learning and application.

A small sample size, extracted from a homogeneous sample of university-aged students also limits the generalizability of these results. It may be possible, that novices (with respect to PDD aptitude) engaging in PDD work as part of a JHSC, with more life and work experience, may demonstrate different proficiencies. However, it is likely that this, albeit small pilot sample, is representative of the young, new university graduate seeking entry level employment in ergonomics.

This study provides preliminary data suggesting that novices have limitations in their ability to measure quantitative aspects of PDD data accurately. In this study, we compared participant's results to criterion measures (established by the research team) in attempt to assess the accuracy of novices, which represents a limitation. To establish the criterion, the research team worked collaboratively to identify all demands and gather all required measurements, benefitting from the opportunity to record averages from multiple raters (for shelf height for example), increasing the likelihood that the measures were indeed representative of the true values. However, future research should measure the performance of individual experts and novices, to determine if more experienced individual's measure elements with more accuracy and precision than our novice participants demonstrated in this study.

@&#CONCLUSION@&#

This research represents a preliminary attempt to measure the PDD proficiency of novices, after receiving introductory level instruction on the topic. On average, participants accurately identified physical demands elements with a success rate of at least 80%. However, as a group and individually, participants did not accurately quantify many of the physical demand elements required to completely describe the physical aspects of the job. These preliminary data suggest that novices may be limited in their ability to produce accurate PDD information, and caution should be taken when considering assigning a PDD task to a novice. In addition, this work provides support for continued research efforts to better understand how PDD information is being gathered and reported, particularly since the PDD remains as the cornerstone of the disability prevention and management framework in Canada.

None.

@&#REFERENCES@&#

