@&#MAIN-TITLE@&#Integrating multiple character proposals for robust scene text extraction

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Proposed system separates text regions from images under unconstrained environment.


                        
                        
                           
                           Generalized clustering utilizes properties of scene text to detect text boundaries.


                        
                        
                           
                           Multiple image segmentations provide various interpretations on text regions.


                        
                        
                           
                           Two-step CRF approach models properties and relationship of text in graph structure.


                        
                        
                           
                           Character proposals are generated and integrated to find proper character regions.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Scene text extraction

Two-stage CRF models

Multiple image segmentations

Component

Character proposal

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

Scene texts such as bottle labels, street signs, and license plates are text regions, which are present as integral parts of pictures (Fig. 1
                     ). The text is usually linked to the semantic context of the image, and it constitutes a relevant descriptor for content-based image indexing. Direct recognition of scene text from the images captured by mobile devices could facilitate the development of a variety of new applications, such as translation, navigation, and tour guide services.

Scene text understanding refers to an attempt to recognize text in camera-captured images. It consists of two parts: scene text extraction and scene text recognition. The purpose of the extraction process is to separate text regions from the natural image. The purpose of the recognition process is to determine labels from the extracted text regions. Robust extraction of text from scene images is an essential step for successful scene text recognition. A very efficient text extraction method would enable the direct use of commercial OCR engines, which are normally optimized for binarized document images. However, errors due to a poor extraction method could be propagated in the recognition process. Therefore, we will focus on the extraction process in this paper.

Extracting text from unconstrained images of natural scenes is difficult owing to the lack of any prior knowledge about the text regions, such as the color, font, size, orientation, or even the location of the text. In addition, scene images usually have uneven illumination, reflections on objects, and inter-reflection between objects owing to uncontrolled lighting conditions and the presence of shadows. These conditions make colors vary drastically, so the text regions may be fragmented, or the boundaries of the text region may be faint. It is also common for outdoor images to have complex layouts in which the content and background are mixed. Shapes in the background can be similar to characters, particularly for textured objects such as windows or buildings. Such complications make extracting text from scene images a persistent challenge.

A two-stage conditional random field (TCRF) approach with multiple image segmentations is proposed to overcome these challenging problems and to extract proper character regions regardless of complex background and outdoor environments. One assumption in dealing with multiple segmentations is that there exists at least one correct segmentation for each character instance. Multiple image segmentations with different partition parameters provide alternative ways of grouping pixels to form homogeneous regions in the image, so having multiple image segmentations increase the possibility of some homogeneous regions matching character regions. Although some segmentations are inaccurate, others are accurate and would prove useful for the extraction task. Hence, having multiple segmentations can provide a more robust basis for character regions than any single image segmentation.

The proposed TCRF approach finds coherent groups of correctly segmented character regions within a large pool of multiple candidate regions by utilizing the properties and hierarchical structure of the scene text. Commonly used structures of the scene text are the following: “the textline (composed of characters),” “the characters,” and “pixels”. An image contains one or more textlines; they are aligned horizontally, vertically or diagonally. These textlines have distinguishable texture patterns. Each textline does not vary in height too much, and it has sufficient width to contain more than a single character. Most characters (alphabet and numbers) are assumed to appear as single regions. Characters in the same textline exhibit common properties such as font and color. For instance, characters have no steep change in their thickness, and the insides of character regions have homogeneous colors, whereas the character boundaries show a distinction from the background. By satisfying these relationships of characters based on the scene text model, the proposed method can extract most plausible configurations of character regions among all possible combinations of segmented regions. As a result, the pixels in the character regions are instantiated as foreground, and the pixels in the other regions (non-text regions, i.e. noises) are indicated as background.

The rest of this paper is organized as follows. Section 2 presents related works on the scene text extraction method. In Section 3, we briefly introduce the proposed extraction framework. The algorithm of partitioning an image into multiple segmentations and generating multiple character proposals is explained in Sections 4 and 5. We explain how to integrate character proposals into textlines in Section 6. The efficiency and performance of the suggested system are experimentally evaluated in Section 7. Finally, the paper is concluded in Section 8.

@&#RELATED WORK@&#

To make scene text extraction a manageable problem, previous researchers made some assumptions by using the domain knowledge on the scene text: one is that the general shapes of text regions are distinctive from those of background regions, and the other is that characters are homogeneous in color (or stroke thickness). The former leads to texture-based approaches, and the latter leads to region-based approaches.

Texture-based approaches focus on the local distinctiveness of text. Researchers assume that the text regions form certain patterns of texture features [1–3]. These methods first try to locate roughly a region of text by analyzing texture features. And an additional stage may follow to separate the text pixels from the background within the region located. Texture features, such as DCTs, wavelet features, or edges, are commonly used for rough segmentation of regions. However, these methods only consider local information on text, so they cannot gather high-level information such as character relationships. As a result, many complex background regions (human-made objects or leaves) that are similar in shape to characters are misclassified as character regions.

Region-based approaches are inspired by the observation that pixels constituting a particular region with the same color or stroke thickness often belong to the same object. Several previous extraction algorithms [4–6,37] assumed that text pixels are homogeneous in color so that they can be separated from the background and grouped on the basis of color values. Epshtein et al. [7] identified text regions on the basis of local homogeneity of stroke thickness. This method measured stroke width for each pixel and merged neighboring pixels having approximately similar stroke thickness into a region. Neuman and Matas [8] assumed that characters are extremal region (ER) in some scalar projection of pixel value, and they used maximally stable extremal region (MSER) algorithm to detect stable regions in an image. These separated regions (called connected components) obtained from regions-based approaches are then classified in turn as foreground or background by further analyzing those using heuristic methods or random field models. However, for region-based methods, character regions cannot be segmented well without proper text information such as position and scale. Region-based methods cannot also handle varying environmental conditions, and they might produce an unwanted separation of regions under large variations in illumination and color. Moreover, it is difficult to recover from errors in the initial segmentation.

From this review of the literature, we may conclude that extracting text from images of natural scenes is still a challenging problem for images with complex backgrounds and natural illumination. Especially, it is difficult to extract character regions directly with a single-pass process without any prior knowledge of the given character regions. To handle these challenging issues, analyzing scene image based on various perspectives with multiple image segmentations is needed.

The goal of the proposed method is to isolate a character region as a single continuous region in any kind of outdoor environments. To handle complex backgrounds and natural illumination of scene images, our proposed system generates diverse character proposals based on bottom-up image processing. Multiple image segmentations with different partition parameters provide alternative ways of grouping the pixels to form homogeneous regions in the image, so these provide different interpretations of the given image. As a result, multiple image segmentations increase the possibility of some homogeneous regions matching character regions. This approach allows label decisions regarding pixels to be postponed until the evidence across multiple segmentations has all been collected. Integrating the information from the multiple image segmentations can provide a more robust basis for text extraction than using the information from one image segmentation alone.

The proposed framework then needs to find groups of correctly segmented character regions within a large pool of multiple candidate regions. We regard a text segmentation problem as a labeling problem for each region obtained from multiple segmentation processes. A two-stage conditional random field (TCRF) approach is proposed to find coherent groups of correctly segmented character regions based on the properties and hierarchical structures of the scene text. The proposed TCRF models are graphical models that represent segmented regions as nodes and their relationships among segmented regions as edges in undirected graph structures [9]. Labels of segmented regions correspond to the random variables of the CRF model. The evidence at each node is computed to tell whether the node belongs to the character region or the background region using a discriminative classifier on the character features. The structure and strength of the edges in the graph are determined dynamically from the segmentations.

The proposed TCRF approach consists of two types of CRF models: one is a local CRF model and the other is a holistic CRF model. In the first stage, each local CRF model in a single segmentation identifies disjoint regions as foreground or background by analyzing shape and geometric features of the regions. The local CRF model alleviates the computation overhead of exhaustively searching for proper character regions by pruning most apparent non-text regions and leaving the remaining regions as character proposals. In the second stage, a holistic CRF model integrates multiple character proposals and finds the most plausible configurations of character regions among all possible combinations of character proposals.

The flowchart of the proposed method is given in Fig. 2
                     . First, the input image is segmented into multiple regions with various measures of color distance, and with the two constraints on the scene text. The labels of segmented regions are inferred by the local CRF model based on the combination of the multiple character features and relationships among the regions for each segmentation. Each segmentation process provides proposals for character regions, which have high probabilities belonging to the character regions. These proposals are grouped into several textlines. The holistic CRF model finds the most plausible configuration among all possible combinations of character proposals in the potential textline which maximally satisfying properties of the scene text model. Finally, the proposed system provides text-only regions as a final result. A detailed description is given in the following sections.

Our text extraction methodology relies on a set of image segmentations generated by bottom-up processes. Segmentation refers to one possible way of grouping pixels to generate a set of disjoint regions. The perfect image segmentation can generate homogeneous regions consistent with object boundaries. However, it is difficult to obtain perfect segmentation with respect to a variety of color gradations and outdoor environments. For better recognition afterwards, it is necessary to isolate character regions from the background without a loss of small details.

To partition an image robustly into the desired regions, we propose a generalized K-means clustering algorithm which seamlessly combines color, texture, and edge. In addition, we generate multiple image segmentations with different partition criteria on homogeneous regions to provide various interpretations for the given image [10]. Even though single segmentation cannot find all text regions, the set of all segmented regions by multiple segmentations could contain all text regions.

The proposed clustering algorithm estimates the distributions of colors in the image, and it finds modes of the color clusters. These clusters can be formed in different ways based on the definition of color similarity or the number of clusters. In some environments, a specific color distance metric is suitable for distinguishing text colors from background colors, but this might not be proper for other environments. As mentioned by Berkhin [11], there is no way to find an appropriate number of clusters with respect to all kinds of natural scenes. Therefore, the extraction system cannot identify the appropriate color metric and number of text colors in advance without any prior knowledge about the image. Instead, the proposed system generates multiple segmentations by varying the partition parameters of the segmentation algorithm so that it can extract character regions regardless of the outdoor environment or lighting conditions. Our aim is to produce sufficient segmentations of the input image to have a high chance of obtaining character regions.

Most characters are assumed to appear as single regions in scene images as shown in Fig. 1. For isolating text regions, we utilize three characteristics of the scene text: the color homogeneity, the geometric alignment, and the distinctiveness from the background. First, characters are assumed to have uniform colors so that the pixels in the character region can be grouped on the basis of their color values. Second, a text region as a periodic repetition of similar shaped objects with a specific alignment presents some of fundamental characteristics of texture. Even though the appearance of a single character depends on a certain font in an image, a text region has common block-based information which is distinctive from a background [12]. For instance, a text region can be characterized as a horizontal or vertical rectangular structure of clustered sharp edges [13]. This fact motivated us to use several heterogeneous texture features from the local and global spatial distribution of the text region, which represent those characteristics of text well. Third, the characters usually have sharp boundaries with the background regions because scene text is superimposed on objects for easy detection by humans. An edge map is utilized to force the groups of interior pixels and exterior pixels around the boundaries into two different clusters.

The proposed K-means clustering algorithm finds K dominant colors (centroids of clusters) where the differences between the color of the cluster centroid and the colors of the pixels belonging to the same cluster are minimized. Pixels in the character region can be grouped into the single region based on the one of K dominant colors. However, bare K-means clustering based on the color distribution often yields an inadequate segmentation result. The reason is that the portion of the text region in the image is relatively small in general so that it cannot find text colors as one of the dominant colors. To overcome the above problems, our image segmentation algorithm first estimates the probabilities of pixels belonging to text regions based on the combination of the texture features. This likelihood of text of pixels, called text saliency map, is used as assistant information in K-means clustering to find text colors by adjusting weight of color frequencies. In other words, the color instances that appeared in the potential text regions can have high weights by utilizing the text saliency map so that they are likely to be chosen as dominant colors.

In detail, a text localizer is designed to generate a text saliency map based on the combination of heterogeneous texture features: Mean Difference Features (MDFs), Standard Deviations (SDs), Histograms of Oriented Gradients (HOGs), and Edge Local Binary Pattern (eLBP). MDFs are calculated as the weighted mean of each text block, and they represent the spatial relationship among text blocks [14]. SDs show statistics on the intensity variance of blocks in the text region, and HOGs describe the strength regularity of text contours [15]. eLBP can capture not only texture characteristic but also structure characteristic of the given region, which is suitable for text detection [16]. For the text localizer, AdaBoost algorithm has been shown to be arguably the most effective method for detecting target objects in an image [17]. The selection of features and weights is learned through supervised training (offline mode) [18]. The AdaBoost algorithm with the cascade approach enables the algorithm to rule out most of the images as non-text locations with a few tests. The text localizer is applied to all sub-regions of the whole image at multiple scales to capture various font sizes. Most falsely detected text locations appear consistently less over multiple scales, while truly detected text locations appear coherently at multiple scales. Therefore, a text saliency map is created by projecting the responses (text=1, non-text=0) of the classifiers in the different scales back to the original scale of the input image. In other words, a text saliency map is initialized by zero. For each pixel at each scale, its response value for a text location is added to the text saliency map at the original image scale. Then, the responses in the text saliency map are normalized by the number of scales. Fig. 3(b) shows the text saliency map of Fig. 3(a) where confidence in text locations is represented by darkness.

Separating text region from the background is difficult under the condition that text is overlaid on scene images and it has low contrast with its backgrounds because of color similarities of both or shading. When clustering all pixels in the image, the color of text boundary pixels can be chosen as a representative color while dropping the true text color. It would bring up an unexpected result such that the text region is fused with background region, or it is over-segmented into small pieces (Fig. 4
                        ). To prevent the adverse effect of boundary pixels, we propose an edge map constraint, which forces text-colored pixels and background-colored pixels assigned into different clusters. In the scene image, the text pixel values are different from the background pixel values, so edges are formed in the boundary between text and background regions. Since the edge gives the boundaries of homogeneous color regions, the color values of a few pixels that lie normal to the contour are good indicators not to be assigned into the same clusters. Lists of two-color pairs in edge map constraints are obtained from the normal vectors of edge contour pixels [19] (Fig. 3(c)). To satisfy the edge map constraint, one color is assigned to the cluster of the closest centroid, and the other is assigned to the cluster of the other centroid.

The proposed generalized K-means clustering algorithm finds the most dominant K colors by calculating the weighted frequencies of color values and penalizing constraint violations in the edge-constraint map. By weighting pixels in salient sections and removing the effects of boundary pixels, colors in text regions and background regions are well separated. The proposed approach can be regarded as finding a solution that minimizes the constrained vector quantization error (CVQE) [20,21]. Given a pair of colors, sp
                         and sq
                        , the formula for CVQE is the following
                           
                              (1)
                              
                                 
                                    
                                       
                                          CVQE
                                          =
                                          
                                             1
                                             2
                                          
                                          
                                             
                                                
                                                   ∑
                                                   
                                                      k
                                                      =
                                                      1
                                                   
                                                   K
                                                
                                                
                                                   
                                                      
                                                         
                                                            ∑
                                                            
                                                               
                                                                  s
                                                                  p
                                                               
                                                               ∈
                                                               
                                                                  C
                                                                  k
                                                               
                                                            
                                                         
                                                         
                                                            
                                                               ω
                                                               p
                                                            
                                                            
                                                               D
                                                               Col
                                                            
                                                            
                                                               
                                                                  
                                                                     c
                                                                     k
                                                                  
                                                                  ¯
                                                               
                                                               
                                                                  s
                                                                  p
                                                               
                                                            
                                                            +
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                          
                                             
                                                ∑
                                                
                                                   
                                                      
                                                         s
                                                         q
                                                      
                                                      
                                                         s
                                                         r
                                                      
                                                   
                                                   ∈
                                                   EC
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            D
                                                            Col
                                                         
                                                         
                                                            
                                                               
                                                                  c
                                                                  ¯
                                                               
                                                               
                                                                  y
                                                                  
                                                                     
                                                                        s
                                                                        q
                                                                     
                                                                  
                                                               
                                                            
                                                            
                                                               
                                                                  c
                                                                  ¯
                                                               
                                                               qr
                                                               *
                                                            
                                                         
                                                         Δ
                                                         
                                                            
                                                               y
                                                               
                                                                  
                                                                     s
                                                                     q
                                                                  
                                                               
                                                               ,
                                                               y
                                                               
                                                                  
                                                                     s
                                                                     r
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                                ,
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where sp
                        , sq
                        , and sr
                         are color instances and the value of y(sq
                        ) is the index of the cluster to which the data point sq
                         belongs. C
                        1,…,CK
                         are the K clusters, and 
                           
                              
                                 
                                    
                                       c
                                       ¯
                                    
                                 
                                 1
                              
                              ,
                              …
                              ,
                              
                                 
                                    
                                       c
                                       ¯
                                    
                                 
                                 K
                              
                           
                         are their centroids. D
                        
                           Col
                        (s
                        
                           p
                        ,s
                        
                           q
                        ) is a function to calculate the distance between sp
                         and sq
                         based on the specific color distance metric Col. The distance metrics used in this paper are explained in Section 4.2. The text confidence score, ωp
                        , which is obtained from the text saliency map, changes the weights of the color frequencies of potential text regions. Weighted frequency of a color cluster is calculated by multiplying the likelihood of the text region and the count of pixels for the color cluster. Because pixels in the text areas have a high confidence score and most of the backgrounds have a low confidence score, the text colors can be chosen as one of the dominant colors by changing the weight of color frequencies that may contain text regions. Edge map constraint, EC in Eq. (1), is also utilized to force text colored pixels and background colored pixels into different clusters. When a cluster contains both color instances in EC, one color instance is moved into the other cluster where its centroid is nearest to the current cluster centroid (
                           
                              
                                 
                                    c
                                    ¯
                                 
                              
                              qr
                              ∗
                           
                         is the next closest centroid to either sq
                         or sr
                        ). That is to say, when colors in EC are assigned to the same cluster 
                           
                              
                                 
                                    
                                       c
                                       ¯
                                    
                                 
                                 
                                    y
                                    
                                       
                                          s
                                          q
                                       
                                    
                                 
                              
                           
                        , the edge map constraint forces them to be assigned to different clusters 
                           
                              
                                 
                                    
                                       
                                          c
                                          ¯
                                       
                                    
                                    qr
                                    ∗
                                 
                                 )
                              
                           
                         by adding a penalty, 
                           
                              
                                 D
                                 Col
                              
                              
                                 
                                    
                                       
                                          c
                                          ¯
                                       
                                    
                                    
                                       y
                                       
                                          
                                             s
                                             q
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                          c
                                          ¯
                                       
                                    
                                    qr
                                    ∗
                                 
                              
                           
                        . Therefore, character colors and background colors around boundaries are likely assigned to the different clusters.

The CVQE equation is solved by using the expectation–maximization (EM) algorithm. The CVQE is calculated for each possible combination of cluster assignments in every iteration, and the instances are assigned to the clusters that minimally increase the CVQE. By continuously updating the centroids of the clusters while minimizing the CVQE, the K dominant colors are obtained. For instance, as shown in Fig. 5
                        , image pixels of the scene image in Fig. 3(a) are separated into four different clusters according to the four dominant colors (the centroids of the clusters). Adjacent pixels with the same dominant color are grouped into a homogeneous region. A homogeneous region is called a connected component or simply a component. The pixels within a component are assumed to belong to the same object. That is to say, the label of each pixel is determined by the label of the component to which it belongs.

For robustly determining text color regardless of large variations in illumination and color, different partition parameters impose different criteria on homogeneous regions. By applying several color distance measures and various numbers of clusters in the CVQE equation, different interpretations are generated for the given image. In other words, the proposed multiple image segmentations produce sufficiently large plausible character region candidates. As a result, it can increase the chance of obtaining accurate text regions from multiple segmentations. Therefore, combining information of multiple segmentations is robust to the misleading single segmentation. In addition, it can handle the diverse image resolutions and various natural scene complexities.

The first parameter in the proposed generalized K-means clustering algorithm is the color distance metric (DCol
                        ). The metrics used in our framework are the Euclidean, HCL (hue, chroma, and luminance) [22], and NBS (National Bureau of Standards) [23] color distances. The Euclidean metric in the RGB color space measures the difference of color magnitudes, and it also calculates the changes in the luminance information. The HCL color metric is an angle-based similarity metric, and it emphasizes hue difference. The NBS color metric provides a perceptually uniform color distance. As opposed to the RGB color space, the color space used in NBS color metric is considered as natural representations of color space (i.e., close to the physiological perception of the human eye) [24]. If the NBS distance is less than 1.5, people can hardly distinguish the difference between two colors.

We found that these different metrics play complementary roles in color segmentations (Fig. 6
                        ). Images that exhibit a strong contrast between the foreground and background are usually better segmented with the Euclidean distance. In contrast, the color variation within the text region would be well handled using the HCL or NBS distance metrics. In particular, hue is less likely to be affected by illumination changes than are other color distance metrics. When images are affected by uneven lighting or curved surfaces, the HCL distance performs better. For instance, bright red and dark red have different magnitudes but similar color orientations. These colors, which are perceived as slightly different by the camera, can be merged by HCL color distance. The NBS color distance was devised to approximate human color perception. Because the colors of the scene text are chosen for effective notification to humans, it would be effective to use a color model close to human perception of colors.

For calculating HCL or NBS color distance, we need to convert the RGB color space to HCL or HCV color space. Color space conversions from the RGB to the HCL or HCV are explained in the literature [23,24]. Given a pair of RGB colors, p
                        =(rp
                        ,gp
                        ,bp
                        ) and q
                        =(rq
                        ,gq
                        ,bq
                        ), as well as the corresponding converted colors in the HCL and HCV color spaces, the color distance, DCol
                        , in Eq. (1) is defined as follows:
                           
                              (2)
                              
                                 
                                    
                                       
                                          
                                             D
                                             Euclidean
                                          
                                          
                                             p
                                             q
                                          
                                          =
                                          
                                             
                                                
                                                   
                                                      r
                                                      p
                                                   
                                                   −
                                                   
                                                      r
                                                      q
                                                   
                                                
                                             
                                             2
                                          
                                          +
                                          
                                             
                                                
                                                   
                                                      g
                                                      p
                                                   
                                                   −
                                                   
                                                      g
                                                      q
                                                   
                                                
                                             
                                             2
                                          
                                          +
                                          
                                             
                                                
                                                   
                                                      b
                                                      p
                                                   
                                                   −
                                                   
                                                      b
                                                      q
                                                   
                                                
                                             
                                             2
                                          
                                          ,
                                       
                                    
                                    
                                       
                                          
                                          
                                             D
                                             HCL
                                          
                                          
                                             p
                                             q
                                          
                                          =
                                          
                                             A
                                             L
                                          
                                          
                                             
                                                
                                                   
                                                      l
                                                      p
                                                   
                                                   −
                                                   
                                                      l
                                                      q
                                                   
                                                
                                             
                                             2
                                          
                                          +
                                          
                                             
                                                0.2
                                                +
                                                
                                                   
                                                      
                                                         h
                                                         p
                                                      
                                                      −
                                                      
                                                         h
                                                         q
                                                      
                                                   
                                                
                                             
                                          
                                          ∗
                                       
                                    
                                    
                                       
                                          
                                          
                                             A
                                             CH
                                          
                                          
                                             
                                                
                                                   c
                                                   p
                                                   2
                                                
                                                +
                                                
                                                   c
                                                   q
                                                   2
                                                
                                                −
                                                2
                                                
                                                   c
                                                   p
                                                
                                                
                                                   c
                                                   q
                                                
                                                cos
                                                
                                                   
                                                      
                                                         h
                                                         p
                                                      
                                                      −
                                                      
                                                         h
                                                         q
                                                      
                                                   
                                                
                                             
                                          
                                          ,
                                       
                                    
                                    
                                       
                                          
                                          
                                             D
                                             NBS
                                          
                                          
                                             p
                                             q
                                          
                                          =
                                          2
                                          ∗
                                          
                                             
                                                
                                                   c
                                                   p
                                                
                                                
                                                   c
                                                   q
                                                
                                             
                                          
                                          
                                             
                                                1
                                                −
                                                cos
                                                
                                                   
                                                      
                                                         2
                                                         π
                                                      
                                                      100
                                                   
                                                
                                                
                                                   
                                                      
                                                         h
                                                         p
                                                      
                                                      −
                                                      
                                                         h
                                                         q
                                                      
                                                   
                                                
                                             
                                          
                                          +
                                       
                                    
                                    
                                       
                                          
                                          
                                             
                                                
                                                   
                                                      c
                                                      p
                                                   
                                                   −
                                                   
                                                      c
                                                      q
                                                   
                                                
                                             
                                             2
                                          
                                          +
                                          16
                                          ∗
                                          
                                             
                                                
                                                   
                                                      v
                                                      p
                                                   
                                                   −
                                                   
                                                      v
                                                      q
                                                   
                                                
                                             
                                             2
                                          
                                          ,
                                       
                                    
                                 
                              
                           
                        where hue (hp
                        ,hq
                        ) refers to the pure spectrum colors and corresponds to the prominent color as perceived by a human. Chroma (cp
                        ,cq
                        ) corresponds to colorfulness relative to the brightness of another color that appears white under similar viewing conditions. Luminance (lp
                        ,lq
                        ) or volume (vp
                        ,vq
                        ) refers to the amount of light in a color. Further, AL
                         is a constant of linearization for luminance, and it is defined as 0.1. ACH
                         is a parameter that helps reduce the distance between colors having the same hue as the target color, and it is defined as 0.2+(hp
                        
                        −
                        hq
                        )/2.

The other parameter in the proposed generalized K-means clustering algorithm is the number (K) of clusters. The shapes and sizes of the regions in the different segmentations change with the number of clusters. When small K is used in the complex image, the text color cannot be chosen as one of dominant colors so that text region and background region can be merged. On the other hand, when large K is used in the simple image, many similar colors to the text color can be chosen as dominant colors so that text region can be over-segmented into sub-regions. Four different cluster numbers (K is from 2 to 5) are applied to provide different scales of partitioned regions for the given image. As a result, coarsely segmented regions as well as finely segmented regions can be realized from a single original image. These combinations of multiple scale segmentations are robust to variations in the image conditions (Fig. 7
                        ).

Image segmentation generates not only text components but also non-text components. From a large pool of components in multiple segmentations, we now need to find coherent groups of correctly segmented character regions and to prune the noise components. Because characters have common shape characteristics distinctive from those of background regions, the geometric shape of a single component is considered for verification measurement. In addition, characters usually have a similar font and color, so spatial relationships among neighboring components are also important factors to determine the label of these components. The probabilistic graphical models such as Markov random field (MRF) model [25] or conditional random field (CRF) model [9] provide a convenient way to model the properties of the individual component as well as spatial relationships among components in an undirected graphic structure. These graphical models encourage adjacent nodes to take the same label via spatial structure. The CRF model has the capability of unifying multiple features simultaneously in a single unified model. It fuses the local features and learns the conditional distribution of the class labeling given the components in the image.

We propose a local CRF model to label each component with one of the two classes (foreground or background) based on the properties of text. When the image is partitioned into multiple fragments with H different segmentation parameters, there are at most H image segmentations (Fig. 8
                     ). In each segmentation, a single component can be regarded as a candidate for a character region. Component verification processes using the local CRF model are applied to each segmentation in parallel. Each local CRF model can measure the posterior of a component being the foreground or background by analyzing the characteristics of the character regions. As a result, if the characteristic of the component is far from the normal properties of character regions, the component will be regarded as noise and removed.

In the CRF model, components are represented as nodes, and relationships among components are represented as edges (Fig. 8). The edges are formed when two components are located within a certain distance of each other. The distance is heuristically defined as three times the width of the narrower of the two components. The nodes are denoted as x∈
                     X, and their hidden labels are denoted as y∈
                     Y. The nodes with the connectivity structure imposed by the undirected edges between them define the conditional distribution P(y|x) over the hidden labels y. Formulated probabilistically, the extraction problem is derived as the conditional probability of the component labels for the given segmented image. Based on the Hammersley–Clifford theorem [25], the conditional distribution of the local CRF is factorized into a product of clique potentials ϕ
                     
                        c
                     (x
                     
                        c
                     ,y
                     
                        c
                     ), where a clique is a fully connected subgraph of the local CRF. These potentials evaluate how many and which labels are likely to occur, given information on the components. Using the clique potentials, the conditional distribution over the hidden labels is expressed as
                        
                           (3)
                           
                              
                                 P
                                 
                                    
                                       y
                                       |
                                       x
                                    
                                 
                                 =
                                 
                                    1
                                    
                                       Z
                                       
                                          x
                                       
                                    
                                 
                                 
                                    
                                       ∏
                                       
                                          c
                                          ∈
                                          C
                                       
                                    
                                    
                                       
                                          ϕ
                                          c
                                       
                                       
                                          
                                             x
                                             c
                                          
                                          
                                             y
                                             c
                                          
                                       
                                       ,
                                    
                                 
                              
                           
                        
                     where Z(x)=∑
                     
                        y
                     
                     ∏
                     
                        c
                        ∈
                        C
                     
                     
                     ϕ
                     
                        c
                     (x
                     
                        c
                     ,y
                     
                        c
                     ) is the partition function for normalization.

In this local CRF model, two clique potentials are designed on the basis of the clique configuration; namely, the unary potential and the pairwise potential (u is for unary, and p is for pairwise potential). The unary potential at each node i, ϕ
                     
                        u
                     (y
                     
                        i
                     ,x
                     
                        i
                     ), measures the likelihood of variable xi
                      taking a label yi
                      based on the character features. Non-text components have irregular feature values that are far different from those of the text components for the specific properties. In addition, the average saliency confidence value is used to reflect that a component in a potential text region has a high chance of being in the foreground. The definition of character features is given in Table 1
                      and characteristics of them are well explained in our previous paper [26]. For instance, if a size of the component is too large or small, it could be a noise component so that it should be discarded. Long bar-shaped component can be figured out as noise by the aspect ratio feature, and component having more complex contour shape than that of the text component can be determined as noise by the compactness feature. These shape features and text saliency confidence score contain the descriptions for the component such as geometric and texture properties of the single component. By utilizing these character-related features, the proposed system can determine labels of the components.

The machine learning classifiers such as artificial neural network (ANN) learn the distributions of feature values on the training data, and they find a discriminant hyperplane which can separate two classes based on the feature vectors. As a discriminant classifier, a multilayer perceptron (MLP) is used to produce likelihood f(y
                     
                        i
                     |x
                     
                        i
                     ) of the component over the label variable yi
                      given the character features xi
                     . The classification score f(y
                     
                        i
                     |x
                     
                        i
                     ) is normalized into the range between zero and unity. As a result, the output of the classifier on the given component can be viewed directly as an approximate conditional probability for each class. The unary potential is defined as
                        
                           (4)
                           
                              
                                 
                                    ϕ
                                    u
                                 
                                 
                                    
                                       y
                                       i
                                    
                                    
                                       x
                                       i
                                    
                                 
                                 =
                                 
                                    w
                                    u
                                 
                                 f
                                 
                                    
                                       
                                          y
                                          i
                                       
                                       |
                                       
                                          x
                                          i
                                       
                                    
                                 
                                 ,
                              
                           
                        
                     where w is a weight that represents the importance of different potentials for correctly identifying the hidden labels. The weights are learned from labeled training data by using a stochastic gradient descent algorithm [27].

In most images, text characters do not appear alone but together with other characters. Characters are subjected to certain geometric restrictions, i.e., their heights, widths and stroke thicknesses usually fall into specific ranges of values. The pairwise potential, ϕ
                     
                        p
                     (y
                     
                        i
                     ,y
                     
                        j
                     ,x
                     
                        ij
                     ), is intended to represent the probability of a given set of two neighboring components belonging to one of three classes (both foreground, both background, or one foreground and one background) based on the geometric relationships between the components. This pairwise potential encourages the smoothness of labels of neighboring nodes when they have similar properties in the image. Two different geometric relationships are considered. First one is discontiguous relationship, and the other is contiguous relationship. When two discontiguous components are located within a certain distance of each other, we evaluate the compatibility between their labels (Fig. 9
                     ). When they have similar properties, they have a high chance of sharing the same labels. Character features used for this purpose are color, texture, geometry, shape, and text saliency confidence (Table 2
                     ). Each similarity feature value is calculated by the ratio between the greater and lesser values of the character features of the two components (Eq. (5)). If the similarity values are closed to one, then the components are classified with the same labels. A statistical classifier is also trained to produce a classification score, f(y
                     
                        i
                     ,
                     y
                     
                        j
                     |x
                     
                        ij
                     ), over the label variables yi
                      and yj
                     , given character similarity features xij
                     .
                        
                           (5)
                           
                              
                                 
                                    x
                                    ij
                                 
                                 =
                                 
                                    
                                       min
                                       
                                          
                                             x
                                             i
                                          
                                          
                                             x
                                             j
                                          
                                       
                                    
                                    
                                       max
                                       
                                          
                                             x
                                             i
                                          
                                          
                                             x
                                             j
                                          
                                       
                                    
                                 
                              
                           
                        
                     
                  

Conversely, when two components touch, we merge the components and evaluate the adjacent pairwise potential score by using the classifier which is used for the unary potential calculation. If the merged component has high foreground likelihood, the adjacent pairwise potential gives a high probability of both components being in the foregrounds. Therefore, these two components are labeled as foreground both, and they are merged into a single component after the inference process (Fig. 10
                     ). Even though some character regions are fragmented into two or three pieces by improper image segmentation, this adjacent pairwise potential function can mitigate these side effects in which the fragmented regions can be misclassified as backgrounds.

By considering two different types of relationships between two components, the pairwise potential is defined as
                        
                           
                              
                                 
                                    ϕ
                                    p
                                 
                                 
                                    
                                       y
                                       i
                                    
                                    
                                       y
                                       j
                                    
                                    
                                       x
                                       ij
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   w
                                                   p
                                                
                                                f
                                                
                                                   
                                                      
                                                         y
                                                         
                                                            merge
                                                            
                                                               d
                                                               ij
                                                            
                                                         
                                                      
                                                      |
                                                      
                                                         x
                                                         
                                                            merge
                                                            
                                                               d
                                                               ij
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   w
                                                   p
                                                
                                                f
                                                
                                                   
                                                      
                                                         y
                                                         i
                                                      
                                                      ,
                                                      
                                                         y
                                                         j
                                                      
                                                      |
                                                      
                                                         x
                                                         ij
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                       
                                          
                                             
                                                if
                                                
                                                
                                                   x
                                                   i
                                                
                                                ,
                                                
                                                   x
                                                   j
                                                
                                                :
                                                contiguous
                                                ,
                                                
                                                   y
                                                   i
                                                
                                                ,
                                                
                                                   y
                                                   i
                                                
                                                :
                                                FG
                                                ,
                                             
                                          
                                          
                                             
                                                otherwise
                                                ,
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     where node j is a neighbor of the node i, which is obtained from Markov blanket in the graph.

Given all potential function values at the nodes and edges in the graph, labels of the nodes need to be determined in order to prune apparent non-text components. Inference techniques find the optimal configuration of labels, which maximizes the posterior probability of the hidden variables Y given the set of observations X among all possible configurations. In other words, the system finds the most probable joint class assignment y⁎ for the components given the local CRF model. This is defined as
                        
                           (6)
                           
                              
                                 
                                    y
                                    ∗
                                 
                                 =
                                 arg
                                 
                                    max
                                    y
                                 
                                 P
                                 
                                    
                                       y
                                       |
                                       x
                                    
                                 
                                 .
                              
                           
                        
                     
                  

Inferring the most probable solution of a CRF model is equivalent to minimizing an energy function. However, the energy minimization problem is NP-hard in general [28]. To solve the problem in the reasonable time, approximation approach called loopy belief propagation (loopy BP) [29] is used in this paper. This loopy BP method is simple to implement. Lan et al. [30] and Potetz [31] showed how belief propagation can be efficiently performed in graphical models containing moderately large cliques. Through the inference algorithm, most apparent non-text regions are pruned out, and the remaining regions are left as character proposals.


                     Fig. 11
                      shows some examples of multiple image segmentations and their corresponding unary potentials, and character proposals. Single component in each segmentation is marked in random color in the second columns of Fig. 11. Corresponding unary potential values on the components are marked in the gray-scale values where intensity brightness represents probabilities that belong to the foreground labels: the components having high potential values for being foreground shown in dark colors. By considering the likelihood of single regions and relationships among neighboring regions together in the local CRF model, components having the high maximum a posteriori (MAP) are shown in the fourth columns of Fig. 11 (MAP probabilities on the components are marked in the gray-scale values). We now need to find true character regions from a pool of remained character candidates, which will be explained in Section 6.

When generating multiple proposals for text regions from the multiple segmentations, most text components have higher foreground belief from the local CRF models so that they are retained as character proposals. However, some of the non-text components might have higher foreground belief so that they are also retained as character proposals. Faced with many character proposals obtained from multiple segmentations, we essentially need to look for the good components that are the most consistent based on the scene text model. Most are true positive regions but some may not be consistent with character boundaries. For instance, the shapes and sizes of the regions depend on the partition parameters. Some components can be over-segmented or merged with the background regions because of incorrect segmentation. Even though single segmentation cannot find all character regions, set of all proposals could contain most character regions. Therefore, instead of choosing the components in the best single segmentation and discarding all other components in the different segmentations, we suggest integrating all information of the character proposals obtained from multiple segmentations through a holistic CRF model.

The process of multiple proposal integration is the following: textline estimation and multiple proposal validation by the holistic CRF model. We assume that characters are linearly aligned in a textline and different textlines have different characteristics such as color, font, or size. Multiple proposals are grouped into several textlines based on the locations and sizes of the proposals. For each textline, most plausible combination of foreground regions is selected by validating the consistency of proposals in the textline.

We need to find the most plausible configuration among all possible character proposals as building character sequence interactively. However, exhaustive enumeration of character sequences is intractable. Instead of exhaustively integrating all character proposals at once, we group proposals into several potential textlines. A textline can be regarded as a linear sequence of characters. All components of text strings are assumed to be roughly aligned. Characters generally do not appear alone but with other characters having similar properties, such as font, size, and color. Fonts of characters in the textline rarely change, which implies that characteristic properties such as height or stroke thickness, etc. are maintained as constant. By grouping proposals into textlines, we can utilize a hierarchical structure of scene text, such as the collinearity among characters in a textline. In addition, this divide-and-conquer approach can reduce the complexities of the problem space.

A potential textline is constructed in the following manner. We focus on horizontal textlines (straight or slightly curved) with angles of less than 30°; nonetheless, vertical textlines can be added if needed. Based on the assumption that components with the higher posteriors on the foregrounds are more likely to be character regions, we sort all character proposals by their posterior values for the foreground which are calculated by local CRF models. The top-ranked unprocessed character proposal is selected first, creating an initial textline. This textline is expanded to the left and right by adding all neighboring unprocessed character proposals that satisfy five constraints which are defined to decide whether two components are neighboring of each other [5]. When the expansion is complete and more than two non-overlapping components exist within the textline, it is selected as a valid textline. All character proposals in the valid textline are marked as processed. All neighboring character proposals are connected with links during estimation of the textline. However, when there is only a single proposal in the textline, these isolated proposals are regarded as outliers and pruned. When the processing textline construction is done, the next unprocessed top-ranked character proposal is selected to initialize another textline. And the expansion process is repeated until no more unprocessed character proposals are available.

For instance, the letter “s” in Fig. 12(a) is selected as the initial element of the set, and its neighboring components (“A,” “h,” and some noise) are added to the set in Fig. 12(b). Expansion to the left and right is repeated alternately (Fig. 12(c), (d)). In Fig. 12(c), character proposals in the same row (horizontal axes) are obtained from the same image segmentation, on the other hand, character proposals in the same column (vertical axes) are overlapping proposals. Only one of the overlapping proposals should be selected as foreground, or all of them should be regarded as background. In Fig. 12(d), character proposals in different columns but connected with links are neighboring components. Finally, a minimum bounding box containing all elements of the set is selected as a textline. Other potential textlines are constructed in the same manner, and the total five textlines are extracted as shown in Fig. 12(e).

Given all possible combinations of the character proposal in the textline, we need to find the most plausible configuration while maximally satisfying conditions of the scene text model. A probabilistic integration method based on the graph optimization algorithm, called a holistic CRF model, is proposed for grouping character proposals to a text string by modeling the global information of multiple characters. It can capture global consistency among characters by reflecting high-order relationships such as text alignment where more than two characters are aligned on a straight line or a smooth curve. In addition, the proposed model can handle overlapping components of different segmentations together.

This holistic CRF model is also formulated within a probabilistic framework, where the maximum conditional probability represents the ideal character proposal configuration that we aim to find. In the holistic CRF model, the character proposals are represented as nodes, and links between two adjacent character proposals located within a certain distance are represented as edges. The distance for connecting the links is heuristically defined as three times the width of the narrower of the two proposals. A CRF graph for the character proposals in the given textline is shown in Fig. 13
                        ; only a partial CRF graph is shown due to the complexity of the graph and the limited space. When two proposals are adjacent, they are linked as neighbors (links are marked in blue colors). If they have similar characteristics, they have a high probability of being regarded as the same object. Conversely, local CRF models produce many potential character regions in multiple segmentations, and some of these overlap. If two proposals overlap, they are linked as mutually exclusive (links are marked in red colors). Overlapping components from different hypotheses may have different class labels. These mutual exclusion constraints prevent two overlapping proposals from both having foreground labels.

In order to alleviate the computational burden, obviously overlapping proposals that have identical shapes and lower foreground likelihoods are filtered out by non-maximum suppression. We retain the character proposals that have the highest likelihoods, or do not overlap significantly. For instance, two proposals among three ‘e’ regions in Fig. 13 are pruned so that the complexity of the graph becomes much simpler.

In the holistic CRF model, the unary potential is calculated based on the foreground likelihood obtained from the local CRF model. The pairwise potentials, which represent the relationship between two proposals, are differently defined according to the two types of edges: one for mutually exclusive relationship between overlapping proposals, and the other is for neighboring relationship between adjacent proposals. The pairwise potential of overlapping components is defined to be zero when both proposals would be assigned to the foreground so that they are never both selected in the foreground. The pairwise potential of adjacent proposals is defined as multiplication of the pairwise potential of a local CRF and a collinearity weight. The pairwise potential functions are defined as
                           
                              
                                 
                                    
                                       ϕ
                                       p
                                    
                                    
                                       
                                          y
                                          i
                                       
                                       
                                          y
                                          j
                                       
                                       
                                          x
                                          ij
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                0
                                             
                                          
                                          
                                             
                                                
                                                   w
                                                   p
                                                
                                                
                                                   λ
                                                   ij
                                                
                                                f
                                                
                                                   
                                                      
                                                         y
                                                         i
                                                      
                                                      ,
                                                      
                                                         y
                                                         j
                                                      
                                                      |
                                                      
                                                         x
                                                         ij
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                    
                                       
                                          
                                             if
                                             
                                             
                                                x
                                                i
                                             
                                             ,
                                             
                                                x
                                                j
                                             
                                             :
                                             overlapping
                                             ,
                                             
                                             
                                                y
                                                i
                                             
                                             ,
                                             
                                                y
                                                i
                                             
                                             :
                                             FG
                                             ,
                                          
                                       
                                       
                                          
                                             otherwise
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

The collinearity weight is calculated on the basis of the angular differences among up to four proposals. This weight is used to satisfy the assumption that neighboring components with the same alignment should have higher belief being foregrounds both [12]. Collinearity weight between the two components i and j, λij
                        , is defined as the function score of θij
                         in Eq. (7).
                           
                              (7)
                              
                                 
                                    
                                       
                                          
                                             λ
                                             ij
                                          
                                          =
                                          
                                             1
                                             2
                                          
                                          
                                             
                                                exp
                                                
                                                   
                                                      −
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        θ
                                                                        ij
                                                                     
                                                                     −
                                                                     
                                                                        θ
                                                                        hi
                                                                     
                                                                  
                                                               
                                                            
                                                            2
                                                         
                                                         
                                                            σ
                                                            θ
                                                            2
                                                         
                                                      
                                                   
                                                
                                                +
                                                exp
                                                
                                                   
                                                      −
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        θ
                                                                        ij
                                                                     
                                                                     −
                                                                     
                                                                        θ
                                                                        jk
                                                                     
                                                                  
                                                               
                                                            
                                                            2
                                                         
                                                         
                                                            σ
                                                            θ
                                                            2
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          ,
                                       
                                    
                                    
                                       
                                          
                                          
                                             θ
                                             hi
                                          
                                          =
                                          arg
                                          
                                             min
                                             
                                                θ
                                                hi
                                             
                                          
                                          
                                             
                                                
                                                   θ
                                                   ij
                                                
                                                −
                                                
                                                   θ
                                                   hi
                                                
                                             
                                          
                                          ,
                                          h
                                          ∈
                                          
                                             n
                                             i
                                          
                                          ,
                                          h
                                          ≠
                                          j
                                       
                                    
                                    
                                       
                                          
                                          
                                             θ
                                             jk
                                          
                                          =
                                          arg
                                          
                                             min
                                             
                                                θ
                                                jk
                                             
                                          
                                          
                                             
                                                
                                                   θ
                                                   ij
                                                
                                                −
                                                
                                                   θ
                                                   jk
                                                
                                             
                                          
                                          ,
                                          k
                                          ∈
                                          
                                             n
                                             j
                                          
                                          ,
                                          k
                                          ≠
                                          i
                                       
                                    
                                 
                              
                           
                        where θ
                        
                           ij
                         is the angle of a vector connecting the center points of the two proposals i and j. θ
                        
                           hi
                         and θ
                        
                           jk
                         are obtained from the neighboring proposals of i and j having the least angular distance from θ
                        
                           ij
                        . Two angular difference terms, θ
                        
                           ij
                        
                        −
                        θ
                        
                           hi
                         and θ
                        
                           ij
                        
                        −
                        θ
                        
                           jk
                        , are used to measure the collinearity between i and its neighbor j.

The collinearity weight becomes high when more than three proposals are aligned. By using the collinearity weighting scheme in the Belief Propagation approach, the influence from aligned neighbors is stronger than that from non-aligned neighbors in determining the label of the character proposal. Compared to the local CRF model, the holistic CRF model considers the geometric relationships among up to four proposals while maintaining low computational complexity of the lower-order CRF framework.

Inference and weight parameter learning are conducted in the same manner as the approach used in the local CRF model. Inference technique in the holistic CRF model finds the optimal configuration of labels for the given proposals. As a result, the label of each character proposal is determined from the most likely values according to the maximum a posteriori likelihood based on the holistic CRF model. For instance, proposals in red colors in Fig. 14(d) are selected as the final output in each textline.

@&#EXPERIMENTS@&#

To evaluate the performance of the proposed methods, we used two public datasets: ICDAR 2005
                        1
                     
                     
                        1
                        
                           http://algoval.essex.ac.uk/icdar/datasets.html.
                      and 2011
                        2
                     
                     
                        2
                        
                           http://robustreading.opendfki.de/wiki/SceneText.
                      Robust Reading Competition databases [32,33]. These public datasets contain various images taken under different natural conditions, such as location (outdoors or indoors), background clutter, and lighting. All images were normalized to a size with variables of 640×480pixels based on the longer dimension of the image. We manually segmented these realistic images to make the ground truth data.

Our system was coded with c++ language and operated on a PentiumD 3.4GHz Intel Machine with 4GB memory. To train a text localizer used for generating a text saliency map, a total of 15 K text segments are used as positive examples. And a total of 75 K non-text segments are obtained randomly outside of the text regions in the images as the negative segments (Fig. 15
                     ). To generate text saliency map, every sub-image is scanned 20 times using different sizes of sliding window. The first scan is conducted with a 32×12 window and subsequent scans are conducted with progressively larger windows.

To train discriminant classifiers for calculating the unary and pairwise potentials in the TCRF model, about 60 K text components and 120 K non-text components are collected. As shown in Fig. 16
                     , the distributions of feature values on the training data show distinguishable patterns for the different labels. The output of the discriminant classifier on the given the component can be viewed directly as an approximate conditional probability for each class.

In contrast to general unsupervised image segmentation algorithms, which tried to split the whole image into homogeneous region evenly, our image segmentation algorithm aimed to extract character regions as the whole regions without splitting multiple sub-regions or merging them with the background regions. In the process of image segmentation, we considered two character-related factors to extract text regions robustly. Two factors are color frequency weighting by the text saliency map and text/background color separation by the edge map. Pixels in the text areas have a high salient score and most of the backgrounds have a low salient score. Text color could be chosen as the dominant color based on the weighted color frequencies even though text area is occupied relatively small section. Edge constraint tries to prevent colors of boundaries between text regions and background regions are chosen for one of dominant colors by forcing neighboring colors around boundaries into different color clusters. As a result, edge constraint could maintain boundaries between text region and surrounding background regions.

The image segmentation performance is measured by matching ratio between the extracted region and the ground truth. When there is a component whose bounding box is overlapped with the bounding box of the ground truth and two regions are also almost identical, we determined they are matched. Overlap ratio between two regions is set 0.8. Each image in the validation set is evaluated and then the average of the matching ratio is obtained for the different conditions. Table 3
                         shows the accuracies of the image segmentation algorithm based on the four different conditions with the two parameters. With the assistance of two character-related factors on K-means clustering algorithm in Eq. (1), the extraction accuracy is much improved than no considering two conditions. The proposed image segmentation algorithm can robustly partition the image into homogeneous regions that correspond the character regions.

To evaluate the effects of multiple segmentations, we analyzed the coverage of ground-truth character regions by measuring that the image segmentation algorithm with different parameters obtains an accurate segmentation of characters. The ground truth is determined as matched when one of the extracted regions in multiple image segmentations is matched with the ground truth. If the ground character region set is represented as GC and the detected character region set is represented as DC, then the recall rate is defined as 
                           
                              r
                              =
                              
                                 
                                    
                                       
                                          DC
                                          ∩
                                          GC
                                       
                                    
                                    
                                       GC
                                    
                                 
                              
                           
                        . Fig. 17
                         shows the average recall for different combinations of cluster number (K) and color distance metrics (DCol
                        ). The horizontal axis shows a combination of different numbers of clusters. When the number of the cluster is three, image pixels are assigned to one of the three cluster centroids. The second column (# of clusters: 2, 3) of Fig. 17 indicates the matching recall on all components which are obtained by the image clustering with one of the two cluster numbers (K is 2 or 3). The section of Fig. 17 labeled 1 color metric to represent that image segmentation is conducted with only one color metric (Euclidean color metric) for calculating color distance. 2 color metrics in the figure include Euclidean and HCL color metrics and 3 color metrics in the figure include Euclidean, HCL, and NBS color metrics.

As we explained the limitation of single image segmentation, maximum performance of image segmentation is low when considering only single color distance metric (one of color distance metrics among Euclidean, HCL, and NBS) and single cluster number in Fig. 17. By increasing the number of segmentations, we can have a high chance of capturing a ground-truth region with a single component in any segmentations. When we use multiple segmentations of the image with the combination of three color distance metrics, most character regions can be found so that the maximum performance on the character region detection is improved. As shown in the partial segmentation results in Fig. 18
                        , the twelve different combinations of color metrics and cluster numbers (K is from 2 to 5) for each input image are sufficient to provide different interpretations for the given image. These combinations are robust to variations in the image conditions.

To evaluate the proposed method on the scene text extraction, we compared the proposed method and the baseline methods which extract the text regions with specific partition criterion. In details, we compared them according to two different parameter options. First, we evaluated the integration of image interpretations obtained by the different color distance metrics and the best image interpretation obtained by the single color distance metric. In this experiment, the baseline method selects the best segmentation result for all text color seeds for the given specific color similarity metric. Second, we also evaluated the coverage of the text regions obtained by the different types of region scales and the best image interpretation obtained by the fixed optimal cluster number. In this case, a baseline method selects the best segmentation result for all color similarity metrics for the given specific number of the clusters. Instead of using single best segmentation with the specific partition criterion in the two baseline methods, the proposed method combines all segmentation results obtained by using several color similarity metrics and different numbers of clusters and selectively integrates character proposals.

As mentioned in the literature [34], once the classification decision for each pixel is available, i.e. we know for each pixel whether it belongs to the character or not, the measure of recall and precision for scene text extraction can be applied on pixel level. We note then that the ground truth must be very precise in order to get robust measures. This is rarely the case as ground truth is mainly obtained through interaction between the images and a human observer, which can easily detect a character but can rarely locate it with 1-pixel precision. Instead, we count the number of corrected matched regions when two regions overlap text and background regions with satisfying the quality constraint, i.e. if the pixel-wise recall and precision are higher than the respective constraints. The constraint values are set as 0.8. Character regions obtained by the proposed method Ec
                         (estimates) are compared to character regions of the ground truth Tc
                         (targets). This uses the notion of a flexible match to a character c in a set of characters C, as 
                           
                              m
                              
                                 c
                                 C
                              
                              =
                              ma
                              
                                 x
                                 
                                    
                                       c
                                       ′
                                    
                                    ∈
                                    C
                                 
                              
                              
                                 m
                                 p
                              
                              
                                 c
                                 
                                    c
                                    ′
                                 
                              
                           
                        , where m
                        
                           p
                        (c,c′) denotes the ratio between the number of matched pixels and the area of the minimum bounding box containing both regions. The precision and recall of the character extraction are defined as
                           
                              (8)
                              
                                 
                                    p
                                    =
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   
                                                      c
                                                      e
                                                   
                                                   ∈
                                                   E
                                                
                                             
                                             
                                                m
                                                
                                                   
                                                      c
                                                      e
                                                   
                                                   T
                                                
                                             
                                          
                                       
                                       
                                          E
                                       
                                    
                                    ,
                                    r
                                    =
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   
                                                      c
                                                      t
                                                   
                                                   ∈
                                                   T
                                                
                                             
                                             
                                                m
                                                
                                                   
                                                      c
                                                      t
                                                   
                                                   E
                                                
                                             
                                          
                                       
                                       
                                          T
                                       
                                    
                                    ,
                                 
                              
                           
                        where the harmonic mean of precision and recall, which is called f-measure, is used to have a single performance value for the ranking of methods. It is defined as 
                           
                              f
                              =
                              1
                              /
                              
                                 
                                    
                                       
                                          α
                                          p
                                       
                                    
                                    +
                                    
                                       
                                          α
                                          r
                                       
                                    
                                 
                              
                           
                        , with a typically set to 0.5.

As shown in Fig. 19
                        , the proposed method shows better performance than any baseline methods for the color distance metric or the number of text color seeds. The baseline method with fixed cluster number degrades the performance especially when dealing with text with many different colors. Instead, the proposed method integrates all character proposals obtained from multiple segmentations with different cluster numbers (from 2 to 5), and it finds plausible combinations of text regions by selectively integrating them based on the probabilistic framework. Instead of using single best segmentation method, having multiple segmentations increase the possibility of finding character regions among all possible regions. Hence, the proposed method can handle the diverse image resolutions and various natural scene complexities.

In addition, we also evaluated the performance between the holistic CRF model and the heuristic searching approach on finding the sequence of characters. A baseline method finds the most the probable textline by sequentially extending from the most plausible proposal to its left and right neighbors. When the compatibility between the target proposal and the neighboring proposal is higher than other neighbors, this neighboring proposal is added into the textline. Comparison on extraction results between proposed method with holistic CRF model and the baseline method with heuristic searching approach is shown in Table 4
                        . The proposed TCRF model shows much higher performance than the baseline method. Whereas the baseline method only considers local relationship with neighboring proposals to determine optimal character sequences, the holistic CRF model considers higher-order relationships among characters and mutual exclusion constraints together. By using short-range and long-range interactions among the proposals in the TCRF model, we can ensure that unambiguous character proposals can send information to their neighbors to help disambiguate them. The experiment result showed that the proposed method can intelligently integrate multiple neighboring character proposals and handle competitive overlapping proposals.

For a fair comparison with existing approaches, we applied the evaluation criteria of the ICDAR 2005 and 2011 competitions respectively. However, the extraction results cannot be compared with those of any existing methods because the end-to-end text extraction was not part of the ICDAR competitions and text extraction results from no other method were presented for the dataset. Most previous methods using the ICDAR datasets were evaluated on text localization, not text segmentation. The criteria of the ICDAR competitions used words as the units for comparison. To group character regions from a textline into words, we applied a hierarchical clustering technique [35]. The technique is a part of the gap clustering algorithm, which distinguishes the ‘gaps between characters in the word’ and the ‘gaps between words’ in the textline. The algorithm partitions the textline into several words based on the ‘gaps between words.’

We employ the method by Wolf and Jolion [34] that is specifically designed to evaluate scene text detection approaches in ICDAR competitions. We used the DetEval evaluation software
                           3
                        
                        
                           3
                           
                              http://liris.cnrs.fr/christian.wolf/software/deteval/index.html.
                         with default parameters for evaluating the detection result of the proposed method. The goal of a word detection evaluation scheme is to take a list of T of ground truth words T
                        
                           i
                        ,
                        i
                        =1,…,|T| and a list of E of detected words E
                        
                           i
                        ,
                        i
                        =1,…,|E| and to measure the quality of the match between the two lists. Bounding boxes of words obtained by the proposed method Ew
                         (estimates) are compared to bounding boxes of words of the ground truth Tw
                         (targets). This uses the notion of a flexible match to a word w in a set of words W, as 
                           
                              m
                              
                                 w
                                 W
                              
                              =
                              ma
                              
                                 x
                                 
                                    
                                       w
                                       ′
                                    
                                    ∈
                                    W
                                 
                              
                              
                                 m
                                 p
                              
                              
                                 w
                                 
                                    w
                                    ′
                                 
                              
                           
                        , where m
                        
                           p
                        (w,w′) denotes the ratio between the area of intersection and the area of the minimum bounding box containing both words. Whereas evaluation criteria on ICDAR 2005 competition considers single match, evaluation criteria on ICDAR 2011 competition considers multiple matches, in the way it combines the figures in order to generate a single measure for multiple rectangles and multiple images. The precision and recall of the text localization are defined as
                           
                              (9)
                              
                                 
                                    p
                                    =
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   
                                                      w
                                                      e
                                                   
                                                   ∈
                                                   
                                                      E
                                                      w
                                                   
                                                
                                             
                                             
                                                m
                                                
                                                   
                                                      w
                                                      e
                                                   
                                                   
                                                      T
                                                      w
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             E
                                             w
                                          
                                       
                                    
                                    ,
                                    r
                                    =
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   
                                                      w
                                                      t
                                                   
                                                   ∈
                                                   
                                                      T
                                                      w
                                                   
                                                
                                             
                                             
                                                m
                                                
                                                   
                                                      w
                                                      t
                                                   
                                                   
                                                      E
                                                      w
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             T
                                             w
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        where recall illustrates the proportion of the ground truth rectangle which has been correctly detected, and precision decreases if the amount of additional incorrectly detected area increases. f-measure is also used to have a single performance value for the ranking of methods.

All performance measures are calculated for each image and then average values over all images are taken as the performance of the method. The proposed method achieves a high precision and a recall for ICDAR 2005 and 2011 datasets, which are comparable with the results of the existing methods, as shown by Tables 5 and 6
                        
                        . Let us further note that the ICDAR 2011 competition was held in an open mode, so authors only submitted outputs of their methods for a previously published competition dataset. In addition, winning method of the ICDAR 2011 competition has not been published, so we do not know how they actually built their system. The methods using stroke width transform (SWT) [7] or using maximally stable extremal regions (MSERs) [8,36] performed well but they required individual characters to be clearly visible. These methods had problems on blurry images or characters with low contrast. The local stroke width in the algorithm [7] is only well defined when the text is resolved clearly enough such that individual stroke is well delineated. As shown in the previous approaches, it is hard for a single segmentation algorithm to partition an image into its constituent objects in the real environment. Instead, we provided an effective way of utilizing image segmentation without suffering from its shortcomings. By varying the parameters of segmenting algorithm, some character regions in some of the segmentations can be found. Even though none of the segmentations are entirely correct, most character regions get segmented correctly at least once. As a result, our proposed method outperformed all published methods in ICDAR 2005 and 2011 competitions. Especially, high recall indicates that character regions are well detected even in the complex outdoor environments. Whereas all published methods in ICDAR competitions conducted word detection only, our proposed method conducted word detection and word extraction both. When considering that the other competitions also need to extract text regions from the detected words and the extraction errors will affect the performance of the final result.

In various cases, a computation time of 7.2s was required to process an image. The average processing time for each stage is listed in Table 7
                        . The preprocessing stage includes normalizing and enhancing the image as well as generating a text saliency map and edge map. The stages for image segmentation and character proposal generation are independently conducted in parallel by means of multi-threading, so that a short processing time is possible when handling a total of 12 multiple segmentations. Unlike the higher-order MRF model in the literature [37] that uses third-order cliques, we only consider second-order cliques due to its high efficiency. Compared to the approach [37], our proposed method also considers the geometric relationships between up to four proposals while maintaining low computational complexity of the lower-order CRF framework. As a result, the character proposal generation and the verification steps are relatively fast. Most of the computation time is spent on bottom-up image processing.

The computational time of competitors on ICDAR 2005 dataset is shown in Table 5, but there is no report on the computation time on ICDAR 2011 dataset. Even though the method of Alex [32] is the fastest algorithm than other entries, it conducted on text localization only not text segmentation, and it was less accurate. Even though the proposed method is slower than the method of Epshtein [7], it still works in an endurable time span. Nevertheless, the speed of the execution still needs to be improved although the proposed system reported encouraging performance. For the future work, the inference on the TCRF can be accelerated with sparse belief propagation [38] which shortens messages by reducing the dependency between weakly supported potentials. By eliminating unlikely characters from consideration in messages passed between the nodes of a graphical model, we can reduce the set of proposals that must be considered. In addition, we can reduce the computational time on bottom-up processing by finding the optimal combination of the partition parameters of multiple segmentation algorithms.


                        Fig. 20
                         shows examples of text extraction results by the proposed method. By applying the proposed method using the textline information, the character regions are well extracted and non-text areas are eliminated effectively in most cases. Integrating the information of character proposals from multiple image segmentations can provide a more robust basis for text extraction than using the information from one image segmentation alone. As a result, the proposed method is very robust to background clutter and color variations in most cases.

There are some errors occurred by the proposed method as shown in Fig. 21
                        . It cannot isolate text regions surrounded by background with the same color because multiple image segmentations rely on the color homogeneity of character regions and distinctiveness from background regions (Fig. 21(a)). Moreover, strong reflection effects cannot be handled since parts of character regions are totally missed. Fragmented or touched character regions cannot be handled well because we assumed that each character region is an atomic region, which is separated from others in a textline (Fig. 21(b)). Repeated background regions are confused as foreground regions (Fig. 21(c)). These regions may be solved by utilizing the lexicon model in the post-processing step. The proposed method cannot detect a single character region in textline because the method assumes that multiple characters form textline (Fig. 21(d)).

There are possible extensions for the proposed method to handle errors. Higher order relationships between different layers in scene text model can be embedded in the proposed method. For instance, the scene text model can be combined with the part-based object shape model which learns not only the shape of each part but also the location of the parts. This makes the method to be robust to the partial degradation of the text regions. Some languages such as Korean or Chinese, which consist not a single but multiple radicals to form a character, require more complicated concept of the scene text. By redesigning the hierarchy of the scene text model, we can expand the usage of the proposed method to multilingual languages. In addition, the character recognizer and the language model can be merged to find optimal configurations among proposals in textlines.

@&#CONCLUSION@&#

In this paper, we have presented a robust scene text extraction framework to separate text regions from an image with a complex background under various lighting conditions. To handle color gradation and variation under unconstrained environments, the proposed method adopts multiple image segmentations that provide alternative ways of grouping the pixels in the image. The multiple image segmentations can increase the chance of obtaining accurate character regions by providing the different interpretations of the given image. This approach allows label decisions regarding pixels to be postponed until the evidence across multiple segmentations has all been collected.

We have designed a two-stage CRF approach to generate and integrate multiple proposals on character regions from multiple image segmentations. It utilizes the properties and hierarchical structure of the scene text in a principled manner. Proposed probabilistic modeling finds optimal configuration of labels from the given multiple segmented regions. In the first stage, each local CRF model in a single segmentation identifies disjoint regions as foreground or background. Some regions that are strongly believed to be character regions are chosen as character proposals. In the second stage, a holistic CRF model integrates multiple character proposals and finds the most plausible configurations among those. In the holistic CRF model, higher-order relationships among characters and mutual exclusion constraints are embedded. Integrating the information of character proposals from multiple image segmentations can provide a more robust basis for text extraction than using the information from one image segmentation alone. In other word, it can recover extraction errors of the single segmentation.

Detailed experimental results and comparisons with the previous methods are reported in this paper. The proposed method achieves promising performance for the ICDAR 2005 and 2011 databases. It is able to extract text regions reasonably well under various backgrounds and lighting conditions.

@&#ACKNOWLEDGMENT@&#

This work was supported by the Industrial Strategic Technology Development Program (10035348, Development of a Cognitive Planning and Learning Model for Mobile Platforms) funded by the Ministry of Knowledge Economy (MKE, Korea).

@&#REFERENCES@&#

