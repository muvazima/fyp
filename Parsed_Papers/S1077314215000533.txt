@&#MAIN-TITLE@&#Incremental learning to segment micrographs

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           An incremental approach to annotation of images is demonstrated for segmentation.


                        
                        
                           
                           Density functions from training mimic those output by traditional annotation methods.


                        
                        
                           
                           Less burdensome for microscopists and allows correction and control on the result.


                        
                        
                           
                           The rationale of manual annotation is more easily accepted with this method.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Image analysis

Segmentation

Incremental learning

Image labelling

Microscopy

Microstructure

@&#ABSTRACT@&#


               
               
                  Supervised learning approaches to image segmentation receive considerable interest due to their power and flexibility. However, the training phase is not painless, often being long and tedious. Accurate image labelling can take several hours of expert operators’ valuable time. User interfaces are often specifically designed to assist the user for the task at hand. This is clearly unfeasible for most application domains.
                  We propose a simple segmentation framework based on classification and supervised incremental learning. A statistical model of pixel classes is learnt by incrementally adding new sample image patches to automatically-learned probability functions. Learning is iterated and refined in a number of steps rather than being executed in a one-shot training phase. We show that one-shot training and incremental labelling tend to produce similar statistical models, as the number of iterations grows. Comparable classification results are thus obtained with considerably less human effort.
               
            

@&#INTRODUCTION@&#

Segmentation of biological imagery is mostly concerned with the macroscopic world, often in the context of medical applications [20,6,7,2]. The lower extent of published work about segmentation of microscopic imagery can be justified by a number of problems in microscopy, such as: the large diversity of imaging approaches, the lack of a priori knowledge on the image content, the lack of common evaluation procedures, and the large variety of imaged objects for different disciplines [15]. Most of the existing literature on automatic segmentation of microscopic imagery is dedicated to biomedical data (e.g., analysis of cells; see [14,17] for a review) or to the recognition of particles, such as bacteria in biological images. Segmentation of micrographs of compact structures has attracted less interest so far. This is demonstrated by the structure and content of existing benchmark databases [3].

The large variety of materials and microstructure images makes it hard to devise a general segmentation approach. General-purpose segmentation methods fail on most classes of microscopy images due to several reasons, the most important being the image formation mechanism of microscopes. In the case of Scanning Electron Microscopes (SEMs), for example, the thin lens model for optical sensors turns out to be a reasonable approximation of electron imaging physics [9,5]. However, SEM imagery violates most of the assumptions at the basis of standard segmentation methods. In fact, pixel intensity is influenced, in the sampled point, by the orientation of the specimen surface with respect to the electron detector, as well as by the extent of the interaction volume, directly connected to the accelerating voltage [9]. Composition of the specimen at that location also plays an important role. As a result, pixel intensities alone do not contain rich and reliable enough information for segmentation. Usual image cues do not suffice for high quality segmentation. As a result, in most disciplines the segmentation of micrographs is mostly carried out with time-consuming user interaction [1,18], by means of techniques as simple as thresholding and basic morphological operators [11,10].

On the other hand, studying specific approaches for every application would be infeasible. More general approaches to manage images from different sources rely on unsupervised data mining. Successful examples are reported for cell segmentation [14,17,4]. More complex structures like those shown in this paper, however, present challenges that rule out most unsupervised approaches. The cheese micrograph in Fig. 3(a) exhibits several holes in the compact matrix, mainly due to whey pockets or to fat droplets, as shown in the manual annotation in Fig. 3(b). Without a proper training, most people would fail to tell the two apart and most microscopists do not even agree on some controversial cases. In our experience, unsupervised techniques generally output three main classes: compact matrix, highlight and crevices, and pores (i.e., large or regular holes). This is probably due to the well-distinguishable response of the most popular texture filters of the three classes. Unfortunately, fat holes and whey pockets often exhibit similar feature responses. Supervised learning approaches try to overcome this limitation by incorporating domain knowledge, with the help of annotated images (e.g., [12]). The statistical distribution of image features can be exploited to segment new images generated by a source with similar statistical properties. This pattern recognition approach has been fruitfully applied in several different contexts. An important property of statistical pixel classification is its ability to measure uncertainty (i.e., classification probability). Uncertainty scores are intuitive and easily related to background knowledge of microscopy experts. They can be used to reject classified pixels whose associated probability falls below a user-specified threshold (useful, e.g., in quantitative image analysis).

The main problem of classification approaches is the long, tedious, human-intensive training phase. In our experience, an accurate image labelling can take several hours for an expert operator to label each image [12]. Whatever user interface assisting the user and to reduce training time, should be specifically designed for the task at hand. This is clearly infeasible for most application domains. Still, the labelling burden remains.

We propose a simple segmentation framework based on Bayesian classification and supervised incremental learning. Conditional probabilities are learned by incrementally adding new sample image patches to automatically-learned probability functions. Supervised learning and image classification are iterated until satisfactory results are obtained.

We tested our method on SEM images of biological samples. We show that one-shot and incremental labelling produce similar statistical models. Thus, comparable classification results are obtained with considerably less human effort.

Although this work is motivated by the search for high-quality segmentation of biological specimens in the context of scanning electron microscopy, the general framework proposed in this paper virtually applies to every supervised segmentation algorithm in every context, irrespective of the imaging technology used.

The basic idea of the proposed framework is to organise the training procedure into two phases: unsupervised bootstrap and iterative supervised model refinement. In the bootstrap phase, training images are independently segmented using an unsupervised segmentation method. A statistical model of the training images is generated using the segmented images as labellings to infer pixel classes. In the iterative phase, the model is refined by letting the user correct segmentations. This process is iterated until acceptable classification results are obtained.


                     Fig. 1
                      shows a graphical description of the whole process. A number of images from the training dataset are automatically segmented using a clustering algorithm. Segmentations are employed as labellings for automatic learning of pixel statistics. The learned statistical model is used to segment again training images. The output segmentations are used to guide the user in order to suggest improvements to the model. A new model is then built by merging the current model with information learnt at each supervised step. This process is iterated until the user is satisfied with the classification results, or until no further improvement is gained. Notice that unsupervised segmentation is executed over one image, whereas supervised segmentation employs the merged statistics of the whole training dataset. The graphical separation into two blocks in Fig. 1 is intended to highlight the fact that the first learning phase is unsupervised or weakly-supervised (we will discuss this issue below), whereas successive steps require user intervention.

In our implementation, supervised learning is executed by image labelling. Classifications are presented in colour to the operator together with input images (see Fig. 2
                     ). The user can select misclassified regions and associate them with a different label. Image regions as well as single pixels can be selected, using whatever tool is available in the labelling software. We implemented our framework as a plugin for ImageJ [16]. The user can thus take advantage of all selection tools available in that software, plus a few custom ones. Each selection can be attached one of the labels defined in the initial step. The model is updated by merging new statistics extracted from user-selected patches. In this incremental approach, the operator is only asked to give exemplar and even imprecise labellings. Consequently, the training phase is less burdensome and time consuming for the user. Moreover, overfitting can be avoided by adding new labellings from time to accommodate for new misclassified images, during the natural life of the tool.

In the following pages, a fully-operational algorithm is presented. Notice, however, that our objective is not to propose a new segmentation algorithm. Rather, we simply provide a new framework to ease the training burden for whatever learning-classification algorithm one wants to use. In this paper, we choose a series of features to cope with the peculiarities of image datasets used in our experiments. Nonetheless, other features can be used for whatever application. The same is true for pre-processing and post-processing. Our experiments aim at showing that there is no remarkable difference if a classifier is trained in a single shot, or in a more distributed iterative fashion.

Image acquisition is an inherently noisy process in microscopy. Most of the acquisition noise is automatically removed by the device software by averaging over multiple acquisitions of the same sample. However, some noise is still present in micrographs that can affect the results of feature extraction.

Median filter followed by Gaussian blur is used to attenuate speckle noise as well as white noise. Although median filtering can remove image features, we experimentally found that filtering with a small window slightly improves segmentation results with our images. Then, the morphological minimum filter is applied to smooth out small irregularities around object borders. This improves classification quality at blob borders. Processed images are used for feature extraction.

Feature vectors are built from image pixels using whatever image filters one may choose for her application. The whole algorithm is then executed in the feature space. Hence, feature vectors can be computed beforehand and stored. Input images are normalised before filtering, to have zero mean and unit standard deviation to accommodate for varying illumination conditions.

In our implementation, we employ ten features: Maximum-Response (MR8) filter bank (8 responses), plus pixel brightness and a radial symmetry detector. The MR8 filter bank consists of 38 filters, but only 8 filter responses are used [19]. Filter responses are taken from two isotropic filters (a Gaussian and a Laplacian of a Gaussian (LoG), both at scale 
                           
                              σ
                              =
                              10
                           
                        ), an edge (first derivative) filter, and a bar (second derivative) filter (both at 6 orientations and 3 scales 
                           
                              
                                 (
                                 
                                    σ
                                    x
                                 
                                 ,
                                 
                                    σ
                                    y
                                 
                                 )
                              
                              =
                              
                                 {
                                 
                                    (
                                    1
                                    ,
                                    3
                                    )
                                 
                                 ,
                                 
                                    (
                                    2
                                    ,
                                    6
                                    )
                                 
                                 ,
                                 
                                    (
                                    4
                                    ,
                                    12
                                    )
                                 
                                 }
                              
                           
                        ). The response of the isotropic filters are used directly, whereas the oriented filters (bar and edge) contribute with one response for each scale, collapsed by using only the maximum filter response across all orientations. This gives 8 filter responses and guarantees rotational invariance.

The radial symmetry transform (RST) computes, for each pixel, the influence of nearby pixels along the gradient direction [13]. Pixels on radial boundaries receive high scores (in magnitude). Sign encodes the gradient direction. Scores are computed at various scales (
                           
                              
                                 r
                                 i
                              
                              =
                              
                                 {
                                 1
                                 ,
                                 2
                                 ,
                                 ⋯
                                 ,
                                 6
                                 }
                              
                           
                        
                        pixels) to take into account contributions of pixels in various neighbourhoods. RST is very effective in capturing borders of radially-symmetric objects. We use RST to characterise points with high radial symmetry, such as globular pores in a compact matrix. Globular structures are present in micrographs related to most disciplines, ranging from food samples to porous rocks. We employ large radii (
                           
                              
                                 r
                                 i
                              
                              =
                              
                                 {
                                 8
                                 ,
                                 11
                                 ,
                                 15
                                 ,
                                 19
                                 ,
                                 23
                                 ,
                                 27
                                 ,
                                 32
                                 ,
                                 36
                                 ,
                                 42
                                 }
                              
                           
                        
                        pixels) to characterise the inner part of pores, not only their borders.

Finally, the intensity feature is just the normalised input.

Since MR8 filters are normalised using the L
                        1 norm, filter responses are of the same magnitude. Contrarily to what suggested in [19], we do not apply the final Weber-like normalisation. RST is normalised as to have zero mean and unit standard deviation. This normalisation ensures that responses in the feature space have the same magnitude along all axes.

Each image is independently segmented using an unsupervised or weakly-supervised segmentation method. Segmentations are used as labellings to bootstrap the iterative procedure.

In our implementation, an initial labelling is automatically obtained from the feature vectors using clustering. Principal components are computed from the set of image feature vectors and only the most significant are retained. In our experiments, we retain only the principal components whose normalised energy sums to 0.8. In most of our experiments, three principal components suffice. In two cases, four components were needed. Clustering is executed in the Principal Component space using k-means. Initial centroids are provided by the user by selecting one pixel sample for each class, only for one training image. This is a straightforward operation for a domain expert, since classes represent morphological structures in the sample. Alternatively, the operator might provide only the number of classes. This, however, would not ensure fast convergence to the desired classes. Our earlier experiments showed that the learning rate is slower, as one might expect. Our microscopists think that this slight extra burden is a price worth to pay for faster convergence and, most of all, it gives the feeling of having more control on the system.

We model the statistical source using a simple Bayesian framework. Let us denote with ck
                         the kth class, 
                           
                              k
                              =
                              1
                              ,
                              ⋯
                              ,
                              K
                           
                        , and let 
                           
                              D
                              
                                 (
                                 x
                                 )
                              
                              =
                              
                                 
                                    {
                                    
                                       F
                                       m
                                    
                                    
                                       (
                                       x
                                       )
                                    
                                    }
                                 
                                 
                                    m
                                    =
                                    1
                                    ,
                                    ⋯
                                    ,
                                    M
                                 
                              
                           
                         be the feature vector for pixel x, where K is the number of classes and M is the number of features. We learn the model for the posterior probability 
                           
                              P
                              (
                              
                                 c
                                 k
                              
                              |
                              D
                              )
                           
                         from the likelihoods 
                           
                              P
                              (
                              D
                              |
                              
                                 c
                                 k
                              
                              )
                           
                         using Bayes’ theorem. Likelihood functions are simply modelled as normalised histograms over the feature space.

We assume that the priors P(ck
                        ) are equiprobable to avoid any bias on the data. These probabilities might be different for each class since, for example, the image area occupied by one class can be substantially higher than for other classes. Indeed, that is the case for many applications. Priors can be easily computed from training sets simply by counting the number of pixels in each class, provided that the training set is representative enough of the dataset.

In our framework, however, the user only selects small image samples as needed. There is no way to tell if the selected class is common or rare, from these samples alone. Priors could be computed from initial labellings, again by counting labelled pixels. However, it would not be wise to try to infer any crucial information from automatic labellings of a few images. Finally, too many uncontrolled elements can affect the percentage of pixels in one class over the total volume, especially when microstructure exhibits high variability, such as in natural materials. Consequently, equiprobability of priors, P(ck
                        ), is a reasonable choice.

Once we get the posterior probabilities 
                           
                              
                                 
                                    p
                                    ^
                                 
                                 k
                              
                              
                                 (
                                 x
                                 )
                              
                              =
                              P
                              
                                 (
                                 
                                    c
                                    k
                                 
                                 |
                                 D
                                 )
                              
                           
                        , we choose the class maximising 
                           
                              
                                 
                                    p
                                    ^
                                 
                                 k
                              
                              
                                 (
                                 x
                                 )
                              
                           
                         (MAP classification rule) as the new label for pixel x.

We adopt an incremental approach to learning mainly due to its versatility and its ability to adapt to new images. Following the definition in Giraud-Carrier [8], a learning algorithm is named incremental if, for any given sample 
                           
                              
                                 s
                                 1
                              
                              ,
                              ⋯
                              ,
                              
                                 s
                                 n
                              
                           
                        , it produces a sequence of hypotheses 
                           
                              
                                 h
                                 0
                              
                              ,
                              ⋯
                              ,
                              
                                 h
                                 n
                              
                           
                        , such that hi
                         depends only on 
                           
                              h
                              
                                 i
                                 −
                                 1
                              
                           
                         and si
                        . In the proposed framework, the initial hypothesis, h
                        0, is given by the unsupervised or weakly-supervised segmentation obtained in the initial step (k-means clustering, in our implementation). Samples si
                         are given as likelihood functions obtained from user re-labellings.

Images that have been classified at step 
                           
                              i
                              −
                              1
                           
                         are used as reference labellings during step i. Basically, label images are presented to the operator together with input micrographs for further refinement of learned statistics (Fig. 2). Image regions can be easily selected in label images and re-labelled. Based on the idea that re-labelling occurs mainly at misclassified regions, feature vectors in the misclassified region act as new samples to update the learned model in the most critical regions of the corresponding likelihood functions.

After the operator is satisfied with the labelling, statistics are gathered from the re-labelled regions alone and integrated into the current model. Let 
                           
                              
                                 p
                                 
                                    
                                       F
                                       m
                                    
                                 
                                 i
                              
                              
                                 (
                                 x
                                 )
                              
                           
                         be the likelihood of the feature Fm
                         computed at pixel x and step i, and 
                           
                              N
                              
                                 
                                    F
                                    m
                                 
                              
                              i
                           
                         be the number of samples used to learn 
                           
                              
                                 p
                                 
                                    
                                       F
                                       m
                                    
                                 
                                 i
                              
                              
                                 (
                                 x
                                 )
                              
                           
                        . The update is executed by merging old and new statistics with the expressions:
                           
                              (1)
                              
                                 
                                    
                                       
                                          
                                             
                                                N
                                                
                                                   
                                                      F
                                                      m
                                                   
                                                
                                                
                                                   i
                                                   +
                                                   1
                                                
                                             
                                             =
                                             α
                                             
                                                N
                                                
                                                   
                                                      F
                                                      m
                                                   
                                                
                                                i
                                             
                                             +
                                             
                                                
                                                   N
                                                   ¯
                                                
                                                
                                                   F
                                                   m
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (2)
                              
                                 
                                    
                                       
                                          
                                             
                                                p
                                                
                                                   
                                                      F
                                                      m
                                                   
                                                
                                                
                                                   i
                                                   +
                                                   1
                                                
                                             
                                             
                                                (
                                                x
                                                )
                                             
                                             =
                                             
                                                
                                                   α
                                                   
                                                      N
                                                      
                                                         
                                                            F
                                                            m
                                                         
                                                      
                                                      i
                                                   
                                                   
                                                      p
                                                      
                                                         
                                                            F
                                                            m
                                                         
                                                      
                                                      i
                                                   
                                                   
                                                      (
                                                      x
                                                      )
                                                   
                                                   +
                                                   
                                                      
                                                         N
                                                         ¯
                                                      
                                                      
                                                         F
                                                         m
                                                      
                                                   
                                                   
                                                      
                                                         p
                                                         ¯
                                                      
                                                      
                                                         F
                                                         m
                                                      
                                                   
                                                   
                                                      (
                                                      x
                                                      )
                                                   
                                                
                                                
                                                   N
                                                   
                                                      
                                                         F
                                                         m
                                                      
                                                   
                                                   
                                                      i
                                                      +
                                                      1
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    p
                                    ¯
                                 
                                 
                                    F
                                    m
                                 
                              
                              
                                 (
                                 x
                                 )
                              
                           
                         is the likelihood of Fm
                         at pixel x, computed from the re-labelled data, and 
                           
                              
                                 N
                                 ¯
                              
                              
                                 F
                                 m
                              
                           
                         is the number of corresponding samples. With reference to the definition above, all the likelihood functions 
                           
                              
                                 
                                    p
                                    ¯
                                 
                                 
                                    F
                                    m
                                 
                              
                              
                                 (
                                 x
                                 )
                              
                           
                         at step i constitute the new sample si
                        . The aging factor
                        
                           
                              α
                              ∈
                              [
                              0
                              ,
                              +
                              ∞
                              [
                           
                         reduces the incidence of previously acquired data (for α < 1) thereby giving more importance to new samples. The extreme values of α represent the cases in which the old model is completely discarded (
                           
                              α
                              =
                              0
                           
                        ) or no new information is taken into account (
                           
                              α
                              →
                              +
                              ∞
                           
                        ). In our experiments, we empirically set 
                           
                              α
                              =
                              0.01
                           
                        . Such a low value causes our method to strongly rely on newcomers. Hence, the whole algorithm is sensitive to wrong labellings, possibly leading to catastrophic forgetting of peculiarities of the previously learned model. This is a risk. However, our main objective is a fast learning rate. Most microscopists might be willing to risk occasional loss of data, if they are rewarded with faster learning rates. Moreover, the training software can be equipped with an undo function to go one step backward in the iterative process. Hence, possible damages to the model trained so far can be undone at the cost of the time wasted in the last iteration.

The underlying idea to this incremental strategy is that, whatever the initial shape of the density functions 
                           
                              
                                 p
                                 
                                    
                                       F
                                       m
                                    
                                 
                                 i
                              
                              
                                 (
                                 x
                                 )
                              
                           
                        , the process should ideally converge towards the perfect statistical model for the imaging system and samples at hand. In particular, the model generated by our framework should be close to likelihood functions learned from an exact pixel-by-pixel labelling, provided that the latter is a good model of the data.

The important point here is that the user does not have to be precise. In fact, rough region selections should suffice for successful learning, since misclassifications can be corrected in further refinement learning steps. Provided that partial and incorrect labellings are acceptable, this allows for a speed-up in the manual labelling process. As a side effect, re-labelling previous classifications is much easier than labelling images from scratch since, for example, they allow the selection of irregularly-shaped regions with one mouse click as well as connected regions of pixels sharing the same classification. Again, this helps to speed-up labelling.

Notice that classes are chosen by to the microscopist in the initial step, and input to k-means or whatever segmentation algorithm one might choose. In the framework discussed so far, new classes appearing after the initial training cannot be added. However, this is a limitation of our implementation, not of the framework itself. A new class can be processed by making room for a new likelihood function in the model. Its shape is learned from the given examples as soon as they are provided by the user. From the user perspective, it suffices to add a “new class” button to the user interface in the program. Selection and labelling can be done in the very same way as iterative training.

Although each pixel is classified using structured neighbourhood information, its classification is independent from classifications of neighbouring pixels. That is, pixels are assumed statistically independent. This makes our method intrinsically pixel-wise. As a result, some pixels can get a different classification from their surrounding, resulting in isolated (often misclassified) points. In our implementation, a simple re-classification procedure is adopted to smooth out those misclassified pixels. Let 
                           
                              p
                              ¯
                           
                         be a pixel with classification 
                           
                              C
                              (
                              
                                 p
                                 ¯
                              
                              )
                           
                        , and let 
                           
                              
                                 Ω
                              
                              (
                              
                                 p
                                 ¯
                              
                              )
                           
                         be its neighbourhood. If 
                           
                              C
                              
                                 (
                                 p
                                 )
                              
                              
                                 
                                    0.25
                                    e
                                    m
                                 
                                 
                                    0
                                    e
                                    x
                                 
                              
                              ≠
                              
                                 
                                    0.25
                                    e
                                    m
                                 
                                 
                                    0
                                    e
                                    x
                                 
                              
                              C
                              
                                 (
                                 
                                    p
                                    ¯
                                 
                                 )
                              
                              ,
                              ∀
                              p
                              ∈
                              
                                 Ω
                              
                              
                                 (
                                 
                                    p
                                    ¯
                                 
                                 )
                              
                              ,
                              
                                 p
                                 ¯
                              
                           
                         takes the classification shared by the majority of its neighbouring pixels. This simple procedure assures the absence of isolated pixel classifications, thus reducing the probability of misclassifications.

@&#EVALUATION@&#

The proposed method was tested on two datasets of SEM images, drawn from different application domains: dairy products and medical research (Fig. 3
                     ). Our framework can easily be applied to different samples, as well as with different imaging technologies. Clearly, image features used for detection should be carefully chosen for the task at hand. The proposed iterative learning framework, however, does not depend on the specific features chosen, as well as on image processing procedures.

Ten SEM images of Ragusano cheese were acquired at an accelerating voltage of 15kV, a working distance of 18mm, and a magnification factor of 
                        
                           1000
                           ×
                           
                        
                     . The main microstructural features of images were gathered into morphologically-meaningful classes: fat globules (blobs of fat), whey pockets (void areas originated from cracks or water), and protein matrix (compact protein structure). Besides, a dummy class was added to encode artifacts and to ease the segmentation. Basically, fat globules often exhibit smooth greylevel gradients, sometimes making them hardly distinguishable from the protein matrix even by expert operators. However, artifacts often appear around cavities in the protein matrix, especially those left by fat pores. This highlight effect is due to several intrinsic factors to scanning electron imaging, and it is strongly influenced by the accelerating voltage [5]. Namely, steep gradients in the protein matrix surface reflect electrons in a wider range of directions. Reflected electrons thus get more chance to hit the detector, provided that it is positioned in front of the surface. This produces artifacts on the matrix that look like specular highlights caused by directed light (see Fig. 3(a)). The accelerating voltage controls the number of electrons emitted by the beam. High voltage thus strengthens this highlight effect. That is why we use an accelerating voltage as high as 15kV. We found that the highlight dummy class has a well-recognisable statistical behaviour and it is thus easily captured by statistical models. Using this class prevents most protein pixels to be misclassified as fat or vice versa. After classification, highlight pixels are re-labelled as protein matrix.

The medical dataset consists of ten dentine SEM images. Dentine is a flat surface of tubular tissue under enamel in tooth. Tubes are clearly visible as pores in the imaged surface. Here, the following acquisition parameters were used: accelerating voltage of 20kV, working distance of 20mm, and magnification factors of 
                        
                           1000
                           ×
                           
                        
                      (two images) and 
                        
                           2000
                           ×
                           
                        
                      (eight images). Two classes are defined: dentine tissue (flat surface) and void pores.

The two test datasets were manually labelled by an expert SEM operator. Annotation consists in manual drawing geometric selection, consistently painted with a chosen class colour. Fig. 3 shows an example for each image dataset, together with manually-annotated ground truth. In the cheese image (Fig. 3(b)), black corresponds to protein matrix, red (dark grey, in the printed version) to fat, and green (light grey) to whey (colours are available in the electronic version). In the dentine image (Fig. 3(d)), black corresponds to pores, whereas the compact matrix is coloured as yellow (light grey). Approximately, it took about 8h per cheese image for one-shot annotation, whereas annotation time for each step in the incremental approach varied from 10 to 30min per step. Computing time to build the model and produce new classifications for another round of labelling is in the order of seconds or of a few minutes.

We compared the segmentation results to ground truth using a Leave-one-out strategy, i.e., one image is used for testing whereas the statistical model is built from the remaining images. We used the same method to classify images during iterative learning. In our tests, we measure accuracy, i.e., the classification error as the number of misclassified pixels, normalised with respect to the total number of pixels in the image. For comparison, we also computed segmentation accuracy for the one-shot trained models, obtained from manual annotation using the same Bayesian approach. Table 1 shows the accuracy at five iterations with the cheese dataset. After a few iterations, the classification results of the incremental method are comparable (and often better) than the performance of the statistical model learnt in a one-shot training phase. Notice that in the first iteration the classification quality drops and then it improves again. This is due to the fact that the initial model is learned in an unsupervised step from automatic classifications, whereas the following steps are interactive. Namely, the initial labelling, obtained from clustering and used for training, can contain a large number of misclassified pixels. This can deteriorate the quality of the initial model, h
                        0. Also recall that unsupervised segmentation is run independently for each training image. The unsupervised model is thus adapted to each image, whereas the initial model h
                        0 tries to simultaneously explain all training images. Hence, this model suffers both from over-adaptation to single images, and from the fact that misclassified pixels contribute to the model itself, as well as correctly classified ones. A similar trend can be observed in Table 2
                        
                         for the dentine dataset. Not all images get a better classification, and in one case (image 10) the performance is severely compromised. This is probably due to the large difference in the images from this dataset (see Fig. 4).
                           
                              (3)
                              
                                 
                                    
                                       
                                          
                                             
                                                D
                                                
                                                   
                                                      
                                                         χ
                                                         ˜
                                                      
                                                   
                                                   2
                                                
                                             
                                             
                                                (
                                                
                                                   M
                                                   1
                                                
                                                ,
                                                
                                                   M
                                                   2
                                                
                                                )
                                             
                                             =
                                             
                                                ∑
                                                
                                                   k
                                                   =
                                                   1
                                                
                                                K
                                             
                                             
                                                ∑
                                                
                                                   m
                                                   =
                                                   1
                                                
                                                M
                                             
                                             
                                                
                                                   
                                                      χ
                                                      ˜
                                                   
                                                
                                                2
                                             
                                             
                                                (
                                                
                                                   p
                                                   
                                                      1
                                                   
                                                   
                                                      k
                                                      ,
                                                      m
                                                   
                                                
                                                ,
                                                
                                                   p
                                                   
                                                      2
                                                   
                                                   
                                                      k
                                                      ,
                                                      m
                                                   
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where
                           
                              (4)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      χ
                                                      ˜
                                                   
                                                
                                                2
                                             
                                             
                                                (
                                                p
                                                ,
                                                q
                                                )
                                             
                                             =
                                             
                                                1
                                                2
                                             
                                             
                                                ∑
                                                i
                                             
                                             
                                                
                                                   
                                                      
                                                         (
                                                         
                                                            p
                                                            i
                                                         
                                                         −
                                                         
                                                            q
                                                            i
                                                         
                                                         )
                                                      
                                                   
                                                   2
                                                
                                                
                                                   
                                                      p
                                                      i
                                                   
                                                   +
                                                   
                                                      q
                                                      i
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        Here, 
                           
                              M
                              1
                           
                         and 
                           
                              M
                              2
                           
                         are the statistical models compared, 
                           
                              
                                 p
                                 
                                    1
                                 
                                 
                                    k
                                    ,
                                    m
                                 
                              
                              ≡
                              
                                 P
                                 1
                              
                              
                                 (
                                 
                                    c
                                    k
                                 
                                 |
                                 
                                    F
                                    m
                                 
                                 )
                              
                           
                         and 
                           
                              
                                 p
                                 
                                    2
                                 
                                 
                                    k
                                    ,
                                    m
                                 
                              
                              ≡
                              
                                 P
                                 2
                              
                              
                                 (
                                 
                                    c
                                    k
                                 
                                 |
                                 
                                    F
                                    m
                                 
                                 )
                              
                           
                         being the corresponding probability functions, K is the number of classes, M is the number of features, ck
                         is the kth class, and Fm
                         is the mth feature. Scores for the two datasets are shown in the corresponding tables. Eqs. (3) and (4) imply that low values represent better similarity between the compared models. As the two tables clearly show, the incrementally learned models tend to become closer and closer to the models obtained from one-shot training with manually labelled images. There is no progress in the dentine model after only two iterations, whereas the cheese model needed more iterations. This is probably due to its greater complexity: the cheese model has four classes, whereas the dentine model has only two (see Table 3 and 4.)
                        
                     

A few segmentation results are presented in Fig. 5. The classification image in Fig. 5(a) shows that most structures are correctly classified. Fat globules are well marked even if no geometric template is explicitly used to recover elliptical objects. Moreover, the border of most fat globules is more accurately marked than in the corresponding manual labelling image. This is partly a consequence of incremental learning. Accurate results can be observed for the dentine example in Fig. 5(b). However, in this case the performance degraded both with respect to one-shot training and to the initial step. This is mainly an effect of the variety in the dataset, as Fig. 4 suggests. Misclassified pixels mainly lie in regions where the compact structure is slightly depressed, such as in the centre-right of Fig. 5(b). This is probably caused by the radial feature (RST). RST is of great value in identifying globular structures. On the other hand, depressed regions often assume a radial intensity gradient in the image. Hence, they can be easily misclassified as in Fig. 5(b).
                        
                     

As shown in Section 3.1, the statistical model acquired by one-shot training can be accurately approximated using the proposed incremental learning framework. Incrementality allows faster bootstrap of the system since it reduces the time burden of the initial training phase. Model imperfections can be corrected by re-labelling misclassifications or by adding new sample images. These advantages, however, should translate into user satisfaction about the software product implementing the incremental framework, usability and purposiveness being among the main objectives.

With the help of an expert microscopist, we extensively compared the incremental method to traditional one-shot image labelling. Far from being exhaustive, this comparison only aims at understanding the potential of the incremental framework and its limitations.

Segmentation systems based on supervised learning present a number of drawbacks that make them less friendly to non-expert users: the training phase is perceived as somehow tricky and its usefulness is not fully understood, the whole learn/classify structure is not considered sufficiently intuitive, the training procedure is considered boring and burdensome, the time complexity of the classification phase does not allow interactive response. Conversely, standard image processing tools based on simple off-the-shelf procedures are considered simple, intuitive, and fast. The classification takes few seconds on most standard machines. This would be considered acceptable in most contexts. Nonetheless, microscopists often expect interactive responses. It is somehow surprising since preparing a sample for observation and observing it can take far more time than image classification. This can be partly explained by the fact that most computer activities are perceived as instantaneous.

The proposed framework shares most of these drawbacks. However, the training phase is deemed faster and can be carried out in shorter steps. This is perceived as a great improvement over one-shot labelling. Due to its automatic bootstrap phase, the incremental framework is more user-friendly since it presents roughly-labelled images for re-labelling, rather than blank images to be labelled from scratch. The incremental framework is also more intuitive and assures a greater control over learning, since the user can improve the statistical model in further learning steps, until the segmentation quality is sufficiently high. The model can be adapted to new image samples without executing the whole training from scratch. Last, and most important, the image labelling is not required to be precise. Sparse, imprecise, and incomplete labellings are acceptable contributions for learning, thus allowing much less attention to be put on the labelling task.

@&#LIMITATIONS@&#

The most fundamental limitation of our method is the use of the Bayesian framework for statistical learning. Bayesian learning is not sufficiently flexible to support class manipulation, such as splitting and merging operations between different classes. Another important limitation lies in the way likelihoods are learned and, again, originates from the Bayesian model. Basically, the regions selected by the user to update the statistical model are used only as positive examples. That is, all feature vectors corresponding to pixels in the selected image region are labelled according to the label chosen by the user. Only the likelihood function corresponding to the chosen label is updated. However, negative examples would probably give a greater contribution if the system could be told that those feature vectors were misclassified. Namely, the part of the statistical model related to the wrong classes could be consequently updated (e.g., by lowering the probability of the corresponding features to belong to those classes) resulting in a faster learning rate. Both these drawbacks could be avoided by using a more powerful model, such as neural networks.

@&#ACKNOWLEDGMENTS@&#

The authors wish to thank Dr. Bilge Hakan Sen from Department of Endodontology, Ege University, Izmir, Turkey for providing the dentine images used for testing. We also thank anonymous reviewers for insightful comments on earlier versions of this paper. Financial support was provided by the Assessorato Agricoltura e Foreste della Regione Siciliana, Palermo, Italy.

@&#REFERENCES@&#

