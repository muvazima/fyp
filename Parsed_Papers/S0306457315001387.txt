@&#MAIN-TITLE@&#Effective language identification of forum texts based on statistical approaches


@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           This investigation deals with the problem of language identification of noisy texts.


                        
                        
                           
                           Two statistical approaches are proposed: High Frequency Approach and Nearest Prototype Approach.


                        
                        
                           
                           The proposed methods are evaluated on forum datasets containing 32 different languages.


                        
                        
                           
                           An experimental comparison is made with LIGA, NTC, Google translate and Microsoft Word.


                        
                        
                           
                           Results show that the proposed approaches are interesting in language identification of forum texts.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Natural language processing

Automatic language identification

Forum texts

Hybrid approaches

Statistical approaches

N-Grams

@&#ABSTRACT@&#


               
               
                  This investigation deals with the problem of language identification of noisy texts, which could represent the primary step of many natural language processing or information retrieval tasks. Language identification is the task of automatically identifying the language of a given text. Although there exists several methods in the literature, their performances are not so convincing in practice.
                  In this contribution, we propose two statistical approaches: the high frequency approach and the nearest prototype approach. In the first one, 5 algorithms of language identification are proposed and implemented, namely: character based identification (CBA), word based identification (WBA), special characters based identification (SCA), sequential hybrid algorithm (HA1) and parallel hybrid algorithm (HA2). In the second one, we use 11 similarity measures combined with several types of character N-Grams.
                  For the evaluation task, the proposed methods are tested on forum datasets containing 32 different languages. Furthermore, an experimental comparison is made between the proposed approaches and some referential language identification tools such as: LIGA, NTC, Google translate and Microsoft Word. Results show that the proposed approaches are interesting and outperform the baseline methods of language identification on forum texts.
               
            

@&#INTRODUCTION@&#

Knowing the actual document language, in advance, is a crucial step before starting any task in Information Retrieval (IR), and that is why language identification represents an important research field in text mining and computational linguistics.

In this research work we deal with the problem of language identification in multilingual and noisy texts. Nowadays, there is relatively little research work in this field compared to other text mining (TM) domains, because this task is not considered as a hard problem, and more researchers are interested in the identification of spoken documents (speech language identification) or text categorization into topic or author.

In reality, there exist different categories of texts, such as books, news, and forums, for which the task of language identification can be performed. In the framework of this investigation, we are interested in language identification in discussion forum documents. This type of document presents a high level of difficulty in practice. In fact, the forum is organized around an open dynamic platform, allowing different people to communicate and share the communication. Anyone can post a new discussion, or reply to other messages in his own way and his own writing style, without any restriction on the manner of communication. For this reason, the forums present a high level of difficulty and ambiguity in some cases. Moreover, we can find a mixture of different languages in the same message, SMS writing style, HTML special characters and tags, person tags, etc.

In the framework of this investigation, we have built a new dataset of forum texts, which we called DLI32, containing 32 different languages. The details of DLI32 can be found at the following website: http://scholarpage.org/ouamour. However, there are some difficulties inherent in this dataset, since it contains some pairs of languages that are quite similar in vocabulary and grammar. It also contains many close languages that share the same character set, at least in part.

To deal with the language identification problem, we propose two statistical approaches, which we have called high frequency approach and nearest prototype approach. In the high frequency approach, we have introduced five new algorithms: two algorithms based on characters, an algorithm based on words, and two hybrid algorithms. Within the nearest prototype approach, we have proposed 11 different distance measures such as: Euclidean distance, Cosine distance, Manhattan distance, Chi2 distance, Squared Euclidean distance, Bray Curtis distance, Canberra distance, Correlation distance, Histogram Intersection distance, Bhattacharyya distance and Out of Place distance (introduced by Cavnar et al.). In this last approach, we used character N-Grams as features.

In the word based algorithms, we decided to limit our number of features to only 20 common words, unlike several other authors who have used more than 100 words as features to identify the language. Also, in our experiments, we used a small noisy dataset to train the proposed algorithms, whereas most existing research work uses large datasets for the training. Regarding the noise present in the datasets, we propose three preprocessing steps that are based on the removal of insignificant characters and the separation of contracted words.

In order to evaluate the objective performances of the proposed algorithms, we have made a comparison of our approaches with some of the state of the art techniques and tools, such as: the LIGA approach, the NTC approach, Google Translate and Microsoft Word.

This paper is organized as follows. In Section 2, we present the background and some previous work in language identification. In Section 3, we define our datasets and the character encoding in details. The proposed approaches of language identification are described in Section 4 (high frequency approach and nearest prototype approach). Section 5 presents the results of the experimental tests and comparisons with the state of the art. Finally, the paper ends with a general conclusion and some prospects for this research work.

@&#RELATED WORK@&#

This section describes some related works on language identification (LI), which are based on machine learning and statistical approaches. Thus, in (Hughes, Baldwin, Bird, Nicholson, & MacKinlay, 2006), the authors present a review of previous research in written language identification.

Several studies have been conducted with statistical approaches. For instance, Cavnar & Trenkle (Cavnar & Trenkle, 1994) developed an N-Gram based text classifier, where they introduce a new distance measure called out of place distance. This distance is based on summing the differences between the frequency ranks of the document under test and those of the reference language samples.

Another work based on N-Grams (Martins & Silva, 2005) was conducted on web pages, with a new distance measure. The authors used a dataset collected from newsgroups containing 12 different dissimilar languages.

A different approach was used in the language identification of online text in (Beesly, 1988), where the authors employed mathematical models that were originally proposed as a cryptanalysis technique. The authors did not perform an evaluation of their approach on a dataset, but they suggested that their approach might be used in various types of data analysis.

A study of language identification was also presented in (Milne, O'Keefe & Trotman, 2012), where the authors made a comparison between several methods, namely: Cavnar method, Padded Tri-Grams method, Top 1000 words method and LangDetect tool. They tested their approaches on different collections of datasets with different text sizes. The authors concluded that the effectiveness of each method was totally dependent on the size and type of the texts, and that in general longer texts produced better results.

Another study of two LI schemes was reported in (Grefenstette, 1995), in which the authors compared a Trigram based technique with a small words based technique. Their experimental results showed that short sentences may not contain enough Small Words and consequently the use of Trigram performed better.

A study of some distance measures was also made in (Singh, 2006) to compare some distance measures using combinations of different types of character N-Grams. The author used a dataset containing raw text documents without noise (such as books) that were written in 39 languages and collected from Project Gutenberg and CIIL. The training data size ranged between 2495 and 102,377 words.

A comparison between some language identification techniques was presented in (Gottron & Lipka, 2010), where the authors used short and query style texts to evaluate various approaches to LI. They tested each approach with different character N-Grams, from 1 to 5 characters. The best accuracy was achieved with the Na√Øve Bayes approach employing 5-Grams.

Other work has been based on different learning machine approaches. For instance, discrete HMM algorithms were used in (Xafopoulos, Kotropoulos, Almpanidis & Pitas, 2004) with three models for comparison. The authors evaluated their approaches on a dataset based on a collection of HTML pages of hotel web sites in 5 different languages. The texts used in their experiments were not noisy.

Another comparison between a Decision Tree model and an ARTMAP model was performed in (Selamat, Ch.Ching & Mikami, 2007), where the authors used two close languages (Arabic and Persian) to evaluate their approaches. Results showed out that the Decision Tree model provided high accuracy, but the approaches were less successful when the dataset was extended to other languages, such as Urdu. That problem was due to the strong similarity between the letters employed in those languages.

Furthermore, the authors proposed a hybrid approach between the Decision Tree and ARTMAP (Selamat & Ch. Ching, 2011) to enhance the weaknesses of the two methods. They used a decision tree approach to extract the features of each document, while the ARTMAP approach was used to classify each pattern. The approach was evaluated on web pages collected from BBC world, and the results showed a better performance by the hybrid approach (over 99% accuracy) compared to the original decision tree and ARTMAP approaches.

Another comparison between the decision tree and N-Gram based approaches was conducted in (Hakkinen & Tian, 2001), where the authors tested different N-Gram types using Bayes‚Äô rule to classify the language. The evaluation was performed on multilingual speech recognition systems, and the results showed that the decision tree outperforms the N-Gram based method in the closed test, whereas in the generalization test, it was slightly worse.

In (Baldwin & Lui, 2010), the authors compared some statistical and machine learning approaches. They used the nearest neighbor method with three distances, Naive Bayes and Support vector machines, and used three datasets with different properties (Wikipedia with 67 languages, TCL with 60 languages and EuropoGov with 10 languages) and the size of the documents ranged between 1480 and 39353 bytes. Their results showed that the nearest neighbor technique using N-Grams was the best performing method on the three datasets.

Some researchers have worked on short messages in social networks like Twitter. For instance, Bergsma et al. (Bergsma, McNamee, Bagdouri, Fink, & Wilson, 2012) used supervised learning approaches like prediction by partial matching and logistic regression with different features. The evaluation was undertaken on nine confusable languages, which were written in the Arabic, Devanagari and Cyrillic scripts. However, instead of classifying the texts into sub languages (e.g. Arabic, Farsi and Urdu) which are radically different, the authors classified the languages by writing scripts.

Another kind of technique was described in (Tromp & Pechenizkiy, 2011), where the authors used a graph approach based on character N-Grams (called LIGA) and applied to Twitter messages. Their approach was computationally expensive due to the construction of the different N-Gram graphs, but the results were quite interesting.

A multivariate analysis (MVA) based approach (Babu & Baskaran, 2005) was used for dimensionality reduction and classification in LI. The MVA approach was compared to N-Gram and Compression based methods, but the MVA approach outperformed the two other techniques.

A Naive Bayes classifier was proposed in (Laboreiro, Sarmento, Bo≈°njak, Rodrigues & Oliveira, 2013), where the authors tested two variants of the Portuguese language: European Portuguese and Brazilian Portuguese. They used a large set of different features and applied them on a Twitter micro-blog. They reported a best accuracy of 87% with their method.

Finally, some neural networks have been used in language identification (Tian & Suontausta, 2003). For instance, the authors in (Tian & Suontausta, 2003) proposed a scalable approach that reduced the memory by 50% and the performance was relatively high compared to the baseline systems. However neural networks may present some disadvantages with small datasets.

In our research work, we present two statistical approaches: the first one is based on the computation of character and word frequencies and the second one is based on the nearest prototype technique. At the end of this investigation we make some performance comparisons between our approaches and some standard language identification methods on forum texts.

A primary step of any textual document processing is a judicious choice of the character encoding, which must preserve the information and optimize the performances of the program as well. In this work, we have used the UTF-8 encoding to encode all the 32 different languages. The main reason of that choice is the fact that it covers more than one million characters, which implicitly enables covering most of existing languages with the use of only 4 bits. However, the major drawback of this encoding is the expensive computation in terms of memory and processing time.

Concerning the task of language identification on forum documents, we constructed two datasets, which we called DLI-32 and DLI-32-2. The details of those datasets can be found in the following website: http://scholarpage.org/ouamour.

The first dataset (DLI-32), contains a collection of several web discussion forums written in 32 different languages, which were extracted from the following forum websites:

                        
                           
                              
                              
                              
                              
                              
                                 
                                    
                                       forums.futura-sciences.com
                                    
                                    
                                       gamedev.net
                                    
                                    
                                       djelfa.info
                                    
                                    
                                       hackzone.ru
                                    
                                 
                                 
                                    
                                       technik-forum.ch
                                    
                                    
                                       pescaok.it
                                    
                                    
                                       forums.gr
                                    
                                    
                                       soloarquitectura.com
                                    
                                 
                                 
                                    
                                       gigapars.com
                                    
                                    
                                       bbs.med66.com
                                    
                                    
                                       frmtr.com
                                    
                                    
                                       foorumi.nintendoklubi.fi
                                    
                                 
                                 
                                    
                                       hydepark.hevre.co.il
                                    
                                    
                                       forum.postcrossing.com
                                    
                                    
                                       prestashop.com
                                    
                                    
                                       polskie-forum.pl
                                    
                                 
                                 
                                    
                                       fr.imvu.com
                                    
                                    
                                       forum.cabalonline.com
                                    
                                    
                                       foramnagaeilge.com
                                    
                                    
                                       forums.plexapp.com
                                    
                                 
                                 
                                    
                                       latindiscussion.com
                                    
                                    
                                       newz.dk
                                    
                                    
                                       vbenterprisetranslator.com
                                    
                                    
                                       myhindiforum.com
                                    
                                 
                                 
                                    
                                       ceskeforum.com
                                    
                                    
                                       malaysianbikers.com.my
                                    
                                    
                                       freak.no
                                    
                                    
                                       urduvb.com
                                    
                                 
                                 
                                    
                                       lokforum.com
                                    
                                    
                                       forumishqiptar.com
                                    
                                    
                                       forums.thaieurope.net
                                    
                                    
                                       forumsains.com
                                    
                                 
                              
                           
                        
                     
                  

Hence, ten different texts, of about 100 words each, are collected for every language, leading to 320 different texts in the first dataset (Table 1
                     ).

The second dataset (DLI-32-2) corresponds to a modification of DLI-32, where each text of DLI-32 is divided into two smaller texts with the same size, so that we obtain 640 different texts in the second dataset and 20 different texts for each language (Table 2
                     ).

The datasets are quite unbalanced in terms of size and the texts are corrupted with the following noises:

                        
                           ‚Ä¢
                           URLs: texts may contain links to web sites, videos, images, user profiles,‚Ä¶etc;

Citations in other language: texts may contain sentences or words in another language;

Tags: texts may contain a tag of persons, events or different objects;

Abbreviations: some people prefer to abbreviate nouns and use SMS writing style;

Unaccented characters: in the accented languages (e.g. French, Italian‚Ä¶etc. ) some people forget or do not accentuate the accented characters (e.g. ‚Äú√©‚Äù in French is written ‚Äúe‚Äù without accent);

Typing errors: sometimes people do not revise their messages before posting them in the forum;

Html tags and objects: accidently people change or modify Html tags properties inserted by the text editor;

Insignificant characters: texts may contain some insignificant characters to express sentiments (e.g. emoticons);

The construction of the datasets was a hard task, because it was made manually and sometimes it was difficult to find forums written in some specific languages (i.e. rare languages). As mentioned previously, the datasets details are available in our personal website and are made free for personal or academic use.

Language identification is based on four global steps described as follows: loading the language references, reading the text files, text preprocessing and finally the identification step.

In this section, we will give some details on the preprocessing step to perform before the LI process and which is considered as a primordial step in information retrieval and natural language processing.

The preprocessing consists in keeping only the pertinent information and removing the remainder. In general, the algorithms based on characters have two preprocessing stages, while those based on words (or hybrid algorithms) require three main preprocessing stages.

Furthermore, one can find other steps of preprocessing in the literature, like stop words removing or word stemming, which require the knowledge of the text language in advance.

A set of insignificant characters and numbers are removed at the first stage of preprocessing (i.e. 89 insignificant characters), because they are meaningless and considered as noisy characters in many cases. That step of preprocessing is applied to all algorithms.

The language identification algorithms based on words (i.e. WBA) have an additional step of preprocessing just after removing the insignificant characters. This step consists in separating the contracted words by replacing the contraction characters with white spaces (Table 4). For example, the French word ‚Äúl'eau‚Äù that means ‚Äúthe water‚Äù in English, becomes ‚Äúl eau‚Äù, where ‚Äúl‚Äù represents a definite article (one word) and ‚Äúeau‚Äù is a subject noun.

Note that the hyphen-minus character is different from the hyphen character cited in Table 3
                        
                         and that they have different Unicode codes (UTF-8).

The last step of preprocessing is the removal of multiple word delimiters. Hence, three types of word delimiters, presented in Table (5)
                        , have been removed.

As mentioned in the beginning of this section, the preprocessing steps are important and should be performed before any language identification task to further enhance the performances of classification.

The task of language identification is based on four global steps described as follows: loading the language references, reading the text files, text preprocessing and finally identifying the text language (Fig. 1
                     ).

In this investigation, two different statistical approaches of language identification are proposed and implemented. The first one is based on the frequency computation and the language is determined according to the highest frequency. The second one is based on distance measures and character N-Grams, and the text language is recognized by using the nearest profile. Moreover, for a purpose of comparison, we have implemented two baseline systems, which were proposed by other researchers: the first baseline system is the graph based language identification (LIGA), and the second one is the neural text categorizer (NTC).

This approach is based on the computation of the character or word frequencies, since each text document X is well characterized by its characters or words. The key idea is to compute the frequency of each character or word defined in the different languages, in the text X. After that, we add all the features frequencies for each language. So, the language of text X will be equal to the language that has the highest sum of frequencies.

That is, we have proposed 5 different methods based on that principle and which we called: character based identification algorithm (CBA), word based identification algorithm (WBA), special characters based identification algorithm (SCA), sequential hybrid algorithm (HA1) and parallel hybrid algorithm (HA2).

In this algorithm, we exploit the fact that each language is defined by its different characters. That is, we compute the number of occurrences of each character in the text and we sum all the related frequencies for each language. Thus, the language possessing the highest sum of frequencies is considered to be the genuine language of the document.

                              
                                 (1)
                                 
                                    
                                       su
                                       
                                          m
                                          i
                                       
                                       =
                                       
                                          ‚àë
                                          
                                             j
                                             =
                                             1
                                          
                                          n
                                       
                                       characte
                                       
                                          r
                                          _
                                       
                                       
                                          frequency
                                          i
                                          j
                                       
                                       
                                       where
                                       
                                       i
                                       
                                       is
                                       
                                       the
                                       
                                       i
                                       th
                                       
                                       language
                                       
                                       and
                                       
                                       j
                                       
                                       is
                                       
                                       the
                                       
                                       j
                                       th
                                       
                                       character
                                    
                                 
                              
                           However, the CBA algorithm presents a serious drawback when the languages share the same character set. In fact, this technique is based on the character sets to identify the language of a text. But in reality, it is not directly exploitable without establishing some logical rules (i.e. Logical Filtering) to try minimizing the confusion errors.

So, when the languages share the same character set, they will have almost the same sum of frequencies, which automatically may lead to false identification decisions. To overcome this problem, we have proposed a specific order, based on logical filtering, to classify those languages (Fig. 2
                           ).

The Test (Test1) is defined as follows: if the highest sum of frequencies (high_sum) corresponds to Chinese, Greek, Thai, Hebrew or Hindi languages then return the appropriate language that was identified. These languages are tested firstly because their characters are very different from the characters of other languages. In case of languages sharing some identical characters, we have defined a special function, which we called OrderFunction.

This function contains a set of logical tests as follows:


                           
                              
                                 
                                    
                                    
                                    
                                       
                                          
                                             Group 1:
                                          
                                          
                                       
                                       
                                          
                                             If 
                                             high_sum = Arabic_sum
                                          
                                          
                                             then return Arabic language
                                       
                                       
                                          
                                             If 
                                             high_sum = Persian_sum
                                          
                                          
                                             then return Persian language
                                       
                                       
                                          
                                             If 
                                             high_sum = Urdu_sum
                                          
                                          
                                             then return Urdu language
                                       
                                       
                                          
                                             Group 2:
                                          
                                          
                                       
                                       
                                          
                                             If 
                                             high_sum = Bulgarian_sum
                                          
                                          
                                             then return Bulgarian language
                                       
                                       
                                          
                                             If 
                                             high_sum = Russian_sum
                                          
                                          
                                             then return Russian language
                                       
                                       
                                          
                                             Group 3:
                                          
                                          
                                       
                                       
                                          
                                             If 
                                             high_ sum= English_ sum
                                          
                                          
                                             then return English language
                                       
                                       
                                          
                                             If 
                                             high_ sum= Dutch_ sum
                                          
                                          
                                             then return Dutch language
                                       
                                       
                                          
                                             If 
                                             high_ sum= Indonesian_ sum
                                          
                                          
                                             then return Indonesian language
                                       
                                       
                                          
                                             If 
                                             high_ sum= Malay_ sum
                                          
                                          
                                             then return Malay language
                                       
                                       
                                          
                                             If 
                                             high_ sum= Latin_ sum
                                          
                                          
                                             then return Latin language
                                       
                                       
                                          
                                             If 
                                             high_ sum= Roman_ sum
                                          
                                          
                                             then return Roman language
                                       
                                       
                                          
                                             If 
                                             high_ sum= French_ sum
                                          
                                          
                                             then return French language
                                       
                                       
                                          
                                             If 
                                             high_ sum= Italian_ sum
                                          
                                          
                                             then return Italian language
                                       
                                       
                                          
                                             If 
                                             high_ sum= Irish_ sum
                                          
                                          
                                             then return Irish language
                                       
                                       
                                          
                                             If 
                                             high_ sum= Spanish_ sum
                                          
                                          
                                             then return Spanish language
                                       
                                       
                                          
                                             If 
                                             high_ sum= Portuguese_ sum
                                          
                                          
                                             then return Portuguese language
                                       
                                       
                                          
                                             If 
                                             high_ sum= Albanian_ sum
                                          
                                          
                                             then return Albanian language
                                       
                                       
                                          
                                             If 
                                             high_ sum= Czech_ sum
                                          
                                          
                                             then return Czech language
                                       
                                       
                                          
                                             If 
                                             high_ sum= Finnish_ sum
                                          
                                          
                                             then return Finnish language
                                       
                                       
                                          
                                             If 
                                             high_ sum= Hungarian_ sum
                                          
                                          
                                             then return Hungarian language
                                       
                                       
                                          
                                             If 
                                             high_ sum= Swedish_ sum
                                          
                                          
                                             then return Swedish language
                                       
                                       
                                          
                                             If 
                                             high_ sum= German_ sum
                                          
                                          
                                             then return German language
                                       
                                       
                                          
                                             If 
                                             high_ sum= Norwegian_ sum
                                          
                                          
                                             then return Norwegian language
                                       
                                       
                                          
                                             If 
                                             high_ sum= Danish_ sum
                                          
                                          
                                             then return Danish language
                                       
                                       
                                          
                                             If 
                                             high_ sum= Icelandic_ sum
                                          
                                          
                                             then return Icelandic language
                                       
                                       
                                          
                                             If 
                                             high_ sum= Turkish_ sum
                                          
                                          
                                             then return Turkish language
                                       
                                       
                                          
                                             If 
                                             high_ sum= Polish_ sum
                                          
                                          
                                             then return Polish language
                                       
                                    
                                 
                              
                           
                        

In this test, the order of language groups could be changed because the groups are independent of each other. But, we cannot change the language order in the same group.

Note, also, that the order used in this scheme is technically chosen in order to minimize the confusion errors of identification. For instance, considering the following French text ‚Äúla requ√™te est re√ßue‚Äù, after computing the characters frequencies, we obtain the following frequencies:

                              
                                 
                                    
                                       F
                                       
                                          (
                                          l
                                          )
                                       
                                       
                                          =
                                          1
                                          ,
                                          F
                                       
                                       
                                          (
                                          a
                                          )
                                       
                                       
                                          =
                                          1
                                          ,
                                          F
                                       
                                       
                                          (
                                          r
                                          )
                                       
                                       
                                          =
                                          2
                                          ,
                                          F
                                       
                                       
                                          (
                                          e
                                          )
                                       
                                       
                                          =
                                          5
                                          ,
                                          F
                                       
                                       
                                          (
                                          q
                                          )
                                       
                                       
                                          =
                                          1
                                          ,
                                          F
                                       
                                       
                                          (
                                          u
                                          )
                                       
                                       
                                          =
                                          2
                                          ,
                                          F
                                       
                                       
                                          (
                                          
                                             e
                                             ^
                                          
                                          )
                                       
                                       
                                          =
                                          1
                                          ,
                                          F
                                       
                                       
                                          (
                                          t
                                          )
                                       
                                       
                                          =
                                          2
                                          ,
                                          F
                                       
                                       
                                          (
                                          s
                                          )
                                       
                                       
                                          =
                                          1
                                          ,
                                          F
                                       
                                       
                                          (
                                          √ß
                                          )
                                       
                                       
                                          =
                                          1
                                       
                                    
                                 
                              
                           
                        

Finally, after computing the sum of frequencies according to the French character set, we obtain 17 in total, whereas the sum of frequencies according to the English character set (or Malay, Indonesian, Dutch, Latin and Roman) is equal to 15. Consequently, the text will be automatically classified as French.

In case of an English text, the sum of frequencies is the same in all Latin languages (group 3), and then, the English test should precede the other Latin languages.

This algorithm has the same principle as the previous one (CBA), but instead of using characters as features, we use common words (i.e. most frequent words for each language) to identify the text language: each language is defined by a set of 20 common words. In fact, two types of common words are used. In the first one, the words are obtained from a trained process using 4 texts of DLI-32 per language. In the second type, those words are collected over the web (from large datasets) by some expert researchers.

So, the occurrence of each word is computed in the unknown text; then, we sum all those word occurrences for each language and the text language will be identified as the language having the highest sum of frequencies. See Fig. 3.
                           
                           
                              
                                 (2)
                                 
                                    
                                       su
                                       
                                          m
                                          l
                                       
                                       =
                                       
                                          ‚àë
                                          
                                             j
                                             =
                                             1
                                          
                                          20
                                       
                                       
                                          
                                             word
                                             _
                                             frequency
                                          
                                          l
                                          j
                                       
                                       
                                       where
                                       
                                       l
                                       
                                       is
                                       
                                       the
                                       
                                       
                                       l
                                       th
                                       
                                       language
                                       
                                       and
                                       
                                       j
                                       
                                       is
                                       
                                       the
                                       
                                       j
                                       th
                                       
                                       common
                                       
                                       word
                                    
                                 
                              
                           
                        

We have proposed another algorithm based on special characters (SCA) that differs from the two others. This algorithm consists of 2 classification steps. The first one represents a classification into classes of languages, where each class regroups a set of several languages that share some similar characters. Thus, we divided our dataset DLI-32 into 8 classes of languages (see Table 6
                           ), where each class is characterized by a set of common characters shared by all the languages of that class (see Table 7
                           ). The second step consists in selecting the most probable language from that class (i.e. identified language) (Fig. 4
                           ).

In other words, each class of languages is composed of a set of characters, which represent the common characters shared by all the languages of that class (see Table 7). For instance, the 3rd row in Table 7 contains all the characters shared by the Arabic, Persian and Urdu languages. However, each language is defined by a set of specific characters (see Table 8
                           ) that are not represented in the corresponding class (see Table 6). For instance, the French special characters are restricted to: ‚Äú√† √ß √© √® √™ √Æ √¥ ≈ì √ª √Ä √é √ä √à √î √õ √á‚Äù, after excluding the English alphabets A, B, C ‚Ä¶ and Z, from the global French character set.

Thus, if the concerned class is class 1, class 2, class 6, class 7 or class 8 then the language is immediately identified, because those classes contain only one language. In the other cases, where the class contains several languages, we use some discriminative special characters that are not common to the languages of the class, in order to perform the identification task (see Table 8). The used identification algorithm is quite similar to CBA, but it is only applied to the sole languages of that class.

Chinese characters are not defined in Tables 6, 7 and 8, because there are more than 40,000 characters. However, we use the UTF-8 code range to detect those characters.

In Table 8, we can note that some languages are not listed because all their characters are defined in the class characters and they do not possess any special character.

The different tests are described here below:

                              
                                 
                                    Test 1 ‚Üí Does the highest SF correspond to Chinese language?


                                    Test 2 ‚Üí Does the highest SF correspond to Greek language?


                                    Test 3 ‚Üí Does the highest SF correspond to Hebrew language?


                                    Test 4 ‚Üí Does the highest SF correspond to Hindi language?


                                    Test 5 ‚Üí Does the highest SF correspond to Thai language?

In order to enhance the language identification accuracy, we have proposed two hybrid algorithms that we called HA1 and HA2. HA1 is a sequential combination between CBA and WBA algorithms, for which the algorithm is displayed in Fig. 5.
                           
                        

Basically, we firstly execute the CBA algorithm and, then, if the text language is recognized as English, Arabic or Bulgarian then we execute the WBA algorithm. In fact, those three languages share the same characters set with other languages (i.e. same group). Thus, for the CBA algorithm, several text documents could be misclassified: for example, in the French text ‚Äúl'enseignant est la persoonne qui donne des cours‚Äù meaning ‚Äúthe teacher is the person who gives courses‚Äù, the French text contains only English characters. So, the text language will be recognized as English instead of French. For this reason, in such cases, we will execute the WBA algorithm to fix the confusion error.

The second hybrid algorithm (HA2) consists in a parallel combination between CBA and WBA (Fig. 6
                           ). The algorithm adds the outputs of the two algorithms CBA and WBA for each language in order to find the highest fused sum (see Eq. 3). Once again, the appropriate language of the text document will be identified as the language having the highest fused sum (see Fig. 6).

                              
                                 (3)
                                 
                                    
                                       
                                          HA
                                          2
                                       
                                       _
                                       su
                                       
                                          m
                                          i
                                       
                                       =
                                       CBA
                                       _
                                       su
                                       
                                          m
                                          i
                                       
                                       +
                                       WBA
                                       _
                                       su
                                       
                                          m
                                          i
                                       
                                       
                                       where
                                       
                                       i
                                       
                                       
                                          represents
                                          
                                          the
                                       
                                       
                                       i
                                       
                                          th
                                          
                                          language
                                       
                                       .
                                    
                                 
                              
                           
                        

This approach is based on the similarity estimation between two text documents using distance measures. In our research work, we defined 11 different types of distances using character N-Grams as features. A character N-Gram is defined as a sequence of N characters including white spaces. Thus, character Uni-Gram is represented by a set of characters, character Bi-Gram is a succession of two characters and character Tri-Gram represents a sequence of three characters (Fig. 7
                              ).

The overall scheme of the algorithm is based on the nearest neighbor principle. So, firstly, all the training documents, of each language, are concatenated together in a same big document, and next, a list of character N-Grams is extracted. Subsequently, a histogram containing the N-Grams and their normalized frequencies is created for each language and stored: it is the reference profile. And similarly, for an unknown text, we create a histogram containing the N-grams and their normalized frequencies in the same manner.

Finally, we compute the distances between the input histogram of the unknown text and each reference profile using some similarity measures. At the end, the most probable language is the one presenting the minimal distance between the input histogram and its reference profile.

That is, 11 different distance measures have been employed, namely: Euclidean distance, Cosine distance, Manhattan distance, Chi-2 distance, Squared Euclidean distance, Bray Curtis distance, Bhattacharyya distance, Canberra distance, Histogram Intersection distance and Correlation distance. Their corresponding formulas are described here below.


                              T1 and T2 represent respectively 2 vectors to compare and n is the length of the vectors.
                              
                                 
                                    (4)
                                    
                                       
                                          T
                                          1
                                          =
                                          (
                                          T
                                          
                                             1
                                             1
                                          
                                          ,
                                          T
                                          
                                             1
                                             2
                                          
                                          ,
                                          T
                                          
                                             1
                                             3
                                          
                                          ‚Ä¶
                                          T
                                          
                                             1
                                             n
                                          
                                          )
                                       
                                    
                                 
                              
                              
                                 
                                    (5)
                                    
                                       
                                          T
                                          2
                                          =
                                          (
                                          T
                                          
                                             2
                                             1
                                          
                                          ,
                                          T
                                          
                                             2
                                             2
                                          
                                          ,
                                          T
                                          
                                             2
                                             3
                                          
                                          ‚Ä¶
                                          T
                                          
                                             2
                                             n
                                          
                                          )
                                       
                                    
                                 
                              
                           


                              Euclidean distance (Euc): is defined as the classical distance between two points (or vectors) and represents the length of the segment connecting them. It is measured by the square root of the summation of the squared differences between the two vector components, and is computed as follows:

                                 
                                    (6)
                                    
                                       
                                          Euc
                                          
                                             (
                                             
                                                T
                                                1
                                                ,
                                                T
                                                2
                                             
                                             )
                                          
                                          =
                                          
                                             
                                                
                                                   ‚àë
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   n
                                                
                                                
                                                   
                                                      
                                                         (
                                                         
                                                            T
                                                            
                                                               2
                                                               i
                                                            
                                                            ‚àí
                                                            T
                                                            
                                                               1
                                                               i
                                                            
                                                         
                                                         )
                                                      
                                                   
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           


                              Cosine distance (Cos): is a measure of similarity between two vectors by using an inner product that measures the cosine of the angle between them. It is given by the following formula.

                                 
                                    (7)
                                    
                                       
                                          Cos
                                          
                                             (
                                             
                                                T
                                                
                                                   1
                                                   ,
                                                
                                                T
                                                2
                                             
                                             )
                                          
                                          
                                             =
                                             1
                                          
                                          ‚àí
                                          
                                             
                                                
                                                   ‚àë
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   n
                                                
                                                
                                                   (
                                                   T
                                                   
                                                      1
                                                      i
                                                   
                                                   T
                                                   
                                                      2
                                                      i
                                                   
                                                   )
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         ‚àë
                                                         
                                                            i
                                                            =
                                                            1
                                                         
                                                         n
                                                      
                                                      T
                                                      
                                                         1
                                                         i
                                                         2
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         ‚àë
                                                         
                                                            i
                                                            =
                                                            1
                                                         
                                                         n
                                                      
                                                      T
                                                      
                                                         2
                                                         i
                                                         2
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                              Manhattan distance (Man): is basically the distance between two points in a grid based on a strictly horizontal and/or vertical path as opposed to the diagonal, and represents the simple sum of the horizontal and vertical components.

                                 
                                    (8)
                                    
                                       
                                          Man
                                          
                                             (
                                             
                                                T
                                                
                                                   1
                                                   ,
                                                
                                                T
                                                2
                                             
                                             )
                                          
                                          =
                                          
                                             ‚àë
                                             
                                                i
                                                =
                                                1
                                             
                                             n
                                          
                                          
                                             |
                                             
                                                T
                                                
                                                   1
                                                   i
                                                
                                             
                                          
                                          ‚àí
                                          
                                             
                                                T
                                                
                                                   2
                                                   i
                                                
                                             
                                             |
                                          
                                       
                                    
                                 
                              
                              Chi-2 distance (Chi): is often used in computer vision to compute distances between two normalized histograms, where their entries sum up to 1. The equation is measured by the summation of the squared element-wise differences between the two vectors divided by their addition.

                                 
                                    (9)
                                    
                                       
                                          Chi
                                          
                                             (
                                             
                                                T
                                                
                                                   1
                                                   ,
                                                
                                                T
                                                2
                                             
                                             )
                                          
                                          =
                                          
                                             ‚àë
                                             
                                                i
                                                =
                                                1
                                             
                                             n
                                          
                                          
                                             
                                                
                                                   
                                                      |
                                                      
                                                         T
                                                         
                                                            1
                                                            i
                                                         
                                                         ‚àí
                                                         T
                                                         
                                                            2
                                                            i
                                                         
                                                      
                                                      |
                                                   
                                                
                                                2
                                             
                                             
                                                
                                                   |
                                                   T
                                                
                                                
                                                   1
                                                   i
                                                
                                                
                                                   |
                                                   
                                                      
                                                         +
                                                         |
                                                         T
                                                      
                                                      
                                                         2
                                                         i
                                                      
                                                   
                                                   |
                                                
                                             
                                          
                                       
                                    
                                 
                              
                              Squared Euclidean distance (SEd): is the square of the Euclidean distance and its formula is given as follows:

                                 
                                    (10)
                                    
                                       
                                          SEd
                                          
                                             (
                                             
                                                T
                                                
                                                   1
                                                   ,
                                                
                                                T
                                                2
                                             
                                             )
                                          
                                          =
                                          
                                          
                                             ‚àë
                                             
                                                i
                                                =
                                                1
                                             
                                             n
                                          
                                          
                                             
                                                (
                                                
                                                   T
                                                   
                                                      2
                                                      i
                                                   
                                                   ‚àí
                                                   T
                                                   
                                                      1
                                                      i
                                                   
                                                
                                                )
                                             
                                             2
                                          
                                       
                                    
                                 
                              
                              Canberra (Can): is a weighted version of Manhattan distance, and represents a numerical measure of the distance between pairs of points in a vector space. The distance is measured by computing the Manhattan distance with normalizing the horizontal and vertical components by their sum. The formula is given as follows:

                                 
                                    (11)
                                    
                                       
                                          Can
                                          
                                             (
                                             
                                                T
                                                
                                                   1
                                                   ,
                                                
                                                T
                                                2
                                             
                                             )
                                          
                                          =
                                          
                                             ‚àë
                                             
                                                i
                                                =
                                                1
                                             
                                             n
                                          
                                          
                                             
                                                |
                                                
                                                   T
                                                   
                                                      1
                                                      i
                                                   
                                                   ‚àí
                                                   T
                                                   
                                                      2
                                                      i
                                                   
                                                
                                                |
                                             
                                             
                                                
                                                   |
                                                   T
                                                
                                                
                                                   1
                                                   i
                                                
                                                
                                                   |
                                                   
                                                      
                                                         +
                                                         |
                                                         T
                                                      
                                                      
                                                         2
                                                         i
                                                      
                                                   
                                                   |
                                                
                                             
                                          
                                       
                                    
                                 
                              
                              Bray Curtis dissimilarity (Bra): is similar to Manhattan distance divided by the element-wise sum of the two vectors, and is given as follows:

                                 
                                    (12)
                                    
                                       
                                          Bra
                                          
                                             (
                                             
                                                T
                                                
                                                   1
                                                   ,
                                                
                                                T
                                                2
                                             
                                             )
                                          
                                          =
                                          
                                             
                                                
                                                   ‚àë
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   n
                                                
                                                
                                                   (
                                                   
                                                      T
                                                      
                                                         1
                                                         i
                                                      
                                                      ‚àí
                                                      T
                                                      
                                                         2
                                                         i
                                                      
                                                   
                                                   )
                                                
                                             
                                             
                                                
                                                   ‚àë
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   n
                                                
                                                
                                                   (
                                                   
                                                      T
                                                      
                                                         1
                                                         i
                                                      
                                                      +
                                                      T
                                                      
                                                         2
                                                         i
                                                      
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                              Histogram intersection distance (His): is the intersection between two histograms (feature vectors), as measured by the sum of the minimum between the vectors elements as follows.

                                 
                                    (13)
                                    
                                       
                                          His
                                          
                                             (
                                             
                                                T
                                                
                                                   1
                                                   ,
                                                
                                                T
                                                2
                                             
                                             )
                                          
                                          =
                                          
                                             ‚àë
                                             
                                                i
                                                =
                                                1
                                             
                                             n
                                          
                                          min
                                          
                                             (
                                             
                                                T
                                                
                                                   1
                                                   i
                                                
                                                ,
                                                T
                                                
                                                   2
                                                   i
                                                
                                             
                                             )
                                          
                                       
                                    
                                 
                              
                              Bhattacharyya distance (Bha): Bhattacharyya's original interpretation of the measure was geometric (Bhattacharyya, 1943). He considered two multinomial populations, each consisting of k probabilities, so the square root of the probabilities is considered as the direction angle of two vectors. As a measure of divergence between the two populations, Bhattacharyya used the square of the angle between the two feature vectors. Therefore, the distance is given as follows:

                                 
                                    (14)
                                    
                                       
                                          Bha
                                          
                                             (
                                             
                                                T
                                                
                                                   1
                                                   ,
                                                
                                                T
                                                2
                                             
                                             )
                                          
                                          =
                                          
                                             ‚àë
                                             
                                                i
                                                =
                                                1
                                             
                                             n
                                          
                                          
                                             
                                                T
                                                
                                                   1
                                                   i
                                                
                                                T
                                                
                                                   2
                                                   i
                                                
                                             
                                          
                                       
                                    
                                 
                              
                              Correlation distance (Cor): is a measure of statistical dependence between two random vectors of arbitrary (but not necessarily equal) dimension. The distance is measured by computing the covariance of the two vectors normalized by the product of their standard deviations.

                                 
                                    (15)
                                    
                                       
                                          Cor
                                          
                                             (
                                             
                                                T
                                                
                                                   1
                                                   ,
                                                
                                                T
                                                2
                                             
                                             )
                                          
                                          =
                                          
                                             
                                                
                                                   ‚àë
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   n
                                                
                                                
                                                   (
                                                   
                                                      T
                                                      
                                                         1
                                                         i
                                                      
                                                      ‚àí
                                                      
                                                         
                                                            T
                                                            1
                                                         
                                                         ¬Ø
                                                      
                                                   
                                                   )
                                                
                                                
                                                   (
                                                   
                                                      T
                                                      
                                                         2
                                                         i
                                                      
                                                      ‚àí
                                                      
                                                         
                                                            T
                                                            2
                                                         
                                                         ¬Ø
                                                      
                                                   
                                                   )
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         ‚àë
                                                         
                                                            i
                                                            =
                                                            1
                                                         
                                                         n
                                                      
                                                      
                                                         
                                                            
                                                               (
                                                               
                                                                  T
                                                                  
                                                                     1
                                                                     i
                                                                  
                                                                  ‚àí
                                                                  
                                                                     
                                                                        T
                                                                        1
                                                                     
                                                                     ¬Ø
                                                                  
                                                               
                                                               )
                                                            
                                                         
                                                         2
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         ‚àë
                                                         
                                                            i
                                                            =
                                                            1
                                                         
                                                         n
                                                      
                                                      
                                                         
                                                            
                                                               (
                                                               
                                                                  T
                                                                  
                                                                     2
                                                                     i
                                                                  
                                                                  ‚àí
                                                                  
                                                                     
                                                                        T
                                                                        2
                                                                     
                                                                     ¬Ø
                                                                  
                                                               
                                                               )
                                                            
                                                         
                                                         2
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                              
                                 
                                    
                                       T
                                       1
                                    
                                    ¬Ø
                                 
                               
                              and
                              
                                 
                                    
                                       T
                                       2
                                    
                                    ¬Ø
                                 
                               
                              represent respectively the mean vectors of T1 and T2.
                           


                              Out of Place distance (OPd): was introduced by Cavnar and Trenkle in 1994 (Martins & Silva, 2005). This distance is based on computing the frequency rank of each element and ordering them by a descending order (for both vectors). Next, we compute the sum of element-wise subtractions of the two rank vectors.

                                 
                                    (16)
                                    
                                       
                                          OPd
                                          
                                             (
                                             
                                                R
                                                
                                                   1
                                                   ,
                                                
                                                R
                                                2
                                             
                                             )
                                          
                                          =
                                          
                                             ‚àë
                                             
                                                i
                                                =
                                                1
                                             
                                             n
                                          
                                          
                                             |
                                             
                                                R
                                                
                                                   1
                                                   i
                                                
                                             
                                          
                                          ‚àí
                                          
                                             
                                                R
                                                
                                                   2
                                                   i
                                                
                                             
                                             |
                                          
                                       
                                    
                                 
                              
                              R1 and R2 represent respectively 2 vectors of ranks by a descending order.
                           

To compare the performances of the proposed approaches with the state of the art, we have implemented two baseline systems. The first baseline method is called ‚Äúlanguage identification based on graph approach‚Äù (LIGA) and the second one is called ‚Äúneural text categorization based approach‚Äù (NTC).

LIGA was firstly introduced by Erik Tromp in (Tromp & Pechenizkiy, 2011), which uses character N-Grams especially. The idea of the LIGA approach is based on the use of training documents for each language to create the language graph, where each node should contain N-Gram strings and their frequencies. Moreover, the graph edges represent the succession between N-Gram features (i.e. each edge is a link between two consecutive features), and the edge weights are given by the frequencies of consecutive N-Grams.

In LIGA, a basic graph model 
                                 
                                    G
                                    =
                                    (
                                    
                                       V
                                       ,
                                       
                                       E
                                    
                                    )
                                 
                               is extended and defined by the following properties:

                                 
                                    -
                                    
                                       V and E are respectively a set of nodes and a set of edges;

A labeling function L¬†:‚ÄâV‚Üí L used to assign vertices to the graph;

A function Wv
                                       :‚ÄâV¬†√ó¬†T‚Üí ‚Ñï, which assigns for each vertex v¬†‚àä¬†V,‚Äâa weight;

A function We
                                       :‚ÄâE¬†√ó¬†T‚Üí ‚Ñï, which assigns for each edge e¬†‚àä¬†E,‚Äâa weight.

Based on the above mentioned graph description, the graph is newly represented by the following quintuple:

                                 
                                    
                                       
                                          G
                                          =
                                          
                                             (
                                             
                                                
                                                V
                                                ,
                                                E
                                                
                                                   ,
                                                   L
                                                
                                                ,
                                                
                                                   W
                                                   v
                                                
                                                
                                                
                                                   W
                                                   e
                                                
                                             
                                             )
                                          
                                       
                                    
                                 
                              
                           

Each language li
                                 ‚àäL has a set of labeled documents Di
                                 ¬†=¬†{dj
                                 ‚àäDi
                                  / 1¬†‚â§¬†j¬†‚â§¬†m} reserved for training and constructing the models.

For every document dj
                                 ‚àä Di
                                 , we extract a list of ordered N-Grams Wj
                                 . Next, for each N-Gram element w‚àäWj
                                 , a new vertex v is created by L(v) = w having 1 as weight only if v does not belong to V, else the vertex weight is incremented by 1.
                                    
                                       (17)
                                       
                                          
                                             
                                                W
                                                v
                                             
                                             
                                                (
                                                
                                                   v
                                                   ,
                                                   
                                                   
                                                      l
                                                      i
                                                   
                                                
                                                )
                                             
                                             
                                                {
                                                
                                                   
                                                      
                                                         
                                                            
                                                               W
                                                               v
                                                            
                                                            
                                                               (
                                                               
                                                                  v
                                                                  ,
                                                                  
                                                                  
                                                                     l
                                                                     i
                                                                  
                                                               
                                                               )
                                                            
                                                            +
                                                            
                                                            1
                                                         
                                                      
                                                      
                                                         
                                                            if
                                                            
                                                            v
                                                            ‚àà
                                                            V
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         1
                                                      
                                                      
                                                         
                                                            
                                                            otherwise
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 For every two successive N-Gram elements (wk
                                 , 
                                    
                                       w
                                       
                                          k
                                          +
                                          1
                                       
                                    
                                 )¬†‚àä¬†
                                    
                                       W
                                       
                                          d
                                          i
                                       
                                    
                                 √ó
                                    
                                       W
                                       
                                          d
                                          i
                                       
                                    
                                  and L(v)¬†=¬†wk
                                 ‚Äâand L(u)¬†=¬†
                                    
                                       w
                                       
                                          k
                                          +
                                          1
                                       
                                    
                                 , a new edge e‚Äâ(having 1 as weight) is created between the two nodes (v, u) only if e‚Äâdoes not belong to E. Else the edge weight is incremented by one.

                                    
                                       (18)
                                       
                                          
                                             
                                                W
                                                e
                                             
                                             
                                                (
                                                
                                                   e
                                                   ,
                                                   
                                                   
                                                      l
                                                      i
                                                   
                                                
                                                )
                                             
                                             =
                                             
                                                {
                                                
                                                   
                                                      
                                                         
                                                            
                                                               W
                                                               e
                                                            
                                                            
                                                               (
                                                               
                                                                  e
                                                                  ,
                                                                  
                                                                  
                                                                     l
                                                                     i
                                                                  
                                                               
                                                               )
                                                            
                                                            +
                                                            
                                                            1
                                                         
                                                      
                                                      
                                                         
                                                            if
                                                            
                                                            e
                                                            ‚àà
                                                            E
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         1
                                                      
                                                      
                                                         otherwise
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 At the end of the training, each graph node should contain one N-Gram string with a set of weights and each edge contains a set of weights corresponding to one language (i.e. the frequency in every language). We illustrate the construction of the graph by the following example. Considering the English text ‚Äúwe are what we are‚Äù, after extracting the character Tri-Grams, we obtain the following list (the ‚Äú_‚Äù character is used to represent white spaces):

                                    
                                       
                                          
                                             
                                                [
                                                
                                                   we
                                                   _
                                                
                                                ]
                                             
                                             
                                             
                                                [
                                                
                                                   e
                                                   _
                                                   a
                                                
                                                ]
                                             
                                             
                                             
                                                [
                                                
                                                   _
                                                   ar
                                                
                                                ]
                                             
                                             
                                             
                                                [
                                                are
                                                ]
                                             
                                             
                                             
                                                [
                                                
                                                   re
                                                   _
                                                
                                                ]
                                             
                                             
                                             
                                                [
                                                
                                                   e
                                                   _
                                                   w
                                                
                                                ]
                                             
                                             
                                             
                                                [
                                                
                                                   _
                                                   wh
                                                
                                                ]
                                             
                                             
                                             
                                                [
                                                wha
                                                ]
                                             
                                             
                                             
                                                [
                                                hat
                                                ]
                                             
                                             
                                             
                                                [
                                                
                                                   at
                                                   _
                                                
                                                ]
                                             
                                             
                                             
                                                [
                                                
                                                   t
                                                   _
                                                   w
                                                
                                                ]
                                             
                                             
                                             
                                                [
                                                
                                                   _
                                                   we
                                                
                                                ]
                                             
                                             
                                             
                                                [
                                                
                                                   we
                                                   _
                                                
                                                ]
                                             
                                             
                                             
                                                [
                                                
                                                   e
                                                   _
                                                   a
                                                
                                                ]
                                             
                                             
                                             
                                                [
                                                
                                                   _
                                                   ar
                                                
                                                ]
                                             
                                             
                                             
                                                [
                                                are
                                                ]
                                             
                                          
                                       
                                    
                                 
                              

The unlabeled text is represented as a path of N-Grams‚ÄâœÄ¬†=¬†(V, E, L,‚Äâv
                                 start), where v
                                 start represents the starting node. The path structure is similar to the graph structure but without node weights; furthermore, a node v¬†‚àä¬†V‚Äâmay occur several times in the path.

To compute the similarity between the path and graphs, Tromp et al. used the so-called path matching scores [15]. This function is denoted by PM (PM: T ‚Üí ‚Ñï) assigning an integer number to any language‚Äâli
                                 ‚àäL. Hence, initially, each PM score of a language li
                                  is initialized with zero: PM(li
                                 )¬†=¬†0 for every li
                                 ‚àäL.

To compute the PM score, the path is traversed node by node, looking for the existence of path nodes in the graph G. If the path node v exists in the graph G (v‚àäG), then we add the node weight (i.e. the weight of language
li
                                 ) to PM(li
                                 ) score using Eq. (19).

                                    
                                       (19)
                                       
                                          
                                             PM
                                             
                                                
                                                   (
                                                   l
                                                   )
                                                
                                                i
                                             
                                             =
                                             
                                                {
                                                
                                                   
                                                      
                                                         
                                                            PM
                                                            
                                                               (
                                                               
                                                                  l
                                                                  i
                                                               
                                                               )
                                                            
                                                            +
                                                            
                                                            
                                                               W
                                                               v
                                                            
                                                            
                                                               (
                                                               
                                                                  v
                                                                  ,
                                                                  
                                                                  
                                                                     l
                                                                     i
                                                                  
                                                               
                                                               )
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            if
                                                            
                                                            v
                                                            ‚àà
                                                            G
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         0
                                                      
                                                      
                                                         else
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 On the other hand, we look for the existence of path edges in the graph G: if the edge e exists in the graph G (e¬†‚àä¬†
G) then we add the edge weight (i.e. the weight of language li
                                 ) to PM(ti
                                 ) score using Eq. (20).

                                    
                                       (20)
                                       
                                          
                                             PM
                                             
                                                (
                                                
                                                   l
                                                   i
                                                
                                                )
                                             
                                             =
                                             
                                                {
                                                
                                                   
                                                      
                                                         
                                                            PM
                                                            
                                                               (
                                                               
                                                                  l
                                                                  i
                                                               
                                                               )
                                                            
                                                            +
                                                            
                                                            
                                                               W
                                                               e
                                                            
                                                            
                                                               (
                                                               
                                                                  e
                                                                  ,
                                                                  
                                                                  
                                                                     l
                                                                     i
                                                                  
                                                               
                                                               )
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            if
                                                            
                                                            e
                                                            
                                                            ‚àà
                                                            G
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         0
                                                      
                                                      
                                                         else
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 Finally, after matching the entire path onto the graphs, the unknown language will be recognized as the language with the highest PM score.

                                    
                                       (21)
                                       
                                          
                                             language
                                             =
                                             
                                                arg
                                                ma
                                             
                                             
                                                x
                                                
                                                   
                                                      l
                                                      i
                                                   
                                                   ‚àà
                                                   L
                                                
                                             
                                             
                                                (
                                                
                                                   PM
                                                   
                                                      l
                                                      i
                                                   
                                                   
                                                      (
                                                      
                                                         l
                                                         i
                                                      
                                                      )
                                                   
                                                
                                                )
                                             
                                          
                                       
                                    
                                 For instance, if we consider the following English text: ‚Äúwe are human‚Äù, then, the list of Tri-Grams will correspond to the following series: [we_] [e_a] [_ar] [are] [re_] [e_h] [_hu] [hum] [uma] [man].

After creating the path and traversing it node-by-node, we obtain a PM matching score (in the previous graph of Fig. 8
                                 ) equal to 16 (Figs. 9
                                  and 10
                                 ).

The NTC classifier was introduced by Jo in (Jo, 2008) for the purpose of text categorization. It is a simple neural network (Perceptron) with one hidden layer and where the synaptic weights of the input layer are connected directly to the output layer. However, it is different from the Perceptron in some aspects, since the weights are only updated (with different learning process) if the document is misclassified. Furthermore, the input vector is a string vector, and not a number vector as it is the case with conventional neural networks.

The number of input nodes is equal to the input vector size, and the number of output nodes is equal to the number of languages. On the other hand, the number of nodes in the hidden layer is equal to the number of output nodes.

The hidden layer represents the learning layer, where each node owns a learning table of entries and each entry contains a string word and its weight. Furthermore, the learning layer determines the synaptic weights between the input layer and output layer, and obviously each input node is connected with all learning tables.

To train the NTC classifier, initially, each learning table is filled with N common words for each language. Next, the training documents are passed in the network as string vectors.

For each word in the input vector, the algorithm looks for the existence of the word in the table of each language (i.e. in the hidden layer), and if it does not exist, then a zero weight is attributed (Eq. 22). Next, for each language we sum all word weights using Eq. (23).

                                 
                                    (22)
                                    
                                       
                                          
                                             w
                                             
                                                j
                                                i
                                             
                                          
                                          =
                                          
                                             {
                                             
                                                
                                                   
                                                      
                                                         table
                                                         
                                                            
                                                               (
                                                               
                                                                  t
                                                                  i
                                                               
                                                               )
                                                            
                                                            
                                                               j
                                                               
                                                            
                                                         
                                                         
                                                         
                                                            if
                                                            
                                                            the
                                                            
                                                            word
                                                         
                                                         
                                                         
                                                            t
                                                            i
                                                         
                                                         
                                                         
                                                            exists
                                                            
                                                            in
                                                            
                                                            tabl
                                                         
                                                         
                                                            e
                                                            j
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         0
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              where‚Äâwji
                               is the ith word weight of the jth language.

                                 
                                    (23)
                                    
                                       
                                          
                                             o
                                             j
                                          
                                          =
                                          
                                             ‚àë
                                             
                                                i
                                                =
                                                1
                                             
                                             T
                                          
                                          
                                             w
                                             
                                                j
                                                i
                                             
                                          
                                          
                                          where
                                          
                                          j
                                          
                                          
                                             is
                                             
                                             the
                                          
                                          
                                          j
                                          t
                                          h
                                          
                                             
                                             language
                                          
                                       
                                    
                                 
                              Finally, the attributed language is obtained according to the output node with the highest probability. If a document of a language‚Äâcj
                               is classified as vlanguage‚Äâck
                               (misclassified document), then the weights are updated as follows:

                                 
                                    (24)
                                    
                                       
                                          If
                                          
                                          
                                             c
                                             k
                                          
                                          ‚â†
                                          
                                             c
                                             j
                                          
                                          ,
                                          Œî
                                          
                                             w
                                             
                                                k
                                                i
                                             
                                          
                                          =
                                          
                                          ‚àí
                                          Œ∑
                                          
                                             w
                                             
                                                k
                                                i
                                             
                                          
                                       
                                    
                                 
                              
                              
                                 
                                    (25)
                                    
                                       
                                          If
                                          
                                          
                                             c
                                             k
                                          
                                          ‚â†
                                          
                                             c
                                             j
                                          
                                          ,
                                          Œî
                                          
                                             w
                                             
                                                j
                                                i
                                             
                                          
                                          =
                                          
                                          Œ∑
                                          
                                             w
                                             
                                                j
                                                i
                                             
                                          
                                       
                                    
                                 
                              where Œ∑ is the learning rate of the neural network.

The training process is repeated until the stopping criterion is reached (e.g. maximum number of iterations).

@&#EXPERIMENTS AND RESULTS@&#

The different experiments of LI were performed on four datasets, the first one is a subset of DLI-32 containing 10 languages (French, English, Arabic, Russian, German, Italian, Greek, Spanish, Persian and Chinese), the second one corresponds to the entire DLI-32 dataset, containing 32 languages (Table 1), the third dataset is a subset of DLI-32-2 dataset containing 10 languages (French, English, Arabic, Russian, German, Italian, Greek, Spanish, Persian and Chinese), and the last one corresponds to the entire DLI-32-2 dataset (Table 2).

During our experiments, 40% of the dataset is used for the training (i.e. 4 texts per language that are selected randomly). On the other hand, 60% of the dataset is reserved for the testing because a big testing data size is important to get significant results.

To make an objective LI evaluation, a comparison has been established between the performances of all the proposed approaches. Furthermore, we have also performed a comparison of our approaches with some state-of-the-art methods such as: LIGA, NTC, Google translate and Microsoft Word.

In Figs. 11
                         and 12
                        , we present the results of LI obtained by the first approach (high frequency approach) on the four datasets DLI-32 (with 10 and 32 languages) and DLI-32-2 (with 10 and 32 languages). Concerning the WBA algorithms, test 1 is performed with 20 common words extracted from the web, and test 2 is performed with 20 common words obtained directly from the training process.

We can notice from this figure that the worst scores are those obtained by WBA, because this last one uses white spaces and line feeds to extract the words from the text, and that may cause some errors in case of Chinese and Thai languages. In fact, Chinese words are not spaced like the other languages, and each Chinese character represents a word. Thus, the algorithm is not suitable for extracting Chinese words. The accuracy of the WBA algorithm is 90% on the two subsets of DLI-32 and DLI-32-2 datasets. Whereas, we notice that the CBA and SCA accuracies reach 100% on DLI-32, which means that these algorithms are more accurate than WBA.

Concerning the hybrid approaches, these last ones show excellent performances with 10 languages, especially HA2, which presents an accuracy of about 100% on the two datasets.

We also notice that the accuracies of the other algorithms decrease on DLI-32-2, probably because this last dataset contains relatively short and noisy texts.

From Fig. 12, we can notice that the accuracy of WBA has slightly decreased, compared to the previous results obtained with 10 languages. However, test 1 seems to be better than test 2, because the words of test 1 are collected over the web (i.e. words obtained by several researchers using large datasets). Contrariwise, the words in test 2 are obtained from a smaller subset (i.e. 4 texts per language).

We also remark that the accuracies of CBA and SCA have strongly decreased in the second evaluation (72.40% and 71.35% on DLI-32, respectively). On the other hand, we notice that their performances are lower than the WBA performance: probably because CBA and SCA are based on characters, and with 32 languages several of those languages may share the same character sets, which consequently leads to some LI confusions.

Concerning the hybrid methods, the accuracies have slightly decreased with 32 languages. Also, the HA1 performances using test 1 and test 2 are slightly lower than the WBA performance with test 1, but they are higher than its performance with test 2. As for the comparison between the two hybrid methods HA1 and HA2, we notice that HA2 brings the best accuracy on the two datasets.

In overall, by comparing Fig. 12 with Fig. 11, we notice that the identification scores have decreased for all the algorithms, which means that the number of languages has a strong impact on the identification accuracy. In fact, the main reason is the presence of many closed languages in the second datasets (32 languages), and consequently, the language identification becomes more difficult. This is due, on one hand, to the inclusion of language characters within others (e.g. Arabic characters are included in the Persian and Urdu languages); on the other hand, we notice that some languages have the same character set as English, Roman, Latin, Malay, Indonesian and Dutch languages, for instance.

In this section, we present the different results of language identification obtained by the nearest neighbor approach on DLI-32 and DLI-32-2.


                        Fig. 13
                         shows the identification accuracies obtained by using the 11 similarity measures with character N-Grams. We can notice that, with 10 languages, all the distances are accurate by providing a score of 100% of good identification, except the Out of Place distance using character Bi-Gram, which gives a score of 98% and the Correlation distance, which gives an accuracy of 96.67% and 91.67% with character Bi-Gram and Tri-Gram respectively.

On DLI-32-2, which contains short texts (Fig. 14
                        
                        
                        ), all distances appear to be affected by the text size, except, the Chi2, Bhattacharyya and Out of place distances, which are more accurate with character Uni-Grams (accuracy of 100%). We also notice that the performances obtained by character Bi-Grams are not very different from those of character Uni-Grams for most similarity measures. Moreover, we can also notice that the worst feature is character Tri-Gram and the best score is given by the Chi2 and Bhattacharyya distances.

In overall, we can say that all the distances are accurate by providing good results (accuracy over 95%), except the Correlation distance.

Now, by observing (Tromp & Pechenizkiy, 2011) and (Babu & Baskaran, 2005), we remark that the accuracies of all distances based on character Uni-Grams and Bi-Grams have appreciably decreased, except the Chi2 distance for which the decrease is not very important. We also notice that Canberra distance with character Bi-Grams brings a high accuracy (94.71% on DLI-32 and 94.01% on DLI-32-2).

However, with character Tri-Grams, most distances bring good accuracies: often over 95%, which means that, with a lot of languages, character Tri-Grams are more efficient than Uni-Grams or Bi-Grams. On the other hand, we notice that the best LI score is obtained by the Out of Place distance: 97.92% on DLI-32 and 97.14% on DLI-32-2, making it the best distance tested in our experiments.

Although the previous results appear to be quite interesting, we noticed several identification problems. In fact, the major problem of all those distances is the confusion between the two couples of languages Indonesian/Malay and Danish/Norwegian, since those languages are very similar and share many types of features. Another noticed problem is the use of foreign language in the main text, which could be considered as a type of noise.

Finally, as mentioned previously, we noticed that character Tri-Grams represent the best LI feature for several types of distances, especially when we deal with many languages. So, the use of character Tri-Grams should be the best choice for those distances, except for the Out of Place measure. On the other hand, we noticed that the absolutely best distance is the Out of Place measure, especially with character Uni-Grams, where we got extremely high LI scores.

In this section, we will make a comparison between the best performances of the two proposed approaches, namely the Out of Place technique, which is the best method of our second approach and the HA2 technique, which is the best method of our first approach.

In Fig. 17
                        , we remark that the nearest prototype approach is slightly better than the high frequency one, bringing good results on the two datasets of 32 languages. On the other hand, the use of the nearest prototype approach appears to be expensive in time and resources, unlike the high frequency approach.

However, with 10 languages (i.e. few languages), the high frequency approach (HA2) seems to be quite more efficient since it gave the best scores of identification. Consequently, and according to all those results, a judicious choice of the best LI technique should be performed, by leading a prior analysis of the corpus size and its complexity.

In order to evaluate the preprocessing effect on the language identification performances, we have made a comparison between the different algorithms of the first approach, once with preprocessing and another time without preprocessing (Fig. 18
                        ).

As we see in Fig. 18, the performances of CBA and SCA are the same with and without preprocessing. Because, they are based on the existence of some specific characters in the text, therefore, removing insignificant characters (i.e. preprocessing) is not very useful. We also notice that the algorithms based on words (with test 1) have the same performances with and without preprocessing, because the common words are collected over the web (large datasets), which increase the accuracy independently from the preprocessing step.

Contrariwise, the other algorithms based on test 2 (i.e. common words are obtained after a training process on small datasets) are extremely sensitive to the preprocessing. That is, we can notice that the improvement brought for WBA (test 2) is over 34%, and it is about 11% and 14% for HA1 (test 2) and HA2 (test 2), respectively. Therefore, the proposed preprocessing steps seem to be interesting on noisy texts, when those methods are used.

In this section, we will present a comparison between our best methods and some well-known methods or tools of languages identification.

As a first evaluation, we made a comparison of our best methods with LIGA and NTC. The two algorithms are trained with 40% of DLI-32 and tested on 60% of DLI-32.

As we can see in Fig. 19
                           , the proposed algorithm HA1 is the worst one in this comparison, where the accuracy is only 89.06% and 86.72% on DLI-32 and DLI32-2 respectively. However, the other proposed algorithms HA2 and Out-of-Place outperform LIGA and NTC for both datasets appreciably. Although, Taeho et al. reported in (Tromp & Pechenizkiy, 2011) that LIGA provided good scores in Twitter (about 99%), its performances were less convincing in our experiments: accuracy of 95.31% on DLI-32 and 94.53% on DLI32-2. We could explain that fact by saying that LIGA and NTC are probably more sensitive to noisy texts. Therefore, the proposed techniques HA2 and Out-of-Place appear to be quite interesting in such conditions.

In this second evaluation, we conduct a comparison between some of our proposed methods and other universal tools of language identification, namely: Google Translate and Microsoft Word.


                           Figs. 20
                            and 21
                            summarize the comparative evaluation between our best LI methods and some universal tools like Google translate and Microsoft Word. Results show that the proposed approaches are more interesting than those universal tools (i.e. Google Translate and Microsoft Word) on the investigated datasets.

We also notice that Microsoft Word did not manage to recognize several documents, for instance: Persian texts were recognized as Arabic texts. Also, Turkish texts were recognized as French texts. Therefore, there are several reported cases of false identification with this LI tool. Google Translate was also mistaken in several cases, for instance: some Malay texts were recognized as Indonesian, because the two couples of languages are too closed. Also, some Latin texts were recognized as Italian, whereas the 2 languages are not very similar. Hence, we have reported different cases of false identification or confusion with Google Translate too.

According to those results, we can say that the proposed approaches: HA2 and Out of Place are more accurate than Google Translate and Microsoft Word for the identification of forum documents (noisy texts). However, we cannot extend this conclusion to all types of text.

In this research work, we have presented an investigation of language identification on noisy and short forum texts. Several statistical approaches were proposed, implemented and evaluated comparatively to some state-of-the-art LI methods.

All the proposed algorithms were tested on four different datasets: short texts or medium texts, and with 10 languages or 32 languages, to evaluate the effectiveness of the approaches. The datasets were constructed manually by collecting several texts with 32 different languages, from several discussion forums.

In this investigation, two statistical approaches were proposed: the high frequency approach and nearest prototype approach.

In the high frequency approach, we have proposed 5 kinds of algorithms, namely: CBA, which is based on the typical characters of each language, WBA, which is based on 20 common words defined for every language, and SCA, which is based on special characters. The other proposed algorithms are called hybrid methods: HA1 and HA2. They were proposed to overcome the weaknesses of the previous single algorithms and enhance the language identification performance.

The second approach consisted of 11 different similarity measures that are employed with character N-Grams. The different corresponding distances were as follows: Euclidean distance, Cosine distance, Manhattan distance, Chi2 distance, Squared Euclidean distance, Bray Curtis distance, Canberra distance, Correlation distance, Histogram intersection distance, Bhattacharyya distance and Out of Place distance.

The most accurate method using the high frequency approach is HA2 (parallel hybrid algorithm), which provided a high accuracy (i.e. 97.92% on DLI-32 and 96.09% on DLI-32-2). We noticed that all algorithms based on words presented better performances in test 1 compared to test 2, probably, because the words in test 1 were collected from over the web, while in test 2, they were obtained from a training process performed on only 4 short texts.

The best method in the nearest neighbor approach is the Out of Place measure (using character Uni-Grams) with accuracies of 98.96% on DLI-32 and 97.4% on DLI-32-2. Overall, the use of character Trigrams seems to be suitable for all distances, except the Correlation distance. In addition, we noticed that the results of character Unigrams and Bigrams depended on the dataset type. We also noticed that character Unigrams were more accurate on DLI-32, while character Bigrams were more accurate on DLI-32-2.

The comparison between the best results of the high frequency approach and those of the nearest prototype approach shows that the Out of place technique is slightly better than HA2 on the DLI-32 and DLI-32-2 datasets, whereas, on small datasets containing only 10 languages, HA2 appears to be more accurate .

As a comparative evaluation, two state-of-the-art approaches have been implemented and evaluated on the same datasets and in the same experimental conditions, namely: the LIGA approach and NTC approach. Furthermore, a special comparison was made with two universal tools of language identification, namely: Google Translate and Microsoft Word.

This comparative evaluation has shown that the two proposed methods: HA2 and Out-of-Place outperform all the tested baseline methods (i.e. LIGA, NTC, Google Translate and Microsoft Word) in language identification of forum documents. Moreover, the proposed preprocessing steps have further enhanced the LI accuracy in many cases, by decreasing the errors of identification.

In future, we think that it would be interesting to try different fusion architectures between the different approaches in order to further reduce the language identification errors, and hopefully improve the LI performances.

@&#ACKNOWLEDGMENTS@&#

We wish to express our great acknowledgements to Prof. Michael Oakes for accepting to proofread the manuscript. We also wish to warmly thank the editor-in-chief and reviewers for their constructive comments.

@&#REFERENCES@&#

