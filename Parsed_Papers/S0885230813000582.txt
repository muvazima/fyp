@&#MAIN-TITLE@&#The listening talker: A review of human and algorithmic context-induced modifications of speech

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Speech production is affected by environmental and interlocutor-related contextual factors.


                        
                        
                           
                           We review speech modifications produced by talkers in response to context.


                        
                        
                           
                           Algorithmic modifications aimed at increasing intelligibility are also reviewed.


                        
                        
                           
                           A summary table of 46 candidate speech modifications is provided.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Speech production

Modification algorithms

@&#ABSTRACT@&#


               
               
                  Speech output technology is finding widespread application, including in scenarios where intelligibility might be compromised – at least for some listeners – by adverse conditions. Unlike most current algorithms, talkers continually adapt their speech patterns as a response to the immediate context of spoken communication, where the type of interlocutor and the environment are the dominant situational factors influencing speech production. Observations of talker behaviour can motivate the design of more robust speech output algorithms. Starting with a listener-oriented categorisation of possible goals for speech modification, this review article summarises the extensive set of behavioural findings related to human speech modification, identifies which factors appear to be beneficial, and goes on to examine previous computational attempts to improve intelligibility in noise. The review concludes by tabulating 46 speech modifications, many of which have yet to be perceptually or algorithmically evaluated. Consequently, the review provides a roadmap for future work in improving the robustness of speech output.
               
            

@&#INTRODUCTION@&#

It is a common experience to miss information-bearing fragments of speech relayed over public address systems due to the presence of background noise, or to be surprised by an inappropriately timed interjection from a speaking navigation system while engaged in other tasks which make additional cognitive demands. Speech output, whether live, recorded or synthetic, is used increasingly in conditions where correct reception of the message is not guaranteed. To ensure that the intended message is correctly understood, developers of speech output technology have little choice at present than to resort to rather crude measures such as increasing speech volume or repeating the message, both of which can create an uncomfortable listening environment. Indeed, for those unfortunate enough to work in an environment of frequent, loud announcements, output speech is a form of noise which can contribute to ill health (World Health Organisation, 2011).

Are there alternative approaches to maintaining speech intelligibility in challenging conditions? One source of potential techniques comes from the observation of human talkers in similar contexts. It has long been known that human talkers are capable of adjusting their own speech delivery in response to context. Here, the context might be noise, which leads to speech production changes collectively described as Lombard speech, or it might be the listener or audience, leading variously to the adoption of clear speech, foreigner-directed speech or infant-directed speech amongst other listener-oriented styles. In general, talkers appear to make continuous modifications to message generation to suit the needs of the situation, targeting an appropriate place on a continuum from casual (hypo) to clear (hyper) speech (Lindblom, 1990). The modifications to speech that characterise each speech style are moderately well understood at the level of changes to acoustic parameters (e.g., f
                     0 and segment duration) and acoustic–phonetic mappings (e.g., vowel space). Precisely if and how observed changes in a talker's speech patterns contribute to intelligibility is far less clear.

Complementary approaches to speech modification are motivated by models of the auditory system, signal enhancement or linguistic entropy. A simple technique is to reallocate speech energy to those frequency regions predicted to contribute most to speech intelligibility, perhaps exploiting estimates of the masker spectrum to determine where to boost speech energy to improve audibility. Speech modification algorithms are also influenced by techniques from cognate domains, e.g., dynamic range compression from audio broadcasting. Likewise, information-theoretic concerns such as increasing message redundancy can inspire modification approaches.

This review article examines both human and algorithmic modifications to speech. Starting with a top-level taxonomy of possible goals of modification as seen from the point of view of a listener (Section 2), those listening contexts which induce modifications in human speech production are identified in Section 3, and the resulting changes in acoustic, phonetic and higher-level parameters are detailed in Section 4. The perceptual effect of human speech modifications is examined in Section 5. Section 6 reviews speech modification algorithms to date. Finally, in Section 7 we draw together both behavioural and algorithmic studies into a unified compilation of speech modifications and suggest new directions for research in intelligibility-enhancing speech modification.

Since maintaining intelligibility is our key focus in this review, one way to look at speech production changes in response to context is in terms of how they might be expected to benefit the listener. In subsequent sections we group both human and algorithmic modifications under the following four categories, which are ordered approximately from low- to high-level.

Here, the aim is to improve the audibility of the target speech by reducing the energetic masking effect of the noise caused by the interaction of speech and noise at the level of the auditory periphery. Both simultaneous and non-simultaneous masking are well-described by computational models (e.g., Moore, 2003; Dau et al., 1996) which allow prediction of the benefit of speech modifications. Changing the spectral slope of speech in a masker-dependent manner is an example of an audibility-promoting speech modification.

Audible fragments of speech which escape masking need to be allocated to a single ongoing linguistic interpretation. This requirement can be challenging when the masker is itself speech, leading to a form of informational masking which increases as the source and target get more similar (Brungart et al., 2001). One approach to coherence-enhancing speech modification is to attach information to the target speech to ease the process of grouping audible fragments together (e.g., by presenting them from a common spatial direction). Conversely, modifications may attempt to increase the distance between the target and masker to enable their separation into distinct auditory streams (Bregman, 1990) (e.g., by changing the f
                        0 to avoid collision with the background source).

Information-bearing elements of speech are encoded with a high degree of redundancy, allowing speech to be degraded in various ways before intelligibility declines. For instance, at low levels, syllable-rate temporal modulations are present across the entire spectrum, leading to a resistance against masking, while at higher levels tactics such as lexical repetition or rephrasing also constitute redundant encoding. Modifications in this category aim to actively increase redundancy (e.g., synthesising speech with multiple cues to voicing; match interlocutor word choices), or better approximate canonical forms of speech.

Interpreting speech output is often just one of several tasks which listeners are engaged in. Modification approaches might seek to minimise the cognitive effort associated with speech processing. This could involve simplifying the message or modifying it to better meet listeners’ expectations, or, in the context of a dialog system, employing back channels to signal agreement or otherwise.

Note that the same modification might help intelligibility in more than one of the above categories. For example, the use of more prototypical vowels could help to increase coherence, by permitting sequential grouping of speech from the same talker, and at the same time decrease the cognitive effort required to process unfamiliar forms of speech (Floccia et al., 2006).

Types of speech that are produced with the goal of improving intelligibility are commonly referred to as ‘clear speech’ or ‘hyper-speech’. However, these labels cover a great variety of situations and speech types that differ in intention. First, there is a distinction between the ‘inadvertent or natural clear speech’ (Bond and Moore, 1994; Krause and Braida, 2004) produced by certain speakers who are intrinsically more intelligible than others (inter-speaker variability), and ‘deliberately clear speech’ produced by one speaker in adaptation to a perturbed situation of communication or to a listener with reduced comprehension (intra-speaker variability). Most studies of deliberately clear speech use an elicitation procedure in which speakers are asked to imagine themselves speaking to a hearing impaired person (Chen, 1980; Picheny et al., 1986; Payton et al., 1994; Uchanski et al., 1996; Bradlow, 2002) or to a non-native listener (Moon and Lindblom, 1989, 1994; Lindblom et al., 1992; Uther et al., 2007). By extension, the term ‘clear speech’ has been used to designate any kind of hyper-articulated speech that aims at improving speech intelligibility or at preserving it in adverse listening conditions. A comprehensive review of clear speech is provided in Uchanski (2005).

For the current review, we avoid using the term ‘clear speech’ in favour of a more detailed categorisation based on types of speech induced by either interlocutor- or environment-related factors. Since we make extensive use of these categories in subsequent sections, a list of acronyms and abbreviations is provided in Table 1
                     .

Interlocutor-induced modifications occur in speech addressed to listeners (or machines) who are perceived by the talker to have intrinsically reduced comprehension (i.e., regardless of listening environment), and includes the following categories:
                           
                              •
                              Infant directed speech (IDS), also called ‘motherese’, ‘parentese’, ‘babytalk’ or child directed speech (CDS) (Snow and Ferguson, 1977; Newport et al., 1977; Papoušek et al., 1985; Lindblom et al., 1992; Burnham et al., 2002; Dodane et al., 2006).

Speech addressed to children with learning disabilities (Bradlow et al., 2003).

Hearing-impaired directed speech (HIDS) (Picheny et al., 1985, 1986; Schum, 1996; Uchanski et al., 1996; Howell and Bonnett, 1997; Bradlow, 2002; Bradlow and Bent, 2002; Lam and Kitamura, 2012).

Hyper-visual speech and speech addressed to deaf persons (Benoit et al., 1996; Beautemps et al., 1999).

Foreigner-directed speech (FDS) addressed to non-native listeners (Freed, 1981; Moon and Lindblom, 1989; Papoušek and Hwang, 1991; Biersack et al., 2005; Scarborough et al., 2007; Smith, 2007; Uther et al., 2007; Little, 2011; Sankowska et al., 2011).

Machine directed speech (MDS) (Burnham et al., 2010a,b; Oviatt et al., 1998; Mayo et al., 2012).

Speech used when correcting/amending (Beckford Wassink et al., 2007).

Pet directed speech (PDS) (Burnham et al., 2002; Sims and Chin, 2002; Batliner et al., 2008).

Environmental modifications are those which occur when audibility (including self-audibility) is affected by – or perceived to be affected by – the distorting effects of additive, channel or convolutional noise:
                           
                              •
                              Speech produced in noise, known as ‘Lombard speech’ (LS; Lombard (1911)) (Summers et al., 1988; Junqua, 1993; Castellanos et al., 1996; Patel and Schell, 2008; Cooke and Lu, 2010; Garnier et al., 2010; Hazan and Baker, 2011) or speech addressed to a listener whose audition is perturbed by noise (Chen, 1980; Hazan and Baker, 2011).

Speech produced in filtered or reverberant environments (Brunskog et al., 2009; Pelegrín-García et al., 2011) or addressed to a listener through a distorted transmission channel (Cutler and Butterfield, 1990; Hazan and Baker, 2011).

Speech addressed to a distant person (Lienard and Di Benedetto, 1999; Traunmuller and Eriksson, 2000; Eriksson and Traunmuller, 2002; Cheyne et al., 2009; Fux et al., 2011, 2012a,b; Pelegrín-García et al., 2011)

The subdivision into interlocutor- and environment-related contexts is by no means absolute. Speech directed to individuals with hearing impairment might share characteristics with speech produced in response to a perceived loss of audibility caused by noise, for example. At the same time, speech induced by interlocutor and environment-related contexts cannot always be treated as similar or even comparable. Interlocutor-related speech changes typically possess adaptations – a slower speech rate, longer and more frequent pauses, exaggerated articulation – that might be considered as communicative strategies that help the listener to retrieve and decode phonetic cues. In contrast, speech changes induced by environmental factors are primarily characterised by an increase in vocal intensity (accompanied by changes in fundamental frequency and spectrum) compared to speech produced at a comfortable level (Rostolland, 1982; Schulman, 1989; Titze, 1989; Sundberg and Nordenberg, 2006). Unlike interlocutor-induced speech, the primary goal of increased vocal intensity is not necessarily to enhance the clarity of phonetic cues, but to preserve their audibility, a strategy that can even be detrimental to their clarity (Pickett, 1956; Pickett and Pollack, 1958; Rostolland and Parant, 1973; Junqua, 1993; Cooke and García Lecumberri, 2012). As a consequence, complementary strategies are often observed in adverse conditions whose aim is to simplify lexical and semantic decoding of the message.

Speech adaptations are not only motivated by the improvement of speech intelligibility, but also by social and affective goals. For example, some of the modifications observed in IDS have no effect on speech intelligibility but instead may express affect or aim at getting the infant's attention (Fernald, 1989; Trainor et al., 2000). Pet directed speech (PDS) demonstrates expressive intonation patterns like IDS, but no similar hyper-articulation (Burnham et al., 2002; Sims and Chin, 2002; Batliner et al., 2008). Conversely, FDS and MDS sometimes exhibit exaggerated segmental and prosodic cues similar to IDS, but lack expressive intonation (Burnham et al., 2010a,b; Papoušek and Hwang, 1991; Oviatt et al., 1998; Biersack et al., 2005; Uther et al., 2007) and in the case of ‘acted’ MDS reduced f
                        0 range has been observed (Mayo et al., 2012).

Another social adaptation to consider in the framework of speech enhancement is that of alignment (also referred to as speech accommodation or phonetic convergence). Indeed, it is now well known that people tend to partially imitate each other when they interact, at the postural level as well as at respiratory, phonetic or lexical levels (Natale, 1975; Pardo, 2006; Delvaux and Soquet, 2007; Kappes et al., 2009; Aubanel and Nguyen, 2010; Bailly and Lelong, 2010; Miller et al., 2010; Babel, 2011; Babel and Bulatov, 2012). This ‘convergence’ phenomenon – and its counterpart, divergence – is believed to be primarily driven by social motivations, such as the desire to yield positive affective evaluation by the interlocutor (Giles et al., 1991). Nevertheless, this adaptation can also be considered as a strategy of speech enhancement, as it helps to harmonise phonetic and lexical repertoires between the speech partners and may therefore contribute towards successful communication (Pickering and Garrod, 2006).

In this section we identify and categorise, according to the taxonomy introduced in Section 2, the large body of previous work which has attempted to describe how speech changes as a response to interlocutor and environmental factors. The identification of speech production changes with each of these levels of processing is not perfect: certain speech production modifications can be expected to improve intelligibility by acting at more than one of these levels (e.g., a slower speech rate can increase the likelihood of salient information escaping masking, decrease cognitive load and at the same time contribute to more canonical speech cues). In these cases, we have described the speech modification at what we consider to be the processing level where the change is most effective.

Preserving and promoting audibility is the main problem in perturbed environments (noise, distance, filtering). To compensate for such environments it has been argued that talkers aim to ‘expand speech sonority’ (Beckman et al., 1992) by increasing their vocal intensity, reallocating speech energy in the frequency domain and enhancing speech modulation over time.

A straightforward strategy to reduce the effect of additive noise is to increase vocal intensity in order to place more of the speech spectral energy distribution above the level of the masker. Indeed, a global increase of speech intensity is especially apparent when masking noise is present in the environment, leading to Lombard speech (e.g., Webster and Klumpp, 1962; Lane et al., 1970; Egan, 1972; Summers et al., 1988; Bond et al., 1989; Junqua, 1993; Castellanos et al., 1996). However, even in the absence of masking noise, level increases are observed, perhaps as a form of compensation for perceived listener difficulties. For example, level increases are present in HIDS (Picheny et al., 1986) as well as in speech produced at distance (Warren, 1968; Michael et al., 1995; Fux et al., 2011, 2012b; Pelegrín-García et al., 2011), with vocal intensity increasing as a quasi-linear function of noise level or distance to the listener (Kryter, 1946; Korn, 1954; Lane et al., 1970; Egan, 1972; Dejonckere and Pepin, 1983; Garnier et al., 2010; Fux et al., 2011; Pelegrín-García et al., 2011). Increasing speech level by mechanisms such as shouting can be detrimental to the clarity of phonetic cues (Pickett, 1956; Rostolland and Parant, 1973; Junqua, 1993).

The increase in vocal intensity is accompanied by a raised f
                           0 (Summers et al., 1988; Bond and Moore, 1990) and a flatter spectral tilt, resulting in enhanced speech energy in medium and high frequencies (Pisoni et al., 1985; Picheny et al., 1986; Stanton et al., 1988; Summers et al., 1988; Mokbel, 1992; Junqua, 1993; Tartter et al., 1993; Castellanos et al., 1996; Pittman and Wiley, 2001). These modifications in average f
                           0 and spectrum can simply be considered as direct consequences of an increase in vocal effort (Titze, 1989; Sundberg and Nordenberg, 2006). Nevertheless, the sensitivity of the human ear to sound pressure level is frequency-dependent and is at its maximum around 3kHz, so that these modifications in f
                           0 and spectrum may result from an attempt to improve audibility. Furthermore, speech produced in noise displays not only a flatter spectral tilt, but also a specific boost in the amplitude of higher formants around 3–4kHz (Garnier and Henrich, in press), similar to the spectrum ‘ring’ observed in stage actors (Bele, 2006), which may help in projecting the voice to a distant listener. Although speakers who are intrinsically more intelligible than others do not necessarily produce speech with greater vocal intensity, they typically demonstrate enhanced speech energy in the 1–3kHz frequency band compared to less intelligible speakers (Krause and Braida, 2004).

Although IDS is not produced with increased vocal effort but rather at comfortable level (Beckford Wassink et al., 2007), it is also commonly characterised by a significant rise in average f
                           0 (Bradlow et al., 2003). This increase depends on the child's age, gender and hearing experience (Niwano and Sugai, 2002; Kitamura and Burnham, 2003; Xu et al., 2007). While f
                           0 increases are observed in a great variety of languages, both tonal (Thanavisuth and Luksaneeyanawin, 1998; Xu et al., 2007) or non-tonal (Stern et al., 1983; Pye, 1986; Grieser and Kuhl, 1988; Beckford Wassink et al., 2007), there is some evidence that they are not a universal feature of babytalk (Ratner and Pye, 1984). A similar f
                           0 increase is also observed in PDS (Burnham et al., 2002) but not in FDS (Papoušek and Hwang, 1991; Uther et al., 2007), supporting the idea that a higher f
                           0 may have an affective or attentional function rather than one of speech enhancement.

Vowels and voiced consonants with formant structure (nasals, liquids) are the most robust sounds to energetic masking by a broadband noise (Junqua, 1993) and to sound attenuation with distance (Peutz, 1971). In LS and speech produced at distance, speakers increase the intensity and duration of vowels more than consonants (Egan, 1972; Stanton et al., 1988; Summers et al., 1988; Mokbel, 1992; Junqua, 1993; Castellanos et al., 1996; Kadiri, 1998; Boril and Pollak, 2005; Garnier and Henrich, in press), but sonorants are no more enhanced than voiceless consonants. Conversely, HIDS is characterized by a relative boost in intensity for consonants (Picheny et al., 1986; Bradlow et al., 2003), especially for voiceless consonants (Chen, 1980). Interestingly, IDS also has a higher vowel to consonant ratio (in both intensity and duration) compared to conversational speech (Payne et al., 2009), even though in that case there is no masking to compensate for. Other formant-related changes observed include increased formant amplitudes in HIDS (Picheny et al., 1986) and narrower formant bandwidths in clear forms of speech (Krause and Braida, 2004).

To a first approximation, the amount of energetic masking is determined by level differences across time and frequency between speech and masker. When speaking in noise, a speaker might therefore use either a boosting or a bypass strategy to decrease the amount of energetic masking. Boosting would entail increasing speech level in those time-frequency regions where the background noise would otherwise be more intense than the speech, while the bypass strategy would operate by shifting energy concentrations in time and frequency to regions where the background noise is less intense. Of course, any reallocation of speech energy is constrained by both articulatory constraints and the need to preserve phonetic cues.

Speech adaptation in noisy environments is indeed significantly influenced by the type of noise (Egan, 1972; Mokbel, 1992; Ternström et al., 2002; Garnier, 2007; Lu and Cooke, 2008; Jung, 2012; Garnier and Henrich, in press). At comparable sound pressure levels (in dB SPL), white and broadband maskers induce greater increases in vocal intensity, average f
                           0 and energy above 1kHz than multi-talker babble noise (Garnier and Henrich, in press). However, when noise types are compared at similar perceived loudness (in dB A), white noise, speech shaped noise, music noise or driving noise do not induce significantly different adaptations in vocal effort (Junqua et al., 1998; Ternström et al., 2002; Jung, 2012).

Some studies (Mokbel, 1992; Junqua et al., 1998) provided evidence for boosting strategies, demonstrating greater increases of speech energy in frequency bands containing high levels of masker energy. In a single talker experiment comparing speech adaptation to broadband noises filtered by different band-pass filters, the increase in vocal intensity varied with noise spectral tilt for a constant masker level (Junqua et al., 1998).

Other studies have suggested that bypass strategies are operative, showing increases in spectral center of gravity (CoG) when speaking in low-pass noises (multi-babble noise, driving noise or low-pass filtered broadband noise) (Garnier, 2007; Lu and Cooke, 2008, 2009a; Garnier and Henrich, in press). There is also evidence that some speakers specifically adjust their f
                           0 and F1 in local energy minima of a multi-talker noise (Garnier, 2007; Garnier and Henrich, in press). However, speakers do not decrease the CoG of their speech spectrum when speaking in high-pass filtered noises (Lu and Cooke, 2009a), calling into question the existence of active bypass strategies. At a temporal level, talkers also reduce the overlaps between speech and background in fluctuating noise backgrounds (competing talker and speech modulated noise) (Cooke and Lu, 2010).

In contrast to perceptual studies of factors which promote the formation of coherent auditory descriptions of objects (Bregman, 1990; Best et al., 2008), relatively little work has been done to explore what changes talkers make (or are capable of making) to increase the coherence of their speech in the face of competing sound sources. Some related work is reviewed here, but even in these cases it is far from clear whether the talker's goal is to facilitate speech separation and perceptual organisation for listeners.

It is known that a voice is better detected in – and in some cases segregated from – an intense background when it demonstrates large amplitude dynamics (Boike and Souza, 2000; Hornsby and Ricketts, 2001), temporal fluctuations (Ishizuka and Aikawa, 2002) and increased frequency modulation (e.g., large vibrato, enhanced intonation) (Marin and McAdams, 1991; Ishizuka and Aikawa, 2002). Both LS and naturally produced clear speech typically exhibit enhanced low-frequency modulations of the intensity envelope (Krause and Braida, 2004; Garnier and Henrich, in press).

Speakers exaggerate f
                           0 modulation in many forms of modified speech. Larger pitch excursions are observed in IDS (Golinkoff and Ames, 1979; Fernald and Simon, 1984; Grieser and Kuhl, 1988; Niwano and Sugai, 2003; Stern et al., 1983; van de Weijer, 1997; Räsänen et al., 2008; Kondaurova and Bergeson, 2011), FDS (Papoušek and Hwang, 1991; Uther et al., 2007) and LS (Boril and Pollak, 2005; Garnier, 2007; Garnier and Henrich, in press). In IDS, this enhancement of pitch modulations varies as a function of the child's age and gender (Stern et al., 1983; Kitamura et al., 2001; Niwano and Sugai, 2002). In tone languages, these supra-segmental modifications of f
                           0 are likely to interact with tone clarity. For example, when talking to infants, speakers appears to prioritise exaggerated intonation contours, while reducing tonal information (Papoušek and Hwang, 1991; Papoušek et al., 1991). On the contrary, in LS and FDS, speakers enhance tonal contrasts in priority to supra-segmental information (Papoušek and Hwang, 1991; Papoušek et al., 1991; Zhao and Jurafsky, 2009). Exaggeration of f
                           0 modulation is observed in IDS for a number of European languages, Japanese and tone languages (Grieser and Kuhl, 1988; Fernald et al., 1989; Kuhl et al., 1997; Kitamura et al., 2001; Falk, 2011). However, further examination reveals some cross-linguistic variations in the use of pitch range (Fernald et al., 1989), in the specific enhancement of prosodic cues to phrase and utterance boundaries (Fisher and Tokura, 1996; Garnier et al., 2006b, 2010; Welby, 2006), and in the preservation of rhythm specificities (Payne et al., 2009).

Informational masking (IM) refers to any reduction in intelligibility once energetic masking in the auditory periphery has been accounted for. IM has many facets, including the difficulty of determining which audible components belong to the target source, a problem which is especially acute when the target and masker are similar in properties such as f
                           0 or vocal tract length. In principle, a talker might reduce IM through modifications that increase the distance between their own speech and that of the background in a number of ways such as adopting a different f
                           0 range or mean vocal output level to avoid clashing with the masker, or by simply changing spatial location to provide binaural cues for an interlocutor. However, the amount of informational masking has not yet been demonstrated to have any significant effect on speech adaptation to a noisy environment. Indeed, Lombard effects are very similar in the presence of competing speech and speech-modulated noise (Cooke and Lu, 2010). In a multitalker background, vocal effort increases with the number of talkers (Lu and Cooke, 2008), i.e., with the amount of energetic masking, in spite of the decrease in IM that accompanies an increase in the number of talkers. Talkers also make temporal adjustments in the face of potential informational maskers, but do so by decreasing the amount of temporal overlap with the masker (Cooke and Lu, 2010; Aubanel and Cooke, 2013b), which can be expected to increase IM. It seems likely that for a talker, minimising energetic masking takes priority over reducing IM.

In the acoustic space of the first two formant frequencies (F1 and F2), the distance between vowel categories increases in HIDS (Chen, 1980; Picheny et al., 1986; Ferguson and Kewley-Port, 2002; Krause and Braida, 2004), IDS (Zajdó, 2006; Ratner, 1984; Andruski and Kuhl, 1996; Kuhl et al., 1997; Bradlow, 2002; Burnham et al., 2002; Kirchhoff and Schimmel, 2005), speech addressed to children with learning disabilities (Bradlow et al., 2003), FDS (Moon and Lindblom, 1989, 1994; Uther et al., 2007) and MDS (Burnham et al., 2010a,b; Oviatt et al., 1998). In IDS, this expansion varies with child age and development (Ratner, 1984). At the articulatory level, lip and jaw movements are exaggerated: vowels are articulated with globally more open and spread lips (Picheny et al., 1986; Lindblom et al., 1992; Green et al., 2010), and with greater peak velocities of lip movements (Matthies et al., 2001).

However, not all vowels in an utterance are hyper-articulated, but only those in stressed syllables (Adriaans and Swingley, 2012). Furthermore, vowel space modification in IDS, FDS and MDS does not appear to be part of an homogeneous expansion of the whole vowel system. Instead, only the extrema of the vowel system /a,i,u/ are hyper-articulated, whereas internal phonemic contrasts such as [i-I] are not enhanced, and may be reduced (Cristia and Seidl, 2013). Furthermore, speaking clearly neither reduces the within-category dispersion nor increases the degree of coarticulation (Bradlow, 2002).

Hyper-articulation is also observed in speech produced in response to environmental changes, but differs from that observed in IDS, FDS or MDS. In LS (Summers et al., 1988; Bond et al., 1989; Garnier, 2007, 2008), speech produced at distance (Lienard and Di Benedetto, 1999) and prosodic focus (Beckman et al., 1992), the main articulatory modification consists in a global increase of jaw and lip aperture for all categories of vowels (Schulman, 1989; Garnier, 2008) and a global amplification of lip opening and closing gestures (Kim et al., 2005; Davis et al., 2006; Garnier et al., 2006a; Fitzpatrick et al., 2011). As a result, the vowel system is shifted towards higher first formant frequencies rather than exhibiting a global expansion in (F1, F2) space. Despite this shift, most speakers demonstrate an enhanced contrast between open and closed vowels, along the F1 dimension, as well as enhanced visible contrasts in lip opening, rounding and protrusion (Junqua, 1992; Garnier, 2008). At low noise levels, speakers are also able to enhance the contrast in F2 between front and back vowels, resulting in an increased vowel space as for IDS, FDS or MDS (Junqua, 1992; Perkell et al., 2007). At moderate and high noise levels, however, the F2 contrast is systematically reduced (Perkell et al., 2007), exhibiting decreased rather than exaggerated tongue movements and lip spreading (Garnier, 2008; Garnier et al., 2012).

Finally, the duration contrast between tense /i,u/ and lax /I,U/ vowels is enhanced in IDS for languages such as English, in which vowel duration is an additional discriminative cue (Kondaurova et al., 2012).

Contrary to the ‘hyperspace’ hypothesis (Johnson et al., 1993), the density of a vowel system does not appear to influence the degree of its expansion in modified forms of speech. Thus, vowel hyper-articulation in IDS is comparable in English, Spanish (Bradlow, 2002), Croatian (Smiljanic and Bradlow, 2005), and Finnish (Granlund et al., 2011, 2012), although these last three languages have fewer vowels than English.

Changes in voice onset time (VOT) for plosives have been reported in HIDS (Chen, 1980; Picheny et al., 1986), IDS (Baran et al., 1977; Sundberg and Lacerda, 1999; Englund, 2005; Sundberg, 2001; Synnestvedt et al., 2010) and LS (Hazan et al., 2012). VOT changes enhance the contrast between voiced and voiceless plosives for some speakers only (Malsheen, 1980; Synnestvedt et al., 2010) who rely on this cue as listeners (Kang and Guion, 2008). Other speakers, who rely more on f
                           0 at the onset of the following vowel for this discrimination, enhance instead that contrast in clear speech (Kang and Guion, 2008). Changes in the VOT of plosive consonants depend more on speaker than language (Granlund et al., 2011, 2012).

Fricative consonants are produced with enhanced contrast in the first spectral moment in MDS (Maniwa et al., 2009) and speech addressed to older infants (12–14 months) but not younger infants (4–6 months) (Cristià, 2010). However, speakers do not enhance this contrast in noise (Hazan et al., 2012) and may in fact reduce it (Perkell et al., 2007).

Information-bearing formant transitions are generally longer in HIDS (Chen, 1980), while both citation and clear speech exhibit steeper F2 transitions (Moon and Lindblom, 1994).

Finally, vowel quality and length are additional cues to discriminate voiced and voiceless consonants in some languages, such as English, in which vowels are shortened when followed by voiceless consonants. However, this phonological contrast is not enhanced in FDS, and has been found to decrease in LS (Sankowska et al., 2011).

The enhancement of audible contrasts between phonological categories extends to tone languages. Like vowels, tone contrasts are exaggerated in both IDS (Liu et al., 2003, 2007, 2009; Xu et al., 2007; Xu and Burnham, 2010; Burnham et al., 2011) and FDS (Papoušek and Hwang, 1991), and tone enhancement varies with a child's age and development (Liu et al., 2009). In contrast, tones are not more contrasted when speaking in noise but instead produced at higher pitch (Zhao and Jurafsky, 2009). The amplification of phonetic gestures also extends to sign languages, in which hand movements are exaggerated when a deaf parent interact with their deaf infant compared to when communicating with deaf adults (Erting et al., 1990; Masataka, 1992).

In addition to the possible coherence-related function of f
                           0 modulations described in Section 4.2.1, raised f
                           0 and exaggerated f
                           0 modulation can have different linguistic and communicative functions. Our focus is on their role – in combination with other prosodic cues such as syllable lengthening and variations in vocal intensity – in speech parsing. Several studies support the idea that speakers enhance segmentation cues in modified forms of speech. First, speakers exaggerate syllabification in noise, speech produced at a distance and in HIDS, both by increasing the modulation of the syllable's intensity envelope and by inserting inter-syllabic pauses (Garnier, 2007; Garnier and Henrich, in press). Likewise, speakers globally insert more and longer pauses in HIDS (Picheny et al., 1986; Imaizumi et al., 1993a,b), to children with learning disabilities (Bradlow et al., 2003; Krause and Braida, 2003), to infants (Fernald et al., 1989) or in noise (Garnier et al., 2010). These pauses are inserted especially before words starting with weak syllables (Cutler and Butterfield, 1990) and before phrase boundaries (Fernald et al., 1989; Imaizumi et al., 1993a). In IDS, exaggeration of pause duration decreases with the age of the infant (Kondaurova and Bergeson, 2011).

In many languages, syllable lengthening is a boundary cue to the end of a word, phrase or utterance, with greater lengthening associated with higher prosodic levels (Beckman and Edwards, 1994; Jun and Fougeron, 2000). In IDS, HIDS and LS, the final vowels or syllables of words (Albin and Echols, 1996), phrases (Fisher and Tokura, 1996; Garnier et al., 2006b, 2010; Kondaurova and Bergeson, 2011) and sentences (Fisher and Tokura, 1996; Church et al., 2005; Garnier et al., 2006b, 2010) are further lengthened compared to that observed in conversational speech.

Pitch rises or falls complement syllable lengthening to mark lexical, phrase or utterance boundaries. Thus, a pitch lowering marks the end of a declarative utterance in many languages. In French, a bi-tonal LH* (low-high) primary accent marks the end of accentual phrases within the utterance (Jun and Fougeron, 2000), while in Japanese, a pitch rise marks their beginning (Warner et al., 2010). In French, a secondary LHi (low-high initial) accent can be found within an accentual phrase (Jun and Fougeron, 2000) and its ‘elbow’ in the f
                           0 contour marks the boundary between the beginning of a content word and its preceding determiner (Welby, 2003). In IDS and LS, speakers exaggerate these intonation cues to sentence, phrase and word boundaries (Papoušek and Hwang, 1991; Fisher and Tokura, 1996; Garnier et al., 2006b, 2010; Welby, 2006).

Lexical stress in languages such as English can also be used as a perceptual cue to word segmentation (Field, 2005). Parents enhance lexical stress when speaking to infants older than 11 months by lending additional stress to strong syllables (Wang et al., 2012).

The goal of communication is not to produce speech sounds and gestures but to use them to transmit a message. Thus, improving the audibility and the phonetic clarity of speech sounds and gestures is not the only way to improve speech intelligibility. Another potential strategy is to reduce message complexity by approaches such as utterance simplification, increasing redundancy over time or over different modalities, or by attracting the listener's attention to those parts of the utterance containing the most important information.

A slower speech rate is typically observed in IDS (Grieser and Kuhl, 1988; van de Weijer, 1997; Räsänen et al., 2008), FDS (Papoušek and Hwang, 1991; Uther et al., 2007), MDS (Burnham et al., 2010a,b; Oviatt et al., 1998), HIDS (Picheny et al., 1986), speech addressed to children with learning disabilities (Bradlow et al., 2003), and speech produced at distance (Warren, 1968; Fux et al., 2012b). Such a decrease in speech rate is also generally observed in LS (Pisoni et al., 1985; Junqua, 1993; Kadiri, 1998; Kim, 2005), although this is not true of all speakers (Kim, 2005; Garnier, 2007). The reduction in the rate at which information is transmitted also applies to sign languages: deaf parents sign at a significantly slower rate when they address their deaf infants compared to the rate used to interact with deaf adult friends (Erting et al., 1990; Masataka, 1992).

However, a global decrease in speech rate does not necessarily reflect an attempt to improve the reception of phonetic information. Indeed, rate reduction comes mainly from more frequent and longer pauses within the speech stream, with a lesser role for speech segment lengthening (Picheny et al., 1986). Furthermore, not all segments benefit from speech rate reductions: vowels are lengthened much more than consonants (Stanton et al., 1988; Junqua, 1993; Castellanos et al., 1996) and in LS consonants may even be shortened (Junqua, 1993; Castellanos et al., 1996; Kim, 2005; Lu and Cooke, 2008; Garnier and Henrich, in press).

IDS is characterised by a reduced vocabulary (Cheskin, 1981; Ratner, 1988) and by more frequent use of common and short words (Phillips, 1973; Ratner, 1988; Zampini et al., 2012). The lexicon addressed to children with Down's syndrome also presents fewer function words but more onomatopoeias, and demonstrates less lexical variability than the lexicon addressed to typically developing children of the same age (Zampini et al., 2012).

Similarly, speech addressed to hearing impaired children (Cheskin, 1981; Imaizumi et al., 1993a,b), to children with Down's syndrome (Zampini et al., 2012) and to non-native adult listeners (Long, 1981, 1983) has shorter sentences, with less syntactic and morphological complexity than for children with normal hearing and typical development. In response to mis-comprehension, talkers tend to reformulate their utterance using a simpler syntactic structure (Valian and Wales, 1976; Valian, 1980).

Parents talk to their young children with incomplete sentences, with skipped prepositions, pronouns and articles, again mirroring utterances produced by the children themselves (Zampini et al., 2012). This kind of imitation is supported by other evidence of syntactic (Branigan et al., 2000) and semantic (Garrod and Anderson, 1987) co-ordination between adult speakers in dialogue interaction (although contrary observations have been reported, e.g., Healey et al., 2010).

At a lexical level, IDS exaggerates f
                           0 peaks even more on focus words (Fernald and Mazzie, 1991) and enhances the existing contrast in f
                           0 and intensity between content and function words (Dodane et al., 2006), to the point that function words are often skipped (Zampini et al., 2012). In noise too, words bearing information about agents, objects or locations are stressed in comparison to verbs or determiners (Garnier et al., 2006b; Garnier, 2007; Patel and Schell, 2008).

The first occurrence of a word is produced with longer duration than subsequent repetitions of this same word (Fowler and Housum, 1987; Fowler, 1988; Bell et al., 2009). Similarly, frequent words in a language are produced with shorter duration and reduced articulation, compared to less frequent words (Zipf, 1929; Bell et al., 2009). In IDS, parents emphasise new words relative to the rest of the utterance (Fisher and Tokura, 1995).

In both spoken and sign languages, parents repeat their own utterances (Cheskin, 1981) or the same signs (Erting et al., 1990; Masataka, 1992) significantly more frequently when addressing to HI children, than normal hearing children or deaf adults.

In noisy environments, some speakers enhance their lip articulatory movements when their interlocutor can see them, compared to when the interlocutor can only hear them (Fitzpatrick et al., 2011). However, not all speakers demonstrate this strategy (Garnier et al., 2012).

While speech production changes as a function of communicative situation, there is no guarantee that the observed modifications have a beneficial effect on speech communication itself, and especially on message intelligibility. We detail below evidence of communicative benefits from modifications, first organised by modification context and subsequently by the speech characteristic believed to be responsible for the benefit.

HIDS benefits speech perception by listeners with severe hearing loss both in quiet (Picheny et al., 1985) and in simulated noisy/reverberant environments (Payton et al., 1994; Uchanski et al., 1996). However, in adverse listening conditions, while HIDS improves intelligibility globally, it does not appear to improve the recognition and discrimination of segments (Ferguson and Kewley-Port, 2002). HIDS also benefits elderly listeners with moderate hearing loss in noise and reverberation (Schum, 1996), and in both audio and audiovisual domains (Helfer, 1998). However, no relationship was found between the amount of benefit and the degree of hearing loss in elderly listeners (Helfer, 1998). HIDS also improves speech perception by normal-hearing listeners in noise/reverberation (Chen, 1980; Payton et al., 1994; Uchanski et al., 1996; Ferguson and Kewley-Port, 2002; Krause and Braida, 2002; Ferguson, 2004), in audio-only, audiovisual and visual-only domains (Gagne et al., 1994, 2002; Gagne, 1995; Helfer, 1997), at both a global and segmental level (Picheny et al., 1985). The intelligibility benefit of HIDS increases with language experience, being larger for native than non-native listeners (Bradlow and Bent, 2002), and for adults compared to school-age children (Bradlow et al., 2003). Independent of the type of listener, benefits typically increase with the level of acoustic degradation of the environment (Payton et al., 1994).

LS globally improves the intelligibility of words and sentences in the audio domain (Dreher and O’Neill, 1957; Summers et al., 1988; Pittman and Wiley, 2001; Chung et al., 2005; Lu and Cooke, 2008), although the degree of Lombard gain varies both with the type and level of the noise used to induce the Lombard effect (Lu and Cooke, 2008). No evidence was found to support the idea that LS utterances would be more intelligible when perceived in the same background noise as the one in which they were produced (Lu and Cooke, 2008). LS also benefits non-native listeners to a similar degree as native listeners, but when presented to the former group in noise-free conditions results in a small loss in intelligibility (Cooke and García Lecumberri, 2012). At the segment level, all studies apart from (Junqua, 1993) showed an increased intelligibility of vowels and consonants produced in noise in the audio domain. However, the intelligibility gain from the inclusion of visual information is weaker in LS than in speech produced in silence (Chung et al., 2005; Davis et al., 2006; Vatikiotis-Bateson et al., 2006).

Concerning IDS, the use of exaggerated acoustic characteristics in the early preverbal period is assumed to primarily aim at attracting the infant's attention and encouraging interaction (Sachs, 1977; Ryan, 1978; Stern et al., 1983). Indeed, numerous studies have shown that infants pay more attention to IDS (e.g., Fernald, 1985; Werker and McLeod, 1989; Pegg et al., 1992; Kaplan et al., 1996; Cooper et al., 1997; Dunst et al., 2012) and demonstrate increased brain activity when listening to it (Zangl and Mills, 2007; Naoi et al., 2012). It seems probable that f
                        0 variations are mainly responsible for this increased attention, as babies show similar preference for sinewaves reproducing IDS intonation (Cooper and Aslin, 1994). IDS is also believed to facilitate the recognition and discrimination of phonetic cues by infants, and to help them develop phonological representations. Indeed, infants’ discrimination of sound categories appears to be directly related to their parents’ vowel space area (Liu et al., 2003) and more globally to the distribution of acoustic cues which they are exposed to (Maye et al., 2002, 2007). The exaggeration of audio and visual contrasts in IDS has been shown to improve infants’ discrimination of vowels (Trainor and Desjardins, 2002) and to enable them to perceive non-native phonemes that they do not perceive in adult directed speech form (Ostroff, 2000). Finally, IDS also helps young children recognize words (Song et al., 2010) and maintain that recognition over the longer term (Singh et al., 2009). The perceptual benefits of IDS for other listeners is controversial: non-native listeners benefited from IDS in their acquisition of new words (Golinkoff and Alioto, 1995), an observation that Yang and Chen (2004) failed to replicate.

While the acoustic and phonetic changes observed in modified speech are well-documented, relatively little can be said with certainty about the origins of any intelligibility benefits which results from these modifications. Rather, the contributions of many of the main changes in speech production appear small and in many cases research findings are equivocal. There is here a need for carefully controlled studies perhaps making use of more sophisticated signal analysis and resynthesis methods to determine where any intelligibility gain comes from. We mention briefly some of the studies which have attempted to relate specific speech characteristics to intelligibility. For an analysis of the specific perceptual effects of clear speech, see also the review in Uchanski (2005).

The increased intensity of forms such as LS naturally explains part of the intelligibility gain over speech produced in quiet, but other aspects are also responsible given that a significant Lombard gain remains even when intensity differences are removed (Dreher and O’Neill, 1957; Summers et al., 1988; Junqua, 1993; Lu and Cooke, 2008). The increase of vocal intensity can affect intelligibility in other ways. Increased vocal effort is accompanied by an increased f
                           0, a flattening of spectral tilt in the medium-high frequency region, by larger mouth and jaw movements, and by a higher F1 frequency (Rostolland, 1982; Schulman, 1989; Titze, 1989; Sundberg and Nordenberg, 2006). While spectral tilt changes can be beneficial (see below), some of the concomitant acoustic modifications are detrimental to segment intelligibility (Pickett, 1956; Rostolland and Parant, 1973).

Shifts in the spectral energy distribution towards the mid-frequency region, as observed in LS, is very effective in enhancing intelligibility in noise (Skowronski and Harris, 2006b; Lu and Cooke, 2009a). However, an artificial boost of speech energy between 1kHz and 3kHz, as observed in the most intelligible talkers, accounts poorly for their intelligibility (Krause and Braida, 2009).

The increased average f
                           0 observed in LS, HIDS and speech produced at a distance does not contribute to the intelligibility benefits brought by these kinds of speech in quiet or in noise (Bond and Moore, 1994; Bradlow et al., 1996; Krause, 2001; Assmann et al., 2002; Hazan and Markham, 2004; Barker and Cooke, 2007; Lu and Cooke, 2009b; Mayo et al., 2012). Synthesis of natural f
                           0 contour variations can improve intelligibility in comparison to a flat contour (Laures and Weismer, 1999; Laures and Bunton, 2003; Watson and Schlauch, 2008). However, the wider f
                           0 range observed in IDS, LS and FDS does not improve infants’ ability to recognize words (Song et al., 2010) and appears not to contribute to the intelligibility benefits of these forms of speech (Krause, 2001; Mayo et al., 2012).

Although clear speech can be produced at a ‘normal’ speech rate, the clear speech intelligibility benefit is modulated by speech rate: increased by a slower speech rate, and decreased by a faster speech rate (Krause and Braida, 2002). Speech rate is one of the modifications of IDS that significantly improve infants’ ability to recognize words (Song et al., 2010) and that may explain some of the prosody-related intelligibility gain of IDS, FDS, HIDS and LS (Mayo et al., 2012). However, the artificial slowing of speech does not lead to significant intelligibility improvements (Schmitt, 1983; Gordon-Salant, 1986; Montgomery and Edge, 1988; Picheny et al., 1989; Uchanski et al., 1996; Nejime and Moore, 1998; Cooke et al., submitted for publication). No systematic correlation has been found between speech rate and the intrinsic intelligibility of speakers (Cox et al., 1987; Bond and Moore, 1994; Bradlow et al., 1996; Hazan and Markham, 2004).

The more frequent insertion of pauses has been related to increased intelligibility (Picheny et al., 1986; Bradlow et al., 2003). However, the artificial insertion of pauses in conversational speech is not an effective speech enhancement technique (Uchanski et al., 1996) and in one study led to reductions in keyword scores for sentence material (Tang and Cooke, 2011), probably due to the disruption of listeners’ expectations.

Based on observations of written English (Shimron, 1993; Lee et al., 2001), consonants are commonly thought to carry more information about sentence intelligibility than vowels. However, apart from (Owren and Cardillo, 2006), it has been found that vowel information enables better recovery of spoken utterances than consonant information (Cole et al., 1996; Kewley-Port et al., 2007; Fogerty and Kewley-Port, 2009; Fogerty et al., 2012). Additionally, the narrowing of formant bandwidths has been related to the intrinsic intelligibility of speakers (Krause and Braida, 2004).

Vowel space expansion in the audio domain and the exaggeration of visible articulatory movements of the lips is related to improved recognition and discrimination of vowels, syllables and words by infants (Liu et al., 2003; Song et al., 2010) and in noise (Gagne et al., 1994, 2002; Helfer, 1997; Beautemps et al., 1999; Ferguson and Kewley-Port, 2002; Rogers et al., 2010). However, the intrinsic intelligibility of a speaker does not appear to be systematically related to greater vowel dispersion (Bond and Moore, 1994; Bradlow et al., 1996; Hazan and Simpson, 2000; Hazan and Markham, 2004). Artificial boosting of the amplitude of F2 and F3 has been found to improve intelligibility in noise for normal hearing listeners, but not for those with hearing impairment (Krause, 2001).

A clear relation has been established between longer VOT and the intelligibility benefits of clear speech at normal speaking rates (Krause and Braida, 2004). However, variations in VOT do not have any effect on the intrinsic intelligibility of speakers (Bond and Moore, 1994) nor on the intelligibility benefits of HIDS (Monsen, 1978; Metz et al., 1985).

Our review of algorithmic modifications to speech follows the taxonomy introduced in Section 2 used to classify human speech modifications. Unsurprisingly perhaps, most of the available algorithms for improving speech intelligibility operate on the speech signal, and therefore, in the main, aim to improve audibility in noise (Section 6.2). There are some examples of modifications at higher levels though, especially in the case of spoken dialogue systems. Before examining the algorithms themselves (Sections 6.2–6.5), we discuss some of the assumptions and constraints which apply to these techniques.

Although sharing the aim of maintaining a target level of intelligibility under challenging conditions, speech modification differs from speech enhancement in that it assumes the existence of a noise-free speech signal. For example, while a considerable amount of work has been done to improve the intelligibility of speech through hearing aids, they of course operate on a speech-and-noise mixture – speech enhancement – so fall out of scope for this review. However, where such algorithms specifically target speech, and could be expected to offer intelligibility gains if applied to clean speech before mixing with noise, we included them (although their performance in this scenario may not have been tested). It must also be noted that methods that improve intelligibility for the hearing-impaired will not necessarily help normal-hearing listeners.

The kind of speech modifications we consider also differ from active noise cancellation approaches in that we assume the noise signal cannot be modified. However, in principle, modifications to the speech signal can be designed to mask information in the masker (e.g., the allocation of energy to the speech signal in spectro-temporal regions whose effect is to mask masker transients). This idea is explored further in Aubanel and Cooke (2013a).

Two constraints are assumed in most modification algorithms. While increasing speech level is included for completeness, in general the modifications of interest are those that, on average, operate without level or loudness increases. Secondly, durational adjustments (e.g., segment lengthening) are possible, but without resulting in an excessive increase in overall duration.

Application of these constraints leads to side-effects and tradeoffs that are not always apparent when considering speech modifications in isolation. For example, boosting formants leads to reduced energy away from formants, and lengthening vowels or inserting pauses requires increases in speech rate elsewhere. More generally, modifications that are designed to be beneficial in noise may distort speech when applied in quiet conditions, and might even reduce intelligibility for those groups (e.g., non-native, hearing-impaired or young listeners) whose performance is already below ceiling in noise-free conditions (see, e.g., Cooke and García Lecumberri (2012)). Our focus in this review is on intelligibility rather than naturalness.

The specific modifications that can be applied to speech will depend on the type of speech itself. For delayed live speech, the range of modifications is limited in practice to those changes that can be applied to short segments of the speech signal (e.g., range compression or spectral filtering). For pre-recorded natural speech, content and style information can be extracted offline, allowing for a greater range of modifications at deployment time (e.g., vowel space expansion, f
                           0 range expansion). For speech generated from text, a full spectrum of modifications is available, including high-level modifications such as choice of syntactic form or lexical substitution. The boundaries between live-recorded and recorded-synthetic are not always clear cut. For instance, if sufficient delay is permitted, higher-level linguistic information could in principle be extracted from live speech. Similarly, lexical substitutions might be considered for pre-recorded natural speech by generating words as a result of adapting synthesis models on fragments of the pre-recorded speech.

Knowing something about the context (e.g., masker or target listener) can, in principle, permit the use of modifications that are tailored to the specific masking pattern or known listener deficits. For some application domains there is a realistic prospect of acquiring detailed noise estimates, either online or offline (e.g., transport-related noise in railway stations where the output devices are fixed) while for others the options are more limited (e.g., environmental noise for mobile devices). Modification candidates differ in the degree of detail required in estimates of context. For instance, for masking noise, this might range from high resolution spectral information in successive time-frames through long-term masker spectral profiles to complete noise-independence. For target listeners, variables such as first language or age might be required to tune a given type of modification.

This approach to improving intelligibility is the most obvious and the methods in this category are some of the simplest to implement, with many being possible in real time with low latency.

Global increases in intensity may be placed under the listener's control, or may be made automatically. When the attack and decay times of dynamic amplitude compression (Section 6.2.5) are very long (seconds rather than milliseconds), it may be known as automatic gain control (AGC) and can be widely found on personal music players and in-car audio systems to even-out the volume of different material or to adjust the volume according to an external control signal. Thus, AGC can be made noise-dependent, with the amount of intensity increase being set in accordance with ambient noise, or directly from another source, such as a vehicle's speed. This approach may be employed on mobile phones, enabling the speech received to remain audible to the listener as the noise environment changes. Whilst this does reduce the dynamic range of the signal, AGC is better categorised as producing relatively slow adjustments to overall intensity rather than rapid short-term modifications.

The system described in Patel et al. (2006) modified the amplitude of salient words (by which they mean content words) using a fixed increase of 4dB. Although this was motivated by experimental evidence from human speech, no experimental results are provided as to the effectiveness of this modification on the intelligibility of the synthetic speech.

Simple high-pass filtering will decrease spectral tilt. If this is followed by energy normalisation back to the original signal energy, this also effectively boosts transients (Section 6.2.7) because low-frequency energy is reduced.


                           Zorilă et al. (2012) uses spectral shaping that includes a decrease in spectral tilt to improve the intelligibility of speech in noise. This is then combined with dynamic range compression (DRC – Section 6.2.5). Combining spectral shaping with DRC (and, crucially, in that order) makes a lot of sense, since in unmodified speech the bulk of the signal energy is in the low frequencies and this consumes much of the available headroom when boosting the amplitude of the waveform. This has long been recognised in music production, where it is common to split signals into several non-overlapping frequency bands and to apply DRC (known there as compression) to each band separately (and with different thresholds and ratios), before re-combining them. Similarly, high-pass filtering is a standard procedure in audio recording, production and broadcast, removing those low frequency components that would consume headroom (of either the transmission channel, storage medium, or reproduction apparatus) but contribute little to perceived quality.

While hearing aid processing in general is out of scope of this paper, the findings reported in Davis et al. (1946) are still of interest. It is noted that patients’ preferred settings for quality are not the same as those that give maximum intelligibility. This is relevant to the design of intelligibility-boosting algorithms because it implies that some loss of quality may reasonably be incurred when maximising intelligibility gain. Encouragingly, Davis et al. (1946) also found that there were two simple strategies that worked for everyone, independent of patterns of individual hearing loss: a uniform boost (i.e., the increasing intensity strategy), or a 6dB/octave high-pass filter (i.e., reducing spectral tilt). However, the evidence is not completely clear, with Horwitz et al. (1991) finding no significant differences for high/low frequency boosts in hearing aids.

As Niederjohn and Grotelueschen (1976) point out, even simple high-pass filtering effectively boosts the second formant relative to the first formant, thus enhancing intelligibility due to the greater importance of F2 frequency. More targeted, specific formant enhancement of course requires reasonably accurate estimates of formant frequencies, which will always be prone to error. Real-time formant peak sharpening is described in Blamey et al. (1993) where it is tested on hearing-impaired listeners, producing small gains in intelligibility. Enhancement is performed using variable-centre-frequency bandpass filters, and it is noted that choosing the bandwidth involves a compromise between wide filters offering little enhancement versus narrow filters that can lead to sudden amplitude variation as a consequence of harmonic peaks moving in and out of bandwidth (and we presume, although not mentioned in the paper, also because of errors in formant tracking). Following on from Blamey et al. (1993), the study by Alcántara et al. (1994) demonstrates modest benefits to normal hearing listeners too in the presence of multi-talker babble. The benefits were only for vowel perception.

Although there do not appear to be algorithms that deliberately sparsify speech signals, the vocoders used in speech coding and statistical parametric speech synthesis do this unintentionally: fine detail is removed, harmonic structure may be over-emphasised and useful redundancy is lost through the processes of spectral envelope estimation and modelling. It seems unlikely that this has positive benefits on intelligibility and, indeed, it is plausible that it is this lack of redundancy that makes synthetic speech intelligibility degrade much more rapidly than that of natural speech as SNR worsens. One form of sparsification in which speech is resynthesised using a very simple two-formant filter was proposed by Blamey et al. (1993), who suggest that there could be benefits for listeners with poor frequency resolution, although this was not tested.

Intriguingly, an attempt to find optimal static spectral weightings (Tang and Cooke, 2012), described below (Section 6.2.9), did produce sparse representations which improved speech intelligibility by around 2dB in a large-scale evaluation (Cooke et al., 2013b).

Depending on the application and the attack and decay times employed, dynamic amplitude compression (or dynamic range compression – DRC) goes under a variety of names. Slow-response systems are commonly known as automatic gain control. In audio and especially music production, with generally fast (1–10ms) attack times, it is widely called ‘compression’ or occasionally ‘companding’.

Both an AGC, which aims to provide a standard speech level, and an automatic level compensator (ALC), which modifies the speech level in response to ambient noise level, are used in the ‘interphone’ (headset-based communication for face-to-face situations with extremely high ambient noise levels, such as inside a noisy military vehicle) described by Torick and Allen (1966). The AGC reduces the dynamic range of the amplitude of the speaker's speech to compensate for variations in their speech – a simple form of speech modification – whilst the ALC promotes audibility in the listener's varying noise conditions – a simple form of listener-dependent behaviour.

Reducing the dynamic range of signals is ubiquitous in broadcast and recorded media production, for both music and speech. In such a compressor, when the level of the input signal exceeds a user-defined threshold, its amplitude starts to be attenuated at a user-defined ratio. Inertia is applied to the attenuation control via controllable attack and release times.

One problem with all forms of dynamic amplitude compression is that any background noise present in speech pauses will be boosted in amplitude. Short attack and long release times can mitigate this, but very short attack times mean that transients will be eliminated. For speech signals this can lead to the loss of initial consonants: Torick and Allen (1966) suggest that users say an ‘expendable word’ at the start of each speech period! Modern systems are able to employ small delays to implement lookahead in order to avoid this mistreatment of transients. Actively boosting transients has been used to increase intelligibility (Section 6.2.7). As described in Section 6.2.2, Zorilă et al. (2012) use spectral shaping followed by dynamic range compression to improve the intelligibility of speech in noise.

Extreme forms of dynamic amplitude compression can be found. With the threshold set just below the clipping level and a very high compression ratio, one obtains a ‘brick-wall limiter’. This is essentially what is used in Niederjohn and Grotelueschen (1976) (their Figure 2). If too much gain is applied, the waveform becomes clipped (in either analogue or digital domains). It has been found that this may slightly enhance intelligibility in some circumstances (Pollack and Pickett, 1959) although Kretsinger and Young (1960) shows that a fast limiter is more effective than clipping.

There are many methods for modifying the fundamental frequency of speech, either directly in the time domain (e.g., using TD-PSOLA (Moulines and Charpentier, 1990)), or on a coded representation of speech, as in Dudley's vocoder (Dudley, 1939), or using one of many modern methods such as harmonic-plus-noise models (Stylianou et al., 1995) or the STRAIGHT vocoder (Kawahara et al., 1999). Whilst these methods are widely employed to manipulate f
                           0 for the purposes of prosodic modifications, few automatic systems have the express intention of increasing intelligibility. A number of investigations into the effects of f
                           0 modification (Lu and Cooke, 2009b; Valentini-Botinhao et al., 2011) have found no benefits. One study (Patel et al., 2006), already mentioned in Section 6.2.1, modified the f
                           0 of content words using a fixed increase of 20Hz. However, its effectiveness was not evaluated.

Recently, Villegas and Cooke (2012) modified speech by seeking the change in f
                           0 which optimised a ‘glimpse’ proportion measure of energetic masking (Cooke, 2006). In general, the outcome was a reduction in f
                           0, probably caused by the increased number of resolved harmonics that such a change produces. While the objective intelligibility measure predicted a modest gain, a later listener-based evaluation (Cooke et al., 2013a) demonstrated modest losses in actual intelligibility.


                           Rasetshwane et al. (2011) found transient-boosting methods can improve intelligibility of speech in noise (recordings of an aircraft auxiliary power unit) presented at various SNRs from −30dB to −10dB, and that gains were still seen when combining this with active noise-cancelling headphones. Although the best-performing method was inspired by a dynamic time-varying filter approach, in order to obtain real-time performance this was approximated as a simple fixed filter, which amounts to attenuation below 700Hz and mid- and high-frequency boosting. So, although the method does boost transients, as any high-pass filtering would, it is not actually detecting them in the speech. A more sophisticated method in the same study using wavelet decomposition of the speech – which did target transient regions more specifically than the fixed filter – offered lesser gains. Distortions introduced by that method may have counteracted the benefits of transient boosting. From the results available in Rasetshwane et al. (2011) and from simple spectral shaping, it is not possible to separate out the effects of a general high-frequency boost from specific localised transient boosting. The investigation reported in Yoo et al. (2007) also found that increasing the intensity of transients relative to steady-state regions increased intelligibility.

Pre-processing of the speech signal by steady-state suppression has been attempted in order to reduce the smearing of energetic speech parts (e.g., vowels) into subsequent segments, which results in forward masking in reverberant environments. However, a modulation filtering technique operating at a syllable rate produced no clear intelligibility benefit (Arai et al., 2002; Kusumoto et al., 2005).

Spectral profile modification is a generalisation of high-pass filtering, spectral tilt or centre of gravity changes, and is typically the outcome of an optimisation procedure based on avoiding energetic masking by a known noise. The response obtained during optimisation depends, of course, on the spectral resolution. In a relatively low-resolution approach, Taal et al. (2013) sought the optimal linear time-invariant filter which maximised an approximation to the Speech Intelligibility Index (ANSI S3.5-1997, 1997). Another low-resolution technique (Petkov et al., 2012) reallocated energy by modifying the speech with a simple filterbank, adjusting the filter gains to maximise intelligibility as measured using automatic speech recognition. A study by Tang and Cooke (2012) also sought a static spectral filter but at a much higher resolution based on a 55-channel gammatone auditory filterbank. Their optimisation process employed a genetic algorithm which attempted to maximise the glimpse proportion (Cooke, 2006) across a corpus of speech, independently for different maskers and SNRs. A striking outcome was the finding, hinted at earlier, that as the SNR decreases, the optimal filter became increasingly sparse, focusing the boosting of energy in 3 or 4 widely separated frequency bands.

Energy reallocation is more general still, denoting techniques that shift energy in both frequency and time. A study by Tang and Cooke (2010) compares various manually designed energy reallocation strategies and found that boosting the SNR of selected frequency bands gave the largest gains but also tended to reduce the speech quality the most (both measured only with objective measures). As in the method of Sauert and Vary (2006), it is most effective to carefully choose to improve the SNR only for those regions where improvement is possible: that is, to avoid processing clean speech or speech that is much lower amplitude than the competing noise signal such that no amount of boosting will improve its audibility.

In Taal et al. (2012), energy is reallocated in energy over time and frequency guided by a spectro-temporal auditory model. The temporal aspects of the processing result in transient enhancement (see Section 6.2.7). Improvements in intelligibility are claimed, although only objective measures are presented.

In Skowronski and Harris (2006b), energy was reallocated across time only, from voiced regions to unvoiced regions, under a constant energy constraint (per word), and compared with a high-pass filter that reallocated energy across frequency only. The intelligibility of 9 out of the 16 talkers used in their listening test was improved when using one or other of these methods, with the energy reallocation across time providing improvements for 7 out of 16 and the high-pass filter improving 6 out of 16. All experiments used white Gaussian noise at −10dB SNR.

The approach of Valentini-Botinhao et al. (2012) reallocates energy within frames via manipulation of a cepstral representation of the spectral envelope, in order to optimise a computational model of intelligibility based on the number of glimpses of the speech signal (Cooke, 2006).

While there have been numerous laboratory studies within the domain of auditory scene analysis which demonstrate the value – or otherwise – of specific organisational cues such as those which encode fundamental frequency (see Darwin, 2008 for a review), there have been surprisingly few explicit engineering attempts to modify speech with the aim of increasing coherence, perhaps due to the paucity of observations of coherence-inducing modifications in speech production.

We speculate that synthetic speech generated using waveform concatenation has impaired coherence due to the use of recorded units in mismatching contexts, and that this could be one reason that this type of synthetic speech is typically less intelligible than natural speech whereas synthetic speech generated using statistical models driving a vocoder can be as intelligible as natural speech (Yamagishi et al., 2008), even under certain additive noise conditions (Suni et al., 2010; Cooke et al., 2013a).

While in practice it is straightforward for a text-to-speech system to enhance salient linguistic information (e.g., via lexical repetition or adding references to previous information) very few studies have been carried out to explore the benefits of this class of modification. For instance, it is common in limited-domain automatic speech recognition systems, where the application design permits, to select a word vocabulary that minimises recognition errors (Pucher et al., 2007). This can be achieved by choosing words that are less confusable (Cox and Vinagre, 2004) or that are robust to speaker effects such as disfluency or prosodic variation (Goldwater et al., 2010). This could easily be applied in the case of speech output systems.


                           Moore and Nicolao (2011) used conventional model adaptation methods from HMM-based speech synthesis to modify vowels, making them either more (hypo-articulated) or less (hyper-articulated) close to the neutral vowel. Intelligibility (measured objectively using SII and not in a listening test) was successfully improved for the hyper-articulated case.

As with f
                           0 modifications, many algorithms are available for modifying speech rate and pause durations, globally or at a finer level (Moulines and Charpentier, 1990; Stylianou et al., 1995; Kawahara et al., 1999). However, the evidence for possible benefits is mixed, as reviewed earlier (Section 5.2.4). One system (Aubanel and Cooke, 2013a) that did produce clear benefits – more than 4dB – in the presence of a fluctuating masker used durational expansion to shift salient speech information (defined by the Cochlear-scaled Spectral Entropy metric; Stilp and Kluender (2010)) in time to avoid epochs of more intense noise. However, the same durational changes did not produce gains when mixed with a stationary masker, suggesting that noise avoidance rather than slower speech rate was responsible for the benefits.


                           Tang and Cooke (2011) explored the insertion of pauses at word boundaries in order to avoid intense masker epochs, under a constant-duration constraint which meant that speech rate was increased to accommodate pause insertion. However, this led to a significant reduction in intelligibility, probably due to a reduction in predictability of word boundaries in noise.

Natural language generation systems can adjust the length and complexity of the sentences they produce, to target certain listener types. This is a large field and we do not attempt a survey of it here. As two illustrative examples, Janarthanam and Lemon (2010) manipulated the complexity of referring expressions to suit different end users and Rieser et al. (2011) showed that more complex sentences (containing more information) led to better task completion.

One study that did take attempt to enhance linguistically relevant information was in the system described by Patel et al. (2006), where the simple modifications to f
                           0, amplitude and duration performed were applied only to content words, thus emphasising the main information bearing elements of the sentence. No detailed evaluation was provided to demonstrate whether this is effective although a 7% intelligibility benefit is mentioned.

@&#SUMMARY@&#


                     Table 2
                      catalogues the 46 speech modifications which have emerged during the course of this review, identifying in each case whether it has been observed in a talker's speech production, whether it forms the basis for a speech modification algorithms, and finally if – to the best of our knowledge – it is beneficial to listeners.

One striking feature evident from this catalogue is lack of behavioural studies on possible benefits of certain types of modification, especially those at the levels of cognitive effort. Cognitive load is known to affect both speech perception (Mattys et al., 2009; Mattys and Wiget, 2011) – where, for instance, it changes the balance between the roles of acoustic and lexical factors in word segmentation – and speech production (e.g., Farris et al., 2008; Estival and Molesworth, 2009). See Mattys et al. (2012) for a recent review of the effect of adverse conditions, including cognitive load, on speech perception.

The absence of speech modification algorithms aimed at increasing coherence has already been noted. In fact, examination of the auditory scene analysis literature suggests many possible avenues for making speech more robust in the presence of competing sources. For instance, contrasting mean intensity between a target and background speech signal might help a listener's scene analysis task, even when this places the target speech at a negative SNR Brungart (2001). Similarly, there have been many studies of the benefit of f
                     0 differences in speech-on-speech mixtures (see Bird and Darwin, 1998; Assmann, 1999; Summers et al., 2010) and of the value of spatial separation between target and masker (see Bronkhorst and Plomp, 1988; Hawley et al., 2004). Note that in some of these cases good estimates of masker properties will be required.

A further task for future studies is to incorporate higher levels of linguistic information into modified speech, something which requires access to the intended message and is therefore limited in practice to text-to-speech systems. Obvious approaches in dialog systems include the use of repetition, filled pauses and back-channels as well as the choice of words known to resist masking.

The speech modification approaches listed in Table 2 focus on modifying speech to promote its intelligibility or to decrease the cognitive effort required to process it. However, speech modifications could, in principle, be used to reduce the detrimental effect of a masker. This might be achieved by reallocating excess speech energy in time and frequency in order to ‘mask the masker’, focusing on masker transients or, in the case of speech maskers, those epochs estimated to convey salient information.

It is tempting to use the proposals listed in Table 2 as a menu from which arbitrary combinations can be selected. Indeed, positive results might be expected from the combination of spectral and temporal approaches, with additional benefits perhaps deriving from modifications targeted at the message level. However, further studies are needed on possible antagonistic effects of combining multiple modification methods. For example, while dynamic amplitude compression has proved to be a successful technique, applying it alongside other temporal energy reallocation methods which operate by unmasking weaker signal epochs may be counter-productive. Likewise, combinations that are not articulatorily coherent, such as increasing vocal effort and decreasing F1, might have negative effects on intelligibility as they may sound unnatural to a listener and consequently lead to attentional disturbance.

Finally, we note that since in many listening scenarios adequate intelligibility is maintained simply by increasing output level, the purpose of speech modification is often considered to reside in using any gain in dB to resist this tactic, i.e., ‘turning down the volume’. However, the headroom gained by the modification can be spent in other ways. For example, instead of reducing output level, information rate might be increased by speeding up speech, or the need for repetition (in the case of public address systems) might be reduced.

@&#ACKNOWLEDGEMENTS@&#

The authors thank the EU Future and Emerging Technology (FET-OPEN) project the ‘Listening Talker’ for supporting the ideas which led to the preparation of this manuscript.

@&#REFERENCES@&#

