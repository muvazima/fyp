@&#MAIN-TITLE@&#Multilingual event extraction for epidemic detection

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           We present DANIEL, a multilingual system for tele-epidemiology.


                        
                        
                           
                           Classical approaches use language-dependent resources, which limits coverage.


                        
                        
                           
                           Our system uses very few resources and performs consistently on any language.


                        
                        
                           
                           It has therefore worldwide coverage and better reactivity than the state-of-the-art.


                        
                        
                           
                           Readily dealing with any language implies faster epidemic detection.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Early event detection

Poorly endowed languages

Multilingual information access

Tele-epidemiology

Epidemic surveillance

@&#ABSTRACT@&#


               
               
                  Objective
                  This paper presents a multilingual news surveillance system applied to tele-epidemiology. It has been shown that multilingual approaches improve timeliness in detection of epidemic events across the globe, eliminating the wait for local news to be translated into major languages. We present here a system to extract epidemic events in potentially any language, provided a Wikipedia seed for common disease names exists.
               
               
                  Methods
                  The Daniel system presented herein relies on properties that are common to news writing (the journalistic genre), the most useful being repetition and saliency. Wikipedia is used to screen common disease names to be matched with repeated characters strings. Language variations, such as declensions, are handled by processing text at the character-level, rather than at the word level. This additionally makes it possible to handle various writing systems in a similar fashion.
               
               
                  Material
                  As no multilingual ground truth existed to evaluate the Daniel system, we built a multilingual corpus from the Web, and collected annotations from native speakers of Chinese, English, Greek, Polish and Russian, with no connection or interest in the Daniel system. This data set is available online freely, and can be used for the evaluation of other event extraction systems.
               
               
                  Results
                  Experiments for 5 languages out of 17 tested are detailed in this paper: Chinese, English, Greek, Polish and Russian. The Daniel system achieves an average F-measure of 82% in these 5 languages. It reaches 87% on BEcorpus, the state-of-the-art corpus in English, slightly below top-performing systems, which are tailored with numerous language-specific resources. The consistent performance of Daniel on multiple languages is an important contribution to the reactivity and the coverage of epidemiological event detection systems.
               
               
                  Conclusions
                  Most event extraction systems rely on extensive resources that are language-specific. While their sophistication induces excellent results (over 90% precision and recall), it restricts their coverage in terms of languages and geographic areas. In contrast, in order to detect epidemic events in any language, the Daniel system only requires a list of a few hundreds of disease names and locations, which can actually be acquired automatically. The system can perform consistently well on any language, with precision and recall around 82% on average, according to this paper's evaluation. Daniel's character-based approach is especially interesting for morphologically-rich and low-resourced languages. The lack of resources to be exploited and the state of the art string matching algorithms imply that Daniel can process thousands of documents per minute on a simple laptop. In the context of epidemic surveillance, reactivity and geographic coverage are of primary importance, since no one knows where the next event will strike, and therefore in what vernacular language it will first be reported. By being able to process any language, the Daniel system offers unique coverage for poorly endowed languages, and can complete state of the art techniques for major languages.
               
            

@&#INTRODUCTION@&#

Information extraction (IE) aims at extracting structured views from text and particularly from newswires that provide instant information from a large number of sources. The European Media Monitor for instance collects about 40,000 news reports written in 43 languages everyday.
                        1
                     
                     
                        1
                        European Media Monitor: http://emm.newsbrief.eu/overview.html [accessed 20.04.15].
                      This information context provides a new opportunity for health authorities, needing to monitor information, placing emphasis on disease outbreaks and spreadings [1].

However, natural language processing historically puts a very strong emphasis on vocabulary and on differences between languages, to the extent computational models heavily rely on the constitution of lexical resources. Special effort has been exerted to collect specialized medical lexica. Therefore, although web news is available in a large number of languages and dialects, the standard pipeline in IE is designed for texts in standard English, with the need to add lexicon and special components (lemmatizer, parser) each time a new language is added. Meanwhile, disease outbreaks ignore national frontiers and when considering epidemiological event extraction (EE), one has to detect diseases from health-related news in many languages to send alerts to health authorities as quickly as possible [2].

Keller has compared existing systems [3] stressing their complementarity. In the same way, the Data Analysis for Information Extraction in any Language (Daniel) system fulfills part of the needs but not all. The strong points advocated here are quick access to new languages, very light programming needed and timeliness in IE [4]. It is also important to get a leveraged epidemiological EE, so as to detect events from multilingual sources both at the same pace and with similar reliability. Since no multilingual corpus was available for comparison with existing systems, a news corpus has been collected and made available for further tests. The Daniel system is a text-genre based EE system designed to manage multilingual news with a large geographical coverage. Multilingual IE with light resources was tested, in order to quickly detect news denoting concern about some disease. Here, the standard approach to text as a bag of words is replaced by a spatial vision of text. Three characteristics are combined to avoid the chore of constituting heavy resources for all languages. A strong hypothesis assumes the constraints of information and dissemination are common to all news writers, and that journalistic genre implies a common use of titles, headers, bodies and feet, whatever the language. The common structure in news is the rhetorical “spatial” basis for the proposed model. Information is found at a specific place. A similar notion is sometimes used in academic literature analysis [5]. The second characteristic is the implicit use of discourse “time”, a.k.a. narrative line in news, with some typical repetitions along the way. The third characteristic is the use of the news date, linking the event to a given time window in conjunction with a geographical location and a disease. Since the system fills a gap in epidemiological monitoring, experiments were conducted on a multilingual corpus of 17 languages. It was manually annotated for 5 of them (Chinese, English, Greek, Polish and Russian). Precision and recall are computed for document wise and event wise detection. The question is how to compare a light resource system aiming at a wide coverage, while everyone is deeply involved in enriching resources and improving results for a very few number of languages. Whenever possible, results are compared with existing systems, or on common corpora.

The present paper is organized as follows. In Section 2, an overview of the multilingual approaches in IE is provided along with proposals to overcome shortcomings in early detection of diseases. In Section 3, we introduce the Daniel system, a text-genre based EE system designed to manage multilingual news. Section 4 introduces the evaluation corpus that we collected for the experiments. In Section 5 the results are presented and discussed. Finally, the efficiency of such a light approach for filtering huge multilingual news feeds is discussed and future directions are sketched in Section 6.

@&#BACKGROUND@&#

IE approaches rely mostly on the use of the generic IE chain [6]. Two systems that rely primarily on English, Puls
                     
                        2
                     
                     
                        2
                        
                           http://puls.cs.helsinki.fi/static/index.html (Accessed: 20 April 2015)
                      
                     [7] and Biocaster
                     
                        3
                     
                     
                        3
                        
                           https://sites.google.com/site/nhcollier/projects/biocaster (Accessed: 20 April 2015)
                      
                     [8], are well-known examples of classic IE systems specializing in epidemiological EE with good results in English and a few other languages. HealthMap
                        4
                     
                     
                        4
                        
                           http://www.healthmap.org/ (Accessed: 20 April 2015)
                      
                     [9] is another well-known example, with the additional feature that it incorporates information manually compiled by human experts. The IE processing chain involves numerous components for each language. Extending the coverage for such a system requires components corresponding to a new language to be gathered. For most languages, the necessary efficient components are lacking [2]. In recent years, machine learning has been used successfully to fill gaps in new languages that have a sufficient number of common properties with a mainstream language [10].

However, in epidemic surveillance, there is a need to cover poorly endowed languages [11] or even dialects without training data. In a multilingual setting, state-of-the-art systems are limited by the cumulative process of their language-by-language approach. A multilingual goal hardly can be fulfilled with classical monolingual approaches. This is particularly the case for highly inflected languages [12]. Despite the sequential aspect of the classic IE chain, a decomposition problem arises: a high marginal cost is needed for analyzing any new language. The detection and appropriate analysis of the very first news report relating to an epidemic event is crucial for timeliness [13], but it may occur in any language: usually the first language of description is that of the (remote) place where the event was located [11]. For these reasons, a recent assumption from studies on media rhetorical devices [14] was put to trial: expository news shows specific patterns of repetition (the main content is given first, then detailed). Interesting findings have been heralded in the past, concerning the distribution of proper names in breaking news [15]. The contrast with “ordinary news” has also been used to extract outburst events [8]. The underlying idea is referred to as pragmatics, or is altogether implicit when no specific knowledge backs the findings. Since explicit knowledge is used in our system, it exploits style properties identified in news discourse. Lejeune et al. [16] introduced genre and discourse properties for EE. Liao et al. also advocated text level inference to improve EE though with a monolingual constraint [17,18]. The approach presented here relies on journalists writing principles: repetition of important terms at salient positions, clarity of style and exploitation of the notion of a model reader (each piece of information does not have to be written explicitly since journalists make the assumption that readers can fill in the blanks). This approach leverages the unique role of structure and rhetorical principles commonly used by journalists (the inverted-pyramid style by Piskorski et al. [19]).

The Daniel system presents an implementation of a discourse-level EE approach. It operates at discourse-level by exploiting the global structure of news in a newswire. It harnesses information ordering as defined by Lucas [14], as opposed to the usual analysis at sentence-level (morphology, syntax and semantics). Entries in the system are news texts, including their title and text-body, and the name of the source when available. The only structural information needed are the positions of the head and the body of the news (from metadata such as RDF/microformat, or extracted with a boilerplate removal tool). The main features of the Daniel system are that it is character-based and that it uses positions of occurrences [20]. Character-based refers to the fact that text is handled as a sequence of characters rather than as a sequence of words, in order to consider all types of languages (even if the definition and delimitation of words are difficult). The descriptors used are not key words but strings of text, exploited if and only if they are repeated in pre-defined salient zones in text. The aim of the process is to extract epidemic events from news feed, and express them in the reduced form of disease-location pairs (i.e. what disease occurs where).


                     Fig. 1
                      describes the steps of the process to detect whether a document describes an epidemic event. The Daniel processing pipeline is composed of three steps: news article segmentation (Section 3.1), event detection (Section 3.3.1), event localization (Section 3.3.2) using a small knowledge base (Section 3.2.3) and substring patterns (motifs, Section 3.2).

The main algorithm exploits properties of the news genre. In a genre-driven approach, a clear understanding of text construction is crucial. The beginning and the end of a news text make up its salient zones. The system zooms in on the title and beginning (the topical head) of a text, and ceases elements that are repeated in these salient positions. The length of the text will determine the interactive relationship between salient zones. Rules reflecting these relationships are described in Table 1
                        . These rules are simple to implement and largely language-independent. Salient zones -Head (title and first paragraph) Tail (last two paragraphs), and Body (the whole article except the Head)- combine for effect. The system will thus extract the substrings found in both Head and Body when confronting medium articles, and in Head and Tail in long articles. For short articles, repeated substrings are exploited irrespective of their position (the beginning overlaps the end, so the whole text is considered salient).

To find text string repetitions in the aforementioned article segments, character level analysis is performed by computing non-gapped character strings as described by Ukkonen [21]. Usually exploited in bioinformatics, where gigabytes of data are processed, this algorithm allows fast access to relevant patterns. This section formally defines motif extraction from text, followed by a demonstration using a sample document from our evaluation corpus.

Motifs are substring patterns of text with the following characteristics: they are repeated (motifs occur twice or more) and they are maximal (motifs cannot be expanded to the left (left maximality) or to the right (right maximality) without lowering the frequency). Following the example of Ukkonen [21], the motifs found in the string HATTIVATTIAA are T, A and ATTI. However, TT is not a motif because it always occurs inside each occurrence of ATTI. In other words, its right-context is always I and its left-context A. All the motifs in a set of strings can be efficiently enumerated using an Augmented Suffix Array [22] (also called Enhanced Suffix Array).

Given two strings 
                              
                                 S
                                 0
                              
                              =
                            
                           HATTIV and 
                              
                                 S
                                 1
                              
                              =
                            
                           ATTIAA, Table 2
                            shows the augmented suffix array of 
                              S
                              =
                              
                                 S
                                 0
                              
                              .
                              
                                 $
                                 1
                              
                              .
                              
                                 S
                                 1
                              
                              .
                              
                                 $
                                 0
                              
                           , where $0 and $1 are lexicographically lower than any character in the alphabet Σ and $0
                           <$
                           1.

The augmented suffix array consists in the list of suffixes sorted lexicographically of 
                              S
                            (SA), together with the length of the longest common prefix (LCP) between each two suffixes in SA (
                              
                                 LCP
                                 i
                              
                              =
                              lcp
                              (
                              S
                              [
                              
                                 SA
                                 i
                              
                              ]
                              .
                              .
                              .
                              S
                              [
                              n
                              −
                              1
                              ]
                              ,
                              S
                              [
                              
                                 SA
                                 
                                    i
                                    +
                                    1
                                 
                              
                              ]
                              .
                              .
                              .
                              S
                              [
                              n
                              −
                              1
                              ]
                              )
                            and LCP
                           
                              n−1
                           =0, n the size of 
                              S
                           ).

The LCP allows for the detection of repetitions. The substring ATTI occurs for example in 
                              S
                            at the offsets (1, 13), according to LCP
                           4 in Table 2. The process enumerates all the repeated substrings by reading through LCP:
                              
                                 •
                                 if LCP
                                    
                                       i
                                    
                                    <
                                    LCP
                                    
                                       i+1: open a potential motif occurring at the offset SA
                                    
                                       i+1;

if LCP
                                    
                                       i
                                    
                                    >
                                    LCP
                                    
                                       i+1: close motifs previously created;

if LCP
                                    
                                       i
                                    
                                    =
                                    LCP
                                    
                                       i+1: valid motifs with the offset SA
                                    
                                       i+1.

The maximal criterion is met when a motif is closed during the enumeration process. Two different potential motifs are equivalent if the last character of these motifs occurs at the same offset. For example, TTI is equivalent to ATTI because the last characters of these two motifs occur at the offsets (4, 10) (these substrings are in a relation of occurrence-equivalence according to Ukkonen [21]). In this case, ATTI is held as a maximal motif, because it is the longest of its equivalents. The others motifs A and T are maximal because their contexts differ in different occurrences. All repetitions across different strings are detected at the end of the enumeration by mapping the offsets in 
                              S
                            with those in 
                              
                                 S
                                 0
                              
                            and 
                              
                                 S
                                 1
                              
                           . This way, any repetition detected in 
                              S
                            can be located in any of the strings 
                              
                                 S
                                 i
                              
                           . SA and LCP are constructed in time-complexity O(n) as described by Kärkkäinen and Sanders [22], while the enumeration process is done in O(k), with k defined as the number of motifs and k
                           <
                           n 
                           [21].
                              5
                           
                           
                              5
                              Code in Python: http://code.google.com/p/py-rstr-max/ [accessed 20.04.15].
                           
                        

An example from a news article in Polish is given in Fig. 2
                            to highlight the value of the process described here above.

This document deals with a case of dengue in Thailand. We will focus on two sentences extracted from this document, 
                              
                                 S
                                 0
                              
                            and 
                              
                                 S
                                 1
                              
                           :
                              
                                 
                                    
                                       
                                          S
                                          0
                                       
                                    :
                                 Tajlandzki rząd ostrzega kobiety przed noszeniem czarnych legginsów, gdyż ciemne kolory przyciągają komary, przenoszące dengę.


                                    [Thai government warns women against wearing black leggings, because dark colors attract mosquitoes carrying dengue.]
                                 

W tym roku w Tajlandii odnotowano ponad 45 tys. przypadków dengi, czyli o 40% więcej niż w ubiegłym roku.


                                    [This year, in Thailand, there were more than 45,000 cases of dengue fever, up 40% from last year.]
                                 

A word-based repetition detection would fail to find similarities between dengę and dengi, as well as between Tajlandzki and Tajlandii. The motif detection focuses on the detection of subpatterns of diseases names, here on the detection of the roots: deng∼ and Tajland∼. Table 3
                            shows a selected sample of the augmented suffix array of the two text fragments 
                              
                                 S
                                 0
                              
                            and 
                              
                                 S
                                 1
                              
                           .

A repetition of length 4 (LCP
                           71) is detected at the offsets (120, 186): deng. Another repetition, Tajland, is detected at the offsets (0, 140). The maximal criterion consists in verifying that these substrings are strictly included in another at each offset where they occur. _deng is actually extracted rather than deng because the left context of deng is always a white space.

Daniel relies on implicit knowledge on the news genre, which allows it to use only light lexical resources automatically collected from Wikipedia with light human moderation to pinpoint relevant information. To integrate a new language, the adequate lexicon of disease names and geographical locations (countries) are the only resources needed. Those are built through a crawl of the Wikipedia using the following procedure:
                              
                                 1.
                                 Crawl the Wikipedia English list of infectious diseases
                                       6
                                    
                                    
                                       6
                                       
                                          http://en.wikipedia.org/wiki/List_of_infectious_diseases [accessed 20.04.15].
                                     then fetch each outgoing link, for instance, the “smallpox”
                                       7
                                    
                                    
                                       7
                                       
                                          http://en.wikipedia.org/wiki/Smallpox [accessed 20.04.15].
                                     page.

For each English Wikipedia disease page, capture the interlingual outgoing links and the corresponding (language code; disease name) pairs. For instance, on the smallpox page, one of these interlingual outgoing links is http://hu.wikipedia.org/wiki/Fekete_himlő (Accessed: 20 April 2015), where hu is the language code, and Fekete himlő is the disease name.

Finally, for each language, construct the disease lexicon from the collected pairs. For instance, to build the Hungarian disease lexicon, we need to collect all the pairs corresponding to the language code hu.

The exact same procedure is used to build the lexicon of locations, with the exception that the initial Wikipedia page is the list of sovereign states.
                              8
                           
                           
                              8
                              
                                 http://en.wikipedia.org/wiki/List_of_sovereign_states [accessed 20.04.15].
                            Finally, the extracted lexicon contains disease names and geographical locations (countries). The lexicon needed with our genre-based system is small: hundreds of items versus tens of thousands in state-of-the-art systems based on linguistic knowledge [23]. The Web-extracted disease names make it possible to deal quickly with new languages, even without the assistance of a native speaker.

In practice the lexica of disease names and locations is used in a very direct way. An interesting text substring is defined by at least 3 occurrences: two in the document (in salient positions) and one in the lexicon. Hence, motif extraction is performed on articles combined with the external knowledge. Let 
                           
                              S
                              2
                           
                         and 
                           
                              S
                              3
                           
                         items of a lexicon to be analysed according to 
                           
                              S
                              0
                           
                         and 
                           
                              S
                              1
                           
                        :
                           
                              
                                 
                                    
                                       S
                                       2
                                    
                                 :
                              Tajlanda [Thailand]
                              

denga [denge]
                              

With 
                           
                              S
                              0
                           
                        , 
                           
                              S
                              1
                           
                         (two segments of a document) and 
                           
                              S
                              2
                           
                        , 
                           
                              S
                              3
                           
                         (two items in an external knowledge base), the augmented suffix array makes it possible to detect repetition between selected parts of a document and any resources a system might need. Table 4
                         shows a sample of this augmented suffix array.

Note that the addition of the lexica allows for sharper extraction. In the example, the detected motif is deng, when with the document alone, the extracted motif was _deng. In the string “
                           
                              S
                              0
                           
                           
                              $
                              3
                           
                           
                              S
                              1
                           
                           
                              $
                              2
                           
                        
                        denge$1
                        Tajlandia$0”, the left context of the substring deng is no longer systematically “_” but “$2” as well. So, deng is a motif occurring twice in the selected parts of the document and once in the disease name lexicon (“denge”).

Daniel filters out motifs in response to article segmentation rules as described in Table 1, and to the list of disease names as explained in Section 3.2.3. It selects motifs that are substrings found in two different sub-units, typically head and tail, and matching at least one disease name. This comes from the genre-related rules stating that:
                              
                                 1.
                                 an important topic in news should be highlighted;

common names should be used to catch the reader's attention;

the topic should be repeated.

More formally, let 
                              
                                 S
                                 0
                              
                            and 
                              
                                 S
                                 1
                              
                            be the head and the tail of an article (i.e. the salient zones 
                              Z
                           ) and 
                              
                                 S
                                 2
                              
                           ... 
                              
                                 S
                                 
                                    n
                                    +
                                    1
                                 
                              
                            the n entries in a diseases knowledge base 
                              K
                            (Algorithm 1). The process enumerates repetitions on 
                              
                                 S
                                 0
                              
                           ... 
                              
                                 S
                                 
                                    n
                                    +
                                    1
                                 
                              
                            (Section 3.2) and selects motifs that occur in 
                              
                                 S
                                 0
                              
                           , 
                              
                                 S
                                 1
                              
                            and any of the 
                              
                                 S
                                 
                                    i
                                    ∈
                                    [
                                    2
                                    ,
                                    n
                                    +
                                    1
                                    ]
                                 
                              
                           . A heuristic ratio is used to verify if a motif matches an entry: 
                              len
                              (
                              m
                              )
                              /
                              len
                              (
                              
                                 S
                                 i
                              
                              )
                              ≥
                              θ
                              
                                 −
                                 disease
                              
                            (Algorithm 1, line 9), with m a motif occurring in salient zones and in an entry 
                              
                                 S
                                 i
                              
                            of the diseases base, len(m) and 
                              len
                              (
                              
                                 S
                                 i
                              
                              )
                            are the number of characters of m and 
                              
                                 S
                                 i
                              
                           . In the previous example, the process tests whether len(deng)/len(denga)=4/5≥
                           θ
                           −disease. The value of θ
                           −disease is discussed in Section 5.3.3. This technique proves especially useful for morphologically rich languages, as it bypasses the need for a morphological analyzer. If no motif matches the knowledge base using the θ
                           −disease threshold, it assumes that the document contains no event and is therefore irrelevant. If several items fill this criterion, the longest is selected.


                           
                              Algorithm 1
                              
                                 isRelevant
                              


                                 
                                    
                                       
                                          
                                          
                                             
                                                
                                                   1  
                                                   Input: 
                                                   
                                                      Z
                                                   , a list of salient zones z of a document
                                             
                                             
                                                
                                                   
                                                   Input: 
                                                   θ-disease, a matching threshold, θ
                                                   −disease∈]0, 1]
                                             
                                             
                                                
                                                   
                                                   Input: 
                                                   
                                                      K
                                                   , a knowledge base (a list of items k)
                                             
                                             
                                                
                                                   
                                                   Data: 
                                                   rstr(s
                                                   0, ..., s
                                                   
                                                      n−1), maximal repeats in strings s
                                                   0, s
                                                   1, ..., s
                                                   
                                                      n−1
                                                
                                             
                                             
                                                
                                                   
                                                   Data: 
                                                   len(s), the length of a string s
                                                
                                             
                                             
                                                
                                                   
                                                   Output: a diagnostic, True if a document is relevant, False otherwise
                                             
                                             
                                                
                                                   2  
                                                   begin
                                                
                                             
                                             
                                                
                                                   3    
                                                   
                                                      R
                                                      ←
                                                      rstr
                                                      (
                                                      Z
                                                      +
                                                      K
                                                      )
                                                    // maximal repeats in salient zones 
                                                   
                                                      Z
                                                    
                                                   and knowledge base 
                                                   
                                                      K
                                                   ;
                                             
                                             
                                                
                                                   4     
                                                   foreach 
                                                   
                                                      r
                                                      ∈
                                                      R
                                                    
                                                   do
                                                
                                             
                                             
                                                
                                                   5  
                                                   
                                                   
                                                      
                                                         match
                                                         z
                                                      
                                                      ←
                                                      {
                                                      z
                                                      ∈
                                                      Z
                                                      ∣
                                                      z
                                                    contains r};
                                             
                                             
                                                
                                                   6  
                                                   
                                                   if 
                                                   
                                                      
                                                         match
                                                         z
                                                      
                                                      =
                                                      Z
                                                    
                                                   then // if a repeat occurs in each salient zone of 
                                                   
                                                      Z
                                                   
                                                
                                             
                                             
                                                
                                                   7
                                                   
                                                   
                                                   
                                                      
                                                         match
                                                         k
                                                      
                                                      ←
                                                      {
                                                      k
                                                      ∈
                                                      K
                                                      ∣
                                                      k
                                                    contains r};
                                             
                                             
                                                
                                                   8
                                                   
                                                   
                                                   foreach 
                                                   k
                                                   ∈
                                                   match
                                                   
                                                      k
                                                    
                                                   do
                                                
                                             
                                             
                                                
                                                   
                                                     /* if a repeat overlaps an item in 
                                                   
                                                      K
                                                    */ 9
                                                   
                                                     
                                                   if 
                                                   
                                                      
                                                         
                                                            len
                                                            (
                                                            r
                                                            )
                                                         
                                                         
                                                            len
                                                            (
                                                            k
                                                            )
                                                         
                                                      
                                                      ≥
                                                      θ
                                                   -disease then return 
                                                   True;
                                             
                                             
                                                
                                                   10
                                                   
                                                   return 
                                                   False;
                                             
                                             
                                                
                                                   11 end
                                                
                                             
                                          
                                       
                                    
                                 
                              

An event is minimally defined as a disease-location pair. Again, journalistic style characteristics are used in Daniel to localize events without sentence-level extraction patterns. The locations are found in the same way as disease names (Algorithm 1), using repetitions in the same salient zones 
                              Z
                            as for the event detection process (as described in Table 1, Section 3.1). The motifs selected are those occurring in those zones and in a knowledge base 
                              K
                            containing a list of country names extracted from Wikipedia. The matching parameter θ-location is used as an alternative to θ-disease. The impact of the value of θ-location is detailed in Section 5.3.6. As in the previous subsection, if several locations fill the criterion according to θ-location, the longest is selected.

When no location is explicitly mentioned, the event described in the document is linked to the issuing place. Hence, the location of the event is assumed to be the country of the source (i.e. that of the newspaper or the news agency). This is referred to as the implicit location rule.

To the best of our knowledge, there is no available corpus for the evaluation of multilingual epidemic surveillance. The only corpus available online, BEcorpus,
                        9
                     
                     
                        9
                        
                           https://code.google.com/p/becorpus/ [accessed 20.04.15].
                      is exclusively built with relevant documents (200), making it unsuitable for evaluating the precision of document filtering. The corpus consists of a list of uniform resource locators (URLs) of Web pages compiled in 2009, and of which 102 source documents were still available in 2014.
                        10
                     
                     
                        10
                        List available on our corpus page: https://daniel.greyc.fr/corpus.php [accessed 20.04.15].
                      All the reports are written in English. We used this corpus to evaluate event extraction as described in Section 5.3.7.

We built a multilingual corpus with documents in Chinese, English, Greek, Polish and Russian taken from the Web. News corpora in Chinese, English and Russian were collected from the health category in Google News. Since this category does not exist in Polish or in Greek, documents were collected from health categories in major newspapers.
                        11
                     
                     
                        11
                        “Gazeta”, “Gazeta polska”, etc. for Polish. “To B
                              
                                 η
                                 ´
                              
                              μ
                              α
                           ”, “EΞΠPEΣ”, etc. for Greek.
                     
                  

Surprisingly, limiting our corpus to documents found in health categories caused low filtering power: only 8% of the resulting documents referred to epidemic events. Nonetheless, this strategy allowed us to collect a significant number of relevant documents at a reasonable cost. For measuring precision and recall of document filtering, event detection and event localization, a set of about 500 documents has been annotated for each language. Native speakers of each language
                        12
                     
                     
                        12
                        Nine professional translators who were not otherwise related to Daniel.
                      annotated documents covering the same 3-month period (November 2011 to January 2012). Evaluation corpus characteristics are shown in Table 5
                     .

The length of documents (in paragraph or characters) vary a lot from one to another. Annotators had to judge whether each document was relevant for informing health authorities about infectious diseases. If a document was judged relevant, the annotator was further requested to provide the disease name and location of the event. The guidelines, the corpus and corresponding annotations are available on the Daniel Web site.
                        13
                     
                     
                        13
                        
                           https://daniel.greyc.fr/corpus.php [accessed 20.04.15].
                     
                  

This section shows the performance of the repetition rule in salient zones to select relevant press articles. Daniel is first demonstrated through examples, then evaluated quantitatively against annotators’ judgements on the evaluation corpus. The system processes 2000 documents in less than 15s,
                        14
                     
                     
                        14
                        Program in Python, using a 2.4GHz dual core processor with 2Gb RAM.
                      which is compatible with on-line surveillance.


                        Fig. 3
                         exhibits an example of the repetition phenomenon in a relevant press article. The term “tuberculosis” is repeated at salient positions (i.e. occurs in salient zones): head and body. The longest common substrings between the disease list and salient zones are highlighted. This is why the capitalized form “Tuberculosis” (last paragraph) is not highlighted. The abbreviation “TB” is not the sole term used in the document, confirming our assumption on news writing: explicit terms are used to ease the transmission of the main topic. No location is repeated in the article, hence the event is implicitly located with respect to the source,
                           15
                        
                        
                           15
                           
                              http://www.dnaindia.com [accessed 20.04.15].
                         “India”.


                        Fig. 2, mentioned in Section 3.2, shows the application of Daniel's principles in Polish, a morphologically rich language. The disease name is repeated with different forms, but still detected. The location is detected with the repetition rule, a sample case of explicit location.

In this study the three main measures used for evaluation are recall, precision and F-measure. These measures are defined as follows: 
                           
                              •
                              
                                 Recall (
                                 R
                                 ): Number of relevant items retrieved by the system (True positives T
                                 
                                    p
                                 ) divided by total number of relevant items (True positives + False Negatives: T
                                 
                                    p
                                 
                                 +
                                 F
                                 
                                    n
                                 ): 
                                    R
                                    =
                                    
                                       
                                          
                                             T
                                             p
                                          
                                       
                                       
                                          
                                             T
                                             p
                                          
                                          +
                                          
                                             F
                                             n
                                          
                                       
                                    
                                 
                              


                                 Precision (
                                 P
                                 ): Number of relevant items retrieved by the system (True positives T
                                 
                                    p
                                 ) divided by total number of retrieved items (True positives + False positives: T
                                 
                                    p
                                 
                                 +
                                 F
                                 
                                    n
                                 ): 
                                    P
                                    =
                                    
                                       
                                          
                                             T
                                             p
                                          
                                       
                                       
                                          
                                             T
                                             p
                                          
                                          +
                                          
                                             F
                                             p
                                          
                                       
                                    
                                 
                              


                                 F-measure (
                                 F
                                 
                                    β
                                 
                                 ): Harmonic mean of recall and precision. This measure can be tuned (β parameter) to add weight to recall or precision: 
                                    
                                       F
                                       β
                                    
                                    =
                                    (
                                    1
                                    +
                                    β
                                    )
                                    
                                       
                                          P
                                          .
                                          R
                                       
                                       
                                          (
                                          β
                                          .
                                          P
                                          )
                                          +
                                          R
                                       
                                    
                                 
                              

In harmony with common field practice, the F-measure is computed with β
                        =1 (F
                        1) and β
                        =2 (F
                        2), the higher β, the more the recall is emphasized. The items considered in the following experiments are documents. Hence, this evaluation is referred to as document wise evaluation (event wise evaluation is discussed in Section 5.3.6).

The performance of the Daniel system is detailed in Table 6
                        . We can see that the performance is globally better in terms of recall than in terms of precision. Good recall results are achieved for three languages of different families: Chinese, Greek and Polish. This is a significant result because Greek is a morphologically rich language whereas Chinese has poor morphology but still causes problems for machine translation. In Polish the system performance was less satisfying due to lack of precision.

With the default θ
                        −disease value (0.80), a F
                        1 score of 0.80 for the cumulated corpus. Tuning the best ratio θ
                        −disease for F
                        1-measure in each language increased the precision to 0.74, with a slightly better recall (0.93). This result is somehow surprising as the small lexicon size was expected to impair recall more than precision. It is an important question for a system that relies on small resources: the system should not miss too many events, particularly for epidemic surveillance, where recall usually matters more than precision.

Interestingly, the default θ
                        −disease value with its greater recall achieves a very good F
                        2-measure of 0.87. It is compatible with recall-oriented needs since it shows that Daniel can perform well without tuning. Table 7
                         shows the extent to which Daniel misses events and the reasons for such errors.

Errors due to the size of the lexicon are rare (5). The repetition phenomenon is trustworthy: only four relevant documents were missed because no repetition matching any disease name in the knowledge base was found. Another issue stemmed from string recognition, as some diseases were referred to by names too short to be detected by Daniel.

The news discourse model implemented through repetition rules at salient positions efficiently selects relevant press articles on epidemiological events. Fig. 4
                         shows how frequent disease name repetition behaves in relevant articles (dotted line) and how rare it is in irrelevant ones (continuous line). This shows how this simple rule truly helps to filter out irrelevant documents: 97% of irrelevant as opposed to only 0.7% of relevant articles contained no repetition.

This section first evaluates the performance of Daniel's processing steps and compares results to three baselines. The influence of the parameters θ
                        −disease and θ
                        −location is evaluated, and then the question of using alternative resources is tackled. Finally, an event-based evaluation is proposed for our multilingual reference corpus as well as for a corpus from the state-of-the-art BEcorpus.

The news segmentation described in Section 3.1 is intended to filter out uninteresting motifs. Table 8
                            shows the impact of this filtering. The point of segmentation filtering is to reduce the noise produced by the system without significantly impairing recall. The filtering rate is lower in Chinese since the alphabet size is much higher (around 3,000 items). Hence, the motif distribution is sparser, and repetitions are less frequent. Frequent n-grams are much more common in other languages (i.e. “_th” in English).

In order to evaluate the different features of our system, Table 9
                            shows the performance of three baselines B1, B2 and B3. B1 assumes an epidemic event whenever a disease name is present in the document while B2 does so only if the disease name is repeated. Finally, B3 combines the repetition criteria to the position of repetition. B1 highlights the problems with morphologically rich languages because of the exact matching required for the disease name. B2 shows the improvement in precision obtained with the use of repetitions. The additional constraint of position used in B3 leads to even better precision while hindering recall. All three baselines use θ
                           −disease=1.

This section describes the determination of the appropriate string matching ratio between motifs extracted and knowledge base entries for the five languages. For instance, a small θ
                           −disease offers a perfect recall with high noise (many irrelevant documents are selected). The aim of the following experiments is to find the value allowing for the best trade-off between recall and precision. Figs. 5
                            and 6
                            show that in Chinese, English and Greek, an increase in the value of θ
                           −disease causes an increase in precision with little impact on recall. This result was expected for Chinese and English but not for Greek which has richer morphology.

Conversely, in Fig. 6 performance drops for Polish (respectively, Russian) when θ
                           −disease is greater than 0.80 (respectively, 0.85). The choice of θ
                           −disease matters more for these two languages, due to their rich morphology. The same experiment was performed with a same θ
                           −disease value for the cumulated corpora. The left graph of Fig. 7
                            shows that θ
                           −disease=0.80 is a good empirical value for processing the five different languages simultaneously. Table 6 contains the optimal value of θ
                           −disease for each language and the scores obtained with θ
                           −disease uniformly set to 0.80.

In Fig. 7, the graph on the right-hand side illustrates the results obtained when all knowledge bases for all languages are merged. In this framework, the language of each document is unknown to the system. The results are very close to those obtained on the left-hand side of the figure, in which only the knowledge bases in the document's language are used. Interestingly, this implies that knowing the language of the document is not decisive for Daniel. This is mostly due to the fact that the languages used in this experiment are significantly different, which implies that there is little overlap between the various lexica. The potential of incorrectly matching a disease in the knowledge base of a given language with an irrelevant string in another language is indeed very unlikely, hence limiting the impact on the results.

The Wikipedia lexica used in Daniel are easy-to-collect and multilingual. Domain ontologies could be used but few offer multilingual coverage. The international classification of diseases provided by World Health Organization's (WHO) ICD-10
                              16
                           
                           
                              16
                              2010 version on the WHO's website: http://apps.who.int/classifications/icd10/browse/2010/en [accessed 20.04.15].
                            is one of them. ICD-10 covers 42 languages, several of which are available online. Daniel has been tested with a lexicon extracted from ICD-10 (using chapters I to XV, II and IV excluded). Because the entries in ICD-10 might be complex (sometimes composed of a dozen of words), two different sub-lexica are exploited. The first one is composed of all the words in the entries of ICD-10. The second one was obtained by removing grammatical and vague words (45% of the English lexicon, e.g., “disease”, “sick” etc.). Performances are analyzed with regards to the document filtering task.

Experiments have been performed on the English corpus (Table 4) with θ
                           −disease=0.80. The results obtained with Wikipedia (Table 10
                           ) are different from Table 6 since the θ
                           −disease value is the default one. The ICD-10 lexicon induces very low precision since all the documents are tagged as relevant. After manually cleansing the ICD-10, by removing grammatical words and vague terms, precision rose from 0.07 to 0.23 which is far from the results obtained with Wikipedia. It appears that ICD-10 gives no added-value to the results. Most of the terms are very specialized and seldom used in the news genre. A more thorough manual cleansing may improve the results further but this would be a costly and language-dependent procedure.


                           Table 11
                            exhibits the performance of the localization algorithm. This experiment compares the location given by Daniel and the location given by the annotators. The implicit location rule has been applied to the majority of the detected events (98 over 136) and achieved a good performance with 87% precision. Two errors came from a source to which the wrong country had been assigned. The explicit location rule performed worse with 79% precision. Most of the mislocations were actually partially correct, since the detected location was often a subregion of the annotated location. (e.g. events concerning the whole Europe were incorrectly located in Poland).

Evaluation can be carried out with respect to the number of documents selected, the technical unit commonly used in information retrieval, or to the number of events, unit expressing the meaningful information for the task [24,25]. For instance, it is possible to detect 99 documents describing the same epidemic event (e.g. flu in Spain in April 2012) and yet miss an event that is contained in only one document (e.g. Ebola in Congo in April 2012). A document wise evaluation would rank this case as 99% recall, which is intuitively wrong since only one out of two events is detected [17].

To evaluate how Daniel performs with respect to events rather than documents, event-based annotations were compiled (corpus described in Section 4). Here, an event is a disease-location pair and a time period. All documents were published during the same 3-month time window. Therefore, each disease-location pair (e.g. flu in Spain) is considered as a unique event, regardless of the number of documents in which it has been reported.


                           Table 12
                            shows the results of the evaluation by event, demonstrating that only a few full-fledged epidemic events (3 out of 62) were missed. The total number of unique events in the corpus (Table 12) is not the sum of unique events in each subcorpus. A single epidemiological event can be reported in several languages. The system takes advantage of its language coverage, which gives it additional opportunities to detect events [26] (e.g. an event missed in Polish documents was detected in Russian documents). This experiment highlights the importance of increasing the geographical coverage by processing more languages rather than optimizing a system in a small number of languages. A more extended coverage limits the time needed to detect an event and minimizes the risk of missing it [4].


                           Fig. 8
                            exhibits heatmaps to show how the θ-location and θ-disease values affect event extraction. The lighter a zone, the better the results for a particular combination of θ-disease and θ-location values. Recall, precision and F1-measure are computed as described in Section 5.2, based on disease-location pairs. In other words, let (d
                           1, l
                           1) be a disease-location pair of the gold standard. If d
                           2 and l
                           2 are a disease and a location in the knowledge bases, then neither (d
                           2, l
                           1) nor (d
                           1, l
                           2) are true positives.

The recall is slightly lower than in Table 12 since each distinct disease-location pair represents a class. For recall, the lighter zone (≥0.8) corresponds to the following combination of parameters: θ
                           −location∈[0.55, 1] and θ
                           −disease∈[0.6, 0.9]. θ-location has little influence on results compared to θ-disease. Two factors lead to this. First, the implicit location rule is used for many documents (72% in the standard configuration as shown in Section 5.3.5). Second, location names include specific substrings that are less commonly found in the corpus (they have a relative invariant basis).

The best parameter combinations for precision are comparable to the ones for recall: θ
                           −location∈[0.65, 1] and θ
                           −disease∈[0.80, 0.90]. The lighter zones cover a smaller area than in the heatmap for recall. However, few false positives represent noise since these events can easily be connected to human-validated ones (for instance (H1N1,China) and (avian flu,China)). This echoes the results shown at the document level (Table 6).

Finally, the heatmap for F
                           1-measure appears as a synthesis of the previous ones. The range of values of both parameters for achieving the best results (F
                           1-measure ≥0.7) are: θ
                           −location∈[0.55, 1] and θ
                           −disease∈[0.80, 1]. The parameters can be adjusted in accordance with users’ objectives. Still, using 0.80 for both θ
                           −location and θ
                           −disease achieves good results.

This corpus has been released and described by Conway et al. in 2009 [27], and is available online.
                              17
                           
                           
                              17
                              
                                 https://code.google.com/p/becorpus/ [accessed 20.04.15].
                            The Biocaster team has used this corpus to evaluate event classification for its system [28]. It consists of 200 reports supplied with, among other things, the URL of the source and the metadata in the form of disease-location pair. Unfortunately, only 102 source web pages (among 200) were still available online at the time of this publication (100 in English, one in Russian and one in French). The evaluation in [27] is done using English reports and news articles, whereas Daniel is specialized in processing news only. Daniel has been evaluated on 102 source web pages using merged resources as described in Section 5.3.3 and θ
                           −disease=
                           θ
                           −location=0.80, the standard configuration of the system.

First, the performance for the document filtering task is evaluated. The precision measure is inappropriate since all documents are tagged as relevant. The recall is 0.88 (90/102). This figure is comparable to the results presented in our own reference corpus (Table 6).

This figure can not be compared with Biocaster since to our knowledge the authors did not report this kind of evaluation. This is probably due to the fact that the corpus was designed for event-wise evaluation only. However, it is interesting to give some insights into the 12 misclassified documents (all in English). First, 6 of them concerned events that were not included in Daniel guidelines (bacterial infections and diseases affecting animals). Second, 4 documents were misclassified because they did not fulfill the genre requirements: they were reports from the program for monitoring emerging diseases (Promed). Daniel is designed to process press articles whilst 23 documents are Promed reports.
                              18
                           
                           
                              18
                              from the human-produced reports available at http://www.promedmail.org [accessed 20.04.15].
                            With the aim of transmitting information as quickly as possible, Daniel is a good alternative. It annihilates the delay in writing reports about an epidemic issue.

Finally, an event-wise evaluation was performed. For 81% of the documents the appropriate disease-location pair was detected. For unique events, the performances are better than those obtained with our own corpus: 0.85 for recall and 0.88 for precision. The recall is lower (0.85 vs. 0.93) but the precision is very high (0.88 vs. 0.80, Fig. 8). The F
                           1-measure increases by 0.07 with a 0.87 score. The Biocaster system obtained an even better score with 0.94 F
                           1-measure [28]. Considering the fact that we expose a simpler and more multilingual scheme, this is a very good result. The heatmaps showed that Daniel achieves comparable results even for poorly endowed languages.

@&#DISCUSSION@&#

The challenge in health surveillance is to ensure world coverage. The current approach is to multiply dedicated systems for each language, but resources are lacking for a very large number of them. The richest state-of-the-art system handles 10 languages, whereas there are about 6000 languages in the world, 300 of which are spoken by more than one million people. The principles of a genre-based IE system called Daniel have been tested on 17 languages and evaluated on 5 languages: Chinese, English, Greek, Polish and Russian. The system relies on light, easy-to-obtain resources, and is intended to help health authorities gather information about on-going infectious diseases spreading throughout the world. In order to be multilingual, it uses news genre-related features. Carefully selected types of string repetitions are used as clues to relevance of a document. Experiments show that the system is lacking in precision, but has a good recall (0.89 for English, 0.91 for the whole corpus), an excellent result for global online epidemic surveillance.

The Daniel algorithm is based on the rhetorical construction of news articles, unlike state-of-the-art systems relying on extended lexicon and syntactic parsing. It focuses on where the useful information should be rather than on what it should be. The detection of string repetitions in texts might seem costly, however, cost is curbed by exploring only salient positions in the text. The longer the document, the more constrained the search space. In short news pieces, the beginning and end cover the whole text (technically there is no middle), but in longer news pieces, the middle is larger. Another savings in processing costs is the fact that an external list of disease names from Wikipedia is used to filter repetition candidates.

Given these constraints, is Daniel 
                        truly language-independent? Language-independence is relative and may be identified with respect to three characteristics:
                           
                              1
                              Consistency in journalistic style;

Establishment of a knowledge base from easily accessible resources;

Determination of a parametric model of language (θ
                                 −disease, θ
                                 −location).

We have proposed a simple and effective model for comparing lexical entries that can have several forms in a document. The parameters θ take variability of prefixes/suffixes into account to find the largest root occurring in a document and in lexical resources. This parameter shows low variability in the languages processed. A single value of θ (0.80) can be used to cover different languages for both θ
                        −disease and θ
                        −location. Daniel factorises the diversity of the entries in the knowledge base. This can be explained by the journalistic-genre assumption (well-known terms are used in news wires) and by the specificity of entries in the knowledge base. Medical terms have a large invariant root from their Greek and Latin origin. Place names also have a relative invariant basis in a given alphabet. It might be argued that Daniel results are not directly linked to specialized medical databases through the Unified Medical Language System, ICD or any other nomenclature. For example, in Fig. 3, the detection of tuberculosis does not necessarily lead to feed databases using the proper entry (TDR-TB) in an ontology. Post-processing would be needed to achieve this goal.

Inflections can affect several words in multiword entries. Therefore, relevant substrings between a document and this kind of units are harder to detect. For instance, “
                           
                        ” and “
                           
                        ” are two inflections of avian flu, found in a relevant Russian article.
                           19
                        
                        
                           19
                           
                              https://daniel.greyc.fr/public/index.php?id=1577 [accessed 20.04.15].
                         To tackle this problem, the motif extraction module might be shifted to a gapped-motif extraction module [21]. The detection of these patterns is greedy, but the complexity of their enumerations can be channeled by limiting the maximum size (in number of characters) of gapped-motif considered. The longest gapped-motifs cannot be longer than the longest entry in the knowledge base.

@&#CONCLUSION@&#

Daniel is a text genre-based IE system devoted to news. It is efficient at distinguishing irrelevant documents in epidemic surveillance and at filtering streams of documents with low-resourced languages. When no classical IE system is available or training data is scarce, Daniel can fill the gap efficiently. The method described increases coverage in number of languages at low cost, rather than optimizing results with a particular language. Wikipedia is used to screen some common disease names to be matched with repeated character strings. The language variations, such as declensions, are handled by processing text at the character level, rather than at the word level. This additionally allows Daniel to handle various writing systems in a similar fashion.

With an average F
                        1-measure of 0.85, Daniel scores are below state-of-the-art systems (Puls or Biocaster), as we confirmed with our comparative evaluation over the BEcorpus. However, the resources that these systems require (lexicon, language parser, ontologies) are far more extensive and costly to acquire. Daniel makes it possible to immediately process new languages if a list of disease names is provided. A list of locations is not a strict requirement since the implicit location rule of Daniel performs well.

Daniel results have demonstrated great promise in multilingual EE at minimal marginal cost. Further research on document structure and segmentation will lead to more refined rhetorical rules. It is also possible to build a hybrid system in which Daniel will filter relevant documents from a general news feed. A language detector and filter could then direct documents in dominant languages to a classical EE system that achieves high precision. High precision with a language can provide a more precise tag to a cluster of related documents [17].

In order to advance EE research, the corpora used for these experiments are available to the community with annotations detached from original URLs. News corpora in Arabic, French, Portuguese, Spanish, Swahili etc. are being annotated to assess Daniel's quality in a wider range of languages as part of the effort to improve multilingual world coverage.

@&#REFERENCES@&#

