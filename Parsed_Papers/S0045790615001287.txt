@&#MAIN-TITLE@&#Non-negative Matrix Factorization on Low-Power Architectures and Accelerators: A Comparative Study

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Performance and energy consumption study of NMF is performed on different systems.


                        
                        
                           
                           General-purpose CPUs yield better execution times at the cost of high energy rates.


                        
                        
                           
                           Low-power architectures offer better trace-off in energy and performance.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

NMF

Low-power processors

GPU

DSP

Accelerators

@&#ABSTRACT@&#


               
               
                  Power consumption is emerging as one of the main concerns in the High Performance Computing (HPC) field. As a growing number of bioinformatics applications require HPC techniques and parallel architectures to meet performance requirements, power consumption arises as an additional limitation when accelerating them. In this paper, we present a comparative study of optimized implementations of the Non-negative Matrix Factorization (NMF), that is widely used in many fields of bioinformatics, taking into account both performance and power consumption. We target a wide range of state-of-the-art parallel architectures, including general-purpose, low-power processors and specific-purpose accelerators like GPUs, DSPs or the Intel Xeon Phi. From our study, we gain insights in both performance and energy consumption for each one of them under a number of experimental conditions, and conclude that the most appropriate architecture is usually a trade-off between performance and energy consumption for a given experimental setup and dataset.
               
            

@&#INTRODUCTION@&#

Recent technological advances in the areas of genetics and molecular biology are the result of interdisciplinary research between diverse fields such as medicine, biology, chemistry and computer science. One of the main challenges these areas will need to face today and in the near future is the ability to process, analyze and interpret massive amounts of data generated in biological experiments.

Different data mining methods have proven to be powerful tools to obtain biological patterns. Two of the most extended techniques in large dataset analysis are clustering algorithms and matrix factorization techniques. These methods facilitate multidimensional analysis, that allows dimensionality reduction, or discovering certain patterns that dramatically reduce the burden towards biological interpretation. Dimensionality reduction is a key step for the effective analysis of high-dimension datasets. In this area, Principal Component Analysis (PCA) [1] or Singular Value Decomposition (SVD) [2] are two of the most popular methods. Non-negative matrix factorization (NMF) is also considered as one of the most efficient methods in the biological disclosure due to its ability to establish relevant relationships between experimental datasets.

NMF was proposed by Brunet et al. [3] as a powerful tool for gene expression data. They applied it to cancer micro-array data to extract molecular patterns in leukemia, medulloblastoma and central nervous system tumor datasets based on consensus clustering. NMF is nowadays widely used in computational biology for molecular pattern discovery [4], supervised learning frameworks as class comparison and prediction [5], or text-mining [6]. However, the importance of NMF is not limited to computational biology but also to other areas such as image processing for image classification [7] or face recognition [8], information retrieval to extract semantic recognition or hidden meaning in text [9], web-based logs [10], or hyperspectral image processing [11], among others. However, both NMF primitives and applications based on them pose two big challenges from the computational perspective that are closely related:


                     Performance requirements. Factorization of large biological datasets makes NMF a very demanding task from the computational point of view that may make this processing unfeasible in practice, unless high-end distributed memory architectures are used. Due to this fact, parallel implementations of NMF have been previously proposed and successfully applied. In particular, previous works have targeted parallel implementations in shared-memory multi-core processors using the OpenMP paradigm [12], distributed-memory clusters based on Message Passing Interface (MPI) [13], and also exploiting modern Graphics Processing Units (GPUs) [14].


                     Power constraints. Many of the scenarios and applications in which NMF is typically used can be strictly limited by the maximum power that can be supplied to the computing platform (for example, on hand-held devices in biological in-place analysis scenarios), or maximum energy consumed (for example, in on-board hyperspectral image analysis on airborne or spaceborne missions). In such cases, a correct selection of the computing platform and its performance/power ratio is crucial to meet the application requirements in terms of power consumption.

Processor manufacturers are nowadays concerned about concepts such as Green Computing and Power-aware HPC. Their goal is to develop efficient processors not only in terms of peak performance rates, but also energy consumption and peak power draw. Besides modern and efficient multi-core CPUs, hardware accelerators such as GPUs or the recently introduced Intel Xeon Phi, specific-purpose architectures such as Digital Signal Processors (DSPs), or ultra-low power processors like ARM-based architectures or hybrid architectures using low-power ARM CPUs and programmable GPUs have been recently introduced in the High Performance Computing field in order to meet at the same time performance and power consumption requirements.

The suitability of this type of architectures is primarily dictated by the specific characteristics of the target application, and more precisely, by the efficiency of the underlying software building blocks (e.g. scientific libraries) on top of which those applications are built. As an example, for many biological applications based on NMF, the performance and power efficiency attained on different architectures for this specific operation will dramatically determine the behavior of the overall application. In this paper, we perform a comparative study of several state-of-the-art parallel architectures in terms of performance and power efficiency for a well-known NMF implementation proposed by Brunet et al. [3]. This choice is motivated by the high acceptance of these implementations, but does not exclude the extension to other implementations as Least Squares NMF [15] or Chih-Jen Li approach [16]. To the best of the authors’ knowledge, there does not exist either any viability study of NMF under power consumption constraints in the literature or optimized implementations for novel architectures like multi-core DSPs, low-power ARM or Intel Xeon Phi. This paper extends the insights already extracted in [17], with the following new contributions:
                        
                           •
                           We target novel, state-of-the-art architectures not covered in [17], including ARM Cortex A7 and A15, Nvidia Kepler and Intel Xeon Phi, to adapt the study to new architectures recently emerged in the HPC arena.

We extend the pmlib framework to gather actual power measurements from the most representative architectures among the evaluated ones.

For these architectures, we extend the power efficiency study already presented in [17] with actual power consumption measurements, describing the measurement environment used in the evaluation process and discussing the suitability of each platform for this specific problem based on realistic considerations.

We propose new experimental results for single and double precision arithmetic, and discuss the feasibility of each architecture depending on the desired accuracy.

The rest of the paper is organized as follows. Section 2 presents an overview of the selected architectures that are employed to evaluate the performance and power efficiency of our approach. Section 3 introduces the basic concepts of the NMF factorization, together with the specific optimization techniques applied to adapt them to each platform. Section 4 reports detailed performance and power efficiency results and discusses the feasibility of each target architecture for a given experimental condition. Section 5 moves through the description of the power measurement environment used to evaluate the power efficiency of the solutions using realistic power data. Finally, Section 6 closes the paper with some general concluding remarks and future work proposals.

In this section, we present the main features of the architectures used in the evaluation of performance and power efficiency of the selected NMF implementations. Four of them (ARM Cortex-A7/A15, CARMA board and DSP) are inherently low-power architectures; two of them are designed as high-end hardware accelerators (Intel Xeon Phi and Nvidia Kepler); the last one (Intel Xeon) is a general-purpose processor that will be evaluated as a baseline for our performance and power consumption measurements and conclusions.


                     Table 1
                      gives an overview of the capabilities of each architecture in terms of theoretical peak performance-measured in GFLOPS (1 GFLOP=1billion floating point operations per second) – and energy efficiency (in terms of GFLOPS per Watt, considering the TDP – Thermal Design Power – as a measure of power consumption for each one of them). We also add information regarding the capability of each architecture to report accurate on-board energy consumption readings; this feature will be leveraged in our empirical energy consumption study in Section 5. Table 2
                      reports the key architectural features of each platform. Finally, Table 3
                      shows the compiler versions, optimization flags and BLAS implementations used in our experimental evaluation. In the rest of the study, we will refer to each architecture using the name reported in Table 1.

The increasing processing demands of the mobile market, together with the necessity of designing highly-efficient architectures, have contributed to the development of processing platforms in which Performance per Watt is the primary design concern. This type of systems, often based on the ARM architecture with some accelerating platform attached – low power GPUs or DSPs – has attracted the attention of the HPC community, and has emerged as an appealing alternative for scenarios in which high performance is needed, but power efficiency or maximum available power is a critical restriction. We report next the low-power architectures employed in our study.

The ARM Cortex is a line of RISC processors that implements a dual-issue superscalar, out-of-order pipeline. In our case, we use an Odroid XU+E board, featuring a Samsung Exynos 5410 heterogeneous SoC that implements the ARM big.LITTLE hybrid architecture: it presents two independent processing clusters, the first with four ARM Cortex A7 ultra-low power cores and the second with four ARM Cortex A15 cores. Each cluster owns an independent cache hierarchy, while main DDR memory is shared between both. The selection of the specific processing cluster can be modified at runtime depending on the experimental necessities.

This type of processors is the base of many current mobile devices (including phones, tablets, hand-held devices and even low-power desktop systems), hence the interest in studying its feasibility for biomedical applications such as those using NMF, in which both mobility and power consumption can be a concern. Their low TDP make them a perfect platform for those applications and scenarios severely limited by the maximum power draw that can be sourced to the computing platform.

The CARMA board was introduced in late 2012 to demonstrate the capabilities of ARM and GPU architectures combined in terms of performance and power efficiency. It exhibits a quad-core ARM Cortex A9 CPU with a Quadro 1000M GPU to support the increasing demand for energy-efficient computing, combining a general-purpose programming model on the CPU side and CUDA on the GPU side. The CARMA board combines a theoretical peak performance of 270 GFLOPs with a power consumption of 45W. While not directly designed neither for mobile devices nor for high performance computing, some projects such as the Mont-Blanc project try to deliver Exascale performance with 15–30 times energy saving using this type of low-power hybrid architecture [21].

While exhibiting a higher peak power consumption than homogeneous ARM-based architectures, this type of heterogeneous platforms are a trade-off between performance and power consumption for those types of applications that exhibit considerable degrees of data-parallel sections in the code, which can efficiently exploit the potential of the attached GPU.

One of novel additions to the energy-aware HPC field is the C6678 multi-core Digital Signal Processor (DSP) from Texas Instruments (TI) [22,23], that implements eight C66x VLIW cores running at 1GHz, and combines a theoretical peak performance of 128 GFLOPs with a power consumption of roughly 10W per chip. Besides, one of its most appealing features is programmability, combining support for general-purpose languages (e.g. C/C++) with well-known parallel programming models such as OpenMP. DSP-based architectures are already present and widely spread in medical equipment and devices specifically designed for biologic analysis, hence the interest in porting routines like NMF for biomedical analysis.

This specific-purpose architecture will be even more attractive in novel hybrid SoCs such as the TI Keystone-II architecture, based on ARM technology accelerated by one or more Digital Signal Processors [24]. However, the DSP is unique as it can be a standalone processor, or act as a classical accelerator, using a low-power host processor.

As of today, the usage of massively parallel hardware accelerators is playing a key role in the HPC arena. Many of the most powerful supercomputers are today equipped with one or more accelerators per computing node. In some cases, even low-end desktop computers exhibit relatively powerful graphics hardware, that can be exploited to accelerate critical applications. Even though specific-purpose GPUs have been historically the chosen accelerating platform for many scientific applications, the appearance of the Intel Xeon Phi as a general-purpose, highly programmable accelerator is attracting the HPC community to evaluate its efficiency, both in terms of performance and power consumption.

The Nvidia Tesla K20 is a high-end GPU that implements the modern Kepler GK110 microarchitecture, featuring 2496 CUDA cores, with a peak performance of 3.52 GFLOPS operating in single precision. Together with the de facto standard in GPGPU programming –CUDA–, these platforms are able to easily exploit the inherent data parallelism available in some applications with a relatively small programming effort. In our case, the evaluated card is equipped with 5 Gbytes of GDDR5 memory, and is attached to a node powered by an Intel Xeon E5 2670 through a PCI-e Gen3 bus.

Opposite to graphics processors, the cores in the Intel Xeon Phi microarchitecture are based on the x86 general-purpose architecture, with wide (512-bit) vector units to support heavy data-parallel applications. Together with high-level language constructions to support a host-accelerator model, this makes the porting of existing codes for x86 architectures straightforward. For our evaluation purposes, we will use an Intel Xeon Phi 5110P card, attached to an Intel Xeon E5 2670 through a PCI-e Gen3 bus.

We will also evaluate the efficiency of our implementations both in terms of performance and power consumption for a general-purpose Intel Xeon processor, using it as a baseline. More specifically, we use an Intel Xeon E5 2670 featuring two quad-core sockets and running at 2.6Ghz, with 16 Gbytes of DDR3 RAM. We consider this processor to be representative of common modern multi-core architectures for desktop and server environments. In addition, as our accelerator-based architectures are attached to this exact processor model, and its power consumption cannot be ignored in heterogeneous platforms, it is necessary to carry out a detailed evaluation also for this architecture.

Non-negative factorization (NMF) was first introduced by Paataro and Tapper in 1994 [15] under the name positive matrix factorization, and popularized by Lee and Seung [8]. The NMF decomposition can be described as
                        
                           (1)
                           
                              V
                              ≈
                              WH
                              ,
                           
                        
                     where 
                        
                           V
                           ∈
                           
                              
                                 R
                              
                              
                                 m
                                 ×
                                 n
                              
                           
                        
                      is a positive matrix with m variables and n objects, 
                        
                           W
                           ∈
                           
                              
                                 R
                              
                              
                                 m
                                 ×
                                 k
                              
                           
                        
                      is the reduced k factor, and 
                        
                           H
                           ∈
                           
                              
                                 R
                              
                              
                                 k
                                 ×
                                 n
                              
                           
                        
                      contains the coefficients of linear combinations of the basis vectors. For the sake of dimensional reduction of NMF, it is assumed that 
                        
                           k
                           ≪
                           min
                           (
                           n
                           ,
                           m
                           )
                        
                     . In particular, for gene expression, the matrix V expresses an experimental biological matrix with m genes and n experimental conditions. For a particular rank 
                        
                           k
                           ,
                           H
                        
                      and W represent metagenes (semantic feature) and metagenes expression patterns (gene semantic profile), respectively.

In Lee and Seung’s approach, NMF iteratively modifies W and H until their product approximates V. Such modifications are derived from minimizing a cost function that describes the distance between the product WH and V. For this work we consider the well-known NMF factorization reformulated by Brunet et al. [3] with the following update rules:
                        
                           (2)
                           
                              
                                 
                                    H
                                 
                                 
                                    α
                                    μ
                                 
                              
                              ←
                              
                                 
                                    H
                                 
                                 
                                    α
                                    μ
                                 
                              
                              
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                       
                                    
                                    
                                       
                                          W
                                       
                                       
                                          i
                                          α
                                       
                                    
                                    
                                       
                                          V
                                       
                                       
                                          i
                                          μ
                                       
                                    
                                    /
                                    (
                                    
                                       
                                          WH
                                       
                                       
                                          i
                                          μ
                                       
                                    
                                    )
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          k
                                       
                                    
                                    
                                       
                                          W
                                       
                                       
                                          k
                                          α
                                       
                                    
                                 
                              
                           
                        
                     
                     
                        
                           (3)
                           
                              
                                 
                                    W
                                 
                                 
                                    i
                                    α
                                 
                              
                              ←
                              
                                 
                                    W
                                 
                                 
                                    i
                                    α
                                 
                              
                              
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          μ
                                       
                                    
                                    
                                       
                                          H
                                       
                                       
                                          α
                                          μ
                                       
                                    
                                    
                                       
                                          V
                                       
                                       
                                          i
                                          μ
                                       
                                    
                                    /
                                    (
                                    
                                       
                                          WH
                                       
                                       
                                          i
                                          μ
                                       
                                    
                                    )
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          ν
                                       
                                    
                                    
                                       
                                          H
                                       
                                       
                                          α
                                          ν
                                       
                                    
                                 
                              
                           
                        
                     
                  


                     H and W are randomly generated, so this approach does not always converge to the same solution. To solve this issue, a consensus matrix assignment is calculated to establish a convergence condition that mitigates probability dispersion in the final solution. Algorithm 1 shows an excerpt of the main NMF code for each iteration of the refinement process; in the algorithm, we only report the necessary operations to update H according to Eq. 2, but similar operations hold for the update of W
                     
                        1
                        The ./ operator stands by element-wise division.
                     
                     
                        1
                     .

In this work we have also considered the study of other NMF variants related to sparseness of both the basis and encoding vectors. This method is referred as Nonsmooth Non-negative Matrix Factorization (nsNMF) [25], and it is defined as:
                        
                           (4)
                           
                              V
                              ≈
                              WSH
                           
                        
                     where V, W, and H are the same as in the original NMF model. The positive symmetric matrix 
                        
                           S
                           ∈
                           
                              
                                 R
                              
                              
                                 k
                                 ×
                                 k
                              
                           
                        
                      known as “smoothing” matrix is defined as:
                        
                           (5)
                           
                              S
                              =
                              (
                              1
                              -
                              θ
                              )
                              I
                              +
                              
                                 
                                    θ
                                 
                                 
                                    k
                                 
                              
                              
                                 
                                    11
                                 
                                 
                                    T
                                 
                              
                           
                        
                     where I is the identity matrix, 1 is a vector of ones, and the parameter 
                        
                           θ
                        
                      (
                        
                           0
                           <
                           θ
                           <
                           1
                        
                     ) controls sparseness of the model.

As both NMF approaches are similar from the algorithmic perspective and the type of operations performed, we will take the Lee and Seung’s approach to describe our NMF implementation and optimizations in the following subsections, and for our experimental study. However, we have observed similar qualitative performance and energy consumption results for nsNMF, and thus many of the insights and conclusions extracted can be directly applied also to this method.

As can be observed in Algorithm 1, the updates of W and H are mainly based on general matrix–matrix multiplications and other fine grained matrix and vector operations. Hence, the most convenient method for optimizing NMF is to cast these matrix operations in terms of BLAS
                           2
                           
                              http://www.netlib.org/blas.
                        
                        
                           2
                         (Basic Linear Algebra Subprograms) routine calls. This enables the use of highly tuned BLAS implementations specific for each architecture, provided such exists, and thus removes the major part of the optimization task from the developer’s shoulders. As the major part of BLAS implementations provide support multi-threaded execution, this also facilitates the parallelization task of the original sequential code. In our case, the first optimization from the basic sequential code consists of casting the updates of W and H in terms of the corresponding high performance parallel BLAS routine implementation, mainly using xGEMM
                        
                           3
                           From now on, in BLAS calls, x can be replaced by s when using single precision floating-point arithmetic, or d for double precision.
                        
                        
                           3
                         for general matrix–matrix multiplication in (
                           
                              ▷
                           
                         (1)) and (
                           
                              ▷
                           
                         (4)) and a sequence of xAXPY for the reduction of W to one column (
                           
                              ▷
                           
                         (3)).
                           Algorithm 1
                           NMF(
                                 
                                    
                                       
                                          V
                                       
                                       
                                          n
                                          ×
                                          m
                                       
                                    
                                    ,
                                    
                                       
                                          W
                                       
                                       
                                          n
                                          ×
                                          k
                                       
                                    
                                    ,
                                    
                                       
                                          H
                                       
                                       
                                          k
                                          ×
                                          m
                                       
                                    
                                    ,
                                    niters
                                 
                              ) 
                                 
                                    
                                       
                                       
                                       
                                          
                                             1: for
                                                
                                                
                                                   
                                                      iter
                                                      ⩽
                                                      niters
                                                   
                                                
                                                
                                                do
                                             
                                             
                                          
                                          
                                             2:
                                             
                                          
                                          
                                             3: 
                                                
                                                
                                                
                                                   
                                                      ▷
                                                   
                                                
                                                
                                                
                                                   
                                                      H
                                                      =
                                                      H
                                                      .
                                                      ∗
                                                      (
                                                      
                                                         
                                                            W
                                                         
                                                         
                                                            ′
                                                         
                                                      
                                                      ∗
                                                      (
                                                      V
                                                      .
                                                      /
                                                      (
                                                      W
                                                      ∗
                                                      H
                                                      )
                                                      )
                                                      )
                                                      .
                                                      /
                                                      
                                                         
                                                            x
                                                         
                                                         
                                                            1
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                          
                                          
                                             4:
                                             
                                          
                                          
                                             5: 
                                                
                                                
                                                
                                                   
                                                      wh
                                                      =
                                                      W
                                                      ∗
                                                      H
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      ▷
                                                   
                                                 (1)
                                          
                                          
                                             6: 
                                                
                                                
                                                
                                                   
                                                      wh
                                                      =
                                                      V
                                                      .
                                                      /
                                                      wh
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      ▷
                                                   
                                                 (2)
                                          
                                          
                                             7:
                                             
                                          
                                          
                                             8: 
                                                
                                                
                                                
                                                   
                                                      ▷
                                                   
                                                 Reduce to one column (
                                                   
                                                      
                                                         
                                                            x
                                                         
                                                         
                                                            1
                                                         
                                                      
                                                   
                                                )
                                             
                                          
                                          
                                             9:
                                                
                                                
                                                
                                                   
                                                      
                                                         
                                                            x
                                                         
                                                         
                                                            1
                                                         
                                                      
                                                      =
                                                      repmat
                                                      (
                                                      sum
                                                      
                                                         
                                                            (
                                                            W
                                                            ,
                                                            1
                                                            )
                                                         
                                                         
                                                            ′
                                                         
                                                      
                                                      ,
                                                      1
                                                      ,
                                                      m
                                                      )
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      ▷
                                                   
                                                 (3)
                                          
                                          
                                             10:
                                             
                                          
                                          
                                             11: 
                                                
                                                
                                                
                                                   
                                                      Haux
                                                      =
                                                      W
                                                      ∗
                                                      wh
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      ▷
                                                   
                                                 (4)
                                          
                                          
                                             12: 
                                                
                                                
                                                
                                                   
                                                      H
                                                      =
                                                      (
                                                      H
                                                      .
                                                      ∗
                                                      Haux
                                                      )
                                                      .
                                                      /
                                                      
                                                         
                                                            x
                                                         
                                                         
                                                            1
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      ▷
                                                   
                                                 (5)
                                          
                                          
                                             13:
                                             
                                          
                                          
                                             14: 
                                                
                                                
                                                
                                                   
                                                      ▷
                                                   
                                                 
                                                
                                                   
                                                      W
                                                      =
                                                      W
                                                      .
                                                      ∗
                                                      (
                                                      (
                                                      V
                                                      .
                                                      /
                                                      (
                                                      W
                                                      ∗
                                                      H
                                                      )
                                                      )
                                                      ∗
                                                      
                                                         
                                                            H
                                                         
                                                         
                                                            ′
                                                         
                                                      
                                                      )
                                                      .
                                                      /
                                                      
                                                         
                                                            x
                                                         
                                                         
                                                            2
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                          
                                          
                                             15: 
                                                
                                                
                                                
                                                   
                                                      ▷
                                                   
                                                 
                                                
                                                   
                                                      …
                                                   
                                                
                                             
                                             
                                          
                                          
                                             16:
                                             
                                          
                                          
                                             17: end for
                                             
                                             
                                          
                                       
                                    
                                 
                              
                           

Intensively using BLAS kernels is the approach taken to optimize the NMF implementation on general-purpose architectures (Arm7, Arm15 and Xeon). We describe next the particularities of the NMF implementations on GPU-based architectures (Carma and Kepler), DSP, and XeonPhi.

To port the NMF algorithm to the CUDA programming model, matrix operations must be rewritten into CUDA kernels. This process is enhanced by the possibility of using the BLAS implementation from Nvidia (CUBLAS) which incorporates xAXPY and xGEMM matrix operations. Algorithm 2 shows the pseudo-code for the NMF implementation in a CUDA-enabled GPU, where each line indicates a kernel invocation. Kernels denoted as k__W_mult_H and k__Wt_mult_WH are wrappers to the corresponding invocation to CUBLAS, while k__mult_M_div_vect has been developed specifically for this platform using CUDA. Although the matrix reduction in (
                           
                              ▷
                           
                         (3)) is based on a sequence of xAXPY calls, we have observed a performance degradation when using the CUBLAS_xAXPY for the specific matrix dimensions (and shapes) in NMF, and it has been replaced by a specific CUDA kernel with the same functionality.
                           Algorithm 2
                           NMF_GPU(
                                 
                                    d
                                    _
                                    V
                                    ,
                                    d
                                    _
                                    W
                                    ,
                                    d
                                    _
                                    H
                                    ,
                                    niters
                                 
                              ) 
                                 
                                    
                                       
                                       
                                       
                                          
                                             1: for
                                                
                                                
                                                   
                                                      iter
                                                      ⩽
                                                      niters
                                                   
                                                
                                                
                                                do
                                             
                                             
                                          
                                          
                                             2:
                                             
                                          
                                          
                                             3:
                                                
                                                
                                                
                                                   
                                                      ▷
                                                   
                                                 
                                                
                                                   
                                                      H
                                                      =
                                                      H
                                                      .
                                                      ∗
                                                      (
                                                      
                                                         
                                                            W
                                                         
                                                         
                                                            ′
                                                         
                                                      
                                                      ∗
                                                      (
                                                      V
                                                      .
                                                      /
                                                      (
                                                      W
                                                      ∗
                                                      H
                                                      )
                                                      )
                                                      )
                                                      .
                                                      /
                                                      
                                                         
                                                            x
                                                         
                                                         
                                                            1
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                          
                                          
                                             4:
                                             
                                          
                                          
                                             5: 
                                                
                                                
                                                
                                                   
                                                      d
                                                      _
                                                      WH
                                                      ←
                                                      k
                                                      _
                                                      _
                                                      W
                                                      _
                                                      mult
                                                      _
                                                      H
                                                      (
                                                      d
                                                      _
                                                      W
                                                      ,
                                                      d
                                                      _
                                                      H
                                                      )
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      ▷
                                                   
                                                 (1)
                                          
                                          
                                             6:
                                                
                                                
                                                
                                                   
                                                      d
                                                      _
                                                      
                                                         
                                                            WH
                                                         
                                                         
                                                            aux
                                                         
                                                      
                                                      ←
                                                      k
                                                      _
                                                      _
                                                      V
                                                      _
                                                      div
                                                      _
                                                      WH
                                                      (
                                                      d
                                                      _
                                                      V
                                                      ,
                                                      d
                                                      _
                                                      WH
                                                      )
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      ▷
                                                   
                                                 (2)
                                          
                                          
                                             7: 
                                                
                                                
                                                
                                                   
                                                      
                                                         
                                                            x
                                                         
                                                         
                                                            1
                                                         
                                                      
                                                      ←
                                                      k
                                                      _
                                                      _
                                                      accum
                                                      (
                                                      d
                                                      _
                                                      W
                                                      )
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      ▷
                                                   
                                                 (3)
                                          
                                          
                                             8: 
                                                
                                                
                                                
                                                   
                                                      d
                                                      _
                                                      
                                                         
                                                            H
                                                         
                                                         
                                                            aux
                                                         
                                                      
                                                      ←
                                                      k
                                                      _
                                                      _
                                                      Wt
                                                      _
                                                      mult
                                                      _
                                                      WH
                                                      (
                                                      d
                                                      _
                                                      W
                                                      ,
                                                      d
                                                      _
                                                      
                                                         
                                                            WH
                                                         
                                                         
                                                            aux
                                                         
                                                      
                                                      )
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      ▷
                                                   
                                                 (4)
                                          
                                          
                                             9: 
                                                
                                                
                                                
                                                   
                                                      d
                                                      _
                                                      H
                                                      ←
                                                      k
                                                      _
                                                      _
                                                      mult
                                                      _
                                                      M
                                                      _
                                                      div
                                                      _
                                                      vect
                                                      (
                                                      h
                                                      ,
                                                      d
                                                      _
                                                      
                                                         
                                                            H
                                                         
                                                         
                                                            aux
                                                         
                                                      
                                                      ,
                                                      
                                                         
                                                            x
                                                         
                                                         
                                                            1
                                                         
                                                      
                                                      )
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      ▷
                                                   
                                                 (5)
                                          
                                          
                                             10:
                                             
                                          
                                          
                                             11: 
                                                
                                                
                                                
                                                   
                                                      ▷
                                                   
                                                 
                                                
                                                   
                                                      W
                                                      =
                                                      W
                                                      .
                                                      ∗
                                                      (
                                                      (
                                                      V
                                                      .
                                                      /
                                                      (
                                                      W
                                                      ∗
                                                      H
                                                      )
                                                      )
                                                      ∗
                                                      
                                                         
                                                            H
                                                         
                                                         
                                                            ′
                                                         
                                                      
                                                      )
                                                      .
                                                      /
                                                      
                                                         
                                                            x
                                                         
                                                         
                                                            2
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                          
                                          
                                             12: 
                                                   
                                                      …
                                                   
                                                
                                             
                                             
                                          
                                          
                                             13:
                                             
                                          
                                          
                                             14: end for
                                             
                                             
                                          
                                       
                                    
                                 
                              
                           

Additionally, a new kernel is built in order to adjust small values to avoid underflow in W and H. The construction of the connectivity matrix and the convergence criterion is performed on CPU due to low impact in the overall execution time.

In Algorithm 2, matrices 
                           
                              V
                              ,
                              W
                           
                         and H are replaced by their equivalents 
                           
                              d
                              _
                              V
                              ,
                              d
                              _
                              W
                           
                         and 
                           
                              d
                              _
                              H
                           
                        , that have been previously allocated and transferred to GPU memory. After computation, the resulting factors are transferred back to main memory. Pinned memory has been used on the host side to optimize data transfers from and to W and H, which are updated regularly in host in order to compute connectivity matrix and the convergence criteria. Padding has also been applied in the dimension k in order to adjust memory alignment to the CUDA warp size for this architecture, which reports a reduction in computing time between 10% and 15%.

Unlike GPU-based architectures, the TI DSP is a standalone processor, running its own real-time operating system, and thus no data transfers between memory spaces are necessary. From this perspective, it is more similar to general-purpose architectures such as ARM or Intel x86. The BLAS implementation from TI [20,26] has been extensively used in our implementation when possible. In general, both inside the TI BLAS implementation and in the rest of the code, three different levels of parallelism are exploited:


                        Instruction-level parallelism. We have performed a deep exploration of loop-related parameters (unrolling factor and bounds, use of restrict keywords to disambiguate pointers, and compiler optimizations) in order to find an optimal combination for NMF and help the compiler in the process of obtaining optimized code for the VLIW architecture.


                        Data-level parallelism. SIMD instructions are supported in the C66x through the use of intrinsics and vector datatypes. While this is exploited heavily in the TI BLAS implementation, we have also leveraged these capabilities in those parts of the code in which BLAS cannot be applied.


                        Thread-level parallelism. To exploit parallelism across multiple cores, we have used the multi-threaded version of TI BLAS; the recently introduced OpenMP support from TI has been applied on those loops that cannot be cast in terms of this (multi-threaded) BLAS implementation.

The implementation of NMF on the Intel Xeon Phi accelerator is carried out using the native Intel MKL library (Intel Math Kernel Library), which provides highly optimized (vectorized and multi-threaded) implementations of the required BLAS routines.

The simplest and most direct way to port NMF algorithm to the XeonPhi accelerator is by using the Automatic Offload (AO) model. It enables the use of the Xeon-Phi co-processor automatically and transparently without modifying the source code, by the use of the corresponding environment variable. In this naive way, selected BLAS operations (including xGEMM in (
                           
                              ▷
                           
                         (1)) and (
                           
                              ▷
                           
                         (4)) and the reduction to a column by means of xAXPY (
                           
                              ▷
                           
                         (3)) mentioned in Algorithm 1) can be automatically executed in the accelerator, without any intervention from the developer. However, according our own experience, it is not easy to control the workload distribution between the host and accelerator performed by the AO scheduler in the MKL library. Moreover, for the matrix sizes involved in NMF, the AO scheduler rarely and randomly decides to perform matrix operations on the accelerator, regardless of forcing the whole task to be downloaded to the accelerator by using the corresponding MKL environment variable (MKL_MIC_WORKDIVISION).

Because of this handicap, we have decided to manually specify which parts of the NMF algorithm are performed on the co-processor. This task only implied the incorporation of a few #pragma offload constructions to identify the parts of the algorithm that will be executed on the accelerator. In order to reduce multiple memory copies for each xAXPY and xGEMM operation between host and accelerator, the offloaded region has been specified only once. Thus, the complete algorithm is performed in the accelerator, with only an initial and final data transfer. The BLAS calls in the offloaded region will be, thus, executed in the co-processor, as the rest of the updating rules; those parts of the code that can be potentially executed in parallel by all the cores in the co-processor have been marked with the corresponding #pragma omp parallel annotation. As thread-to-core affinity is also a critical factor when pursuing high performance in the Xeon Phi, we have explored a number of different configurations; for our NMF configuration, a compact affinity policy, in which consecutive threads are mapped to consecutive hardware threads in the architecture, turns out to be the optimal configuration.

@&#EXPERIMENTAL RESULTS@&#

We next describe the performance and power efficiency results of the NMF implementation described in Section 3 on the proposed parallel architectures. For each one, we report results for single and double precision arithmetic. Experimental results for performance and energy consumption are given in Figs. 1 and 2
                        
                         for single precision, and 3 and 4 for double precision.

The chosen datasets are detailed in Table 4
                         and are extracted from the study in [27]. The selection criteria is mainly based on the problem size (e.g. small datasets like Colon to large datasets like Lung), to evaluate the impact of this factor for different types of computing architectures. For each dataset, plots on the left side show execution time in seconds for each one of the target architectures, and an increasing value of k (rank); plots on the right show power consumption in Joules, obtained by multiplying the execution time by the average power dissipated by each one of the processors during the computation. In this set of tests, we use the TDP specified by the processor manufacturer as a measure of the power dissipated by each one. This is an optimistic value of power consumption, and the real one is intended to be significantly lower. In all experiments, we use the maximum number of cores offered by the corresponding architecture, by using the appropriate multi-threaded BLAS implementation. For the accelerator-based architectures, time devoted to data transfers between RAM and GPU memory, and viceversa, is included in the reported results.

To perform the experiments, we chose rank k to vary in range 
                           
                              
                                 
                                    2
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    2
                                 
                                 
                                    2
                                 
                              
                              ,
                              
                                 
                                    2
                                 
                                 
                                    3
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    2
                                 
                                 
                                    6
                                 
                              
                           
                        ; although these are not the most representative values in biological scenarios and applications (where usually 
                           
                              k
                              ≪
                              min
                              (
                              n
                              ,
                              m
                              )
                           
                        ), these values can be used to predict both power-consumption and performance behavior in a wide range of biological datasets and applications. To perform a fair comparison, the number of iterations is fixed to 2000 (as in Brunet’s Matlab version
                           4
                           
                              http://www.broadinstitute.org/mpr/publications/projects/NMF/nmf.m.
                        
                        
                           4
                        ), consensus matrix assignment is performed after 10 iterations and random seed is set for identical initial generation of W and H.

@&#DISCUSSION@&#

Considering the large amount of experimental conditions and platforms evaluated, we divide our discussion in three main topics: Performance, describing the behavior of each architecture in terms of execution time; Energy consumption, with notes about the most appealing platform for different experimental conditions; and Impact of precision, stating the most appropriate platform depending on the desired accuracy in floating-point calculations.

Considering exclusively execution time, the Arm7 and the Kepler platforms are clearly the slowest and fastest of the tested architectures, respectively, for all datasets and rank values. This is not surprising, considering the theoretical peak performance values shown in Table 2. However, empirical results for ARM are not that far in terms of performance compared with the rest of the low-power architectures tested (Carma and DSP) as theoretical values suggest, especially for small datasets (e.g. Colon dataset).

Let us focus on the performance attained by each one of the tested platforms. Arm7, being an ultra-low power architecture, is clearly the slowest in terms of execution time. Only for small problems (e.g. Colon or Leukemia datasets) and low ranks (not higher than 
                              
                                 k
                                 =
                                 16
                              
                           ) it can compete with the rest of the low power architectures. Thus, if execution time is the main concern, this is the range of problems in which Arm7 can be considered as a suitable alternative. For larger datasets or ranks, the Arm7 architecture yields execution times up to two orders of magnitude higher than the fastest tested platforms, and thus its choice as an execution platform is not recommended.

Regarding the rest of the low power architectures, Arm15 and DSP offer in general comparable performance results for all experimental conditions, although DSP is often slightly faster, especially for large values of k. The decision of which one to choose would be thus based on this parameter, and additional energy considerations that are targeted next. Carma clearly outperforms the rest of the low power architectures in terms of execution time, being around 
                              
                                 1.5
                                 ×
                              
                            and 
                              
                                 3
                                 ×
                              
                            faster than DSP for the main part of the conducted experiments. While Carma offers the best theoretical peak performance from the four low power architectures, we observe a big penalty from data transfers involved in the algorithm; from our observations, this factor is severely limiting the potential of the architecture.

Accelerator-based architectures (XeonPhi and Kepler) require a separate discussion. Although the reported theoretical peak performance for each one is comparable, we have observed abnormal low performance rates for XeonPhi, not only compared with Kepler (its natural competitor), but also with other architectures, when working with small datasets (e.g. Colon and leukemia), and small values of k. From our observations, the architecture, and more specifically, the BLAS implementation used in it –Intel MKL–, is not yet optimized for the small rank-k updates involved in the factorization. In fact, in our experiments on XeonPhi
                           we have also tested the Automatic Offload functionality of MKL, in which the library decides at runtime the optimal execution resource available in the platform among the Xeon and the XeonPhi. The decision taken by the library was always selecting the Xeon for the specific problem sizes that appear in our problems, which enforces the idea of considering the XeonPhi as an unsuitable platform for this specific problem. Beyond these limitations, XeonPhi becomes more competitive platform as dataset size increases, reaching performances comparable with general-purpose multi-core processors (Xeon) for the largest tested dataset. For larger datasets (e.g. GCM and Lung), the XeonPhi is a competitive alternative to the high-performance Kepler GPU.

The second accelerator-based architecture evaluated, Kepler, is clearly the best in terms of performance comparison. It is important to note that, unlike other platforms, the performance attained by Kepler is comparatively better as both dataset dimensions and rank increase. This observation is not only important for the chosen datasets, but also gives a valid orientation for potentially larger datasets that can arise in other applications. Powerful accelerated platforms (in our case GPUs) are thus of great appeal when processing large amount of biological data, and when the targeted dimensionality is large enough.

Finally, the Xeon platform is a very competitive architecture in terms of performance, even compared with accelerator-based alternatives. As for Kepler, we have observed better quantitative results as the rank k increases, mainly due to the large amount of threads involved in the computation (up to 32 when HyperThreading is active) that can exploit better the potential of the application as more parallelism is exposed.

The behavior of a given architecture for NMF in function of the desired accuracy in floating point operations is important for two reasons: first, NMF is based on an iterative refinement process, and thus the accuracy is the key towards convergence; second, different architectures show different theoretical ratios between single and double precision throughput. For example, while Xeon or Kepler offer a 1:2 ratio, other architectures like Arm7, Arm15 or DSP decrease this ratio up to 1:4, 1:4 or 1:8, respectively. From our experiments we have observed an overall ratio 1:2 for all architectures, independently from the theoretical ratio given by the manufacturer. While the performance offered by BLAS-based operations is usually heavily determined by the chosen precision, given the narrow matrices involved in the factorization, and the amount of supplementary memory-bounded operations (consensus calculation, classification prior to determination of convergence, …), this factor plays a secondary role for this specific operation. In conclusion, independently from the desired accuracy in floating-point calculations, all tested architectures present a very similar behavior from the performance point of view. In addition, the major part of the conclusions and insights for performance behavior extracted from single precision can be easily extrapolated to double precision arithmetic.

Let us focus next on energy consumption results (right-side plots in Figs. 1–4
                           
                           ). In general, comparing those plots with their performance counterparts, we can extract two general insights: first, differences between architectures are now smaller than those for performance for all problem sizes and ranks; second, the architectures that yielded the best performance results (Xeon and Kepler) are not always the best in terms of energy efficiency. These facts enforce the idea of the effort put by manufacturers towards designing and developing highly efficient parallel architectures, independently from the peak performance attained by them.

Focusing on low-power architectures (Arm7, Arm15, DSP and Carma), the small gap between them in terms of energy consumption is remarkable. Especially for small datasets (Colon and Leukemia), the energy consumption rates for those platforms is comparable, and remarkably smaller than the rest of the platforms, including accelerator-based architectures. The only exception is Arm7, that yields dramatically larger energy consumption rates for large datasets and ranks (e.g. for GCM and Lung datasets), mainly due to the poorer execution times offered in those cases. Arm15 yields the best energy consumption results for small datasets, and DSP is the most suitable option for large datasets in terms of energy consumption, even more as the rank k increases. These results are qualitatively similar for double precision; in that scenario, though, Carma is significantly less efficient in terms of energy consumption, mainly due to its inability when operating with double precision arithmetic, and the bigger impact of data transfers.

Regarding accelerator-based architectures, XeonPhi (as discussed before) yields dramatically higher rates for energy consumption, mainly due to the inefficient BLAS implementation for this specific operation. This phenomenon is more evident for small datasets, as also reported for execution times. On those scenarios in which the XeonPhi attains high performance (mainly large datasets and values of k), the Intel accelerator is competitive and even outperforms the GPU in terms of energy consumption. The Kepler platform, despite of being the most efficient in terms of performance, is not as competitive in terms of power efficiency, if we consider TDP. In single precision, the Kepler architecture is less efficient than the second GPU-based platform (Carma); for double precision, though, the benefits of Kepler are more evident, and it outperforms Carma. In general (with the mentioned exception of the ultra-low power Arm7), Kepler is slightly less efficient than the rest of the low power architectures considering TDP as valid metric.

Finally, for small datasets, the general-purpose Xeon architecture is comparable in terms of energy consumption to Kepler, especially in single precision. For double precision floating-point arithmetic, the gap between both is increased. Note, however, that Xeon is more efficient for medium-sized ranges (for k between 8 and 32); from our observations, these matrix sizes maximize the exploitation of the efficient caches in this type of architectures, and boost the performance of the underlying BLAS calls.

In summary, performance and energy consumption, while related, dictate that different architectures can target the same algorithm efficiently depending on the criteria used to evaluate it, and the restrictions imposed by the application or environment in which it is run. In general, the low-power processors are the perfect candidates if energy is a constraint, while general-purpose architectures and accelerator-based platforms can target those scenarios in which performance is the limiting factor. From the experimental results, the three of the low-power architectures evaluated (Arm15, Carma, and DSP) can be considered as trade-off solutions in terms of performance and energy consumption.

However, performance and energy consumption are not the only factors that can determine which architecture is most suitable for NMF. Peak power consumption is also important, as many mobile or hand-held devices pose restrictive values that can only be attained using ultra-low power processors; in this sense, only ARM and DSP seem to meet this criteria. Another important factor is ease of programming. General-purpose architectures like ARM and Intel Xeon, and also the TI DSP, support widely extended programming languages and models (C/C++ with OpenMP support), which makes the port of existing codes straightforward. While CUDA is de facto the standard for GPU programming nowadays, it can imply some burdens for the programmer, mainly due to the existence of specific memory hierarchy or the necessity of developing ad hoc kernels and optimizations following this programming model.

The energy consumption results presented in Section 4 considered the TDP as a representative value for the power consumption of a given architecture. While this number can be useful for qualitative comparison purposes, or even the only valid metric on those systems without mechanisms to empirically measure power consumption, actual power readings can differ due to the power saving techniques usually present in modern processors, and other factors such as the specific type of operation being executed, workloads used in the target application, or number of cores running at a certain execution point. Thus, when possible, it is mandatory to perform a realistic power consumption analysis on those architectures that provide proper mechanisms.

In this section, we propose a power measurement environment by adapting and extending an existing library (pmlib) to the specific power measurement mechanisms available in some of the architectures targeted in previous sections. For them, we report empirical results of energy consumption of NMF under different problem configurations. These results will be useful to validate or refute the energy consumption numbers extracted in our previous theoretical study.


                        pmlib 
                        [28] is a library developed at University Jaume I that eases the process of measuring energy consumption and instrumenting applications running on different target architectures. In previous works, the developers of the library have mainly used pmlib combined with ad hoc or commercial power monitors in order to estimate the energy consumption of a number of target platforms. pmlib is a client–server framework in which the server module directly communicates with the power consumption measuring mechanism, and the client module (that can be embedded on the target code though a provided API) queries for gathered power measurements, synchronizing the obtained data with the execution of the target program.

One of the main contributions of this paper is the extension and adaptation of pmlib to interact and gather energy consumption results from the hardware or software mechanisms available in some our target architectures. Those mechanisms depend on the target architecture to be measured:
                           
                              •
                              
                                 Arm7 and Arm15. The ODROID XU+E board features four independent built-in sensors (INA 231 from Texas Instruments [29]) that return accurate and independent power consumption readings from four different components present in the Samsung Exynos SoC. The sensors are configured to gather up to four power measurements per second on each one of the components. These values correspond to instantaneous power consumption for the A7 cores, A15 cores, memory banks and GPU. For our measurement purposes, we will focus exclusively on the A7 and A15 cores, gathering measurements from the corresponding sensor depending on the specific execution cluster used.


                                 Kepler. In their modern GPUs, NVIDIA has introduced power measurement capabilities that can be queried from userspace by using NVML (NVIDIA Management Library). Among other parameters, NVML can report accurate data about the current board power draw and power limits. This feature is supported in the Kepler architecture and is leveraged by our pmlib extension to gather power measurement from the card. Note that, being an accelerator-based architecture, it is necessary to aggregate these measurements with those extracted for the host (i.e. Xeon architecture) to offer a fair comparison.


                                 XeonPhi. Offering similar capabilities to the NVML library for NVIDIA GPUs, the MicAccessAPI is a C/C++ library that allows access to internal control and status registers of the Intel Xeon Phi cards. Among others, this library facilitates access to voltage and temperature sensors on the card. Focusing on energy consumption, the API provides detailed access to the energy consumed through each one of the power sources of the accelerator, including the PCI-e bus and two additional power connectors to the system power supply. Our adaptation of the pmlib infrastructure will aggregate those three power sources to extract a precise measurement of the real power consumption of the card at execution time. As for Kepler we will consider this measurement together with that of the host in order to extract the overall system energy consumption.


                                 Xeon. Starting with the Sandy Bridge microarchitecture, Intel introduced a mechanism called RAPL (Running Average Power Limit) to gather energy consumption data directly from on-board hardware sensors. This mechanism is now available in all modern Xeon processors. With RAPL, it is possible to measure power consumption of CPU-level components (mainly cores and memory controller) by reading the appropriate Machine-Specific Registers (MSR). According to Intel, those values are updated every 1ms. We have adapted pmlib to interact with RAPL and gather these results. The obtained values will be useful not only to characterize the efficiency of the isolated Xeon platform, but also to aggregate them to the accelerator-based architectures in order to offer a realistic power measurement.

As can be observed, not every architecture proposed in our work supports power consumption readings at core level. While it is certainly possible to measure power at system level for all platforms (e.g. measuring the instantaneous power consumed by the power supply), we have opted to only report empirical results on those platforms that are able to expose power measurements at core level. However, the platforms supporting this feature are representative enough for our comparative study, as they include ultra low power (Arm7), low power (Arm15), general-purpose (Xeon) and high-end hardware accelerators (XeonPhi and Kepler).

Empirical energy consumption results are reported in Figs. 5 and 6
                        
                         for single and double precision, respectively. First, we will remark those results that resemble or substantially differ from those reported in Section 4 that took into account the TDP as a valid power metric:
                           
                              1.
                              Low-power architectures (Arm7 and Arm15) are still the most efficient platforms for most datasets and ranks. Only Kepler is more efficient for selected large datasets and ranks, especially compared with Arm15 on double precision arithmetic.


                                 Arm7 is, in all cases, the most efficient platform, independently from the chosen dataset and rank. The gap between experimental measurements and those reported using TDP is considerable.


                                 XeonPhi keeps offering poor energy consumption results for small datasets, mainly due to the aforementioned inefficiency in terms of execution time for this specific problem, but is a competitive solution for large datasets, especially when operating in single precision arithmetic.


                                 Xeon is competitive with the Kepler architecture only for small datasets and ranks, only when employing double precision. For the rest of scenarios (especially for small ranks in single precision), its inefficiency is evident.

Many of these facts can be explained supported by the data reported in Table 5
                        , that reports empirical peak, average and idle power measurements for each one of the target platforms gathered during the execution of the NMF factorization of the largest problem proposed, using the Lung dataset and rank 
                           
                              k
                              =
                              128
                           
                        . Similar qualitative power values have been observed for the rest of the problems. First, let us compare the TDP for each architecture with the actual average and maximum power values observed. The differences between both differ depending on the architecture: while for Xeon the observed power is in the range of 70–75% of the TDP, this difference is dramatically reduced for low power architectures (8–10% of TDP for Arm7). This clearly explains why TDP is not a valid guide for all types of architectures, and why Arm7 is, in practice, the most efficient architecture among the studied ones.

Focusing on accelerator-based architectures, it is remarkable the difference in efficiency between XeonPhi and Kepler. Although both offer identical TDP values, the maximum dissipated power observed dramatically varies between them (158W for XeonPhi, 94W for Kepler). This differences also appear in idle times, in which Kepler (46W) is clearly more efficient than XeonPhi
                        (100W).

From our observation, the Xeon architecture is not particularly efficient in terms of energy consumption when executing codes, but the power saving mechanisms available (mainly DVFS to reduce frequency on idle times) makes it very efficient when not using it (9.2W vs. a TDP of 225W). This is of special importance for accelerator-based architectures. Note how the actual power observed on the host for hybrid executions on Kepler and XeonPhi (around 30W) is roughly a 17% of the observed peak power consumption of the Xeon platform. This makes us suspect that only one core is being used while the accelerator is working, while the rest apply power efficiency mechanisms to reduce power consumption.

From the insights extracted after the experimental analysis, both in terms of performance and energy consumption, it seems clear that the chosen platform for a given experimental setup (dataset size and rank) will ultimately be a trade-off between expected performance and desired energy consumption. To better put the former results in context, we propose two different representation of the gathered data, both for performance and energy consumption.

For Fig. 7
                        , we have selected two representative datasets (Colon and Lung) to illustrate a comparative analysis of each architecture in terms of execution time and empirically measured energy consumption
                           5
                           For the sake of clarity, we avoid reporting data for XeonPhi here, due to the abnormally high execution time and energy consumption for NMF.
                        
                        
                           5
                        . In the Figure, time and energy are normalized to the best result attained on the most efficient architecture for each experiment. If we compare Arm7
                        and Kepler the trade-off is clear: although Arm7 is the most efficient platform in terms of energy consumption – improving the results for Kepler in factors between 
                           
                              5
                              ×
                           
                         and 
                           
                              20
                              ×
                           
                         depending on the experimental dataset and rank – the execution times for the low-power architecture can be, in many occasions, unacceptable (Kepler outperforms it up to a factor 
                           
                              80
                              ×
                           
                         for the largest problem and rank size). Intermediate qualitative conclusions can also be applied to Arm15 and Xeon. Thus, the final chosen platform will ultimately be a trade-off between the restrictions imposed in execution time and energy consumption by the specific scenario in which the platform will run.


                        Fig. 8
                         illustrates a similar idea by representing the power footprint of different architectures for the largest tested problem (Lung dataset with 
                           
                              k
                              =
                              128
                           
                        ). This type of information can be easily extracted from the pmlib framework with our integrated extensions. Note the larger execution time as the theoretical peak performance of the architecture decreases, but more importantly, compare the actual power consumption: the average power consumed by Arm7 is one order of magnitude lower than that of Arm15, and more than two orders of magnitude smaller than the rest of the tested architectures, which reinforces the idea of this low-power architecture being the most suitable candidate if energy efficiency is the primary concern, and execution time is secondary.

@&#CONCLUSIONS@&#

In this paper, we have presented a detailed performance and energy consumption study of the well known Brunet’s version of NMF algorithm on different architectures, including low-power, general-purpose, and accelerator-based platforms. In scenarios and applications in which performance is not the only restriction, the selection of an architecture for NMF cannot be taken attending exclusively to factors like theoretical peak performance. On the contrary, it must be a trade-off solution. We have gained insights about the strengths and weaknesses of a set of representative low-power and specific and general purpose architectures attending simultaneously performance and energy consumption considerations, for a wide range of problem conditions and datasets. We believe these contributions can be of wide appeal also for other types of numerical codes widely used in bioinformatics, as well as for other present and future computing architectures. Although not reported in the paper, the major part of the insights have also been validated for the Nonsmooth NMF [25].

Future research lines will include scalability studies for different number of cores on each architecture and experimental dataset, and the deployment of power measurement mechanisms on more architectures. We plan to study other NMF methods on different architectures to gain insights about the suitability of each platform for a given NMF variant. As nearly all tested platforms support (or will support in the near future) a common parallel programming paradigm (OpenCL), we will also evaluate a third key factor when selecting a platform for acceleration of NMF: programmability. We will conduct research on the trade-off between performance, energy consumption and ease of programming using common OpenCL codes for all architectures.

@&#ACKNOWLEDGMENTS@&#

This work has been supported by Spanish Projects MICINN-TIN 2008/508 and TIN 2012/32180. We thank Texas Instruments for the donation of the DSP evaluation board used in this work.

@&#REFERENCES@&#

