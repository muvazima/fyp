@&#MAIN-TITLE@&#Feature extraction based on the high-pass filtering of audio signals for Acoustic Event Classification

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           We propose a new front-end for Acoustic Event Classification tasks (AEC).


                        
                        
                           
                           The proposed front-end relies on the high pass filtering of acoustic event signals.


                        
                        
                           
                           Results show that it outperforms the baseline system in clean and noisy conditions.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Acoustic Event Classification

High-pass filtering

Auditory filterbank

@&#ABSTRACT@&#


               
               
                  In this paper, we propose a new front-end for Acoustic Event Classification tasks (AEC). First, we study the spectral characteristics of different acoustic events in comparison with the structure of speech spectra. Second, from the findings of this study, we propose a new parameterization for AEC, which is an extension of the conventional Mel-Frequency Cepstral Coefficients (MFCC) and is based on the high pass filtering of the acoustic event signal. The proposed front-end have been tested in clean and noisy conditions and compared to the conventional MFCC in an AEC task. Results support the fact that the high pass filtering of the audio signal is, in general terms, beneficial for the system, showing that the removal of frequencies below 100–275Hz in the feature extraction process in clean conditions and below 400–500Hz in noisy conditions, improves significantly the performance of the system with respect to the baseline.
               
            

@&#INTRODUCTION@&#

In recent years, the problem of automatically detecting and classifying acoustic non-speech events has attracted the attention of numerous researchers. Although speech is the most informative acoustic event, other kind of sounds (such as laughs, coughs, keyboard typing, etc.) can give relevant cues about the human presence and activity in a certain scenario (for example, in an office room). This information could be used in different applications, mainly in those with perceptually aware interfaces such as smart-rooms (Temko and Nadeu, 2006), automotive applications (Muller et al., 2008), mobile robots working in diverse environments (Chu et al., 2006) or surveillance systems (Clavel et al., 2005). Additionally, acoustic event detection and classification systems, can be used as a pre-processing stage for Automatic Speech Recognition (ASR) in such a way that this kind of sounds can be removed prior to the recognition process increasing its robustness. In this paper, we focus on Acoustic Event Classification (AEC).

Several front-ends have been proposed in the literature, some of them based on short-term features, such as Mel-Frequency Cepstral Coefficients (MFCC) (Temko and Nadeu, 2006; Zieger, 2008; Zhuang et al., 2010; Kwangyoun and Hanseok, 2011), log filterbank energies (Zhuang et al., 2010), Perceptual Linear Prediction (PLP) (Portelo et al., 2009), log-energy, spectral flux, fundamental entropy and zero-crossing rate (Temko and Nadeu, 2006). Other approaches are based on the application of different temporal integration techniques over these short-term features (Meng et al., 2007; Mejía-Navarrete et al., 2011; Zhang and Schuller, 2012). Finally, other relevant works in the literature have shown that the activation coefficients produced by the application of Non-Negative Matrix Factorizarization (NMF) on audio spectrograms can be used as acoustic features for AEC and other related tasks (Weninger et al., 2011; Cotton and Ellis, 2011). In order to distinguish between the different acoustic classes, some classification tools are then applied over these acoustic features, as for example, Gaussian Mixture Models (GMM) (Temko and Nadeu, 2006), Hidden Markov Models (HMM) (Cotton and Ellis, 2011), Support Vector Machines (SVM) (Temko and Nadeu, 2006; Mejía-Navarrete et al., 2011), Radial Basis Function Neural Networks (RBFNN) (Dhanalakshmi et al., 2008) and Deep Neural Networks (DNN) (Kons and Toledo, 2013). The high correlation between the performance of different classifiers suggests that the main problem is not the classification technique, but a design of a suitable feature extraction process for AEC (Kons and Toledo, 2013).

In fact, as pointed in Zhuang et al. (2010), conventional acoustic features are not necessarily the more appropriate for AEC tasks because they have been design according to the spectral characteristics of speech which are quite different from the spectral structure of acoustic events. To deal with this issue in Zhuang et al. (2010) it is proposed a boosted feature selection method to construct a more suitable parameterization for AEC.

In this work, we follow a different approach. Based on the empirical study of the spectral characteristics of different acoustic events in comparison with the structure of speech spectra, we propose a new parameterization for AEC, which is an extension of the conventional MFCC and is based on the high pass filtering of the acoustic event signal. The proposed front-end has been tested in clean and noisy conditions achieving, in both scenarios, significant improvements with respect to the baseline system.

This paper is organized as follows: in Section 2 the main spectral characteristics of acoustic events are described. Section 3 is devoted to the explanation of the proposed parameterization. Section 4 describes the experiments and results to end with some conclusions and ideas for future work in Section 5.

As it is well known, the spectrograms of speech signals are characterized by the presence of a higher energy in the low-frequency regions of the spectrum. However, in general, non-speech sounds do not show this speech spectral structure. In fact, in many cases, their relevant spectral contents are located in other frequency bands, as it will be shown in the empirical study of the spectral characteristics of several AEs performed in this section.

As an example, Fig. 1
                      represents the spectrograms of two instances of the same acoustic event, Phone ring. Although it is possible to extract conclusions about the spectral nature of this AE by means of the visual inspection of these spectrograms, their high variability due in part to the intrinsic frequency characteristics of the acoustic event and in part to the presence of noise (microphone, environment noise, etc.), motivates us to use an automatic method such as Non-Negative Matrix Factorization (NMF) (Lee and Seung, 1999), which is capable of providing a more compact parts-based representation of the magnitude spectra of the AEs.

Given a nonnegative matrix 
                        
                           V
                           e
                        
                        ∈
                        
                           
                              
                                 ℝ
                              
                           
                           +
                           
                              F
                              ×
                              T
                           
                        
                     , where each column is a data vector (in our case, V
                     e contains the short-term magnitude spectrum of a set of audio signals), NMF approximates it as a product of two nonnegative matrices W
                     e and H
                     e, such that


                     
                        
                           (1)
                           
                              
                                 V
                                 e
                              
                              ≈
                              
                                 W
                                 e
                              
                              
                                 H
                                 e
                              
                           
                        
                     where 
                        
                           W
                           e
                        
                        ∈
                        
                           
                              
                                 ℝ
                              
                           
                           +
                           
                              F
                              ×
                              K
                           
                        
                      and 
                        
                           H
                           e
                        
                        ∈
                        
                           
                              
                                 ℝ
                              
                           
                           +
                           
                              K
                              ×
                              T
                           
                        
                      and F, T and K represent frequency bins, frames and basis components, respectively. This way, each column of V
                     e can be written as a linear combination of the K building blocks (columns of W
                     e), weighted by the coefficients of activation located in the corresponding column of H
                     e. In this work, we are interested on retrieving the matrix W
                     e as it contains the building blocks or Spectral Basis Vectors (SBVs) which encapsule the frequency structure of the data in V
                     e (Smaragdis, 2004).

For each for the acoustic events considered, their SBVs were obtained by applying NMF to the corresponding matrix V
                     e composed by the short-term magnitude spectrum of a subset of the training audio files belonging to this particular class. The magnitude spectra were computed over 20ms windows with a frameshift of 10ms. In total, 364,214 magnitude spectral examples were used for performing NMF, which corresponds to approximately 60min of audio. The NMF matrices were initialized using a multi-start initialization algorithm (Cichocki et al., 2009), in such a way that 10 pairs of uniform random matrices (W
                     e and H
                     e) were generated and the factorization producing the smallest Euclidean distance between V
                     e and (W
                     e
                     H
                     e) was chosen for initialization. Then, these initial matrices were refined by minimizing the KL divergence between the magnitude spectra V
                     e and their corresponding factored matrices (W
                     e
                     H
                     e) using an iterative scheme and the learning rules proposed in Lee and Seung (1999) until the maximum number of iterations (in our case, 200) was reached.

The number of basis vectors K was set taking into account a trade-off between an accurate reconstruction of the magnitude spectra (i.e. the average approximation error between V
                     e and (W
                     e
                     H
                     e) computed over all AEs) and a good visualization of the SBVs. In particular, we used K
                     =23 which corresponds to the case in which the relative change in the average approximation error between two successive numbers of SBVs is less than 2%. It is also worth mentioning that when the number of basis vectors increases, NMF tends to place more and smaller bands in the areas of the spectrum with high energy (i.e. provides more resolution in these regions) and therefore reduces the overall reconstruction error (Bertrand et al., 2008). Nevertheless, for the purpose of this analysis, a larger value of K does not provide relevant information and produces a worse visualization of the SBVs.


                     Fig. 2
                      represents the 23 SBVs of four different non-speech sounds (Laugh, Applause, Phone ring and Spoon/cup jingle) and two different kind of noises (Restaurant and Subway). From this figure, the following observations can be extracted:
                        
                           •
                           The spectral content of the AEs are very different each other, presenting, in general, relevant components in medium-high frequencies. As it is well-known that the spectral components of speech are concentrated in low frequencies, it is possible to infer that the parameterizations designed for speech (as the conventional MFCC) are not suitable enough for representing non-speech sounds.

In all cases, low frequency components are presented to a greater or lesser extent, so this part of the spectrum seems not to very discriminative when comparing different types of AEs.

Comparing the SBVs of the non-speech sounds, it can be observed that large differences can be found in the medium-high part of the spectrum, suggesting that these frequency bands are more suitable (or at least, they cannot be negligible) than the lower part of the spectrum for discriminating between different acoustic events.

Different environment noises present very different spectral characteristics. For example, in the case of Restaurant, most of the frequency content is located in the band below 1kHz, whereas the SBVs of the Subway noise are distributed in two different regions of the spectrum: a low frequency band below 750Hz and a medium-high band of frequencies between 2 and 3kHz. The analysis of other kind of noises (Airport, Babble, Train and Exhibition Hall) yields to similar observations. This way, the distortion produced over the AE signals due to the presence of additive noise will vary considerably depending of the nature of the noise. As a consequence of this fact, some noises will be presumably more harmful than others, producing more noticeable degradations in the performance of the AEC system.

The observation of the SBVs of the different acoustic events shown in Section 2 motivated us to derive an extension of the conventional MFCC more suitable for AEC. As it is well known, MFCC is the most popular feature extraction procedure in speech and speaker recognition and also in audio classification tasks. The basic idea behind the new front-end is to take explicitly into account the special relevance of certain frequency bands of the spectrum into the feature extraction procedure through the modification of the characteristics of the conventional mel-scaled auditory filterbank.

One of the main conclusions drawn from the empirical study in Section 2 is that medium and high frequencies are specially useful for discriminating between different acoustic events. For this reason, this band of frequencies should be emphasized in some way into the parameterization process. This can be accomplished by high-pass filtering the short-term frames of the signal (using the appropriate filter) prior to the application of the auditory filterbank and the cepstral parameters computation. However, in this work, we have adopted a straightforward method which consists of modifying the auditory filterbank by means of the explicit removal of a certain number of the filters placed on the low frequency region of the spectrum. In Fig. 3
                      it can be observed the upper frequency of the complete stopband as a function of the number of removed filters in the auditory filterbank for the Mel scale.

In practice, this procedure consists of setting to a small value the energies corresponding to the outputs of the low-pass filters which are required to be removed. This threshold must be different to zero in order to avoid numerical problems with the logarithm, being, in our particular case, equal to 2−52 (the value of the roundoff level eps in the programming language Matlab).

Once the high-pass filtering is carried out following the procedure previously described and the remaining log filterbank energies are computed, a Discrete Cosine Transform (DCT) is applied over them as in the case of the conventional MFCC yielding to a set of cepstral coefficients.
                        1
                     
                     
                        1
                        Another alternative to this method was considered in which the cepstral coefficients were obtained by applying the logarithm and the DCT exclusively on the outputs of the non-removed filters. The first method was finally adopted in this work because a preliminary experimentation showed that it outperformed this second approach.
                      Finally, it is applied a temporal feature integration technique which consists of dividing the sequence of cepstral coefficients into sliding windows of several seconds length and computing the statistics of these parameters (in this case, mean, standard deviation and skewness) over each window. These segment-based parameters are the input to the acoustic event classifier, which is based on Support Vector Machines (SVM). The whole process is summarized in Fig. 4
                     .

@&#EXPERIMENTS@&#

The database used for the experiments consists of a total of 2114 instances of target events belonging to 12 different acoustic classes: Applause, Cough, Chair moving, Door knock, Door open/slam, Keyboard typing, Laugh, Paper work, Phone ring, Steps, Spoon/cup jingle and Key jingle. The composition of the whole database was intended to be similar to the one used in (Zhuang et al., 2010) and it is shown in Table 1
                        . Audio files were obtained from different sources: websites,
                           2
                        
                        
                           2
                           
                              http://www.freesound.org/.
                         the FBK-Irst database
                           3
                        
                        
                           3
                           
                              http://catalog.elra.info/product_info.php?products_id=1093.
                         (FBK-Irst, 2009) and the UPC-TALP database
                           4
                        
                        
                           4
                           
                              http://catalog.elra.info/product_info.php?products_id=1053.
                         (UPC-TALP, 2008) and were converted to the same format and sampling frequency (8kHz). The total number of segments of 2s length (the window size used for the segment-based features computation as indicated in Section 4.2) in the whole database is 7775. Fig. 5
                         shows the histogram of the number of segments per target event in the database, being the average about 3.75 segments.

Since this database is too small to achieve reliable classification results, we have used a 6-fold cross validation to artificially extend it, averaging the results afterwards. Specifically, we have split the database into six disjoint balanced groups. One different group is kept for testing in each fold, while the remainders are used for training.

For the experiments in noisy conditions, the original audio recordings were contaminated with six different types of noise (Airport, Babble, Restaurant, Train, Exhibition Hall and Subway) obtained from the AURORA framework (Pearce and Hirsch, 2000) at SNRs from 0dB to 20dB with 5dB step. In order to calculate the amount of noise to be added to the clean recordings, the audio and noise powers were calculated following the procedure indicated in Steeneken (1991), which takes into account the non-stationary characteristics of the signals.

The AEC system is based on a one-against-one SVM with RBF kernel on normalized features (Mejía-Navarrete et al., 2011). The system was developed using the LIBSVM software (Chang and Lin, 2011). Concerning SVM training, for each one of the subexperiments, a 5-fold cross validation was used for computing the optimal values of the RBF kernel parameters using clean data (i.e., these parameters were not optimized for noisy conditions). In the testing stage, as the SVM classifier is fed with segmental-based features computed over sliding windows, the classification decisions are made at segment level. In order to obtain a decision for the whole instance (target event level), the classifier outputs of the corresponding windows are integrated using a majority voting scheme, in such a way that the most frequent label is finally assigned to the whole recording (Geiger et al., 2013).

This set of experiments were carried out in order to study the performance of the proposed front-end in clean conditions (i.e. when no noise is added to the original audio files). For the baseline experiments, 12 cepstral coefficients (C
                        1 to C
                        12) were extracted every 10ms using a Hamming analysis window of 20ms long and a mel-scaled auditory filterbank composed of 40 spectral bands. Also, the log-energy of each frame (instead of the zero-order cepstrum coefficient) and the first derivatives (where indicated) were computed and added to the cepstral coefficients. The final feature vectors consisted of the statistics of these short-term parameters (mean, standard deviation and skewness) computed over segments of 2s length with overlap of 1s.


                        Tables 2 and 3
                        
                         show, respectively, the results achieved in terms of the average classification rate at segment level (percentage of segments correctly classified) and at target event level (percentage of target events correctly classified) by varying the number of eliminated low frequency bands in the auditory filterbank. Results for the baseline systems (when no frequency bands are eliminated) are also included. Both tables contain the classification rates for two different set of acoustic parameters, CC (cepstral coefficients+log-energy) and CC+ΔCC (cepstral coefficients+log-energy + their first derivatives).

As can be observed for the CC parameterization, the high pass filtering of the acoustic event signal outperforms the baseline, being the improvements more noticeable when the number of eliminated filters varies from 3 to 7. From Fig. 3, it can be seen that these ranges of eliminated filters roughly correspond to a stopband from 0Hz to 100–275Hz. In particular, the best performance is obtained when the seven first low frequency filters are not considered in the cepstral coefficients computation. In this case, the difference in performance at segment level with respect to the baseline is statistically significant at 95% confidence level. The relative error reduction with respect to the corresponding baseline is around 12.1% at segment level and around 11.2% at target event level.

Similar observations can be drawn for the CC+ΔCC parameterization: best results are obtained when low frequencies (below 100–275Hz) are not considered in the feature extraction process. When comparing to CC for the case in which the first 7 filters are eliminated, it can be observed that CC+ΔCC achieves an improvement about 1.4% absolute and a decrement around 0.5% absolute at, respectively, segment and target event level over CC. However, these differences are not statistically significant.

Other frequency scales (in particular, ERB and Bark) have been experimented observing, as is expected, a similar behavior than the Mel scale with respect to the elimination of low frequency bands. Nevertheless, the Mel scale produces slightly better results than ERB and Bark. More details about these experiments can be found in Ludeña-Choez and Gallardo-Antolín (2013).

In order to perform a more detailed analysis of the AEC system performance, we have analyzed the confusion matrices produced by the baseline and the proposed front-end. As an example, Fig. 6
                        (a) and (b) shows the confusion matrices at segment level for the baseline CC+ΔCC parameters and the modified version of this parameterization in which the first 7 low frequency filters are removed. In both tables, the columns correspond to the correct class, the rows to the hypothesized one and the values in them are computed as the average over the six folds. As can be observed, in the baseline system the less confusable classes (with a classification rate greater than 80%) are Applause, Keyboard typing, Laugh, Paper work and Phone ring, whereas the highest confusable ones are Cough, Chair moving, Door knock and Spoon/cup jingle. In particular, 23% of the Cough instances are classified as Laugh and 12% of the Chair moving and Door knock instances are assigned to the class Steps. It is worth mentioning the large amount of confusions between the human vocal-tract non-speech sounds (i.e. Cough and Laugh) which has been previously reported in the literature (Temko and Nadeu, 2006). In the proposed front-end, the recognition rates of all the acoustic classes increase with the exception of Cough and Key jingle. The acoustic events which are better classified are the same than in the baseline, whereas there are only two AEs with a classification rate less than 70% (Cough and Chair moving). This is because classes Door knock and Spoon/cup jingle reduce significantly their amount of confusions in comparison to the baseline.

In order to study the impact of noisy environments on the performance of the AEC system, several experiments were carried out with six different types of noise (Airport, Babble, Restaurant, Train, Exhibition Hall and Subway) at SNRs from 0dB to 20dB with 5dB step. For the sake of brevity, we only report in this subsection the results for the baseline and for the proposed front-end in the case of CC+ΔCC parameters.


                        Fig. 7
                         represents, for each noise, the average of the relative error reduction with respect to the baseline (noisy conditions without high-pass filtering of the audio signal) computed across the SNRs considered (0dB to 20dB with 5dB step) as a function of the number of removed low frequency filters at both, segment and target event level. The mean of the relative error reduction over all noises and SNRs is also indicated. In order to observe in more detail the behavior of the AEC system with respect to different SNRs and noises, we also show Table 4
                         which contains the classification rates at segment level for the baseline and for the proposed front-end at several selected SNRs (20, 10 and 0dB) for the six noises evaluated and the range of number of eliminated filters from 7 to 12.

Although all the evaluated noises produce a dramatic decrease in the classification rates, results in Table 4 suggest that each type of noise has a different effect over the system performance, being some noises (Airport, Babble, Restaurant and Train) less harmful than others (Exhibition Hall and Subway). This fact can be explained by analysing the spectral characteristics of each noise. In Fig. 2, the SBVs of the Restaurant and Subway noises are represented. In the first case, most of the spectral content is concentrated in low frequencies and, for this reason, this kind of noise affects to lesser extent the most relevant frequencies of the AEs. However, in the second case, part of the SBVs spreads over medium-high frequencies, and therefore, this noise can considerably mask the underlying AEs. Note that, on the one hand, the SBVs of Airport, Babble, Restaurant and Train noises are concentrated in the same range of frequencies than those of the Restaurant noise and, on the other hand, the SBVs of Exhibition Hall and Subway have also similar characteristics.

From results in Fig. 7 it can be observed that with respect to the performance of the proposed front-end, for the Airport, Babble, Restaurant and Train noises, the classification rates at segment level improve considerably when the number of eliminated filters increases, specially for medium-low SNRs (see the corresponding rows labeled as “0dB” and “10dB” in Table 4). In this case, optimal values are obtained when frequencies below 400–500Hz are not considered in the feature computation, which corresponds to the elimination of the 10–11 first low-frequency filters. Similar observations can be drawn by analysing the results at target event level. For the Exhibition Hall and Subway noises, results at segment level suffer a slight variation with respect to the number of removed filters, achieving smaller improvements with respect to the baseline than in the case of the other noises. At target level, the variations are greater, yielding to a decrease in the classification rate in most of the cases for these two noises.

Nevertheless, in average the proposed front-end, when 11 filters are removed, obtains relative error reductions with respect to the baseline (see Fig. 7) about 7.81% at segment level and 7.78% at target event level.

Further experiments were carried out for other scales (Bark and ERB) and the CC parameterization. In all the cases, the results follow similar trends in comparison to the Mel scale and the CC+ΔCC parameters.

@&#CONCLUSION@&#

In this paper, we have presented a new parameterization method for Acoustic Event Classification tasks, motivated by the study of the spectral characteristics of non-speech sounds. First, we have performed an empirical study of the spectral contents of different acoustic events, concluding that medium and high frequencies are specially important for the discrimination between non-speech sounds. Second, from the findings of this study, we have proposed a new front-end for AEC, which is an extension of the MFCC parameterization and is based on the high pass filtering of the acoustic event signal. In practice, the proposed front-end is accomplished by the modification of the conventional mel-scaled auditory filterbank through the explicit elimination of a certain number of its low frequency filters.

The proposed front-end have been tested in clean and noisy conditions and compared to the conventional MFCC in an AEC task. Results support the fact that the high pass filtering of the audio signal is, in general terms, beneficial for the system, showing that the removal of frequencies below 100–275Hz in the parameterization process in clean conditions and below 400–500Hz in noisy conditions, improves significantly the performance of the system with respect to the baseline.

For future work, we plan to use feature selection techniques for automatically determining the most discriminative frequency bands for AEC. Other future lines include the unsupervised learning of auditory filterbanks by means of NMF and the use of the NMF activation coefficients as acoustic features for AEC.

@&#ACKNOWLEDGMENTS@&#

This work has been partially supported by the Spanish Government Grants IPT-120000-2010-24 and TEC2011-26807. Financial support from the Fundación Carolina and Universidad Católica San Pablo, Arequipa (Jimmy Ludeña-Choez) is thankfully acknowledged.

@&#REFERENCES@&#

