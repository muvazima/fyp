@&#MAIN-TITLE@&#Compression of multiple user photo galleries

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           The paper presents a new coding scheme for photo collections of social events.


                        
                        
                           
                           The approach is intended for server-side storage of image databases.


                        
                        
                           
                           Heterogeneous users could contribute to the dataset.


                        
                        
                           
                           The algorithm implements a predictive coder exploiting the visual similarity.


                        
                        
                           
                           The coding efficiency significantly improves with respect to existing solutions.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Image coding

Photo collections

Geotagging

Synchronization

SIFT

Predictive coding

@&#ABSTRACT@&#


               
               
                  The possibility of sharing multimedia contents in easy and ubiquitous way has brought to the creation of multiuser photo albums. Pictures and video sequences taken by different people attending common social events (e.g., concerts and sport competitions) are gathered together into huge sets of heterogeneous multimedia data. These databases require effective compression strategies that exploit the common visual information related to the scene but compensate effectively the differences depending on the acquiring viewpoints, camera models, and acquisition time instants.
                  The paper presents a predictive coding strategy for multi-user photo gallery, which initially localizes each picture in terms of viewpoint, orientation, time, and acquired elements. This information permits ordering all the images in a prediction tree and associates to each of them a reference picture. From this structure, it is possible to build a predictive coding strategy that exploits the redundant elements between the image to be coded and its reference. Experimental results show an average bit rate reduction up to 75% with respect to HEVC Intra low complexity coding.
               
            

@&#INTRODUCTION@&#

According to recent surveys, everyday about one quarter of the world's population shares data and information through social media [1]. Many of these connections involve the exchange of multimedia material such as images and videos. Every 60s 4.7 million posts are uploaded to Tumblr; 277,000 snaps are shared on Snapchat; and more than five million videos are viewed on YouTube.

This mass multimedia distribution has been permitted by the development of multimedia sharing applications and web platforms, together with the availability of camera-enabled connected mobile devices. These two elements allow users to easily take, edit, and share pictures within their network. This sharing is more and more common both during large-scale events (e.g., sports, music), where networked communities of users share contents of common interest, and in personal life, where relatives or friends merge their photo collections to create a unique archive documenting that common experience [2].

The aggregation of huge amounts of multimedia data brings the need for data processing algorithms that permit effective search, retrieval, analysis, selection, and storage of these contents. Some of the approaches presented in the literature aim at automatically selecting and indexing photo collections via a semi-automatic summarization [3,4]. Other strategies are intended to enable a quick browsing of multimedia data exploiting their timestamp values [5]. Ordering images in correct chronological order is an important issue as well [6]. Broilo et al. [2] also show that, by combining timestamp information with visual features, it is also possible to have a correct synchronization when merging photo albums taken by different users. Other approaches ease the organization of pictures by clustering them according to EXIF data [7]. When the uploaded contents are video sequences, parameterizing the activity level of the different regions in a frame permits using this information to synchronize different video streams [8]. Synchonization becomes utterly crucial whenever multiple media need to be aggregated together [9], and it is still nowadays a difficult task to solve in a distributed and heterogeneous scenario [10].

Together with search, synchronization, and retrieval issues, research has recently focused on data compression as well. The design of effective coding algorithms, which are able to store multimedia data at high quality without requiring huge amounts of memory, is still a challenging issue when referred to contents acquired by heterogeneous devices [11]. Current state-of-the-art storage technology adopts non-predictive coding solutions, where each picture/video is compressed independently from the other data available on the server. This solution permits fast retrieval and decoding but requires a significant amount of storage space whenever the population of multimedia contents grows. Predictive coding [12] permits mitigating this effect since common elements can be predicted from previously-stored data. This reduces the amount of required disk storage but implies an increment in computational complexity and number of decodings; accessing a single picture or video implies decoding all the elements that are related to it according to the coding dependencies. As a matter of fact, predictive strategies need to control the dependency structure accurately.

This paper presents a new predictive coding approach for photo collections of the same event generated by multiple users. The proposed solution assumes that multiple photo collections have been generated by different people which assist at the same event during the same period of time from different viewpoints (see Fig. 1
                     ). Pictures are gathered together, and aggregated into a new common photo album. The approach relies on an accurate estimation of the dependencies between different pictures estimated from geometrical (acquisition position and orientation) and temporal properties, as well as from visual content. Note that this task is more difficult than standard temporal prediction operated in video coding since pictures could be quite different due to different acquisition locations, different camera and color distortions, and long time intervals intercurring from one picture to the following one. The proposed solution permits improving the coding performance of state-of-the-art solutions [12] and effectively controls the decoding delay related to the dependencies created by the predictive coding paradigm.

The remainder of the paper is organized as follows. Section 2 overviews some of the related works concerning the compression of multiview images. Section 3 presents the general structure of the coder, while Section 4 describes how images are ordered and references are computed. Section 5 describes the predictive coding scheme that was adopted, and Section 6 reports the coding performance of the proposed solution compared with that of the approach in [12]. Final conclusions are drawn in Section 7.

@&#RELATED WORKS@&#

The compression of multimedia data from heterogeneous devices is one of the newest challenging issues in the data compression world. Most of the approaches presented in the last years have been derived starting from standard predictive coding architectures, like H.264/AVC [13] and the more recent HEVC [14].

In fact, the direct application of video coding schemes does not allow to obtain optimal performances due to the large variance between images acquired at different instants, with different devices, and from different viewpoints. Furthermore, the images do not have an implicit ordering like the frames in a standard video since timestamps of different cameras could have been generated by different unsynchronized clocks [2]. Because of this, finding the order of images in the prediction chain that ensures an efficient compression is a critical issue.

Even if this problem presents some similarities with multi-view image and video coding [15,16], most multi-view coding schemes assume that pictures have been taken at the same time instant from suitable arrays of synchronized cameras with spatially-close viewpoints. Some schemes aim at reducing the inter-view redundancy by some low-pass transformation across the pixels of the different views, e.g., [17] that is based on a 3D-DCT. Motion compensation techniques can be adapted in order to predict one image from another taken from a different viewpoint, as it is done by the H.264 MVC video coding standard [18]. Many extensions have been proposed to this standard [19]. Another interesting possibility is given by the method in [20], which provides scalability and flexibility as well as good coding performances exploiting a multi-dimensional wavelet transform combined with an ad-hoc disparity-compensated view filter (DCVF).

Despite the presence of correlated parts between different images (displaying the same objects or places in the background), there could exist significant differences due to different viewpoints, acquisition times, and light conditions. Strong changes in the viewpoints of the images induce complex transforms between corresponding regions that cannot be handled with simple translational motion models (as in standard video coding). Moreover, the variations in light conditions make the equalization of different images a difficult task. Finally, since the acquisition points and absolute acquisition time are unknown, there is not an obvious ordering of the frames for the prediction structure as in standard video coding.

The availability of matching keypoints [21] between different images permits identifying the common parts and defining a set of image templates to be used as predictors. In [22] a descriptor-based approach is used for the compression of faces. As for videos, the approach in [11] aims at predicting the background data of videos re-using similar parts extracted from other available video sequences. Similarly, the patented solution in [23] codes geotagged video sequences selecting similarly-tagged video segments from an encoding server and predicting the current video with the selected segments as references. The authors of [24] exploit the idea that low frequency content is less affected by the variations between the different images and extract a low frequency template from a set of images that is then used for the prediction of the different images. The issue of how to group and sort the images for optimal compression performances is instead analyzed in [25] where different hierarchical clustering schemes are considered for this task. The issue of multiple image compression has been considered also in the medical imaging field [26]. Finally a recent interesting approach has been presented in [12]. This work is based on the idea of extracting and matching robust features from the images that are then used both to construct the prediction structure using Minimum Spanning Trees and to estimate the geometric transformations between the images in order to compute accurate predictions.

Nevertheless, it is possible to observe that even if standard block-based motion estimation [27] approaches make possible to reuse a huge amount of previous work on video compression, they do not fully exploit the three dimensional nature of multi-view data. The information on the 3D geometric structure of the scene permits the identification of corresponding points in different images; this leads to the localization of the viewpoint associated to the considered image. The locations of the cameras prove to be extremely useful in building an effective prediction [28,29]. In [30] the geometric information is used to support the scalable transmission of multiple views of the same scene. These approaches are nowadays more feasible thanks to the recent availability of effective algorithms that permit an accurate estimation of the scene geometry and camera locations [31,32].

This information, together with time stamp and visual features, can be used to estimate the similarity between different pictures and build a relational graph that maps the coding dependencies between different images. In the following, the coding procedure will be described in detail.

The proposed coder can be synthetically described by the block diagram of Fig. 2
                     . Let us assume that a set of N images {I
                     1
                     
                        c
                        1
                     ,…,
                     I
                     
                        N
                        1
                     
                     
                        c
                        1
                     ,…,
                     I
                     1
                     
                        c
                        
                           M
                        
                     ,…,
                     I
                     
                        N
                        
                           M
                        
                     
                     
                        c
                        
                           M
                        
                     } is aggregated to be compressed, where c
                     
                        q
                      refers to the q-th user/camera that acquired N
                     
                        q
                      images so that N
                     1
                     +…+
                     N
                     
                        M
                     
                     =
                     N.

At first, images are temporally-ordered according to their timestamp values and their visual features into the sequence I
                     1
                     ,…,
                     I
                     
                        N
                     . After this operation, the index c
                     
                        q
                      is omitted since they are no more related to the originating user. The corresponding reconstructed images will be referenced as I
                     
                        r
                        ,1
                     ,…,
                     I
                     
                        r
                        ,
                        N
                     . Images are initially ordered in their coding order according to their dependencies; more precisely, every image I
                     
                        n
                      is associated to a reference image I
                     
                        r
                        ,
                        k
                      (the best prediction reference). This operation is the core element of the coder since the final rate–distortion performance depends on the similarity between current and reference images. This implies that I
                     
                        k
                      must be coded before I
                     
                        n
                      so that the reconstructed image I
                     
                        r
                        ,
                        k
                      can be used as reference.

The dependencies between different images can be modeled by a relational graph, which needs to be converted into a Minimum Spanning Tree. Traversing the tree from root to leaves makes possible to build the coding order for images I
                     
                        n
                      so that all the dependencies are satisfied.

The compression algorithm computes the sets of SIFT keypoints [21] on both images (named S
                     
                        n
                      and S
                     
                        k
                     , respectively) and evaluates the matching descriptors. These correspondences, together with color histograms, are used to generate a reference image I'
                        r
                        ,
                        k
                      that approximates the current image I
                     
                        n
                     . This prediction is then refined by a block matching prediction algorithm, and the residual image E
                     
                        n
                      is compressed using a standard transform coding paradigm. The reconstructed image I
                     
                        r
                        ,
                        n
                      is then stored in the buffer and can be used as reference for the following pictures to be coded. Coefficients, motion vectors, and color compensation information are finally converted into a binary stream that is stored on the server.

In this implementation, the gray blocks of Fig. 2 corresponds to the building blocks of the HEVC coder [14], although any hybrid predictive transform-based video coder can be used. More precisely, the chain of gray blocks corresponds to the coding operations of P frames in HEVC where image I'
                        r
                        ,
                        k
                      has been included in the frame buffer.

The following sections will describe all these operations in detail. More precisely, Section 4 will present how dependencies between the images are computed and how reference frames are identified. Section 5 reports how the reference image is warped, the color is compensated, and the current image is compressed.

The core element in predictive photo compression algorithms is finding good reference images for each picture to be coded.

The approach in [12] exploits the average Euclidean distance of matching SIFT descriptors between couples of images to identify the optimal reference. Every SIFT descriptor s
                     
                        n
                        ,
                        i
                     
                     ∈
                     S
                     
                        n
                      (i
                     =0,…,|
                        S
                     
                        n
                     
                     |−1) can be localized at the pixel coordinates (u
                     
                        n
                        ,
                        i
                     ,
                     v
                     
                        n
                        ,
                        i
                     ) of image I
                     
                        n
                     . Let us assume that it can be matched to the SIFT descriptor s
                     
                        k
                        ,
                        j
                      of image I
                     
                        k
                     : this implies that the distance
                        
                           (1)
                           
                              d
                              
                                 
                                    s
                                    
                                       n
                                       ,
                                       i
                                    
                                 
                                 
                                    s
                                    
                                       k
                                       ,
                                       j
                                    
                                 
                              
                              =
                              ∥
                              
                                 s
                                 
                                    n
                                    ,
                                    i
                                 
                              
                              −
                              
                                 s
                                 
                                    k
                                    ,
                                    j
                                 
                              
                              
                                 ∥
                                 2
                              
                           
                        
                     
                  

is lower than 0.6
                     d(s
                     
                        n
                        ,
                        i
                     ,
                     s
                     
                        k
                        ,
                        h
                     ), ∀
                     
                        h
                     ≠
                     j.

Given the set of matching SIFT descriptor (s
                     
                        n
                        ,
                        i
                     ,
                     s
                     
                        k
                        ,
                        j
                        ⁎(i)) (where j
                     ⁎(i) maps the descriptor index of I
                     
                        n
                      to its matching descriptor index in I
                     
                        k
                     ), the algorithm in [12] computes the average
                        
                           (2)
                           
                              
                                 D
                                 
                                    n
                                    ,
                                    k
                                 
                              
                              =
                              E
                              
                                 
                                    
                                       d
                                       
                                          
                                             s
                                             
                                                n
                                                ,
                                                i
                                             
                                          
                                          
                                             s
                                             
                                                k
                                                ,
                                                
                                                   j
                                                   *
                                                
                                                
                                                   i
                                                
                                             
                                          
                                       
                                    
                                 
                                 
                                    i
                                    =
                                    0
                                    ,
                                    …
                                    ,
                                    
                                       
                                          S
                                          n
                                       
                                    
                                    −
                                    1
                                 
                              
                              ,
                           
                        
                     
                  

which parameterizes the dissimilarity between images I
                     
                        n
                      and I
                     
                        k
                     . In case no matching SIFT exists between I
                     
                        n
                      and I
                     
                        k
                     , D
                     
                        n
                        ,
                        k
                      is set to +∞.

Parameters D
                     
                        n
                        ,
                        k
                      are used to build a relational graph G, where nodes correspond to images, and therefore, image index can be used as node indexes as well. An edge e
                     
                        n
                        ,
                        k
                      links the n-th node/image with the k-th node/image if D
                     
                        n
                        ,
                        k
                     
                     <+∞; in this case, the weight of the edge is set equal to D
                     
                        n
                        ,
                        k
                     .

The solution in [12] clusters the edges according to the D
                     
                        n
                        ,
                        k
                      and runs a modified version of the Minimum Spanning Tree (MST) Kruskal algorithm on G in order to find the set of edges that connect all nodes and have minimum distances.

By traversing the tree from the roots to the leaves it is possible to build ordered sequences of nodes/images. In case the node k is a parent of node n in the MST, the image I
                     
                        k
                      is to be used as prediction reference for image I
                     
                        n
                     , and therefore it needs to be coded first.

Unfortunately, the weight metric D
                     
                        n
                        ,
                        k
                      proves to be ineffective in modeling the dependencies between images whenever pictures are taken from distant viewpoints in complex scenarios. In this case, the number and the location of matching SIFT points play an important role in ordering images. Moreover, time is an important factor in ordering images since temporally-close pictures report similar elements. Since images are taken by heterogeneous cameras, it is important to perform a preliminary synchronization of the different sets. Moreover, accurate localization of the acquiring viewpoint is required in order to match common areas. The following subsections will describe these two operations in detail and how they are combined into the distance metric.

As mentioned in [2], the image timestamp value t(I
                        
                           l
                        
                        
                           c
                           
                              q
                           
                        ) for image I
                        
                           l
                        
                        
                           c
                           
                              q
                           
                         could not be a reliable absolute chronological index since camera clocks could not be perfectly aligned. As a matter of fact, timestamps are a reliable ordering element only for images taken by the same camera. From this premise, an accurate synchronization can be obtained using SIFT descriptors. Although several temporal alignment strategies have been proposed in relation to unsynchronized video sequences, synchronizing different photo collections is more complex since time intervals between one picture and the following one have randomly-variable lengths.

The synchronization strategy implemented by the proposed scheme is derived from the approach in [2]. More complex and accurate approaches can be found in literature (see [8] as an example), but the main purpose of the presented strategy is maximizing the compression performance rather than achieving accurate synchronization. For this reason, a simplified implementation of the solution in [2] was considered.

Without lack of generality, let us assume that if l
                        <
                        b then t(I
                        
                           l
                        
                        
                           c
                           
                              q
                           
                        )<
                        t(I
                        
                           b
                        
                        
                           c
                           
                              q
                           
                        ). Moreover, timestamp is shifted so that ∀
                           c
                        
                           q
                        
                        
                           
                              (3)
                              
                                 
                                    
                                       
                                          t
                                          
                                             
                                                I
                                                l
                                                
                                                   c
                                                   q
                                                
                                             
                                          
                                          ←
                                          t
                                          
                                             
                                                I
                                                l
                                                
                                                   c
                                                   q
                                                
                                             
                                          
                                          −
                                          t
                                          
                                             
                                                I
                                                1
                                                
                                                   c
                                                   q
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          t
                                          
                                             
                                                I
                                                1
                                                
                                                   c
                                                   q
                                                
                                             
                                          
                                          ←
                                          0
                                          .
                                       
                                    
                                 
                              
                           
                        
                     

As a matter of fact, it is possible to organize input images into ordered sequences o
                        
                           c
                           
                              q
                           
                        
                        =(I
                        1
                        
                           c
                           
                              q
                           
                        ,…,
                        I
                        
                           N
                           
                              q
                           
                        
                        
                           c
                           
                              q
                           
                        ) for every camera. These can be progressively merged together into the ordered set O
                        =(I
                        
                           n
                        )
                           n
                           =1,…,
                           N
                         as sketched by Fig. 3
                        .

At first, O simply includes o
                        1. Then, o
                        2 is merged into O by assuming that images in o
                        2 are displaced by a time interval τ. This implies that image I
                        
                           l
                        
                        2 can be included between images I
                        
                           k
                         and I
                        
                           k
                           +1 if t(I
                        
                           l
                        
                        2)+
                        τ
                        ≥
                        t(I
                        
                           k
                        ) and t(I
                        
                           l
                        
                        2)+
                        τ
                        <
                        t(I
                        
                           k
                           +1). For every image I
                        
                           l
                        
                        2 in o
                        2, the algorithm finds a location in O and computes the metrics
                           
                              (4)
                              
                                 
                                    f
                                    
                                       l
                                       ,
                                       τ
                                    
                                 
                                 =
                                 
                                    D
                                    
                                       l
                                       ,
                                       k
                                    
                                 
                                 +
                                 
                                    D
                                    
                                       l
                                       ,
                                       k
                                       +
                                       1
                                    
                                 
                              
                           
                        
                     

where D
                        
                           l
                           ,
                           k
                         is the average distance of matching SIFT descriptors between image I
                        
                           l
                        
                        2 and I
                        
                           k
                         (see Eq. (2)).

The average F
                        
                           τ
                        
                        =
                        E[f
                        
                           l
                           ,
                           τ
                        ]
                           l
                         is computed and considered a fitting index for the time delay τ. The merging between o
                        2 and O is repeated for different values of τ, and in the end, the value τ
                        ⁎ is selected so that
                           
                              (5)
                              
                                 
                                    τ
                                    *
                                 
                                 =
                                 arg
                                 
                                    min
                                    τ
                                 
                                 
                                    F
                                    τ
                                 
                                 .
                              
                           
                        
                     

In this implementation, the parameter τ is varied with granularity equal to 1s. The timestamps t(I
                        
                           l
                        
                        2) are then updated as t(I
                        
                           l
                        
                        2)←
                        t(I
                        
                           l
                        
                        2)+
                        τ
                        ⁎, and images I
                        
                           l
                        
                        2 are then definitely included in O. The operations are repeated for the remaining o
                        
                           c
                           
                              q
                           
                         as long as all the images are included in O. At this point, it is possible to associate to every image I
                        
                           n
                         a timestamp τ
                        
                           n
                        .

After ordering images in chronological order, it is necessary to estimate the best prediction reference for every picture I
                        
                           n
                         in the set. This estimation must take into consideration the time information, the acquisition viewpoint and orientation, how many and which points of the 3D scene have been acquired.

To this purpose it is necessary to estimate the geometry of the recorded scene and the camera locations. This task can be accomplished by exploiting structure-from-motion (SfM) algorithms for unordered set of cameras (see [32,31]). The input data are the images I
                        
                           n
                        ; the outputs are the viewpoint location P
                           n
                         and orientation v
                           n
                         of the n-th image, and a set of 3D points P in the scene which can mapped to pixels (u
                        
                           n
                        (P),
                        v
                        
                           n
                        (P)) in the image I
                        
                           n
                        . This information permits computing the best reference image with more accuracy with respect to the approach in [12].

For every image I
                        
                           n
                        , it is possible to compute a feature array
                           
                              (6)
                              
                                 
                                    g
                                    n
                                 
                                 =
                                 
                                    
                                       
                                          P
                                          n
                                       
                                       
                                          v
                                          n
                                       
                                    
                                 
                                 ,
                              
                           
                        
                     

which can be mapped into the dissimilarity metrics D'
                           n
                           ,
                           k
                         between couples of images I
                        
                           n
                        
                        ,
                        I
                        
                           k
                         as
                           
                              (7)
                              
                                 D
                                 
                                    '
                                    
                                       n
                                       ,
                                       k
                                    
                                 
                                 =
                                 ∥
                                 
                                    g
                                    n
                                 
                                 −
                                 
                                    g
                                    k
                                 
                                 
                                    ∥
                                    2
                                 
                                 .
                              
                           
                        
                     

According to the values D'
                           n
                           ,
                           k
                        , images are clustered into different sets B
                        
                           m
                         grouping images that have been acquired from the same location with the same viewpoint. This does not necessarily imply that images have been taken by the same users since cameras could be moving across the scene.

Given the set B
                        
                           m
                        , it is possible to compute the MST on the directed graph whose nodes correspond to images in B
                        
                           m
                        . The weight w
                        
                           n
                           ,
                           k
                         of edge e
                        
                           n
                           ,
                           k
                         linking the n-th image/node with the k-th one is set as
                           
                              (8)
                              
                                 
                                    w
                                    
                                       n
                                       ,
                                       k
                                    
                                 
                                 =
                                 D
                                 
                                    '
                                    
                                       n
                                       ,
                                       k
                                    
                                 
                                 
                                    
                                       1
                                       −
                                       exp
                                       −
                                       
                                          
                                             
                                                τ
                                                n
                                             
                                             −
                                             
                                                τ
                                                k
                                             
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

where geometrical distance is combined with the temporal distance as well.

Traversing the obtained MST makes possible to estimate the prediction reference for every image in the set.

After computing the best reference image for every picture I
                     
                        n
                      in the set, it is possible to adopt a predictive coding paradigm that permits minimizing the final bit rate.

Although I
                     
                        k
                      is the best reference for I
                     
                        n
                      according to geometrical and temporal parameters, some additional transformations are required in order to maximize the coding performance. Image I
                     
                        k
                      needs to be warped so that pixels corresponding to the same 3D point in I
                     
                        k
                      and I
                     
                        n
                      lie as close as possible. Moreover, color statistics, which is affected by the camera model, needs to be equalized as well. Final adjustment is then performed using a block-based motion estimation strategy.

In the following, these steps will be described in detail.

After selecting the best image reference I
                        
                           r
                           ,
                           k
                         for the current image I
                        
                           n
                         to be coded, the coder tries to predict I
                        
                           n
                         using the data contained in I
                        
                           r
                           ,
                           k
                        . To this purpose, at first the coder estimates the affine transformation T
                        
                           n
                           ,
                           k
                         that maps pixels of image I
                        
                           n
                         to pixels of image I
                        
                           r
                           ,
                           k
                        .

The function T
                           
                              n
                              ,
                              k
                            is obtained by solving the overdetermined linear system generated from matching couples of SIFT keypoints. Assuming that the i-th SIFT descriptor in I
                           
                              n
                            matches the j-th descriptor in I
                           
                              k
                           , it is possible to write
                              
                                 (9)
                                 
                                    
                                       
                                          u
                                          
                                             k
                                             ,
                                             j
                                          
                                       
                                       
                                          v
                                          
                                             k
                                             ,
                                             j
                                          
                                       
                                    
                                    =
                                    
                                       T
                                       
                                          n
                                          ,
                                          k
                                       
                                    
                                    
                                       
                                          u
                                          
                                             n
                                             ,
                                             i
                                          
                                       
                                       
                                          v
                                          
                                             n
                                             ,
                                             i
                                          
                                       
                                    
                                    
                                    ∀
                                    
                                       s
                                       
                                          n
                                          ,
                                          i
                                       
                                    
                                    ,
                                    
                                       s
                                       
                                          k
                                          ,
                                          j
                                       
                                    
                                    
                                    matching
                                    .
                                 
                              
                           
                        

It is possible to estimate the two-dimensional homography that approximates the relation of Eq. (9) using SVD and RANSAC. Homography T
                           
                              n
                              ,
                              k
                            is then used to generate an image I
                           
                              r
                              ,
                              k
                           
                           
                              t
                            such that
                              
                                 (10)
                                 
                                    
                                       I
                                       k
                                       t
                                    
                                    
                                       u
                                       v
                                    
                                    =
                                    
                                       I
                                       
                                          r
                                          ,
                                          k
                                       
                                    
                                    
                                       
                                          
                                             T
                                             
                                                n
                                                ,
                                                k
                                             
                                          
                                          
                                             u
                                             v
                                          
                                       
                                    
                                 
                              
                           
                        

i.e., I
                           
                              k
                           
                           
                              t
                            is the picture I
                           
                              r
                              ,
                              k
                            warped on the viewpoint of I
                           
                              n
                           .

It is possible to improve the performance of the warping operations by estimating different homographies for different regions of the image. The mapping function T
                           
                              n
                              ,
                              k
                            should operate differently for static and background elements with respect to foreground and moving objects.

Assuming that the i-th SIFT descriptor s
                           
                              n
                              ,
                              i
                            is located at (u
                           
                              n
                              ,
                              i
                           ,
                           v
                           
                              n
                              ,
                              i
                           ) and corresponds to a three-dimensional point P
                              n
                              ,
                              i
                           , it is possible to evaluate the distances
                              
                                 (11)
                                 
                                    
                                       δ
                                       
                                          n
                                          ,
                                          i
                                       
                                    
                                    =
                                    ∥
                                    
                                       P
                                       n
                                    
                                    −
                                    
                                       P
                                       
                                          n
                                          ,
                                          i
                                       
                                    
                                    
                                       ∥
                                       2
                                    
                                    .
                                 
                              
                           
                        

These values permit clustering points (u
                           
                              n
                              ,
                              i
                           ,
                           v
                           
                              n
                              ,
                              i
                           ) into two possible classes including foreground and background. In order to perform the clustering, a simple K-means strategy is adopted.

As a result, it is possible to compute a different transformation T
                           
                              n
                              ,
                              k
                            for background and foreground elements. In the following, notation will reference the first with T
                           
                              n
                              ,
                              k
                           
                           1 and the second with T
                           
                              n
                              ,
                              k
                           
                           2. The obtained images are named I
                           
                              k
                           
                           
                              t1 and I
                           
                              k
                           
                           
                              t2
                           .
                        

Then, since I
                        
                           n
                         and I
                        
                           r
                           ,
                           k
                         could be taken at different time instants (and with different cameras), colors and illumination differences between the two images need to be compensated. In this implementation, the coder performs a simple gamma correction of the three color components (independently). More precisely, naming γ
                        
                           a
                           ,
                           k
                         the gamma parameter for the color component a of image I
                        
                           k
                        
                        
                           t
                        , the coder tries to find the optimal γ
                        
                           a
                           ,
                           k
                         value that permits equalizing the histogram of values for component a of I
                        
                           k
                        
                        
                           t
                         with respect to that of I
                        
                           n
                        . After this compensation the image I
                        
                           k
                        
                        
                           t
                         is then referenced as I'
                           k
                        .

In the multiple transformation case, these operations are repeated for I
                        
                           k
                        
                        
                           t1 and I
                        
                           k
                        
                        
                           t2: the compensated images will then be called I'
                           k
                        
                        1 and I'
                           k
                        
                        2.

Note that parameters γ
                        
                           a
                           ,
                           k
                        , T
                        
                           n
                           ,
                           k
                        , and the index k need to be specified in the bit stream in order to allow a correct decoding. Nevertheless, they can be characterized by a few bytes (29) that have a neglibile impact on the final coded rate for the current image.

After generating the image reference I'
                           k
                        , it is possible to predict the current image I
                        
                           n
                        . Unfortunately, since global homography T
                        
                           n
                           ,
                           k
                         is computed and applied on the whole image (and the geometry of the framed scene can be quite complex), displacements still exist in some image regions between the corresponding pixels of I
                        
                           k
                         and I
                        
                           n
                        . This spatial translation can be compensated using the algorithms for motion estimation of the coder HEVC [14].

To this purpose, images I
                        
                           n
                         and I'
                           k
                         are first converted from the RGB 4:4:4 color space to the YUV 4:2:0 color space. Then, image I
                        
                           n
                         is motion compensated using I'
                           k
                         as reference frame and generating the image I
                        
                           k
                        
                        
                           m
                         via a set of motion vectors. The generated image difference E
                        
                           n
                        
                        =
                        I
                        
                           n
                        
                        −
                        I
                        
                           k
                        
                        
                           m
                         presents a limited energy which is concentrated on those parts that could not be effectively predicted.

The residual image E
                        
                           n
                         is compressed via a transform based coder. To this purpose, the coding solutions designed in the HEVC standard were adopted for the compression of P images. The components of E
                        
                           n
                         are block-wise transformed and then quantized. The loss of information in the quantization process is controlled by the quantization parameter (QP).

The quantized coefficients are then converted into a binary stream, together with motion vectors, and the parameters for image transformation (γ
                        
                           a
                           ,
                           k
                        , T
                        
                           n
                           ,
                           k
                        , and the index k).

The image E
                        
                           n
                         is then reconstructed via inverse quantization and transform obtaining image E
                        
                           r
                           ,
                           n
                        . This permits recovering the final coded image I
                        
                           r
                           ,
                           n
                        
                        =
                        E
                        
                           r
                           ,
                           n
                        
                        +
                        I
                        
                           k
                        
                        
                           m
                        , which could be used as reference for the following pictures to be coded. Motion estimation and transform coding operations (gray blocks in Fig. 2) were implemented using the building blocks of the HEVC coder [14].

In case images I'
                           k
                        
                        1 and I'
                           k
                        
                        2 are used, it is possible to include them in the reference frame buffer and perform a multiple reference block-based prediction. In this case, the reference frame is chosen adaptively for every prediction unit according to the rate–distortion performance (see [14] for more details).

@&#EXPERIMENTAL RESULTS@&#

In order to test the coding performance of the proposed approach, different photo collections were generated from a set of unsynchronized and uncalibrated images and video sequences recording a common scene or event.

The coding performance was evaluated measuring the average peak-signal-to-noise ratio (PSNR), the average bit rate, and the coding time. Adopting the same metrics employed in the performance evaluations on HEVC [14], the Bjontegaard rate distortion metrics ΔPSNR (dB) and ΔRate (bpp) were employed as well [33]. The reference rate–distortion curve was the one provided by HEVC Intra coding. At first static scenes were considered, and the temporal dimension did not come into play. This scenario is represented by datasets portello [38] and notre_dame, which are described in Table 1
                     . In this case, τ
                     
                        n
                      is constant since common elements belong to the landscape and no actions are taking place.

Experimental results (see Table 2
                     ) show that the proposed approach permits obtaining a more effective compression with respect to the solution by Shi et al. [12]. This is caused by the fact that the approach in [12] does not take into account the number of SIFT matching and their locations within the image. Therefore, it is possible that images are paired according to the matching quality of a limited region of the picture to be coded. This could reduce the overall efficiency of the prediction. The proposed solution performs better since images are paired according to the position and the orientation of the camera. On dataset portello, the Bjontegaard ΔPSNR improves of 3.55dB, while the ΔRate decreases of 75%.

Note that the performances on dataset notre_dame are similar. This is mainly due to the fact that the location and the viewing directions for all the images in the dataset do not change significantly, and therefore, clustering them does not bring significant differences.

Additional tests were performed considering a dynamic event attended by different users and documenting it taking multiple pictures. To this purpose, the video sequences presented in the work [36] and available at [37] were adopted. Each video is associated to a different user/camera, and therefore, it is referred by index c
                     
                        q
                     . Some frames I
                     
                        l
                     
                     
                        c
                        
                           q
                        
                      from each video are randomly-sampled and included into the set to be coded as still images. Note that sampling time instants can differ for the different sequences; as a matter of fact, image database includes unsynchronized and uncalibrated pictures of the attended event. These datasets are referenced with the names magician and juggler.

In addition to these, further tests involved the image galleries 1–7 from the London dataset in the MediaEval 2014 SEM database [34]. In this case, 192 images were considered which were taken at different locations by different users. The dataset is referenced with name london2012. Finally, the last dataset reports 200 frames from a set of 10 Youtube videos reporting scenes and scores from World Cup Soccer Tournament 2014; 10 different footbal matches were considered. Each action was manually segmented from each video (which included more than one scene and footbal match). Each action was taken from 3 to 4 different viewing angles. This last dataset was named worldcup2014.

All the details about datasets are reported in Table 1.

The proposed coder (with both single and multiple transformations) is compared to the approach in [12].


                     Fig. 4
                      reports the PSNR (in dB) versus the average rate (expressed in bit-per-pixel) for the dataset magician. More precisely, Fig. 4a reports the rate–distortion curves computed on the whole set of coded images, while Fig. 4b reports the rate–distortion curves for the common subset of predicted images only.

It is possible to notice that PSNR-vs.-rate curves for the proposed solution lie over that of the approach in [12]. This difference is enhanced whenever considering P frames only. Moreover, it is also possible to notice that using multiple transformations permits improving the PSNR of 0.5dB over the approach [12].

The results displayed by Fig. 5
                      report the performance on the juggler dataset. In this case, the compression efficiency of the algorithm by Shi et al. is improved up to 1dB (see Fig. 5b). It is possible to notice that the multiple transformation approach does not bring significant improvements in the final rate–distortion curve. In this case, the scene is much simpler than in the magician dataset, and therefore, a single function T
                     
                        n
                        ,
                        k
                      is enough.

Similar results were obtained with larger datasets, where each gallery includes more than one action: localization data and synchronization permitted to maximize the coding performance. Fig. 6
                      reports the rate distortion performance of the proposed single transformation strategy on london2012 and worldcup2014 datasets with and without synchronization data (i.e., in the latter case considering all the images taken at the same time instant). This latter configuration is referenced with the label Proposed w/out time. It is possible to notice that the performance of the solution by [12] is improved in this case as well.

From these experimental results, it is possible to conclude that, even for a server-based offline strategy, visual similarity in a distributed and heterogeneous scenario is quite difficult to measure without a significant computational effort. The solution by Shi et al. [12] is simply based on image similarity measured via SIFT descriptors. Rate–distortion performances show that the coding performance can be significantly improved by adding location and time information. The matching of SIFT descriptors is more difficult when images are taken by different users, at different times and locations, with different weather and light conditions. Moreover, keypoint locations are usually sparse. This limits the effectiveness of this approach in characterizing the visual dependency between images. The information about locations and orientations of cameras presents some advantages since it permits identifying which part of the background scene is acquired and predict them. Nonetheless, when all the users are attending a common social event, common foreground and moving objects are present as well. Unfortunately, the dependencies among these cannot be easily estimated. SIFT descriptors fail to match since viewing angles for foreground objects is usually larger than those of background objects (SIFT descriptors are robust for angles lower than 30°). Moreover, motion could be quite complex.

Synchronizing the acquisition time of the different image sets composing the album permits improving the coding performance since it makes possible to align the different pictures that reproduce a specific action and to maximize the amount of common elements between the different pictures. In terms of timing, synchronization does not need to be highly accurate for the proposed approach since it is simply required to maximize the number of visually-similar elements, i.e., maximizing the coding performance.

In addition, it is also worth evaluating the decoding delay introduced by the predictive scheme. Table 3
                      reports the average number of decodings required to download a single image for different algorithms and datasets. It is possible to notice that the proposed solution permits minimizing the decoding delay with respect to the other schemes.

Note that the computational complexity of coding for the proposed approach is approximately the same with respect to [12] since the SIFT matching routine and the predictive compression strategy involve similar operations. Nevertheless, in the proposed strategy a 3D model of the scene was computed via the Bundler software [31] in 570s for portello dataset, 584s for notre_dame dataset, 382s for juggler dataset, and 233s for magician dataset (on a Intel Quad-core i5 1.7GHz with 16GB RAM). Note that, once the prediction order, transformation T
                     
                        n
                        ,
                        k
                     , and color compensation are computed, there is no need to keep the 3D model in memory. As a matter of fact, the computation of the 3D model does not imply an extra bit rate to be stored.

@&#CONCLUSIONS@&#

The paper proposes a novel coding scheme for the challenging compression of multiuser photo collections. The 3D information estimated from matched keypoints is combined with the temporal coordinates and used to find the similarities between images, while the critical issue of finding the correct ordering has been solved through a graph-based optimization scheme. Experimental results prove how the proposed approach is able to obtain good performances even on images taken with different cameras and in different lighting conditions. Further research will focus on designing locally-optimized warping transformations.

@&#ACKNOWLEDGMENTS@&#

The work has been supported by the Robotic 3D and by the CloudVision projects (prot. CPDA142399), funded by the University of Padova, Italy.

@&#REFERENCES@&#

