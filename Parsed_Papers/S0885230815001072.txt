@&#MAIN-TITLE@&#Ensemble of deep neural networks using acoustic environment classification for statistical model-based voice activity detection

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           We develop the voice activity detection based on statistical model.


                        
                        
                           
                           The DNNs are used for the voice activity detection.


                        
                        
                           
                           Ensemble of the DNN is devised for different noise environments.


                        
                        
                           
                           A separate DNN is built to detect the current environment.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Voice activity detection

Statistical model

Acoustic environment classification

Deep neural network

Ensemble

@&#ABSTRACT@&#


               
               
                  In this paper, we investigate the ensemble of deep neural networks (DNNs) by using an acoustic environment classification (AEC) technique for the statistical model-based voice activity detection (VAD). From an investigation of the statistical model-based VAD, it is known that the traditional decision rule is based on the geometric mean of the likelihood ratio or the support vector machine (SVM), which is a shallow model with zero or one hidden layer. Since the shallow models cannot take an advantage of the diversity of the space distribution of features, in the training step, we basically build the multiple DNNs according the different noise types by employing the parameters of the statistical model-based VAD algorithm. In addition, the separate DNN is designed for the AEC algorithm in order to choose the best DNN for each noise. In the on-line noise-aware VAD step, the AEC is first performed on a frame-by-frame basis using the separate DNN so the a posteriori probabilities to identify noise are obtained. Once the probabilities are achieved for each noise, the environmental knowledge is contributed to allow us to combine the speech presence probabilities which are derived from the ensemble of the DNNs trained for the individual noise. Our approach for VAD was evaluated in terms of objective measures and showed significant improvement compared to the conventional algorithm.
               
            

@&#INTRODUCTION@&#

Voice activity detection (VAD) which classifies the period of speech and non-speech from an input speech signal is an essential part of speech signal processing in tasks such as speech recognition, speech enhancement, and efficient variable-rate speech coding. Among the various VAD methods, we focus on a statistical model-based approach, which was originated from the work of Ephraim and Malah Ephraim and Malah (1984) for speech enhancement due to its high detection accuracy as well as low computational complexity. Sohn et al. 
                     Sohn et al. (1999) devised the VAD based on a Gaussian statistical model by employing the decision rule based on the geometric mean of the likelihood ratio (LR), which can be considered as a heuristic way. The novelty of the statistical model-based VAD was extensively reviewed, so that further improved methods based on the LR test according to two hypotheses for speech presence and absence have been presented in many studies. Kang et al. (2008) proposed a decision rule based on a discriminative weight training method which fuses the LR through a linear weighed combination in which weights are optimized by the minimum classification error (MCE) training method based on the gradient descent algorithm. Yu and Hansen (2010) further improved the method of Kang et al. (2008) by applying a multiple observation technique to the decision rule which reflects the LR of not only the current frame, but also previous few frames. On the other hand, Jo et al. (2009) found that the LR corresponding to speech absence and presence cannot be separated by a linear decision function such as the geometric mean and a linear weighted combination due to its considerable class overlap in the feature space. They thus applied a support vector machine (SVM) to the statistical model-based VAD as a robust decision function since the SVM makes it possible to build an optimal hyper-plane among many possible hyper-planes separating the two classes of speech absence and presence and especially has the advantages on addressing nonlinear properties of the input feature vector by applying the kernel function. However, the SVM cannot be considered as an effective method especially for the statistical model-based VAD since it cannot fully take the advantage of multiple features due to its shallow properties. Shallow architecture lacks the ability to take into account the diversity of the nonlinear distribution of the feature vector since it has zero or at most one hidden layer.

Recently, the deep belief network (DBN) has been proposed, by Hinton and Salakhutdinov (2006), as a powerful hierarchical generative model not only for feature representation but also for classification by taking multiple-layered deep architecture. It is noted that the DBN is known to avoid the poor local-minima and over-fitting by the greedy layer-wise unsupervised learning process called pre-training. The superiority of the DBN compared to conventional shallow architecture-based machine learning techniques including the SVM has been reviewed, and thus the DBN has been successfully applied to various pattern recognition applications such as speech recognition (Mohamed et al., 2009, 2012; Hinton et al., 2012) and hand-written character recognition (Hinton and Salakhutdinov, 2006; Lee et al., 2007) as a state-of-the-art technique. The key idea behind the method of Zhang and Wu (2013) is to extract new features by transferring the acoustic features through deep nonlinear hidden layers since the deep model can combine multiple features in a nonlinear way to discover the regularity among the features. Thereafter, they proposed the denoising deep neural network (DNN)-based VAD approach (Zhang and Wu, 2013) which differs from Zhang and Wu (2013) in that it tries to minimize the reconstruction cross-entropy loss between the input noisy feature and its corresponding clean feature at the target while (Zhang and Wu, 2013) uses the noisy feature at the target in the pre-training step. In addition, Ryant et al. (2013) proposed the DNN-based VAD for web video such as YouTube by using 13 normalized MFCCs as feature vector. However, the DNN-based VAD is far from fully investigated yet in the area of the statistical model-based VAD under various noise environments, which is a main topic of interest in this study.

Before presenting our work, it is worthwhile to mention the performance of the VAD by incorporating the acoustic environment classification (AEC) technique since it is useful to build and use a different DNN scheme for various acoustic environments. In the literature, Sangwan et al. (2007) proposed a technique to use the SVM for environmentally aware VAD and to find the best operating point for the competitive Neyman–Pearson VAD. Gaussian mixture model (GMM), in the method of Choi and Chang (2012), was applied to perform the AEC for speech enhancement to adaptively select the optimal parameters for a given noise type. Recently, Xia and Bao (2014) applied the GMM-based noise classification to speech enhancement, which is different from the work of Choi and Chang (2012) in that the weighted denoising auto-encoder (WDA) model is chosen among a number of models which were trained for each kind of noise in the training data-set. Unfortunately, these methods are restricted in tracking the subtle changes in acoustics, which cause nonlinear change in the feature space since the acoustic features are extracted through the fixed filters such as a Mel-filter bank or linear prediction filter. In addition, both the SVM and GMM belong to the shallow method, and they thus cannot represent the diversity of the feature yet in this AEC task. We note that the DNN can be successfully used in representing raw speech data to encapsulate the underlying information associated with various acoustic scenes.

In this paper, we develop the statistical model-based VAD by employing the DNN with a multiple layer deep architecture as a novel decision rule in classifying the signal into speech or noise. The first step is to establish the baseline of the DNN by which the improved speech presence probability (SPP) is obtained based on the novel features of the statistical model-based VAD, namely the LR, the a priori signal-to-noise ratio (SNR), and the a posteriori SNR. As the key point that contributes to the success of DNN-based VAD, distinct DNNs according to different noise types are established via the separate training. Then different SPPs which correspond to a number of total noises are derived. As for the environmental awareness, an independent DNN module is constructed by a separate training process to offer the probabilities of occurrence for each noise type. The probability of occurrence for each noise environment is calculated to combine the SPPs, derived from the multiple DNNs and thus the final decision for VAD is obtained by comparing the combined SPP with a given threshold. The proposed VAD was evaluated in terms of an objective measure and found to have better results than the conventional SVM-based algorithm.

We briefly review the statistical model-based VAD. It is assumed that a noise signal d(t) is added to a speech signal x(t) in a time domain, with their sum being denoted as the noisy speech signal y(t), that is
                        
                           (1)
                           
                              y
                              (
                              t
                              )
                              =
                              x
                              (
                              t
                              )
                              +
                              d
                              (
                              t
                              )
                              .
                           
                        
                     After taking a short-time Fourier transform (STFT) of the noisy speech y(t), we then have the following in the time-frequency domain as
                        
                           (2)
                           
                              Y
                              (
                              k
                              ,
                              n
                              )
                              =
                              X
                              (
                              k
                              ,
                              n
                              )
                              +
                              D
                              (
                              k
                              ,
                              n
                              )
                           
                        
                     where Y(k, n), X(k, n) and D(k, n) denote the STFT coefficients of the noisy speech signal, clean speech signal and additive noise signal, respectively, where k and n denote the frequency-band index (k
                     =0, 1, …, L
                     −1) and frame index (n
                     =0, 1, …), respectively. Given two hypotheses, H
                     0 and H
                     1, which respectively indicate speech absence and presence, it is assumed that 
                        
                           
                              (3)
                              
                                 
                                    H
                                    0
                                 
                                 :
                                 
                                 
                                    speech
                                       
                                    absence
                                 
                                 :
                                    
                                 Y
                                 (
                                 k
                                 ,
                                 n
                                 )
                                 =
                                 D
                                 (
                                 k
                                 ,
                                 n
                                 )
                              
                           
                           
                              (4)
                              
                                 
                                    H
                                    1
                                 
                                 :
                                 
                                 
                                    speech
                                       
                                    presence
                                 
                                 :
                                    
                                 Y
                                 (
                                 k
                                 ,
                                 n
                                 )
                                 =
                                 X
                                 (
                                 k
                                 ,
                                 n
                                 )
                                 +
                                 D
                                 (
                                 k
                                 ,
                                 n
                                 )
                                 .
                              
                           
                        
                      With the complex Gaussian probability distribution assumption, the probability density functions (PDFs) conditioned on the two hypotheses of H
                     0 and H
                     1 are given by 
                        
                           
                              (5)
                              
                                 p
                                 (
                                 Y
                                 (
                                 k
                                 ,
                                 n
                                 )
                                 |
                                 
                                    H
                                    0
                                 
                                 )
                                 =
                                 
                                    1
                                    
                                       π
                                       
                                          λ
                                          d
                                       
                                       (
                                       k
                                       ,
                                       n
                                       )
                                    
                                 
                                 ·
                                 exp
                                 
                                    
                                       
                                          −
                                          
                                             
                                                |
                                                Y
                                                (
                                                k
                                                ,
                                                n
                                                )
                                                
                                                   |
                                                   2
                                                
                                             
                                             
                                                
                                                   λ
                                                   d
                                                
                                                (
                                                k
                                                ,
                                                n
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                           
                              (6)
                              
                                 p
                                 (
                                 Y
                                 (
                                 k
                                 ,
                                 n
                                 )
                                 |
                                 
                                    H
                                    1
                                 
                                 )
                                 =
                                 
                                    1
                                    
                                       π
                                       (
                                       
                                          λ
                                          x
                                       
                                       (
                                       k
                                       ,
                                       n
                                       )
                                       +
                                       
                                          λ
                                          d
                                       
                                       (
                                       k
                                       ,
                                       n
                                       )
                                       )
                                    
                                 
                                 ·
                                 exp
                                 
                                    
                                       
                                          −
                                          
                                             
                                                |
                                                Y
                                                (
                                                k
                                                ,
                                                n
                                                )
                                                
                                                   |
                                                   2
                                                
                                             
                                             
                                                
                                                   λ
                                                   x
                                                
                                                (
                                                k
                                                ,
                                                n
                                                )
                                                +
                                                
                                                   λ
                                                   d
                                                
                                                (
                                                k
                                                ,
                                                n
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                      where λ
                     
                        x
                     (k, n) and λ
                     
                        d
                     (k, n) denote the variances of the clean speech and of the noise for the individual frequency band, respectively. The LR of the kth frequency-band is derived as
                        
                           (7)
                           
                              Λ
                              (
                              Y
                              (
                              k
                              ,
                              n
                              )
                              )
                              ≡
                              
                                 
                                    p
                                    (
                                    Y
                                    (
                                    k
                                    ,
                                    n
                                    )
                                    |
                                    
                                       H
                                       1
                                    
                                    )
                                 
                                 
                                    p
                                    (
                                    Y
                                    (
                                    k
                                    ,
                                    n
                                    )
                                    |
                                    
                                       H
                                       0
                                    
                                    )
                                 
                              
                              =
                              
                                 1
                                 
                                    1
                                    +
                                    ξ
                                    (
                                    k
                                    ,
                                    n
                                    )
                                 
                              
                              ·
                              exp
                              
                                 
                                    
                                       
                                          
                                             γ
                                             (
                                             k
                                             ,
                                             n
                                             )
                                             ξ
                                             (
                                             k
                                             ,
                                             n
                                             )
                                          
                                          
                                             1
                                             +
                                             ξ
                                             (
                                             k
                                             ,
                                             n
                                             )
                                          
                                       
                                    
                                 
                              
                           
                        
                     where ξ(k, n)=
                     λ
                     
                        x
                     (k, n)/λ
                     
                        d
                     (k, n) and γ(k, n)=|Y(k, n)|2/λ
                     
                        d
                     (k, n) which are called the a priori SNR and the a posteriori SNR, respectively. The a posteriori SNR γ(k, n) is obtained by λ
                     
                        d
                     (k, n), which is updated during the periods of speech absence. Indeed, the long-term smoothed noise power spectrum of the background noise is estimated by the soft decision method (Chang and Kim, 2001) at the previous frame and only during speech pauses. The a priori SNR ξ(k, n) is estimated based on the well-known decision-directed approach (Sohn et al., 1999) as follows:
                        
                           (8)
                           
                              
                                 
                                    ξ
                                    ˆ
                                 
                              
                              (
                              k
                              ,
                              n
                              )
                              ≡
                              α
                              
                                 
                                    |
                                    
                                       
                                          X
                                          ˆ
                                       
                                    
                                    (
                                    k
                                    ,
                                    n
                                    −
                                    1
                                    )
                                    
                                       |
                                       2
                                    
                                 
                                 
                                    
                                       λ
                                       d
                                    
                                    (
                                    k
                                    ,
                                    n
                                    −
                                    1
                                    )
                                 
                              
                              +
                              (
                              1
                              −
                              α
                              )
                              P
                              [
                              γ
                              (
                              k
                              ,
                              n
                              )
                              −
                              1
                              ]
                           
                        
                     where 
                        
                           
                              X
                              ˆ
                           
                        
                        (
                        k
                        ,
                        n
                        −
                        1
                        )
                     , the speech spectral amplitude estimate obtained in the previous frame, is obtained by a minimum mean square error (MMSE) estimator (Ephraim and Malah, 1984) and P[z]=
                     z if z
                     ≥0 and P[z]=0 otherwise. Also, α is a weight that is usually determined in the range (0.95, 0.99) (Ephraim and Malah, 1984).

The final decision in the statistical model-based VAD has been accomplished by taking the geometric mean of the LR computed from the individual frequency-bands and it is obtained by
                        
                           (9)
                           
                              log
                              Λ
                              (
                              n
                              )
                              =
                              
                                 1
                                 L
                              
                              
                                 ∑
                                 
                                    k
                                    =
                                    0
                                 
                                 
                                    L
                                    −
                                    1
                                 
                              
                              
                                 log
                                 Λ
                                 (
                                 k
                                 ,
                                 n
                                 )
                              
                              
                                 
                                    
                                       ≷
                                       
                                          
                                             H
                                             0
                                          
                                       
                                    
                                 
                                 
                                    
                                       H
                                       1
                                    
                                 
                              
                              η
                           
                        
                     where L and η denote the total number of frequency-bands and a given threshold for speech detection, respectively. As in (9), an input frame is classified as the period of speech if the geometric mean of the LR is greater than the given threshold η and the period of noise otherwise.

Since the shallow machine learning techniques such as SVM and geometric mean as in (9) are not desirable for the statistical model-based VAD as we mentioned earlier, the DNN, a powerful hierarchical generative model, is herewith applied to the decision function for the statistical model-based VAD.

To fully take a consideration of nonlinear distribution of input features, the key parameters of the statistical model-based VAD such as the a priori SNR, the a posteriori SNR, and the LR are employed into the input layer of the DNN and go through the multiple hidden layers in the training and test steps. Specifically, as shown in Fig. 1
                        , the microphone input signal is transformed to the STFT coefficients and then the a priori SNR, the a posteriori SNR, and the LR are estimated as in (7) and (8). The input feature vector Z is composed of the aforementioned three parameters and their delta and delta–delta components. Then, these are mapped into the target values, which are given in the training stage, through multiple nonlinear hidden layers as follows:
                           
                              (10)
                              
                                 f
                                 (
                                 
                                    
                                       Z
                                    
                                 
                                 (
                                 n
                                 )
                                 |
                                 Θ
                                 )
                                 =
                                 g
                                 (
                                 g
                                 (
                                 g
                                 (
                                 
                                    
                                       Z
                                    
                                 
                                 (
                                 n
                                 )
                                 
                                    
                                       
                                          w
                                       
                                    
                                    
                                       (
                                       0
                                       )
                                    
                                 
                                 +
                                 
                                    
                                       
                                          b
                                       
                                    
                                    
                                       (
                                       0
                                       )
                                    
                                 
                                 )
                                 
                                    
                                       
                                          w
                                       
                                    
                                    
                                       (
                                       1
                                       )
                                    
                                 
                                 +
                                 
                                    
                                       
                                          b
                                       
                                    
                                    
                                       (
                                       1
                                       )
                                    
                                 
                                 )
                                 
                                    
                                       
                                          w
                                       
                                    
                                    
                                       (
                                       2
                                       )
                                    
                                 
                                 +
                                 
                                    
                                       
                                          b
                                       
                                    
                                    
                                       (
                                       2
                                       )
                                    
                                 
                                 )
                                 
                                    
                                       
                                          w
                                       
                                    
                                    
                                       (
                                       3
                                       )
                                    
                                 
                                 +
                                 
                                    
                                       
                                          b
                                       
                                    
                                    
                                       (
                                       3
                                       )
                                    
                                 
                              
                           
                        where Z(n) is the feature vector at the nth frame, Θ={w
                        (0), b
                        (0), w
                        (1), b
                        (1), w
                        (2), b
                        (2), w
                        (3), b
                        (3)}, and g(·) denotes the activation function. And, w
                        (j) and b
                        (j) denote the weight term and bias term between the jth layer and the (j
                        +1)th layer, respectively. Note that all activation functions adopt the logistic function as in Hinton and Salakhutdinov (2006). One of the important issues in training the DNN is how to pre-train the weight and bias parameters of each layer. The solution adopted in this paper is to stack the multiple restricted Boltzmann machines (RBMs), which can be trained layer-by-layer in a unsupervised greedy fashion (Hinton and Salakhutdinov, 2006; Hinton et al., 2006). After the weights and biases of each RBM are initialized by the layer-wise unsupervised learning procedure, they are fine tuned by the gradient descent-based back-propagation algorithm to minimize the multi-class cross entropy error function (Hinton and Salakhutdinov, 2006). The number of output units for the VAD is two, speech presence and absence and thus target output values of the speech frames are [10]T and those of the non-speech frames are [01]T. When the fine-tuning process begins, the weights and biases between the last hidden layer and the output layer are set randomly, not yet initialized, whereas the other weights and biases have been already initialized through the pre-training process. Hence, only the weights and biases between the last hidden layer and the output layer are trained in first few epoches and then all weights and biases are optimized thereafter.

Specifically, the output units of each DNN for speech detection is represented by the output of the DNN for the VAD such that
                           
                              (11)
                              
                                 
                                    y
                                    out
                                 
                                 (
                                 n
                                 )
                                 =
                                 
                                    f
                                    0
                                 
                                 (
                                 
                                    
                                       Z
                                    
                                 
                                 (
                                 n
                                 )
                                 |
                                 Θ
                                 )
                                 −
                                 
                                    f
                                    1
                                 
                                 (
                                 
                                    
                                       Z
                                    
                                 
                                 (
                                 n
                                 )
                                 |
                                 Θ
                                 )
                              
                           
                        where f
                        
                           m
                        (Z(n)|Θ) is the output value of the mth output node and can be computed by
                           
                              (12)
                              
                                 
                                    f
                                    m
                                 
                                 (
                                 
                                    
                                       Z
                                    
                                 
                                 (
                                 n
                                 )
                                 |
                                 Θ
                                 )
                                 =
                                 g
                                 (
                                 g
                                 (
                                 g
                                 (
                                 
                                    
                                       Z
                                    
                                 
                                 (
                                 n
                                 )
                                 
                                    
                                       
                                          w
                                       
                                    
                                    
                                       (
                                       0
                                       )
                                    
                                 
                                 +
                                 
                                    
                                       
                                          b
                                       
                                    
                                    
                                       (
                                       0
                                       )
                                    
                                 
                                 )
                                 
                                    
                                       
                                          w
                                       
                                    
                                    
                                       (
                                       1
                                       )
                                    
                                 
                                 +
                                 
                                    
                                       
                                          b
                                       
                                    
                                    
                                       (
                                       1
                                       )
                                    
                                 
                                 )
                                 
                                    
                                       
                                          w
                                       
                                    
                                    
                                       (
                                       2
                                       )
                                    
                                 
                                 +
                                 
                                    
                                       
                                          b
                                       
                                    
                                    
                                       (
                                       2
                                       )
                                    
                                 
                                 )
                                 
                                    
                                       
                                          
                                             
                                                w
                                             
                                          
                                       
                                       ˆ
                                    
                                    m
                                    
                                       (
                                       3
                                       )
                                    
                                 
                                 +
                                 
                                    b
                                    m
                                    
                                       (
                                       3
                                       )
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    
                                       w
                                    
                                 
                                 ˆ
                              
                              m
                              
                                 (
                                 3
                                 )
                              
                           
                           =
                           
                              
                                 [
                                 
                                    w
                                    
                                       0
                                       m
                                    
                                    
                                       (
                                       3
                                       )
                                    
                                 
                                 ,
                                 
                                    w
                                    
                                       1
                                       m
                                    
                                    
                                       (
                                       3
                                       )
                                    
                                 
                                 ,
                                 
                                    w
                                    
                                       2
                                       m
                                    
                                    
                                       (
                                       3
                                       )
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    w
                                    
                                       
                                          N
                                          node
                                       
                                       m
                                    
                                    
                                       (
                                       3
                                       )
                                    
                                 
                                 ]
                              
                              T
                           
                        , which is the mth column vector of w
                        (3), where N
                        node and T denote the number of nodes on the third hidden layer and the vector transposition, respectively. And, 
                           
                              b
                              m
                              
                                 (
                                 3
                                 )
                              
                           
                         denotes the mth element of b
                        (3). The SPP can be derived from the output of the DNN by adopting a parametric way to employ the sigmoid function as given by
                           
                              (13)
                              
                                 p
                                 (
                                 H
                                 (
                                 n
                                 )
                                 =
                                 
                                    H
                                    1
                                 
                                 |
                                 
                                    y
                                    out
                                 
                                 (
                                 n
                                 )
                                 )
                                 =
                                 
                                    1
                                    
                                       1
                                       +
                                       exp
                                       (
                                       A
                                       ·
                                       
                                          y
                                          out
                                       
                                       (
                                       n
                                       )
                                       +
                                       B
                                       )
                                    
                                 
                                 .
                              
                           
                        Here, the principal parameters such as the slope parameter A and the bias parameter B can be obtained by discriminative training in a way to minimize the negative log likelihood of the data, which is the cross-entropy error function (Bishop, 1995) defined as
                           
                              (14)
                              
                                 F
                                 (
                                 A
                                 ,
                                 B
                                 )
                                 =
                                 −
                                 
                                    ∑
                                    
                                       n
                                       =
                                       1
                                    
                                    N
                                 
                                 
                                    t
                                    (
                                    n
                                    )
                                    log
                                    (
                                    p
                                    (
                                    H
                                    (
                                    n
                                    )
                                    =
                                    
                                       H
                                       1
                                    
                                    |
                                    
                                       y
                                       out
                                    
                                    (
                                    n
                                    )
                                    )
                                    )
                                    +
                                    (
                                    1
                                    −
                                    t
                                    (
                                    n
                                    )
                                    )
                                    log
                                    (
                                    1
                                    −
                                    p
                                    (
                                    H
                                    (
                                    n
                                    )
                                    =
                                    
                                       H
                                       1
                                    
                                    |
                                    
                                       y
                                       out
                                    
                                    (
                                    n
                                    )
                                    )
                                    )
                                 
                              
                           
                        where t(n) is the target value of the parametric sigmoid function at the nth frame of the training dataset and given by manually labeling of every frame in the training stage. More specifically, t(n)=1 if the nth frame belongs to a speech frame and t(n)=0 otherwise. The model-trust algorithm based on the Levenberg–Marquardt algorithm (Platt, 2000) is performed to obtain the optimal parameters A and B, which minimize the cross-entropy error function for each noise type.

Ensemble of DNN and AEC methods are adopted to statistical model-based VAD to adopt the best DNN scheme according to noise type. For this, first, the ensemble of DNNs used for VAD can be built through the layer wise pre-training process by using all type of the noisy speech dataset without a label, namely, the unsupervised learning. Then, each DNN is optimized, separately, through the fine-tuning process by using each type of the noisy training data with the label and by starting with the initialized weights and biases. As a result, the optimal parameters for estimating the SPP from the output of the DNNs are obtained by the model-trust algorithm in a separate manner. Specifically, the optimal DNN parameters Θ1, Θ2, …, 
                           
                              Θ
                              
                                 
                                    N
                                    DNNs
                                 
                              
                           
                         and the parameters 
                           
                              A
                              VAD
                              
                                 (
                                 1
                                 )
                              
                           
                        , 
                           
                              B
                              VAD
                              
                                 (
                                 1
                                 )
                              
                           
                        , 
                           
                              A
                              VAD
                              
                                 (
                                 2
                                 )
                              
                           
                        , 
                           
                              B
                              VAD
                              
                                 (
                                 2
                                 )
                              
                           
                        , …, 
                           
                              A
                              VAD
                              
                                 (
                                 
                                    N
                                    DNNs
                                 
                                 )
                              
                           
                        , 
                           
                              B
                              VAD
                              
                                 (
                                 
                                    N
                                    DNNs
                                 
                                 )
                              
                           
                         for estimating the SPP are obtained from which Θ
                           i
                        , 
                           
                              A
                              VAD
                              
                                 (
                                 i
                                 )
                              
                           
                        , and 
                           
                              B
                              VAD
                              
                                 (
                                 i
                                 )
                              
                           
                         are optimized for the ith type of noisy training data where N
                        DNNs denotes the number of DNNs on the number of the acoustic environments.

For the realtime implementation, each type of noise should be classified during the periods of non-speech, so the classification is updated only when the SPP of the previous frame is greater than a given threshold (i.e., 0.35). This process is performed by applying the DNN in a similar reason described earlier. Indeed, the separate DNN for acoustic environment classification is implemented by incorporating the log-power spectrum (LPS) as the feature vector. For this, the acoustic signal data manually labeled as noise frames was used, so optimal weights and biases set ΘAEC were found at the off-line training stage.

Generally, each frame is classified from the value of output nodes of the DNN into the classes by taking the soft-max and winner-take-all methods. Instead of the hard decision, we adopt the parametric way to employ the sigmoid function, which was also used to predict the SPPs as in (14) at the ensemble of the DNNs, by simplifying the output of each DNN as follows:
                           
                              (15)
                              
                                 
                                    
                                       
                                          y
                                          ˆ
                                       
                                    
                                    i
                                 
                                 (
                                 n
                                 )
                                 =
                                 
                                    f
                                    i
                                 
                                 (
                                 log
                                 (
                                 
                                    
                                       Y
                                    
                                 
                                 (
                                 n
                                 )
                                 )
                                 |
                                 
                                    Θ
                                    AEC
                                 
                                 )
                                 −
                                 
                                    ∑
                                    
                                       j
                                       =
                                       1
                                    
                                    
                                       i
                                       −
                                       1
                                    
                                 
                                 
                                    
                                       f
                                       j
                                    
                                    (
                                    log
                                    (
                                    
                                       
                                          Y
                                       
                                    
                                    (
                                    n
                                    )
                                    )
                                    |
                                    
                                       Θ
                                       AEC
                                    
                                    )
                                 
                                 −
                                 
                                    ∑
                                    
                                       j
                                       =
                                       i
                                       +
                                       1
                                    
                                    N
                                 
                                 
                                    
                                       f
                                       j
                                    
                                    (
                                    log
                                    (
                                    
                                       
                                          Y
                                       
                                    
                                    (
                                    n
                                    )
                                    )
                                    |
                                    
                                       Θ
                                       AEC
                                    
                                    )
                                 
                              
                           
                        where 
                           
                              
                                 
                                    y
                                    ˆ
                                 
                              
                              i
                           
                           (
                           n
                           )
                         is the final output value of the DNN for the ith type of noise condition at the nth frame and Y(n)=[Y(0, n), Y(1, n), …, Y(L
                        −1, n)], which is a vector of power spectra at the nth frame. The probability of the ith type of noise given the observation Y(n) is derived by
                           
                              (16)
                              
                                 p
                                 (
                                 s
                                 (
                                 n
                                 )
                                 =
                                 
                                    s
                                    i
                                 
                                 )
                                 |
                                 
                                    
                                       Y
                                    
                                 
                                 (
                                 n
                                 )
                                 )
                                 =
                                 
                                    1
                                    
                                       1
                                       +
                                       exp
                                       (
                                       
                                          A
                                          AEC
                                          
                                             (
                                             i
                                             )
                                          
                                       
                                       ·
                                       
                                          
                                             
                                                y
                                                ˆ
                                             
                                          
                                          i
                                       
                                       (
                                       n
                                       )
                                       +
                                       
                                          B
                                          AEC
                                          
                                             (
                                             i
                                             )
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        where 
                           
                              A
                              AEC
                              
                                 (
                                 i
                                 )
                              
                           
                         and 
                           
                              B
                              AEC
                              
                                 (
                                 i
                                 )
                              
                           
                         denote the slope parameter and bias parameter for the ith noise type, respectively, and can be optimized to the ith type of noise environment based on the model-trust algorithm (Platt, 2000). In the online classification stage, SPPs can be computed from each DNN by transferring the statistical feature vector into each well-trained DNN and then they are derived from the simplified output of each DNN by the parametric sigmoid function with specific optimal parameters 
                           
                              A
                              VAD
                              
                                 (
                                 i
                                 )
                              
                           
                        , 
                           
                              B
                              VAD
                              
                                 (
                                 i
                                 )
                              
                           
                        . A final SPP can be obtained by the linear weighted combination way with the weights derived from the result of the acoustic environment classification. Note that the weights must satisfy the following conditions: 
                           
                              
                                 (17)
                                 
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          
                                             N
                                             noise
                                          
                                       
                                    
                                    
                                       
                                          w
                                          i
                                       
                                       (
                                       n
                                       )
                                    
                                    =
                                    1
                                 
                              
                              
                                 (18)
                                 
                                    
                                       w
                                       i
                                    
                                    (
                                    n
                                    )
                                    ≥
                                    0
                                    .
                                 
                              
                           
                         The weights satisfying the above conditions can be derived from the probabilities for each type of noise as follows:
                           
                              (19)
                              
                                 
                                    w
                                    i
                                 
                                 (
                                 n
                                 )
                                 =
                                 
                                    
                                       exp
                                       (
                                       p
                                       (
                                       s
                                       (
                                       n
                                       )
                                       =
                                       
                                          s
                                          i
                                       
                                       |
                                       log
                                       (
                                       
                                          
                                             Y
                                          
                                       
                                       (
                                       n
                                       )
                                       )
                                       )
                                    
                                    
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          
                                             
                                                N
                                                noise
                                             
                                          
                                       
                                       
                                          exp
                                          (
                                          p
                                          (
                                          s
                                          (
                                          n
                                          )
                                          =
                                          
                                             s
                                             j
                                          
                                          |
                                          log
                                          (
                                          
                                             
                                                Y
                                             
                                          
                                          (
                                          n
                                          )
                                          )
                                          )
                                       
                                    
                                 
                                 .
                              
                           
                        The final SPP is estimated by fusing the probability of each DNN with the weights for each noise as follows:
                           
                              (20)
                              
                                 p
                                 (
                                 H
                                 (
                                 n
                                 )
                                 =
                                 
                                    H
                                    1
                                 
                                 |
                                 
                                    
                                       Z
                                    
                                 
                                 (
                                 n
                                 )
                                 )
                                 =
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       
                                          N
                                          DNNs
                                       
                                    
                                 
                                 
                                    
                                       w
                                       i
                                    
                                    (
                                    n
                                    )
                                    ·
                                    
                                       p
                                       i
                                    
                                    (
                                    H
                                    (
                                    n
                                    )
                                    =
                                    
                                       H
                                       1
                                    
                                    |
                                    
                                       y
                                       out
                                    
                                    (
                                    n
                                    )
                                    )
                                 
                              
                           
                        where p
                        
                           i
                        (H(n)=
                        H
                        1|y
                        out(n)) is calculated by using the parameters Θ
                           i
                        , 
                           
                              A
                              VAD
                              
                                 (
                                 i
                                 )
                              
                           
                        , and 
                           
                              B
                              VAD
                              
                                 (
                                 i
                                 )
                              
                           
                         used to estimate the SPP at ith DNN for VAD. In order to install the weights adaptively, weights have to be updated during the period of noise. Here, the threshold for the criterion of speech absence was set to 0.35 for updating the weights. This scheme is revised by the hang-over scheme method (Sohn et al., 1999) when the final SPP of the previous frame is less than the given threshold as given by
                           
                              (21)
                              
                                 
                                    w
                                    i
                                 
                                 (
                                 n
                                 )
                                 =
                                 α
                                 
                                    w
                                    i
                                 
                                 (
                                 n
                                 −
                                 1
                                 )
                                 +
                                 (
                                 1
                                 −
                                 α
                                 )
                                 
                                    
                                       exp
                                       (
                                       p
                                       (
                                       s
                                       (
                                       n
                                       )
                                       =
                                       
                                          s
                                          i
                                       
                                       |
                                       log
                                       (
                                       
                                          
                                             Y
                                          
                                       
                                       (
                                       n
                                       )
                                       )
                                       )
                                    
                                    
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          
                                             
                                                N
                                                noise
                                             
                                          
                                       
                                       
                                          exp
                                          (
                                          p
                                          (
                                          s
                                          (
                                          n
                                          )
                                          =
                                          
                                             s
                                             j
                                          
                                          |
                                          log
                                          (
                                          
                                             
                                                Y
                                             
                                          
                                          (
                                          n
                                          )
                                          )
                                          )
                                       
                                    
                                 
                              
                           
                        where α is a smoothing factor. Note that the AEC operates on the first few frames since those can be considered as the non-speech frames, which can be an acceptable rule (Choi and Chang, 2012). Finally, each frame is initially classified as the period of speech if the SPP as in (20) is greater than a given threshold η for speech detection or the period of noise otherwise.

@&#EXPERIMENTS AND RESULTS@&#

This section describes the performance evaluation of the proposed VAD approach based on the statistical model and ensemble of DNNs employing the AEC. For the objective comparison, the proposed approach was compared with the conventional SVM-based VAD (Jo et al., 2009) and the method based on single DNN without the AEC. The proposed approach was also compared with the DNN-based VAD algorithm using the LPS which is the simple acoustic feature for the VAD to validate the superiority of the statistical model parameters in the VAD. Each VAD algorithm was assessed by investigating the probability of error P
                     E, the probability of miss P
                     M, and the probability of false-alarm P
                     F which are defined as the ratio of incorrect decision to the overall frames, as the ratio of miss speech decision to the hand-marked speech frames, and as the ratio of false speech decisions to the hand marked non-speech frames, respectively. The VAD algorithms were also evaluated in terms of receiver-operating-characteristics (ROC) curve, which shows trade-off characteristic between speech detection probability P
                     D
                     =(1−
                     P
                     M) and P
                     F. In this work, the input signal was sampled at 8kHz, and the frame size was 10ms, which implies that each frame consists of 80 samples. Then, the windowed signal is obtained by the hamming window with the 10ms frame shift and transformed to 128 fast Fourier transform (FFT) coefficients after zero padding.

To train the DNNs, we made reference decisions on clean speech corpus of 304s long by manually labeling the active and inactive regions of the speech signal for every 10ms frame. The proportions of voiced, unvoiced, and silence frames of the training material were 45.29%, 13.37%, and 41.34%, respectively. And, TIMIT (Garofolo et al., 1993) and AURORA2 (Hirsch and Pearce, 2000) were also used as a labeled and unlabeled dataset, respectively. In order to make a input noisy speech data, we added the airport, babble, car, exhibition, restaurant, street, subway, and train noises, which stem from the AURORA2, to clean speech signals while keeping the SNR at −5, 0, 5, and 10dB. As a result, about 258h long labeled noisy training data and about 141h long unlabeled noisy training data were prepared. In the pre-training stage, unlabeled training data (399h) was used for the unsupervised training and the labeled training data (258h) was only used in the fine-tuning stage for the supervised training.

For performance analysis of the proposed approach, we prepared different speech data of 152s in duration and manually labeled as the active and inactive regions of the speech signal for every 10ms frame. The proportions of voiced, unvoiced, and silence frames of the material for testing were 43.86%, 13.40%, and 42.74%, respectively. To simulate the noise conditions, aforementioned noise signals were again added to test corpus while keeping the SNR at −5, 0, 5, 10dB. And, the factory and office noises from NOISEX-92 (Varga and Steeneken, 1993) were also added to clean speech while keeping same SNR to evaluate the proposed approach under the unseen noise conditions.

The parameter settings used to train the DNNs for the VAD and AEC are explained in the following. For training each DNN for VAD, the number of hidden layers was set to three and the number of units on each hidden layers were set to 256, 128, and 64, respectively. To train the DNN for the AEC, the number of hidden layers was also set to three and the numbers of units on hidden layers were set to same as 512. For both DNNs, the mini-batch sizes for the pre-training and fine-tuning were set to 100 and 1000, respectively. In the unsupervised pre-training process, the maximum epoch was set to 80 and the learning rate was set to 0.001. As for the fine-tuning process, the maximum epoch was set to 150 and the learning rate was set to 0.1 for the first 10 epochs, while optimizing the weights and biases between the last hidden layer and the output layer and then the learning rate was decreased by 10% after 10 epochs at which we did not consider the early stopping scheme. The parameters used to train the SVM compared with the proposed approach were set rigorously according to the author's settings that the radial basis function is used as the kernel function and the kernel parameter was set to 1.0 (Jo et al., 2009).

@&#EXPERIMENTAL RESULTS@&#


                        Table 1
                         summarizes the results of the proposed AEC approach based on the DNN in the form of a confusion matrix. The rows correspond to the actual class of input features and the columns correspond to the class predicted by the DNN-based AEC approach. The diagonal terms of the confusion matrix indicate the classification accuracy, so it was discovered that the proposed DNN-based algorithm yielded the high classification accuracy of at least 97.16% and the average accuracy of 99.33%. The evaluated results demonstrate that the proposed AEC is suitable for the proposed DNN approach based on the ensemble of DNNs.

In addition, Table 2
                         including P
                        E, P
                        M, and P
                        F shows comparative results for the DNN-based VAD using LPS as feature, the SVM-based VAD (Jo et al., 2009) and the proposed DNN-based method using statistical model parameters as feature with or without the AEC technique, where bold number means the best result in term of P
                        
                           E
                        . Note that it can be seen that the statistical model parameters such as the LR, a priori SNR, and a posteriori SNR can be considered as superior features for the VAD than the LPS. It is observed that the proposed approach with AEC yielded best performance of P
                        
                           E
                         on most of the noise conditions including unseen noise conditions. Next, the ROC curves are shown in Fig. 2
                         for airport, babble, car, exhibition, restaurant, street, subway, train, factory, and office noises, at 5dB SNR. From these results, it can be seen that the proposed approach yielded significant improvement compared to not only LPS-based VAD and the SVM-based VAD (Jo et al., 2009) but also single DNN-based VAD approaches. Proposed approach was also evaluated under the unseen noise environments such as factory and office noises. Fig. 3
                         shows the ROC curves for factory and office noises at 5dB SNR which were not used in the training stage. According to these results, the proposed approach is also found to show the robust performance in the unseen noise conditions. Fig. 4
                         shows the example of weights derived from the AEC module and estimated SPP at the office noise at 5dB SNR. Note that weights derived by the AEC module are adaptively set according to result of the AEC only if the SPP of previous frame is less than the threshold value (i.e., 0.35) for the AEC.

@&#CONCLUSIONS@&#

In this paper, we have presented the novel VAD technique provided by the ensemble of the DNNs with AEC. The first contribution of this work is the use on the statistical model parameters such as the a priori SNR, the a posteriori SNR, and the LR in the feature extraction part and then they map to the target output value in a nonlinear way for transferring them into the multiple DNNs, which were trained for each kind of noise conditions. In order to implement the frame-by-frame pooling of the ensemble of the DNN, the SPPs are first derived from the output of the DNNs by using parametric sigmoid function with optimized parameters. Then, final SPP is computed by fusing the estimate of each DNN in a linearly weighed combination way by the use of weights derived from the result of the DNN-based AEC. The proposed method was evaluated in terms of an objective measure and was found to have significant improvement compared to the previous method under the various noise conditions in terms of types and SNR levels.

@&#ACKNOWLEDGMENTS@&#

This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIP) (No. 2014R1A2A1A10049735). This work was also supported by the ICT R&D program of MSIP/IITP [R0126-15-1119, Development of a solution for situation-awareness based on the analysis of speech and environmental sounds].

@&#REFERENCES@&#

