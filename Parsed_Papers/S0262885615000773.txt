@&#MAIN-TITLE@&#Unified multi-lateral filter for real-time depth map enhancement

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           New multi-lateral filter to efficiently increase the spatial resolution of low-resolution and noisy depth maps in real-time.


                        
                        
                           
                           ToF camera coupled with a 2-D camera of higher resolution to which the low-resolution depth map will upsampled.


                        
                        
                           
                           We account for the inaccuracy of depth edges position due to the low-resolution ToF depth maps.


                        
                        
                           
                           Unwanted artefacts such as texture copying and edge blurring are almost entirely eliminated.


                        
                        
                           
                           The proposed filter is convolution-based and achives a real-time performance by data quantization and downsampling.


                        
                        
                           
                           The proposed filter has been effectively and efficiently implemented for dynamic scenes in real-time applications.


                        
                        
                           
                           The proposed filter can be easily adapted for alternative depth sensing systems than ToF cameras.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Depth enhancement

Data fusion

Sensor fusion

Multi-modal sensors

Adaptive filters

Active sensing

Time-of-Flight

@&#ABSTRACT@&#


               
               
                  This paper proposes a unified multi-lateral filter to efficiently increase the spatial resolution of low-resolution and noisy depth maps in real-time. Time-of-Flight (ToF) cameras have become a very promising alternative to stereo-based range sensing systems as they provide depth measurements at a high frame rate. However, there are actually two main drawbacks that restrict their use in a wide range of applications; namely, their fairly low spatial resolution as well as the amount of noise within the depth estimation. In order to address these drawbacks, we propose a new approach based on sensor fusion. That is, we couple a ToF camera of low-resolution with a 2-D camera of higher resolution to which the low-resolution depth map will be efficiently upsampled. In this paper, we first review the existing depth map enhancement approaches based on sensor fusion and discuss their limitations. We then propose a unified multi-lateral filter that accounts for the inaccuracy of depth edges position due to the low-resolution ToF depth maps. By doing so, unwanted artefacts such as texture copying and edge blurring are almost entirely eliminated. Moreover, the proposed filter is configurable to behave as most of the alternative depth enhancement approaches. Using a convolution-based formulation and data quantization and downsampling, the described filter has been effectively and efficiently implemented for dynamic scenes in real-time applications. The experimental results show a sensitive qualitative as well as quantitative improvement on raw depth maps, outperforming state-of-the-art multi-lateral filters.
               
            

@&#INTRODUCTION@&#

Time-of-Flight cameras have become an alternative 3-D sensing system to stereo-based range sensing systems such as stereo vision systems, laser scanners or structured light. They present several advantages such as simultaneously providing intensity and depth information for every pixel at a high frame rate. Moreover, the recent advances in industrializing and producing economic, compact, robust to illumination changes and low-weight ToF cameras is starting to have an impact on commercial applications [1–3]. This is the case of the Xbox One's Kinect version 2, which uses a ToF based depth camera. It has shown a significant improvement over the structured light based Kinect version that was shipped with the Xbox 360 [3]. However, as a compromise for their higher robustness to ambient conditions; i.e., larger working temperature range and higher reliability under sun light; and in contrast to depth sensing systems intended for gaming applications or research purposes, the resolution of ToF cameras that are used in automotive or industrial automation is still far below the resolution of common 2-D cameras. Indeed, a very few ToF cameras slightly exceed the Quarter-Quarter Video Graphics Array (QQVGA) resolution, i.e., (160×120) pixels. Note that consumer depth cameras such as Microsoft Kinect or Asus Xtion Pro Live are not compliant to the standards in automotive or industrial automation. Hence, their rather low spatial resolution and the high noise level within their depth measurements are currently restricting their use.

A suitable solution to handle the limited resolution of ToF cameras is sensor fusion [4–11,47]. That is, depth data is complemented with data from a coupled sensor, usually 2-D cameras [12]. Previous works in ToF and 2-D data fusion have proven to deliver dense depth maps at near real-time frame rates, outperforming, in some cases, alternative 3-D sensing systems [4,6]. In this paper, we first introduce existing low-resolution depth map enhancement methods and discuss their limitations. Then, we propose a multi-lateral filter that efficiently tackles inaccurate depth measurements from low-resolution depth maps, producing an enhanced depth map of the same resolution as the considered 2-D guidance image. To this end, we rely on a confidence measure, to be incorporated in the filter formulation, that describes the reliability of each depth measurement. In addition, depth pixels with low confidence will be further treated to overcome unwanted artefacts resulting from data fusion. We also discuss the practical issues such as the configuration of the filter's parameters in order to make it behave as most of the existing multi-lateral filters in the literature. That is, we propose a unified formulation for fusion based depth enhancement filters. Finally, we propose a mathematical formulation that enables the filter to be used in real-time applications. We remark that the proposed filter was already introduced in [13] where we provided advice on how to speed up its performance as well as some preliminary results. Herein, we provide a detailed description of its components as well as a comprehensive qualitative and quantitative evaluation and comparison against alternative depth enhancement filters. In addition, we discuss its possible configurations to be adapted for different sensing system technologies and/or applications.

The remainder of the paper is organized as follows: Section 2 covers the background and a literature review on depth map enhancement by means of low-level data fusion. In Section 3 we present our multi-lateral filter as well as the confidence measure that is used to combine depth and 2-D information. Section 4 proposes a mathematical formulation that enables for a real-time implementation. In Section 5, we compare and quantify the proposed filter with alternative depth enhancement methods using synthetic scenes from the Middlebury dataset as well as our own recorded sequences. Finally, concluding remarks are provided in Section 6.

The idea of coupling a ToF camera and a 2-D video camera in a hybrid multi-camera rig is a convenient strategy to overcome the limitations of ToF cameras. Indeed, sensor fusion exploits the advantages of each of the cameras in the camera rig avoiding their individual drawbacks. In this paper we target a low-level data fusion in contrast to higher fusion levels that concern post-processed data [14]. To this end, we need first to ensure a good data matching between the cameras that constitute the camera rig. Existing 3-D warping techniques, i.e., forward warping techniques [15,16,11], assign to each depth pixel its corresponding RGB pixel. However, low-level data fusion resulting in a high-resolution depth map requires the opposite mapping, that is, to assign to each high-resolution 2-D pixel its corresponding depth pixel. Note that this so-called backward warping is far from a trivial task due to the need of determining the distance-dependent disparity between each 2-D and depth pixel correspondences. Herein, we use the real-time backward warping algorithm proposed in [17], an iterative approach that addresses backward warping mapping with an accuracy of half the high-resolution 2-D pixel.

Among the early results on low-resolution depth map enhancement, we emphasize those that use Markov Random Fields (MRFs) to fuse different sources of data. Diebel et al. [5] proposed a direct application of MRFs to range sensing with very promising results. Their approach was later extended by Gloud et al. [7] in the field of robotics, enabling a robot to detect and recognize objects from enhanced depth data. However, the authors did not tackle depth discontinuities along depth edges in a dedicated. This leads to strange artefacts within depth edges. Besides, the evaluation of depth enhancement methods based upon an MRF is in general computationally intensive and thus not suitable if real-time is required. Park et al. [18] proposed a dedicated edge weighting scheme to be used in combination with a non-local means regularization in a least-square optimization approach. User interaction and depth edge mark-up might be required to obtain optimal results as well as image segmentation to define the colour similarity for their edge confidence weighting. Yang et al. [9] proposed to improve the quality of a given low-resolution depth map through an iterative refinement module based upon a cost volume. But again, the authors neither tackle depth discontinuities and real-time remained an open question. An alternative solution for depth enhancement is to consider bilateral filtering [19,20] while fusing ToF and 2-D data. Although bilateral filtering is commonly used in image processing and/or computer vision applications for 2-D noise removal and edge-preserving, it has become the basis of recent depth enhancement approaches, outperforming, in some cases, those approaches based upon an MRF or iterative techniques. A more noticeable aspect is the possibility of performing in real-time thanks to its latest implementation strategies [21–23]. Indeed, the real-time aspect was one of the strongest reasons that motivated us to opt for bilateral filtering when fusing the data. In the following, we introduce the bilateral filter. Later, we review the literature on depth enhancement approaches based on bilateral filtering.

The bilateral filter was first introduced by Tomasi et al. as an alternative to iterative approaches for image noise removal [20] such as anisotropic diffusion, weighted least squares, and robust estimation [19]. This non-iterative filter formulation is a simple weighted average of the local neighbourhood samples, i.e.,
                           
                              (1)
                              
                                 
                                    J
                                    
                                       B
                                       F
                                    
                                 
                                 
                                    p
                                 
                                 =
                                 
                                    
                                       
                                          
                                             ∑
                                             
                                                q
                                                ∈
                                                N
                                                
                                                   p
                                                
                                             
                                          
                                       
                                       
                                          w
                                          
                                             B
                                             F
                                          
                                       
                                       
                                          p
                                          q
                                       
                                       ⋅
                                       I
                                       
                                          q
                                       
                                    
                                    
                                       
                                          
                                             ∑
                                             
                                                q
                                                ∈
                                                N
                                                
                                                   p
                                                
                                             
                                          
                                       
                                       
                                          w
                                          
                                             B
                                             F
                                          
                                       
                                       
                                          p
                                          q
                                       
                                    
                                 
                                 ,
                              
                           
                        where N(p) is the neighbourhood at the pixel indexed by the position vector p
                        =(i,
                        j)
                           T
                        , with i and j indicating the row, respectively column corresponding to the pixel position. The weights are computed based on spatial and radiometric distances between the centre of the considered sample and the neighbouring samples. That is, the kernel is decomposed into a spatial weighting term f
                        S(⋅) that applies to the pixel position p, and a range weighting term f
                        I(⋅) that applies to the pixel value I(p) as follows
                           
                              (2)
                              
                                 
                                    w
                                    
                                       B
                                       F
                                    
                                 
                                 
                                    p
                                    q
                                 
                                 =
                                 
                                    f
                                    S
                                 
                                 
                                    p
                                    q
                                 
                                 ⋅
                                 
                                    f
                                    I
                                 
                                 
                                    
                                       I
                                       
                                          p
                                       
                                       ,
                                       I
                                       
                                          q
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

The weighting terms f
                        S(⋅) and f
                        I(⋅) are generally chosen to be Gaussian functions with standard deviations σ
                        S and σ
                        I, respectively. The parameter σ
                        S defines the spatial dimension of the kernel, i.e., the size of the pixel neighbourhood N(p), whereas σ
                        I defines the resolution in I for edge preserving. That is, pixels p and q influence each other as long as their values differ within the range ±
                        σ
                        I. When both standard deviations are well chosen, the resulting filtered image JBF is a smoothed version of I, with preserved edges and significantly reduced noise level. Thus, by replacing I by a low-resolution depth map D, the resulting filtered depth map JBF will present much less noise even though the low-resolution problem will not be tackled.

In order to tackle the low-resolution problem, most of the recent depth enhancement techniques by fusion [4,24,6,13] turn to the Joint Bilateral Upsampling (JBU) filter, introduced by Kopf et al. in [8]. The JBU filter is a modification of the bilateral filter in which the data to be filtered and the data through which the weighting term is computed are not coincident. That is, I in (1) is replaced by the low-resolution depth map D, i.e.,
                           
                              (3)
                              
                                 
                                    J
                                    
                                       J
                                       B
                                       U
                                    
                                 
                                 
                                    p
                                 
                                 =
                                 
                                    
                                       
                                          
                                             ∑
                                             
                                                q
                                                ∈
                                                N
                                                
                                                   p
                                                
                                             
                                          
                                       
                                       
                                          w
                                          
                                             B
                                             F
                                          
                                       
                                       
                                          p
                                          q
                                       
                                       ⋅
                                       D
                                       
                                          q
                                       
                                    
                                    
                                       
                                          
                                             ∑
                                             
                                                q
                                                ∈
                                                N
                                                
                                                   p
                                                
                                             
                                          
                                       
                                       
                                          w
                                          
                                             B
                                             F
                                          
                                       
                                       
                                          p
                                          q
                                       
                                    
                                 
                                 ,
                              
                           
                        whereas the weighting term w
                        BF in (2) considers the corresponding high-resolution 2-D image I. Thereby and in the following depth enhancement techniques by fusion, the low-resolution depth map D has to be upsampled to have the same spatial resolution as I. To this end, nearest-neighbour interpolation might be used [25,48] since alternative data interpolation approaches such as bilinear or bicubic interpolation might introduce non-valid depth measurements among the upsampled edges of the depth map [17]. As in (1), the resulting depth map JJBU is an enhanced version of D that has been upsampled to the 2-D guidance image resolution. Nevertheless, the violation of the heuristic assumption in which depth and colour are strongly correlated [2] may lead to erroneous copying of 2-D texture into actually smooth geometries within the depth map. That is, depth measurements within nearby pixels will be mixed together if their corresponding 2-D intensity values are close enough, yielding to a texture copying effect within the enhanced depth map. This effect is illustrated in Fig. 5f, where we show an enhanced depth map resulting from the JBU filter in which texture from the guidance image (see Fig. 5a) has been copied into smooth areas of the enhanced depth map. In addition, depth edges that have no corresponding edges in the 2-D image, i.e., in situations where objects on either side of a depth discontinuity have a similar colour, will be blurred. The same occurs when depth edges and their corresponding 2-D edges are not perfectly aligned. An effect which often occurs when filtering real data as the low-resolution of the acquired depth map restricts the accuracy of the data mapping process, as illustrated in Fig. 1c. We note that processing time restrictions and memory constraints usually force the use of a grayscale guidance image instead of the originally recorded colour image, making this latest effect more noticeable. Hereafter, we present different strategies from the literature that extend and/or adapt the JBU filter to overcome common artefacts in low-level data fusion.

Kim et al. [26] proposed a modification of the weighting term of the JBU filter by adding an additional range weighting term f
                        D(⋅), i.e.,
                           
                              (4)
                              
                                 
                                    w
                                    NJBU
                                 
                                 
                                    p
                                    q
                                 
                                 =
                                 
                                    f
                                    S
                                 
                                 
                                    p
                                    q
                                 
                                 ⋅
                                 
                                    f
                                    I
                                 
                                 
                                    
                                       I
                                       
                                          p
                                       
                                       ,
                                       I
                                       
                                          q
                                       
                                    
                                 
                                 ⋅
                                 
                                    f
                                    D
                                 
                                 
                                    
                                       D
                                       
                                          p
                                       
                                       ,
                                       D
                                       
                                          q
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

By doing so, the authors slightly reduce the JBU's texture copying artefact as well as the global amount of noise. However, this straightforward combination of the weighting terms does not cope with unmatched edges between the given depth map and its corresponding guidance image. The authors developed a post-processing framework for boundary refinement using a colour segment set. The refined depth edges result from the colour segment set after removing outside pixels from the object boundary, or extending inside pixels to the boundary using linear interpolation. As discussed in the original JBU filter version (see Section 2.2), the low-resolution depth map to be enhanced must be upsampled in order to have the same spatial resolution as its corresponding guidance image.

Chan et al. proposed in [4] an improved version of the JBU filter that intends to preserve the benefits of using the JBU filter while preventing artefacts in those areas where JBU is likely to cause erroneous texture copying. This filter is referred to as Noise-Aware Filter for Depth Upsampling (NAFDU). In contrast to directly combining 2-D and depth information within the filter kernel as Kim et al. did in [26], the NAFDU filter splits each data source contribution within the weighting term as follows
                           
                              (5)
                              
                                 
                                    w
                                    NAFDU
                                 
                                 
                                    p
                                    q
                                 
                                 =
                                 
                                    f
                                    S
                                 
                                 
                                    p
                                    q
                                 
                                 ⋅
                                 
                                    
                                       α
                                       
                                          
                                             Δ
                                             
                                                N
                                                
                                                   p
                                                
                                             
                                          
                                       
                                       ⋅
                                       
                                          f
                                          I
                                       
                                       
                                          
                                             I
                                             
                                                p
                                             
                                             ,
                                             I
                                             
                                                q
                                             
                                          
                                       
                                       +
                                       
                                          
                                             1
                                             −
                                             α
                                             
                                                
                                                   Δ
                                                   
                                                      N
                                                      
                                                         p
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       ⋅
                                       
                                          f
                                          D
                                       
                                       
                                          
                                             D
                                             
                                                p
                                             
                                             ,
                                             D
                                             
                                                q
                                             
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        where α(⋅) is the blending function that decides how much each of the data sources I and D contributes to the kernel. A high weight α makes the filter behaves like the original JBU filter whereas a low weight α makes it behaves like the standard bilateral filter, i.e., both spatial and range weighting terms apply to the same data source D, without considering the 2-D image I information. Intuitively, NAFDU tries to preserve the benefits of JBU except in the areas that are geometrically smoothed but heavily contaminated by noise within the distance measurements. The blending function is defined as 
                           α
                           
                              
                                 Δ
                                 
                                    N
                                    
                                       p
                                    
                                 
                              
                           
                           =
                           1
                           /
                           
                              
                                 1
                                 +
                                 
                                    e
                                    
                                       −
                                       ε
                                       
                                          
                                             
                                                Δ
                                                
                                                   N
                                                   
                                                      p
                                                   
                                                
                                             
                                             −
                                             τ
                                          
                                       
                                    
                                 
                              
                           
                        , with Δ
                           N(p) the difference between the maximum and minimum measured depth values in the pixel neighbourhood N(p). We note that the authors apply a low-pass filter on D before computing Δ
                           N(p). By doing so, they minimize the influence of noise within depth measurements. Parameters ε and τ control at what min-max difference the blending interval shall be centred. The downside of this approach is related to the complexity of determining those values, which in addition, have to be manually tuned. Besides, the NAFDU expression corresponds to a weighted average of two non normalized kernels, making the contribution of each of the kernels inconsistent and inaccurate.

More recent work on low-resolution depth enhancement can be summarized as an adaptation of the above presented filters to some specific cases or scenarios. For example, we proposed in [27] a new colour space to which one can encode the guidance image to preserve the perceptual properties of the hue-chroma-luminance (HCL) colour representation and thus, leading to a solution that is more accurate than when using JBU with grayscale guidance images. In [28], Choi et al. improved the performance of the NAFDU filter by reducing its complexity. To do so, the authors proposed a new blending function based on a depth threshold value. Although we herein focus on depth enhancement methods by fusing depth and 2-D data acquired at the same time stamp, there are alternative spatio-temporal based approaches that consider multiple video frames within their fusion process [29,30]. That way, the authors minimize temporal depth flickering artefacts on stationary objects. Min et al. [31,32] proposed a weighted mode filtering based on a joint histogram. The final solution results from seeking a global mode on the histogram constructed from the similarity measure between a reference pixel and its neighbourhood. A measuring colour distance approach is proposed to prevent edge blurring and, similarly to [29,30], temporal consistency is considered to generate flicker-free depth video. Another possible solution would be to fuse ToF and stereo data acquired by on-demand passive stereo systems [33–35]. The use of stereo data allows to get rid of texture copying and to improve the accuracy of depth measurements. However, the aforementioned depth enhancement approaches are highly sensitive to the registration of depth and 2-D data. That is, edge blurring will appear if depth and 2-D data are not pixel-wise matching.

Adjusting the right distance measurement along object boundaries without mismatching with texture is quite challenging. Indeed, depth pixels from low-resolution depth maps may cover foreground and background objects at the same time and thus, the depth estimation in these cases cannot be accurate. In addition, the depth estimation principle used by ToF cameras requires a certain integration time to estimate depth, which might increase this uncertainty in dynamic scenes where objects and/or people constantly change their location. To fix the right distance measurement within object boundaries, we proposed in [6] an improvement of the JBU filter, to which we referred as Pixel Weighted Average Strategy (PWAS) filter. In contrast to alternative depth enhancement methods proposed in the literature, the PWAS filter adds an additional weighting term Q(⋅) to the expression in (2). We referred to this term as credibility map as it assigns a reliability weight to each depth map value as a function of the scene's geometry. By doing so, depth measurements that are considered to be unreliable are replaced by reliable values in their neighbourhood while being adjusted to the 2-D guidance image. The weighting term for the PWAS filter to be used in (3) is defined as
                        
                           (6)
                           
                              
                                 w
                                 
                                    PWAS
                                    −
                                    I
                                 
                              
                              
                                 p
                                 q
                              
                              =
                              
                                 f
                                 S
                              
                              
                                 p
                                 q
                              
                              ⋅
                              
                                 f
                                 I
                              
                              
                                 
                                    I
                                    
                                       p
                                    
                                    ,
                                    I
                                    
                                       q
                                    
                                 
                              
                              ⋅
                              Q
                              
                                 q
                              
                              .
                           
                        
                     Similarly to the filters presented in Section 2, the weighting functions f
                     S(⋅) and f
                     I(⋅) are taken to be Gaussian functions with standard deviations σ
                     S and σ
                     I, respectively.

As discussed before, the estimated depth along object boundaries may be inaccurate. In addition and, as depicted in Fig. 1c, depth edges are not always perfectly aligned with respect to the 2-D guidance edge from which results the edge blurring artefact discussed in Section 2. Hence, the introduction of this new factor Q(⋅) within the weighting term allows to explicitly account for the unreliability of depth measurements along depth edges. In [6], the credibility map was defined as Q=
                        f
                        Q(|∇D|), with f
                        Q(⋅) being a Gaussian function with standard deviation σ
                        Q. ∇ stands for the gradient operation on D. From our experiments, the Sobel operator was a valid solution to compute image derivatives and thus, the gradient of D. A separable kernel of (3×3) was used to calculate the derivative. Note that no manual tuning or parameter setting is required for the computation of Q(⋅). In a nutshell, low-weighted credibility map pixels define those pixels in the depth map that contain unreliable depth measurements, whereas high-weighted credibility map pixels define reliable depth pixels. From the credibility map, low-weighted pixels are not considered during the filtering process. Note that in general, pixels along depth edges are low-weighted in the credibility map, and thus, depth edges are adjusted according to the 2-D guidance image. We show, in Fig. 2
                        , the credibility map computed on the raw depth map shown in Fig. 1b. From the figure, we can observe how low weight values are assigned to depth pixels representing object boundaries. By doing so, their depth values will be replaced by valid depth measurements in their neighbourhood and the depth edge will be adjusted according to the 2-D guidance image.

Although the PWAS filter correctly addresses depth measurements along depth edges with a significant reduction of the texture copying effect, the fact that the range weighting term f
                        I(⋅) only considers the 2-D information may cause texture copying in regions that actually are geometrically smooth and where depth values are used to be reliable. We therefore proposed in [13] the Unified Multi-Lateral (UML) filter in order to increase the accuracy of depth measurements within smooth regions. To this end, we defined two separate normalized kernels with each one considering a different data source, 2-D and depth information, as illustrated in the flow diagram of Fig. 3
                        . The decision on which kernel the filter has to consider is directly given by the reliability weight β(⋅) assigned to the pixel to be filtered. The UML filter takes the form of
                           
                              (7)
                              
                                 
                                    J
                                    
                                       U
                                       M
                                       L
                                    
                                 
                                 
                                    p
                                 
                                 =
                                 
                                    
                                       1
                                       −
                                       β
                                       
                                          p
                                       
                                    
                                 
                                 ⋅
                                 
                                    J
                                    
                                       PWAS
                                       −
                                       I
                                    
                                 
                                 
                                    p
                                 
                                 +
                                 β
                                 
                                    p
                                 
                                 ⋅
                                 
                                    J
                                    
                                       PWAS
                                       −
                                       D
                                    
                                 
                                 
                                    p
                                 
                                 .
                              
                           
                        
                     

The PWAS filter from which results JPWAS−I(⋅) uses the weighting term in (6). That is, both 2-D and depth information are considered within the weighting term. Instead, the PWAS filter from which results JPWAS−D(⋅) uses the following weighting term
                           
                              (8)
                              
                                 
                                    w
                                    
                                       PWAS
                                       −
                                       D
                                    
                                 
                                 
                                    p
                                    q
                                 
                                 =
                                 
                                    f
                                    S
                                 
                                 
                                    p
                                    q
                                 
                                 ⋅
                                 
                                    f
                                    D
                                 
                                 
                                    
                                       D
                                       
                                          p
                                       
                                       ,
                                       D
                                       
                                          q
                                       
                                    
                                 
                                 ⋅
                                 Q
                                 
                                    q
                                 
                                 ,
                              
                           
                        in which both spatial and range weighting terms are applied on the depth information D. Note that the credibility map clearly reflects those pixels within the depth map presenting accurate depth measurements. Hence, we can use the credibility map to identify those regions in which if we consider 2-D information while filtering, we might introduce texture copying. Therefore, we herein set the reliability weight equal to the credibility map, i.e., β
                        =Q.

We chose the weighting functions f
                        S(⋅), f
                        I(⋅), f
                        D(⋅), and f
                        Q(⋅) to be Gaussian functions with standard deviations σ
                        S, σ
                        I, σ
                        D, and σ
                        Q, respectively. One of the main reasons to choose Gaussian functions is because of their constant time computation [36]. We notice that these standard deviations are data-dependent and thus, they cannot be fixed to a unique value. Nevertheless, we next define how to automatically set the standard deviations for each weighting function. The parameter σ
                        S must be at least as large as the depth edge width which is, in fact, the width of low-weighted object boundaries within the credibility map. In general, this value coincides with the upsampling scale factor between the low-resolution depth map D and the high-resolution 2-D guidance image I (see Section 2.2). The parameters σ
                        I and σ
                        D define the resolution in I, and respectively D, for depth edge preserving. That is, pixels p and q influence each other as long as their values differ within the range ±
                        σ
                        I and ±
                        σ
                        D, respectively. We recommend to set them equal to the mean of the 2-D image gradient 
                           
                              
                                 
                                    ∇
                                    I
                                 
                                 ¯
                              
                           
                         and depth map gradient 
                           
                              
                                 
                                    ∇
                                    D
                                 
                                 ¯
                              
                           
                        , respectively. Finally, the value of σ
                        Q is directly related to the noise level within the depth data.

Depending on the application and/or on the sensing system technology to be used, one can configure the UML filter in order to adapt its behaviour to specific cases or scenarios. To do so, the reliability weight β(⋅) has to be considered as a data source flag. That is, by setting β(⋅)=1, the guidance information will be the depth map D whereas otherwise β(⋅)=0, it will be the 2-D image I. Another parameter to be taken into account is the standard deviation σ
                        Q for the credibility map. If it tends to infinity σ
                        Q
                        →∞, the credibility map becomes constant and equal to one for all pixel values, that is, we neglect its contribution. Hence, the multiple configurations of the UML filter allow it to behave like a:
                           
                              •
                              Bilateral filter, by setting the data source flag β(⋅)=1 and σ
                                 Q
                                 →∞ to neglect the credibility map contribution.

JBU filter, by setting the data source flag β(⋅)=0 and σ
                                 Q
                                 →∞.

PWAS filter, by setting the data source flag β(⋅)=0. The remaining part in (7) coincides with the PWAS filter.

In contrast to the NAFDU filter, the proposed multi-lateral filter is a weighted average of two normalized kernels. Thus, each kernel in (7) provides a consistent contribution. Making our filter behaves like the NAFDU filter implies a complex β(⋅) function that contains the pixel-dependent normalization factor of the two kernel contributions and thus, it does not represent a limit case. The same occurs with the JBU extension presented in Section 2.3.

Although bilateral filtering is known to be time consuming, its latest implementation strategies based on data quantization and downsampling enable for a high-performance. Indeed, there are a few studies [21–23] that prove that the use of such implementation strategies outperforms state-of-the-art methods for accuracy, speed and memory consumption. Based on these techniques, we next propose a real-time implementation for the UML filter presented in Section 3.2.

Similarly to [23], we quantize the range of the 2-D intensity values and depth measurements, i.e., I
                        
                           k
                        
                        =
                        s
                        I
                        ⋅
                        k, and D
                        
                           l
                        
                        =
                        s
                        D
                        ⋅
                        l, with k
                        =0,…,
                        K and l
                        =0,…,
                        L. s
                        I and s
                        D are the 2-D and depth quantization factors; thus (s
                        I
                        ×
                        K) and (s
                        D
                        ×
                        L) are equal or larger than the maximum 2-D intensity values and depth measurements, respectively. Then, inserting in (6) and in (8) the quantized levels I
                        
                           k
                         and D
                        
                           l
                         for I(p), respectively D(p), one obtains a different weighting term for each level, i.e.,
                           
                              (9)
                              
                                 
                                    w
                                    
                                       PWAS
                                       −
                                       I
                                    
                                 
                                 
                                    p
                                    
                                       I
                                       k
                                    
                                 
                                 =
                                 
                                    f
                                    S
                                 
                                 
                                    p
                                    q
                                 
                                 ⋅
                                 
                                    f
                                    I
                                 
                                 
                                    
                                       
                                          I
                                          k
                                       
                                       ,
                                       I
                                       
                                          q
                                       
                                    
                                 
                                 ⋅
                                 Q
                                 
                                    q
                                 
                                 ,
                              
                           
                        and
                           
                              (10)
                              
                                 
                                    w
                                    
                                       PWAS
                                       −
                                       D
                                    
                                 
                                 
                                    p
                                    
                                       D
                                       l
                                    
                                 
                                 =
                                 
                                    f
                                    S
                                 
                                 
                                    p
                                    q
                                 
                                 ⋅
                                 
                                    f
                                    D
                                 
                                 
                                    
                                       
                                          D
                                          l
                                       
                                       ,
                                       D
                                       
                                          q
                                       
                                    
                                 
                                 ⋅
                                 Q
                                 
                                    q
                                 
                                 .
                              
                           
                        
                     

We define four mappings, i.e., 
                           
                              E
                              
                                 I
                                 k
                              
                           
                           
                              ⋅
                           
                         and 
                           
                              F
                              
                                 I
                                 k
                              
                           
                           
                              ⋅
                           
                        , for a quantized intensity value at the pixel position p such that
                           
                              (11)
                              
                                 
                                    E
                                    
                                       I
                                       k
                                    
                                 
                                 :
                                 q
                                 ↦
                                 
                                    w
                                    
                                       PWAS
                                       −
                                       I
                                    
                                 
                                 
                                    p
                                    
                                       I
                                       k
                                    
                                 
                                 ⋅
                                 D
                                 
                                    q
                                 
                                 ,
                              
                           
                        
                        
                           
                              (12)
                              
                                 
                                    F
                                    
                                       I
                                       k
                                    
                                 
                                 :
                                 q
                                 ↦
                                 
                                    w
                                    
                                       PWAS
                                       −
                                       I
                                    
                                 
                                 
                                    p
                                    
                                       I
                                       k
                                    
                                 
                              
                           
                        and 
                           
                              G
                              
                                 D
                                 l
                              
                           
                           
                              ⋅
                           
                         and 
                           
                              H
                              
                                 D
                                 l
                              
                           
                           
                              ⋅
                           
                         for a quantized depth measurement at the pixel position p, such that
                           
                              (13)
                              
                                 
                                    G
                                    
                                       D
                                       l
                                    
                                 
                                 :
                                 q
                                 ↦
                                 
                                    w
                                    
                                       PWAS
                                       −
                                       D
                                    
                                 
                                 
                                    p
                                    
                                       D
                                       l
                                    
                                 
                                 ⋅
                                 D
                                 
                                    q
                                 
                                 ,
                              
                           
                        
                        
                           
                              (14)
                              
                                 
                                    H
                                    
                                       D
                                       l
                                    
                                 
                                 :
                                 q
                                 ↦
                                 
                                    w
                                    
                                       PWAS
                                       −
                                       D
                                    
                                 
                                 
                                    p
                                    
                                       D
                                       l
                                    
                                 
                                 .
                              
                           
                        
                     

We then may rewrite JPWAS−I(⋅) and JPWAS−D(⋅) as follows
                           
                              (15)
                              
                                 
                                    J
                                    
                                       PWAS
                                       −
                                       I
                                    
                                 
                                 
                                    p
                                    
                                       I
                                       k
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             ∑
                                             
                                                q
                                                ∈
                                                N
                                                
                                                   p
                                                
                                             
                                          
                                       
                                       
                                          f
                                          S
                                       
                                       
                                          p
                                          q
                                       
                                       ⋅
                                       
                                          E
                                          
                                             I
                                             k
                                          
                                       
                                       
                                          q
                                       
                                    
                                    
                                       
                                          
                                             ∑
                                             
                                                q
                                                ∈
                                                N
                                                
                                                   p
                                                
                                             
                                          
                                       
                                       
                                          f
                                          S
                                       
                                       
                                          p
                                          q
                                       
                                       ⋅
                                       
                                          F
                                          
                                             I
                                             k
                                          
                                       
                                       
                                          q
                                       
                                    
                                 
                                 ,
                              
                           
                        and
                           
                              (16)
                              
                                 
                                    J
                                    
                                       PWAS
                                       −
                                       D
                                    
                                 
                                 
                                    p
                                    
                                       D
                                       l
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             ∑
                                             
                                                q
                                                ∈
                                                N
                                                
                                                   p
                                                
                                             
                                          
                                       
                                       
                                          f
                                          S
                                       
                                       
                                          p
                                          q
                                       
                                       ⋅
                                       
                                          G
                                          
                                             D
                                             l
                                          
                                       
                                       
                                          q
                                       
                                    
                                    
                                       
                                          
                                             ∑
                                             
                                                q
                                                ∈
                                                N
                                                
                                                   p
                                                
                                             
                                          
                                       
                                       
                                          f
                                          S
                                       
                                       
                                          p
                                          q
                                       
                                       ⋅
                                       
                                          H
                                          
                                             D
                                             l
                                          
                                       
                                       
                                          q
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

We note that f
                        S(p,
                        q) is a function of the difference (p
                        −
                        q). We may hence write (15) and (16) as:
                           
                              (17)
                              
                                 
                                    J
                                    
                                       PWAS
                                       −
                                       I
                                    
                                 
                                 
                                    p
                                    
                                       I
                                       k
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                f
                                                S
                                             
                                             *
                                             
                                                E
                                                
                                                   I
                                                   k
                                                
                                             
                                          
                                       
                                       
                                          p
                                       
                                    
                                    
                                       
                                          
                                             
                                                f
                                                S
                                             
                                             *
                                             
                                                F
                                                
                                                   I
                                                   k
                                                
                                             
                                          
                                       
                                       
                                          p
                                       
                                    
                                 
                                 ,
                              
                           
                        and
                           
                              (18)
                              
                                 
                                    J
                                    
                                       PWAS
                                       −
                                       D
                                    
                                 
                                 
                                    p
                                    
                                       D
                                       l
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                f
                                                S
                                             
                                             *
                                             
                                                G
                                                
                                                   D
                                                   l
                                                
                                             
                                          
                                       
                                       
                                          p
                                       
                                    
                                    
                                       
                                          
                                             
                                                f
                                                S
                                             
                                             *
                                             
                                                H
                                                
                                                   D
                                                   l
                                                
                                             
                                          
                                       
                                       
                                          p
                                       
                                    
                                 
                                 ,
                              
                           
                        where * denotes the convolution between functions.

The filtered value JPWAS−I(p,I(p)) results from a linear interpolation of the filtered depth images J
                        PWAS−I(p,⋅) obtained for the different levels at position p and intensity value I(p) between I
                        
                           k
                         and I
                        
                           k
                           +1, i.e.,
                           
                              (19)
                              
                                 
                                    J
                                    
                                       PWAS
                                       −
                                       I
                                    
                                 
                                 
                                    
                                       p
                                       ,
                                       I
                                       
                                          p
                                       
                                    
                                 
                                 =
                                 
                                 interpolate
                                 
                                 
                                    
                                       
                                          J
                                          
                                             PWAS
                                             −
                                             I
                                          
                                       
                                       
                                          p
                                          ⋅
                                       
                                       ,
                                       I
                                       
                                          p
                                       
                                    
                                 
                                 =
                                 
                                    1
                                    
                                       s
                                       I
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                I
                                                
                                                   k
                                                   +
                                                   1
                                                
                                             
                                             −
                                             I
                                             
                                                p
                                             
                                          
                                       
                                       
                                          J
                                          
                                             PWAS
                                             −
                                             I
                                          
                                       
                                       
                                          p
                                          
                                             I
                                             
                                                k
                                                +
                                                1
                                             
                                          
                                       
                                       +
                                       
                                          
                                             I
                                             
                                                p
                                             
                                             −
                                             
                                                I
                                                k
                                             
                                          
                                       
                                       
                                          J
                                          
                                             PWAS
                                             −
                                             I
                                          
                                       
                                       
                                          p
                                          
                                             I
                                             k
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

The same applies to JPWAS−D(p,D(p)); thus from a linear interpolation between D
                        
                           l
                         and D
                        
                           l
                           +1:
                           
                              (20)
                              
                                 
                                    J
                                    
                                       PWAS
                                       −
                                       D
                                    
                                 
                                 
                                    
                                       p
                                       ,
                                       D
                                       
                                          p
                                       
                                    
                                 
                                 =
                                 
                                 interpolate
                                 
                                 
                                    
                                       
                                          J
                                          
                                             PWAS
                                             −
                                             D
                                          
                                       
                                       
                                          p
                                          ⋅
                                       
                                       ,
                                       D
                                       
                                          p
                                       
                                    
                                 
                                 =
                                 
                                    1
                                    
                                       s
                                       D
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                D
                                                
                                                   l
                                                   +
                                                   1
                                                
                                             
                                             −
                                             D
                                             
                                                p
                                             
                                          
                                       
                                       
                                          J
                                          
                                             PWAS
                                             −
                                             D
                                          
                                       
                                       
                                          p
                                          
                                             D
                                             
                                                l
                                                +
                                                1
                                             
                                          
                                       
                                       +
                                       
                                          
                                             D
                                             
                                                p
                                             
                                             −
                                             
                                                D
                                                l
                                             
                                          
                                       
                                       
                                          J
                                          
                                             PWAS
                                             −
                                             D
                                          
                                       
                                       
                                          p
                                          
                                             D
                                             l
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

Finally, the enhanced depth map JUML results from (7), considering JPWAS−I in (19), and JPWAS−D in (20).

As discussed by Paris et al. in [21] and also shown in Section 5.4, the enhanced depth map does not present significant errors when filtering using a downsampled version of the input data. This in turn reduces both running time and memory consumption. We therefore propose, in addition to quantizing the data range, to downsample the input data to be filtered, i.e., I↓
                        =downsample(I,
                        λ), D↓
                        =downsample(D,
                        λ), and Q↓
                        =downsample(Q,
                        λ), with ↓ standing for downsampled version, and λ being the downsampling scale factor.

Two low-resolution filtered depth maps JPWAS−I↓ and JPWAS−D↓ will result from Eq. (19), respectively (20), by replacing I and D by their respective downsampled versions I↓ and D↓. In order to recover JPWAS−I(p,I(p)) and JPWAS−D(p,D(p)), we spatially interpolate the low-resolution depth maps using a bi-linear (i.e., four point) interpolation, i.e.,
                           
                              (21)
                              
                                 
                                    J
                                    
                                       PWAS
                                       −
                                       I
                                    
                                 
                                 
                                    
                                       p
                                       ,
                                       I
                                       
                                          p
                                       
                                    
                                 
                                 =
                                 
                                 interpolate
                                 
                                 
                                    
                                       
                                          J
                                          
                                             PWAS
                                             −
                                             I
                                             ↓
                                          
                                       
                                       
                                          
                                             ⋅
                                             ,
                                             I
                                             
                                                p
                                             
                                          
                                       
                                       ,
                                       p
                                       /
                                       λ
                                    
                                 
                                 ,
                              
                           
                        and
                           
                              (22)
                              
                                 
                                    J
                                    
                                       PWAS
                                       −
                                       D
                                    
                                 
                                 
                                    
                                       p
                                       ,
                                       D
                                       
                                          p
                                       
                                    
                                 
                                 =
                                 
                                 interpolate
                                 
                                 
                                    
                                       
                                          J
                                          
                                             PWAS
                                             −
                                             D
                                             ↓
                                          
                                       
                                       
                                          
                                             ⋅
                                             ,
                                             D
                                             
                                                p
                                             
                                          
                                       
                                       ,
                                       p
                                       /
                                       λ
                                    
                                 
                                 .
                              
                           
                        
                     

We note that to perform this bi-linear, the low resolution depth maps JPWAS−I↓ and JPWAS−D↓ would have to be computed for each value I(p) and D(p) of their respective higher resolution versions. At this point, we combine both the linear range interpolation and the bi-linear spatial interpolation to a tri-linear (i.e., eight point) interpolation as follows
                           
                              (23)
                              
                                 
                                    J
                                    
                                       PWAS
                                       −
                                       I
                                    
                                 
                                 
                                    
                                       p
                                       ,
                                       I
                                       
                                          p
                                       
                                    
                                 
                                 =
                                 
                                 interpolate
                                 
                                 
                                    
                                       
                                          J
                                          
                                             PWAS
                                             −
                                             I
                                             ↓
                                          
                                       
                                       
                                          ⋅
                                          ⋅
                                       
                                       ,
                                       p
                                       /
                                       λ
                                       ,
                                       I
                                       
                                          p
                                       
                                    
                                 
                                 ,
                              
                           
                        and
                           
                              (24)
                              
                                 
                                    J
                                    
                                       PWAS
                                       −
                                       D
                                    
                                 
                                 
                                    
                                       p
                                       ,
                                       D
                                       
                                          p
                                       
                                    
                                 
                                 =
                                 
                                 interpolate
                                 
                                 
                                    
                                       
                                          J
                                          
                                             PWAS
                                             −
                                             D
                                             ↓
                                          
                                       
                                       
                                          ⋅
                                          ⋅
                                       
                                       ,
                                       p
                                       /
                                       λ
                                       ,
                                       D
                                       
                                          p
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

Thereby, JPWAS−I↓(⋅,⋅) and JPWAS−D↓(⋅,⋅) is the set of low resolution filtered depth maps calculated at each level I
                        
                           k
                         and D
                        
                           l
                        , respectively. The final output of the UML filter results, according to (7), from superposing the two filter outputs in (23) and (24), and using the credibility map Q that defines a pixel-dependent weight for each of the two contributions.

In practise, the spatial resolution of the original/raw depth map D acquired by a low-resolution ToF camera is lower than the resolution of its corresponding guidance image I. In such a case, it might not be necessary to upsample D to the same resolution of I, as discussed in Section 2.2, and then downsample it to obtain D↓. However, we have to ensure a good data matching between D↓ and I and thus, either D↓, I, or both data sources might be warped. Hence, a further optimization of the proposed real-time implementation can be obtained when considering that both the high-resolution depth map D, and thus the credibility map Q result from upsampling a low-resolution depth map, respectively its weighted gradient (approximated by the nearest pixel in the low-resolution versions of the maps). Consequently, the weighting of the superposition can be done on low-resolution level before the interpolation, and the tri-linear interpolation in (24) of JPWAS−D can be approximated by a bi-linear spatial interpolation of a single low resolution filtered image JPWAS−D↓
                        =JPWAS−D↓(⋅,D↓(⋅)). This interpolation for JUML takes the following form
                           
                              (25)
                              
                                 
                                    J
                                    
                                       U
                                       M
                                       L
                                    
                                 
                                 
                                    p
                                 
                                 =
                                 
                                 interpolate
                                 
                                 
                                    
                                       
                                          Q
                                          ↓
                                       
                                       
                                          ⋅
                                       
                                       
                                          J
                                          
                                             PWAS
                                             −
                                             I
                                             ↓
                                          
                                       
                                       
                                          ⋅
                                          ⋅
                                       
                                       ,
                                       p
                                       /
                                       λ
                                       ,
                                       I
                                       
                                          p
                                       
                                    
                                 
                                 +
                                 
                                 interpolate
                                 
                                 
                                    
                                       
                                          
                                             1
                                             −
                                             
                                                Q
                                                ↓
                                             
                                             
                                                ⋅
                                             
                                          
                                       
                                       
                                          J
                                          
                                             PWAS
                                             −
                                             D
                                             ↓
                                          
                                       
                                       
                                          
                                             ⋅
                                             ,
                                             
                                                D
                                                ↓
                                             
                                             
                                                p
                                             
                                          
                                       
                                       ,
                                       p
                                       /
                                       λ
                                    
                                 
                                 .
                              
                           
                        
                     

The main benefit of this implementation is, apart from some run-time optimization, the fact that no high resolution image except the 2-D image I has to be kept in memory.

In order to avoid filtering artefacts due to the data quantification and sampling introduced above, the standard deviations σ
                        I, σ
                        D, and σ
                        S shall be chosen greater than s
                        I, s
                        D, and λ, respectively. Otherwise, the approximation may be poor, i.e., numerically unstable. According to the above mappings (see Eqs. (12) and (14)), the noise due to quantization only affects the range mapping functions, i.e., 
                           
                              F
                              
                                 I
                                 k
                              
                           
                         and 
                           
                              H
                              
                                 D
                                 l
                              
                           
                        , and both the intensity values of the 2-D image I(q) as well as the depth measurements of the depth map D(q) are preserved.

Background pixels correspond to those pixels in the imager that have not been able to estimate a distance measurement. These pixels are identified during the generation of the raw depth map and set to a pre-defined value. In the case of ToF cameras, these pixels describe the background of the scene, i.e., a situation in which the light power density of the active illumination is too low. Background pixels must be identified and treated separately during the filtering process in order to not misuse their pre-defined value as a valid measurement. Indeed, if we do not pay special attention to background pixels, non-valid distance measurements might appear within the enhanced depth map. To do so, we propose to compute a relative background weight w
                        
                           bg
                         as follows
                           
                              (26)
                              
                                 
                                    w
                                    
                                       b
                                       g
                                    
                                 
                                 
                                    p
                                 
                                 =
                                 
                                    
                                       
                                          
                                             ∑
                                             
                                                q
                                                ∈
                                                
                                                   N
                                                   
                                                      b
                                                      g
                                                   
                                                
                                                
                                                   p
                                                
                                             
                                          
                                       
                                       ⋅
                                       
                                          f
                                          S
                                       
                                       
                                          p
                                          q
                                       
                                       ⋅
                                       Q
                                       
                                          q
                                       
                                       ⋅
                                       B
                                       
                                          q
                                       
                                    
                                    
                                       
                                          
                                             ∑
                                             
                                                q
                                                ∈
                                                N
                                                
                                                   p
                                                
                                             
                                          
                                       
                                       
                                          f
                                          S
                                       
                                       
                                          p
                                          q
                                       
                                       ⋅
                                       Q
                                       
                                          q
                                       
                                    
                                 
                                 ,
                              
                           
                        with N
                        
                           bg
                        (p) the neighbourhood of p identified as background pixels. B is a binary mask of same resolution as D in which only those pixels identified as background pixels in D are set to 1. Non-background pixels are set to 0. The final value for the selected pixel p within the enhanced depth map, i.e., JUML(p), will correspond to a pre-defined background value when w
                        
                           bg
                        (p)≥0.5. Otherwise, the filtered value resulting from (7) will be used. We note that in the above presented expressions, only valid depth pixels (non-background pixels) are considered within the neighbourhood of a pixel p, i.e., N(p).

@&#EXPERIMENTAL RESULTS@&#

In the following we provide a comprehensive analysis of the proposed depth enhancement filter. First, we quantify the improvement on depth accuracy. To do so, we compare the enhanced depth map against its raw version. A test box with known geometry and dimensions has been used for this purpose. Next, we perform a quantitative analysis in which we compare the UML filter against the alternative depth enhancement approaches introduced in Section 2. To this end, both real and synthetic data have been considered. Finally, a robustness analysis against noise and a runtime analysis of filter implementation described in Section 4 are provided.

We start the UML filter assessment with a quantitative analysis where the estimated size of a test box from both the raw depth map acquired by a ToF camera, and the enhanced depth map resulting from the UML filter are compared. For this quantification, we have used a camera rig that integrates a 3-D MLI Sensor from IEE S.A. [37] and a Dragonfly 2 CCD camera from Point Grey [38].

Both cameras have been coupled with a narrow baseline of 65mm and calibrated to achieve an accurate data alignment, as discussed in Section 2. Data synchronisation has been done using hardware triggers. Whereas the Dragonfly 2 video camera provides grayscale images of (648×488) pixels, the 3-D MLI Sensor provides raw depth maps of only (56×61) pixels. In practise, the resolution of the acquired depth map D is already much lower than the resolution of its corresponding guidance image I. Hence, one might think to directly use D in the real-time implementation scheme proposed in Section 4.2. However, we note that data matching between I and D↓ is a must for any depth enhancement approach. Herein we have adopted the backward warping approach proposed in [17]. By doing so, the resulting resolution of D↓ is of (80×60) pixels whereas the resolution of I is of (640×480) pixels. Therefore, we consider a magnification factor between the low-resolution depth map D↓ and the guidance image I of 8 times.

As shown in Fig. 4
                        , the quantification has been performed based on a test box with known geometry and dimensions (L=350mm, H=175mm, W=330mm), placed on a table and the camera rig facing from the top (top-view). Eight different setup configurations have been considered for the evaluation. That is, the test box has been displaced along the x, y, and z axes in order to cover the field-of-view and sensing range of the camera rig. Each recorded sequence contains a total of 20 frames.

In order to estimate the test box size reported in Table 1
                        , a bounding rectangle has been fitted to the test box area. The area computation results from segmenting the test box at the best depth threshold, i.e., a depth threshold that accurately segments the perimeter of the test box. However, we note that the selection of this threshold value is far from a trivial task when considering the raw depth map. Indeed, a slight variation on the depth threshold value may significantly affect to the area computation, as illustrated by the red circle in Fig. 4c. We remark the significantly difference along the left edge of the test box (see the plot of the box section in Fig. 4c) when considering either a depth threshold value of 750mm or 810mm. Contrary, it results much easier to select the depth threshold value from the enhanced depth map (see Fig. 4d), in which depth edges have been accurately aligned to those from the guidance image as they were low-weighted by the credibility map (see Fig. 4b). We note that, as discuss in Section 3, low-resolution depth pixels may cover foreground and background objects at the same time. Indeed, for the given setup, each 3-D MLI Sensor pixel roughly covers (16×25) mm of the scene, whereas a Dragonfly 2 camera pixel covers (2.8×2.8) mm. Table 2
                         reports the details of robustness against multiple selected depth thresholds. From the table, we can observe that, in contrast to the box size estimated from the raw depth map, the measurements from the enhanced depth map are always accurate regardless of the depth threshold.

The filling ratio in Tables 1 and 2 corresponds to the area differences between the ground truth area and the area of the fitted bounding rectangle. 100% means that the perimeter of the test box corresponds to the one of the bounding rectangle. The filling ratio has been calculated using the regionprops function from Matlab, a useful Matlab function to measure image regions properties. From the reported results in Table 1, we clearly notice a better and more accurate size estimation of the test box when considering the enhanced depth maps. The achieved accuracy on the lateral dimensions is of 1.4mm (L) and 1.2mm (W), which roughly corresponds to half of the Dragonfly® 2pixel resolution (note that in our setup, a Dragonfly 2pixel covers (2.8×2.8) mm). In contrast, the achieved accuracy from the raw depth maps is only 11.7mm (L) and 6.3mm(W). We also note that the filling ratio of the bounding rectangle is much higher when related to the enhanced depth map. This demonstrates that depth edges have been accurately aligned to those from the guidance image. The accuracy of test box height estimation is not related to the pixel resolution but to the depth precision, i.e., the depth estimation accuracy of the ToF camera. Nevertheless, there is an improvement of a factor of 3, which we also attribute to the achieved depth enhancement.

In the following, we perform a qualitative and quantitative evaluation of the proposed UML filter. To do so, a comparison against alternative depth enhancement approaches, including our previous PWAS filter has been performed. To address the qualitative evaluation we have considered our own recorded sequences using the camera rig presented in Section 5.1. For the quantitative evaluation, several scenes from the Middelbury dataset [39] have been considered.

We perform a qualitative evaluation in which we compare the enhanced depth maps resulting from the proposed UML filter against those from the JBU, NAFDU, and PWAS filters. We show, in Fig. 5
                           , the filters behaviour when setting σ
                           S
                           ∈[5,25]. We can observe that the JBU filter presents edge blurring when σ
                           S is too small, e.g., σ
                           S
                           =5 (see Fig. 5d), whereas it presents the texture copying artefact for large σ
                           S values, e.g., σ
                           S
                           =25 (see the copied texture within the test box in Fig. 5f). Thus, a trade-off between the tolerated edge blurring and texture copying must be empirically found. The NAFDU filter identifies depth variations related to depth edges from those related to noise and thus, it can be more robust to texture copying than the JBU filter. However, when filtering depth edges, i.e., α
                           ≃0 in (5), the filter behaviour is exactly the same as the one from the JBU filter. That is, edge blurring is still present due to the edge misalignment between the guidance image and the raw depth map. This effect is magnified when filtering depth edges with no corresponding guidance edge, as illustrated by the blue ellipse in Fig. 5f. We note that ε and τ have been manually tuned to achieve the best performance, i.e., ε
                           =1/σ
                           S
                           2. A limit case occurs when setting σ
                           S too large, e.g., σ
                           S
                           =25. In such a case, the NAFDU and JBU filter behaviours are identical, as demonstrated in Fig. 5f and i. Fig. 5j and k show the responses of the PWAS filter. As can be observed, the PWAS filter effectively addresses the edge blurring artefact with a significant reduction of texture copying. It also has a much better behaviour when filtering depth edges with no corresponding guidance edge. Indeed, edge blurring is not related to the value of σ
                           S, but limited to the boundary defined by the low-weighted depth pixels in the credibility map, as shown in Fig. 5. However, texture copying may appear if σ
                           S is too large, e.g., σ
                           S
                           =25. On the contrary, the UML filter (see the last row of Fig. 5) perfectly addresses both edge blurring and texture copying artefacts, regardless of the value of σ
                           S. In a similar way as its previous version, the UML filter also handles depth edge filtering when no corresponding guidance edge is available.

Another visual example of the behaviour of the UML filter on a dynamic scene is shown in Fig. 6
                           . In this case, the UML filter is able to upsample depth maps of (80×60) pixels (see 2
                              nd
                            column of Fig. 6) to the VGA resolution of their respective guidance images (see 1
                              st
                            column of Fig. 6), with a significant noise reduction. Thanks to the credibility map contribution, low-weighted depth pixels, i.e., Q≃0, are efficiently treated and thus, fine details such as fingers can be resolved. This is shown in Fig. 6d, h, and p. However, the UML filter presents a few limitations. Indeed, edge blurring may appear if the misalignment between the guidance image and the low-resolution depth map is larger than low-weighted edge boundaries in the credibility map. In addition, this effect can be emphasized in the case of guidance images with low-contrast edges, as shown in Fig. 6p. More details can be observed in Fig. 7
                            where, in addition, a visual comparison against JBU, NAFDU, and PWAS filters is provided. Note that the UML filter behaves as the PWAS filter along depth edges providing the same response. However, the distance accuracy of high-weighted depth pixels by the credibility map is significantly increased. Finally, we remark that contrary to the PWAS and UML filters, the JBU and NAFDU filters have some difficulties to enhance low-resolution depth maps that are not perfectly aligned with their corresponding guidance image. This effect is shown in Fig. 7 where data misalignment cannot be avoided due to the poor resolution of the raw depth map to be enhanced.

The quantitative evaluation of the UML filter is performed over the Teddy, Cones, Art, Dolls, Moebius, Books, and Laundry scenes from the Middlebury dataset. Each synthetic scene provides an intensity image with its corresponding disparity map; from which we have generated their respective ground truth depth map J (using the provided sensing system specifications). Different downsampling factors have been applied to J in order to generate their corresponding low-resolution D↓ version. This in turn enables to reproduce a similar scenario as when using a ToF camera.

As expected, and shown in Fig. 8
                           , the UML filter presents a similar behaviour when filtering either synthetic or real data. That is, the enhanced depth maps present the same resolution and details as their corresponding guidance images with a global reduction of the amount of noise. However, we note that tiny objects in the scene that are represented by a very few depth pixels, are often tackled as outliers. Indeed, depth pixels related to tiny objects are generally low weighted by the credibility map and thus, their value is replaced by the value of valid neighbourhood pixels. This undesired effect is shown on the pencils from the Art scene, which are not resolved in the resulting depth map (see Fig. 8l). However, the UML response when filtering larger surfaces is much more accurate than those of JBU or NAFDU and thus, on average, a better performance is achieved, as reported in Table 3
                           . The root mean square error (RMSE) is a frequently-used measure to quantify the errors between a treated image and its reference image. However, RMSE cannot provide a clear quantification between two different depth enhancement approaches. Indeed, RMSE does not distinguish small depth variations within smooth regions from larger variations of few pixels along depth edges (note that depth maps are generally encoded by 16 bits). Therefore, an alternative complementary framework for quality assessment based on the degradation of structural information, the Structural SIMilarity (SSIM) Index [40], is also provided. Table 3 reports both the SSIM and RMSE measures of this evaluation. We can observe that under a global error measure, the UML filter performs at least as well as the PWAS filter. However, a much clear impression of the UML filter performance can be noticed from Fig. 9
                           , where we show the visibility of errors between the filters' response and their respective ground truth depth map. Note the close to 0 value peak in the histogram of the differences between the ground truth J and the enhanced depth map JUML (see Fig. 9h). Tiny depth variations are mainly located along the low-weighted depth boundaries by the credibility map, where the UML filter behaves as the PWAS filter. We note that the reported SSIM measures are slightly different from those reported in our previous works [13,41]. The reason is that the non-valid/background depth pixels have not been considered in this evaluation.

The main sources of noise within the distance measurements in continuous modulated ToF cameras are electronic noise, dark noise, and photon shot noise [42]. Electronic noise is a random fluctuation which is characteristic of all electronic circuits such as analog to digital converters. Dark noise summarizes additional photodetector noise sources such as thermal noise. Photon shot noise is due to the photon character of light. The generation of a given number of photoelectrons in a fixed interval of time occurs randomly with a known average rate and independently of time. Therefore, the photon shot noise can be modelled by a Poisson distribution. However, since the number of observed photoelectrons is large, the Poisson distribution can be approximated by a Gaussian distribution. Similarly, the electronic noise may also generate or vary the number of photoelectrons while converting from analog to digital. In the same way, thermal noise also varies the number of photoelectrons by the excited photoelectrons from the temperature. Therefore, they both can be also modelled as additive Gaussian noises with zero mean.

In the following, we simulate the ToF sources of noise behaviour by adding Gaussian noise with a standard deviation linearly dependent on the distance measurement [42]. To do so, we have considered the Teddy scene downsampled by a factor of 2×, to which we have added up to ±100mm of noise at the maximum distance. The results in the graph from Fig. 10
                         were obtained by a Monte Carlo simulation [43] over 100 iterations, which gave us an accuracy of ±1.2×10−3, ±2.2×10−4, ±2.2×10−4, and ±2.2×10−4 for the JBU, NAFDU, PWAS, and UML filters, respectively. Depth map border and non-valid depth pixels have not been considered in the evaluation. Within individual executions only the last digit varies. Then, from Fig. 10 we note that the analysed filters share a similar noise related behaviour being the UML filter the most robust to the added noise.

We next present a runtime analysis that shows that the UML filter implementation proposed in Section 4 enables the use of low-resolution ToF cameras in applications where real-time is required. All reported results have been obtained using an Intel Core 2 Solo processor SU3500 (1.4GHz, 800MHz FSB) with an Intel GMA 4500MHD integrated graphic card. JBU, PWAS and UML filters have been implemented following the implementation scheme proposed in Section 4 using C++ and the OpenCV [44] library. However, we note that the written code has not been optimized.

As in the previous section, the Teddy scene has been considered for this analysis. Both the guidance image I and the ground truth depth map J have been mapped to (640×480) pixels. Low-resolution depth maps D↓ result from downsampling J by a factor of 2×, 4×, 8×, and 16×. The enhanced depth maps have the same (640×480) pixels resolution as their corresponding guidance images I, regardless of the selected downsampling factor. We report, in Table 4
                        , the computation time required by each of the analysed depth enhancement approaches related to the downsampling factor. Reported values have been calculated over 1000 iterations. As can be observed from the table, a significant time consumption decrease is achieved by downsampling the data to be filtered. Indeed, only 180ms are needed to enhance a low-resolution depth map D↓ to the 8 times higher resolution of its corresponding guidance image. Indeed, without code optimization we can ensure around 6 fps, whereas the 3-D MLI Sensor provides up to 11 low-resolution depth maps per second. We note that more than 7s are needed to generate an enhanced depth map using a brute force implementation of the JBU filter (also in C++). In this case, the output is very similar to our JBU filter implementation at 1× (without data downsampling), where only 1.88s are required. That is, a factor close to 3 is achieved by range data quantization (see Section 4.1) whereas the rest of the consumption time improvement is due to data downsampling (see Section 4.2).

Also from Table 4 we observe that both JBU and PWAS filters share a similar consumption time. Indeed, the time required to compute the additional factor within the kernel in (6), i.e., the credibility map Q, can be negligible. However, this is not the case of the UML filter, as it requires twice the computation time of the PWAS filter. Although in this analysis the two PWAS filters to generate the UML response are consecutively performed, we note that they are totally independent and thus, they can run in parallel. By doing so, we would reduce the time complexity of the UML filter to the one of the PWAS filter, i.e., 60ms per frame when donwsampling by a factor of 8×.

To quantify how the quality of the resulting depth map is affected by the downsampling factor we need to refer to Table 3. Indeed, we can appreciate that the higher the downsampling factor, the worse is the quality of the resulting depth map. However, the loss of quality is worth compared to the gain on processing time, mainly if this enables the use of the ToF camera in applications where it was restricted by its resolution. The NAFDU filter has not been considered in this analysis because its implementation has not been optimized for real-time.

The UML filter presented herein upsamples a low-resolution depth map to the resolution of its corresponding guidance image while improving the accuracy and precision of its depth measurements. Common artefacts in low-level data fusion such as texture copying and edge blurring have been almost entirely eliminated by using a confidence measure that describes the confidence of each pixel within the low-resolution depth map. Nevertheless, although edge blurring have been significantly reduced, it may still appear when filtering low confidence depth pixels with no corresponding 2-D guidance edge. In general, the lack of 2-D guidance edges is due to the RGB to grayscale transformation of the guidance image, which is a must for real-time applications. This situation occurs in Fig. 6p, where the shirt's intensity values are too close to the intensity values from the background. As a result, no guidance edge is present to which to align the low-resolution depth edge, which yields to edge blurring in the resulting depth map.

Despite the high computational time and memory requirements, Paris et al. [21] conducted a bilateral filter study applied to colour images. They showed that edge blurring or bleeding effects still appear whether the filtering considers each RGB channel independently or the three channels together. Indeed, edges may be visible in one channel while not visible in another. However, if one transforms the image from the RGB space to a perceptually meaningful colour space such as the CIE-Lab space, the colour-bleeding problem should be overcome, but not the demanding memory and computation time. We suggested in [27] a new 1-D colour model that exploits the geometrical structure of 3-D conical colour spaces and showed how to accurately define one parameter to represent the solid HCL conical colour space. With this compact colour representation, it becomes possible to overcome the limitations due to grayscale image filtering without the need of modifying the filtering algorithm. However, a simple and discriminative distance is to be defined for effective real-time colour filtering.

In order to achieve a close to real-time performance, we proposed in [41] to not consider the full 3-D colour information but only the pixel-wise RGB channel that best describes the 2-D edge. Although the processing time is slightly increased, this UML filter extension that we named RGB-D filter addresses the edge blurring artefact owing to RGB to grayscale transformation. The RGB-D filter selects among the channels of the colour guidance image the best pixel-wise channel to guide the filter along a depth edge. To this end, we defined specific confidence measures to identify the channel that pixel-wise presents the highest contrast. Real-time was achieved after adapting the quantization and downsampling techniques presented in Section 4. We have to note that the use of the RGB-D filter ensures a much better accuracy and precision on the enhanced depth edges. Unfortunately, texture copying appears within depth areas with no depth information, i.e., depth holes or occluded regions. This is due to selecting the RGB channel that presents the highest pixel-wise contrast in the filter and thus, the most textured region.

Although the RGB-D filter was intended for RGB-D cameras, it can also apply to alternative sensing systems such as ToF cameras. To this end, we advise in [41] on how the reliability weight β in (7) has to be set to deal with depth discontinuities along object boundaries. An alternative UML configuration to enhance depth data from passive stereo sensing systems was also proposed in [45]. In that work the filter accounted for occlusions along object boundaries but failed when filling depth holes.

@&#CONCLUSION@&#

In this paper, we have presented a new multi-lateral filter to correct inaccurate and/or erroneous depth measurements in a given low-resolution depth map, and produce an enhanced version with the same resolution as the 2-D guidance image. Our main contribution is directed towards the combination of two separate normalized kernels with each one considering a different data source. That is, intensity information from the 2-D guidance image and depth information form the depth map. To this end, we have proposed a confidence measure, the so-called credibility map, that weights each depth pixel according to the reliability of its depth measurement. Low-weighted credibility map pixels define unreliable depth measurements, whereas high-weighted credibility map pixels define reliable depth measurements. Then, by incorporating the credibility map in the filter formulation, non-reliable depth measurements are not considered within the filtering process. By doing so, we significantly reduce the number of non-valid depth values in the resulting depth map. In addition, from the use of two normalized kernels we address the texture copying artefact within reliable depth regions, i.e., 2-D information is not taken into account.

A real-time implementation based on data quantization and downsampling has been proposed and shown in the experimental evaluation. A downsampling factor of 8× which coincides with the difference of resolution between our ToF camera and the 2-D camera coupled with it, presents a good trade-off between the computation time and the resulting quality of the enhanced depth map. We note that the real-time constraint restricts the use of alternative reliability measures of depth value [46] that would improve the final result. The experimental results show that our filter outperforms previous depth enhancement approaches, delivering better results even when filtering depth edges with no corresponding 2-D guidance edge. Applications in which the low-resolution and noisy depth measurements were restricting the use of ToF cameras can then benefit from the proposed depth enhancement approach to improve their performance. To conclude, the proposed filter is not limited to ToF data but it can be adapted to enhance depth data from any alternative depth sensing system such as structured light based systems, stereo vision based systems, or laser scanners. In these cases, we have also provided the references where we describe how to configure the UML filter as well as its components for each specific sensing system.

@&#ACKNOWLEDGMENTS@&#

This work was supported by the National Research Fund, Luxembourg, under the CORE project C11/BM/1204105/ FAVE/Ottersten.

@&#REFERENCES@&#

