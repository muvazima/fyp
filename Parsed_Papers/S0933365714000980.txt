@&#MAIN-TITLE@&#Biomedical visual data analysis to build an intelligent diagnostic decision support system in medical genetics

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           The proposed methodology, visual diagnostic DSS, employs ML algorithms and image processing techniques for automated diagnosis in medical genetics.


                        
                        
                           
                           The proposed system was trained using a real dataset of previously published face images of subjects with syndromes.


                        
                        
                           
                           A high accuracy rate was achieved using this automated diagnosis technique.


                        
                        
                           
                           The results show that the accurate classification of syndromes is feasible using ML techniques.


                        
                        
                           
                           The study demonstrates the benefits of using hybrid image processing and ML-based computer-aided diagnostics for identifying facial phenotypes.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Decision support system

Machine learning

Visual data analysis

Principal components analysis

Medical genetics

Dysmorphology

Facial genotype–phenotype

@&#ABSTRACT@&#


               
               
                  Background
                  In general, medical geneticists aim to pre-diagnose underlying syndromes based on facial features before performing cytological or molecular analyses where a genotype–phenotype interrelation is possible. However, determining correct genotype–phenotype interrelationships among many syndromes is tedious and labor-intensive, especially for extremely rare syndromes. Thus, a computer-aided system for pre-diagnosis can facilitate effective and efficient decision support, particularly when few similar cases are available, or in remote rural districts where diagnostic knowledge of syndromes is not readily available.
               
               
                  Methods
                  The proposed methodology, visual diagnostic decision support system (visual diagnostic DSS), employs machine learning (ML) algorithms and digital image processing techniques in a hybrid approach for automated diagnosis in medical genetics. This approach uses facial features in reference images of disorders to identify visual genotype–phenotype interrelationships. Our statistical method describes facial image data as principal component features and diagnoses syndromes using these features.
               
               
                  Results
                  The proposed system was trained using a real dataset of previously published face images of subjects with syndromes, which provided accurate diagnostic information. The method was tested using a leave-one-out cross-validation scheme with 15 different syndromes, each of comprised 5–9 cases, i.e., 92 cases in total. An accuracy rate of 83% was achieved using this automated diagnosis technique, which was statistically significant (p
                     <0.01). Furthermore, the sensitivity and specificity values were 0.857 and 0.870, respectively.
               
               
                  Conclusion
                  Our results show that the accurate classification of syndromes is feasible using ML techniques. Thus, a large number of syndromes with characteristic facial anomaly patterns could be diagnosed with similar diagnostic DSSs to that described in the present study, i.e., visual diagnostic DSS, thereby demonstrating the benefits of using hybrid image processing and ML-based computer-aided diagnostics for identifying facial phenotypes.
               
            

@&#INTRODUCTION@&#

Dysmorphology is an area of clinical genetics that is concerned with abnormal patterns of human development and syndrome diagnosis in patients who possess congenital malformations and unusual facial features, often with delayed motor and cognitive development [1]. A high degree of experience and expertise is required to diagnose a dysmorphic patient correctly [2] because most of these syndromes are very rare. However, in some parts of the world, the diagnosis of syndromes is generally performed by medical professionals who are not well trained in dysmorphology, such as general practitioners, pediatricians, or dermatologists, rather than medical geneticists, because the latter are scarce. In general, the diagnosis of dysmorphology is conducted based on databases that contain limited numbers of images, which use standard terminology. Medical professionals might not be highly familiar with this terminology, especially in areas where expert knowledge is not readily accessible, which may make it difficult to obtain correct diagnoses. These challenges may lead to diagnostic inaccuracy, thereby compromising the appropriate treatment for patients to suit their specific needs as well as the provision of adequate guidance to the parents of patients. Delays in diagnosis may also hinder access to critical services, such as clinical trials, and a patient's referral to supportive services, including early intervention, physical therapy, and occupational therapy. Correct diagnoses and appropriate treatments, particularly during the early stages, can influence the course of dysmorphic diseases. For example, bone marrow transplantation or enzyme replacement therapy can now be offered for some innate metabolic disorders (e.g., Fabry disease) [1], in addition to many other specific treatments for other syndromes.

The face is acknowledged to be the attribute that best distinguishes a person from others, even at the first glance. Facial features provide many clues about the identity, age, gender, and even ethnicity of a person. The face may be influenced by many genes, particularly the genes related to syndromes, and the face provides significant information related to dysmorphology in many cases. Thus, the facial appearance is a significant cue during the early diagnosis of syndromes that are generally associated with cognitive impairments. Therefore, many decision support systems (DSSs) for dysmorphic diagnosis have been developed based on anthropometry, particularly craniofacial anthropometry, as well as stereophotogrammetry. Anthropometry is used to measure the weight, size, and proportions of the human body [3], while craniofacial anthropometry measures the distances between landmarks on the surface anatomy of the head [4]. Stereophotogrammetry employs multiple views in two-dimensional (2D) images to generate three-dimensional (3D) images [5]. Previous studies have shown that many syndromes can be diagnosed correctly using computer-aided face analysis DSSs [2,3,6–8]. In particular, Farkas [3] was the first to study facial morphology based on anthropometry using several methods. These techniques include the use of rulers, protractors, calipers, and tape measures, and they have been applied widely in the analysis of facial dysmorphology [8]. Similar craniofacial analyses that compare a patient's phenotype to the standardized norms in a control population are employed by many clinicians [8].

A possible approach for diagnosing dysmorphic patients is to define rule sets and to apply them manually based on standardized norms. This approach may be feasible in some cases, but it has many drawbacks and is prone to errors. In practice, it is very difficult for health care professionals to keep track of all the relevant up-to-date knowledge regarding syndromes and to deal effectively with large volumes of information in many dimensions [9]. Indeed, humans might not be able to develop a systematic response to any problem that involves more than seven variables [10]. Moreover, constructing and employing rule sets is also a labor-intensive process.

Assigning faces to classes based on appearance is unlikely to be accepted by medical professionals unless the mathematical features determined and identified by feature selection algorithms for discrimination can be related to facial patterns [11]. Several methods have been applied previously to the detection and analysis of facial patterns, such as principal components analysis (PCA), kernel PCA, independent components analysis, probability density estimation, local feature analysis, elastic graph matching (EGM), multi-linear analysis, kernel discriminant analysis, Gabor wavelet (GW), Fisher's linear discriminant analysis (LDA), and support vector machines. In particular, EGM, Fisher's LDA, GW, and PCA using eigenfaces have been employed widely to extract features from a face region. The high accuracy of these methods for extracting features and subsequently discerning patterns in faces has been demonstrated in many studies. Among these popular techniques, it is not easy to choose the best to implement a diagnostic DSS for dysmorphology. Thus, we suggest the use of ensembles of some of these methods to reduce error rates in future research. However, excellent results can be achieved using PCA based on feature extraction and the subsequent discernment of people from others, with accuracy rates of up to 96% [12]. Using PCA, good success rates can be obtained by detecting patterns in faces from images captured in ideal environments, particularly with good illumination, or by employing several image processing techniques to enhance images before feature extraction. PCA is an optimal transition scheme that minimizes the mean squared error between an image and its reconstruction [13]. PCA using eigenfaces is computationally efficient compared with other similar methods [14] because reducing the dimension from 2D to one-dimensional can be performed easily to accelerate the calculations. Thus, we employ the PCA-based eigenface method to extract features from faces in our visual diagnostic DSS. Furthermore, this machine learning (ML) method was selected because of its extensive and successful applications to many datasets.

In addition, Bayesian decision theory, multiple similarity, city block distance, subspace, Mahalanobis distance, and Euclidean distance are well-known methods for measuring the distance between two points in a features dataset [13]. The Mahalanobis distance and Euclidean distance are the most widely used of these methods [13]. However, Kapoor [13] showed that the Mahalanobis distance is more effective than the Euclidean distance. It differs from the Euclidean distance because it considers the correlations in the dataset and it is scale-invariant, i.e., it is not dependent on the scale of measurements [13,15].
                        1
                     
                     
                        1
                        More information about the Mahalanobis distance can be found in Gul's thesis [15].
                      In the present study, we tested these two methods using our features dataset to determine the best for use in our method, and we found that the Euclidean distance outperformed the Mahalanobis method for measuring distances. Thus, this matching technique was selected for our study.

Hammond [8] claimed that the analysis of 2D or 3D facial morphology images using computer-aided DSSs based on genotype–phenotype correlations could potentially benefit syndrome diagnosis, and our study supports this claim. The method established in the present study is called visual diagnostic DSS. This method aims to provide the required on-site expertise, but it also attempts to eliminate the time-consuming search of catalogs by practitioners and geneticists to diagnose syndromes, because there are approximately 4700 known syndromes.
                        2
                     
                     
                        2
                        Many new dysmorphic diseases are described each year in the London Dysmorphology Database (http://www.lmdatabases.com [accessed 25.01.14]).
                     
                  

In the proposed methodology, ML algorithms and digital image processing techniques are employed in a hybrid approach to detect meaningful facial features in reference images of disorders by indicating visual genotype–phenotype interrelationships. The proposed system was trained using a real dataset constructed from previously published images of dysmorphic faces, which included accurate diagnostic information about the syndromes considered in the present study. After training, during the diagnosis phase, the system compares the patient's facial features to all the trained features in the database to obtain a ranked list of possible matches based on confidence values above the threshold value specified by the user. The ranking list with similarity values explains how similar a disease is to those classified in the database relative to a particular threshold value. The application can be implemented easily at any site. New syndromes can be trained and the dataset can be extended by the end user to improve the implementation. Our statistical method represents facial image data in terms of principal component (PC) features and it diagnoses syndromes using these features. We evaluated the accuracy of the method using a leave-one-out cross-validation scheme.

@&#METHODOLOGY@&#

@&#BACKGROUND@&#

We evaluated previous studies of visual DSSs in terms of their advantages and disadvantages as a first step to facilitate the production of an effective diagnostic system for dysmorphology. Most previous genotype–phenotype association studies have focused on a limited number of specific diseases or traits to test whether a computer can classify syndromes and then diagnose new cases based on comparisons with the training cases. Previous studies have reported successful classification [2,6,7,16] using several methods (e.g., using dense surface models for the construction of 3D images
                           3
                        
                        
                           3
                           The drawbacks of using 3D construction for syndrome delineation are mentioned in the Discussion section.
                        ) based on images of children with a limited number of syndromes, typically 5–10. Most of these studies involved labor-intensive data preprocessing steps, such as manual cropping of images to obtain faces from whole images, and several image enhancement methods in various applications, including commercial tools, were used to obtain better images for further processing. In general, these studies addressed a single aspect of the needs of medical professionals, such as recognizing a syndrome from a photograph based on comparisons with several trained syndromes, thereby developing a specific computer-based system rather than providing a complete solution to meet the needs of the overall field of dysmorphology. The field of dysmorphology needs a composite solution that allows the automatic diagnosis of dysmorphic diseases from raw data (live/video inputs or photographs) without human intervention and that also satisfies the everyday needs of medical professionals when considering their cases, by recognizing a wide range of dysmorphisims, especially when appropriate genetic tests are not available. Moreover, a solution is needed to inform subsequent investigations, including more appropriate genetic testing, thereby avoiding or delaying the need to undertake more expensive genetic tests.

Our evaluation of related studies forms the basis of the software requirements analysis, which comprises behavioral, architectural, and functional requirements. Our proposed method is based on the idea that incorporating several well-known methods into a new hybrid approach may facilitate better diagnosis by clinicians in several ways, as suggested in previous studies [17,18] that used hybrid approaches.

The visual diagnostic DSS method was developed using the C++ programming language and it comprises several main modules: 1, image acquisition and face detection; 2, image enhancement and feature extraction; 3, training; and 4, diagnosis. These main modules are divided into several sub-modules as shown in Fig. 1
                        . The interface of the implemented method is depicted in Fig. 2
                        . We briefly explain the main modules in the following subsections.

The images of patients are captured by this module for further analysis. Frontal face images related to different syndromes can be acquired in various environments, e.g., using a camera attached to a computer in real time, from previously recorded videos of patients with syndromes, or from a self-maintained dataset of images stored in a folder, as shown in the image acquisition and face detection phase in Fig. 1.

A high-resolution digital camera is mounted across a dysmorphic patient and images can be captured automatically, provided that a frontal face image is detected. The time required to capture an image is 0.2s, which is triggered by the system automatically, excluding the time when an appropriate frontal face image cannot be acquired. Thus, image capture is instantaneous, which may be more suitable for photographing children with mental retardation who are unable to hold a pose for long periods or who may be uncooperative. Photographs that lack frontal face images are not saved and the application remains idle while searching for an appropriate frontal face image during this period, especially when the head turns to the sides, up, or down. Similarly, frontal face images of dysmorphic patients can be acquired from video inputs as well as from local databases. Acquiring proper frontal faces is an essential requirement for the automatic data preparation and model-building phase. Images that lack appropriate frontal faces, as shown in Fig. 3
                           , are not captured and saved. Thus, preset frontal faces are ready to be treated and they require no manual preprocessing or data preparation steps. The characteristics of the dataset used in this study are described in Section 2.3.

Images of patients are acquired for training a specific syndrome or for diagnosis depending on the function triggered by the user. The images are processed automatically for enhancement and feature extraction before the diagnosis or training phases. After acquisition, the images are converted into grayscale and cropped to include only face regions, i.e., the forehead, two eyes, cheeks, and mouth, as shown in Fig. 2, which are delineated as shown in Fig. 3. The size of a cropped image depends on the resolution, image size, and the face occupation area of the image. Thus, comparisons between features in subsequent processes are not possible using images of different sizes. Therefore, the cropping stage is followed by normalization of the image using well-known interpolation and extrapolation methods. In the first training stage employed to establish classifiers for syndromes, all of the images are cropped initially and an average mapping size is calculated by the application based on all the sizes of the cropped images, and all of the cropped images are then scaled to this new specified size. Some of the images are extrapolated if their height and width are smaller than the average size, whereas they are interpolated otherwise. In the following phases, i.e., diagnosis and the individual training of other syndromes, normalization is performed according to the previously specified averaged size. Thus, each cropped image becomes smaller or larger and it is mapped to the same size in terms of the specified width and height before comparisons are made among similar features.

The images are then standardized using two essential image enhancement methods, i.e., histogram equalization and median filtering, to remove illumination variations and to obtain standard brightness and contrast levels. Thus, excessively dark or low contrast images are enhanced and better features can be captured.

We apply PCA to the standardized face images to extract significant features for classification. The PCA method accelerates our application during the training and diagnosis phases due to its simplicity, learning capacity, and reduced computational complexity. A set of images in a high-dimensional feature space is transformed into a lower-dimensional feature space with a set of feature images, i.e., using a few eigenfaces (e.g., PCs). The first eigenface represents the direction where the data has its maximum variance, which considers the most valuable feature of the images in the training set. Each subsequent eigenface has the next most valuable feature with respect to its variance. To implement PCA, the color images are converted into 8-bit grayscale images. Next, rows of these gray images are placed into a column vector (I) because PCA works on vectors instead of images. An average face vector that represents the mean image (ψ) is obtained using Eq. (1), as follows:
                              
                                 (1)
                                 
                                    ψ
                                    =
                                    (
                                    I
                                    1
                                    +
                                    I
                                    2
                                    +
                                    ⋯
                                    +
                                    
                                       I
                                       m
                                    
                                    )
                                    /
                                    m
                                    )
                                    ,
                                 
                              
                           where m represents the number of images in the dataset.

Next, we normalize the images by subtracting the mean vector from each image vector (θ
                           
                              i
                           
                           =
                           I
                           
                              i
                           
                           −
                           ψ). This process generates unique features that differ from the mean image for each face, i.e., the mean vector. A covariance matrix is also calculated from this normalized feature matrix to prepare a subspace with reduced dimensionality. Eigenvectors can be calculated from the covariance matrix (C) as: C
                           =
                           AA
                           
                              T
                            for real space and C
                           =
                           A
                           
                              T
                           
                           A for subspace, where A
                           =[θ
                           1, θ
                           2, θ
                           3, …, θ
                           
                              m
                           ]. Single value decomposition is then performed to obtain the most significant eigenvectors as A
                           =
                           UDV
                           
                              T
                           , where D corresponds to the diagonal (σ
                           
                              i
                            singular values of A), U corresponds to the eigenvectors of AA
                           
                              T
                            for real space, and V corresponds to the eigenvectors of A
                           
                              T
                           
                           A in subspace. We select the k most significant eigenvectors to operate on the subspace to reduce the cost of calculations and to significantly reduce the noise of the dataset. The k top columns are picked from V for A
                           
                              T
                           
                           A to obtain the top k eigenvectors. After this step, the images are represented by a reduced number k of eigenvectors. A weight vector 
                              [
                              
                                 w
                                 1
                              
                              ,
                              
                                 w
                                 2
                              
                              ,
                              
                                 w
                                 3
                              
                              ,
                              …
                              ,
                              
                                 w
                                 k
                              
                              ]
                            is obtained for each image in terms of the contribution of k eigenvectors based on several matrix operations, which describe the contribution of each eigenvector (e.g. 
                              
                                 w
                                 1
                              
                            represents the contribution of the first eigenvector that comprises the image) to the representation of an image. Each image can be regenerated from the sum of the mean vector and the weighted sum of these k eigenvectors, which is a linear combination of the best k eigenfaces. Thus, these weights represent the face as a combination of k eigenfaces, which indicates the proportion of each image comprised by the eigenface relative to the sum of the mean image (ψ).

Comparisons are made between the weight vectors of the input image and those of the other images in the dataset to calculate the extent of similarity, as described in detail in Section 2.2.4.

Our statistical method represents facial image data in terms of PCs and a leave-one-out evaluation scheme is used to quantify the accuracy. In the leave-one out-cross-validation scheme, we leave one image out as a test image and train the system using the remaining images. This process is repeated as many times as there are images (n) in the dataset, so every data point is used as a test sample to measure its similarity to the others (n
                           −1).

Images of syndromes are captured for feature extraction during training or diagnosis using the functions shown in Fig. 2, as described in the following subsections. The selection of the number of PCs used is explained in Section 2.4 with respect to the size of the dataset.

The implementation of PCA requires the manipulation of the eigenvectors/eigenvalues of the covariance matrix, rather than the raw image data, as mentioned above. Thus, the eigenvectors/eigenvalues, the average image vector, and the weight vectors generated by PCA are stored in a database with their syndrome names in the training phase. Classifiers are trained with the features extracted by the image enhancement and feature extraction module. A classifier that includes the features of all cases of any specific syndrome is generated for each syndrome. At least two images per syndrome are required to train a classifier. The measurements in the database are recalculated as the new syndromes are trained. Users can easily add new syndromes themselves using the functions “train from directory” for any syndrome stored in a directory and “train captured images” for a syndrome where the images are captured from a video/live input. The function “train all database” allows the user to train all of the syndromes at once according to the implementation shown in Fig. 2. In this case, the directory names are the inputs for the syndrome (classifier) names, thereby allowing automatic training without human intervention.

The trained classifiers are employed for prediction in this module. The diagnostic prediction of a patient requires the detection of a frontal face image from a camera or a file, conversion into a grayscale image, and processing by the two image enhancement methods mentioned above, which are followed by cropping, normalization (θ
                           
                              i
                           
                           =
                           I
                           
                              i
                           
                           −
                           ψ), and projection of the normalized vector onto the eigenface space to obtain the weights in a weight vector 
                              [
                              
                                 w
                                 1
                              
                              ,
                              
                                 w
                                 2
                              
                              ,
                              
                                 w
                                 3
                              
                              ,
                              …
                              ,
                              
                                 w
                                 k
                              
                              ]
                           . These weights represent the test face as a combination of k eigenfaces, which indicate the proportion of the image comprised by each eigenface relative to the summed mean image (ψ). The weight vectors of the input image (a test image in a broader sense) are compared with those of the images classified after training, where the weight vectors are the only variables and the other parameters are constant, such as eigenfaces and mean vectors. Thus, a diagnosis can easily be obtained within a few seconds by comparing several features.

The comparison is performed for each trained image in the database to identify all similar syndromes that exceed the threshold value supplied by the user. The best matches that exceed the threshold value are found for the syndromes with the minimum distances. These syndromes are the probable diagnoses, which are then displayed to the user with confidence values. These values represent the similarity of the input image to those in the trained set and they are used to assess the reliability of the proposed diagnostic inference.

Many algorithms are available for comparing the weight vector of the input image to those of the trained images (measurement of the distance between two points) such as the city block distance, sub-space, multiple similarity, and Mahalonobis distance [19]. However, a simple and intuitive approach is to compare an individual face with other faces in the vector space using nearest-neighbor classification, i.e., the Euclidean distance.
                              4
                           
                           
                              4
                              Interested readers may read Calva's article [20] for more information about Euclidean distance formulations and their implementation for comparing the weight vector of a test image to those of trained labeled images.
                            In image recognition, Euclidean distance comparisons aim to capture how similar or different a test object is compared with trained objects in terms of their weight vectors. All the PCs of the test image, which are represented as weight vector values, 
                              [
                              
                                 w
                                 1
                              
                              ,
                              
                                 w
                                 2
                              
                              ,
                              
                                 w
                                 3
                              
                              ,
                              …
                              ,
                              
                                 w
                                 k
                              
                              ]
                           , are compared with the eigenvectors of the other images in the dataset and the mean total distances obtained from each comparison (in terms of the distances between the eigenvectors compared) correspond to the degree of difference between two images. The formula used to obtain the distances between the test image and other images in the dataset is Eq. (2), as follows:


                           
                              
                                 (2)
                                 
                                    
                                       D
                                       1
                                       
                                          (
                                          m
                                          −
                                          1
                                          )
                                       
                                    
                                    {
                                    
                                       
                                          D
                                          1
                                       
                                       ,
                                       
                                          D
                                          2
                                       
                                       ,
                                       …
                                       ,
                                       
                                          D
                                          
                                             (
                                             m
                                             −
                                             1
                                             )
                                          
                                       
                                    
                                    }
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            (
                                                            m
                                                            −
                                                            1
                                                            )
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         1
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               ∑
                                                               
                                                                  n
                                                                  =
                                                                  1
                                                               
                                                               k
                                                            
                                                            
                                                               
                                                                  
                                                                     
                                                                        
                                                                           I
                                                                           
                                                                              
                                                                                 t
                                                                                 
                                                                                    
                                                                                       w
                                                                                       n
                                                                                    
                                                                                 
                                                                              
                                                                           
                                                                        
                                                                        −
                                                                        
                                                                           I
                                                                           
                                                                              
                                                                                 t
                                                                                 
                                                                                    
                                                                                       m
                                                                                       n
                                                                                    
                                                                                 
                                                                              
                                                                           
                                                                        
                                                                     
                                                                  
                                                               
                                                               2
                                                            
                                                         
                                                         k
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           where D is the column array that contains the distances between the test image and the other images, m is the number of images, 
                              
                                 I
                                 
                                    
                                       t
                                       
                                          
                                             w
                                             n
                                          
                                       
                                    
                                 
                              
                            represents the constant weight values in the weight vectors of the test image in terms of each comparison with all other images, 
                              
                                 I
                                 
                                    
                                       t
                                       
                                          
                                             m
                                             n
                                          
                                       
                                    
                                 
                              
                            represents the weight values of the other images in the dataset, and k is the number of values in a weight vector. The distance values are mapped between 0 and 1 to quantify the values. These values are then subtracted from a value of 1 to determine the similarity of the two images rather than their degree of difference. All of these values are placed in a table where the junction points of columns and rows indicate the similarity values between two images, one of which is in the row and the other is in the column. This table can be used to evaluate all of the confidence values generated for each comparison. The functions of the implementation stages (Fig. 2) performed during the diagnosis phase are explained in detail in Section 2.4.

We aimed to collect all possible frontal faces related to syndromes from previous publications to evaluate the method thoroughly and to quantify our results better. Thus, 612 publications with images of patients with syndromes were examined to obtain a good dataset. Specifically, studies of syndromes that included distinctive facial features were our main target because not all syndromes have distinctive facial features. After filtering, 31% of the publications (189) were selected because they described syndromes with facial images that provided clues related to syndromes that had been validated using a genetic test (a gold standard). The quality of some of these images was too low to acquire essential features in terms of the resolution of these images, especially in old publications. Capturing appropriate frontal face images was not possible in some cases due to the poses in the images. The eyes were also obscured in the faces in some of these images due to ethical issues. Images for which appropriate face views were not available were omitted from the study. The system requires at least two images per syndrome for training, but we aimed to include at least five images to reduce bias and to increase the significance of the results. Therefore, the syndromes that lacked at least five images were also discarded from the study. Ideally, the raw data for images used should preserve the original resolution, but only 27% of the raw images used in the study could be obtained whereas the remaining 73% of the images were cropped from the manuscripts.
                           5
                        
                        
                           5
                           Note that the Ethical permission and copyright section at the end of this manuscript explains that ethical approval was obtained for our reuse of these images.
                        
                     

The final dataset assembled for this study comprised 15 syndromes with a total of 92 frontal face images, i.e., 5–9 per syndrome, as shown in Fig. 4
                        . These syndromes comprised Mowat-Wilson [21,22], Goldenhar [23], Treacher Collins [24,25], Williams-Beuren [2,26], X-linked mental retardation [27,28], Cardiofaciocutaneous [29–32], Cohen [33–36], Angelman [37–41], Craniofrontonasal [42–44], Crisponi [45], Laron [46–49], Polyhydramnios, Megalencephaly, and Symptomatic Epilepsy (PMSE) [50], Fragile X [2,51], Pitt-Hopkins [52,53], and Potocki-Lupski [54]. The diagnoses of these syndromes had been validated by appropriate genetic tests and they were confirmed by the authors in their respective publications. The original images can be accessed via the references provided in the present manuscript.

@&#EXPERIMENTAL DESIGN@&#

A screenshot of the implemented method is shown in Fig. 2. Image detection is performed to capture faces from the images in a folder selected by the user, or from a live/video stream of patients, and the application displays the images on the screen during processing. The “detect images from camera” function detects face appearances from a live video stream of patients who have been diagnosed with the same dysmorphic disease. The detection process stops and the training process begins when the user clicks the “train captured images” function. The saved data for all the labeled diseases from patients can be used to train the system automatically without any human intervention. The application only asks the user to specify the location of the files for training and the syndrome names for detection in the live/video input. Individual syndromes can be trained with the “train from directory” function and many syndromes can be trained at the same time with the “train all database” function. Using this function, the system is trained with cases that have syndromes, which are located in specific directories, where the names indicate the syndrome names and the directory names are accepted as the syndrome names by the system during automatic training. After training, the system is ready to be used for diagnosis with images or live/video inputs. Several images can be selected in a folder, which can be compared to the labeled trained syndromes stored in the database using the “identify from directory” function to obtain a diagnosis.

The functions mentioned above are capable of meeting the expectations of specialists, especially medical geneticists. In this study, we employed two of these functions to evaluate the methodology: the “train all database” function to train the syndrome images acquired in previous publications, as mentioned in Section 2.3; and the “identify from directory” function to test the syndrome images. These two functions allowed us to evaluate the dataset via a cross-validation process. The number of PCs in our study was equal to m
                        −1, where m was the number of cases in the dataset because the dataset was not very large. The number of PCs should be adjusted if the dataset is larger because high numbers of PCs may produce noise and incur high calculation costs. The method used to select the top k eigenvectors is described in Section 2.2.2. The noise caused by selecting high numbers of PCs can be observed by examining the eigenfaces, an example of which is shown in Fig. 5
                        . The eigenfaces repeat as the number of PCs increases above a level and this noise can even be distinguished based on visual observations.

The dataset used for training was not large, thus we performed a cross-validation of the entire dataset using a leave-one-out scheme and we determined the average performance level. The leave-one-out scheme was used to train the training set and to test the system to determine its accuracy. The leave-one-out cross-validation was simply an n-fold cross-validation, where n is the number of instances in the dataset [55]. Each instance was omitted in turn and the learning scheme was trained with all the remaining instances. We determined the correctness based on the remaining instances, i.e., one for success and zero for failure. The results of all n judgments, i.e., one for each member of the dataset, were averaged and this average represented the final error estimate. This scheme is attractive for two reasons. First, the greatest possible amount of data is used for training in each case, which presumably increases the likelihood that a classifier is accurate. Second, the procedure is deterministic, i.e., random sampling is not involved [55]. Therefore, the accuracy estimate obtained using the leave-one-out scheme is known to be virtually unbiased [56].

Thus, we excluded one image as a test image and trained the system using the remaining 91 images in the leave-one-out cross-validation. Each separate case was used for testing and the remaining cases were used for training. We repeated this process 92 times, so every data point was left out as a test sample. The system could build a training set for these syndromes in less than 5min due to the computational efficiency of PCA. The eigenfaces and the mean images for the dataset are shown in Fig. 5. The application extracted all the confidence values into a table and the confidence values that exceeded the threshold value were displayed on the screen with the syndrome names. In the case study, we aimed to determine the most probable diagnosis based on rule-in I, II, and III diagnoses by adjusting the threshold value for each test image. The rule-in I diagnosis corresponded to the most likely diagnosis, rule-in II was the second most likely diagnosis in addition to the first according to rule-in I, and rule-in III was the third most likely diagnosis in addition to the other two determined by rule-in I and II. The total time required to search all 15 classes trained with syndromes to find the most likely syndrome given the threshold value was 3s for the test case. The user could adjust the threshold value to rule in or rule out diseases during the diagnosis process. Fewer diagnoses were suggested with a higher threshold value. With a lower threshold value, more diagnoses were suggested to the user, together with their confidence values. Thus, the success rates of the rule-in observations were obtained.

@&#EXPERIMENTAL RESULTS@&#

A table containing the confidence values for all the pairwise comparisons among the syndromes was created with the “identify from directory” function in the diagnosis process. The three most likely diagnoses were selected automatically for the syndromes in this table by adjusting the threshold value in terms of rule-in I, II, and III diagnoses. An outline of this table is shown in Tables 1 and 2
                     
                     . Table 1 presents the confidence values of the diagnosed syndromes and Table 2 shows the names of the diagnosed syndromes based on rule-in I, II, and III diagnoses. For example, the patient labeled as 1a matched with the patients labeled as 1e with a confidence value of 0.787 in terms of the rule-in I diagnosis. The patient with a known diagnosis of Mowat Wilson matched correctly according to rule-in I with 1e, who was also classified as Mowat Wilson. A geneticist could confirm the diagnosis after applying the first molecular test specified for Mowat Wilson, thereby avoiding other tests. The second case labeled as 1b matched with the patients labeled as 12e, 4a, and 2i with confidence values of 0.681, 0.653, and 0.653, respectively, based on rule-in I, II, and III diagnoses. The patient with a known diagnosis of Mowat Wilson did not match correctly based on the rule-in I (PMSE), rule-in II (Williams Beuren), and rule-in III (Goldenhar) diagnoses. The third case labeled as 1c was diagnosed correctly based on the rule-in II diagnosis, while the fourth case, 1d, was diagnosed based on the rule-in III diagnosis. In addition, for the last case of the Mowat Wilson, 1e, correct diagnosis was identified on the rule-in I diagnosis.

The rule-in I, II, and III diagnoses obtained 49, 64 (15 more syndromes based on rule-in I), and 76 (12 more syndromes based on rule-in II) correct diagnoses, respectively. The cumulative success rates of the system for diagnosing syndromes correctly based on rule-ins I (53%), II (70%), and III (83%) are shown in Fig. 6
                     . The success rate for each separate syndrome is also shown in Fig. 7
                     . In particular, all of the frontal faces tested for two syndromes, i.e., Goldenhar (n
                     =9) and Laron (n
                     =8), were diagnosed correctly by rule-in I. In addition, all the cases for three syndromes were identified correctly by rule-in II, while all the cases for four syndromes were determined correctly by rule-in III. Thus, it can be concluded that a high number of syndromes with characteristic facial anomaly patterns can be diagnosed using computer-assisted ML algorithms because the face is affected by many genes that cause syndromes.

The dataset used for training was not very large, thus a leave-one-out scheme based on the training set was used for training and testing the system to quantify its accuracy. The leave-one-out cross-validation exploited this small dataset to its maximum extent and we obtained accurate estimates, although this is usually infeasible with large datasets due to their high computational cost [55] and the time-consuming work involved.

All 92 cases were diagnosed manually by five specialists (two pediatricians and three medical geneticists) in terms of the rule-in III diagnosis as well. The mean syndrome diagnosis success rates (average ≈50%) of the specialists are shown in Fig. 8
                     . We compared the results obtained using the proposed method and the diagnostic results of the specialists to determine the significance of the results generated by the proposed method. A paired t-test was used to evaluate the differences in the results obtained in the cross-validation assessment, as described by Witten [55]. A test of normality was performed using SPSS 
                        7
                     
                     
                        7
                        Statistical and computational software tool, SPSS, version 17, SPSS Inc., Chicago, Illinois, USA.
                      via the “Explore” function in the “Analyze-Descriptive Statistics” menu, which showed that the results were normally distributed because the p-value was greater than 0.05 [57]. Thus, a parametric t-test analysis was performed using SPSS to compare the results obtained by the proposed methodology with the results of the specialists.

The null hypothesis was “there is no significant difference in the diagnosis success of the method compared with the pre-test diagnosis success of the specialists in terms of the rule-in III diagnoses (μ
                     =
                     μ
                     0).” The paired-samples t-test showed that the results obtained by the proposed method were significantly (rejecting the null hypothesis) different to the test results, where p
                     <0.01. This strongly suggests that the results obtained using the proposed method were significantly better than those of the specialists.

Finally, we analyzed the success rates on a syndrome basis. The statistical analysis of the diagnostic tests for each syndrome based on the rule-in I, II, and III diagnoses are shown in Table 3
                     , where all other cases with syndromes that differed from the processed syndromes were denoted by “condition negative.” The sensitivity (Se) and specificity (Sp) values of the overall system were 0.857 and 0.870, respectively. In particular, the Se values for three syndromes were relatively low, i.e., Treacher Collins, Cardiofaciocutaneous, and Cohen. However, the Se values of the other 12 syndromes were satisfactory. Goldenhar, Crisponi, Laron, and PMSE syndromes had Se values of 1.00, which means that all of the positive cases were diagnosed correctly. The Sp values for all syndromes were adequate, but these values might not lead to reliable conclusions because the negative conditions outnumbered the positive conditions, where all other cases with syndromes that differed from the processed syndrome were denoted by “condition negative.” Therefore, other measures should be considered such as the positive predictive value (PPV), negative predictive value (NPV), Type I error (α), Type II error (β), likelihood ratio (LR)+, LR−, and LR+/LR−-−. Evaluations of these measurements are presented in the following.

A strength of the proposed method is its very high NPV (NPV>0.975), thus if a negative result is obtained for an individual, there is very high confidence that this negative result will be true when using the method as a negative screening test. Thus, a negative result is very good evidence that a patient does not have a syndrome. However, the Mowat-Wilson, X-linked mental, Craniofrontonasal, Crisponi, Pitt-Hopkins, and Potocki Lupski syndromes were confirmed poorly (PPV<50%), so further investigations should be performed in terms of employing the proposed method as a positive screening test. The results obtained using the proposed method had low error rates (Type I error (α) and Type II error (β)), especially with α values. Increasing α will reduce β, and vice versa, given a fixed sample size. After α has been set, the only way to decrease β is to increase the sample size. Thus, the relatively high β values for some syndromes, such as Treacher Collins, Cardiofaciocutaneous, and Cohen, suggest that incorporating more cases with the syndromes would yield lower β errors.

Furthermore, LRs are useful statistics for summarizing the diagnostic accuracy because they have several particularly powerful properties, which make them more clinically useful than other statistics [58], e.g., the test is better when LR+ is higher and when LR− is smaller, thus if LR+ is high and LR− is small, it is probably a good test. It is considered that LR+/LR− values <50 indicate weak tests [59]. Similarly, LRs above 10 for LR+ and below 0.1 for LR− are considered highly satisfactory [58]. For our syndromes, except Potocki Lupski, Craniofrontonasal, Angelman, and X-linked mental, all of the LR+/LR− values indicated that the diagnosis success values of these 11 syndromes were very strong. The X-linked mental diagnosis was the weakest test with a value of 21.111. Finally, p
                     <0.05 according to Fisher's exact probability test, which is sufficient evidence that the diagnoses of all the syndromes were statistically significant, thus they were diagnosed correctly by the proposed methodology.

@&#DISCUSSION@&#

The statistical analysis of the performance of the proposed method indicates that the results agreed with the phenotype–genotype correlations reported in previous studies of specific syndromes. Thus, the present study confirmed the findings based on facial phenotypes and their relationships with specific genotypes, as reported in many other studies by medical geneticists.

The experimental results showed that the visual diagnostic DSS method can obtain biometric diagnoses of syndromes based on the frontal features of patients’ faces. Moreover, the results demonstrated that the application could be trained with many syndromes within several minutes and syndrome recognition was achieved within a few seconds based on data acquired by an attached camera or a file, thereby illustrating the promise of this system as it scales up. Our system could diagnose the syndromes of infants correctly, even though it is difficult to observe the dysmorphic features in the faces of infants. For example, the second image with Crisponi syndrome (10b in Table 1) was diagnosed correctly based on the rule-in I diagnosis and the final image with Pitt Hopkins syndrome (14e in Table 1) was diagnosed based on the rule-in III diagnoses, where the confidence values were 0.72 and 0.63, respectively. If more faces with the characteristics of a syndrome are included in the training set, the recognition of that syndrome will improve in terms of distinguishing the sub-groups with that syndrome, such as Laron and Goldenhar syndromes.

Capturing 3D information yields a richer dataset to facilitate excellent visualization, despite the difficulties acquiring the technology and detecting 3D data. Thus, the 3D analysis of syndromes would certainly yield better results, as shown by Hammond's study [7] where better features were obtained that represented syndromes. However, the lack of available 3D data prevents the implementation of an automatic 3D-based diagnosis method for dysmorphology syndromes in the near future. One of the reasons why we restricted our study to frontal 2D images is that most of the geneticists who study dysmorphic diseases retain 2D images of their patients in their databases so the common databases (i.e., Oxford Medical Databases (OMD)) comprise 2D images of patients. However, 2D image analysis has several advantages during practical use: (1) the image acquisition equipment is cheap and easy to handle [2]; (2) conventional 2D photography facilitates the rapid and easy capture of facial images; and (3) it is easier for investigators to collect and analyze 2D dysmorphic visual data associated with genotypes.

Hammond [7] suggest that “average faces” could be constructed by combining computer measurements of faces from a large samples of individuals. Clearly, a learning system performs better with a higher number of images for training. However, a large collection of images is not available at present, at least not in the public domain. In this study, we demonstrated the feasibility of automatic diagnosis using a reasonable dataset that we assembled from previous publications. Our results obtained using the leave-one-out strategy demonstrated the feasibility of this approach. Indeed, the methodology we developed using PCA eigenfaces could (in a linear mathematical sense) be treated as the capture of average faces. The PCA approach could be viewed as capturing the average in the first PC, before accounting for the additional variance in the subsequent PCs.

@&#CONCLUSIONS@&#

In this study, we provided an experimental demonstration that automatic image processing and computational modeling can accurately classify underlying syndromes based on face images of dysmorphic patients. Using 92 published images of 15 different syndromes, the classification accuracy was 83%, with an Se value of 0.857 and an Sp value of 0.870. We consider that similar computational hybrid modeling approaches could be integrated with a clinical decision support environment in several ways: (1) rare genetic disorders may be difficult for each specialist to remember but an automatic system such as ours can obtain correct matches with previous cases; (2) correct identification at the image level can guide the direction of correct molecular/cytogenetic analysis; and (3) computer-aided diagnosis can make knowledge available to remote and rural areas where specialists might not be available.

@&#FUTURE WORK@&#

In this study, we developed an approach for diagnosing syndromes using a computer-aided system that could be applied in clinical practice. However, we consider that predictions specific to patients and groups, as well as different gender and age groups, would be more distinctive. Thus, dysmorphic faces from the main databases and individual databases related to syndromes should be categorized and used to train the application based on sex, age, and even ethnicity differences to provide better diagnostic decision support, as emphasized in a recent study by Hopman [60]. We expect that this form of categorization would improve the success rate of our method. We would like to test this assumption when a suitable dataset is available.

At present, we are performing a critical evaluation of this computational approach in three areas. First, we want to classify dysmorphic face images as a group that can be separated from a collection of normal faces. Second, we want to establish a robust classification system that combines various (or the best performing) classification methods via an ensemble of classifiers or a multi-voting system approach, thereby minimizing inaccurate classification decisions. Third, we aim to develop an unsupervised learning (or clustering) approach for the analysis of these data, which would have the potential to discover new groups, such as disease sub-types that have been unclassified in clinical practice. We are working in collaboration with clinicians to make our system available in a clinical setting where it could be refined based on expert feedback. The extension of this method to 3D scans of faces, rather than 2D images, is also a goal, but the unavailability of adequate data is a major issue.

The required written permissions for the reuse of the frontal face images in Fig. 4 were obtained from the original publishers. The original publishers of the images are as follows: 1a, 1b, 1c (Biomed Central); 1d, 1e, 5a, 5b, 5f, 6a, 6c, 6d, 6e, 7a, 7b, 7d, 7d, 8a, 8b, 8c, 8h, 8f, 11d, 13e (BMJ Publishing Group Ltd.); 2a, 2b, 2c, 2d, 2e, 2f, 2g, 2h, 2i, 3a, 3b, 3c, 3d, 3e, 4a, 4d, 4e, 6b, 8d, 8e, 9a, 9b, 9c, 13a, 13b, 13c, 13d, 13f (Nature Publishing Group); 4b, 4c, 9d, 12a, 12b, 12c, 12d, 12e (Oxford University Press); 5c, 5d, 5e, 5g, 7c, 9e, 10a, 10b, 10c, 10d, 10e, 10f, 10g, 14a, 14b, 14c, 14d, 14e, 15a, 15b, 15c, 15d, 15e, 15f, 15g (Elsevier); 8g (Brazilian Society of Genetics); 11a, 11b, 11c, 11g (Thieme); 11e (Karger AG); 11h, 11i (Springer (Human Press was purchased by Springer)). Anyone who wants to use any of these images should request permission from the original publishers, who retain the copyright.

@&#ACKNOWLEDGMENTS@&#

We are very grateful to the Scientific and Technological Research Council of Turkey for supporting this study with a grant (grant number: B.14.2.TBT.0.06.01-219-5-123). The committee of the European Society of Human Genetics also provided a fellowship for the study. This study was improved in terms of both technical and genetic aspects thanks to our attendance at the European Human Genetics Conference 2012 [61] and the 11th International Conference on ML and Applications [62]. We would like to thank all of the publishers (as specified in the Ethical permission and copyright section) who permitted us to reuse the frontal face images in Fig. 4 from their published sources.

@&#REFERENCES@&#

