@&#MAIN-TITLE@&#Support vector machine-based optimized decision threshold adjustment strategy for classifying imbalanced data

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We analyze the reason why SVM can be damaged by class imbalance in theory.


                        
                        
                           
                           We propose SVM-OTHR algorithm to find the optimal moving distance of hyperplane.


                        
                        
                           
                           We integrate SVM-OTHR into Bagging ensemble framework to promote its robustness.


                        
                        
                           
                           The time complexity of SVM-OTHR is merely a little higher than standard SVM.


                        
                        
                           
                           Two proposed algorithms often outperform some other bias correction algorithms.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Class imbalance

Support vector machine

Decision threshold adjustment

Optimization search

Ensemble learning

@&#ABSTRACT@&#


               
               
                  Class imbalance problem occurs when the number of training instances belonging to different classes are clearly different. In this scenario, many traditional classifiers often fail to provide excellent enough classification performance, i.e., the accuracy of the majority class is usually much higher than that of the minority class. In this article, we consider to deal with class imbalance problem by utilizing support vector machine (SVM) classifier with an optimized decision threshold adjustment strategy (SVM-OTHR), which answers a puzzled question: how far the classification hyperplane should be moved towards the majority class? Specifically, the proposed strategy is self-adapting and can find the optimal moving distance of the classification hyperplane according to the real distributions of training samples. Furthermore, we also extend the strategy to develop an ensemble version (EnSVM-OTHR) that can further improve the classification performance. Two proposed algorithms are both compared with many state-of-the-art classifiers on 30 skewed data sets acquired from Keel data set Repository by using two popular class imbalance evaluation metrics: F-measure and G-mean. The statistical results of the experiments indicate their superiority.
               
            

@&#INTRODUCTION@&#

In the past decade, the class imbalance problem has received considerable attention in several fields, such as artificial intelligence [1], machine learning [2] and data mining [3,4]. A data set is said to be imbalanced when and only when the instances of some classes are obviously much more than that in other classes. The problem is important due to it widely emerges in many real-world applications, including financial fraud detection [5], network intrusion detection [6], spam filtering [7], video monitoring [8], medical diagnosis [9], Bioinformatics [10], etc. Generally, in these applications, we are more interested in the pattern represented by the examples of the minority class. However, majority traditional classification algorithms pursuing the minimal training errors would heavily damage the recognition accuracy of the minority class, thus it is necessary to adopt some bias correction techniques before/after constructing a classifier.

The bias correction techniques can be roughly divided into four major categories as follows:
                        
                           1.
                           Resampling the original training set until all the classes are approximately equally represented. Resampling includes oversampling [11–13], undersampling [14,15] and hybrid sampling [16].

Cost-sensitive learning, which is also called instances weighting method, assigns different weights for the training instances belonging to different classes so that the misclassification of the minority class can be highlighted [17–19].

Moving the decision boundary (decision threshold adjustment) towards the majority class in order to remedy the bias caused by skewed sample distributions [20,21]. Unlike the other correction techniques, decision threshold adjustment strategy runs after modeling a classifier.

Ensemble learning that provides a framework to incorporate resampling strategy, weighting strategy or decision threshold adjustment strategy, usually produces better and more balanced classification performance [22–29].

Among those correction techniques mentioned above, decision threshold adjustment is regarded as a potential solution for dealing with class imbalance in recent studies [20,21]. However, the existing decision threshold adjustment approaches generally give the moving distance of classification boundary empirically, thus fail to answer a significant question: how far the classification hyperplane should be moved towards the majority class? This study solves this puzzle in the context of support vector machine (SVM) [30]. SVM is a robust classifier and is relatively insensitive to class imbalance in comparison with many other classification algorithms, because its classification hyperplane only associates with a few support vectors [31].

In this paper, we first investigate the reason that the classification performance of SVM can be destroyed by skewed classification data in theory, and then we analyze the merits and drawbacks of some existing SVM-based bias correction techniques. Next, the computational formula of the moving distance in SVM-THR algorithm proposed by Lin and Chen [21] is intensively modified to lead to one optimized version (SVM-OTHR). Furthermore, we incorporate SVM-OTHR into Bagging ensemble learning framework and present a novel classification algorithm named EnSVM-OTHR. In particular, to avoid overfitting and to guarantee the diversity of different individuals, a small random perturbation term is inserted into each SVM-OTHR to disturb the final position of classification hyperplane. Finally, we compare the two proposed classification algorithms with many state-of-the-art imbalanced classifiers on 30 data sets acquired from Keel data set Repository via non-parametrical statistical testing [32,33], indicating their superiority.

The rest of this paper is organized as follows. In Section 2, we introduce SVM theory and explain the reason that the performance of SVM can be damaged by imbalanced classification data. Section 3 briefly reviews some existing SVM-based class imbalance correction techniques and indicates their pros and cons. In Section 4, one optimized SVM decision threshold adjustment strategy (SVM-OTHR) and its extended version based on ensemble learning (EnSVM-OTHR) are described in detail. Experimental results and discussions are presented in Section 5. Finally in Section 6, the main contributions of this study are summarized.

Support vector machine (SVM), which comes out of the theory of structure risk minimization, has several merits as follows: high generalization capability, absence of local minima and adaptation for high-dimension and small sample data [31,34].

Given some training data D, a set of m points of the form: 
                        
                           D
                           =
                           {
                           (
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           ,
                           
                              
                                 y
                              
                              
                                 i
                              
                           
                           )
                           |
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           ∈
                           
                              
                                 R
                              
                              
                                 p
                              
                           
                           ,
                           
                              
                                 y
                              
                              
                                 i
                              
                           
                           ∈
                           
                              
                                 {
                                 -
                                 1
                                 ,
                                 1
                                 }
                              
                              
                                 i
                                 =
                                 1
                              
                              
                                 m
                              
                           
                           }
                        
                     , where 
                        
                           
                              
                                 y
                              
                              
                                 i
                              
                           
                        
                      is either 1 or −1, indicating the class to which the point 
                        
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                        
                      belongs. Each 
                        
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                        
                      is one p-dimensional real vector. SVM is used to find the maximum margin hyperplane that divides the points having 
                        
                           
                              
                                 y
                              
                              
                                 i
                              
                           
                           =
                           1
                        
                      from those having 
                        
                           
                              
                                 y
                              
                              
                                 i
                              
                           
                           =
                           -
                           1
                        
                     . The decision function of SVM is described as:
                        
                           (1)
                           
                              h
                              (
                              x
                              )
                              =
                              〈
                              w
                              ,
                              ϕ
                              (
                              x
                              )
                              〉
                              +
                              b
                           
                        
                     where 
                        
                           ϕ
                           (
                           x
                           )
                        
                      represents a mapping of sample x from the input space to high-dimensional feature space, 
                        
                           〈
                           ·
                           ,
                           ·
                           〉
                        
                      denotes the dot product in the feature space, 
                        
                           w
                        
                      denotes the weight vector for learned decision hyperplane and 
                        
                           b
                        
                      is the model bias. We can optimize the values of w and b by solving the following optimization problem:
                        
                           (2)
                           
                              
                                 
                                    
                                    
                                       
                                          minimize
                                          :
                                          
                                          g
                                          (
                                          w
                                          ,
                                          ξ
                                          )
                                          =
                                          
                                             
                                                1
                                             
                                             
                                                2
                                             
                                          
                                          ‖
                                          w
                                          
                                             
                                                ‖
                                             
                                             
                                                2
                                             
                                          
                                          +
                                          C
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   m
                                                
                                             
                                          
                                          
                                             
                                                ξ
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                    
                                       
                                          subject to
                                          :
                                          
                                          
                                             
                                                y
                                             
                                             
                                                i
                                             
                                          
                                          (
                                          〈
                                          w
                                          ,
                                          ϕ
                                          (
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          )
                                          〉
                                          +
                                          b
                                          )
                                          ⩾
                                          1
                                          -
                                          
                                             
                                                ξ
                                             
                                             
                                                i
                                             
                                          
                                          ,
                                          
                                          
                                             
                                                ξ
                                             
                                             
                                                i
                                             
                                          
                                          ⩾
                                          0
                                       
                                    
                                 
                              
                           
                        
                     where 
                        
                           
                              
                                 ξ
                              
                              
                                 i
                              
                           
                        
                      is the ith slack variable and C is regularization parameter (penalty factor) which is used to regulate the relationship between training accuracy and generalization. Then the minimization problem in formula (2) can be transformed to a dual form and be rewritten as:
                        
                           (3)
                           
                              
                                 
                                    
                                    
                                       
                                          maximize
                                          :
                                          
                                          W
                                          (
                                          α
                                          )
                                          =
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   m
                                                
                                             
                                          
                                          
                                             
                                                α
                                             
                                             
                                                i
                                             
                                          
                                          -
                                          
                                             
                                                1
                                             
                                             
                                                2
                                             
                                          
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   m
                                                
                                             
                                          
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                
                                                   m
                                                
                                             
                                          
                                          
                                             
                                                y
                                             
                                             
                                                i
                                             
                                          
                                          
                                             
                                                y
                                             
                                             
                                                j
                                             
                                          
                                          
                                             
                                                α
                                             
                                             
                                                i
                                             
                                          
                                          
                                             
                                                α
                                             
                                             
                                                j
                                             
                                          
                                          K
                                          (
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          ,
                                          
                                             
                                                x
                                             
                                             
                                                j
                                             
                                          
                                          )
                                       
                                    
                                 
                                 
                                    
                                    
                                       
                                          subject to
                                          :
                                          
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   m
                                                
                                             
                                          
                                          
                                             
                                                y
                                             
                                             
                                                i
                                             
                                          
                                          
                                             
                                                α
                                             
                                             
                                                i
                                             
                                          
                                          =
                                          0
                                          ,
                                          
                                          ∀
                                          i
                                          :
                                          
                                          0
                                          ⩽
                                          
                                             
                                                α
                                             
                                             
                                                i
                                             
                                          
                                          ⩽
                                          C
                                       
                                    
                                 
                              
                           
                        
                     where 
                        
                           
                              
                                 α
                              
                              
                                 i
                              
                           
                        
                      is the sample xi
                     ’s lagrange multiplier, 
                        
                           K
                           (
                           ·
                           ,
                           ·
                           )
                        
                      is a kernel function that maps the input vectors into a suitable feature space:
                        
                           (4)
                           
                              K
                              (
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    x
                                 
                                 
                                    j
                                 
                              
                              )
                              =
                              〈
                              ϕ
                              (
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              )
                              ,
                              ϕ
                              (
                              
                                 
                                    x
                                 
                                 
                                    j
                                 
                              
                              )
                              〉
                           
                        
                     
                  

Some previous work has found that radial basis kernel function (RBF) generally provides better classification accuracy than many other kernel functions [31,34]. RBF kernel is presented as follows:
                        
                           (5)
                           
                              K
                              (
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    x
                                 
                                 
                                    j
                                 
                              
                              )
                              =
                              exp
                              
                                 
                                    
                                       -
                                       
                                          
                                             ‖
                                             
                                                
                                                   x
                                                
                                                
                                                   i
                                                
                                             
                                             -
                                             
                                                
                                                   x
                                                
                                                
                                                   j
                                                
                                             
                                             
                                                
                                                   ‖
                                                
                                                
                                                   2
                                                
                                             
                                          
                                          
                                             2
                                             
                                                
                                                   σ
                                                
                                                
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     where 
                        
                           σ
                        
                      is the width of RBF kernel.

Although previous work found that SVM is more robust to class imbalance than many other machine learning methods as its classification hyperplane only associates with a few support vectors, it can be still hurt by skewed class distributions to some extent. We try to analyze its reason in theory.

After training an SVM classifier, lagrange multiplier 
                        
                           
                              
                                 α
                              
                              
                                 i
                              
                           
                        
                      can be divided into three categories as follows:
                        
                           
                              Case 1: 
                              
                                 
                                    
                                       
                                          α
                                       
                                       
                                          i
                                       
                                    
                                    =
                                    0
                                 
                              , it means the instance 
                                 
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                 
                               is classified accurately.


                              Case 2: 
                              
                                 
                                    0
                                    <
                                    
                                       
                                          α
                                       
                                       
                                          i
                                       
                                    
                                    <
                                    C
                                 
                              , the corresponding instance 
                                 
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                 
                               is called a normal support vector which is exactly on one of the margin hyperplanes.


                              Case 3: 
                              
                                 
                                    
                                       
                                          α
                                       
                                       
                                          i
                                       
                                    
                                    =
                                    C
                                 
                              , 
                                 
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                 
                               is called a boundary support vector that lies between margins. The percentage of boundary support vectors reflects the error rate of SVM to some extent.

Suppose 
                        
                           
                              
                                 N
                              
                              
                                 +
                              
                           
                        
                      and 
                        
                           
                              
                                 N
                              
                              
                                 -
                              
                           
                        
                      represent the number of instances belonging to the positive class (minority class) and the negative class (majority class), respectively. 
                        
                           
                              
                                 N
                              
                              
                                 sv
                              
                              
                                 +
                              
                           
                        
                      and 
                        
                           
                              
                                 N
                              
                              
                                 sv
                              
                              
                                 -
                              
                           
                        
                      are the number of support vectors (including normal and boundary support vectors) in two classes, while 
                        
                           
                              
                                 N
                              
                              
                                 boundary
                              
                              
                                 +
                              
                           
                        
                      and 
                        
                           
                              
                                 N
                              
                              
                                 boundary
                              
                              
                                 -
                              
                           
                        
                      represent the number of boundary support vectors in two classes, respectively. According to formula (3), we can get:
                        
                           (6)
                           
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       m
                                    
                                 
                              
                              
                                 
                                    α
                                 
                                 
                                    i
                                 
                              
                              =
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       
                                          
                                             y
                                          
                                          
                                             i
                                          
                                       
                                       =
                                       +
                                       1
                                    
                                 
                              
                              
                                 
                                    α
                                 
                                 
                                    i
                                 
                              
                              +
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       
                                          
                                             y
                                          
                                          
                                             i
                                          
                                       
                                       =
                                       -
                                       1
                                    
                                 
                              
                              
                                 
                                    α
                                 
                                 
                                    i
                                 
                              
                           
                        
                     
                     
                        
                           (7)
                           
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       
                                          
                                             y
                                          
                                          
                                             i
                                          
                                       
                                       =
                                       +
                                       1
                                    
                                 
                              
                              
                                 
                                    α
                                 
                                 
                                    i
                                 
                              
                              =
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       
                                          
                                             y
                                          
                                          
                                             i
                                          
                                       
                                       =
                                       -
                                       1
                                    
                                 
                              
                              
                                 
                                    α
                                 
                                 
                                    i
                                 
                              
                           
                        
                     
                  

Because αi
                     ’s value is C at most, it can deduce two inequalities as follows:
                        
                           (8)
                           
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       
                                          
                                             y
                                          
                                          
                                             i
                                          
                                       
                                       =
                                       +
                                       1
                                    
                                 
                              
                              
                                 
                                    α
                                 
                                 
                                    i
                                 
                              
                              ⩾
                              
                                 
                                    N
                                 
                                 
                                    boundary
                                 
                                 
                                    +
                                 
                              
                              ×
                              C
                           
                        
                     
                     
                        
                           (9)
                           
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       
                                          
                                             y
                                          
                                          
                                             i
                                          
                                       
                                       =
                                       +
                                       1
                                    
                                 
                              
                              
                                 
                                    α
                                 
                                 
                                    i
                                 
                              
                              ⩽
                              
                                 
                                    N
                                 
                                 
                                    sv
                                 
                                 
                                    +
                                 
                              
                              ×
                              C
                           
                        
                     
                  

By integrating formula (8) and (9), we get:
                        
                           (10)
                           
                              
                                 
                                    N
                                 
                                 
                                    sv
                                 
                                 
                                    +
                                 
                              
                              ×
                              C
                              ⩾
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       
                                          
                                             y
                                          
                                          
                                             i
                                          
                                       
                                       =
                                       +
                                       1
                                    
                                 
                              
                              
                                 
                                    α
                                 
                                 
                                    i
                                 
                              
                              ⩾
                              
                                 
                                    N
                                 
                                 
                                    boundary
                                 
                                 
                                    +
                                 
                              
                              ×
                              C
                           
                        
                     
                  

Similarly, it is not difficult to get the following inequality:
                        
                           (11)
                           
                              
                                 
                                    N
                                 
                                 
                                    sv
                                 
                                 
                                    -
                                 
                              
                              ×
                              C
                              ⩾
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       
                                          
                                             y
                                          
                                          
                                             i
                                          
                                       
                                       =
                                       -
                                       1
                                    
                                 
                              
                              
                                 
                                    α
                                 
                                 
                                    i
                                 
                              
                              ⩾
                              
                                 
                                    N
                                 
                                 
                                    boundary
                                 
                                 
                                    -
                                 
                              
                              ×
                              C
                           
                        
                     
                  

Suppose 
                        
                           
                              
                                 ∑
                              
                              
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 +
                                 1
                              
                           
                           
                              
                                 α
                              
                              
                                 i
                              
                           
                           =
                           
                              
                                 ∑
                              
                              
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 -
                                 1
                              
                           
                           
                              
                                 α
                              
                              
                                 i
                              
                           
                           =
                           M
                        
                     , if formula (10) and (11) respectively divide by 
                        
                           
                              
                                 N
                              
                              
                                 +
                              
                           
                           ×
                           C
                        
                      and 
                        
                           
                              
                                 N
                              
                              
                                 -
                              
                           
                           ×
                           C
                        
                     , we get:
                        
                           (12)
                           
                              
                                 
                                    
                                       
                                          N
                                       
                                       
                                          sv
                                       
                                       
                                          +
                                       
                                    
                                 
                                 
                                    
                                       
                                          N
                                       
                                       
                                          +
                                       
                                    
                                 
                              
                              ⩾
                              
                                 
                                    M
                                 
                                 
                                    
                                       
                                          N
                                       
                                       
                                          +
                                       
                                    
                                    ×
                                    C
                                 
                              
                              ⩾
                              
                                 
                                    
                                       
                                          N
                                       
                                       
                                          boundary
                                       
                                       
                                          +
                                       
                                    
                                 
                                 
                                    
                                       
                                          N
                                       
                                       
                                          +
                                       
                                    
                                 
                              
                           
                        
                     
                     
                        
                           (13)
                           
                              
                                 
                                    
                                       
                                          N
                                       
                                       
                                          sv
                                       
                                       
                                          -
                                       
                                    
                                 
                                 
                                    
                                       
                                          N
                                       
                                       
                                          -
                                       
                                    
                                 
                              
                              ⩾
                              
                                 
                                    M
                                 
                                 
                                    
                                       
                                          N
                                       
                                       
                                          -
                                       
                                    
                                    ×
                                    C
                                 
                              
                              ⩾
                              
                                 
                                    
                                       
                                          N
                                       
                                       
                                          boundary
                                       
                                       
                                          -
                                       
                                    
                                 
                                 
                                    
                                       
                                          N
                                       
                                       
                                          -
                                       
                                    
                                 
                              
                           
                        
                     where 
                        
                           
                              
                                 
                                    
                                       N
                                    
                                    
                                       boundary
                                    
                                    
                                       +
                                    
                                 
                              
                              
                                 
                                    
                                       N
                                    
                                    
                                       +
                                    
                                 
                              
                           
                        
                      and 
                        
                           
                              
                                 
                                    
                                       N
                                    
                                    
                                       boundary
                                    
                                    
                                       -
                                    
                                 
                              
                              
                                 
                                    
                                       N
                                    
                                    
                                       -
                                    
                                 
                              
                           
                        
                      approximately reflect the upper bounds of error rates in the minority class and the majority class, respectively. It is clear that 
                        
                           
                              
                                 M
                              
                              
                                 
                                    
                                       N
                                    
                                    
                                       +
                                    
                                 
                                 ×
                                 C
                              
                           
                        
                      is larger than 
                        
                           
                              
                                 M
                              
                              
                                 
                                    
                                       N
                                    
                                    
                                       -
                                    
                                 
                                 ×
                                 C
                              
                           
                        
                      as 
                        
                           
                              
                                 N
                              
                              
                                 +
                              
                           
                        
                      is smaller than 
                        
                           
                              
                                 N
                              
                              
                                 -
                              
                           
                        
                     . Therefore, it is easy to draw a conclusion that the error rate of the minority class is usually larger than that of the majority class. Based on the analysis above, we find that the minority class often sacrifices more in the process of modeling SVM classifier, thus the impartiality of SVM can be destroyed by class imbalance.

In previous work, some class imbalance correction strategies for SVM classifier have been proposed, including resampling [11,13], weighting [17,19] and decision threshold adjustment [21].

Resampling can be accomplished either by oversampling the minority class or undersampling the majority class. In fact, resampling repairs the difference between 
                        
                           
                              
                                 M
                              
                              
                                 
                                    
                                       N
                                    
                                    
                                       +
                                    
                                 
                                 ×
                                 C
                              
                           
                        
                      and 
                        
                           
                              
                                 M
                              
                              
                                 
                                    
                                       N
                                    
                                    
                                       -
                                    
                                 
                                 ×
                                 C
                              
                           
                        
                      by either increasing 
                        
                           
                              
                                 N
                              
                              
                                 +
                              
                           
                        
                      or reducing 
                        
                           
                              
                                 N
                              
                              
                                 -
                              
                           
                        
                     . However, both techniques have their advantages and disadvantages. Oversampling makes the classifier overfitting and increases the time of modeling, while undersampling often causes information loss [35]. Random oversampling (ROS) and random undersampling (RUS) are the simplest resampling approaches [11]. Akbani et al. [13] found that combining SVM classifier and SMOTE oversampling method which proposed by Chawla et al. [12] can acquire better classification results than ROS and RUS.

Different from resampling, the weighting strategy (CS-SVM) assigns different weights to the instances in different classes, where the weight reflects in the penalty factor C 
                     [17]. That means for the samples in the minority class, the penalty factor is assigned as 
                        
                           
                              
                                 C
                              
                              
                                 +
                              
                           
                        
                     , while the penalty factor in the majority class is assigned as 
                        
                           
                              
                                 C
                              
                              
                                 -
                              
                           
                        
                     . To guarantee 
                        
                           
                              
                                 M
                              
                              
                                 
                                    
                                       N
                                    
                                    
                                       +
                                    
                                 
                                 ×
                                 
                                    
                                       C
                                    
                                    
                                       +
                                    
                                 
                              
                           
                           =
                           
                              
                                 M
                              
                              
                                 
                                    
                                       N
                                    
                                    
                                       -
                                    
                                 
                                 ×
                                 
                                    
                                       C
                                    
                                    
                                       -
                                    
                                 
                              
                           
                        
                     , we should make 
                        
                           
                              
                                 
                                    
                                       C
                                    
                                    
                                       +
                                    
                                 
                              
                              
                                 
                                    
                                       C
                                    
                                    
                                       -
                                    
                                 
                              
                           
                           =
                           
                              
                                 
                                    
                                       N
                                    
                                    
                                       -
                                    
                                 
                              
                              
                                 
                                    
                                       N
                                    
                                    
                                       +
                                    
                                 
                              
                           
                        
                     , then the formula (2) can be rewritten as:
                        
                           (14)
                           
                              
                                 
                                    
                                    
                                       
                                          minimize
                                          :
                                          
                                          g
                                          (
                                          w
                                          ,
                                          ξ
                                          )
                                          =
                                          
                                             
                                                1
                                             
                                             
                                                2
                                             
                                          
                                          ‖
                                          w
                                          
                                             
                                                ‖
                                             
                                             
                                                2
                                             
                                          
                                          +
                                          
                                             
                                                C
                                             
                                             
                                                +
                                             
                                          
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   
                                                      
                                                         y
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   =
                                                   +
                                                   1
                                                
                                             
                                          
                                          
                                             
                                                ξ
                                             
                                             
                                                i
                                             
                                          
                                          +
                                          
                                             
                                                C
                                             
                                             
                                                -
                                             
                                          
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   
                                                      
                                                         y
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   =
                                                   -
                                                   1
                                                
                                             
                                          
                                          
                                             
                                                ξ
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                    
                                       
                                          subject to
                                          :
                                          
                                          
                                             
                                                y
                                             
                                             
                                                i
                                             
                                          
                                          (
                                          〈
                                          w
                                          ,
                                          ϕ
                                          (
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          )
                                          〉
                                          +
                                          b
                                          )
                                          ⩾
                                          1
                                          -
                                          
                                             
                                                ξ
                                             
                                             
                                                i
                                             
                                          
                                          ,
                                          
                                          
                                             
                                                ξ
                                             
                                             
                                                i
                                             
                                          
                                          ⩾
                                          0
                                       
                                    
                                 
                              
                           
                        
                     
                  

z-SVM can be regarded as another weighting strategy of SVM [19]. Unlike CS-SVM that assigns different penalty factors for different classes, z-SVM directly rewrites the final decision function (formula (1)) by assigning a larger weight for the positive class. According to Ref. [19], formula (1) can be rewritten as:
                        
                           (15)
                           
                              h
                              (
                              x
                              )
                              =
                              z
                              ×
                              〈
                              w
                              ,
                              ϕ
                              (
                              
                                 
                                    x
                                 
                                 
                                    p
                                 
                              
                              )
                              〉
                              +
                              〈
                              w
                              ,
                              ϕ
                              (
                              
                                 
                                    x
                                 
                                 
                                    N
                                 
                              
                              )
                              〉
                              +
                              b
                           
                        
                     where x
                     p and x
                     N denote the positive and negative support vectors in one learned SVM model, and z is weighted factor for the positive class. In Ref. [19], G-mean is adopted as the evaluation measure to determine the optimal z value z
                     ∗. Meanwhile, as one univariate unconstrained optimization problem, golden section optimization technology is adopted into the optimization process of z
                     ∗.

The third method is decision threshold adjustment that is used after classifier modeling. Actually, the decision threshold adjustment method improves the classification accuracy of the minority class by directly moving the classification hyperplane towards the majority class. Lin and Chen [21] suggest to use the following function to calculate the moving distance of decision threshold:
                        
                           (16)
                           
                              θ
                              =
                              
                                 
                                    
                                       
                                          N
                                       
                                       
                                          -
                                       
                                    
                                    -
                                    
                                       
                                          N
                                       
                                       
                                          +
                                       
                                    
                                 
                                 
                                    
                                       
                                          N
                                       
                                       
                                          +
                                       
                                    
                                    +
                                    
                                       
                                          N
                                       
                                       
                                          -
                                       
                                    
                                    +
                                    2
                                 
                              
                           
                        
                     
                  

For each test instance x, it can be divided into the minority class or the majority class by the adjusted decision function as follows:
                        
                           (17)
                           
                              
                                 
                                    h
                                 
                                 
                                    ′
                                 
                              
                              (
                              x
                              )
                              =
                              h
                              (
                              x
                              )
                              +
                              θ
                           
                        
                     where h(x) and h′(x) represent the original decision function and the adjusted decision function, respectively. They named this adjustment strategy as SVM-THR and found it often performs better than many other correction techniques by experiments on several Bioinformatics data sets. However, the formula (16) was given empirically and the authors failed to provide its theoretical foundation. Then a confusing question emerges: how to find the optimal moving distance for the decision threshold? In Section 4, we will exhibit an novel optimized method (SVM-OTHR) which may be a potential answer for the question above.


                     Fig. 1
                      gives the graphical representations of standard SVM, SVM-RUS, SVM-ROS, SVM-SMOTE, CS-SVM/z-SVM and SVM-THR, respectively. It can be seen that each correction technique can alleviate the affection of class imbalance more or less.

To adjust decision threshold (classification boundary) to the optimal position, the concept of optimization should be primarily defined. Obviously, for balanced classification tasks, SVM can spontaneously find the optimal position of classification boundary which is a tradeoff between classification margin and classification accuracy. But when the classification task is skewed, classification accuracy generally gives bias evaluation, thus some other specific evaluation metrics, such as F-measure and G-mean, are needed to evaluate the classification performance of a learner. F-measure and G-mean can be regarded as functions of the confusion matrix as shown in Table 1
                     .

They are calculated as follows:
                        
                           (18)
                           
                              F-measure
                              =
                              
                                 
                                    2
                                    ∗
                                    Precision
                                    ∗
                                    Recall
                                 
                                 
                                    Precision
                                    +
                                    Recall
                                 
                              
                           
                        
                     
                     
                        
                           (19)
                           
                              G-mean
                              =
                              
                                 
                                    TPR
                                    ×
                                    TNR
                                 
                              
                           
                        
                     where Precision, Recall, TPR and TNR are further defined as:
                        
                           (20)
                           
                              Precision
                              =
                              
                                 
                                    TP
                                 
                                 
                                    TP
                                    +
                                    FP
                                 
                              
                           
                        
                     
                     
                        
                           (21)
                           
                              Recall
                              =
                              TPR
                              =
                              
                                 
                                    TP
                                 
                                 
                                    TP
                                    +
                                    FN
                                 
                              
                           
                        
                     
                     
                        
                           (22)
                           
                              TNR
                              =
                              
                                 
                                    TN
                                 
                                 
                                    TN
                                    +
                                    FP
                                 
                              
                           
                        
                     
                  

From the formula (19), it obviously expresses that G-mean reflects the balance between the accuracy of the minority class and that of the majority class. Therefore, G-mean can be regarded as an excellent criterion to determine the optimal position of the classification hyperplane. Unfortunately, it is difficult to find this optimal hyperplane taking advantage of direct optimization technology by rewriting the formula (2). Here, we give an iterative algorithm that can be also seen as an exhaustive search algorithm, to search the optimal position for the classification hyperplane. Considering each misclassified minority class instance in the training set, undoubtedly, its decision function (see formula (1)) output value will be negative. Therefore, we can acquire all candidate positions for the optimal classification hyperplane according to the output values of misclassified instances. By comparing G-mean values of these candidate positions, it is easy to find the optimal adjusted position. Furthermore, to maintain the generalization ability of the classifier to some extent, we profit from the idea of SVM to adjust each candidate position to the middle between the corresponding misclassified minority class instance xi
                      and its nearest neighbor instance belonging to the majority class that lies far from the original classification hyperplane, denoting as nxi
                     . Then the adjusted distance θi
                      can be calculated with the following formula:
                        
                           (23)
                           
                              
                                 
                                    θ
                                 
                                 
                                    i
                                 
                              
                              =
                              
                                 
                                    -
                                    h
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    -
                                    h
                                    (
                                    
                                       
                                          nx
                                       
                                       
                                          i
                                       
                                    
                                    )
                                 
                                 
                                    2
                                 
                              
                           
                        
                     where h is the original decision function. Then we compare G-mean values of all candidate positions to find the optimal position and its corresponding adjusted distance, representing as θ
                     optimal. Finally, the decision function is adjusted as:
                        
                           (24)
                           
                              
                                 
                                    h
                                 
                                 
                                    ′
                                 
                              
                              (
                              x
                              )
                              =
                              h
                              (
                              x
                              )
                              +
                              
                                 
                                    θ
                                 
                                 
                                    optimal
                                 
                              
                           
                        
                     where h(x) and h′(x) are original and adjusted decision functions, respectively. Fig. 2
                      gives the schematic diagram of the search process.

According to the idea above, we also present a novel SVM decision threshold adjustment algorithm that is named SVM-OTHR. The pseudo code description of SVM-OTHR algorithm is provided in Fig. 3
                     .


                     Fig. 3 indicates that the time complexity of SVM-OTHR is merely correlated with the number of misclassified instances belonging to the minority class, thus it will not increase computational burden to a large extent. Of course, from Fig. 3, we can also observe one obvious drawback of SVM-OTHR that is the appearance of overfitting and/or the decrease of generalization ability. The algorithm can be only local optimization for the limited training samples, as well the adjusted classification hyperplane cannot guarantee to minimize the structural risk.

To overcome the defects described above, an extended ensemble learning version of SVM-OTHR named as EnSVM-OTHR is proposed. As indicated in previous work [36,37], ensemble learning can help repair the bias of single classifier and improve the generalization ability of classification results. Here, we select Bagging ensemble learning framework [38] to develop EnSVM-OTHR algorithm. Bagging adopts Bootstrap strategy to generate multiple diverse training subsets and majority voting rule to make the final decision. EnSVM-OTHR simply inherits both Bootstrap strategy and majority voting rule from Bagging. To alleviate the impact of class imbalance, we carry out SVM-OTHR algorithm on each training subset. In addition, to further promote the generalization ability and guarantee the diversity of different base classifiers, a small random perturbation term is inserted into each SVM-OTHR to disturb the final position of classification hyperplane. It is notable that the value of random perturbation term should be moderate due to a too small value is helpless to increase the diversity among base classifiers and a too large value can destroy the performance of each base classifier. According to the feedback from a mass of experimental results, we empirically assign each random perturbation term in the range of [−0.1×
                     θ
                     optimal, 0.1×
                     θ
                     optimal], then the actual moving distance θ′ can be calculated as:
                        
                           (25)
                           
                              
                                 
                                    θ
                                 
                                 
                                    ′
                                 
                              
                              =
                              
                                 
                                    θ
                                 
                                 
                                    optimal
                                 
                              
                              ×
                              (
                              1
                              +
                              0.1
                              ×
                              rand
                              )
                           
                        
                     where rand denotes one random number whose value is in the range of [−1,1]. The schematic diagram and pseudo code description of EnSVM-OTHR algorithm are presented in Figs. 4 and 5
                     
                     , respectively.

@&#RESULTS AND DISCUSSIONS@&#

We tested the proposed algorithms on 30 Keel data sets [39]. Similar to much previous work, we focused on binary-class imbalanced problem, too. Information about these data sets is summarized in Table 2
                     .

Statistical analysis is an efficient means to detect the effectiveness of results in machine learning, e.g., Wang et al. [40] used statistical analysis results of instances to validate concept matching technologies, acquiring excellent performance. Therefore, to compare multiple classification algorithms on lots of data sets, we also provided statistical analysis results. Specifically, Friedman test [32,33] is used to detect statistical differences among a group of results, and the Holm post hoc test [33] is adopted to examine whether the proposed algorithm is distinctive among a 1×
                     N comparison. The post hoc procedure allows us to know whether a hypothesis of comparison of means could be rejected at a specified level of significance α. The adjusted p-value (APV) is also computed to denote the lowest level of significance of a hypothesis that results in a rejection. Furthermore, we consider the average rankings of the algorithms in order to measure how good a method is with respect to its partners. This ranking is obtained by assigning a position to each algorithm depending on its performance on each data set. The algorithm which achieves the best accuracy on a specific data set will have the first ranking (value 1); then, the algorithm with the second best accuracy is assigned rank 2, and so forth. This task is carried out for all data sets and finally an average ranking is calculated. The procedure of our statistical tests is the same as Ref. [41]. Additional information and software about these statistical tests can be found on the web: http://sci2s.ugr.es/sicidm/.

First, we tested the proposed SVM-OTHR algorithm in comparison with seven other single classification algorithms based on SVM, including:
                        
                           1.
                           Standard SVM (SSVM) [30]: SVM classifier without any class imbalance correction technologies.

SVM with random undersampling (SVM-RUS): It firstly adopts RUS [11] to preprocess the original training set, then trains SVM classifier using the balanced training data.

SVM with random oversampling (SVM-ROS): It firstly adopts ROS [11] to preprocess the original training set, then trains SVM classifier using the balanced training data.

SVM with SMOTE [12] (SVM-SMOTE): It firstly adopts SMOTE algorithm to oversampling the minority class instances and to make the data set balance, then trains SVM classifier using the balanced training data. Based on the recommendation in Ref. [12], the number of neighbors K is assigned as 5.

Weighted SVM [17] (CS-SVM): It assigns different values for the penalty factor C belonging to different classes, to guarantee the fairness of classifier modeling, we make C
                              +/C−
                              
                              =
                              N
                              −/N
                              +, where C
                              − is tuned by grid search.

z-SVM [19]: This is the algorithm proposed by Imam et al. [19]. Golden section search algorithm was used to find z
                              ∗ that is the optimal z value which corresponds to the maximal G-mean value on the training set.

SVM with decision threshold adjustment [21] (SVM-THR): This is the algorithm proposed by Lin and Chen [21]. We simply used their default function (see formula (16)) to move the classification hyperplane of SVM towards the majority class.

In the experiments, for each data set, we performed a fivefold cross validation. In view of the randomness of classification results, we randomly repeated the cross-validation process for ten times and provided the results in the form of mean±standard deviation. In addition, all algorithms were implemented in Matlab 2013a running environment and SVM was realized by libSVM toolbox with the version of 3.17 [42]. Specifically, for SVM, RBF kernel function was adopted, as well the penalty factor C and the width parameter σ of RBF kernel function were tuned by using grid search with fivefold cross-validation, where C
                     ∈[2−2, 2−1,…, 210], σ
                     ∈[2−6, 2−5,…, 25]. We evaluated these eight classification algorithms by two widely used evaluation criterions in imbalanced classification tasks: F-measure and G-mean. The F-measure and G-means values of each algorithm on each data set are summarized in Tables 3 and 4
                     
                     , respectively. The statistical results are provided in Table 5
                     .

From Tables 3 and 4, we observed that almost all bias correction strategies can promote F-measure and G-mean values compared with standard SVM except SVM-RUS on F-measure evaluation criterion. We consider that the classification hyperplane of RUS can be exceedingly adjusted towards the majority class, consequently causing one low Precision value. Also, we found that two oversampling technologies often perform better on those datasets with relatively high class imbalance ratio, while RUS shows better performance on those ones with relatively low imbalance ratio. The same phenomenon have been observed in Refs. [35,43]. Actually, for highly imbalanced data sets, RUS tends to lose much information, while for relatively balanced data sets, oversampling is apt to be overfitting.

Another interesting phenomenon observed from Tables 3 and 4 is that F-measure and G-mean values not just associate with class imbalance ratio, e.g., on shuttle-2_vs_5 dataset which has a imbalance ratio of 66.67, most classifiers including standard SVM can acquire 100% classification accuracy. As indicated in Refs. [35,44], the destroy of class imbalance should be estimated by examining several different factors, including class overlapping, the size of training instances, class imbalance ratio, noisy level and small disjuncts, etc. In our previous work [45], one pre-estimated strategy by scatter matrix-based class separability measure for estimating the harmfulness of class imbalance had been proposed.


                     Tables 3 and 4 also show that our proposed SVM-OTHR algorithm outperforms SVM-THR algorithm that was proposed by Lin and Chen [21] on majority data sets. Specifically, SVM-OTHR obtains the best F-measure and G-mean values on 12 and 11 data sets, respectively. While SVM-THR only acquires the best F-measure and G-mean values on 4 and 6 data sets. Actually, the experimental results have indicated that SVM-THR is quite competitive with the other bias correction strategies. SVM-OTHR, however, could find better position of decision hyperplane than SVM-THR as the adoption of priori knowledge (original distributions of training instances).


                     Table 5 shows the average ranking computed for all algorithms according to F-measure and G-mean, respectively. In addition, Table 5 also presents that the APV computed by the Holm post hoc test [33] reported between our algorithm and the other algorithms. We can observe that SVM-OTHR has obtained the lowest values in both rankingF and rankingG, thus it is the best algorithm. Moreover, since all APVs are lower than a standard used level of significance of α
                     =0.05, the null-hypothesis of equality is rejected in all cases, which supports the conclusion that SVM-OTHR outperforms the remaining algorithms. From Table 5, we also observed that z-SVM has acquired lower rankingF and rankingG values than CS-SVM. That means one optimized weight can be more advisable than one empirical weight in cost-sensitive learning.

Then we compared the running time of eight single classification algorithms, the results are summarized in Fig. 6
                      which shows that the proposed SVM-OTHR algorithm generally consumes a little more training time than SSVM, CS-SVM and SVM-THR, but much fewer in comparison with two oversampling strategies (SVM-ROS and SVM-SMOTE). As we know, the training time of SVM has the closest relationship with the number of training instances, thus it is not difficult to understand why oversampling often consumes more training time. Meanwhile, it is observed that SVM-SMOTE is often more time-consuming than SVM-ROS as SMOTE needs to compute lots of distances for determining the neighborhood relationship, while ROS only needs to duplicate the positive instances. In addition, it is notable that the time complexity of z-SVM is usually a little higher than that of SVM-OTHR because z-SVM needs to find the optimal z value by one optimization technology, while the training time of SVM-OTHR is merely dependent on the number of initially misclassified instances of the minority class. Moreover, we found that when there are a few training samples, the difference of running time of various algorithms can be ignored. With the increase of training instances, however, the gap will be gradually enlarged.

Next, we tested the performance of EnSVM-OTHR whose all parameters stay the same as that of SVM-OTHR and the number of base classifiers is empirically designated as 40. We compared the performance of EnSVM-OTHR and SVM-OTHR, then provided the percentage of performance increment on each data set in Fig. 7
                      which shows that on majority data sets, EnSVM-OTHR performs better than SVM-OTHR. Specifically, EnSVM-OTHR improves F-measure values on 24 data sets and G-mean values on 27 data sets. The increment of the performance are often higher than 10%. On abalone19 data set, the increment of G-mean value can be even higher than 50%. The experimental results indicate that the combination of SVM-OTHR and Bagging ensemble learning framework is feasible as it can further improve classification performance of the single classifier.

Finally, we compared the performance of EnSVM-OTHR algorithm in comparison with that of six well-known class imbalance ensemble classifiers, including asymmetric Bagging (abbreviated as asBagging) [24], SMOTEBagging [26], RUSBoost [23], SMOTEBoost [22], EasyEnsemble (abbreviated as EasyE) [25] and BalanceCascade (abbreviated as BalanceC) [25]. To guarantee the fairness of the compared results, all ensemble classifiers share one common parameter, i.e., the number of base classifiers was designated as 40. The other parameters adopted the default best ones according to the corresponding Refs. [22–26]. For each algorithm, fivefold cross validation is randomly repeated for 10 times, and then the experimental results are provided in the form of mean±standard deviation. The F-measure and G-mean values of these seven ensemble classification algorithms are described in Tables 6 and 7
                     
                     , respectively. Table 8
                      shows the corresponding statistical results.


                     Tables 6 and 7 show that EnSVM-OTHR performs best on 12 data sets referring to F-measure criterion and obtains the highest values on 9 data sets according to G-mean evaluation measure. As for six other ensemble learning algorithms, they present quite different classification performance on different types’ data sets. Specifically speaking, two SMOTE-based ensemble learning algorithms often outperform the others on those highly imbalanced data sets, while four undersampling-based algorithms usually performs better on those data sets with low imbalance ratio. From the results of these two tables, we can deduce a conclusion that for highly imbalanced classification data, SMOTEBagging can be considered as an excellent candidate irrespective of the acquirement of running time.


                     Table 8 shows the average rankings and APVs computed for all seven ensemble learning algorithms following the same schema of Table 5. We can observe that the proposed EnSVM-OTHR algorithm has obtained the lowest rankingF and rankingG values, indicating it is the best one in these seven algorithms. In this case, all APVs except APVG of EasyE algorithm, are lower than a standard used level of significance of α
                     =0.05, indicating EnSVM-OTHR obviously outperforms 5 other algorithms in statistics. Meanwhile, it indicates that there is no obvious difference between EnSVM-OTHR and EasyE on G-mean evaluation measure.

@&#SUMMARY@&#

In this article, we firstly analyzed the reason why the classification performance of SVM can be damaged by class imbalance in theory. To our best knowledge, it is the first time to theoretically study the influence of class imbalance towards SVM. We also indicated the drawbacks of the existing decision threshold adjustment algorithm SVM-THR, transformed it as an optimization problem, as well as designed an exhaustive search algorithm named SVM-OTHR to automatically find the optimal position of decision hyperplane. Furthermore, we indicated that SVM-OTHR would often lead to the loss of generalization ability, then inserted a small random perturbation term into each SVM-OTHR, finally integrated multiple SVM-OTHRs into Bagging ensemble learning framework to present a improved version of algorithm called EnSVM-OTHR. By a mass of experiments, we found that both proposed algorithms could not only acquire more balanced classification results than SVM-THR algorithm without obviously increased time costs, but also outperform many state-of-the-art class imbalance learning algorithms.

In future work, we wish to further optimize the proposed algorithms to lower their time and space complexity. Additionally, the possibility of combining SVM-OTHR and Boosting ensemble learning framework will be investigated, too.

@&#ACKNOWLEDGMENTS@&#

The work was supported in part by National Natural Science Foundation of China under Grant No. 61305058, No. 61473086, No. 61375001, No. 61100116 and No. 61005008, Natural Science Foundation of Jiangsu Province of China under Grant No. BK20130471 and No. BK2011492, China Postdoctoral Science Foundation under Grant No. 2013M540404, Jiangsu Planned Projects for Postdoctoral Research Funds under Grant No. 1401037B, Nature Science Foundation of the Jiangsu Higher Education Institutes of China under Grant No. 12KJB520003, open fund of Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education under Grant No. MCCSE2013B01, and open fund of the Jiangsu Key Laboratory of Image and Video Understanding for Social Safety (Nanjing University of Science and Technology) under Grant No. 30920130122006.

@&#REFERENCES@&#

