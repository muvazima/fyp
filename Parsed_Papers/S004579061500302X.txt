@&#MAIN-TITLE@&#Workload modeling for resource usage analysis and simulation in cloud computing

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We model a web application to support analysis and simulation of cloud environments.


                        
                        
                           
                           We implement the model as an extension of the CloudSim simulator.


                        
                        
                           
                           We found that the user behavior has a strong influence on the resource utilization.


                        
                        
                           
                           We found Generalized Extreme Value and Generalized Lambda distributions instead of Exponential distribution to represent session time.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Cloud computing

Workload modeling and characterization

Simulation

Distribution analysis

Web applications

@&#ABSTRACT@&#


               
               
                  Workload modeling enables performance analysis and simulation of cloud resource management policies, which allows cloud providers to improve their systems’ Quality of Service (QoS) and researchers to evaluate new policies without deploying expensive large scale environments. However, workload modeling is challenging in the context of cloud computing due to the virtualization layer overhead, insufficient tracelogs available for analysis, and complex workloads. These factors contribute to a lack of methodologies and models to characterize applications hosted in the cloud. To tackle the above issues, we propose a web application model to capture the behavioral patterns of different user profiles and to support analysis and simulation of resources utilization in cloud environments. A model validation was performed using graphic and statistical hypothesis methods. An implementation of our model is provided as an extension of the CloudSim simulator.
               
            

@&#INTRODUCTION@&#

Clouds are being used as a platform for various types of applications with different Quality of Service (QoS) aspects, such as performance, availability and reliability. These aspects are specified in a Service Level Agreement (SLA) negotiated between cloud providers and customers. The failure to comply with QoS aspects can compromise the responsiveness and availability of service and incur SLA violations, resulting in penalties to the cloud provider. The development of resource management policies that support QoS is challenging and the evaluation of these policies is even more challenging because clouds observe varying demand, their physical infrastructure has different sizes, software stacks, and physical resources configurations, and users have different profiles and QoS requirements [1]. In addition, reproduction of conditions under which the policies are evaluated and control of evaluation conditions are difficult tasks.

In this context, workload modeling enables performance analysis and simulation, which brings benefits to cloud providers and researchers. Thereby, the evaluation and adjustment of policies can be performed without deployment of expensive large scale environments. Workload models have the advantage of allowing workload adjustment to fit particular situations, controlled modification of parameters, repetition of evaluation conditions, inclusion of additional features, and generalization of patterns found in the application [2], providing a controlled input for researchers. For cloud providers, the evaluation and simulation of resource management policies allow the improvement of their systems’ QoS. Finally, the simulation of workloads based on realistic scenarios enables the production of tracelogs, scarce in cloud environments because of business and confidentiality concerns [3,4].

Workload modeling and characterization is especially challenging when applied in a highly dynamic environment, such as cloud data centers, for different reasons:(i) heterogeneous hardware is present in a single data center and the virtualization layer incurs overhead caused by I/O processing and interactions with the Virtual Machine Monitor (VMM); and (ii) complex workloads are composed of a wide variety of applications submitted at any time, and with different characteristics and user profiles. These factors contribute to a lack of methodologies to characterize the different behavioral patterns of cloud applications.

To tackle the above issues, a web application model able to capture the behavioral patterns of different user profiles is proposed to support analysis and simulation of resources utilization in cloud environments. The proposed model supports the construction of performance models used by several research domains. Performance models improve resource management because they allow the prediction of how application patterns will change. Thus, resources can be dynamically scaled to meet the expected demand. This is critical to cloud providers that need to provision resources quickly to meet a growing resource demand by their applications.

In this context, the main contribution of this paper is a model capable of representing resource demand of Web application supported by different user profiles in a context of cloud environment. The workload patterns are modeled in the form of statistical distributions. Therefore, the patterns fluctuate based on realistic parameters in order to represent dynamic environments. A model validation is provided through graphical and analytical methods in order to show that the model effectively represents the observed patterns. A secondary contribution of this paper is the validation and implementation of the proposed model as an extension of the CloudSim simulator [1], making the model available for the cloud research community.

The rest of the paper is organized as follows: Section 2 presents the challenges and importance of workload modeling in clouds. Section 3 describes related works. Section 4 details the adopted methodology and how it was achieved. Section 5 presents and discusses the modeling and simulation results. Section 6 concludes and defines future research directions.

Workload characterization and modeling problems have been addressed over the last years, resulting in models for generation of synthetic workloads similar to those observed on real systems [2]. The main objective of such models is enabling the behavior patterns detection on the collected data.

Workload modeling and characterization is especially challenging when applied in a highly dynamic environment such as a cloud, for various reasons, as discussed below:
                           
                              1.
                              Hardware platforms heterogeneity: Information Technology (IT) managers update about 20% to 25% of their platforms every year [5], resulting in the combination of different hardware in the same data center. Besides, the virtualization layer promotes an overhead caused by I/O processing and interactions with the Virtual Machine Manager (VMM). This overhead depends on the hardware platform.

Business and confidentiality: due to business and confidentiality reasons, there are few cloud tracelogs available for analysis. Thus, there is a lack of methodologies to characterize the different behavioral patterns of cloud applications [3,4]. Nevertheless, recent efforts in this direction, such as, Google TraceLog [6] and Yahoo!, enable data analysis and characterization for specific scenarios [7].

Workload complexity: the cloud hosts a wide variety of applications submitted at any time, with different characteristics and user profiles, which have heterogeneous and competing QoS requirements [1]. This leads to complex workloads depending on users’ behavior and resource consumption. Thus, it is challenging to predict workload patterns over time.

Workload modeling increases the understanding of typical cloud workload patterns and leads to more informed decisions to better manage resources [3,7]. From the workload characterization, performance models can be constructed to support research topics such as energy-efficiency and resource management and to answer important research questions, such as: how are overall cloud data center utilization levels affected by user behavior? How cloud data center energy efficiency can be improved while the QoS is maintained?

In addition, workload modeling in cloud computing enables performance analysis and simulation, which brings benefits to cloud providers and researchers as it allows: (i) the evaluation, through simulation, of resource management policies allowing the improvement of cloud services’ QoS; (ii) the evaluation of these policies without deployment and execution of the applications in expensive large-scale environments; and (iii) the simulation of realistic cloud environments with controlled modification, adjustment, and repetition. The use of simulation models enables the production of tracelogs based on realistic scenarios, filling the gap previously identified in the cloud computing area [3].

@&#RELATED WORK@&#

The conflict of priorities between cloud providers, aiming high resource utilization with low operating costs, and MapReduce applications users addressing small execution time, led to characterization and modeling of this application type [4,7,8]. Chen et al. [4] developed a tool for generation of realistic workloads with the goal of analyzing the tradeoff between latency and resources utilization. In attempt to obtain resource utilization data, statistical models were used to generate the job stream processed in the environment provided by Amazon’s Elastic Cloud Computing (EC2). However, the authors do not present information concerning the distributions, parameters and Goodness of Fit (GoF) tests. Thus, the reproduction of the model by other researchers is infeasible.

Ganapathi et al. [8] proposed a model to predict the execution time of MapReduce jobs in order to maximize performance while minimizing costs. A workload generator based on statistical models was used to guide the prediction. However, the authors did not consider the full distribution to build the model, instead, the distribution is estimated from the 1st, 25th, 75th and 99th percentiles, causing information loss that compromises the accuracy of the model.

Kavulya et al. [7] characterized resource utilization patterns and sources of failures to predict job completion times in MapReduce applications. They did not present an analysis of data justifying the distributions used. Furthermore, they use only the method of Maximum Likelihood Estimation (MLE), which is sensitive to outliers. Our approach uses different estimation methods and the results show that the estimator has great influence on the predictor accuracy. Kavulya et al. [7] presented Kolmogorov–Smirnov (KS) values with no critical value. Thus, it is not clear which set of distributions and parameters provide a good fit to the modeled data.

Most works discussed in this session focus on resource utilization imposed by jobs, tasks, and requests. However, different types of users directly impact the cloud workload as observed by the cloud provider. In view of this, Moreno et al. [3] and Grozev and Buyya [9] created models based on resource utilization and users’ behavioral patterns. Moreno et al. [3] proposed a new approach for characterization of the Google trace log in the context of both user and task in order to derive a model to capture resource utilization patterns. However, although the model parameters are presented, estimation methods and the results of the goodness-of-fit tests are omitted. These factors compromise the use of the model by other researchers.

Grozev and Buyya [9] also made use of the Rice University Bidding System (RUBiS) workload and implemented the model in the CloudSim. The Central Processing Unit (CPU) load is modeled in terms of average number of instructions required by a user session. However, the average is not robust, which makes it easily influenced by peak usage. Also, the authors adopt a non-statistical approach for modeling, thus, they did not analyze dispersion and shape measurements, which are relevant for a statistical workload characterization.

From these related works, we observe that the application characterization and modeling supports researchers and cloud providers in understanding complex tradeoffs and predicting applications behavior. This understanding leads to more efficient techniques for resources management. However, the lack of a well-defined methodology, containing steps to achieve distributions, estimate parameters, and goodness-of-fit tests, prevents reproduction and usage of models. In this scenario, our proposal consists in a Web application model achieved through a well-defined methodology. A summary of the related work is presented in Table 1
                     .

@&#MATERIAL AND METHODS@&#

@&#METHODOLOGY@&#


                        Fig. 1
                         presents the methodology [2] used to create the proposed models. This methodology begins with users submitting requests to cloud environment while the operational metrics are measured and stored in data logs.

After the monitoring and tracing, the user activity and performance modeling is carried out based on three steps: (i) statistical analysis, which analyzes the data characteristics, determines if some data transformation is necessary and defines the candidate distributions to represent the model; (ii) parameter estimation, which, given the distribution selected in advance, uses estimation methods to set the parameters of the model based on the collected samples; and (iii) GoF tests, which are methods to evaluate whether the distributions and their respective parameters, provide satisfactory approximation to the empirical data.

The cloud environment is suitable for interactive real-time applications. However, recent works have been focusing on provisioning and scheduling techniques for batch applications, while cloud resource management in interactive applications has not received much attention so far [9]. In view of this, we utilize the widely used RUBiS [10] benchmark in order to evaluate the impact of the user in the resource consumption patterns.

In the context of cloud computing, RUBiS has been employed in the construction of a performance model of a 3-tier application in cloud environments [9] and on the evaluation of a Benchmark-as-a-Service platform, enabling the determination of the bottleneck tier and to tune the application servers to improve application performance [11]. The used methodology is not limited to RUBiS and thus it can be extrapolated to other workload categories [2].

The RUBiS benchmark is an auction site, based on eBay.com, implementing functionalities such as selling, browsing, and bidding. It provides two user profiles: browsing profile, including only read interactions with the database, and the bidding profile, which includes 85% read and 15% reading and writing in a database [10].

A series of decisions must be made before the application modeling process is carried out. This includes the definition of the experimental environment and interest metrics. The experimental environment consisted of a private cloud comprising three high-performance servers interconnected through a Gigabit Ethernet switch. The physical resources management was accomplished via OpenStack (Grizzly) [12]. Our experiments had two types of VMs, small and medium, whose hardware configurations are described in Table 2
                        . The server had twice the Random Access Memory (RAM) existed on the client. For compatibility with the monitoring tool Bro [13], RUBiS version 1.4.3 was used.

The client VM was configured with the load generator. The server VM hosts a Linux, Apache, MySQL and Hypertext Preprocessor (PHP) (LAMP) stack. The Linux version on the server was the same used on the client (Ubuntu 12.04.2 LTS). The MySQL relational database (14.14) was configured with a maximum number of connections equal to the number of virtual CPUs on the server VM. Finally, we used version 5.3.10 of the PHP scripting language.

To accurately model the user behavior impact on the processor, we monitored the total number of executed instructions and how they arrived in the processor during a user session through the CPU utilization, which was the second metric analyzed in this study.

RUBiS randomly generates the size of user sessions from a negative exponential distribution. This value is static for any CPU that processes the session. Converting such a value to time, according to the CPU clock, is reasonable if all CPUs have the same characteristics (architecture, core number, manufacturer, etc.). When CPU characteristics are heterogeneous, estimation of the execution time is a difficult task. Thus, the instructions number is captured with the aim of creating a model that characterizes the user session in terms of number of executed instructions. Therefore, the session execution time varies according to the hardware’s processing capacity.

The third and fourth metrics analyzed in this study were memory and disk utilization. The latter one is measured in terms of Transactions Per Second (TPS), which represents the number of reads and writes transfers per second performed on the disk. This measure is more easily extrapolated to different disk hardware settings when compared to percentage of disk utilization.

The fifth metric considered was the response time, which was monitored and traced to characterize the Quality of Service (QoS) provided by RUBiS to its users. In this work, the user response time definition consists of the sum of the reply time and transfer time. The reply time is the amount of time between the instant at which the client receives the first reply packet, and the time at which the client issued the HTTP request. Transfer time is the time taken for the entire reply to be transferred to the client [14]. Since the bidding profile offers a more realistic mix of user actions (registration, bid on items and consult current bids, rating, and comments left by other users), its response time provides a more meaningful indication of performance than the response time of the Browsing profile.

Nonetheless, Hashemian et al. [14] verified that RUBiS introduces significant errors in the measured end-user response times. For this reason, this work adopts their monitoring approach, where the tcpdump tool [15] is used to capture all HTTP packets. At the end of the user session, a modified version of the Bro tool [13] captures the tcpdump output file and calculates the reply times and transfer times of HTTP requests and records them. The Bro was executed in an off-line manner to avoid the overhead on the client.

The metrics samples were obtained through 100 executions of the same experiment, which consists of requests submission from a single client to Apache and MySQL servers. The data was captured throughout the processing of the user session on the server. RUBiS was configured using the parameters presented in Table 3
                        .

Empirically, we observed that 100 repetitions of the experiment are large enough to capture most of the patterns found in the user sessions. To avoid interference of outliers, our statistical analysis is performed on the median of the repetitions. In the experiments, both RUBiS user profiles are used. RUBiS reproduces the think time via negative exponential distribution with mean between 7 and 8 s, as defined by TCP-W benchmark [16].

Most of e-commerce sessions last less than 16.66 min [17] and the reasonable longest time for user Web browsing are 15 min [18]. In this context, the session time uses the negative exponential distribution with the mean equals to 15 min, consistent with the RUBiS specifications [10].

The patterns identified in collected data were modeled in the form of statistical distributions. Then, given n observations, 
                           
                              
                                 x
                                 1
                              
                              ,
                              
                                 x
                                 2
                              
                              ,
                              …
                              ,
                              
                                 x
                                 n
                              
                              ,
                           
                         representing a metric from an unknown population, our goal was to find the Probability Density Function (PDF) that represents the data adequately. The PDF has the form f(x, θ), where θ is the vector of parameters estimated from the available samples.

Understanding the characteristics of such samples is important to determine which distributions better approximate the collected data. To this end, we performed a data analysis through test statistics and graphs, focusing on statistics that characterize the shape of the data: skewness and kurtosis.

Since the characteristics of the sample are known, the distribution candidates for the metrics number of instructions, CPU utilization, memory utilization, and disk transactions per second were selected considering both user profiles supported by RUBiS. Additionally, the parameters of response time metric were estimated considering the Bidding profile. The Generalized Lambda (GL) distribution is able to represent different shapes, including those with negative skewness. Therefore, we used it as an alternative to represent the observed number of instructions. The GL is a generalization of the four parameters of lambda distribution family [19]: the first parameter is the location, represented by the median (μ), the second parameter is the scale, represented by the inter-quartile range (σ), and the last two parameters are related to shape, featuring the skewness (α
                           3) and the steepness of the distribution (α
                           4), respectively.

Another alternative selected is the Generalized Extreme Value (GEV) distribution [20]. This distribution has three parameters, namely location (μ), scale (σ), and shape (ξ) and can represent a variety of forms that include three different distributions: (i) Gumbel, when 
                              
                                 ξ
                                 =
                                 0
                                 ,
                              
                            (ii) Frechet, when ξ < 0, and (iii) Weibull, when ξ > 0.

To represent the CPU, memory and disk utilization, 21 different continuous distributions were analyzed for the user profiles. Among them, we highlight the Generalized Weibull Distribution (GWD) [21] and 3-parameter Error distribution [22]. Both have the location (μ), scale (σ) and shape (ξ) parameters. As GL and GEV, these distributions can also be specialized to represent other distributions, which makes them extremely flexible in fitting different kinds of data.

After choosing the distributions, it is necessary to estimate the parameters of the models. One option is to calculate the moments of the sample and use them as estimators for the moments of the distribution. However, this approach is highly sensitive to outliers, especially for the third and fourth moments. This limits its utilization in the case of distributions used in this work [2,23]. As noted in Table 5, five different estimation methods were used [23]: Maximum Log-Likelihood (mle), Histogram Fitting (hist), Quantile Matching (quant), Probability Weighted Moments (pwm) [24], and Maximum Product of Spacing Estimator (mps) [25].

The CPU utilization metric proved much harder to adjust when compared to the other metrics. Even compared to a higher number of distributions (total = 21), we obtained high values of D and values of 
                              
                                 p
                                 -Value
                              
                            lower than the critical value. Therefore, a parameter estimation previous phase called pre-fitting was performed. This step consists in changing the data scale to make the normality assumption plausible. Thereby, the following mathematical transformations were applied to this metric: log, square-root, and inverse.

Once the selected parameters and their distributions are known, the next step was the determination of the models that fit to the data through GoF tests. The GoF statistics verifies if the empirical and theoretical data belong to the same distribution. In this study, two methods were used to assess if the selected distributions provide good fit to the data: one graphic method using Quantile–Quantile (Q–Q) plots, and one analytical method using the KS test.

The Q–Q plots technique consists in the calculation of empirical and theoretical distribution quantiles and plotting one in terms of the other [2]. It allows verifying if two data sets belong to the same distribution and other aspects simultaneously, for example, the presence of outliers.

The KS test evaluates the hypothesis that the observed data belongs to a population that follows one or more probability distributions. This test can be defined as the hypotheses: (i) Null Hypothesis (H
                           0
                           ): the observed data follows the specified distribution and (ii) Alternative Hypothesis (Ha): the observed data does not follow the specified distribution.

The KS test has an important limitation: the parameters from F(x) must be known in advance rather than estimated from the observed data, as performed in this work. In order to circumvent this limitation, we used the bootstrapping method [26].

In this section, we detail the implementation of the web application modeling as an extension of the CloudSim simulator [1]. The application modeling was developed in CloudSim because this simulator contains abstractions for representing cloud infrastructures and power consumption.

The process of generating the load simulator is shown in Fig. 2
                           . At the start of the simulation, the ECommerceApp class is instantiated with the parameters number of users and arrival rate of users, which can be a fixed value or generated by a distribution, and, finally, the user profiles (browsing and bidding). With these parameters, the ECommerceApp instantiates the UBehavior class responsible for encapsulating the users’ behavior. This behavior is defined by the statistical distributions that represent the total number of instructions, CPU, memory and disk utilization, and response time. The models are developed using the R statistical language [27] that communicates with CloudSim through the REngine library.

Once the users’ behavior (number of requests, their arrival time, their size in terms of number of instructions, memory and disk demands, and response time) is known, a USession is instantiated for each user. Afterwards, USession instantiates the set of Request that will compose it. In each simulation step, one or more requests are processed until all are completed.

Graphical and statistical hypothesis test approaches were used to evaluate the accuracy of the model in the simulator by comparing the simulated against the observed data. The Wilcox Mann–Whitney (WMW) hypothesis test [28] consists in the evaluation of the hypothesis that the simulated data belongs to the population that follows the probability distribution of the observed data. If 
                              
                                 p
                                 -
                                 Value
                                 >
                                 α
                                 ,
                              
                            the hypothesis cannot be rejected. Otherwise, the null hypothesis is rejected. The level of significance was set at 
                              
                                 α
                                 =
                                 0.05
                              
                            for all the tests.

@&#RESULTS AND DISCUSSION@&#


                        Table 4
                         presents the descriptive statistics related to the sum of the number of instructions consumed by Apache and MySQL services, CPU, memory and disk utilization for both user profiles, and response time for Biding profile. Regarding the number of instructions and memory, the negative value of skewness is reinforced because the median is greater than the average. This characteristic is clearly observed in the histograms of the number of instructions consumed by Apache and MySQL services (Figs. 3
                        a and 4
                        a) through the long left tail relative the right tail. The negative skewness was primordial in the choice of distributions used to fit the number of instructions.


                        Table 4 also shows high kurtosis values for CPU, disk and response time. This characteristic is seen in Figs. 3a and 4a through a well pronounced peak, near the median. These metrics have many time intervals equal or close to zero. Therefore, the non-zero values promote a large scale difference contributing to the presence of peaks.

The change from browsing to bidding profile implies in a memory consumption increment. However, the disk consumption is much lower compared to memory consumption because, in general, e-commerce applications are in-memory, i.e., the information is transferred from disk to memory (cache) to avoid slow Web response times.


                        Figs. 3b and 4b show the number of instructions executed by the CPU concerning the Apache and MySQL services for each of the 100 user sessions performed during the experiment. In both profiles, MySQL requires more processing than Apache. Furthermore, none of these services are bottlenecks for the application in this experimental setting.


                        Fig. 5
                        a depicts the scatterplot of percentage of CPU utilization over a user session, where we found a higher CPU consumption at the beginning of the session. This consumption decays rapidly to zero or close to zero and so continues until the end of the session. The concentration of data in a single well-pronounced peak near the median with fast decay reinforced the high kurtosis presented in Table 4.

Due to the large difference in scale between the percentages of CPU utilization reflected in Fig. 5a, the values of the axes are limited in order to observe the CPU utilization behavior when it is equal or close to zero, as shown in Fig. 5b. The same pattern of behavior is identified in the bidding profile.


                        Fig. 5b shows that instructions arrive to the processor in bursts followed by periods of inactivity, because of think times of users, for both profiles. However, the number of instructions in each cycle and the frequency with which they occur varies over time, so there is no pattern about where the peaks and troughs of cycles will happen, indicating a stationary time series. Thus, the data are subjected to a statistical hypothesis test of stationarity, where the Augmented Dickey–Fuller (ADF) [29] and Kwiatkowski–Phillips–Schmidt–Shin (KPSS) [30] tests are computed. In these tests, the significance level are fixed at 
                           
                              α
                              =
                              0.05
                           
                        . Both tests showed that the data are stationary: for ADF, 
                           
                              p
                              -Value
                              =
                              0.01
                           
                         and, for KPSS, 
                           
                              p
                              -Value
                              =
                              0.1
                           
                        .


                        Table 5
                         shows the values of the estimated parameters for the selected distributions in combination with the different estimation methods for the number of instructions considering both user profiles. The GL distribution has four estimated parameters, because the sample is the same for all estimation methods. The median values for the browsing profile (
                           
                              
                                 μ
                                 ^
                              
                              =
                              4.873
                              e
                              +
                              08
                           
                        ) and for the bidding profile (
                           
                              
                                 μ
                                 ^
                              
                              =
                              4.877
                              e
                              +
                              08
                           
                        ), as well as inter-quartile range for the browsing profile (
                           
                              
                                 σ
                                 ^
                              
                              =
                              1.919
                              e
                              +
                              07
                           
                        ) and for the bidding profile (
                           
                              
                                 σ
                                 ^
                              
                              =
                              1.735
                              e
                              +
                              07
                           
                        ) remain constant for all the combinations.

In contrast, the shape parameters (α
                        3) and (α
                        4) of the GL distribution have their values influenced by the estimation method. Similarly, the value of the location parameter for the browsing profile (
                           
                              
                                 μ
                                 ^
                              
                              =
                              4.783
                              e
                              +
                              08
                           
                        ) and for the bidding profile (
                           
                              
                                 μ
                                 ^
                              
                              =
                              4.820
                              e
                              +
                              08
                           
                        ) of the GEV distribution remains constant, while the scale parameters (σ) and shape (ξ) are influenced by the estimation method. Therefore, if the selected fitting distributions have shape parameters, it is important to verify the existence of outliers in the sample in order to choose the appropriated estimation method. Fig. 6
                         shows how sensitive the mle and pwm estimation methods are to the presence of outliers.


                        Table 6 contains the parameters estimated through the mle method, for the distributions that represent the CPU, memory and disk utilization, and response time metrics and offer the best fit for the data, according to the KS test. The GEV distribution best fits CPU (Bidding), memory (Browsing), disk (Browsing), and response time (Bidding) enhancing the results found in Moreno et al. [3] that uses GEV to model the consumption of CPU and memory from the data provided by the Google Cloud TraceLog [6]. The other scenarios are covered by the GWD and Error(3P). All these distributions have a shape parameter that allows a better fitting.


                        Fig. 6 shows the Q–Q graphs for the following pairs distribution/estimation method: GL/mle and GEV/pwm. For each graph, the reference line is plotted. It can be noticed that the pair GEV/pwm quantiles (Fig. 6b), in terms of the observed data quantiles, have greater proximity to the reference line. Thus, this pair is a strong candidate to represent the observed number of instructions behavior. It is interesting to compare the pairs GEV/pwm and GL/mle (Fig. 6a). So, it can be observed how sensitive the mle estimation method is to the presence of outliers.


                        Table 7
                         presents the values of D and 
                           
                              p
                              -Value
                           
                         test statistics for the number of instructions observed to the distributions of probability specified in Section 4.4.1. Considering the browsing profile, there are only 3 cases where the null hypothesis cannot be rejected because the 
                           
                              p
                              -Value
                              >
                              0.05
                           
                        : GL/mle, GL/mps and GEV/pwm. Considering the profile bidding, there are 2 cases where the null hypothesis cannot be rejected: GL/mle and GL/mps. Other important information that can be inferred from the table is that the results are sensitive to the applied estimation method. The GL distribution with hist and quant estimation methods presents performance near or below to the symmetric and positive asymmetric distributions.

The estimation methods pwm and mps perform better than the mle method for both GEV and GL distributions, for both profiles. This can be justified by the fact that the mle method is equivalent to maximizing the geometric mean. Therefore, it is highly sensitive to outliers. Fig. 6a shows the presence of outliers in the data. Furthermore, the mle method provides good results for a small sample size, while the pwm and mps are more robust methods.

The GEV distribution with parameters estimated using the pwm method presents the best fit to browsing profile. In contrast, the GL distribution with parameters estimated using the mps method presents the best fit for the bidding profile.

In accordance with the presented results, the models are defined representing the number of instructions executed during a user session for both user profiles. These models aim to characterize user session based on the number of instructions instead of session runtime. Thus, the session runtime will vary according to the characteristics of the CPU. On the other hand, if the model characterized the session runtime, this value would be constant regardless the processor. Also, establishing relationship between session runtime for different processors in a heterogeneous environment is a complex task, since several factors impact the runtime such as number of cores, clock frequency, and architecture.

The GoF tests defined the distribution/parameters pairs more apt to represent the observed metrics. These pairs are used to compose the model that represents the resources demand of the web application according to user profiles. Table 8
                         shows the 
                           
                              p
                              -Value(max)
                              ,
                           
                        
                        
                           
                              p
                              -Value(min)
                           
                         and 
                           
                              error
                              (
                              %
                              )
                           
                         calculated based on 500 replications of KS test. An error is computed when sampled simulated data does not belong to the same sample data observed, i.e., 
                           
                              p
                              -Value
                              <
                              0.05
                           
                        . No resource has an error greater than 6%, indicating that the models can correctly represent the collected data.

Therefore, the Browsing model is represented by the GEV distribution with 
                           
                              
                                 ξ
                                 ^
                              
                              =
                              −
                              1.324
                              e
                              +
                              00
                              ,
                              
                                 μ
                                 ^
                              
                              =
                              4.873
                              e
                              +
                              08
                              ,
                              
                                 σ
                                 ^
                              
                              =
                              1.839
                              e
                              +
                              07
                           
                         parameters, representing the total number of instructions that will compose the user session, and distribution GWD with 
                           
                              
                                 ξ
                                 ^
                              
                              =
                              5.386
                              ,
                              
                                 μ
                                 ^
                              
                              =
                              −
                              0.976
                              ,
                              
                                 σ
                                 ^
                              
                              =
                              0.014
                           
                         parameters, representing how the total of instructions will be distributed throughout the session based on CPU utilization.

The Bidding model, on the other hand, is modeled by the GL distribution with 
                           
                              
                                 μ
                                 ^
                              
                              =
                              4.877
                              e
                              +
                              08
                              ,
                              
                                 σ
                                 ^
                              
                              =
                              1.735
                              e
                              +
                              07
                              ,
                              
                                 
                                    α
                                    3
                                 
                                 ^
                              
                              =
                              −
                              9.483
                              e
                              −
                              01
                              ,
                              
                                 
                                    α
                                    4
                                 
                                 ^
                              
                              =
                              9.777
                              e
                              −
                              01
                           
                         parameters, representing the total number of instructions, and the distribution GEV with 
                           
                              
                                 ξ
                                 ^
                              
                              =
                              −
                              0.259
                              ,
                              
                                 μ
                                 ^
                              
                              =
                              0.576
                              ,
                              
                                 σ
                                 ^
                              
                              =
                              0.004
                           
                         parameters, representing the CPU utilization. The models that represents the disk and memory demands of the two profiles and the response time experienced by the Bidding profile are presented in Table 6.

Due to trace or model unavailability, unrealistic assumptions are made in the literature about the workload [31], such as set of requisitions with fixed inter arrival times, simple Poisson models to represent instruction arrivals and exponentially distributed session time. However, in this work, we achieved different results from assumptions commonly made, such as: stationarity in the data observed, the distribution representing the arrival of instructions in the processor is GWD, and the distributions that represent the total number of instructions, which is a metric correlated with session time, are GEV and GL.

The graphical validation is shown in Fig. 7
                        , while the WMW test results are reported in Table 9
                        . Fig. 7 shows a comparison of the histogram of observed data against probability density function of the simulated data to the number of instructions metric, considering both user profiles. The simulated data are consistent with the observed data for both profiles. However, visually, the Browsing profile provides a better approximation to the observed data. This result is reinforced by Table 9, where the Browsing profile has an error three times smaller than the Bidding profile.


                        Table 9 shows the maximum and minimum 
                           
                              p
                              -Values
                           
                         obtained through the WMW test applied to a set of 100 samples of observed data and 100 samples of simulated data. For the total of 10,000 comparisons, the error was also calculated. The error is computed when sampled simulated data does not belong to the same sample data observed, i.e., 
                           
                              p
                              -Value
                              <
                              0.05
                           
                        . The error for all metrics is less than 10%. Among the mathematical transformations applied to the CPU utilization metric, the inverse transformation offers the best results, reducing the Wilcox error rate of 34% to 6%, considering the Browsing profile.

Then, we can conclude the implementation of web application modeling in the CloudSim simulator is capable of producing data to accurately represent both user profiles. Thus, it can be used by researchers to build performance models and to produce tracelogs based on realistic scenarios and extrapolating the results with controlled modification of parameters such as number of users, software stack, and physical and virtual machine configuration. Furthermore, this implementation contributes to the development of performance models to support emerging cloud computing research domains, such as resource allocation in Mobile Cloud Computing (MCC) in which the trade-off between time and energy is a management challenge [32].

@&#CONCLUSION@&#

We applied a well-defined methodology to generate a Web application model for a cloud data center workload. It contains steps and justifications to achieve the distributions and parameters derived from application analyses and it can be extrapolated to other workload categories. Thereby, our model can be easily reproduced by researchers and cloud providers to support different research domains. It was implemented as an extension of the CloudSim simulator and its validation demonstrated that the Web application modeling can produce data to accurately represent different user profiles.

Based on our model and experiments, the following observations can be highlighted: (i) the user profile type (i.e., model of the user behavior) has a strong influence on resource utilization, so we need different statistical distributions to represent the total number of instructions and CPU, memory and disk utilization. Therefore, user behavior must be considered in workload modeling to reflect realistic conditions; and (ii) we observe the presence of stationarity instead of fixed arrival times to represent instructions arrival on the processor, GWD and GEV distributions instead of simple Poisson models to represent instruction arrivals, and Generalized Extreme Value and Generalized Lambda distributions instead of Exponential distribution to represent session time.

As future work, we are planning to (i) incorporate a model of user arrival including daily cycle characteristic; (ii) evaluate the impact of different sizes of user population on the observed metrics; and (iii) develop provisioning policies based on the proposed model to meet the web applications demand.

@&#ACKNOWLEDGMENTS@&#

The authors thank Nikolay Grozev, Ph.D. candidate, for his valuable suggestions on the manuscript. Deborah M.V. Magalhães thanks the financial support from CAPES (Ph.D. scholarship) and CNPq (Doctorate Sandwich Abroad – SWE). This research was funded by the Australian Research Council through Future Fellowship program. This is also a partial result of the National Institute of Science and Technology – Medicine Assisted by Scientific Computing (INCT-MACC) and the SLA4Cloud project (STIC-AmSud program, CAPES process: 23038.010147/2013-17).

@&#REFERENCES@&#

