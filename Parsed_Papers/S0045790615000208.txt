@&#MAIN-TITLE@&#Automatic face recognition system based on the SIFT features

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We proposed and implemented a new face corpus creation algorithm.


                        
                        
                           
                           We created a new facial corpus from the data of the Czech News Agency.


                        
                        
                           
                           We evaluated a novel face recognition method, the SIFT based Kepenekci approach.


                        
                        
                           
                           We proposed and evaluated two novel confidence measure techniques.


                        
                        
                           
                           We proposed, implemented and evaluated the fully automatic face recognition system.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Face recognition

Face detection

Czech News Agency

Corpus creation

Confidence measure

Scale Invariant Feature Transform (SIFT)

@&#ABSTRACT@&#


               
               
                  The main goal of this paper is to propose and implement an experimental fully automatic face recognition system which will be used to annotate photographs during insertion into a database. Its main strength is to successfully process photos of a great number of different individuals taken in a totally uncontrolled environment. The system is available for research purposes for free. It uses our previously proposed SIFT based Kepenekci approach for the face recognition, because it outperforms a number of efficient face recognition approaches on three large standard corpora (namely FERET, AR and LFW). The next goal is proposing a new corpus creation algorithm that extracts the faces from the database and creates a facial corpus. We show that this algorithm is beneficial in a preprocessing step of our system in order to create good quality face models. We further compare the performance of our SIFT based Kepenekci approach with the original Kepenekci method on the created corpus. This comparison proves that our approach significantly outperforms the original one. The last goal is to propose two novel supervised confidence measure methods based on a posterior class probability and a multi-layer perceptron to identify incorrectly recognized faces. These faces are then removed from the recognition results. We experimentally validated that the proposed confidence measures are very efficient and thus suitable for our task.
               
            

@&#INTRODUCTION@&#

Automatic Face Recognition (AFR) consists in identification of a person from an image or from a video frame by a computer. This field has been intensively studied by many researchers during the past few decades. Nowadays, it can be seen as one of the most progressive biometric authentication methods. Numerous AFR methods have been proposed and the face recognition has become the key task in several applications as for instance surveillance of wanted persons, access control to restricted areas, automatic annotation of photos in photo sharing applications or in social networks, and so on.

From the viewpoint of the image nature and quality, it is possible to divide this spectrum of applications to two groups: controlled and uncontrolled scenario systems. The image quality of the controlled systems is usually high, a face pose variation and other dissimilarities (lighting conditions, face tilt and rotation, etc.) are limited. Automatic face recognition is easier because the face detection and further image preprocessing are usually not necessary and the performance of current AFR approaches is often sufficient. Most of the current AFR systems belong into this group.

However, few uncontrolled scenario systems exist. The main issue of these systems is that the face pose and position significantly differ and also the other dissimilarities within the images are common. A successful face recognition is thus much more challenging. This issue will be addressed in this paper and solved by the goals described next.

The first goal consists in proposing an algorithm to extract the faces from the photographs and to create a facial corpus as high quality as possible. The design and implementation of this algorithm is one important contribution of this paper. We will further show that the usage of this algorithm in our AFR system in the preprocessing step is beneficial.

It is necessary to identify an optimal face recognition approach to be integrated into our system. In our previous work [23] we proposed an efficient SIFT based Kepenekci face recognition approach and evaluated it on the standard ORL [15] corpus. We showed that this approach gives very good results using this small well controlled face dataset. The second goal thus consists in evaluation of our SIFT based Kepenekci on the other larger standard face corpora. We have chosen three large challenging face databases: FERET [33], AR [29] and LFW [17]. Our method is then compared with a number of very efficient AFR approaches in order to identify the most suitable one for the large data and for processing of the less controlled images. This method will be integrated into our system.

Face recognition in an uncontrolled environment is erroneous and it is beneficial to identify incorrectly recognized examples. The third goal of this paper thus consists in proposing and evaluating the confidence measure methods in order to identify such examples and remove them from the recognition results or propose them to the user for manual correction. Two novel supervised confidence measures based on a posterior class probability and a multi-layer perceptron are proposed.

The last goal of this paper is to introduce an experimental fully automatic face recognition system. This system will be used by the Czech News Agency (ČTK)
                        1
                        
                           http://www.ctk.eu.
                     
                     
                        1
                      to annotate people in photographs during insertion into the ČTK database.
                        2
                        
                           http://multimedia.ctk.cz/en/foto/.
                     
                     
                        2
                      Its main strength is to successfully process photos of a great number of different persons taken in a totally uncontrolled environment. The original images have high resolution but the size of the faces varies substantially. There are also significant variances in lighting, ageing, face poses and angles. Moreover, another property is presence of more objects/faces in the images. The recognition of such images is thus very challenging. The system (with the source code) is available for research purposes for free. Note that the resolution of face images is strictly related to our proposed application. However, our face recognition approach is general enough to process images of different resolutions.

This paper is organized as follows. The following section summarizes successful approaches in the face recognition domain including confidence measure methods and face recognition systems. Section 3 describes the proposed corpus creation algorithm. Section 4 details the SIFT based Kepenekci method which we use for the face recognition. The next section details the proposed confidence measure methods which are used in order to detect and remove incorrectly recognized faces from the recognition results. Section 6 describes the architecture of our face recognition system. The performed experiments are presented in Section 7. This section describes the corpora which are used to evaluate the system. Furthermore, we compare our AFR module with the other efficient methods on these corpora. The performance of our system on the lower quality real ČTK data with the particular focus on the proposed corpus creation algorithm is shown further. In the last part of this section we evaluate our confidence measure methods. Section 8 briefly summarizes the main scientific contribution of this paper. In the last section, we conclude the results and propose some future research directions.

@&#RELATED WORK@&#

One of the first successful approaches in the AFR domain is the Principal Component Analysis (PCA), so called Eigenfaces [39]. This is a statistical method that takes into account the whole image as a vector. First, the image vectors are put together and the image matrix is formed. The eigenvectors of this matrix are calculated. The face images can then be expressed as a linear combination of these vectors. Each image is represented as a set of weights for the corresponding vectors. The PCA based approaches are still popular, as shown in [34].

Another method, the Fisherfaces [5], is derived from Fisher’s Linear Discriminant (FLD). Similarly to Eigenfaces, Fisherfaces project an image into another, less dimensional, space. The original dimensionality, which is given by the resolution of the images, is reduced to the number of images (distinct classes). The projections of the facial images are then compared using a suitable similarity measure. The key point is maximization of the ratio of between-class scatter and within-class scatter. However, Eigenfaces maximize the total scatter across all images. According to the authors, this approach should be insensitive to variations in lighting conditions. A recent extension of this approach, called L-Fisherfaces, is proposed in [48]. Authors experimentally showed that this method achieves higher accuracy than four other evaluated methods on the three datasets.

Independent Component Analysis (ICA) can be also successfully used in the automatic face recognition field [11]. The main principle is to find a linear combination of non-Gaussian data that reconstructs the original data. Contrary to PCA, ICA uses higher order statistics. ICA thus provides more powerful data representation. ICA approach is still worthy of attention as shown in [10]. Authors further propose a Locality Pursuit (LP) approach, which uses locality preserving projections in the high-dimensional whitened space. This approach addresses the issue of pursuing low-dimensional features. Experimental results show that the LP approach achieves on the FERET corpus higher accuracy than ICA.

Another efficient AFR approach is the Elastic Bunch Graph Matching (EBGM) [6]. This approach uses features constructed by the Gabor wavelet transform. Initially, a set of manually labelled landmarks is presented to the algorithm. These landmarks are used as examples to determine the landmark positions in novel images. The Gabor wavelet convolutions (so called Jets) are computed in the landmark positions and are used for a face representation. A “bunch graph” is created from these examples. Each node in the graph contains a set of Jets for one landmark across all of the images. The similarity of faces is determined from the landmark positions and jet values.

Kepenekci proposes in [20] an algorithm that outperforms the classical EBGM and that addresses the main issue of Elastic Bunch Graph Matching, manual labelling of the landmarks. Landmark positions are obtained dynamically by the Gabor filter responses.

Other successful approaches [1] use so-called, Local Binary Patterns (LBP) for facial feature extraction. The LBP operator thresholds a local region in the image by a value of the central pixel. It labels the pixels either 0 or 1 if the value is lower or higher than the threshold. Then, a histogram of the labels is computed and used as a descriptor. The original method used a 3
                     
                        
                           ×
                        
                     
                     3 neighbourhood which was later extended to use neighbourhoods of various sizes. In the application for face recognition, a facial image is first divided into rectangular regions. The LBP descriptor is constructed in each region and the results are put together to create one vector representing the face. The face representations are compared using a nearest neighbour rule.

A modification of the original LBP approach called Dynamic Threshold Local Binary Pattern (DTLBP) is proposed in [25]. It takes into consideration the mean value of the neighbouring pixels and also the maximum contrast between the neighbouring points. This variation is less sensitive to the noise than the original LBP method. Another extension of the original method is Local Ternary Patterns (LTP) proposed in [38]. It uses three states to capture the differences between the centre pixel and the neighbouring ones. Similarly to the DTLBP the LTP is less sensitive to the noise. The so called Local Derivative Patterns (LDP) are proposed in [47]. In contrast with the original LBP, it uses features of higher order. It thus can represent more information than the original LBP.

Yang and Chen show in [45] the accuracy of the LBP based approaches on the low resolution images. The experimental results are compared with some subspace algorithms on the four down-sampled benchmark face datasets (FERET, Extended YaleB, CMU PIE and AR [29]). The image resolution vary from 32
                     
                        
                           ×
                        
                     
                     32 to 60
                     
                        
                           ×
                        
                     
                     60pixels.

Speeded-Up Robust Features (SURF) [3] is a useful method for key-point detection and creation of a descriptor. An integral image [40] is utilized to speed-up the key-point detection process. The detector is based on the Hessian matrix.
                        3
                        The Hessian matrix is a square matrix of the second order partial derivatives of a given function.
                     
                     
                        3
                      Therefore, it is called the “Fast-Hessian” detector. Box filters are used as an approximation of second order Gaussian derivatives. The box filters are then up-scaled and applied to the original image. This method is invariant to face rotation. To ensure rotation invariance, one orientation is assigned to each key-point. The computation is based on the circular neighbourhood of the key-points.

Another efficient AFR method is based on the Scale Invariant Feature Transform (SIFT) [2]. The SIFT algorithm is used to create a set of descriptors (features) for each image. These features have the ability to detect and describe local image properties and the algorithm ensures invariance to image scaling, translation, rotation and also lighting conditions.

Another approach using SIFT is called Fixed-key-point-SIFT (FSIFT) [21]. As opposed to the previous method, SIFT descriptors are fixed in predefined locations determined in the training step.

Another interesting face recognition approach based on linear regression is proposed by Naseem et al. [32]. The authors reported very good recognition scores on two different subsets of the ORL and FERET corpora. Lu et al. proposed in [28] a novel discriminative multi-manifold analysis method which learn discriminative features from image patches. Conversely to our approach, this approach uses exactly one image sample for training. Experimental results on three face databases (AR, FERET and FG-NET) showed the efficacy of the proposed approach. Wang et al. [42] describe another successful approach based on the linear regression which also uses one face example for training. This approach is evaluated on two down-sampled benchmark datasets (AR and FERET) and the results are compared with the other state-of-the-art methods.

Probably the most important task of the face recognition consists in identification of the important face points (landmarks). A number of recent studies is focused on this research topic as shown for instance in the comparative study [9]. These approaches are often evaluated on the Annotated Facial Landmarks in the Wild (AFLW) dataset. The landmarking is recently very popular, because it can be also used in many other tasks as for instance, face coding, face expression, gesture understanding, gaze detection, face tracking, etc.

For further information about the face recognition, please refer to the review [4].

Confidence Measure (CM) is used as a post-processing of the recognition to determine whether a recognition result is correct or not. The incorrectly recognized samples should be removed from the recognition set or another processing (e.g. manual correction) can be further realized. This technique is mainly used in the automatic speech processing field [19,43] and mostly based on the a posterior class probability. However, it can be successfully used in another research areas as shown in [37] for genome maps construction, in [16] for stereo vision or in [31] for handwriting sentence recognition.

Another related confidence measure approach is proposed by Proedrou et al. in the pattern recognition task [36]. The authors use a classifier based on the nearest neighbours. Their confidence measure is based on the algorithmic theory of randomness and on transductive learning.

Unfortunately, only few works about the confidence measure in the face recognition domain exist. Li and Wechsler propose a face recognition system which integrates a confidence measure [24] in order to reject unknown individuals or to detect incorrectly recognized faces. Their confidence measure is, as in the previous case, based on the theory of randomness. The proposed approaches are validated on the FERET database.

Eickeler et al. propose and evaluate in [12] five other CMs also in the face recognition task. They use a pseudo 2-D Hidden Markov Model classifier with features created by the Discrete Cosine Transform (DCT). Three proposed confidence measures are based on a posterior probabilities and two others on ranking of results. Authors experimentally show that a posterior class probability gives better results for the recognition error detection task.

As already shown above, numerous papers presented in the face recognition domain concentrate only on the recognition task itself. In a few cases, the process of face localization (or detection) is considered. Because the faces in the original images are usually sufficiently well aligned, this task is often reduced to define the face position more accurately. Moreover, to the best of our knowledge, relatively few works about whole face recognition systems exist.

One example of a system that addresses the issue of imprecisely localized faces is proposed in [30]. The system compensates the face position and also solves partial occlusion and different facial expressions. Only one training example per person is used.

A complete face recognition system is described in [41]. The training images are well aligned (acquired in controlled conditions) whereas the recognized images are real-world photos. The system is based on the Sparse Representation and Classification (SRC) [18] algorithm. It achieves very good results on the FERET database.

Another face recognition system is presented in [8]. The authors localize the face in the images and then compute the facial features. Their face recognition algorithm is based on the EBGM, but the fiducial points are detected completely automatically. The system is evaluated on the FERET corpus. The authors show that their system has recognition scores comparable to the Elastic Bunch Graph Matching.

For additional information about the face recognition and the face recognition systems, please refer to the survey [49]. The authors also mention some commercial face recognition systems. Unfortunately, neither the system architecture nor the approaches used are usually reported. Moreover, these systems are not evaluated on the standard face datasets and it is thus impossible to compare them with our system.

As already stated, our system will be used by the ČTK for automatic labelling of individuals in new photographs before insertion into the database. Currently annotated pictures contain information about the person’s identity. Unfortunately, the photos contain not only the face itself. They may be composed of more people, some background objects, etc. The faces usually differ in size, pose and angle and are also often partially occluded. Moreover, some labels are erroneous due to annotator errors.

For a correct estimation of face models, a face corpus as good as possible is necessary. We propose an algorithm in order to detect and extract faces from pictures and to create a face corpus automatically. This algorithm is composed of four main steps:
                        
                           1.
                           face detection,

eye detection,

face rotation,

corpus cleaning.

Note that this task is very specific and application dependent. Therefore, to the best of our knowledge there is no similar previous work available.

We use an implementation of the Viola–Jones algorithm [40] which uses a boosted cascade of simple classifiers. This algorithm was chosen because it is one of the most successful face detection methods.


                        Fig. 1
                         shows one example of a correctly and one example of an incorrectly detected face.

Unfortunately, a certain number of incorrectly detected faces occur in the output. Verification of the detected faces is thus indispensable. In order to avoid manual processing, we propose utilizing eye detection for this task. The eyes are detected by the Viola–Jones algorithm (as in the face detection task). Only those face images with two successfully detected eyes are kept while the others are discarded.


                        Fig. 2
                         shows one example of the eye detection task where two, one and no eye was detected by the algorithm. This step eliminates a great number of incorrectly detected faces. Moreover, the successful detection of both eyes ensures we obtain more or less frontal images, while profiles should be removed.

If both eyes are detected successfully, the face is rotated so that the eyes are on a horizontal line according to the detected positions. The face is further placed in the image centre. This transformation cannot completely resolve the issue of variations in pose and tilt. Nevertheless, face variations are significantly reduced.


                        Fig. 3
                         shows the tasks of face and eye detection and face rotation according to the eyes.

We obtained a large number of face images for each individual. However, these numbers differ significantly. In our previous work [23], we proved that more training examples play a crucial role in the correct estimation of face models. Nevertheless, we also showed that this number should be balanced and the quality of faces should be as high as possible.

Therefore, we perform a step called “corpus cleaning” in order to choose the same number of the most representative face images. We manually verified on a randomly chosen small face sub-set that the majority of face images is correct and the erroneous examples differ substantially from this representative set. Our algorithm used to choose only the representative faces is based on this observation.

Let S be the set of all face images extracted by the three steps described previously. Let 
                           
                              
                                 
                                    S
                                 
                                 
                                    i
                                 
                              
                           
                         be the set of n face images 
                           
                              
                                 
                                    I
                                 
                                 
                                    1
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    I
                                 
                                 
                                    n
                                 
                              
                              ∈
                              
                                 
                                    S
                                 
                                 
                                    i
                                 
                              
                           
                         representing one individual. Each face image 
                           
                              
                                 
                                    I
                                 
                                 
                                    j
                                 
                              
                           
                         is represented by the feature vector 
                           
                              
                                 
                                    F
                                 
                                 
                                    j
                                 
                              
                           
                         computed by the SIFT algorithm. The pseudo-code of the proposed algorithm is given below.
                           
                              
                                 
                                 
                                    
                                       
                                          N
                                          =face image number/individual
                                    
                                    
                                       
                                          K
                                          =required face image number/individual
                                    
                                    
                                       
                                          for all 
                                          
                                             
                                                
                                                   
                                                      S
                                                   
                                                   
                                                      i
                                                   
                                                
                                                ∈
                                                S
                                             
                                           
                                          do
                                       
                                    
                                    
                                       
                                          
                                          while 
                                          
                                             
                                                N
                                                >
                                                K
                                             
                                           
                                          do
                                       
                                    
                                    
                                       
                                          
                                          for all 
                                          
                                             
                                                
                                                   
                                                      F
                                                   
                                                   
                                                      j
                                                   
                                                
                                                ∈
                                                F
                                             
                                           
                                          do
                                       
                                    
                                    
                                       
                                          
                                          compute the face model M using the feature set 
                                             
                                                F
                                                ⧹
                                                
                                                   
                                                      F
                                                   
                                                   
                                                      j
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                          compute the similarity 
                                             
                                                
                                                   
                                                      FS
                                                   
                                                   
                                                      j
                                                   
                                                
                                             
                                           between 
                                             
                                                
                                                   
                                                      F
                                                   
                                                   
                                                      j
                                                   
                                                
                                             
                                           and M by the Eq. (2)
                                       
                                    
                                    
                                       
                                          
                                          end for
                                       
                                    
                                    
                                       
                                          compute an average value 
                                             
                                                
                                                   
                                                      av
                                                   
                                                   
                                                      FS
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          compute a standard deviation 
                                             
                                                
                                                   
                                                      sd
                                                   
                                                   
                                                      FS
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          compute a similarity threshold 
                                             
                                                ST
                                                =
                                                
                                                   
                                                      av
                                                   
                                                   
                                                      FS
                                                   
                                                
                                                +
                                                
                                                   
                                                      
                                                         
                                                            sd
                                                         
                                                         
                                                            FS
                                                         
                                                      
                                                   
                                                   
                                                      σ
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                          if 
                                          
                                             
                                                ∃
                                                
                                                   
                                                      I
                                                   
                                                   
                                                      j
                                                   
                                                
                                                :
                                                
                                                   
                                                      FS
                                                   
                                                   
                                                      j
                                                   
                                                
                                                <
                                                ST
                                             
                                           
                                          then
                                       
                                    
                                    
                                       
                                          
                                          
                                          for all 
                                          
                                             
                                                
                                                   
                                                      FS
                                                   
                                                   
                                                      k
                                                   
                                                
                                                <
                                                ST
                                             
                                           
                                          do
                                       
                                    
                                    
                                       
                                          
                                          remove the face image 
                                             
                                                
                                                   
                                                      I
                                                   
                                                   
                                                      k
                                                   
                                                
                                             
                                           from the face corpus
                                    
                                    
                                       
                                          
                                          
                                          
                                             
                                                N
                                                ←
                                                N
                                                -
                                                1
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                          
                                          end for
                                       
                                    
                                    
                                       
                                          
                                          else
                                       
                                    
                                    
                                       
                                          
                                          remove the face image 
                                             
                                                
                                                   
                                                      I
                                                   
                                                   
                                                      min
                                                   
                                                
                                             
                                           with minimal similarity 
                                             
                                                
                                                   
                                                      FS
                                                   
                                                   
                                                      min
                                                   
                                                
                                             
                                           from the face corpus
                                    
                                    
                                       
                                          
                                          
                                          
                                             
                                                N
                                                ←
                                                N
                                                -
                                                1
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                          end if
                                       
                                    
                                    
                                       
                                          
                                          end while
                                       
                                    
                                    
                                       
                                          end for
                                       
                                    
                                 
                              
                           
                        
                     

The similarity 
                           
                              
                                 
                                    FS
                                 
                                 
                                    j
                                 
                              
                           
                         of image 
                           
                              
                                 
                                    I
                                 
                                 
                                    j
                                 
                              
                           
                         to face model M created from the remaining images 
                           
                              I
                              ⧹
                              
                                 
                                    I
                                 
                                 
                                    j
                                 
                              
                           
                         is computed. The average 
                           
                              
                                 
                                    av
                                 
                                 
                                    FS
                                 
                              
                           
                         of the similarity values and consequently the standard deviation 
                           
                              
                                 
                                    sd
                                 
                                 
                                    FS
                                 
                              
                           
                         is computed. The similarity threshold ST is computed using these two values. Note that the optimal value of the constant σ is not possible to compute analytically and is thus obtained by the development corpus. Images 
                           
                              
                                 
                                    I
                                 
                                 
                                    j
                                 
                              
                           
                         with lower similarity 
                           
                              
                                 
                                    FS
                                 
                                 
                                    j
                                 
                              
                           
                         than the threshold ST are discarded. The above described task is realized iteratively. If there is no similarity value lower than the threshold ST given and there are still more images than the required number K, the images with the minimal similarities 
                           
                              
                                 
                                    FS
                                 
                                 
                                    min
                                 
                              
                           
                         are further discarded. Finally the number of images/person is equal to K, which is the required face image number defined at the beginning of the algorithm.

For the face recognition task, we use the SIFT based Kepenekci method [23] which combines the efficient SIFT algorithm with the adapted Kepenekci matching. This algorithm was chosen, because as proven previously, it significantly outperforms the other approaches particularly for lower quality real data. Note that the above mentioned modifications do not influence the applicability of the original Kepenekci method. Moreover, this approach is not limited by the image resolution.

The Scale Invariant Feature Transform algorithm basically has four steps: extrema detection, removal of key-points with low contrast, orientation assignment and descriptor calculation [21].

The Difference of Gaussian (DoG) filter is applied to the input image. The image is gradually down-sampled and the filtering is performed at several scales. Fig. 4
                            demonstrates the process of the creation of DoG filters at different scales [27]. Filtering at several scales ensures scale invariance. Each pixel is then compared with its neighbours. Neighbours on its level as well as on the two neighbouring (lower and higher) levels are examined. If the pixel is the maximum or minimum of all the neighbouring pixels, it is considered to be a potential key-point.

The detected key-points are further examined to choose the “best” candidates. For the resulting set of key-points their stability is determined. Locations with low contrast and unstable locations along edges are discarded.

The orientation of each key-point is computed. The computation is based upon gradient orientations in the neighbourhood of the pixel. The values are weighted by the magnitudes of the gradient.

The final step consists in the creation of descriptors. The computation involves the 
                              
                                 16
                                 ×
                                 16
                              
                            neighbourhood of the pixel. Gradient magnitudes and orientations are computed in each point of the neighbourhood. Their values are weighted by a Gaussian. For each sub-region of size 
                              
                                 4
                                 ×
                                 4
                              
                            (16 regions), orientation histograms are created. Finally, a vector containing 128 (
                              
                                 16
                                 ×
                                 8
                              
                           ) values is created.

The original Kepenekci method achieves very good accuracy. The reported recognition rates on the ORL and FERET datasets are 95.25% and 96.3%, respectively.

In our previous work [23] we modified this approach by replacing the features obtained using Gabor wavelet filters with the SIFT features. The original Kepenekci matching was simplified by removing the “similarity” threshold which excludes some feature vectors from the matching procedure. We proved that this modified approach significantly outperforms the original Kepenekci method from the viewpoint of recognition accuracy particularly on lower quality real data. Moreover, because of the lower number of image representation features, the computation cost of the proposed method is lower than the original one. This fact is very important for practical usage.

This approach combines two methods of matching and uses the weighted sum of the two values as a result as follows.

Let T be a test image and G a gallery image. For each feature vector t of face T we determine a set of relevant vectors g of face G. Vector g is relevant iff:
                              
                                 (1)
                                 
                                    
                                       
                                          
                                             
                                                (
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      t
                                                   
                                                
                                                -
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      g
                                                   
                                                
                                                )
                                             
                                             
                                                2
                                             
                                          
                                          +
                                          
                                             
                                                (
                                                
                                                   
                                                      y
                                                   
                                                   
                                                      t
                                                   
                                                
                                                -
                                                
                                                   
                                                      y
                                                   
                                                   
                                                      g
                                                   
                                                
                                                )
                                             
                                             
                                                2
                                             
                                          
                                       
                                    
                                    <
                                    distanceThreshold
                                 
                              
                           where x and y are the coordinates of the feature vector points.

If no relevant vector to vector t is identified, vector t is excluded from the comparison procedure. The overall similarity of two faces OS is computed as the average of similarities S between each pair of corresponding vectors as:
                              
                                 (2)
                                 
                                    
                                       
                                          OS
                                       
                                       
                                          T
                                          ,
                                          G
                                       
                                    
                                    =
                                    mean
                                    
                                       
                                          
                                             S
                                             (
                                             t
                                             ,
                                             g
                                             )
                                             ,
                                             t
                                             ∈
                                             T
                                             ,
                                             g
                                             ∈
                                             G
                                          
                                       
                                    
                                 
                              
                           Then, the face with the most similar vector to each of the test face vectors is determined. The 
                              
                                 
                                    
                                       C
                                    
                                    
                                       i
                                    
                                 
                              
                            value informs how many times gallery face 
                              
                                 
                                    
                                       G
                                    
                                    
                                       i
                                    
                                 
                              
                            was the closest one to some of the vectors of test face T. The similarity is computed as 
                              
                                 
                                    
                                       C
                                    
                                    
                                       i
                                    
                                 
                                 /
                                 
                                    
                                       N
                                    
                                    
                                       i
                                    
                                 
                              
                            where 
                              
                                 
                                    
                                       N
                                    
                                    
                                       i
                                    
                                 
                              
                            is the total number of feature vectors in 
                              
                                 
                                    
                                       G
                                    
                                    
                                       i
                                    
                                 
                              
                           . The weighted sum of these two similarities FS is used for similarity measure:
                              
                                 (3)
                                 
                                    
                                       
                                          FS
                                       
                                       
                                          T
                                          ,
                                          G
                                       
                                    
                                    =
                                    α
                                    
                                       
                                          OS
                                       
                                       
                                          T
                                          ,
                                          G
                                       
                                    
                                    +
                                    β
                                    
                                       
                                          
                                             
                                                C
                                             
                                             
                                                G
                                             
                                          
                                       
                                       
                                          
                                             
                                                N
                                             
                                             
                                                G
                                             
                                          
                                       
                                    
                                 
                              
                           The face is recognized as follows:
                              
                                 (4)
                                 
                                    
                                       
                                          
                                             
                                                FS
                                             
                                             
                                                T
                                                ,
                                                G
                                             
                                          
                                       
                                       
                                          ̂
                                       
                                    
                                    =
                                    arg
                                    
                                       
                                          
                                             max
                                          
                                          
                                             G
                                          
                                       
                                    
                                    (
                                    
                                       
                                          FS
                                       
                                       
                                          T
                                          ,
                                          G
                                       
                                    
                                    )
                                 
                              
                           Note that the cosine similarity is used for vector comparison.

The Confidence Measure (CM) is used in the last step of the pipeline in order to identify and remove incorrectly recognized faces from the resulting set. As in many other papers [19,13,43], our confidence measure is based on the estimation of the a posterior class probability.

Let the output of the classifier be 
                        
                           P
                           (
                           F
                           |
                           C
                           )
                        
                     , where C is the recognized face class and F represents the face features. The values 
                        
                           P
                           (
                           F
                           |
                           C
                           )
                        
                      are normalized to compute a posterior class probabilities as follows:
                        
                           (5)
                           
                              P
                              (
                              C
                              |
                              F
                              )
                              =
                              
                                 
                                    P
                                    (
                                    F
                                    |
                                    C
                                    )
                                    ·
                                    P
                                    (
                                    C
                                    )
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          I
                                          ∈
                                          FIM
                                       
                                    
                                    P
                                    (
                                    F
                                    |
                                    I
                                    )
                                    .
                                    P
                                    (
                                    I
                                    )
                                 
                              
                           
                        
                     
                     
                        
                           FIM
                        
                      represents the set of all individuals and 
                        
                           P
                           (
                           C
                           )
                        
                      denotes the prior probability of the individual’s (face) class C.

The two confidence measure methods already presented in [22] are used. In the first approach, called absolute confidence value, only faces 
                        
                           
                              
                                 C
                              
                              
                                 ̂
                              
                           
                        
                      complying with
                        
                           (6)
                           
                              
                                 
                                    C
                                 
                                 
                                    ̂
                                 
                              
                              =
                              arg
                              
                                 
                                    
                                       max
                                    
                                    
                                       C
                                    
                                 
                              
                              (
                              P
                              (
                              C
                              |
                              F
                              )
                              )
                           
                        
                     
                     
                        
                           (7)
                           
                              P
                              (
                              
                                 
                                    C
                                 
                                 
                                    ̂
                                 
                              
                              |
                              F
                              )
                              
                              >
                              
                              T
                           
                        
                     are considered as being recognized correctly.

The second approach, called relative confidence value, computes the difference between the best score and the second best one by the following equation:
                        
                           (8)
                           
                              P
                              Δ
                              =
                              P
                              (
                              
                                 
                                    C
                                 
                                 
                                    ̂
                                 
                              
                              |
                              F
                              )
                              -
                              
                                 
                                    
                                       max
                                    
                                    
                                       C
                                       ≠
                                       
                                          
                                             C
                                          
                                          
                                             ̂
                                          
                                       
                                    
                                 
                              
                              (
                              P
                              (
                              C
                              |
                              F
                              )
                              )
                           
                        
                     Only the faces with 
                        
                           P
                           Δ
                           >
                           T
                        
                      are accepted. This approach aims to identify the “dominant” faces among all the other candidates. T is the acceptance threshold and its optimal value is adjusted experimentally.

We further propose another confidence measure approach which uses the scores R obtained by the methods presented previously. We use a Multi-layer Perceptron (MLP) to model a posterior probability 
                           
                              P
                              (
                              H
                              |
                              R
                              )
                           
                        . The variable H has only two values and determines whether the face image was classified correctly or not.

Three MLP configurations are built and evaluated:
                           
                              1.
                              supervised absolute confidence value method,

supervised relative confidence value method,

combination of methods 1 and 2.

The MLP topology will be described in detail in the experimental section. Note, that this confidence measure is proposed in order to improve the scores of the methods described above.

The presented system has (as shown in Fig. 5
                     ) a modular architecture. It is composed of five modules (see the rectangles) connected by dependencies (see the oriented edges). The input image and the recognition results are represented by parallelograms. The storage of the face representation is shown by the Face Gallery sign.

The first module 
                        
                           M
                           1
                        
                      deals with face extraction. This module converts a colour image into its grey-scale representation, then it performs face detection. The detected face is further extracted from the image in the next step. This module also detects the eyes in the detected face region and transforms and resizes the face.

The second module 
                        
                           M
                           2
                        
                      is used to create the face representation. It detects the SIFT key-points and creates a set of SIFT descriptors for a representation of the face image.

The next module 
                        
                           M
                           3
                        
                      is used to select the most representative face vectors in order to create a precise face model M. The algorithm implemented within this module is described in Section 3.4.

The fourth module 
                        
                           M
                           4
                        
                      deals with face recognition. A recognized face is compared to the face models stored in the Face Gallery and the most similar model is chosen as the recognized one.

The last confidence measure module 
                        
                           M
                           5
                        
                      is dedicated to identifying whether the recognition result is correct or not. This unique step is particularly important, because when the user knows that the recognition is probably not correct, he can manually correct the recognition result.

Note that the modules 
                        
                           M
                           1
                        
                      and 
                        
                           M
                           2
                        
                      are used in both face representation (or modelling) and face recognition tasks. However, module 
                        
                           M
                           3
                        
                      is used only for face representation and modules 
                        
                           M
                           4
                        
                      and 
                        
                           M
                           5
                        
                      are used only for recognition. The last remark is that every module should be used separately in order to create another face processing system.

The time complexity of the whole system is 
                        
                           O
                           (
                           
                              
                                 N
                              
                              
                                 3
                              
                           
                           )
                        
                      for training and 
                        
                           O
                           (
                           
                              
                                 N
                              
                              
                                 2
                              
                           
                           )
                        
                      for recognition. The first value is the time complexity of the module 
                        
                           M
                           3
                        
                     , i.e. module with the maximal complexity over all modules in the training task. The second value is the time complexity of the modules 
                        
                           M
                           2
                        
                      and 
                        
                           M
                           4
                        
                     , i.e. also the modules with the maximal complexity over all modules in the recognition pipeline.

@&#EXPERIMENTS@&#

In our previous work [23], we evaluated and compared the recognition results of our proposed approach on the ORL dataset and on a sub-set of the ČTK corpus of comparable size. The number of recognized individuals for the ORL and ČTK datasets was 40 and 37, respectively. We used 9 face examples for training and the remaining one for testing. The achieved recognition rate was 100% on the ORL dataset and 72.7% on the ČTK dataset. Both these scores outperform the original Kepenekci method.

We would like to extend our previous work and evaluate our approach on huge and real corpora. In the first experiment, we compare our approach with the other most efficient methods on a large sub-set of the FERET corpus. The second and third experiments show the performance of our approach on two other large standard corpora (AR Face Database and Labeled Faces in the Wild) composed of the significantly varying face images. The remaining experiments show the results of our approach on a real ČTK dataset. These experiments are motivated by the need to evaluate our face recognition system under real conditions. It is worth mentioning that the image resolutions for each experiment differ.

The FERET dataset [33] contains 14,051 images of 1199 individuals. The images were collected between December 1993 and August 1996. The resolution of the images is 
                           
                              256
                              ×
                              384
                           
                        
                        pixels. The images are divided into the following categories according to the face pose: frontal, quarter-left, quarter-right, half-left, half-right, full-left and full-right, and they are stored in the .tiff format. The images are also grouped into several probe sets. The main probe sets of the frontal images are summarized in Table 1
                        .

There are usually only a few seconds between the capture of the gallery-probe pairs in the f* sets. The individuals in the fb set differ in facial expressions, while the images in the fc set differ in illumination conditions. The images in the dup1 probe set were obtained over a three year period and the dup2 set is a sub-set of the dup1. Note that only one image per person/set is available. Fig. 6
                         shows three example images from the FERET database.

Another face dataset, called the AR Face Database
                           4
                           
                              http://www2.ece.ohio-state.edu/aleix/ARdatabase.html.
                        
                        
                           4
                         
                        [29], was created at the Univerzitat Autonòma de Barcelona. This database contains more than 4000 colour images of 126 individuals. The images are stored in a raw format and their size is 
                           
                              768
                              ×
                              576
                           
                        
                        pixels. The individuals are captured under significantly different lighting conditions and with varying expressions. Another characteristic is a possible presence of glasses or scarf. Fig. 7
                         shows four example images from this dataset.

The Labeled Faces in the Wild (LFW)
                           5
                           
                              http://vis-www.cs.umass.edu/lfw/.
                        
                        
                           5
                         
                        [17] is a new dataset created in 2007 in order to allow testing of face recognition methods in unconstrained environment. It contains more than 13,000 images of 5749 people in a resolution 
                           
                              250
                              ×
                              250
                           
                        
                        pixels which were collected from the web. The images are stored in a jpg format. 1680 people have at least two distinct images and they are used usually for face recognition experiments. The face images were detected by the Viola–Jones algorithm [40].

The database contains four different sub-sets for testing. The first one is an original dataset. The other three sets are composed of images obtained using different methods of alignment. The proposed testing protocol concentrates mainly on the face verification task. It means to decide whether two images belong to the same person or not. Fig. 8
                         shows three example images from the LFW face database.

This corpus was created as a result of the proposed corpus creation algorithm presented in Section 3. It is composed of images of individuals in an uncontrolled environment that were randomly selected from the large ČTK database of high resolution photographs. All images were taken over a long time period (20years or more). The resulting corpus contains gray-scale images of 638 individuals of size 
                           
                              128
                              ×
                              128
                           
                        
                        pixels. Two versions of this corpus are used in following experiments. The first one has at least 10 images/person and represents the output of the proposed algorithm without the corpus cleaning step, while the second version (cleaned) contains at most 10 images for each person. Note that the orientation, lighting conditions and background of the images differ significantly. A correct face recognition on this dataset is thus very difficult.


                        Fig. 9
                         shows examples of one face from this corpus. This corpus is available for free for research purposes at http://home.zcu.cz/∼pkral/sw/ or upon request to the authors.


                        Table 2
                         compares and evaluates our approach with the other very efficient methods on a large sub-set of the FERET corpus. The fa set is used for training, while the fb set is for testing.

The first reported method is an implementation of the EBGM algorithm by Bolme [6]. The second algorithm is proposed by Ahonen in [1]. This approach is based on local binary patterns. Wagner et al. propose in [41] another efficient approach, based on the Sparse Representation and Classification (SRC) algorithm, whose score is reported next. The results of the novel approach based on linear regression proposed by Naseem et al. [32] are shown in the following line of this table. Recognition accuracy of the novel Discriminative Multi-Manifold Analysis (DMMA) method (Lu) [28] is presented next. The sixth method is introduced by Kepenekci in [20] and is based on the Gabor wavelets. The last approach is our implementation of the SIFT based Kepenekci method with the adapted Kepenekci matching.

The table shows that the recognition rates of all approaches are very good and close to one another. This experiment also shows that our SIFT based Kepenekci method slightly outperforms the other approaches. Regarding this fact and our previous experiments we decided to integrate this method into our AFR system.

The following experiments are undertaken in order to show the performance of this method on real data. We would also like to determine the optimal parameters of this approach for our task.


                        Table 3
                         compares and evaluates our approach with the other very efficient methods on the AR face corpus composed of the images recorded in significantly varying conditions. This experiment was realized in order to show the performance of our method on another corpus recorded in relatively real conditions.

Unfortunately, the other studies often use different experimental set-ups on this database. Subsets with less individuals are usually used to evaluate the proposed methods. Therefore, we report also the number of the recognized individuals (subj.) of the other studies. In order to show the superior recognition abilities of the proposed SIFT based Kepenekci approach, we recognize 120 individuals.

The recognition rates of the first four reported methods presented by Wright et al. in [44] show that the SVM approach is the best performing one on this dataset. The results of the second three methods are chosen from the comparative study of Yeh et al. [46]. The authors show that the LBP method outperforms significantly two other methods, the SVM and the SRC on the AR corpus. The following two methods proposed by Geng and Jiang [14] are based on the SIFT features. The last line of this table shows the results of our proposed SIFT based Kepenekci approach. This table reports that our proposed method gives better recognition accuracy than all previous approaches.

As already stated, this face corpus is used for face verification. Therefore, it is not possible to compare our proposed approach directly with the other ones. We implemented three best performing methods described in the literature. These approaches are used to recognize 1680 different people from this corpus. Then, we use our proposed approach with the similar experimental set-up.


                        Table 4
                         reports the recognition results of this experiment. This table shows that the proposed method outperforms significantly the other approaches. The recognition rate improvement is 1.79% in the absolute value.

The images in the ČTK corpus differ significantly from the images in the FERET database. Therefore, an optimization of our method for this corpus is beneficial.

The most important parameter is the distance threshold (see Eq. (1)) which significantly influences recognition accuracy.

This threshold defines the surrounding region to a given key-point. Only the vectors in this region are used for comparison in the recognition step. The optimal value of this threshold is strictly related to the size and quality of the recognized images. Unfortunately, it is not possible to analytically set its optimal value. Therefore, this value will be set experimentally on a development corpus as demonstrated in Fig. 10
                        . The evaluated values are in the interval 
                           
                              ]
                              0
                              ;
                              50
                              ]
                           
                        . However, for better clarity of this figure, the reported interval is reduced by 10 from both sides.

The highest obtained recognition rate is 61.91% when the threshold value is set at 35. Therefore, this value will be used in the following experiments.

The first experiment demonstrates the performance of the proposed corpus creation algorithm presented in Section 3 on the ČTK corpus with a particular focus on corpus cleaning. The recognition rates in two cases, without and with the corpus cleaning step, are compared.

From the viewpoint of the application of our system, we would like to identify whether a correctly recognized face belongs to the N best recognized ones. If this is the case, we can propose to a user choosing the correctly recognized face, for example, from a drop-down list.

Therefore, three different evaluation metrics are described next. The Correct metric is a classical recognition rate computed as 
                           
                              
                                 
                                    correct
                                 
                                 
                                    all
                                 
                              
                           
                         where correct represents the number of correctly recognized faces and all is the number of all recognized faces. The second metric, called Correct in 5 most similar, considers a correct recognition result when a correct face belongs to the set of the five most similar faces. The last metric is similar to the previous one, while a correct result is considered when a correct face belongs to the set of the ten most similar (best) recognized faces.

The first section of Table 5
                         compares the recognition rates on the ČTK corpus without and with the corpus cleaning step, i.e. on the raw and on the cleaned version of the ČTK corpus. The table shows that corpus cleaning is beneficial for the correct estimation of the face models. The improvement in recognition accuracy is greater than 30% for all three metrics. This table also shows that the error rate of the Correct metric is about 38% and that about 9% and 13% of errors belong to the 5 and 10 best recognized faces, respectively.

Note that we assume the use of the confidence measures in order to automatically detect incorrectly recognized faces. In these cases, users should manually correct these incorrectly recognized examples.

In the next experiment, we compare the performance of the SIFT-based Kepenekci method with the original Kepenekci approach. The results of the original Kepenekci approach are shown in the second section of Table 5. This table clearly shows that the SIFT-based Kepenekci method significantly outperforms the original Kepenekci approach. The error rate reduction is about 26% in relative value when corpus cleaning is used (i.e. on the cleaned corpus).

As in many other articles in the confidence measure field [26], we will use the Receiver Operating Characteristic (ROC) curve [7] to evaluate our methods. This curve clearly shows the relationship between the True Positive (TP) and the False Positive (FP) rate for different values of the acceptance threshold.


                        Fig. 11
                         shows the results of the absolute confidence value method, while the results of the relative confidence value approach are given in Fig. 12
                        . These figures show that both confidence measures are suitable for our task in order to identify incorrectly recognized faces. Moreover, the relative confidence value method significantly outperforms the absolute confidence value approach.

In the last part of this experiment, we would like to evaluate the results of the proposed supervised confidence measure methods which use an MLP. The best MLP topology uses three layers (in all cases): one or two input neurons, 10 neurons in the hidden layer and two outputs (correctly and incorrectly recognized face). One input is used in the case when the CM methods are used separately and two input neurons are used when we combine both confidence measures. The MLP topology was found empirically on a small development corpus which contains 120 examples (i.e. 120 confidence values).


                        Table 6
                         shows the results of these CMs. As in many other works, F-measure (F-mes) [35] is used as an evaluation metric. Because of the importance of evaluating the quality of the classification as well as the identification, Precision (Prec) and Recall (Rec) are also reported in this table. This table clearly shows that the second supervised relative confidence value method significantly outperforms the first supervised absolute confidence value approach. However, the first method brings further relevant information regarding the second approach. This fact is confirmed by the result of the third combined approach, which gives the best recognition score from the whole.

The main scientific contribution of this work can be summarized below:
                        
                           •
                           We proposed and implemented a new face corpus creation algorithm. This algorithm allows to create a face corpus from annotated pictures which may be composed of more people, some background objects, etc. The algorithm was evaluated on the lower quality real ČTK data.

We created a new facial corpus from the ČTK data designed particularly for the evaluation of automatic face recognition in real conditions. This corpus is available for research purposes for free at http://home.zcu.cz/∼pkral/sw/ or upon request to the authors.

We evaluated a novel face recognition method, called the SIFT based Kepenekci approach, which is based on the SIFT features and adapted Kepenekci matching on four large corpora: FERET, AR, LFW and ČTK. This algorithm significantly outperforms the other approaches particularly on the lower quality real data.

We evaluated two confidence measure techniques based on a posterior class probability. Then, we proposed and evaluated two supervised confidence measures which use an MLP as a classifier. These CMs extend both previously proposed ones.

We have proposed, implemented and evaluated an experimental fully automatic face recognition system. This system slightly outperforms the other efficient approaches from the viewpoint of face recognition accuracy and it is also sufficiently robust on lower quality real data.

The last and most important contribution is that we have made this system (with the source code) publicly available for research purposes for free at http://home.zcu.cz/∼pkral/sw/ or upon request to the authors.

@&#CONCLUSIONS AND FUTURE WORK@&#

In this paper, we proposed an experimental fully automatic face recognition system for the Czech News Agency. The system is used for the labelling of people in photographs during insertion into the ČTK database.

We compared our SIFT based Kepenekci face recognition approach with a number of the most efficient AFR methods on three large corpora: FERET, AR and LFW. We showed that this approach outperforms all other methods especially on the lower quality real data. Therefore, we decided to integrate this method into our face recognition system.

Then, we proposed a corpus creation algorithm in order to extract the faces from the database and to create a facial corpus. We showed that it is necessary to use this algorithm in our AFR system in the preprocessing step in order to create sufficiently precise face models. We also compared the performance of our SIFT based Kepenekci face recognition approach with the original Kepenekci method on the created ČTK corpus. The experiments show that our approach significantly outperforms the original one and that the error rate reduction is about 26% in a relative value.

The last goal of this paper consisted in proposing and evaluating confidence measure methods in order to identify the incorrectly recognized examples and remove them from the recognition results. Two novel supervised confidence measures based on the a posterior class probability and a multi-layer perceptron are proposed. We experimentally proved that both proposed CMs are suitable for our task and that they bring complementary information for the detection of incorrectly recognized examples. Therefore, the best F-measure is obtained by the combined MLP approach.

We envisage two perspectives for our further research in the near future. The recognition rate of our face recognition system on the large FERET dataset is about 97%. The recognition score on the real ČTK data is quite good, however, it is still far to be perfect. We assume that our face recognition module is accurate enough whereas the detection and extraction module needs significant improvement. Therefore, we would like to propose a more suitable face detection and extraction approach. For example, the simple rotation according to the eyes will be replaced by more sophisticated image transformations.

Another perspective consists in proposing better confidence measures in the post-processing step. The novel confidence measures will be based on the main properties of the face model. We further assume combining them with the CMs proposed in this paper.

@&#ACKNOWLEDGEMENTS@&#

This work was partly supported by the UWB Grant SGS-2013-029 Advanced Computer and Information Systems and by the European Regional Development Fund (ERDF), Project “NTIS – New Technologies for Information Society”, European Centre of Excellence, CZ.1.05/1.1.00/02.0090. We would also like to thank the Czech News Agency for their support and for providing the photo data.

@&#REFERENCES@&#

