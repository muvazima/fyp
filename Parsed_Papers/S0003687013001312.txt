@&#MAIN-TITLE@&#An investigation of training strategies to improve alarm reactions

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Training alarm responders to analyze data can supplement alarm design improvements.


                        
                        
                           
                           Alarm responders benefited most from single sensor or spatial pattern training.


                        
                        
                           
                           Alarm responders did not benefit from temporal interval training.


                        
                        
                           
                           Alarm responders decided how to react before experiencing individual signals.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Alarms

Training

Reliability

@&#ABSTRACT@&#


               
               
                  Researchers have suggested that operator training may improve operator reactions; however, researchers have not documented this for alarm reactions. The goal of this research was to train participants to react to alarms using sensor activity patterns. In Experiment 1, 80 undergraduates monitored a simulated security screen while completing a primary word search task. They received spatial, temporal, single sensor, or no training to respond to alarms of differing reliability levels. Analyses revealed more appropriate and quicker reactions when participants were trained and when the alarms were reliable. In Experiment 2, 56 participants practiced time estimation by simple repetition, performance feedback, or performance feedback and temporal subdivision. They then reacted to alarms based on elapsed time between sensor activity and alarm onset. Surprisingly, results indicated that participants did not benefit differentially from temporal interval training, focusing instead on advertised system reliability. Researchers should replicate these findings with realistic tasks and real-world complex task operators.
               
            

@&#INTRODUCTION@&#

As technology has become more complex and capable, task operators have increasingly relied on sensor-driven signals to inform them about dangerous conditions and event. Researchers investigating the link between the reliability of automated signaling systems and operator reactions have drawn several conclusions. Chief among these is the idea that signaling systems producing false alarms inspire lack of trust (termed the “Cry-Wolf Effect”) and that subsequent reactions to such systems may become slower or less frequent (Breznitz, 1984). Initial work by researchers such as Janis (1962) and Breznitz (1984) documented well the degradation of reaction behaviors. Subsequently, other researchers have focused on the characteristics of personal reactions to unreliable alarms, the variability of alarm reaction behavior associated with competing tasks and personal motivators, and the influence of information availability and performance consequences on behaviors. Such investigations have demonstrated that alarm reaction behavior is a complex phenomenon that is inextricably tied to the target task, signaling equipment, and operator cognition.

To broaden the understanding of human reactions to unreliable signals, researchers have relied on existing theories of learning and human cognition. These have included probability matching, where observed alarm responses approximate the perceived true alarm rate (Bliss et al., 1995), Signal Detection Theory, where signal detection and response is more rapid for historically reliable alarm systems (Getty et al., 1995), and automation trust, where exhibited trust corresponds to behavioral patterns of system use and misuse (Parasuraman and Riley, 1997). A clear conclusion is that, regardless of specific task domain, alarm reaction patters are undeniably influenced by perceived alarm system reliability. Specific investigations have shown application to domains as diverse as aviation (Pritchett, 2001), surgical theaters (Xiao and Seagull, 1999) and security monitoring (Marra and Playford, 2009). Because of the implications of low alarm reliability, designers and researchers have endeavored to remedy the problem. Designers of signaling systems have focused their efforts on sensor and display technologies such as likelihood alarm displays (Sorkin et al., 1988). This approach to signal design may be used to embed information about anticipated alarm validity within the signal itself. Bustamante (2008) showed the power of this approach. However, the benefits of likelihood alarm displays are evident only after signals have occurred; furthermore, individual operators frequently exhibit variability in responding, even when signal urgency and reliability are unequivocally high (Bliss, 2003). In addition, designed likelihood alarm displays may not be flexible enough to retain effectiveness across operational environments and task situations. The current work is complementary, targeting behavioral change strategies to insulate task operators from signal unreliability.

One of the most common strategies advocated for task performers to manage signal unreliability is responding to every signal, regardless of its perceived validity. Such a strategy is common in high-consequence task environments such as medicine, where failure to acknowledge a valid signal may outweigh the impact of responding to false alarms (Xiao et al., 2004). One issue, however, is that task operators are forced to allocate their attention to stimuli that may not be consequential. This imparts inefficiency in response behavior and may actually lengthen response times in cases where multiple signals occur simultaneously. A second problem is that over time, operators may react more slowly to all signals, assuming that a certain proportion of them will be false (Getty et al., 1995). Implications such as these illustrate the need for a more discriminative training solution, especially in task environments where false alarms are frequent such as medical care or security monitoring (Xiao et al., 2004; Marra and Playford, 2009).

An alternative approach to training is to train operators to recognize and anticipate trends in the underlying data that drive signal annunciation. Such an approach capitalizes on the tendency of operators to rely on redundant sources of information to judge signals (Bliss, 2003). Our first experiment was intended to provide empirical support for the idea that data pattern training could improve operator reactions.

Our approach involved presenting participants with a simulated security monitoring task where periodic signals warned of motion in certain building rooms. Participants were advised about the signaling system's reliability before participating and received training to recognize spatial or temporal sensor signal patterns or focus on one sensor. We hypothesized that participants would respond most to high-reliability alarms, regardless of training (Bliss et al., 1995). We also anticipated that participants receiving single-sensor training would respond most often and most appropriately because of that method's simplicity. Researchers have not directly compared spatial and temporal training effectiveness; therefore, we made no hypotheses about the relative benefit of these training conditions.

For Experiment 1 we employed a 2 × 4 split-plot design. The within subjects variable was the stated reliability of the alarm system and had two levels: 20% and 40% true alarms. These levels were chosen to reflect the low reliability rates associated with security alarm systems (Sampson, 2002). The between subjects variable was alarm reaction training type and had four levels: single sensor training, spatial pattern sensor training, temporal pattern sensor training, and no training (control).

Performance based dependent measures for the signaling task included reaction time to the sensors (time taken to click on the acknowledge button after a sensor activated), “respond” and “ignore” rates for the alarm signals (proportion alarms to which participants clicked on the “respond” or “ignore” buttons) and reaction time for alarm reaction behaviors (“respond” or “ignore). A running alarm score reflected the number of times participants correctly responded to and ignored alarms. If a correct choice was made (responding to a true alarm or ignoring a false alarm), 1.5 points were added to the score; the same number was deducted from the score after each incorrect choice (ignoring a true alarm or responding to a false alarm). The score was presented visually to the participants during the task. Correct choices were accompanied auditorily with a female voice saying “Correct.” Incorrect choices were accompanied by “Incorrect.” These components facilitated participant decision-making during the task.

Subjective trust ratings for the alarm systems were also recorded, using Jian, Bisantz, and Drury's subjective trust questionnaire. Word search primary task performance was also recorded as the number of words identified during each experimental session. Word search was required as a loading task to more realistically simulate dual-task signal monitoring conditions. No statistically significant differences were noted for this variable, indicating similar primary task loading across groups and conditions.

Eighty undergraduate students enrolled in psychology courses at Old Dominion University in Norfolk, Virginia, were tested. The mean age of the sample was 23.3 (SD = 2.25). The sample consisted of 17 males and 63 females. No participant indicated hearing loss or color deficiency. Participants were awarded course credit for participating in the 2-h experiment. A $20 performance incentive was also offered to the participant with the highest combined primary and secondary task score.

The alarm task was modeled after a building security-monitoring scenario (see Fig. 1
                           ). Participants viewed a building schematic that contained motion sensors in selected rooms. Participants acknowledged every sensor signal (red light in a room and a 1000-Hz. tone) by clicking on the “ACK” icon. An alarm signal followed five prior sensor signal activations. The alarm signal was a red area containing the word “INTRUDER!” at the bottom of the screen and a fire bell from a Boeing 757. Participants could react to an alarm by selecting between two icons, “RESPOND” (true alarm) or “IGNORE” (false alarm). As noted above, correct reactions depended on prior sensor activations.

Referring to Fig. 1 below, single sensor training involved focusing on a single sensor (highlighted in red) to determine the validity of the subsequent alarm. Participants in this group were told that if the sensor activated at any time before the subsequent alarm signal, the alarm would be true. Spatial pattern training required detection of a clockwise sensor activation pattern to judge subsequent alarm validity. Participants were told to note the spatial pattern of sensors that activated prior to an alarm signal. If the pattern was clockwise, this indicated that the alarm signal would be true. Temporal pattern training required detection of a short sensor–alarm time interval to judge alarm validity. Participants in this group were told to judge whether the time interval between sensor activations was becoming longer or shorter. If it was becoming successively shorter, this indicated that the subsequent alarm signal was true. Participants in the no-training control group were not provided any information about the sensors. They were told to judge alarm validity based on the reliability information provided before the session began. Importantly, the actual validity of individual alarms and overall reliability of the alarm system was maintained within sessions; true and false alarms were presented to match the sensor activation behavior for each group and reflected the stated reliability of the alarm system (20% or 40%). Therefore, following training should have led to appropriate reactions in all cases.

The sensor activations and alarm activations were the same for every group; the only difference was the training participants received. Control participants received no training to help them discriminate true from false alarms. In all cases, the correct reaction was to respond to true alarms and ignore false alarms. Two word searches from http://www.puzzle-club.com were used as a primary task during each experimental session. No word bank was available.

Participants completed an 11-item human-computer trust questionnaire (Jian et al., 2000). The survey demonstrated high internal consistency, α
                           Cronbach = .95.

@&#PROCEDURE@&#

The precise schedule of activity is included in Table 1
                           . Upon arrival participants received and signed an informed consent form and filled out a Background Questionnaire. Prior to the experimental sessions participants were randomly assigned to a training group (single sensor, spatial, temporal, or no training) and were provided instructions for the word search and alarm reaction tasks. After a general overview of the experimental procedure participants received specific sensor reaction training based on their training group. All participants then completed a short quiz to demonstrate their understanding of sensor reactions. Participants were required to score a 70% on the quiz to proceed to the experimental session. If the participant was unable to reach this cutoff then they were retrained until they understood the task.

Participants then completed two experimental sessions, interacting with a 20% reliable alarm system and a 40% reliable alarm system (counterbalanced). Prior to each session participants were told the reliability of the signaling system. They used reliability and their training to help them decide which alarms to respond to and which to ignore. Participants completed a word search task while reacting to sensors and alarms. Upon receiving alarms, participants could respond to send a simulated security team to investigate a building security breach. Responding or ignoring carried simulated consequences of misdirected resources or the theft of valuables; actual consequences included losing points toward their accumulated scores, presented at the bottom of the screen. During each session there were five sensor activations followed by an alarm; this sequence repeated five times (total of 25 sensor activations and 5 alarms during each session). Alarm validity matched the stated reliabilities; the 20% reliability session included one true alarm and four false alarms; the 40% reliability session included two true alarms and three false alarms. Following each experimental session participants completed the human-computer trust questionnaire. After completing an experimental session for each reliability level, participants were debriefed and dismissed.

The data were inspected to ensure normality and that each group had equal numbers and no outliers. Multiple 2 (reliability) × 4 (training) split-plot Analyses of Variance (ANOVAs) were used to identify differences. An alpha level of p < .05 was established to indicate statistical significance. An ANOVA revealed no significant main effects or interactions for primary task performance (percentage of word search items found per session) (p > .05).

An ANOVA revealed a main effect of reliability on sensor reaction time, F(1, 76) = 8.80, p = .004, partial η
                        
                           2
                         = .104. However, a significant interaction between reliability and training type was observed, F(3, 76) = 2.89, p = .041, partial η
                        
                           2
                         = .102, which indicated a more pronounced decrease in sensor reaction time for the control and temporal groups than for the single sensor and the spatial pattern groups. No main effect of training type on sensor reaction time was observed, F(3, 76) = 1.06, p > .05 (See Fig. 2
                        ).

An ANOVA showed no significant interaction of reliability and training type on alarm reaction time, F(3, 76) = 1.99, p > .05. There were also no observable main effects for reliability, F(1, 76) = .73, p > .05, or training group, F(3, 76) = .90, p > .05, on alarm reaction time.

There was a significant main effect of training on alarm response rate, F(3, 76) = 3.11, p = .031, partial η
                        
                           2
                         = .109. Follow-up Tukey tests did not indicate significant differences among the training groups. However, participants in the spatial pattern training group responded less often than those in the control group (p = .071) (see Fig. 2). There was no observable main effect of reliability on alarm response rate, F(1, 76) = 1.37, p > .05. There was also no interaction between reliability and training group, F(3, 76) = 1.58, p > .05.

An ANOVA revealed a significant main effect of training on alarm score, F(3, 76) = 7.87, p < .001, partial η
                        
                           2
                         = .237. Follow-up Tukey tests indicated that participants in the single sensor and spatial training groups reacted more appropriately than those in the control group (p = .003 and .001, respectively, see Fig. 2). No main effect of reliability was observed for alarm score, F(1, 76) = .35, p > .05. There was also no interaction between reliability and training group, F(3, 76) = .61, p > .05.

A final ANOVA revealed a significant main effect of training on subjective trust in the alarm system, F(3, 76) = 5.42, p = .002, partial η
                        
                           2
                         = .176 (see Fig. 2). Follow-up Tukey tests showed that participants without training indicated significantly less trust than those in the spatial pattern training group (p = .002). Participants in the single sensor training group also indicated less trust in the alarm system than those with spatial pattern training (p = .032). A main effect of subjective trust in the alarm system approaching significance was observed for reliability, F(1, 76) = 2.96, p = .090, partial η
                        
                           2
                         = .037, where participants in the 20% reliability group (M = 48.15, SE = 1.77) trusted the alarm system less than the 40% reliability group (M = 51.23, SE = 1.71). There was no interaction between reliability and training group for subjective trust in the alarm system, F(3, 76) = .44, p > .05.

The results suggest that alarm score and subject trust were particularly sensitive metrics of training success. Participants in the spatial training group scored higher and manifested greater trust than control participants. Those in the single sensor group outscored control participants but did not trust the alarm system as much as those in the spatial group. In addition to confirming the success of our training materials, the results validated the measurement of performance by way of a derived score.

We expected faster reaction times to reliable sensors based on past research (Getty et al., 1995); however, our findings illustrated difficulty of estimating temporal intervals and of performing with no training. Indeed, those groups reduced their sensor reaction times considerably, suggesting they may have learned most over the course of the experiment. The challenges faced by these participants may have caused control and temporal group participants to rely more on stated historical reliability levels during the experiment.

Whereas sensor reaction time proved sensitive to reliability, alarm reaction time was surprisingly less sensitive. Given the complexity of the tasks required, participants may have made reaction decisions prior to the experimental session. This tendency has been discussed in prior research (Bliss et al., 1995).

From prior research, we had expected response rate to be a sensitive index of trust. However, the group differences we observed were not significant. It is admittedly difficult to interpret these findings because participants were trained to react appropriately, not necessarily to respond more often.

Perhaps most compelling was the relative difficulty of temporal training. In real-world task situations, it is frequently important for task operators to judge the time interval between pre-alarm signals (“attensons”; see Patterson, 1980) and subsequent alarms. For this reason, we chose to focus our effort on temporal training in the second experiment, reported below.

An important conclusion from Experiment 1 was the challenge faced by participants in the temporal training group. As in real-world process monitoring (Papadopoulos and McDermid, 2001), participants were asked to infer alarm validity based on the elapsed time since a preceding sensor signal. Compounding the difficulty was attending to a continuous, primary task. As noted by Vercruyssen et al. (1989), doing so is critical for many occupations, including emergency responders.

Time estimation is an important skill underlying signal responses. Accident databases are rife with situations in which task operators failed to note lapses in signal occurrence, or failed to recognize that signals were occurring more frequently over time. One poignant example is the 1995 grounding of a Panamanian cruise ship, the Royal Majesty, after the automated navigation system malfunctioned and the crew failed to notice absence of alerts across time (National Transportation Safety Board, 1997).

Training time estimation has been attempted by other researchers, notably Brown (2008). Brown showed that performance feedback could improve time interval estimation skills in a dual-task paradigm. To facilitate interval recognition, music teachers often employ the Takadimi method, using beat subdivision to illustrate and reinforce tempo maintenance (Houlahan and Tacka, 2008).

The results of Experiment 1 demonstrated that training could lead participants to recognize patterns in underlying data. However, participants struggled to infer alarm validity from temporal sensor–alarm intervals. For the second experiment, we planned a follow-on investigation to compare methods for training temporal interval estimation skills. Using a similar research paradigm as in Experiment 1, we compared subdivision training and feedback training to a no-training control group. We expected that trained participants would react most appropriately to true and false alarms. We also expected participants with additional subdivision training to react more appropriately to alarms than those trained using feedback alone. Last, we expected that participants would react more appropriately to signals from a high-reliability system.

For Experiment 2 we employed a 2 × 3 split-plot design. Similar to Experiment 1, the within subjects variable was the stated reliability of the alarm system: 20% reliable and 40% reliable. The between-subjects variable was temporal interval training method, with three levels. In training with feedback, participants received feedback about the accuracy of interval approximation during training. In training with feedback and subdivision, participants received feedback and an auditory subdivision aid every 5 s during training. In the control condition, participants received no feedback and no subdivision aid.

The dependent measures for the alarm task were the same as those measured in Experiment 1. Temporal deviation scores (secs above or below) were also recorded before and after the pre-alarm task temporal training session in which 30, 60, 75, and 90-s intervals were estimated.

We tested fifty-six undergraduate students (21 males, 35 females) enrolled in psychology courses at Old Dominion University in Norfolk, Virginia. The mean age of the sample was 22.56 (SD = 4.72). Participants were awarded course credit for participating. A $20 performance incentive was also offered to the participant with the highest primary and secondary (alarm score) task score.

The alarm task was similar to Experiment 1, except for the number of sensors and alarms presented and the pattern in which they occurred. Before interacting with the alarm task, participants underwent extensive temporal training. Participants were asked to estimate random 30, 45, 60, 75, and 90-s intervals by pressing a mouse key after the set time had elapsed. Participants with feedback training received an indication of their estimate accuracy (above or below) in seconds. Participants with feedback and temporal subdivision aid received a 1500 hz. tone every 5 s in addition to feedback. Participants in the control group completed training sessions without feedback or subdivision. To simulate realistic monitoring and primary task loading, participants completed word search puzzles as the primary task, retrieved from http://www.puzzle-club.com. As in Experiment 1, there were no statistically significant differences in primary task performance between groups or conditions. Participants completed the subjective trust questionnaire as in Experiment 1.

@&#PROCEDURE@&#

The precise schedule of activity is indicated in Table 2
                           . Upon arrival participants provided informed consent and completed the Background Questionnaire. Participants were then randomly assigned to a training group and were given instructions for the word search and alarm reaction tasks. After a general overview of the experimental procedure participants received specific sensor reaction training based on their training group. All participants completed a pretest of time estimation, five training sessions, and a posttest of time estimation accuracy. The training tasks lasted 1 h.

Participants next completed two experimental sessions, interacting with counterbalanced 20% reliable and 40% reliable alarm systems. Participants knew the reliabilities prior to participating. The alarm response task, including sensor acknowledgments, alarm reactions, and simulated and score consequences, were identical to Experiment 1 (see Fig. 1). In addition to the stated reliability, participants were also told a sensor–alarm time interval of less than 60 s indicated a true alarm; one of more than 60 s indicated a false alarm. Participants were to react to true alarms by selecting the “RESPOND” icon, and to react to false alarms by selecting “IGNORE.” There were 10 sensor activations, each followed by an alarm. The ten alarms in each session were matched to the stated reliability (two of the ten were true in the 20% reliability condition; four of the ten were true in the 40% reliability condition). Participants were required to complete a word search task while reacting to alarms. Following each experimental session participants completed the human-computer trust questionnaire. They were then debriefed and dismissed.

The data were inspected to ensure that each group had approximately equal numbers and that data were distributed normally. We conducted multiple 2 (reliability) × 3 (training) split-plot ANOVAs to identify differences among dependent measures. An ANOVA showed no main effects or interaction for primary task performance (p > .05). There were no observed differences among the training groups for the pre- and post-time estimation accuracy rates during training (p > .05).

An ANOVA revealed a main effect of reliability on sensor reaction time, F(1, 53) = 4.29, p = .043, partial η
                        
                           2
                         = .075, where the 20% reliability group (M = 2.70 s, SE = .09) reacted quicker to sensor activations than the 40% reliability group (M = 4.00 s, SE = .61). No main effect of training type on sensor reaction time was observed, F(2, 53) = .55, p > .05, or interaction, F(2, 53) = .46, p > .05 (see Fig. 3
                        ).

An ANOVA did not reveal a significant interaction of reliability and training type on alarm reaction time, F(2, 53) = .36, p > .05. There were also no observable main effects for reliability, F(1, 53) = 2.35, p > .05, or training group, F(2, 53) = 1.99, p > .05, on alarm reaction time.

There was a significant main effect of reliability on alarm response rate, F(1, 53) = 32.47, p < .001, partial η
                        
                           2
                         = .380, where the 20% reliability group (M = .31%, SE = .02) responded less frequently to alarms than the 40% reliability group (M = .47%, SE = .03). There was no observable main effect of training type on alarm response rate, F(2, 53) = .03, p > .05. There was also no interaction between reliability and training group for alarm response rate, F(2, 53) = .42, p > .05 (see Fig. 3).

A main effect of reliability on alarm response appropriateness approached statistical significance, F(1, 53) = 3.40, p = .071, partial η
                        
                           2
                         = .060, where the 20% reliability group (M = .76%, SE = .02) responded more appropriately to alarms than the 40% reliability group (M = .71%, SE = .02). There was no observable main effect of training type on alarm response appropriateness, F(2, 53) = .58, p > .05. There was also no interaction between reliability and training group for alarm response appropriateness, F(2, 53) = .21, p > .05 (see Fig. 3).

An ANOVA did not reveal a significant interaction of reliability and training type on alarm score, F(2, 53) = .62, p > .05. There were also no observable main effects for reliability, F(1, 53) = .15, p > .05, or training group, F(2, 53) = 62, p > .05, on alarm score.

Several participants did not complete the trust questionnaire, which is reflected in the smaller df. An ANOVA revealed a main effect of reliability on subjective trust in the alarm system, F(1, 37) = 6.82, p = .013, partial η
                        
                           2
                         = .156, where the 20% reliability group (M = 42.04, SE = 3.00) indicated less trust in the alarm system than the 40% reliability group (M = 49.49, SE = 2.53). No main effect of training type on trust was observed, F(2, 37) = .85, p > .05, or interaction, F(2, 37) = .56, p > .05 (See Fig. 3).


                        Brown (2008) has suggested that intervals in the minute range may be too lengthy for participants to learn well. Therefore, we made sure to provide participants with repeated training sessions. Our failure to find an effect of temporal interval training on sensor and alarm reaction data was therefore perplexing. Though the interval used was ecologically valid, it may have exceeded the memory abilities of the participants tested. A second related explanation is that our dual-task experimental setup may have placed excessive demands on the participants' working memories. Neurological research has suggested that the prefrontal cortex is used to reproduce and discriminate lengthy temporal intervals (Lalonde and Hannequin, 1999) and that subvocalization is instrumental for interval judgment (Gruber et al., 2000). Therefore, our use of subdivision training should have improved temporal discrimination. However, Pouthas and Perbal (2004) emphasize the importance of memory and attention for accurate time perception, so it is possible that participants were unable to benefit from training because of competing cognitive task demands.

Another possible explanation for the inadequacy of training might be lack of motivation on the part of the participants. We attempted to increase motivation by instituting a monetary incentive; however, it is possible that participants were still not sufficiently motivated. Further research should involve trained task operators and a more realistic task.

We did find it encouraging that participants differentiated between reliability levels. One interesting aspect is that they apparently paid greater attention to a single advertisement of reliability than to temporal intervals (reinforced by anecdotal comments from participants). This finding echoes past results showing that participants seem to generate reaction strategies heuristically, prior to interacting with the system.

Environmental design often constrains the potential for optimizing physical sensor and alarm signals. The results discussed here provide some guidance about how operators may be trained to ensure that signal reactions are timely and appropriate. From Experiment 1, it is clear that training operators to focus on underlying data may increase reaction appropriateness. However, the results of Experiment 2 provide a caveat: the selected data source matters. Though individual alarm validity may be predictable from recent precursory signals, training participants to discriminate time passage for this purpose holds little promise, especially when operators are cognitively loaded. In the medical field, some pulse oximetry monitors have been designed with a time-delay that reduces false alarm occurrence, without sacrificing the actionable alarms responded to by medical staff (Welch, 2011). To this point, it may be more feasible to circumvent human performance issues with time estimation by offloading this to the alarm system itself.

Technological innovation has resulted in abundant source data. For example, medical personnel now have access to a plethora of real-time data regarding patient health. As a result, it may be increasingly difficult to scrutinize individual data sources to determine alarm validity. The data presented here represent initial guidance for such situations. When training reaction decisions, focusing on one data source is predictable to processing a group of data sources. Doing so may yield benefits in terms of reaction speed and appropriateness.

Also revealing is the tendency of participants to make reaction decisions on the basis of available data, perhaps before experiencing individual signals themselves. Such a strategy may insulate task operators from excessive workload levels. If so, making a priori reaction decisions is one more example of how vulnerable task operators are to increased levels of workload. For that reason, alarm designers and trainers should be careful to advocate solutions and guidance that add no new workload or attention requirements.

@&#REFERENCES@&#

