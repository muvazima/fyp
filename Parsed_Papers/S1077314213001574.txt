@&#MAIN-TITLE@&#ObjectPatchNet: Towards scalable and semantic image annotation and retrieval

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We build ObjectPatchNet (OPN) from large-scale loosely annotated Internet images.


                        
                        
                           
                           OPN preserves regionlevel semantic labels and contextual clues for image annotation.


                        
                        
                           
                           OPN could be utilized as visual words with semantics for scalable image retrieval.


                        
                        
                           
                           Though not exhaustively tuned, OPN performs decently in these two applications.


                        
                        
                           
                           OPN could be an open platform for different applications and further improvements.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Visual vocabulary

Large-scale image retrieval

Image annotation

@&#ABSTRACT@&#


               
               
                  The ever increasing Internet image collection densely samples the real world objects, scenes, etc. and is commonly accompanied with multiple metadata such as textual descriptions and user comments. Such image data has potential to serve as a knowledge source for large-scale image applications. Facilitated by such publically available and ever-increasing loosely annotated image data on the Internet, we propose a scalable data-driven solution for annotating and retrieving Web-scale image data. We extrapolate from large-scale loosely annotated images a compact and informative representation, namely ObjectPatchNet. Each vertex in ObjectPatchNet, which is called as an ObjectPatchNode, is defined as a collection of discriminative image patches annotated with object category labels. The edge linking two ObjectPatchNodes models the co-occurrence relationship among different objects in the same image. Therefore, ObjectPatchNet models not only probabilistically labeled image patches, but also the contextual relationship between objects. It is well suited to scalable image annotation task. Besides, we further take ObjectPatchNet as a visual vocabulary with semantic labels, and hence are able to easily develop inverted file indexing for efficient semantic image retrieval. ObjectPatchNet is tested on both large-scale image annotation and large-scale image retrieval applications. Experimental results manifest that ObjectPatchNet is both discriminative and efficient in these applications.
               
            

@&#INTRODUCTION@&#

With the far reach of Internet and the popularity of digital cameras, digital camcorders, and camera cellphones, etc., the amount of user generated Internet photos has been increasing explosively. The explosive growth of these visual data has brought new challenges to multimedia research community. However, it is necessary to note that such ever increasing Internet image collection densely samples the real world objects, scenes, etc., thus has potential to serve as a knowledge source for large-scale image applications.

For example, Torralba et al., verify that 80 million tiny loosely labeled Internet images can be used as a reference dataset for image annotation, and recognition, etc., in simple data-driven ways [1]. Similarly, with the recent development of large-scale image datasets containing lots of image categories, such as ImageNet [2], a naïve strategy for image annotation would be running K Nearest Neighbor (KNN) tagging. More specifically, given an image, a set of image labels can be collected by retrieving its KNN Internet images. This image could hence be annotated by the most frequent labels collected. However, the efficiency and effectiveness of such data-driven strategy are inevitably confronted by the large memory and computation demand to store and process these large-scale image data. Moreover, the cluttered backgrounds and noisy tags in Internet images may also result in inaccurate annotation. Consequently, large-scale loosely annotated Internet images provide valuable information, but more effective and more efficient utilization of such Internet data is highly desired.

To achieve both scalable and accurate image annotation and semantic image retrieval, we propose to build the ObjectPatchNet from large-scale loosely annotated Internet images. As illustrated in Fig. 1
                     , ObjectPatchNet is composed of vertices and edges linking them. We call each vertex as an ObjectPatchNode. Each ObjectPatchNode contains visually similar image patches and is labeled with probabilistic object tags. The edges model the contextual relationships between pairs of ObjectPatchNodes. More specifically, in ObjectPatchNet, we define the contextual relationship as the co-occurrence probability between two objects in an image. As shown in Fig. 1, the thickness of the edge represents the strength of the contextual relationship. From the figure, it can be observed that, the object “Bear” co-occur frequently with objects “Grass” and “Creek” in the same image.

As shown in Fig. 1, with ObjectPatchNet we can largely reduce the negative effects of image background clutter, since only discriminative and semantically meaningful patches are kept and noisy patches are discarded. Moreover, since annotation information is assigned to image patches and extra contextual clues between objects are captured, ObjectPatchNet would be more accurate in annotating new images than the raw image dataset. In addition, we can easily remove information redundancy in each ObjectPatchNode based on the visual similarity of image patches it contains. It is easy to infer that, ObjectPatchNet would be more discriminative and compact than the original image dataset, and image applications based on it would be more efficient and accurate.

Based on ObjectPatchNet, both image annotation and retrieval can be carried out in efficient and effective ways. As shown in Fig. 1, images to be annotated can first be divided into patches; KNN ObjectPatchNodes of each patch can hence be retrieved. The labels of these retrieved ObjectPatchNodes can be collected, and then refined with the help of the contextual clues to be the final image labels. Experiments show that ObjectPatchNet based image annotation achieves satisfactory accuracy with high efficiency.

In large-scale near-duplicate image retrieval, the Bag-of-visual-Words (BoWs) image representation and inverted file indexing have shown very good efficiency [3]. Inspired by this, we propose to use ObjectPatchNet as a visual vocabulary with semantic labels. As shown in Fig. 1, each image is first represented as a Bag-of-ObjectPatchNodes representation, and then indexed with inverted file. During retrieval, images containing same ObjectPatchNodes and similar semantics with the query will be returned. Therefore, large-scale image retrieval combining both visual and semantic clues can be achieved with high efficiency. More details about our image annotation and image retrieval strategies will be given in Section 4.

Our work is partially similar to the recent image-web [4], which also explores the connectivity between image regions in a large-scale image dataset. However, image-web [4] is designed for photo collection exploration and does not consider semantic clues. ObjectPatchNet also differs from Object bank [5] and Object-graph [6], which are designed for specific visual tasks at limited data scale, i.e., visual recognition and unsupervised category learning, respectively. Some works also utilize loosely annotated images for image annotation [7,8]. For example in [8], a sparse graph is constructed among loosely annotated images. A label refinement strategy is then developed to effectively filter the noisy tags and update this sparse graph. The noisy images could hence be utilized to infer the concepts of unlabeled images. Our work differs from them in that, our work mines the concepts in finer scale, i.e., image patch level and captures extra contextual clues among patches, thus is potential to extract more valuable clues. Moreover, the meaningful patches could be regarded as semantic-preserving visual words in large-scale image search.

Compared with the existing image annotation algorithms [9–19], ObjectPatchNet based image annotation is data-driven, scalable, and efficient. Indeed, each ObjectPatchNode can be regarded as an attribute [20–23]. However, their differences are: (1) ObjectPatchNode preserves both visual and semantic clues, but attribute most often conveys semantic clues; (2) ObjectPatchNode can be directly used as a knowledge source in data-driven ways, thus is scalable and valid for different tasks; and (3) extra contextual clues between ObjectPatchNodes are conveniently modeled, which are proven helpful to improve the performance of ObjectPatchNet.

Mining contextual information to improve theaccuracy of semantic annotation has been done for a while in the field ofmultimedia research [24–26]. For instance, Qi et al. propose a Correlative Multi-Label framework (CML) [24] to learn the correlations between labels for video annotation. The CML approach shows satisfying performance on a dataset containing 61,901 sub video shots and 39 concepts. The flicker distance also utilizes the co-occurrence clues of different labels in the identical images to measure the semantic similarities [25]. However, the difference between our work and the previous ones are: (1) we mine both contextual clues and useful visual information from a large-scale loosely annotated image dataset and (2) contextual clue is stored in a compact ObjectPatchNet, thus can be efficiently applied for different applications in flexible data-driven ways. Capturing extra contextual clues is also one of the major differences between this work and our previous work in [27].

The contributions of this work are summarized as follows:
                        
                           (1)
                           We propose to construct ObjectPatchNet from the publically available and ever-increasing loosely labeled Internet images, which results in more effective and more efficient utilization of such Internet image data.

ObjectPatchNet preserves useful information for image annotation, i.e., both region-level semantic labels and contextual clues between objects.

Each vertex in ObjectPatchNet, i.e., ObjectPatchNode, could be utilized as a visual word with semantic labels. Based on the corresponding BoWs model, scalable semantic image retrieval can be achieved.

ObjectPatchNet could be an open platform which allows for different applications and further improvements on top of it. In this paper, ObjectPatchNet is not exhaustively tuned for image annotation and semantic image retrieval. However, it still achieves decent performance in these two applications.

The remainder of this paper is organized as follows. Section 2 reviews the related work on image annotation and semantic image retrieval. Section 3 introduces our proposed ObjectPatchNet construction strategy. Section 4 presents ObjectPatchNet based applications. Section 5 analyzes the experimental results, followed by the conclusions and future work in Section 6.

@&#RELATED WORK@&#

As a popular research topic in recent years, image annotation combines both textual and visual information to model the semantics of an image. Most often, image annotation is performed by conducting a machine learning strategy to combine multiple information channels including textual features, such as user comments, user tags, and surrounding texts, and visual features, such as color feature and texture feature. After being annotated, images would be able to be indexed and retrieved with existing scalable indexing framework, i.e., the one similar to the current image search engines such as Google Image, Bing, and Yahoo!. Therefore, image annotation leads to a promising solution for large-scale image understanding and retrieval.

Image annotation may incorporate the following tasks: (1) automatic tagging, which assigns tags to unlabeled images [9–12]; (2) tag refinement, which refines the initial noisy tags [13,14]; (3) tag-to-region, which assigns image tags to corresponding image regions [15,16]; and (4) tag ranking which ranks the initial tags based on their relevance [17,18] to the image. In the following part, we will briefly review these algorithms.

To mention some works on automatic tagging, in [9] the authors propose to build classifiers for each individual semantic tag and the automatic tagging task is hence transformed into an image classification task. The methods presented in [10] learn relevance models between images and textual keywords to assign images with their most relevant keywords. All these algorithms rely on sufficient training samples with high quality labels, which are difficult to obtain in many real-world applications. To mitigate this issue, some researchers propose to leverage semi-supervised learning which utilizes both labeled and unlabeled images to finish the automatic tagging [11,12] task. For instance, Zhou et al. [11] present a graph-based label propagation algorithm for automatic image tagging.

For tag refinement, Content based Annotation Refinement [13,14] re-ranks the tags of an image and reserves the top ones as the refined results. For instance, Liu et al. [14] propose to refine the tags based on the visual and semantic consistency in social images. For tag ranking, Liu et al. [17] use the kernel density estimation to identify the tag relevance and hence rank the image tags, which is further followed by a random walk algorithm to boost the estimation. Wang et al. [18] propose a semi-supervised learning model, which learns a ranking projection from visual word distribution to relevant tag distribution, to rank image tags.

As a more challenging task, Tag-to-region assigns image labels to corresponding image regions. To tackle this task, Liu et al. [15] use a bi-layer sparse coding strategy to uncover how an image region could be reconstructed from over-segmented image patches of the entire image repository. In [16], a multi-edge-graph is proposed to propagate the image annotation information from image level to region level. The proposed label propagation strategy in [16] successfully integrates automatic tagging, tag refinement, and tag-to-region tasks in a unified framework.

Notwithstanding the demonstrated success of current image annotation algorithms, most of them are designed and trained for small-scale datasets with only thousands of images and tens of tags. Therefore, it is not clear if these algorithms would be scalable to perform large-scale image annotation dealing with millions of images and thousands of tags.

In recent years, the attribute based algorithms are popular in large-scale image retrieval [20–23] and many other vision tasks [29]. Each attribute commonly corresponds to a concept detector and is defined as a basic semantic concept, such as red color, sky, road, and house, that may appear in the image. From each image, an attribute vector can be extracted as a semantic-preserving middle-level feature to bridge the gap between high-level semantics and low-level visual features, i.e., the Semantic Gap. Thus, attribute vector shows great potential in semantic image retrieval.

For example, Deng et al. [20], propose to use attribute vector as a mid-level feature for semantic image retrieval. Given the attribute vector, the similarity between two images can be computed by matching their attributes. One of the contributions of this work is that the hierarchy relationship between attributes conveyed by the WordNet [28] is considered for image similarity computation. According to the traditional strategies, two attributes “cat” and “dog” cannot be matched, thus their semantic similarity would be zero. However, considering they have a common ancestor “mammal” in the WordNet, their semantic similarity should be larger than zero. The authors manifest that taking the semantic hierarchy into consideration makes the computed image similarity more reasonable and hence improves the image retrieval performance [20].

In [20], only attribute vector is considered for image retrieval. However, completely relying on attribute vector may degrade the retrieval performance, because of the limited detection accuracy of the trained attribute detector. In [21], the authors propose to combine attribute vector and visual clues, i.e., Fisher vector for large-scale image retrieval. The authors find that combing attribute vector and Fisher vector further improves the retrieval performance. In [22], Siddiquie et al. present a novel framework for multi-attribute image retrieval and ranking. In this framework, images are retrieved not only based on attributes contained in the query, but also other correlated attributes that could be potentially informative to the query. Experiments show that the proposed strategy in [22] outperforms the state-of-the-art image ranking and retrieval methods.

In general, attribute is promising for many image applications. However, it still shows some shortcomings. In large-scale image retrieval, lots of attributes are needed to cover the semantics in large-scale images. In [20,21], the vocabularies from Large-Scale Concept Ontology for Multimedia (LSCOM) [28] are used as attributes. Therefore, thousands of attribute detectors have to be trained. For each test image, thousands of detectors also have to be computed to produce the attribute vector. In addition, the intra-class variations and cluttered backgrounds of objects make attribute detector training a challenging task. Therefore, attribute vector is expensive to compute, in both the training and testing stages. Moreover in [29], the authors state that, the existing attribute set training algorithms might not be optimal. For instance, some attributes are irrelevant with the visual tasks and some are not discriminative enough. Hence, more efficient and effective alternatives to attribute are highly desired.

To achieve both scalable and accurate image annotation and semantic image retrieval, we propose to build ObjectPatchNet from large-scale loosely annotated Internet images. Fig. 2
                      illustrates the four steps for ObjectPatchNet construction:
                        
                           (1)
                           We first collect a large-scale loosely annotated image dataset, and classify the images into different categories according to their annotated key words.

We divide images into patches, and separate the background patches from the object patches in each category by grouping [30] visually similar patches into dense patch groups (also called as patch group or group for short) (c.f. Section 3.1). Each group is defined as a collection of image patches that are visually similar to one another. For instance in Fig. 2, the generated dense patch groups in the “Bear” category would contain different types of patches, e.g., the ones from the head of bear, the body of bear, and the patches of grass and creek from the background.

We group [30] again the collection of the patch groups from different categories into DPGSs (Dense Patch Group Sets) and annotate these DPGSs by our proposed relevance voting strategy (c.f. Section 3.2). Each patch group will hence be annotated by the label of DPGS it belongs to.

Since some groups co-occur in the same category, intuitively they show contextual relationship with each other. Thus, after annotating the groups, we conveniently learn the contextual clues between them from each category (c.f. Section 3.3). Each group annotated with labels and contextual clues is taken as an ObjectPatchNode. The ObjectPatchNet is finally obtained by aggregating all the annotated groups and the contextual clues among them.

Noted that, in ObjectPatchNet, dense patch groups are annotated with the names of collected image categories. Therefore, the category number in training data should be large and diverse enough to cover the semantics of these groups and image patches. Fortunately, there are many ways to get such dataset with lots of categories from Internet. For example, it can be collected by retrieving image categories with key words in available image search engines like Google Image, Bing, i.e., similar way to the one used by ImageNet [2] and 80million tiny images [1], or we can start with the available datasets such as the ImageNet [2].

As illustrated in Fig. 2, we first divide images into patches and generate dense patch groups (also called as patch group or group for short) within each category. Image patch extraction can be carried out in multiple ways, including image segmentation [31], dense sampling [32], and salient region detection [33], etc. The image segmentation and salient region detection generally extract either visually coherent or visually salient image regions. However, they are often expensive to compute. Taking scalability into consideration, we adopt the dense sampling strategy to extract patches for its high efficiency and reasonable performance in object recognition tasks [32].

Inspired by spatial pyramid matching [32] strategy, we extract image patches with dense sampling at multiple scales. As shown in Fig. 3
                        , image patches are extracted with grids at different scales. Each type of grid is densely scanned over the image with overlap. In our experiments, the scales of the grids are set as 60×60, 120×120, and 180×180. The corresponding overlaps are set as 15, 30, and 45, respectively. Based on the extractor illustrated in Fig. 3, image patches can be efficiently extracted by simply computing the positions of the grids, which produces a highly redundant image patch collection in each image category.

After extracting image patches, we proceed to dense patch group generation. The goal is to obtain several discriminative dense patch groups for each image category, and each group is desired to contain visually similar image patches. To finish this task, we can cluster patches together with k-means. However, in k-means clustering, the number of clusters has to be manually set. This is not ideal since different categories contain different objects and backgrounds, thus should produce different numbers of groups. In addition, k-means clustering assigns each image patch to a cluster center. This strategy cannot guarantee each cluster contains densely distributed image patches. The shortcomings of k-means clustering are illustrated in Fig. 4
                        .

In Fig. 4, the desired dense patch groups and three clusters generated from k-means are shown in feature space. It can be seen that, the data points in Cluster 1 are loosely distributed and show large visual variations. Because images in the same category contain the same object, the loosely distributed patches are more likely to be noisy and indiscriminative patches for this category, e.g., cluttered and unstable backgrounds, and thus should not be kept as dense patch groups. In this sense, k-means clustering is not optimal for discovering the desired dense patch groups. Although some other clustering algorithms such as Affinity Propagation [34] may potentially get better clustering result, they cannot overcome the aforementioned problem, either.

Fortunately, some algorithms, such as Mean Shift [35] and Graph Shift [30] are designed for discovering densely distributed feature sets. Mean Shift [35] is a procedure for locating the maxima of adensity function in the feature space. Similarly, Graph Shift [30] operates in the affinity graph to find the dense sub-graphs with strong within-sub-graph similarities. Starting from each vertex, Graph Shift operates on a local sub-graph, and shifts toward one of the nearest dense sub-graphs by iterating between shrink phase and expansion phase.

During the shrink phase, Graph Shift discards some of the vertices in the local sub-graph, and finds a very compact subset of vertices. This makes sure the identified subset of vertices to be a dense subset for the local sub-graph. During the expansion phrase, new neighbor vertices will be selected and added to the local sub-graph. The newly added vertices will guide the algorithm toward one of the global dense sub-graphs. Since the algorithm operates on small local sub-graphs, it could find all significant dense sub-graphs with low time and memory complexity. Detailed descriptions of Graph Shift can be found in [30]. In this work, we adopt Graph Shift for its better performance than Mean Shift [30].

In each category, we construct a sparse affinity graph, where each image patch is a vertex, and the edge denotes similarity between two patches computed in the following equation:
                           
                              (1)
                              
                                 sim
                                 (
                                 
                                    
                                       p
                                    
                                    
                                       a
                                    
                                 
                                 ,
                                 
                                    
                                       p
                                    
                                    
                                       b
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                       
                                    
                                 
                                 
                                    
                                       s
                                    
                                    
                                       i
                                    
                                 
                                 (
                                 
                                    
                                       p
                                    
                                    
                                       a
                                    
                                 
                                 ,
                                 
                                    
                                       p
                                    
                                    
                                       b
                                    
                                 
                                 )
                              
                           
                        where s
                           i
                        (⋅,⋅) is the similarity of the ith visual feature. We extract three kinds of features to comprehensively describe the visual content of an image patch, i.e., the 128 dimensional SIFT [36] feature, the 256 dimensional Local Binary Pattern (LBP) [37], and the 128 dimensional color histogram. These features describe an image patch from different aspects, such as appearance and shape [36], texture [37], and color. In our experiments, we set three features with equal weights.

After constructing the graph, it is intuitive to infer that, image patches visually similar to each other will compose a dense sub-graph. Each identified dense sub-graph is hence taken to compute the corresponding dense patch group. We denote a dense patch group as:
                           
                              (2)
                              
                                 G
                                 {
                                 
                                    
                                       N
                                    
                                    
                                       ∗
                                    
                                 
                                 ,
                                 N
                                 ,
                                 Θ
                                 ,
                                 
                                    
                                       p
                                    
                                    
                                       i
                                    
                                 
                                 }
                                 ,
                                 
                                 
                                    
                                       p
                                    
                                    
                                       i
                                    
                                 
                                 ∈
                                 Θ
                                 ,
                                 i
                                 =
                                 1
                                 ,
                                 …
                                 ,
                                 N
                                 ,
                                 
                                 
                                    
                                       N
                                    
                                    
                                       ∗
                                    
                                 
                                 ⩾
                                 N
                              
                           
                        where N∗
                         is the total number of image patches in a dense sub-graph, N denotes the number of patches i.e., the p, kept in the patch set Θ of this dense patch group. Although, there could be N∗
                         patches in the dense sub-graph, we only keep the N patches with the highest density to lower the computational cost. The choice of N will be discussed in the experimental part (c.f. Section 5.2). Fig. 5
                         shows the numbers of identified groups for 1000 categories in ImageNet [2]. It can be seen that the number of groups generated in each category varies from 10 to 81. It is interesting to observe that some categories which show small visual diversity generate fewer groups, e.g., “great white shark”, “pumpkin seeds” and “sandbar” generate 10 groups. However, the visually complex categories generate more groups, e.g., “buther shop”, “tarantula”, and “whiptail” generate 81 categories.

Examples of generated groups are illustrated in Fig. 6
                        . In Fig. 6, each row shows some image patches marked by red rectangles, which belong to the same group. It can be seen that the patches in the same group show high visual similarity to each other, and could reasonably represent the visual patterns of the category from a certain aspect, e.g., object or stable backgrounds. For instance, the groups from “great white shark” typically show the visual patterns of “ocean”, “body of the shark”, “teeth and mouth of the shark”, and “the body of shark when it is near the surface of the sea”, respectively. It intuitively illustrates the fact that the generated groups in each category contain both the real object and the stable backgrounds, or other objects that co-occur within this category. Therefore, it would be ideal to label these groups and hence learn the contextual relationship between them.

After separating background patches from object patches into dense patch groups, we proceed to annotate them. To achieve this, we first collect category names as tags. Because lots of categories are included in the large-scale training set, we assume that most of the extracted groups could be reasonably annotated with the category names.

Our group annotation is implemented based on two criteria: (1) visually similar groups should be annotated with similar tags and (2) groups that are discriminative or relevant to a certain category should be likely to be annotated with this category.

This idea is illustrated in Fig. 7
                        . The groups from four categories: “Bear”, “Grass”, “Sky”, and “Creek” are shown and visually similar groups are linked. The size of circle denotes the relevance degree of the group to its source category. It can be seen that, three groups from categories: “Bear”, “Grass”, and “Creek” are linked, and they should be annotated with similar tags. Therefore, to meet the first criterion and to improve the efficiency, we first discover sets of visually similar patch groups across categories, which are called Dense Patch Group Sets (DPGSs), and then annotate these DPGSs, rather than individual groups.

To identify DPGSs, it is straightforward to apply Graph Shift [30]. Each dense patch group is hence taken as a vertex and we construct a sparse affinity graph across different categories. The group similarity is computed with the following equation:
                           
                              (3)
                              
                                 
                                    
                                       
                                       
                                          
                                             SIM
                                             (
                                             
                                                
                                                   G
                                                
                                                
                                                   a
                                                
                                             
                                             ,
                                             
                                                
                                                   G
                                                
                                                
                                                   b
                                                
                                             
                                             )
                                             =
                                             (
                                             S
                                             (
                                             
                                                
                                                   Θ
                                                
                                                
                                                   a
                                                
                                             
                                             ,
                                             
                                                
                                                   Θ
                                                
                                                
                                                   b
                                                
                                             
                                             )
                                             +
                                             S
                                             (
                                             
                                                
                                                   Θ
                                                
                                                
                                                   b
                                                
                                             
                                             ,
                                             
                                                
                                                   Θ
                                                
                                                
                                                   a
                                                
                                             
                                             )
                                             )
                                             /
                                             2
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             S
                                             (
                                             
                                                
                                                   Θ
                                                
                                                
                                                   a
                                                
                                             
                                             ,
                                             
                                                
                                                   Θ
                                                
                                                
                                                   b
                                                
                                             
                                             )
                                             =
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   
                                                      
                                                         
                                                            N
                                                         
                                                         
                                                            a
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      max
                                                   
                                                   
                                                      j
                                                      =
                                                      1
                                                   
                                                   
                                                      
                                                         
                                                            N
                                                         
                                                         
                                                            b
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               sim
                                                               
                                                                  
                                                                     
                                                                        
                                                                           
                                                                              p
                                                                           
                                                                           
                                                                              a
                                                                           
                                                                           
                                                                              (
                                                                              i
                                                                              )
                                                                           
                                                                        
                                                                        ,
                                                                        
                                                                           
                                                                              p
                                                                           
                                                                           
                                                                              b
                                                                           
                                                                           
                                                                              (
                                                                              j
                                                                              )
                                                                           
                                                                        
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   N
                                                
                                                
                                                   a
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where Na
                         is the number of patches in group Ga
                        , and pa
                         denotes an image patch contained in corresponding patch set Θa
                        . After constructing the graph, we identify dense sub-graphs, each of which is hence taken as a DPGS.

To annotate the DPGS, we define its label as:
                           
                              (4)
                              
                                 L
                                 =
                                 [
                                 
                                    
                                       l
                                    
                                    
                                       (
                                       1
                                       )
                                    
                                 
                                 ,
                                 
                                    
                                       l
                                    
                                    
                                       (
                                       2
                                       )
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    
                                       l
                                    
                                    
                                       (
                                       η
                                       )
                                    
                                 
                                 ]
                                 ,
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          η
                                       
                                    
                                 
                                 
                                    
                                       l
                                    
                                    
                                       (
                                       i
                                       )
                                    
                                 
                                 =
                                 1
                              
                           
                        where η is the number of tags, i.e., the category number in the training set. l
                        (
                        
                           i
                        
                        ) denotes the probability that this group is labeled with the ith tag. Therefore, the task is to obtain a probabilistic label vector for each DPGS.

Each DPGS may contain groups from different categories, therefore several candidate tags can be collected for each DPGS. In Fig. 7, three visually similar groups from “Bear”, “Grass”, and “Creek” would compose a DPGS, i.e., three groups connected by red lines, and the candidate tags of this DPGS are “Bear”, “Grass”, and “Creek”. According to the second criterion mentioned above (i.e., groups relevant to a certain category should be likely annotated with this category), to annotate the DPGS, we compute the relevance degree of each group to its source category tag. The label of each DPGS will hence be computed by voting and normalizing the relevance degrees of the groups it contains. For example in Fig. 7, suppose the relevance degrees of the three groups in the DPGS to “Grass”, “Bear”, and “Creek” are 0.8, 0.2, and 0.2, respectively. The membership weights of three tags in the final probabilistic label of the DPGS would be 0.667, 0.167, and 0.167, respectively.

The relevance degree of each group to its source category tag can be computed with many ways. Taking efficiency into consideration, we simply compute it based on two clues: (1) positions of patches in the group on the original image planes where they are extracted and (2) the portion of patch number between this group and the total patch number in its source category. We will discuss more possible strategies in Section 6.

As illustrated in Fig. 8
                        , if image patches in a group are from centers of image planes, the image patches would be more likely to be the object of the corresponding category. Thus this group should be more relevant to the label of this category. In addition, considering each category contains same object but different backgrounds, intuitively the object patches are likely to produce larger groups than the background patches. In Fig. 8, suppose two groups from category “Grass” and “Bear” compose a DPGS, and we need to compute their relevance degree to two tags “Grass” and “Bear”, respectively. Since most of the patches in the group from “Grass” are located at the centers of images, the relevance of this group to “Grass” should be stronger than the relevance of the other group to “Bear”. In addition, the patches in the group from “Grass” take larger portion in “Grass” category. Thus based on the portion clue, its relevance to “Grass” is stronger than the relevance of the other group to “Bear”. Based on these two clues, we compute the relevance of a group Ga
                         to the tag T with Eq. (5),
                           
                              (5)
                              
                                 Rel
                                 (
                                 T
                                 ,
                                 
                                    
                                       G
                                    
                                    
                                       a
                                    
                                 
                                 )
                                 =
                                 R
                                 _
                                 pos
                                 (
                                 
                                    
                                       G
                                    
                                    
                                       a
                                    
                                 
                                 )
                                 
                                    R
                                 _
                                 num
                                 (
                                 
                                    
                                       G
                                    
                                    
                                       a
                                    
                                 
                                 )
                                 ,
                                 
                                 
                                    
                                       G
                                    
                                    
                                       a
                                    
                                 
                                 ∈
                                 Ψ
                                 ∩
                                 Ω
                              
                           
                        
                        
                           
                              (6)
                              
                                 R
                                 _
                                 pos
                                 (
                                 
                                    
                                       G
                                    
                                    
                                       a
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   
                                                      
                                                         N
                                                      
                                                      
                                                         a
                                                      
                                                      
                                                         ∗
                                                      
                                                   
                                                
                                             
                                          
                                          Gaussian
                                          (
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          ,
                                          
                                             
                                                y
                                             
                                             
                                                i
                                             
                                          
                                          ,
                                          
                                             
                                                x
                                             
                                             
                                                ¯
                                             
                                          
                                          ,
                                          
                                             
                                                y
                                             
                                             
                                                ¯
                                             
                                          
                                          ,
                                          
                                             
                                                σ
                                             
                                             
                                                x
                                             
                                          
                                          ,
                                          
                                             
                                                σ
                                             
                                             
                                                y
                                             
                                          
                                          )
                                       
                                    
                                 
                                 
                                    
                                       N
                                    
                                    
                                       a
                                    
                                    
                                       ∗
                                    
                                 
                              
                           
                        
                        
                           
                              (7)
                              
                                 R
                                 _
                                 num
                                 (
                                 
                                    
                                       G
                                    
                                    
                                       a
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       N
                                    
                                    
                                       a
                                    
                                    
                                       ∗
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   
                                                      
                                                         G
                                                      
                                                      
                                                         b
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                N
                                             
                                             
                                                b
                                             
                                             
                                                ∗
                                             
                                          
                                       
                                    
                                 
                                 ,
                                 
                                 
                                    
                                       G
                                    
                                    
                                       b
                                    
                                 
                                 ∈
                                 Ω
                              
                           
                        where Ψ is the DPGS containing Ga
                        , Ω is the collection of patch groups generated in the category with tag T, (x, y) and (
                           
                              
                                 
                                    x
                                 
                                 
                                    ¯
                                 
                              
                              ,
                              
                                 
                                    y
                                 
                                 
                                    ¯
                                 
                              
                           
                        ) are the coordinates of the image patch center and the image center, respectively. We set σx
                        ,
                        σy
                         equal to 
                           
                              
                                 
                                    x
                                 
                                 
                                    ¯
                                 
                              
                              /
                              2
                              ,
                              
                              
                                 
                                    y
                                 
                                 
                                    ¯
                                 
                              
                              /
                              2
                           
                        , respectively.

Hence Eqs. (6) and (7) compute the relevance degrees based on position and potion clues, respectively. Based on Eq. (5), the label of each DPGS can be computed by voting and normalizing the relevance degrees of its candidate tags. Each dense patch group is then annotated with the label of the DPGS containing it.

After group annotation, we proceed to discover the contextual clues, i.e., the co-occurrence relationship between groups. For instance in Fig. 7, from the labeled groups in the category “Bear”, it is easy to infer that “Bear” shows high co-occurrence probability with the “Creek” and “Grass”. Hence, the contextual clues can be conveniently captured with the labeled groups.

We define the contextual clue as:
                           
                              (8)
                              
                                 C
                                 =
                                 [
                                 
                                    
                                       c
                                    
                                    
                                       (
                                       1
                                       )
                                    
                                 
                                 ,
                                 
                                    
                                       c
                                    
                                    
                                       (
                                       2
                                       )
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    
                                       c
                                    
                                    
                                       (
                                       η
                                       )
                                    
                                 
                                 ]
                                 ,
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          η
                                       
                                    
                                 
                                 
                                    
                                       c
                                    
                                    
                                       (
                                       i
                                       )
                                    
                                 
                                 =
                                 1
                              
                           
                        where η is the number of tags. c
                        (
                        
                           i
                        
                        ) is the co-occurrence probability between the current group and the object labeled with the ith tag.

Because the contextual relationship reflects the co-occurrence strength between different patch groups in the same image, we compute it within each image category. For a group Ga
                         in category Ω,we compute its contextual information as:
                           
                              (9)
                              
                                 
                                    
                                       
                                          
                                             C
                                          
                                          
                                             ∼
                                          
                                       
                                    
                                    
                                       a
                                    
                                 
                                 =
                                 Normalize
                                 
                                    
                                       
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   
                                                      
                                                         L
                                                      
                                                      
                                                         b
                                                      
                                                   
                                                   ≠
                                                   
                                                      
                                                         L
                                                      
                                                      
                                                         a
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                L
                                             
                                             
                                                b
                                             
                                          
                                          
                                             R
                                          _
                                          num
                                          (
                                          
                                             
                                                G
                                             
                                             
                                                b
                                             
                                          
                                          )
                                       
                                    
                                 
                                 ,
                                 
                                 
                                    
                                       G
                                    
                                    
                                       a
                                    
                                 
                                 ,
                                 
                                    
                                       G
                                    
                                    
                                       b
                                    
                                 
                                 ∈
                                 Ω
                              
                           
                        where L is the label information of the group. R_Num(Gb
                        ) reflects the ratio of image patch number between group Gb
                         and groups in the entire category, i.e., the relevance degree based on portion. It is computed in Eq. (7).

Note that in the same DPGS, groups from different categories are annotated with the same labels. However, because Eq. (9) is computed within each individual image category, the groups with same labels in a DPGS may show different contextual information, i.e., 
                           
                              
                                 
                                    C
                                 
                                 
                                    ∼
                                 
                              
                           
                        . Therefore, the contextual clue computed in Eq. (9) is not comprehensive. For example, as shown in Fig. 7, the three groups annotated with “Grass” should show strong contextual relationship with “Sky”, “Creek”, and “Bear”, simultaneously. However, based on Eq. (9), the contextual information computed within each category cannot be gathered together. Hence, we further merge the contextual clues 
                           
                              
                                 
                                    C
                                 
                                 
                                    ∼
                                 
                              
                           
                         of groups in the same DPGS with the following equation:
                           
                              (10)
                              
                                 
                                    
                                       C
                                    
                                    
                                       Ψ
                                    
                                 
                                 =
                                 Normalize
                                 
                                    
                                       
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   
                                                      
                                                         G
                                                      
                                                      
                                                         a
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      C
                                                   
                                                   
                                                      ∼
                                                   
                                                
                                             
                                             
                                                a
                                             
                                          
                                       
                                    
                                 
                                 ,
                                 
                                 
                                    
                                       G
                                    
                                    
                                       a
                                    
                                 
                                 ∈
                                 Ψ
                              
                           
                        where Ga
                         is a patch group in DPGS Ψ. As shown in Eq. (10), we merge the contextual information of the groups in Ψ to obtain the comprehensive contextual clue. CΨ
                         will be considered as the final contextual information for the groups in DPGS Ψ.

Suppose two groups Ga
                         and Gb
                         are labeled with tag x and y, respectively. It is intuitive to infer that, if these two groups co-occur in the same category, their contextual vectors, i.e., Ca
                         and Cb
                        , would show relatively large values in the dimensions corresponding to tag y and tag x, i.e., 
                           
                              
                                 
                                    C
                                 
                                 
                                    a
                                 
                                 
                                    (
                                    y
                                    )
                                 
                              
                           
                         and 
                           
                              
                                 
                                    C
                                 
                                 
                                    b
                                 
                                 
                                    (
                                    x
                                    )
                                 
                              
                           
                         respectively. In Section 4, we will introduce how we use contextual information to help the image annotation and image retrieval tasks.

After computing the annotation information and the contextual clues, we generate an ObjectPatchNode from each dense patch group. The ObjectPatchNode is denoted as:
                           
                              (11)
                              
                                 Node
                                 {
                                 F
                                 ,
                                 L
                                 ,
                                 C
                                 }
                              
                           
                        where F is the visual feature of an ObjectPatchNode. It is computed as the mean of patch features in its corresponding dense patch group to reduce the storage and computational cost. L and C denote the label information and contextual information, respectively.

ObjectPatchNet is finally constructed by labeling all the dense patch groups and learning their contextual relationships. Note that the construction stage is divided into two steps: within category step (Section 3.1) and across category step (Section 3.2). The within category step is parallelable. The across category step operates on the generated dense patch groups in each category, the numbers of which are limited. Thus, it is rational to say the proposed network construction strategy is scalable.

Obviously, the performance of ObjectPatchNet is largely relied on the discriminative powers of ObjectPatchNodes. To increase the discriminative power and suppress the noisy, we further evaluate the discriminative power of each ObjectPatchNode and remove the indiscriminative ones. This is similar to the removal of Stop Word strategy in textual information retrieval [3], which removes the noisy words such as “the”, and “a”. We evaluate the discriminative power of an ObjectPatchNode with Eq. (12), i.e.,
                           
                              (12)
                              
                                 Dpower
                                 (
                                 
                                    
                                       Node
                                    
                                    
                                       a
                                    
                                 
                                 )
                                 =
                                 std
                                 (
                                 
                                    
                                       L
                                    
                                    
                                       a
                                    
                                 
                                 )
                                 +
                                 std
                                 (
                                 
                                    
                                       C
                                    
                                    
                                       a
                                    
                                 
                                 )
                              
                           
                        where std(⋅) returns the standard deviation of a vector. Essentially, if both the label information and the contextual information of an ObjectPatchNode are distributed on lots of tags, i.e., flat distribution, its labels would be fuzzy and hence shows poor discriminative power. Instead, if both the annotation information and contextual information focus on certain tags, the ObjectPatchNode would be discriminative. In our experiments, we rank the groups based on their discriminative powers, and remove the bottom 10% groups with the smallest discriminative powers.

This step finalizes the construction of ObjectPatchNet, which is a more clean and informative representation of the large-scale loosely labeled Internet images. The meaning of “clean” includes two aspects: (1) the patch-level labels are more precise than original image-level labels and (2) only object related and stable patches are kept, the loosely distributed noisy ones are discarded in group construction. Hence, compared with original ImageNet, the labels and patches are clean. This is one of the reasons why ObjectPatchNet performs better than ImageNet. After getting ObjectPatchNet, it is interesting to visualize this net to show the annotated labels and the contextual information. Fig. 9
                         illustrates a certain part of the generated ObjectPatchNet.

In Fig. 9, each image patch collection denotes an ObjectPatchNode. The labels represent the annotation information of the ObjectPatchNodes. The linkage between each pair of ObjectPatchNodes means that these two ObjectPatchNodes show strong contextual relationship with each other. It can be observed that the annotated labels and the learned contextual relationships are reasonable. For instance, the image patches of sharks are rationally labeled. Meanwhile, the ObjectPatchNodes about sea animals, ships, and sea shore all reasonably show strong contextual relationships with the ObjectPatchNode about ocean. The ObjectPatchNet cannot distinguish the “Great White Shark” with “Tiger Shark” or “Stingray” because these sharks are visually similar to each other. It also can be observed that some ObjectPatchNodes are annotated with identical labels. This is because they are from the same DPGS, thus are assigned with the same labels. We will discuss possible strategies to further improve the discriminative ability of ObjectPatchNet in Section 6. In next section, we will introduce our strategies to utilize ObjectPatchNet in various tasks.

Based on the generated ObjectPatchNet, an image can be annotated with a simple KNN annotator. Specifically, the image can first be divided into patches, each of which is then utilized to retrieve its K nearest ObjectPatchNodes. Finally, the image is annotated according to the labels of retrieved ObjectPatchNodes. This strategy is intuitive but ignores the contextual clue. To fully utilize ObjectPatchNet, we propose a novel image annotation algorithm, which is called as ContextRank.

ContextRank is inspired by the PageRank algorithm [38]. In PageRank, a matrix is built to record the inherent importance of different webpages and the contextual relationships between them. Iterations are then carried out to update the weight of each webpage based on this matrix. After several iterations, the weight of each webpage is obtained combining both its inherent importance and contextual relationships with other webpages. In ContextRank, we also construct a η
                        ×
                        η sized matrix M based on two types of information, i.e., the label information and contextual information. η is the number of tags in ObjectPatchNet. In this matrix, the diagonal element Mi,i
                         and the non-diagonal element Mj,i
                         are computed with Eq. (13) and Eq. (14), respectively,
                           
                              (13)
                              
                                 
                                    
                                       M
                                    
                                    
                                       i
                                       ,
                                       i
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             
                                                Node
                                             
                                             
                                                α
                                             
                                          
                                       
                                    
                                 
                                 (
                                 
                                    
                                       sim
                                    
                                    
                                       α
                                    
                                 
                                 
                                    
                                 
                                 
                                    
                                       l
                                    
                                    
                                       α
                                    
                                    
                                       (
                                       i
                                       )
                                    
                                 
                                 )
                                 ,
                                 
                                    
                                       Node
                                    
                                    
                                       α
                                    
                                 
                                 ∈
                                 KNN
                                 (
                                 img
                                 )
                              
                           
                        
                        
                           
                              (14)
                              
                                 
                                    
                                       M
                                    
                                    
                                       j
                                       ,
                                       i
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             
                                                Node
                                             
                                             
                                                α
                                             
                                          
                                       
                                    
                                 
                                 (
                                 
                                    
                                       sim
                                    
                                    
                                       α
                                    
                                 
                                 
                                    
                                 
                                 
                                    
                                       l
                                    
                                    
                                       α
                                    
                                    
                                       (
                                       i
                                       )
                                    
                                 
                                 
                                    
                                 
                                 
                                    
                                       c
                                    
                                    
                                       α
                                    
                                    
                                       (
                                       j
                                       )
                                    
                                 
                                 )
                                 ,
                                 i
                                 ≠
                                 j
                                 ,
                                 
                                    
                                       Node
                                    
                                    
                                       α
                                    
                                 
                                 ∈
                                 KNN
                                 (
                                 img
                                 )
                              
                           
                        where KNN(img) returns the set of 5-NN ObjectPatchNodes of the patches from the image under annotation. simα
                         is the similarity between a image patch and one of its 5-NN ObjectPatchNodes, i.e., the Nodeα
                        .

After computing the matrix M, we utilize ContextRank to compute the weights of tags. Algorithm 1 summarizes the details of ContextRank. ContextRank can be explained by an example. Let’s suppose the image under annotation contains tigers and trees. After retrieving the KNN ObjectPatchNodes, we may collect many candidate tags including “tiger”, “cat”, “tree” and some noisy tags. Because ObjectPatchNodes annotated with “tiger” and “tree” show high contextual relationship to each other, the values of Mtiger,tree
                         and Mtree,tiger
                         would be relatively large. As a result, the rank values of these two tags will be boosted during the ContextRank iterations, and the influences of other noisy tags are suppressed. After ContextRank, we select the tags with the highest rank values as the tags for the image.
                           
                              
                                 
                                 
                                    
                                       
                                          Algorithm1: ContextRank
                                    
                                    
                                       
                                          Input: the matrix M; maximum iteration time: T
                                       
                                    
                                    
                                       
                                          Output: The rank value of each tag
                                    
                                    
                                       
                                          Initialize: the initial rank value for each tag as 1/η, iter
                                          
                                          =
                                          0
                                    
                                    
                                       
                                          Compute 
                                          
                                             
                                                
                                                   
                                                      M
                                                   
                                                   
                                                      ^
                                                   
                                                
                                                =
                                                (
                                                d
                                                
                                                   
                                                
                                                M
                                                +
                                                (
                                                1
                                                -
                                                d
                                                )
                                                ·
                                                φ
                                                )
                                             
                                          
                                       
                                    
                                    
                                       
                                          Normalize the sum of each column of 
                                             
                                                
                                                   
                                                      M
                                                   
                                                   
                                                      ^
                                                   
                                                
                                             
                                           as 1 [38];
                                    
                                    
                                       
                                          While 
                                          iter
                                          <
                                          T
                                       
                                    
                                    
                                       
                                          
                                          
                                             
                                                NewRank
                                                =
                                                
                                                   
                                                      M
                                                   
                                                   
                                                      ^
                                                   
                                                
                                                
                                                   
                                                
                                                OldRank
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                          If (|NewRank
                                          −
                                          OldRank|⩽
                                          ɛ)
                                    
                                    
                                       
                                          
                                          
                                          break
                                       
                                    
                                    
                                       
                                          
                                          OldRank
                                          =
                                          NewRank
                                       
                                    
                                    
                                       
                                          
                                          iter++
                                       
                                    
                                    
                                       
                                          End
                                       
                                    
                                    
                                       
                                          Output 
                                          NewRank
                                       
                                    
                                 
                              
                           
                        
                     

Inspired by the high efficiency of visual vocabulary [3] in near-duplicate image retrieval, we consider ObjectPatchNodes as visual words and propose to use the inverted file to index images. It should be noted that, different from the SIFT based visual words [3], ObjectPatchNode preserves rich semantic information, therefore, it is suited to perform semantic image retrieval. The indexing structure of our proposed image retrieval strategy is illustrated in Fig. 10
                        .

As illustrated in Fig. 10, we use standard inverted file structure for image indexing. Therefore, image retrieval can be finished with TF-IDF (Term Frequency-Inverse Document Frequency) weighting [3], where the TF term indicates the importance of a visual word to a certain image. Differently, to make the TF (Term Frequency) weighting [3] better reflect the semantics of the image, we propose a novel weighting strategy, namely Semantic Weighted TF (SWTF) in the following equation:
                           
                              (15)
                              
                                 
                                    
                                       SWTF
                                    
                                    
                                       I
                                    
                                    
                                       (
                                       α
                                       )
                                    
                                 
                                 =
                                 〈
                                 
                                    
                                       L
                                    
                                    
                                       α
                                    
                                 
                                 ,
                                 
                                    
                                       L
                                    
                                    
                                       I
                                    
                                 
                                 〉
                                 
                                    
                                 
                                 
                                    
                                       TF
                                    
                                    
                                       I
                                    
                                    
                                       (
                                       α
                                       )
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    TF
                                 
                                 
                                    I
                                 
                                 
                                    (
                                    α
                                    )
                                 
                              
                           
                         denotes the standard Term Frequency of the ObjectPatchNode: Nodeα
                         in image I 
                        [3]. Lα
                         is the probabilistic label of Nodeα
                        . LI
                         is the probabilistic label of the image I computed by ContextRank. 〈⋅,⋅〉 denotes the inner product.

The effect of Semantic Weighted TF is explained in Fig. 11
                        . As illustrated in the figure, in Semantic Weighted TF, the semantic weight is the inner product of the probabilistic labels between ObjectPatchNode and image. According to Eq. (15) for each query image, the weights of semantically relevant ObjectPatchNodes will be increasing, and the weights of irrelevant ones will be decreasing. As a result, semantic image retrieval can be realized with efficient inverted file indexing, without degrading the retrieval efficiency.

To further improve the retrieval and indexing efficiency, we organize the ObjectPatchNodes in the leaf nodes of a hierarchical tree structure similar to the vocabulary tree [3]. This can be finished by hierarchically bottom-up clustering ObjectPatchNodes and taking the cluster centers as parent nodes and the other elements in the cluster as child nodes. Similar to the vocabulary tree [3], this hierarchical structure allows for faster approximate nearest ObjectPatchNode search. The effectiveness and efficiency of the proposed image retrieval strategy will be evaluated in Section 5.

@&#EXPERIMENTS@&#

ObjectPatchNet is designed based on a large-scale image dataset with a large number of image categories. Currently, ImageNet [3] contains more than 12million images and 17,624 categories. Thus, it is ideal to build ObjectPatchNet based on the entire ImageNet. However, this dataset is beyond our computational and storage capacity. To make our experimental implementation and evaluation feasible, we adopt the dataset of ILSVRC2010 [39], a subset of ImageNet [2]. This dataset includes 1000 image categories and about 1.2million training images and 150K test images. It covers different types of categories including scenes, man-made objects, plants, animals, etc. Therefore, it is a good dataset to build ObjectPatchNet. Each image in this dataset only has one category label, thus it is rational to call them as loosely annotated Internet images. We use ILSVRC2010 data also because it can be easily acquired, and we can conveniently make comparisons with many reported results on this dataset.

We conduct our experiments on three PCs, each of which has 4GB memory and a 4-Core 2.8GHz CPU. Note that, group construction is finished parallelly within each image category, and group labeling and contextual information learning are implemented based on DPGSs, the number of which is limited. In this way, ObjectPatchNet construction is divided into several offline steps with limited data scale. Therefore, the cost of our algorithm is manageable. We finally construct several ObjectPatchNets by keeping different numbers of patches, i.e., the parameter N in Eq. (2), in each group. The influence of this parameter will be discussed in Section 5.2. The effectiveness and efficiency of ObjectPatchNet in two large-scale image applications will be evaluated in the following two parts.

In this part, we test ObjectPatchNet in large-scale image annotation task. Many image annotation algorithms have been proposed and they have achieved good performance in different annotation tasks. However, few of them focus on the annotation tasks in large-scale datasets. Therefore, to make the experimental comparison more convincing and feasible, we first compare our algorithms with the baseline KNN annotator and simultaneously study the effects of different parameter settings in ObjectPatchNet. Then, we show the reported results in ILSVRC2010 [39] to compare our annotation algorithms with the state-of-the-art.

The 1.2million training images are adopted to generate an ObjectPatchNet, which finally contains 42,010 ObjectPatchNodes. To evaluate the performance of image annotation using ObjectPatchNet, we use the annotation error score defined by ILSVRC2010 [39] as a measurement, i.e.,
                           
                              (16)
                              
                                 Error
                                 =
                                 
                                    
                                       
                                          min
                                       
                                       
                                          i
                                          =
                                          1
                                          :
                                          5
                                       
                                    
                                 
                                 (
                                 D
                                 (
                                 gtruth
                                 ,
                                 
                                    
                                       tag
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 )
                              
                           
                        where gtruth denotes the ground truth tag of the test image. tagi
                         is one of the top five tags assigned by the annotation algorithm. D(⋅,⋅) reflects the semantic dissimilarity between two tags. It is computed as the height of the lowest common ancestor of the two tags in the WordNet [28].

To evaluate the performance of our proposed ObjectPatchNet, as well as the effect of the number of patches kept in each group, i.e., the parameter N in Eq. (2), we first make comparisons among the following three algorithms:
                           
                              •
                              
                                 A1: KNN voting based on the raw training set, i.e., ILSVRC2010 training set. K is set as 100.


                                 A2: KNN voting based on ObjectPatchNet. K is set as 5.


                                 A3: ContextRank based on ObjectPatchNet.

The average annotation errors of the three algorithms are compared in Fig. 12
                        (a). Obviously, the performance of ObjectPatchNet is closely related with the number of patches kept in each patch group. When the patch number is smaller than 40, both A2 and A3 perform worse than A1. This is because patch groups would be too sparse to be labeled accurately with only a few image patches. As the patch number increases, the accuracies of both A2 and A3 increase. The fact that A3 performs better than A2 clearly validates the efficacy of our ContextRank algorithm. In all experiments thereafter, the patch number N is set as 140.

In addition to the annotation accuracy, we also compare the efficiency in Fig. 12(b). From Fig. 12(b), A1 is more time consuming than A2. This is because for each input image, A1 has to compute KNN images from 1.2million images. As for A2, each image patch only needs to compute its KNNs from 42,010 ObjetNodes. A3 needs extra computation to rank the tags in ContextRank, therefore it needs about 0.332s more than A2. From the comparisons in Fig. 12, we conclude that the generated ObjectPatchNet is more discriminative and more efficient than its original ILSVRC2010 dataset [39] for large-scale image annotation.

Besides the comparisons with the baseline, we also make comparisons with the state-of-the-art. Fig. 13
                         shows the comparison between our algorithms with the reported results in ILSVRC2010. It can be observed that, the algorithms proposed by the teams from NEC-UIUC, XRCE, and University of Tokyo [39] show better performances than ours. However, it should be noted that their algorithms are based on expensive features and classifiers, such as stochastic SVM and Fisher Kernel SVM, and are specifically tuned for this task. It also can be seen that A2 and A3 outperform most of the other KNN based algorithms, e.g., the ones from IBM-ensemble and LIG. Considering that ObjectPatchNet is not exhaustively tuned for image annotation task, we can conclude that the generated ObjectPatchNet shows quite decent performance in large-scale image annotation task.

Some examples of ObjectPatchNet based image annotation are illustrated in Fig. 14
                        . Obviously, ObjectPatchNet works well in the examples shown in Fig. 14(a). ObjectPatchNet fails in the cases in Fig. 14(b). This is mainly because the objects in these images are too small or visually similar to other objects. In addition, these objects and backgrounds hardly co-occur in the training set, which makes the contextual information in ObjectPatchNet helpless. Therefore, these objects are confusing and are easily suppressed by the noisy backgrounds.

In this experiment, we also use the dataset from ILSVRC2010 [39] for training and testing. From each image category, we randomly select 400 images to train an ObjectPatchNet containing 42,790 ObjectPatchNodes. The remaining 0.8million images are indexed. The 150K test images with ground truth are used as queries. For each query, we compute the Mean Average Precision (MAP) with the following equation:
                           
                              (17)
                              
                                 MAP
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       100
                                    
                                 
                                 
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          100
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         1
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            ∑
                                                         
                                                         
                                                            k
                                                            =
                                                            1
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                   
                                                   
                                                      max
                                                   
                                                   (
                                                   0
                                                   ,
                                                   10
                                                   -
                                                   D
                                                   (
                                                   gtruth
                                                   ,
                                                   
                                                      
                                                         img
                                                      
                                                      
                                                         k
                                                      
                                                   
                                                   )
                                                   )
                                                
                                             
                                          
                                          10
                                       
                                    
                                 
                              
                           
                        where D(gtruth, imgk
                        ) represents the semantic dissimilarity between the ground truth tag of the query and category tag of the kth returned image, i.e., the imgk
                        . If it is larger than 10, the corresponding image will be considered as irrelevant.

To demonstrate the efficacy of our algorithm, we make comparisons with the following algorithms:
                           
                              •
                              
                                 B1: Linear search strategy+Euclidean Distance. The image features include SIFT [36], color histogram and LBP [37].


                                 B2: Visual vocabulary tree with 44,100 visual words [3] + inverted file indexing + TF-IDF weighting. The visual vocabulary tree is generated by hierarchical k-means clustering. Each non-leaf node contains 210 branches, i.e., the parameter k in k-means clustering. The dense sampling strategy in Section 3.1 is adopted for image patch extraction. We extract SIFT [36], color histogram and LBP [37] features from each patch and use Eq. (1) as the similarity measurement.


                                 B3: Binary attribute vector + linear search strategy + Hamming distance. We extract SIFT [36], color histogram and LBP [37] features and train the attribute detectors with linear 1-vs-all SVM. The training data is identical with the one for ObjectPatchNet. We train 1000 binary SVMs with LIBSVM, each corresponds to a bin (i.e., a category label) in attribute vector. To compute the binary attribute vector, we set all bins corresponding to positive SVMs as 1. Thus, each image may have multiple positive bins. Finally, we get 1000 attribute detectors and hence the attribute vector of each image is a 1000-dimensional binary vector. We use linear search strategy and Hamming distance to compute the similarities between query image and database images.


                                 B4: Probabilistic attribute vector + linear search strategy + Euclidean distance. Different from B3, we use the classification scores of SVMs as the attribute vector. We further normalize the classification scores to form a vector whose entries sum to one. Linear search strategy and Euclidean distance are used to compute the similarities between query image and database images.


                                 B5: ObjectPatchNet + inverted file indexing + TF-IDF weighting. The ObjectPatchNodes are clustered into a two layer tree structure. The first layer of the tree has 400 branches.


                                 B6: ObjectPatchNet + inverted file indexing + Semantic Weighted TF-IDF weighting. Similar to B4, ObjectPatchNodes are also organized in a two-layer tree structure. However, Semantic Weighted TF-IDF weighting is adopted in B6.

The comparison of MAP is illustrated in Fig. 15
                        (a). From Fig. 15(a), it can be observed that B1 shows the worst performance. This is largely due to the fact that B1 is based on the global feature, which is not descriptive enough to the visual content of image. It can also be seen that B5 performs better than B2. Note that B2 and B5 are based on the same patch feature and TF-IDF weighting, and the visual word number of B2 is also similar as the ObjectPatchNode number in B5. Hence, we can conclude that the Graph Shift based group construction in B5 is more effective than the k-means clustering in B2. From the figure, we can observe that B4 outperforms B3, which is reasonable because the probabilistic attribute vector in B4 preserves more information than the binary vector in B3. It is also obvious that B6 outperforms B5. This clearly illustrates that the Semantic Weighted TF weighting is more effective than the classic TF weighting in semantic image retrieval. From the MAPs of B3, B4, B5, and B6, we found that the ObjcetPatchNet outperforms the attribute. The performance of attribute is largely limited by the poor accuracy and generalization ability of the trained attribute detectors. Each object category commonly shows large intra-class visual variations and complicated back-grounds. Therefore, single object is hard to be modeled by single attribute detectors. However in ObjectPatchNet, each object category corresponds to multiple ObjectPatchNodes, each of which represents a typical visual appearance of this category. Therefore, this problem could be largely mitigated. In addition, ObjectPatchNet captures helpful contextual clues, which could hardly be modeled in attribute detectors.

The efficiencies of these six algorithms are compared in Fig. 15(b). It can be observed that B1 and B4 are the most time consuming since expensive linear search strategy and Euclidean distance are adopted. B4 has to compute the classification scores of 1000 SVM classifiers, making it further consumes more time than B1. Because visual words and ObjectPatchNodes are organized in tree structures and inverted file indexing is adopted, B2, B5, and B6 achieve very high retrieval efficiency. Among them, B6 consumes more time because it needs extra time to label the query image for Semantic Weighted TF computation. Because efficient Hamming distance is adopted for similarity computation, the retrieval efficiency of B3 is acceptable. However, the binary representation of B3 loses significant information, resulting in its poor retrieval accuracy.

Generally, we conclude that our ObjectPatchNet based image retrieval algorithm achieves efficient retrieval in a 0.8million image database with decent accuracy and efficiency. In addition, from the comparisons between ObjectPatchNet and attributes, we prove that ObjectPatchNet is more effective and efficient than attribute in large-scale semantic image retrieval. Some examples of ObjectPatchNet based image retrieval are illustrated in Fig. 16
                        .

@&#CONCLUSIONS@&#

In this paper, we propose to build ObjectPatchNet from loosely labeled Internet Images, which results in more effective and more efficient utilization of such Internet image data. ObjectPatchNet is composed of ObjectPatchNodes, each of which is a collection of discriminative image patches with label information and contextual information. ObjectPatchNet can be seen as a compact and informative knowledge source. Based on ObjectPatchNet, we propose novel image annotation and image retrieval algorithms such as ContextRank and inverted file indexing with Semantic Weighted TF weighting. Experimental results reveal that the generated ObjectPatchNet and the proposed algorithms are both efficient and effective in large-scale image annotation and large-scale semantic image retrieval applications.

The goal of our work is to utilize large-scale loosely labeled images for scalable image applications. This goal is different from the ones of most related image annotation works, which are based on small-scale training data, complicated machine learning models, and are tuned for specific tasks. Our work utilizes large-scale training data and simple data-driven strategy for different image applications. Compared with related works, it is data-driven, scalable, and efficient.

There are still many interesting problems that are worthy of further study in ObjectPatchNet construction. For example, feature weighting strategies, distance metric learning, or multi-kernel learning can be adopted to make the visual similarity computation between image patches and image patch groups more semantically reasonable. More clues and algorithms such as relevance modeling [10], sparse coding [15], semi-supervised learning [40] can be considered to compute the relevance degrees between groups and categories in Eq. (5). Visually similar images for each image patch can also be collected and utilized for estimating the semantics of image patches in Eq. (5). The semantic relationships defined by the WordNet [28] can also be taken into consideration to guide the group labeling. In addition, ObjectPatchNet can be constructed from multi-million-scale datasets and can be tested in more image annotation tasks such as tag-to-region and tag ranking and datasets like the NUS-WIDE [41], which contains a large number of loosely annotated images. We will continue to improve ObjectPatchNet in our future work.

@&#ACKNOWLEDGMENTS@&#

This work was supported in part to Dr. Qi Tian by ARO grant W911NF-12-1-0057, NSF IIS 1052851, Faculty Research Awards by Google, FXPAL, and NEC Laboratories of America, and 2012 UTSA START-R Research Award respectively. This work was supported in part by NSFC 61128007, in part by National Basic Research Program of China (973 Program): 2012CB316400, in part by National Natural Science Foundation of China: 61025011 and 61332016. Gang Hua is partly supported by US National Science Foundation Grant IIS 1350763, China National Natural Science Foundation Grant 61228303, GH’s start-up funds from Stevens Institute of Technology, a Google Research Faculty Award, a gift grant from Microsoft Research, and a gift grant from NEC Labs American.

@&#REFERENCES@&#

