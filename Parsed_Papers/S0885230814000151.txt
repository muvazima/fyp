@&#MAIN-TITLE@&#Data-driven models for timing feedback responses in a Map Task dialogue system

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We train models for detecting suitable feedback locations in the user's speech.


                        
                        
                           
                           We evaluated the trained model in a dialogue system through real user interactions.


                        
                        
                           
                           We exploit prosodic, lexico-syntactic and contextual cues for online detection.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Spoken dialogue systems

Timing feedback

Turn-taking

User evaluation

@&#ABSTRACT@&#


               
               
                  Traditional dialogue systems use a fixed silence threshold to detect the end of users’ turns. Such a simplistic model can result in system behaviour that is both interruptive and unresponsive, which in turn affects user experience. Various studies have observed that human interlocutors take cues from speaker behaviour, such as prosody, syntax, and gestures, to coordinate smooth exchange of speaking turns. However, little effort has been made towards implementing these models in dialogue systems and verifying how well they model the turn-taking behaviour in human–computer interactions. We present a data-driven approach to building models for online detection of suitable feedback response locations in the user's speech. We first collected human–computer interaction data using a spoken dialogue system that can perform the Map Task with users (albeit using a trick). On this data, we trained various models that use automatically extractable prosodic, contextual and lexico-syntactic features for detecting response locations. Next, we implemented a trained model in the same dialogue system and evaluated it in interactions with users. The subjective and objective measures from the user evaluation confirm that a model trained on speaker behavioural cues offers both smoother turn-transitions and more responsive system behaviour.
               
            

@&#INTRODUCTION@&#

Traditionally, dialogue systems have rested on a very simple model for turn-taking: the system uses a fixed silence threshold to detect the end of the user's utterance, after which the system responds. However, this model does not capture human–human dialogue very accurately. Sometimes a speaker simply hesitates and no turn-change is intended; sometimes the turn changes after barely any silence (Sacks et al., 1974). Therefore, such a simplistic model can result in systems that frequently produce responses at inappropriate occasions, or produce delayed responses or no response at all when expected, thereby causing the system to be perceived as interruptive or unresponsive. Related to the problem of turn-taking is that of backchannels (Yngve, 1970). Backchannel feedback – short acknowledgements such as uh-huh or mm-hmm – are used by human interlocutors to signal continued attention to the speaker, without claiming the conversational floor. If a dialogue system is to manage smooth exchange of speaking turns and provide backchannel feedback without being interruptive, it must be able to first identify suitable locations in the user's speech to do so.

Human conversational partners are skilled at managing smooth turn-transitions. Duncan (1972) observed that human interlocutors continuously monitor cues, such as content, syntax, intonation, paralanguage, and body motion, in parallel to manage turn-taking. Similar observations have been made in various other studies investigating the turn-taking and back-channelling phenomena in human conversations. Ward (1996) has suggested that a low pitch region is a good cue that backchannel feedback is appropriate. On the other hand, Koiso et al. (1998) have argued that both syntactic and prosodic features make significant contributions in identifying turn-taking and backchannel relevant places. Cathcart et al. (2003) have shown that syntax in combination with pause duration is a strong predictor for backchannel continuers. Gravano and Hirschberg (2011) identified seven turn-yielding and six backchannel-inviting cues spanning over prosodic, acoustic, and lexico-syntactic feature that could be used for recognition and generation of turns and backchannel.

However, there is a general lack of studies on how such models could be used online in dialogue systems and to what extent that would improve the interaction. There are two problems in doing so. First, the data used in the studies mentioned above are from human–human dialogue and it is not obvious to what extent the models derived from such data transfers to human–machine dialogue. Second, many of the features used in the proposed models were manually extracted. This is especially true for the transcription of utterances, but several studies also rely on manually annotated prosodic features.

This article builds upon our earlier work on automatic detection of relevant feedback response locations in the user's speech (Meena et al., 2013). In our earlier work, we presented a data-driven model of what we call Response Location Detection (RLD). The model is fully online and only relies on automatically extractable features – comprising syntax, prosody and context. The model has been trained on human–computer dialogue data and has been implemented in a dialogue system that is in turn evaluated by users. The setting is that of a Map Task, in which the user describes a route and the system may respond with, for example, acknowledgements or clarification requests. The presented approach exemplifies a boot-strapping procedure where more and more advanced versions of the system are built iteratively. After each iteration, users interact with the system and data is collected, which is then used to improve the data-driven models in the system. In this article, we extend the analysis by exploring the use of other classifiers, testing the robustness of a model against ASR errors, and further analysing the objective and subjective performance of the trained model used in user evaluation.

In Section 2, we discuss previous studies on cues that human interlocutors use to manage turn-taking and backchannels. We will also discuss some of the proposed computational models. In Section 3, we describe the test bed that we used for boot-strapping a Map Task dialogue system to collect data and develop an improved incremental version of the system. In Section 4, we will discuss the various data-driven models that we have trained in this work. We describe the various features we have explored, and discuss their performance using various learning algorithms on our data for online use. In Section 5, we discuss the subjective and objective evaluation schemes used for verifying the contributions of a trained model in user interactions. Finally, in Section 6, we discuss the key contributions and limitations of the models presented in this paper, and conclude with some ideas for future extensions of this work.

@&#BACKGROUND@&#

Two influential theories that have examined the turn-taking mechanism in human conversations are the signal-based mechanism of Duncan (1972) and the rule-based mechanism proposed by Sacks et al. (1974). According to Duncan, “the turn-taking mechanism is mediated through signals composed of clear-cut behavioural cues, considered to be perceived as discrete”. Duncan identified six discrete behavioural cues that a speaker may use to signal the intent to yield the turn, involving prosody, syntax and gestures. Speakers may display these behavioural cues either singly or together, and when displayed together the likelihood that the listener attempts to take the turn increases. According to the rule-based mechanism of Sacks et al. (1974) turn-taking is regulated by applying rules (e.g. “one party at a time”) at Transition-Relevance Places (TRPs) – possible completion points of basic units of turns, in order to minimize gaps and overlaps. The basic units of turns (or turn-constructional units) include sentential, clausal, phrasal, and lexical constructions.

While these theories have offered a function-based account of turn-taking, another line of research has looked into corpora-based techniques to build models for detecting turn-transition and feedback relevant places in speaker utterances. Ward (1996) suggested that a 110 millisecond (ms) region of low pitch is a fairly good predictor for backchannel feedback in casual conversational interactions. He also argued that more obvious factors, such as utterance end, rising intonation, and specific lexical items, account for less than they seem to. He contended that prosody alone is sometimes enough to tell you what to say and when to speak. Truong et al. (2010) extended this model by including pause information. They observed that the length of a pause preceding a backchannel is one of the important features in their model, next to the duration of the pitch slope at the end of an utterance.


                     Koiso et al. (1998) analyzed prosodic and syntactic cues to turn-taking and backchannels in Japanese Map Task dialogues. They observed that some part-of-speech (POS) features are strong syntactic cues for turn-change, and some others are strongly associated with turn-holds. Using manually extracted prosodic features for their analysis, they observed that falling and rising F0 patterns are related to changes of turn, and flat, flat-fall, and rise-fall patterns are indications of the speaker continuing to speak. Extending their analysis to backchannels, they suggested that syntactic features, such as filled pauses, alone might be sufficient to discriminate when back-channelling is inappropriate, whereas the presence of backchannels is always preceded by certain prosodic patterns.


                     Cathcart et al. (2003) presented a shallow model for predicting the location of backchannel continuers in the HCRC Map Task Corpus (Anderson et al., 1991). In their model they explored features such as word count in the preceding speaker utterance, POS tag, and silent pause duration. A model based on silent pauses only inserted a backchannel in every speaker pause longer than 900ms, and yet performed better than a baseline word model that predicted a backchannel every seventh word. A tri-gram POS model predicted that nouns and pronouns before a pause are the two most important cues for predicting backchannel continuers. The combination of the tri-gram POS model and pause duration model offered a 5-fold improvement over the baseline model.


                     Gravano and Hirschberg (2011) examined seven turn-yielding cues that occur with a significantly higher frequency in speaker utterances prior to a turn transition than those preceding turn holds. These events are: (i) a falling or high-rising intonation at the end of speaker turn; (ii) an increased speaker rate; (iii) a lower intensity level; (iv) a lower pitch level; (v) a longer duration; (vi) a higher value of three voice quality features: jitter, shimmer, and noise-to-harmonic ratios; and (vii) a point of textual completion. Gravano and Hirschberg (2011) also investigated whether backchannel-inviting cues differ from turn-yielding cues. They examined a number of acoustic and lexical cues in the speaker utterances preceding smooth turn-changes, backchannels, and holds. As in the case of turn transition, they identified six measureable events, but with different patterns then in the case of transitions, that are strong predictors of a backchannel at the end of an inter-pausal unit (IPU).

Another example is the line of work on building virtual rapport with animated agents, in which the agent should, for example, be able to produce head nods at relevant places. Using data on human–human interaction, Morency et al. (2010) investigated the use of multi-modal features (including gaze, prosody and transcribed speech) for estimating the probability of giving non-verbal feedback and obtained statistically significant improvement over an approach based on hand-crafted rules.

When it comes to using these features for making online turn-taking decisions in dialogue systems, however, there is very little related work. One notable exception is Raux and Eskenazi (2008) who presented an algorithm for dynamically setting endpointing silence thresholds based on features from discourse, semantics, prosody, timing, and speaker characteristics. The model was also applied and evaluated in the Let's Go dialogue system for bus timetable information. However, that model only predicted the endpointing threshold based on the previous interaction up to the last system utterance; it did not base the decision on the current user utterance to which the system response is to be made. Another case is Huang et al. (2011) who presented a virtual interviewer which used a data-driven backchannel model (for producing head nods) in combination with a handcrafted turn-taking model (for asking the next question). An evaluation showed that subjects did indeed experience the turn-taking capabilities of the system as better compared to a baseline system. However, their system did not explore if word-level features play any role in predicting backchannels.

In order to improve current systems, we need a better understanding of the phenomena of human interaction, better computational models, and better data to build these models. As the review above indicates, a common procedure is to collect data on human–human dialogue and then train models that predict the behaviour of the interlocutors. However, we think that it might be problematic to use a corpus of human–human dialogue as a basis for implementing dialogue system components. One problem is the interactive nature of the task. If the system produces a slightly different behaviour than what was found in the human–human training data, this would likely result in a different behaviour in the interlocutor. Another problem is that it is hard to know how well such a model would work in a dialogue system, since humans are likely to behave differently towards a system as compared to another human (even if more human-like behaviour is being modelled). Yet another problem is that much dialogue behaviour is optional and therefore makes the actual behaviour hard to use as a gold standard. Indeed, although many of the classifiers in the studies reported above show a better performance than baseline, they typically have a fairly low accuracy or F-score. It is also possible that much of human behaviour that is “natural” is not necessarily preferable for a dialogue system to reproduce, depending on the purpose of the dialogue system.

A common practice for collecting realistic human–computer interaction data in the absence of a working prototype is to use a Wizard-of-Oz setup. A human wizard operates “behind the curtain” while the users are made to believe that they are interacting with a real dialogue system. While this methodology has proven to be useful, it has its limitations. For example, the wizard's performance may not be consistent (across users or even for the same users) (Dahlbäck et al., 1993). The responsiveness of the wizard in responding to user behaviour is another issue, which makes the method hard to use when the issue under investigation is time-critical behaviours such as turn-taking and backchannels.

An alternative to Wizard-of-Oz studies is using a “boot-strapping” procedure, where more and more advanced (or human-like) versions of the system are built iteratively. After every iteration, users interact with the system and data is collected. This data is then used to improve the data-driven models in the system. A problem here, however, is how to build the first iteration of the system, since many components, such as Automatic Speech Recognition (ASR), need some data to be useful at all. In a previous study, we presented a test-bed for collecting realistic human-computer interaction – a fully automated spoken dialogue system that can perform the Map Task with a user (Skantze, 2012). By implementing a trick, the system could convincingly act as an attentive listener, without any speech recognition. The data from user interaction with the system was used to train an online model of Response Location Detection (RLD). Based on automatically extractable prosodic and contextual features, 200ms after the end of the user's speech, the trained model was able to identify response locations with a significantly higher accuracy as compared to the majority class baseline. The trained model was, however, not evaluated in user interactions.

In this paper, we extend the approach presented in Skantze (2012) in the following ways: first, we use an ASR component in order to model lexico-syntactic features. Second, we explore a range of automatically extractable features for online use – covering prosody, lexico-syntax, and context – and different classes of learning algorithms. We explore the contribution of each of these modalities to the task of RLD, individually as well as in combination. Third, we evaluate one of the proposed models by integrating it in the same system that was used for data collection, and testing the model in live interaction with users.

The Map Task is a common experimental paradigm for studying human–human dialogue, where one subject (the information giver) is given the task of describing a route on a map to another subject (the information follower). In our case, the user acts as the giver and the system as the follower. The choice of the Map Task is partly motivated because the system can allow the user to keep the initiative during the whole dialogue, and thus only produce responses that are not intended to take initiative, most often some kind of feedback.

Implementing a Map Task dialogue system with full speech understanding would indeed be a challenging task, given the state-of-the-art in automatic recognition of conversational speech. In order to make the task feasible, we have implemented a trick: the user is presented with a map on a screen (see Fig. 1
                     ) and instructed to move the mouse cursor along the route as it is being described. The user is told that this is for logging purposes, but the real reason for this is that the system tracks the mouse position and thus knows what the user is currently talking about. It is thereby possible to produce coherent system behaviour without any speech recognition at all, but only using voice activity detection (VAD). This often results in a very realistic interaction, as compared to what users are typically used to when interacting with dialogue systems – in our experiments, several users thought that there was a hidden operator behind it.
                        1
                     
                     
                        1
                        An example video can be seen at http://www.youtube.com/watch?v=MzL-B9pVbOE.
                     
                  

The system is implemented using the IrisTK dialogue system framework (Skantze and Al Moubayed, 2012). The basic components of the system can be seen in Fig. 2
                     . The system uses a simple energy-based speech detector to chunk the user's speech into inter-pausal units (IPUs), that is, periods of speech that contain no sequence of silence longer than 200ms. Such a short threshold allows the system to give backchannels (seemingly) while the user is speaking, or take the turn with barely any gap. Similarly to Gravano and Hirschberg (2011) and Koiso et al. (1998), we define the end of an IPU as a candidate for the Response Location Detection (RLD) model to identify as a response location. We use the term turn to refer to a sequence of IPUs which do not have any interlocutor responses between them.

Since we initially did not have any sophisticated model of response location detection, the system was simply set to wait for a random period between 0 and 800ms after an IPU ended. If no new IPUs were initiated during this period, a response location was detected, resulting in random response delays between 200 and 1000ms. Each time the RLD model detected a response location, the dialogue manager produced a Response, depending on the current state of the dialogue and the position of the mouse cursor. Table 1
                      shows the different types of responses the system could produce. The dialogue manager always started with an Introduction and ended with an Ending, once the mouse cursor had reached the destination. Between these, it selected from the other responses, partly randomly, but also depending on the length of the last user turn and the current mouse location. Longer turns often led to Restart or Repetition Requests, thus discouraging longer sequences of speech that did not invite the system to respond. If the system detected that the mouse had been in the same place over a longer time, it pushed the task forward by making a Guess response. We also wanted to explore other kinds of feedback beyond backchannels, and therefore added short Reprise Fragments and Clarification Requests (see for example Skantze (2007) for a discussion on these).

Ten subjects participated in the data collection. Although the task was conducted in English, no participants were native speaker (though proficient). They were seated in front of the display showing the map, wearing a headset. The instructor told them that they were supposed to describe a route to the computer. They were told that they should imagine the system having a similar picture as seen on the screen, but without the route. Each subject did five consecutive tasks with five different maps, resulting in a total of 50 dialogues. Table 2
                         shows an example interaction with the system.

The users’ speech was recorded and all events in the system were logged. Each IPU in the corpus was manually annotated into three categories: Hold (a response would be inappropriate), Respond (a response is expected) and Optional (a response would not be inappropriate, but it is perfectly fine not to respond). To validate the coding scheme two human annotators labelled 20% of the corpus separately. For all three categories the kappa score was 0.68, which is substantial agreement (Landis and Koch, 1977). Since only 2.1% of all the IPUs in the corpus were identified for category Optional they would not have been sufficient for statistical learning of this class. Therefore we excluded them from the corpus and used the data instances for the Respond and Hold categories only. The dataset contains 2272 IPUs in total, the majority of which belong to the class Respond (50.7%), which we take as our majority class baseline. Since the two annotators agreed between Respond and Hold in 87.2% of the cases, this can be regarded as an approximate upper limit for the performance expected from a model trained on this data.

In contrast to some related work (e.g. Koiso et al., 1998), we do not discriminate between locations for backchannels and turn-changes. Instead, we propose a general model for response location detection. Given the nature of the task, the system only produces utterances that are not intended to take the initiative or claim the floor. Rather the utterances provide different types of feedback (cf. Table 1). Thus, suitable response locations will be where the user invites the system to give feedback, regardless of whether the feedback is simply an acknowledgement that encourages the user to continue, or a clarification request. Moreover, it is not clear whether the acknowledgements the system produces in this domain should really be classified as backchannels, since they do not only signal continued attention, but also that some action has been performed (cf. Clark, 1996).

Another assumption behind the current model is that the system will only consider response locations at the end of IPUs. While other models have applied continuous decisions for producing backchannels (e.g. Ward, 1996), we follow the approach taken in many of the related studies mentioned above (e.g. Koiso et al., 1998; Gravano and Hirschberg, 2011). This is again partly motivated by the fact that the acknowledgements produced by the system should perhaps not be considered as backchannels. Indeed, none of the annotators felt the need to mark relevant response locations within IPUs.

We used the human–machine Map Task corpus described in the previous section for training various new models for response location detection. Here, we discuss the various prosodic, contextual and lexico-syntactic features that we explored while training these models. We describe how we extracted the feature values from the IPUs, and also present the contributions of the features – individually as well as in combination – in classifying user IPUs as either Respond or Hold type. As for the choice of models, we explored four data-driven models: the Naïve Bayes classifier (NB) as a generative model, and three discriminative models, namely a J48 decision tree classifier, a Support Vector Machine (SVM) with radial basis kernel function, and a Voted Perceptron (VP). We compare the performances of these models against the majority class baseline of 50.7%. For all three classifiers we used the implementations available in the WEKA toolkit (Hall et al., 2009). All results presented here are based on 10-fold cross-validation.

We extracted pitch and intensity (sampled at 10ms) for each IPU using ESPS in Wavesurfer/Snack (Sjölander and Beskow, 2000). The values were transformed to log scale and z-normalized for each user. We then identified the final 200ms voiced region for each IPU. For this region, the mean pitch and pitch slope (using linear regression) were calculated. We used mean pitch in conjunction with its absolute value as a feature. Pitch slope was also used as a feature in combination with its correlates such as the correlation coefficient 
                        
                           r
                         for the regression line and the absolute value of the slope. In addition to these, we also used the duration of the voiced region as a feature. The last 500ms of each IPU were used to obtain the mean intensity and intensity slope measures. As with the pitch features, we used these two measures in combination with the absolute value and the two correlates of slope, respectively.

The individual and collective performances of these prosodic features for the four classifiers in identifying user IPUs as either Respond or Hold type is presented in Table 3
                        . The combined pitch features offer the best accuracy of 66.2% using the SVM classifier. Using the intensity features in combination, the best accuracy of 60.7% is obtained by the J48 classifier. All the prosodic features used together offer the highest accuracy of 66.9% using the SVM classifier.

We explored discourse context features such as turn and IPU length (in terms of duration in seconds), and last system dialogue act. We also used the pause duration between the onset of a user IPU and the end of previous user/system IPU as a feature. The values for the duration features were automatically extracted from the output of the voice activity detector (VAD). These values are therefore subject to the performance of the VAD component. Table 4
                         shows the performance of these contextual features, individually as well as in combination, in discriminating user IPUs as either Respond or Hold type. In general, all features offer an improvement over the baseline accuracy of 50.7%. We observed that the IPU length feature generally appears to offer slightly better performance compared to the turn length feature. Using all the contextual features together the best accuracy, 63.7%, is achieved by the Support Vector Machine classifier. This is significantly better than the baseline.

Various studies have observed that dialogue act history information is a significant cue for predicting a listener response when the speaker has just responded to the listener's request for clarification (Koiso et al., 1998; Cathcart et al., 2003; Gravano and Hirschberg, 2011; Skantze, 2012). Some of the rules learned by the J48 decision tree classifier suggest that this pattern is also observable in our Map Task corpus. One of the rules states: if the last system dialogue act is Clarification or Guess (cf. Table 1) and the turn word count is less than or equal to 1, then Respond. In other words, if the system had previously sought a clarification, and the user has responded with a yes/no utterance, then the system must respond to acknowledge. A more general rule in the decision tree suggests that: if the last system dialogue act was a Restart or Repetition Request (cf. Table 1) and the turn word count is more than 4 then Respond otherwise Hold. In other words, having requested information from the user, the system should wait until it receives a certain amount of information from the user.

As lexico-syntactic features, we used word form of the last two words in an IPU, and explored its performance in combination with other related features such as part-of-speech tag, semantic tag, and the word-level confidence score obtained for the speech-recognized user utterances. To obtain the word form feature values all the IPUs in our Map Task corpus were manually transcribed. Filled pauses (e.g. ehm, uh-hun) were also orthographically transcribed and assigned the class tag FP. To obtain the part-of-speech tag information we used the LBJ toolkit (Rizzolo and Roth, 2010). In our training data we observed 23 unique POS tags. We added the FP tags to this set of POS tags. The five most frequent POS tag patterns for the last two words in IPUs corresponding to the Respond and Hold categories are shown in Table 5
                        . The differences in the phrase final POS tag patterns and their respective frequencies suggest that some POS tag patterns are strong cues for discriminating the Respond and Hold type user IPUs.

The discriminatory power of the word form and part-of-speech tag features, corresponding to the four classifiers, is presented in Table 6
                        . The figures under column sub-heading “Text” are accuracy scores achieved on feature values extracted from the manual transcriptions of the IPUs. Using only the last word a best accuracy of 83.9% was obtained by the SVM classifier. The addition of the penultimate word generally does not result in any further improvement. Using the last two words, the Voted Perceptron classifier achieved a best accuracy of 83.1%. The POS tag feature for the last two words in IPUs offer a best accuracy of 81.4% with the J48 classifier. While POS tag is a generic feature that would enable a model to generalize, using word form as a feature has the advantage that some words, such as yeah, are strong cues for predicting the Respond class, whereas fillers, such as ehm, are strong predictors of the Hold class.

A model for online prediction of response locations requires that the values of the lexico-syntactic features word form and POS tag are extracted from the output of a speech recognizer. Since speech recognition is prone to errors, a model trained on manual transcriptions alone – suggesting perfect recognition – would not be robust when making predictions on noisy data. For this reason, we trained and tested our models on actual speech recognized results. To this end, we did an 80–20 split of the Map Task corpus into training and test sets, respectively. The manual transcriptions of IPUs in the training set were used to train the language model of an off-the-shelf ASR system.
                           2
                        
                        
                           2
                           Due to licencing terms we cannot disclose the name of the ASR system.
                         The trained ASR system was then used to recognize the audio recordings of the IPUs in the test set. After performing five iterations of splitting, training and testing, we had obtained speech-recognized results for all of the IPUs in the Map Task corpus. The mean Word Error Rate (WER) for the five iterations was 17.3% (SD
                        =4.45%).

The performances of the lexico-syntactic features word form and POS tag, extracted from the best speech recognized hypotheses for the IPUs, are presented under the columns with sub-heading “ASR” in Table 6. With the introduction of a word error rate of 17.3%, the performances of all the models using the feature word form slightly decline, as expected (cf. rows 1 and 2). However, in contrast to this, the corresponding decline in accuracy of the model using the POS tag feature alone is much larger (cf. row 3). This is because the POS tagger itself uses the left context to make POS tag predictions. With the introduction of errors in the left context, the tagger's accuracy is affected, which in turn affects the accuracy of these models for detecting response locations. This performance is bound to decline even further with an increase in ASR errors. In order to identify the correlation between the ASR WER and the performance of our models, we first obtained five different ASR performances by using increasingly smaller training sets in the iterative process described above.

The performance of the J48 classifier using word form and POS tag features compared to these five ASR performances is illustrated in Fig. 3
                        . The results corresponding to 0 on the WER axis reflect the classifier's performance on feature values extracted from manual transcriptions. We can observe that while the performance of the model declines with an increase in WER, word form as a feature offers constantly better performance in contrast to using the POS tag feature alone. This suggests that using context-independent lexico-syntactic features would offer better performance for an online model for response location detection. We therefore created a word class dictionary, which generalises words into domain-specific semantic tags or classes in a simple way (much like a class-based n-gram model). The semantic tags used in our dictionary were based on the domain ontology used in our earlier work on automatic semantic interpretation of verbally given route descriptions (Meena et al., 2012a). For words that were not specific to this domain, their most frequent POS tag was used as a semantic tag. As a result, in our dictionary we had 12 classes, 4 of which were domain-specific (illustrated in Table 7
                        ), and the remaining 8 of which were POS tags.

The performance of the semantic tag feature for the last two words corresponding to four classifiers is presented in row 4 in Table 6. The best accuracy using the semantic tag feature was achieved by the J48 classifier, 83.4% on manual transcriptions and 79.4% on ASR results. These figures are better than the corresponding performances of J48 classifier using the POS tag feature only, 81.4% and 76.5% respectively. The classifier performances on ASR results, suggest that using the semantic tag feature instead of POS tag (cf. row 3) generally improves the performance of the model for online predictions of response locations. This is also evident in Fig. 3, where we observe that the semantic tag feature consistently performs better than the POS tag feature despite the increase in ASR errors. An additional advantage of using a semantic tag feature over word form is that new examples could be easily added to the model without having to retrain it. However, a model trained in this manner would be domain-specific.

We explored the word-level confidence scores (ASR wConf) from the ASR module as another lexico-syntactic feature that could possibly reinforce a classifier's confidence in trusting the recognized words. Using the word-level confidence score in combination with other lexico-syntactic features does not lead to any improvements over using word form alone (cf. Table 6, column “ASR”, rows 2 vs. 4). The performance graph for this feature combination in Fig. 3 also illustrates this observation.

The best accuracy for a model of response location detection, using lexico-syntactic features extracted from manual transcriptions, is achieved by the Voted Perceptron classifier (84.4%, using the features word form and semantic tag). For an online model of RLD, the best performance, 82.0%, is again achieved by the Voted Perceptron classifier using the features word form, semantic tag, and ASR word-level confidence score.

Next, we explored the performance of various model combinations among the three feature categories. Table 8
                         shows the contribution of these categories – individually as well as in combination – in discriminating Respond and Hold type user IPUs. The top three rows show the best individual performances of the three categories. Since the prosodic and contextual features that we have used are independent of ASR WER, the performances of these feature categories remain unaffected despite the introduction of an ASR WER of 17.32% (sub-column “ASR” in Table 8). For the models using lexico-syntactic features (Lex-Syntax), the figures in the “Text” column do not include the word-level confidence score feature, as it is not available for manual transcriptions.

All three feature categories offer significantly improved performance over the baseline of 50.7%, and the model performances achieved through combining prosodic and contextual features exhibit improvement over using feature categories individually (cf. row 4, Table 8). The best accuracy for a model using context and prosody in combination, 69.8%, is achieved by the SVM learner. Lexico-syntactic features alone provide a large improvement over prosody and context categories. Using prosody in combination with lexico-syntax, the Naïve Bayes model achieves the best accuracies – 84.6% on feature values extracted from manual transcriptions and 81.7% on values extracted from ASR results. Using prosody, context and lexico-syntax in combination, the J48 classifier achieves the best accuracies, 84.2% on manual transcriptions and 82.4% on ASR output. These figures are significantly better than the majority class baseline of 50.7% and approach the expected upper bound – the inter-annotator agreement of 87.2% on Hold and Respond types in our Map Task corpus.

We performed significance tests on the performances of J48 classifier for some of the feature category combinations shown in Table 8. For these tests we used performance scores obtained from 10 repetitions of 10-fold cross-validation using the Weka toolkit. Table 9
                         shows the results of these tests. Since the majority class baseline accuracies were not normally distributed, we applied the Mann–Whitney U test to compare the model performances with the baseline. For the remaining tests, a two-tailed t-test for independent samples was applied. The Mann–Whitney U test suggests that using compare prosodic features alone results in significant improvement over the majority class baseline of 50.7%. Prosody achieves significantly better performance compared to context alone. Using prosodic and contextual features in combination offers significant improvement in performance over prosodic features alone. Lexico-syntactic features alone offer significant performance gain over using prosodic and context features together. Using prosody in addition to lexico-syntax does not offer any significant improvement over using lexico-syntactic features alone. However, when context is used as an additional feature the resulting gain in performance is significant.

We also tested the performance of the J48, NB, SVM and VP classifiers against the majority class baseline, for significance. A Kruskal–Wallis H test indicates that all classifier performances differ significantly (H(4)=364.3, p
                        <0.001). A pairwise (k-sample nonparametric) test indicates that the four classifiers perform significantly better than the baseline (p
                        <0.001). Among the four learners, all learner performances except for those of SVM and Voter Perceptron (p
                        =0.393), and Naïve Bayes and J48 (p
                        =0.262), are significantly different, with p
                        <0.001 for the remaining pairs.

An ideal model would have a high precision and recall for both Respond and Hold prediction classes. Table 10
                         shows the precision (proportion of correct decisions in all model decisions), recall (proportion of all relevant decisions correctly made) and F-measures for the baseline model and the J48 classifiers, using all the three feature categories together, with feature values extracted from ASR results. The J48 classifier appears to balance both recall and precision, and has the highest F-values: 0.84 for Respond and 0.81 for Hold.

While the t-test results in Table 9 suggest that the gain in model performance achieved by using prosodic and contextual features in addition to lexico-syntactic features is significant, it is worthwhile to investigate when and which contributions were made by these two feature categories in the model decisions. The role of prosodic features is emphasized when syntax alone cannot disambiguate (Koiso et al., 1998) or when errors in speech recognition impair the strength of lexico-syntactic features. Fig. 4
                         shows the performances of the J48 classifier corresponding to various ASR WERs, and using various feature category combinations. As expected, the performance of the model using only lexico-syntactic features declines (linearly, R
                        2
                        =0.91) with an increase in WER. In contrast, the models using only prosody or only context, or prosody and context in combination, are unaffected by an increase in ASR WER. Thus, prosody and context may provide features that are robust to noise, in contrast to lexico-syntactic features. This is demonstrated by the performance curve of the model combining prosodic, context, and lexico-syntactic features. With a WER of 49.0%, the J48 classifier achieves an accuracy of 71.4%, using only lexico-syntactic features; however, together with the model combining prosodic and contextual features (an accuracy of 68.1%), the model achieves an accuracy of 74.7% – an absolute performance gain of 3.3%, which is substantial.

An example illustrates the role of prosodic features when lexico-syntactic features could be misleading. The user phrase “pass through” (also an IPU) was recognized as “bus tunnel” (against the ASR WER of 49.0%). As the user phrase is syntactically incomplete, the expected model decision is a Hold. However, using the erroneous ASR output, which is syntactically complete, the lexico-syntactic model falsely identified the IPU as Respond type. The prosodic model, on the other hand, correctly classified the user phrase as Hold type. The model using both lexico-syntactic and prosodic features correctly classified the user IPU as a Hold type.

At times, the prosodic model led to incorrect model decisions as well. This is largely due to our simplistic method for extraction of prosodic feature values. For example, the user utterance “then you will get to a corner where you have a church on your right” was recognized as “then the way yes the before way you eh the church all and garage.” The expected model decision is to classify this user IPU as Respond type. The lexico-syntactic model correctly identified the user IPU as Respond type. However, the prosodic model incorrectly identified it as Hold type. The model using both lexico-syntactic and prosodic features also falsely classified the IPU as Hold type, suggesting that prosody may overrule syntax.

As regarding the contribution of context to the model combinations, context plays a role when neither syntax nor prosody can disambiguate. As an illustration, the user utterance “go south until you reach the pedestrian crossing” was expected to be classified by the model as Respond type. The utterance was recognized as “go south continue the church the the the station go see.” The lexico-syntactic and the prosodic models incorrectly identified the ASR recognized user utterance as Hold type. The model combining lexico-syntax and prosody also incorrectly identified it as Hold type. However, the addition of contextual features to this model resulted in classifying the utterance as a Respond type. The IPU and turn length features contributed to this decision, suggesting that longer user turns should be followed by a system response. Two other instances where context made clear contributions are (i) the model decision to respond to acknowledge the user's response to a previously asked clarification question, and (ii) the model decision to wait (a Hold) for some information from the user, after having requested the user to repeat the instructions (cf. Section 4.2).

The best accuracy of 82.4% in discriminating user utterances as Respond and Hold type, achieved by the J48 classifier using prosodic, contextual and lexico-syntactic features extracted from ASR results (with 17.3% WER), is significantly better than the majority class baseline performance of 50.7%. Would such a trained model also be perceived as significantly better in managing smooth interactions with real users? In order to evaluate the usefulness of a trained model in real user interactions we conducted a user evaluation at an early stage of this work. Two versions of the Map Task dialogue system that was used to collect the corpus (cf. Section 3) were created. One version used a Random model, which made a random choice between Respond and Hold type when classifying the end of a user IPU as a response location. The Random model thus approximated our majority class baseline. Another version of the system used a Trained data-driven model to make the classification decision. The Trained model used seven features in total: four prosodic features (mean pitch, pitch slope, pitch duration, and mean intensity), two contextual features (turn word count, and last system dialogue act), and two lexico-syntactic features (word form, and ASR word-level confidence score). A Naïve Bayes classifier trained on these seven features achieved an accuracy of 84.6% on manual transcriptions (excluding the feature word-level confidence score) and 82.0% on speech-recognized results. For both models, a decision was made whenever the system detected a silence of 200ms. If the model decision was a Hold, the system waited 2s and then responded anyway if no more speech was detected from the user. Fig. 5
                      illustrates the components of the dialogue system using the Trained RLD model. In addition to the components used in the in the first iteration of the system during data collection (cf. Fig. 2), this system had three new components: an ASR module for online extraction of lexico-syntactic features, a prosodic analysis component for online extraction of prosodic features, and a module to retrieve dialogue act history.

We hypothesize that since the Random model makes random choices, it is likely to produce false-positive responses as well as false-negative responses in equal proportion. While false-positive responses would result in occasional overlaps and interruptions in interactions, false-negative responses would result in gaps, delayed responses or simultaneous starts during the interactions. The Trained model, in contrast, should produce fewer overlaps and gaps, which would provide for smoother interactions and enhanced user experience. Users interacting with both models should be able to tell the systems apart based on the appropriate timing of system responses.

In order to evaluate the two models, 8 subjects (2 female, 6 male) were asked to perform the Map Task with the two systems. Subjects were recruited from the school of Computer Science and Communication at KTH. Each subject performed five dialogues in total. This included one trial session with the system using the Trained model and two test sessions in which the subjects interacted with both versions of the system, one after the other. At the end of the first test session, subjects responded to a questionnaire asking them about which of the two systems responded at appropriate places (the timing), and which systems was more responsive. At the end of the second test session, subjects again provided feedback on the same questionnaire, but were asked to respond with the overall experience from both test sessions in mind. The purpose of the feedback questionnaire after the first test session was to prime subjects’ attention to the issues under evaluation. The trial session was used to allow the users to familiarize themselves with the dialogue system. Also, the audio recording of the users’ speech from this session was used to normalize the user pitch and intensity for the online prosodic extraction. The order in which the systems and maps were presented to the subjects was varied over the subjects to avoid any ordering effect in the analysis.

We obtained 16 test dialogues each for the two systems. The 32 dialogues were, on average, 1.7min long (SD
                     =0.5min). The duration of the interactions with the Random and the Trained model were not significantly different. Both the models made about the same number of decisions: a total of 557 IPUs by the Random model and 544 IPUs by the Trained model. While the Trained model classified 57.7% of the IPUs as Respond type the Random model classified only 48.2% of the total IPUs as Respond type, suggesting that the Random model was somewhat quieter.

It turned out that it was very hard for the subjects to perform the Map Task and at the same time make a valid subjective comparison between the two versions of the system. Some subjects reported that when responding to the questionnaire they were unable to recall their impressions of the first system they had interacted with, or their experience from the first session at the end of second test session. From their oral feedback, we had the impression that at least some subjects based their judgements on the type of the response and not on response timing. For these reasons, we did not analyze the user feedback any further, and instead conducted another subjective evaluation to compare the two systems. We asked subjects to listen to the Map Task user interactions from the user evaluation and press a key whenever a system response was either lacking or inappropriate. The subjects were asked not to consider how the system actually responded, but only to evaluate the timing of the response.

Eight users participated in the subjective judgement task. Although five of these were from the same set of users who had performed the Map Task in the user evaluation, none of them judged their own interactions. Our decision to not ask the participants to judge their own interactions was in order to avoid subjects’ bias towards their own performances. The judges listened to the Map Task interactions in the same order as the original interaction, including the trial session. Whereas it had been hard for the subjects who participated in the dialogues to characterize the two versions of the system, almost all of the judges could clearly tell the two versions apart, without being told about the properties of the two versions. They indicated that the Trained system provided for a smoother flow of dialogue compared to the Random system.

A total of 149 key presses for the Random model and 62 key presses for the Trained model were obtained. Since the judges were asked to simply press a key, we did not have access to the information whether a key was pressed due to perceived inappropriate response location (false-positive) or absolute lack of a system response (false-negative). To obtain this information we analyzed all the turn-transition instances where judges had pressed the key. The timing of the IPUs was aligned with the timing of the judges’ key presses in order to measure the number of IPUs that had been given inappropriate response decisions. We found that 11 instances of key press could be attributed to the failure of the voice activity detector in detecting user speech immediately after a system response or during the 2s (timeout) following a Hold decision. Although judges were instructed not to judge the system utterance on the basis of the type of the response, 4 key presses were identified against responses that we believe were at appropriate response locations, but with inappropriate response type. There were 4 instances of key press where the system had correctly detected a Hold, but the current position of mouse cursor on the destination landmark triggered a system response of type End (cf. Table 1), which was perceived as inappropriate by the judges. Two key press instances could not be associated with any IPU and no plausible reason could be identified as to why the key presses occurred. We excluded these 21 instances from our analysis as it would be inappropriate to hold the response location detection models responsible for these decisions. A Chi-Squared one-variable test suggests that the proportion of key presses received by the two systems (Random: 141 and Trained: 49) are significantly different 
                           
                              (
                              
                                 χ
                                 2
                              
                              =
                              44.54
                              ,
                               
                              d
                              f
                              =
                              1
                              ,
                               
                              p
                              <
                              .001
                              )
                           
                        . We can therefore conclude that the Trained model was perceived to have significantly fewer inappropriate turn-transition instances than the Random model.

Responding at an inappropriate time, could be ascribed to a false-positive (FP) decision by the model. Similarly, the lack of response from system when it is expected could be ascribed to a false-negative (FN) decision by the model. Fig. 6
                         illustrates the distribution of the perceived FP and FN decisions made by the Random and the Trained models. A Chi-squared test of independence of categorical variables suggests that there is insufficient evidence to conclude that the number of key presses for one of the two models is influenced by the judges’ perception of FN or FP 
                           
                              (
                              
                                 χ
                                 2
                              
                              =
                              1.0
                              ,
                               
                              d
                              f
                              =
                              1
                              ,
                               
                              p
                              =
                              .317
                              )
                           
                        . In fact, both of the models seem to have received key presses for FN and FP decisions in equal proportion.

Responsiveness of a dialogue system has been identified as an important issue in turn-management that affects user experience (Ward et al., 2005). Delayed system responses, or lack of them, could be confusing for users. They have no idea whether the system is still processing their input and they should wait for system response, or whether the system has heard them at all, which may prompt them to repeat their utterances. Repeated user utterances may increase processing overhead for the system, which could result into even longer processing and response time. As mentioned earlier, it was difficult for the subjects who interacted with the Map Task system in the user evaluation to give proper feedback regarding the responsiveness of the systems. However, the judges in the perception test were able to perceive delayed system responses. While Fig. 6 suggests that both systems received key presses for the FN and FP model decisions in equal proportion, we observed that the tendencies to press a key when the response did not appear in expected places (or was delayed) varies across judges. Our intuition is that some judges adapted to the delay in system responses (on timeout after an actual FN model decision), and therefore didn’t press the key, thereby causing the actual false-negative model decisions to be accounted as perceived true-positives. However, it would be preferable if the system could reply as quickly as possible. Therefore, we additionally compared the two system versions by measuring the response time for responses that were perceived as true positives.

Among the perceived true-positive system responses, a total of 267 for the Random and 312 for the Trained model, we identified two categories: responses which were produced timely (actual true-positive model decision) and those which were delayed (on timeout after an actual false-negative model decision). The mean response time for the timely responses was 301ms (SD
                        =2ms) whereas for the delayed responses it was 2324ms (SD
                        =4ms). These figures include the 200ms silence threshold for triggering the response location detection task, and the additional 2s of wait in case of FN decisions. Our system requires on average 100ms for processing and decision making. Table 11
                         shows the distribution of timely and delayed responses for the Trained and the Random model. A Chi-Squared test suggests that the mean number of timely and delayed responses for the two models differ significantly 
                           
                              (
                              
                                 χ
                                 2
                              
                              =
                              27.844
                              ,
                               
                              d
                              f
                              =
                              1
                              ,
                               
                              p
                              <
                              .001
                              )
                           
                        . We can therefore conclude that the Trained system has significant faster response time than the Random system.

An ideal data-driven model would have both high precision and high recall (i.e. a high F-measure). Table 12
                         shows that on the training dataset the precision, recall and F-measure of the Naïve Bayes model for online decision-making were significantly better than to the majority class baseline model. To measure whether similar performance was achieved by the model during the user evaluation (live usage) a performance standard is needed. One way to obtain this is for the annotators who labelled the training data to annotate the user evaluation interaction data as well. However, such an evaluation would, at best, help confirm whether the models’ live performance agreed with the annotators. An alternative – which is perhaps stronger and more viable – is to use the judges’ key press feedback from the subjective evaluation as standards.

In Section 5.1, we used judges’ key presses to identify perceived false-positive and false-negative system decisions. We considered the remaining instances of system responses as True-Positive (TP) and system holds as True-Negative (TN). Using the counts of FP, FN, TP and TN we obtained the perceived precision, recall and F-measure scores for the two models, as shown in Table 13
                        . When compared with Table 12 these figures confirm that during the real user interactions as well, the Trained model achieved better recall, precision and F-measure for both Respond and Hold type decisions, compared to the Random model.

We have presented a data-driven approach for detecting suitable response locations in user speech. We used human–computer interaction data that we collected using a fully automated dialogue system that can perform the Map Task with the user (albeit using a trick). Two annotators labelled all the user inter-pausal units (IPUs) with regard to whether a system response is appropriate at the end of each unit. We used automatically extractable features – covering prosody, context, and lexico-syntax – and trained various models (generative as well as discriminative) for online detection of feedback response locations. We tested the performances of these three feature categories, individually as well as in combination, for the task of response location detection. To evaluate the contributions of such a trained model in real interactions with users, we integrated a trained model in the same dialogue system that was used to collect the training data, and tested it in user interactions. The results from a perception test show that the trained model exhibited significantly fewer instances of inappropriate turn-transitions compared to a baseline model. We also found that the trained model is significantly better at being responsive in comparison to the baseline system. To our knowledge, this is the first work on actual verification of the contributions of the type of models proposed in the literature for modelling human-like turn-taking and back-channelling behaviour in dialogue systems. Our findings confirm that a model for turn-taking trained on prosodic, contextual and lexico-syntactic features offers both smooth turn-transitions and responsive system behaviour.

Our work differs from earlier work in many regards. First, in contrast to the traditional procedure of using human–human interaction data to model human-like turn-taking behaviour in dialogue systems, we used human–computer interaction data that we collected using a fully automated dialogue system. Second, most earlier work presented model performances on manual transcriptions only (Ward, 1996; Koiso et al., 1998; Cathcart et al., 2003; Gravano and Hirschberg, 2011; Morency et al., 2010). In contrast, we trained and demonstrated model performances based on automatically extractable features for online detection of response locations. Third, we explored the contributions of features pertaining to prosody, context, and lexico-syntax, whereas earlier works used features comprising only prosody (Ward, 1996), or combinations of lexico-syntax and prosody (Koiso et al., 1998), lexico-syntax and context (Bell et al., 2001; Cathcart et al., 2003), prosody and context (Skantze, 2012), or prosody, context, and semantics (Raux and Eskenazi, 2008), or gaze, prosody and lexico-syntax (Morency et al., 2010), and pause and gaze (Huang et al., 2011). Fourth, very few of the models proposed earlier have been tested in user interactions. An exception is Raux and Eskenazi (2008), who trained models for online endpointing of user turns on human–computer interaction data. However, they excluded prosodic features from the online model used for live user interactions. Also, to evaluate model performance, Raux and Eskenazi (2008) used latency as the objective measure, but no subjective evaluation was carried out. We have evaluated the usefulness of our data-driven approach by integrating a trained model in a dialogue system and testing it in real user interactions. We have presented both subjective as well as objectives metrics for evaluating the improvements achieved by the trained model in contrast to a baseline model. In another work, Huang et al. (2011) trained a data-driven model (using pause and gaze information) for production of backchannel head-nods, and evaluated it in user interactions. Their model, however, does not use word-level information for predictions.

We tested the contribution of prosodic, contextual and lexico-syntactic feature categories, individually as well in combination, for online detection of response locations. Using the contextual features alone, the Voted Perceptron (VP) classifier achieved an accuracy of 65.2%. This is a significant improvement over a majority class baseline of 50.7% in our data. Using the prosodic feature values extracted by our methods, a SVM classifier achieved an accuracy of 66.9%. While this performance is comparable to that achieved from using context alone, using prosody and context in combination offered the best accuracy of 69.5% using the Voted Perceptron classifier. This is again a significant improvement over the individual performances of the two feature categories. Using the lexico-syntactic features alone the best accuracy of 80.0% was achieved by the VP classifier taking into account an ASR WER of 17.3%. A model using prosodic, contextual, and lexico-syntactic features in combination achieved the best accuracy of 81.9% using the VP classifier. This is again significantly better than the majority class baseline of 50.7% and approached the expected upper bound – the inter-annotator agreement of 87.2% on Hold and Respond types in our Map Task corpus.

We found that lexico-syntactic features make a strong contribution to the combined model. This is consistent with earlier observations about their significant contribution in predicting turn-transition and backchannel relevant places (Koiso et al., 1998; Cathcart et al., 2003; Gravano and Hirschberg, 2011). It has also been observed in these studies that POS tag alone is a strong generic lexico-syntactic feature for making predictions on manual transcriptions. However, our results show that the contribution of the POS tag is reduced when using it online in dialogue systems, due to errors in speech recognition. This is because the POS tagger itself uses the left context to make predictions, and is not typically trained to handle noisy input. We have shown that using context-independent lexico-syntactic features, such as word form or semantic tag (which generalises the words into domain-specific semantic classes in a simple way, much like a class-based n-gram model) offers better and more robust performance despite speech recognition errors. However, this of course results in a more domain-dependent model.


                     Koiso et al. (1998) reported that prosodic features contribute almost as strongly to response location detection as lexico-syntactic features. We do not find such results in our data. This difference could be partly attributed to inter-speaker variation in our training data. All the users who participated in the collection of human–computer Map Task interaction data were non-native speakers of English. Also, our algorithms for extracting prosodic features are not as powerful as the manual extraction scheme used in Koiso et al. (1998). Although prosodic and contextual features do not seem to improve performance very much when lexico-syntactic features are available, they are clearly useful when no ASR is available (best accuracy of 69.8% as compared to the baseline of 50.7%, cf. Table 8) or when ASR performance is poor. As ASR WER approached 50% the performance of lexico-syntactic features approached the performance of the model combining both prosodic and contextual features (cf. Fig. 4).


                     Koiso et al. (1998), Cathcart et al. (2003), and Skantze (2012) have observed that the last speaker dialogue act is useful for predicting a listener feedback response to acknowledge the speaker's response to a previously asked listener clarification request. One of the rules learned by the J48 decision tree classifier on our human–computer interaction data corroborates this observation. We also found that inclusion of additional contextual features, such as turn and IPU duration, enhances the model's discriminatory power when syntax and prosody cannot disambiguate.

During the user evaluation users found it difficult to recall their experiences and make subjective comparison based on only the timing of system responses and not the response type. However, when these users listened to other users interactions, they were able to easily tell the two systems apart without being told about the properties of the two versions. Thus the perception test of user interactions helped us identify instances of perceivable incorrect model decisions. The results show that the Trained model produced significantly fewer instances of inappropriate turn-transitions in contrast to the Random model.

Obtaining subjective feedback from real users is nonetheless vital for complete evaluation of dialogue system behaviour. One possibility for evaluation in future studies could be to use the method of collecting users’ expectations and experiences first presented in Jokinen and Hurtig (2006), and used in Meena et al. (2012b). In this scheme, subjects first fill out a questionnaire that has dual purpose: (i) to obtain a measure of users’ expectations (on a Likert scale) regarding the system behaviour(s) under investigation; and (ii) to prime users’ attention to these behavioural aspects so that the users are conscious about what to evaluate. Next the users interact with one version of the system, following which they fill the same questionnaire again. However, this time they are asked to provide feedback on their experience. The user repeats this step for the remaining system versions. The users’ feedback from the questionnaires – on what they expect and what they actually experienced – could be then used to draw conclusions as to which system was perceived as being closer to their expectations.

Besides the issues with coordination of speaking turns, the responsiveness of a system is crucial for keeping the users engaged. We observed that in contrast to the Random model, the Trained model has a significantly larger number of responses that were produced as timely as possible, and significantly lower number of responses that were delayed. These results bring us to the conclusion that a model for feedback response location detection trained on features covering prosody, context, and lexico-syntax offers a dialogue system that is both more responsive and interrupts the user less often.

While one of our trained models has been shown to enhance the turn-management performance of the Map Task dialogue-system, there is still room for further improvement. In the current model, lexico-syntactic features appear to dominate the decision making. This is evident from the respective performances of these feature categories, as well as from user comments that the system appeared to respond well when they mentioned a landmark on the map. Our observation is that if the model could better exploit prosodic cues, it could enhance the system's responsiveness to other subtle, yet general behavioural cues present in users’ speech. In this work we have mainly used pitch- and intensity-related prosodic features that we could extract automatically for online use. Also, the algorithms used for extracting these feature values are very simple, and perhaps using better extraction methods would improve the model's performance.

Other prosodic features such as intonation patterns, speaking rate, and acoustic cues such as jitter, shimmer and noise to harmonic ratio have also been identified as useful behavioural cues for prediction of turn-taking and backchannel relevant places (Koiso et al., 1998; Gravano and Hirschberg, 2011). As tools for online extraction of these cues become available, they could easily be incorporated in the current models. Thus, the dataset from this experiment could serve as a test for evaluating the applied usefulness of new models for prosodic analysis.

A general limitation of data-driven approaches is sparseness in training data. Due to the limited number of tasks in the Map Task user interactions, it is plausible that users share a common lexical space. However, due to inter-speaker variations in prosodic realization and usage, the prosodic space is sparse, which makes it difficult for the models to generalize across users. It would be interesting to explore algorithms that are better at generalizing across speakers.

We have so far explored prosodic, contextual, and lexico-syntactic features for predicting response location. An immediate extension to our model would be to bring more general syntactic features in the model. The motivation for this is syntactically incomplete user utterances such as “and after the hotel…” Our current model using lexico-syntactic features only, on observing the “DT NN” phrase final pattern, would predict a Respond. However, syntactic knowledge suggests that the predicate is missing in the user utterance, and therefore the system should predict a Hold.

In a future version of the system, we do not only want to determine when to give responses but also what to respond. This would require processing at a higher level, more specifically understanding the semantics of spoken route descriptions, to play an active role in decision making. In Meena et al. (2012a) we presented a data-driven method for automatic semantic interpretation of verbal route descriptions into conceptual route graphs (CRG) – a semantic representation that captures the way humans structure information in route descriptions. A CRG with missing concepts or relations should suggest incompleteness, and therefore a Hold. However, the incompleteness could also be due to ASR misrecognitions. Perhaps the confidence scores from the ASR and the spoken language understanding component could be used to identify what to respond, and also to select between different forms of clarification requests and acknowledgements.

We would also like to test whether our models for response location detection will generalize to other domains. While both context and prosody offer features that are domain independent, POS tag would still be a more suitable lexico-syntactic feature (in contrast to word form and semantic tag features) for a domain independent model of RLD.

Another possible extension is to situate the Map Task interaction in a face-to-face Map Task between a human and a robot (similar to Skantze et al., 2013a) and add features from speaker's gaze, which has been identified as a visual cue that humans use to coordinate turn-taking in face-to-face interactions (Mutlu et al., 2009; Morency et al., 2010; Skantze et al., 2013b).

@&#ACKNOWLEDGMENTS@&#

This work is supported by the Swedish Research Council (VR) project Incremental processing in multimodal conversational systems (2011-6237). We would like to thank the participants of our user evaluation and perception tests. We would also like to thank our colleagues for their comments and feedback on this work.

@&#REFERENCES@&#

