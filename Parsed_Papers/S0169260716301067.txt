@&#MAIN-TITLE@&#Different medical data mining approaches based prediction of ischemic stroke

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We assessed different medical data mining approaches to predict ischemic stroke.


                        
                        
                           
                           Grid search were used for improving classification performance of the models.


                        
                        
                           
                           The accuracy and AUC values were higher than 0.8947 and 0.8953, respectively.


                        
                        
                           
                           SVM and SGB models yielded remarkable performance for classifying ischemic stroke.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Ischemic stroke

Medical data mining

Penalized logistic regression

Stochastic gradient boosting

Support vector machine

@&#ABSTRACT@&#


               
               
                  Aim
                  Medical data mining (also called knowledge discovery process in medicine) processes for extracting patterns from large datasets. In the current study, we intend to assess different medical data mining approaches to predict ischemic stroke.
               
               
                  Materials and methods
                  The collected dataset from Turgut Ozal Medical Centre, Inonu University, Malatya, Turkey, comprised the medical records of 80 patients and 112 healthy individuals with 17 predictors and a target variable. As data mining approaches, support vector machine (SVM), stochastic gradient boosting (SGB) and penalized logistic regression (PLR) were employed. 10-fold cross validation resampling method was utilized, and model performance evaluation metrics were accuracy, area under ROC curve (AUC), sensitivity, specificity, positive predictive value and negative predictive value. The grid search method was used for optimizing tuning parameters of the models.
               
               
                  Results
                  The accuracy values with 95% CI were 0.9789 (0.9470–0.9942) for SVM, 0.9737 (0.9397–0.9914) for SGB and 0.8947 (0.8421–0.9345) for PLR. The AUC values with 95% CI were 0.9783 (0.9569–0.9997) for SVM, 0.9757 (0.9543–0.9970) for SGB and 0.8953 (0.8510–0.9396) for PLR.
               
               
                  Conclusions
                  The results of the current study demonstrated that the SVM produced the best predictive performance compared to the other models according to the majority of evaluation metrics. SVM and SGB models explained in the current study could yield remarkable predictive performance in the classification of ischemic stroke.
               
            

@&#INTRODUCTION@&#

Ischemic stroke (IS) is associated with high mortality worldwide and is considered among the most important public health problems [1]. IS influences the management, diagnosis, and outcome. Treatments for acute IS should be made according to subtype of IS. Classification of subtypes for IS was arranged utilizing medical/clinical characteristics and the finding of supplementary clinical studies. The classification of Trial of Org in Acute Stroke Treatment (TOAST) defines five subtypes of IS: (1) big-artery atherosclerosis, (2) cardioembolism, (3) little-vein occlusion, (4) stroke of other identified etiology/causes, and (5) stroke of unidentified etiology/causes. The proposed rating system can determine etiologic diagnosis of IS in high proportions [2]. The important inference demonstrates the determination and prediction of causes and markers for the diagnosis and prevention of IS [1,2].

Data mining (also called knowledge discovery process) is a methodology for discovering hidden patterns from enormous datasets by using statistical approaches [3]. This methodology has many advantages compared to classical methods. For instance, in contrast to traditional statistical methods, data mining approaches require less presumptions in the classification and regression applications [4].

Alexopoulos et al. [5] applied inductive machine learning (ML) approaches in the diagnosis of stroke disease and used C4.5 algorithm by building a decision tree. These authors reported that inductive ML is a promising approach for computer-aided diagnosis of stroke. Linder et al. [6] used logistic regression (LR) and artificial neural networks (ANNs) for classifying acute ischemic stroke from the Database of German Stroke, and suggested that LR was the gold standard for the classification of acute ischemic stroke in comparison with ANNs, which may be employed as an alternative multivariate analysis. Khosla et al. [7] presented the comparison of the Cox proportional hazards model with a ML method for the prediction of stroke on the dataset of the Cardiovascular Health Study, and determined that combined with their suggested feature selection algorithm combined with support vector machine (SVM) achieved a higher area under the ROC curve when compared to the Cox proportional hazards model. In our previous study, ANNs and SVM were utilized to predict stroke disease using knowledge discovery process (KDP) approaches, and the results of the study determined that ANNs yielded more predictive performance as compared with SVM for the prediction of stroke and that the suggested ANNs might be beneficial for predictive purposes concerning stroke illness [3]. Additionally, there are some studies on ischemic stroke lesion segmentation using data mining or ML procedures [8–10].

The use of data mining approaches in many disciplines, especially in medicine, is increasing day by day. The medical application of data mining is called as medical data mining (MDM). Thence, MDM (also called knowledge discovery process in medicine) processes for extracting patterns from large datasets. In the current study, we intend to assess medical data mining approaches to predict ischemic stroke.

@&#MATERIAL AND METHODS@&#

This study which included 80 ischemic stroke patients (group I) and 112 healthy individuals (group II) was conducted in the department of emergency medicine, Turgut Ozal Medicine Center, Inonu University, Malatya, Turkey. Power analysis revealed that each group encapsulated minimum 68 individuals considering mean difference of creatinine for ischemic stroke patients and healthy individuals groups of 0.6, estimated standard deviations of 1.01 and 1.43, type I error (alpha) of 0.05 and type II error (beta) of 0.20. The definition of the variables that may associate with ischemic stroke [3,11,12] is summarized in Table 1
                        .

In the current study, outliers were detected by local density cluster-based outlier factor [13]. This technique employs a cluster algorithm and allocates clusters into small and big ones. The outlier factor was calculated by dividing minimum sample distance to average cluster distance of all samples to the big cluster [14]. X-means was utilized as clustering algorithm in this technique. Also, z-transformation (standardization) was applied to the dataset.

SVM is a supervised learning approach for classification and regression tasks and is utilized in order for linear/nonlinear classification problems with high-dimensional datasets [15]. To solve nonlinear classification problem, SVM maps the input sets to a high-dimensional space by applying various kernel functions [3]. A detailed explanation of SVM approach can be achieved in [16]. In this paper, SVM was employed with radial basis function (RBF) kernel function. SVM with RBF was applied by kernlab package [17] in R.

Boosting is an effective data mining ensemble meta-algorithm since it improves the prediction and classification performance of any learning approaches [18]. Stochastic gradient boosting (SGB) is a data mining approach presented by [19]. SGB is an important technique used for building prediction and classification tasks, and tunes predictive performance owing to implementation of preprocessing procedures. SGB was applied by gbm package [20] in R. More detailed definition of this method can be seen in [19].

The penalized log likelihood (PML) was maximized in a penalized logistic regression (PLR):
                           
                              
                                 
                                    PML
                                    =
                                    log
                                    (
                                    L
                                    )
                                    −
                                    0.5
                                    λ
                                    ∑
                                    
                                       
                                          
                                             (
                                             
                                                
                                                   (
                                                   s
                                                   )
                                                
                                                i
                                             
                                             
                                                β
                                                i
                                             
                                             )
                                          
                                          2
                                       
                                    
                                 
                              
                           
                        where L: the regular likelihood function, λ: a penalty factor, β
                        
                           i
                        : the predicted regression model coefficients and s
                        
                           i
                        : the scale factors. This prediction process reduces the coefficients of regression model toward zero, which increases the accuracy of novel predictions. The procedure of penalization decreases the estimated parameters and prevents overfitting difficulties [21]. PLR was implemented using stepPlr package [22] in R. More detailed information of this approach can be obtained from [23].

10-fold cross validation resampling method was employed to evaluate the model performance, and to obtain unbiased outputs and to take over fitting problem away. A seed number was determined randomly and used for each data partition process to ensure using the same training and the testing data in the modeling process. All of the modeling procedures were carried out under caret package [24] in R. The tuning parameters of the related classification models were optimized by grid search algorithm.

In the current study, the tuning parameters and their ranges of each models are tabulated in Table 2
                        . The optimal values of the tuning parameters were identified based on the testing accuracy values that were calculated for each fold and were averaged. After the optimal tuning parameters were discovered, the ultimate models were trained for the prediction.

In the current study, accuracy, area under receiver operating characteristic curve (AUC), sensitivity, specificity, positive predictive value and negative predictive value were utilized as model performance evaluation metrics [25]. These metrics are defined below:
                           
                              
                                 
                                    Accuracy
                                    =
                                    
                                       
                                          TP
                                          +
                                          TN
                                       
                                       
                                          TP
                                          +
                                          TN
                                          +
                                          FP
                                          +
                                          FN
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    Sensitivity
                                    =
                                    
                                       
                                          TP
                                       
                                       
                                          FN
                                          +
                                          TP
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    Specificity
                                    =
                                    
                                       
                                          TN
                                       
                                       
                                          FP
                                          +
                                          TN
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    Positive
                                       
                                    predictive
                                       
                                    value
                                    =
                                    
                                       
                                          TP
                                       
                                       
                                          TP
                                          +
                                          FP
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    Negative
                                       
                                    predictive
                                       
                                    value
                                    =
                                    
                                       
                                          TN
                                       
                                       
                                          TN
                                          +
                                          FN
                                       
                                    
                                 
                              
                           
                        where TP is the number of true positives, TN is the number of true negatives, FN is the number of false negatives and FP is the number of false positives [26].

@&#RESULTS@&#

Initially, the dataset was examined in terms of outliers. According to the outlier detection analysis, two observations were discarded from the data analysis. The remaining observations were 190 records used in the subsequent analysis. The study included 79 patients (41.6%) and 111 healthy persons (58.4%). The gender distribution of the study was 100 (52.6%) for men and 90 (47.4%) were women. The mean and standard deviation of age was 53.97±21.38 years.


                     Table 3
                      presents the detailed results of the performance metrics of each models with 95% CI. The accuracy values with 95% CI were 0.9789 (0.9470–0.9942) for SVM, 0.9737 (0.9397–0.9914) for SGB and 0.8947 (0.8421–0.9345) for PLR. The AUC values with 95% CI were 0.9783 (0.9569–0.9997) for SVM, 0.9757 (0.9543–0.9970) for SGB and 0.8953 (0.8510–0.9396) for PLR.


                     Table 4
                      gives the model based-variable importance values of the best classifier (SVM) which was selected by the majority of evaluation metrics. The most related variables with the ischemic stroke were ranked by the importance values from higher to smaller.

The model based-variable importance values of the best classifier are plotted in Fig. 1
                     . All the performance metrics of each model together with 95% CI values are illustrated in Fig. 2
                     . Also, Fig. 3
                      illustrates the comparison of the ROC curves for each model.

@&#CONCLUSIONS@&#

MDM is one of the main application areas where performance metrics are very important to evaluate the predictions of the models [27]. In the current study, different data mining approaches were constructed and proposed for the prediction of ischemic stroke. For this purpose, SVM, SGB and PLR models were explained and were compared based on several predictive performance metrics: accuracy, AUC, sensitivity, specificity, positive predictive value and negative predictive value. When the values of accuracy and AUC were considered, SVM had the highest predictions as compared to GBM and PLR. While the values of accuracy, AUC, sensitivity and negative predictive value of SVM were slightly higher than GBM, all of the performance metric values for SVM and GBM were considerably higher than PLR. The values of all the performance metrics concerning the models were quite high and may be acceptable for the classification of ischemic stroke. The results achieved from the current study indicated that the constructed SVM showed a remarkable predictive performance in the majority of the performance metrics. For obtaining much more accurate and robust comparison results, comprehensive simulation study is necessary.

Finally, the current study and our previous study [3] revealed that, computer-aided approaches such as medical data mining or medical knowledge discovery are an effective instrument in the prediction of ischemic stroke and explore the hidden relationships and associations in the datasets.

@&#ACKNOWLEDGMENT@&#

We would like to thank to the RapidMiner Academia Team so much for providing RapidMiner Studio Academia Edition free licence key.

@&#REFERENCES@&#

