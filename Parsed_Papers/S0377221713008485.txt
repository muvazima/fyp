@&#MAIN-TITLE@&#A variable neighborhood search with an effective local search for uncapacitated multilevel lot-sizing problems

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A new VNS algorithm for solving uncapacitated multilevel lot-sizing problems is developed.


                        
                        
                           
                           A local search called the Predecessors Depth-first Traversal Search is embedded in VNS.


                        
                        
                           
                           An approach for fast calculation of the cost change for the VNS algorithm is proposed.


                        
                        
                           
                           The experimental results show that the new VNS algorithm outperforms all of the existing algorithms.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Metaheuristics

Multilevel lot-sizing (MLLS) problem

ADTS local search

Variable neighborhood search (VNS)

@&#ABSTRACT@&#


               
               
                  In this study, we improved the variable neighborhood search (VNS) algorithm for solving uncapacitated multilevel lot-sizing (MLLS) problems. The improvement is twofold. First, we developed an effective local search method known as the Ancestors Depth-first Traversal Search (ADTS), which can be embedded in the VNS to significantly improve the solution quality. Second, we proposed a common and efficient approach for the rapid calculation of the cost change for the VNS and other generate-and-test algorithms. The new VNS algorithm was tested against 176 benchmark problems of different scales (small, medium, and large). The experimental results show that the new VNS algorithm outperforms all of the existing algorithms in the literature for solving uncapacitated MLLS problems because it was able to find all optimal solutions (100%) for 96 small-sized problems and new best-known solutions for 5 of 40 medium-sized problems and for 30 of 40 large-sized problems.
               
            

@&#INTRODUCTION@&#

The multilevel lot-sizing (MLLS) problem addresses how to best determine the trade-off cost in a production system with the purpose of satisfying customer demand with a minimum total cost. The MLLS problem plays an important role in modern production systems of manufacturing and assembly firms. Many planning systems, such as Material Request Planning (MRP) and Master Product Scheduling (MPS), depend heavily on the basic mathematical model and solution approaches for the MLLS problem. Nevertheless, the MLLS problem was proven to be strongly NP-hard (Arkin, Joneja, & Roundy, 1989). Optimal solutions to large-sized problems with complex product structures are notably difficult to find. In addition, optimal algorithms exist only for small-sized MLLS problems, and these algorithms include dynamic programming formulations (Zangwill, 1968, 1969), an assembly–structure-based method (Crowston & Wagner, 1973), and branch-and-bound algorithms (Afentakis & Gavish, 1986; Afentakis, Gavish, & Kamarkar, 1984). Many heuristic approaches have been developed to solve the MLLS problem and its variants with near-optimal solutions. Early studies first applied sequential applications of single-level lot-sizing models to each component of the product structure (Veral & LaForge, 1985; Yelle, 1979), and later studies used an approximate application of multilevel lot-sizing models (Blackburn & Millen, 1982, 1985; Coleman & McKnew, 1991).

The uncapacitated MLLS acts as a fundamental problem, and its solution approach could be highly meaningful to many of its extended versions, including the capacitated MLLS, the MLLS with time-windowing, and the MLLS with order acceptance. In practice, many SME firms in China’s electromechanical industry are more willing to adopt dynamic capability policies because they can improve their capacities during busy seasons with many methods, such as extra working-time, temporal employment, and rented machines. Therefore, the uncapacitated MLLS model caters to the situations of their ERP systems.

Over the past decade, several metaheuristic algorithms have been developed to solve uncapacitated MLLS problems with complex product structures. It has been reported that these algorithms are capable of providing highly cost-efficient solutions with a reasonable computing load. Dellaert and Jeunet (2000) and Dellaert, Jeunet, and Jonard (2000) first presented a hybrid genetic algorithm (HGA) for solving uncapacitated MLLS problems with a general product structure and introduced a competitive strategy for mixing the use of five operators in the evolution of the chromosomes from one generation to the next. Homberger (2008) presented a parallel genetic algorithm (PGA) and an empirical policy for deme migration (rate, interval, and selection) for the MLLS problem. These researchers used the power of parallel calculations to decentralize the large calculation load over multiple processors (30 processors were used in their experiments). In addition to genetic algorithms, other metaheuristic algorithms, such as simulated annealing (SA) algorithms (Homberger, 2010; Tang, 2004), the particle swarm optimization (PSO) algorithm by Han, Tang, Kaku, and Mu (2009), the MAX–MIN ant colony optimization (ACO) system by Pitakaso, Almeder, Doerner, and Hartl (2006, 2007), and the soft optimization approach (SOA) based on segmentation by Kaku and Xu (2006) and Kaku, Li, and Xu (2010), have been developed for solving uncapacitated MLLS problems.

The variable neighborhood search (VNS) algorithm initiated by Mladenovic and Hansen (1997) is a top-level methodology for solving optimization problems. Because its principle is simple and easy to understand and implement, various VNS-based algorithms have been successfully applied to many optimization problems (Hansen, Mladenovic, & Moreno-Perez, 2008a, 2008b, 2010). Mladenovic, Urosevic, Hanafi, and Ilic (2012) presented a new schema of the general variable neighborhood search (GVNS), which is an extended version of the basic VNS that considers multiple neighborhood structures. Labadie, Mansini, Melechovsky, and Calvo (2012) proposed a VNS procedure based on the idea of exploring, most of the time, granular instead of complete neighborhoods in order to improve the algorithms efficiency without loosing effectives. A brief summarization of recent successful VNS applications can be found in Mladenovic, Kratica, Kovacevic-Vujcic, and Cangalovic (2012).


                     Xiao, Kaku, Zhao, and Zhang (2011a) first developed a VNS-based algorithm for basic schema and a shift rule to solve small- and medium-sized MLLS problems; this algorithm performed better than the HGA in small- and medium-sized problems. Xiao, Kaku, Zhao, and Zhang (2011b) developed a reduced VNS (RVNS) combined with six SHAKING techniques to solve large-sized MLLS problems. The term “reduced” indicates a simplified version of the classical VNS algorithm because the local search (the most time-consuming component of VNS) was removed from the basic scheme. Although RVNS is still a generate-and-test algorithm, it differs significantly from the single-point stochastic search (SPSS) algorithm (Jeunet & Jonard, 2005) because it uses a systematic method to change multiple bits (with a maximum K
                     max) in the incumbent to generate a candidate, whereas the latter changes only a single bit. In the study conducted by Xiao, Kaku, Zhao, and Zhang (2012), three indices (i.e., the distance, changing range, and changing level) were proposed for a neighborhood search based on which three hypotheses were verified and can be used as common rules to enhance the performance of any existing generate-and-test algorithm. Using these three hypotheses, the proposed iterated neighborhood search (INS) algorithm delivered notably good performance when tested against 176 benchmark problems.

In our previous research, the neighborhood structure is defined under a distance-based metric that measures the distance (of two solutions) using the number of different bits, and another type of neighborhood structure based on problem decomposition was also studied in the recent literature (Helber & Sahling, 2010; Lang & Shen, 2011; Seeanner, Almada-Lobo, & Meyr, 2013). Several decomposition methods (i.e., product-oriented, time-oriented, and resource-oriented) combined with fix-and-optimized (or partial optimization) strategies were adapted to decompose the original problem into multiple sub-problems in order to restrain the optimization to a smaller area of the binary variables.

In this paper, we developed an effective local search procedure known as the Ancestors Depth-first Traversal Search (ADTS) for the RVNS algorithm such that the RVNS algorithm can be restored to a standard VNS. Although the ADTS procedure adds a considerable amount of computing load to the algorithm, we successfully developed an efficient method (known as trigger) using a new formulation of MLLS problems to rapidly calculate the change in the objective cost during the neighborhood search process. Thus, the new VNS algorithm is both effective and efficient in solving MLLS problems with high-quality solutions and within an acceptable computing time.

The remainder of this paper is organized as follows. In Section 2, we describe the new formulation of the MLLS problem. In Section 3, we detail a local search procedure known as ADTS which was added to our previously presented RVNS algorithm for effectively solving the MLLS problem. Section 4 outlines a highly efficient approach for rapidly calculating the cost variation of the objective function as the incumbent solution changes. In Section 5, we test the proposed algorithm on 176 benchmark MLLS problem instances of different scales (small, medium, and large) and compare its performance with that of existing methods. Finally, Section 6 presents our concluding remarks.

The MLLS problem under investigation is considered an uncapacitated, discrete-time, multilevel production/inventory system with a general product structure
                        1
                        In a pure assembly structure, each item has multiple immediate predecessors but at most only one direct successor; in a general structure, each item can have multiple immediate predecessors and multiple direct successors.
                     
                     
                        1
                      and multiple-end items. We assume that external demands for the end items are known throughout the planning horizon and that backlog is not allowed. Below, we present the notations used to model the MLLS problem, which also can be found in the reports by Dellaert and Jeunet (2000) and Xiao et al. (2012).
                        
                           •
                           
                              i: Index of items, i
                              =1,2,…,
                              m.


                              t (and t′): Index of periods, t
                              =1,2,…,
                              n.


                              Hi
                              : Unit inventory holding cost for item i.


                              Si
                              : Setup cost for item i.


                              dit
                              : External demand for item i in period t.


                              Dit
                              : Total demand for item i in period t.


                              Cij
                              : Quantity of item i required to produce one unit of item j.


                              Γi
                              : The set of immediate successors of item i.


                              
                                 
                                    
                                       
                                          Γ
                                       
                                       
                                          i
                                       
                                       
                                          -
                                          1
                                       
                                    
                                 
                              : The set of immediate predecessors of item i.


                              li
                              : The lead time required to assemble, manufacture, or purchase item i.

The decision problem focuses on how to set the production setup for all of the items in all of the planning periods such that the decision variable is an m
                     ×
                     n matrix denoted as follows:
                        
                           •
                           
                              Yit
                              : Binary decision variable addressed to capture the setup cost for item i in period t.

Depending on the decision variable, two other important variables are addressed to quickly capture the inventory holding costs. These can be introduced as follows:
                        
                           •
                           
                              Pit
                              : The period in which the demands of item i in period t will be set for production.


                              Xit
                              : The production quantity for item i in period t.

The objective function is to minimize the sum of the setup cost and the inventory holding cost for all of the items over the entire planning horizon and is denoted by TC (total cost). We extend the formulation described by Xiao et al. (2012) to cover the external demand for non-end items. Thus, the uncapacitated MLLS problem can be modeled as follows:
                        
                           (1)
                           
                              Min.
                              
                              TC
                              (
                              Y
                              )
                              =
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       m
                                    
                                 
                              
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       t
                                       =
                                       1
                                    
                                    
                                       n
                                    
                                 
                              
                              [
                              
                                 
                                    S
                                 
                                 
                                    i
                                 
                              
                              ·
                              
                                 
                                    Y
                                 
                                 
                                    it
                                 
                              
                              +
                              
                                 
                                    H
                                 
                                 
                                    i
                                 
                              
                              ·
                              
                                 
                                    D
                                 
                                 
                                    it
                                 
                              
                              ·
                              (
                              t
                              -
                              
                                 
                                    P
                                 
                                 
                                    it
                                 
                              
                              )
                              ]
                              ,
                           
                        
                     
                     
                        
                           (2)
                           
                              s
                              .
                              t
                              .
                              
                              
                                 
                                    Y
                                 
                                 
                                    it
                                 
                              
                              ∈
                              {
                              0
                              ,
                              1
                              }
                              ,
                              
                              ∀
                              i
                              ,
                              t
                              ,
                           
                        
                     
                     
                        
                           (3)
                           
                              
                              
                                 
                                    P
                                 
                                 
                                    it
                                 
                              
                              =
                              
                                 max
                              
                              {
                              
                                 
                                    t
                                 
                                 
                                    ′
                                 
                              
                              ·
                              
                                 
                                    Y
                                 
                                 
                                    
                                       
                                          it
                                       
                                       
                                          ′
                                       
                                    
                                 
                              
                              |
                              1
                              ⩽
                              
                                 
                                    t
                                 
                                 
                                    ′
                                 
                              
                              ⩽
                              t
                              }
                              ,
                              
                              ∀
                              i
                              ,
                              t
                              ,
                           
                        
                     
                     
                        
                           (4)
                           
                              
                              
                                 
                                    X
                                 
                                 
                                    it
                                 
                              
                              =
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       
                                          
                                             P
                                          
                                          
                                             
                                                
                                                   it
                                                
                                                
                                                   ′
                                                
                                             
                                          
                                       
                                       =
                                       t
                                    
                                 
                              
                              
                                 
                                    D
                                 
                                 
                                    
                                       
                                          it
                                       
                                       
                                          ′
                                       
                                    
                                 
                              
                              ,
                              
                              ∀
                              i
                              ,
                              t
                              ,
                           
                        
                     
                     
                        
                           (5)
                           
                              
                              
                                 
                                    D
                                 
                                 
                                    it
                                 
                              
                              =
                              
                                 
                                    d
                                 
                                 
                                    it
                                 
                              
                              +
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       j
                                       ∈
                                       
                                          
                                             Γ
                                          
                                          
                                             i
                                          
                                       
                                    
                                 
                              
                              
                                 
                                    C
                                 
                                 
                                    ij
                                 
                              
                              ·
                              
                                 
                                    X
                                 
                                 
                                    j
                                    ,
                                    t
                                    +
                                    
                                       
                                          l
                                       
                                       
                                          j
                                       
                                    
                                 
                              
                              ,
                              
                              ∀
                              i
                              ,
                              t
                              ,
                           
                        
                     
                     
                        
                           (6)
                           
                              
                              
                                 
                                    X
                                 
                                 
                                    it
                                 
                              
                              ⩾
                              0
                              ,
                              
                              
                                 
                                    X
                                 
                                 
                                    it
                                 
                              
                              ·
                              (
                              1
                              -
                              
                                 
                                    Y
                                 
                                 
                                    it
                                 
                              
                              )
                              =
                              0
                              ,
                              
                              1
                              ⩽
                              
                                 
                                    P
                                 
                                 
                                    it
                                 
                              
                              ⩽
                              n
                              ,
                              
                              ∀
                              i
                              ,
                              t
                              .
                           
                        
                     
                  

In the above equations, Eq. (1) indicates that every item in every period will incur a setup cost, an inventory holding cost, or nothing if a zero demand is associated with it. Please note that it is no longer necessary to calculate the inventory levels of the items in the new model, which saves a substantial amount of computational effort. Eq. (2) states that the decision variables for the production setup Yit
                      are binary, and Eq. (3) states that the demands of item i in period t are always set for the production in its closest setup period (indicated by 
                        
                           
                              
                                 Y
                              
                              
                                 
                                    
                                       it
                                    
                                    
                                       ′
                                    
                                 
                              
                           
                           =
                           1
                        
                     ) prior to period t (indicated by t′⩽
                     t). Apparently, the relationship 1⩽
                     Pit
                     
                     ⩽
                     t holds such that backlog is not allowed. Eq. (4) states that the total demand for item i in period t includes both its external demand and the sum of the lot sizes of its direct successors (items in Γi
                     ) multiplied by the production ratio with a lead time correction. Eq. (5) guarantees that the lot size of item i in period t will cover all of the latter periods of the demands that have been set for production in period t (indicated by 
                        
                           
                              
                                 P
                              
                              
                                 
                                    
                                       it
                                    
                                    
                                       ′
                                    
                                 
                              
                           
                           =
                           t
                        
                     ). Eq. (6) guarantees that the lot size is non-negative and can only be positive in the periods in which production has been set up, which is useful for solving the mathematical programming model with certain commercial solvers. Fig. A1 in Appendix A1 shows an example of solving this programming model using IBM ILOG CPLEX 12.2.

Based on the newly formulated model, we obtain selected properties that can be used to develop efficient heuristic rules that may aid the algorithm in finding better solutions and enhance the search efficiency.
                        Property 1
                        
                           Feasibility validation:
                              
                                 (7)
                                 
                                    
                                       
                                          Y
                                       
                                       
                                          i
                                          ,
                                          
                                             
                                                P
                                             
                                             
                                                it
                                             
                                          
                                       
                                    
                                    ·
                                    
                                       
                                          D
                                       
                                       
                                          it
                                       
                                    
                                    >
                                    0
                                    ,
                                    
                                    if
                                    
                                    
                                       
                                          D
                                       
                                       
                                          it
                                       
                                    
                                    >
                                    0
                                    
                                    ∀
                                    i
                                    ,
                                    t
                                    .
                                 
                              
                           
                        


                     This property can be read as all positive demands have been set up for production in their corresponding periods.
                        Property 2
                        
                           Cost uniqueness:
                              
                                 (8)
                                 
                                    
                                       
                                          Y
                                       
                                       
                                          it
                                       
                                    
                                    ·
                                    (
                                    t
                                    -
                                    
                                       
                                          P
                                       
                                       
                                          it
                                       
                                    
                                    )
                                    =
                                    0
                                    ,
                                    
                                    ∀
                                    i
                                    ,
                                    t
                                    ,
                                 
                              
                           
                           which indicates that the setup cost and the inventory holding cost cannot synchronously occur in the same period of time for an item.


                           Backlog forbiddance:
                              
                                 (9)
                                 
                                    1
                                    ⩽
                                    
                                       
                                          P
                                       
                                       
                                          it
                                       
                                    
                                    ⩽
                                    t
                                    ,
                                    
                                    ∀
                                    i
                                    ,
                                    t
                                    ,
                                 
                              
                           
                           which indicates that all of the demands must be set for production in the period prior to or equal to the demand period. Pit cannot be zero or negative.


                           No demand no setup: The optimal solution of an MLLS problem satisfies
                           
                              
                                 (10)
                                 
                                    
                                       
                                          Y
                                       
                                       
                                          it
                                       
                                    
                                    =
                                    0
                                    ,
                                    
                                    if
                                    
                                    
                                       
                                          D
                                       
                                       
                                          it
                                       
                                    
                                    =
                                    0
                                    ,
                                    
                                    ∀
                                    i
                                    ,
                                    t
                                    .
                                 
                              
                           
                        

Suppose that Y is the optimal solution of an MLLS problem and that there is a bit Yit
                           
                           ∊
                           Y that violates this property by Yit
                           
                           =1 and Dit
                           
                           =0. If we can change Y into Y′ by setting Yit
                           
                           ←0 and Yi
                           
                           ,
                           
                              t
                           
                           +1
                           ←1, we can state that TC(Y′) is lower than TC(Y) because the production setup cost does not produce an increase but the inventory holding cost has been decreased by 
                              
                                 Δ
                                 h
                                 =
                                 
                                    
                                       H
                                    
                                    
                                       i
                                    
                                 
                                 ·
                                 
                                    
                                       X
                                    
                                    
                                       it
                                    
                                 
                                 -
                                 
                                    
                                       ∑
                                    
                                    
                                       j
                                       ∈
                                       
                                          
                                             Γ
                                          
                                          
                                             i
                                          
                                          
                                             -
                                          
                                       
                                    
                                 
                                 
                                 
                                    
                                       H
                                    
                                    
                                       j
                                    
                                 
                                 ·
                                 
                                    
                                       X
                                    
                                    
                                       it
                                    
                                 
                                 ·
                                 
                                    
                                       C
                                    
                                    
                                       ji
                                    
                                 
                              
                           . Because the unit inventory holding cost of an item is always greater than or equal to the sum of the inventory holding cost of all of its immediate components, we can state that Δh
                           ⩾0. Thus, the bit Yit
                           
                           
                           
                           Y that violates this property does not exist.□


                           Inner corner property for assembly structure (Tang, 2004):
                              
                                 (11)
                                 
                                    
                                       
                                          Y
                                       
                                       
                                          it
                                       
                                    
                                    ⩾
                                    
                                       
                                          Y
                                       
                                       
                                          j
                                          ,
                                          
                                             
                                                t
                                             
                                             
                                                ′
                                             
                                          
                                          -
                                          
                                             
                                                L
                                             
                                             
                                                j
                                             
                                          
                                       
                                    
                                    ,
                                    
                                    ∀
                                    i
                                    ,
                                    t
                                    ,
                                    
                                    and
                                    
                                    j
                                    ∈
                                    
                                       
                                          Γ
                                       
                                       
                                          i
                                       
                                       
                                          -
                                       
                                    
                                    .
                                 
                              
                           
                        

For an assembly product structure without an external demand of non-end items, the demand of non-end items stems only from the lot size of its unique successor multiplied by the production ratio with a lead-time correction. In other words, if an item i is not set up for production in period t (i.e., Yit
                           
                           =0), then all of its immediate and non-immediate predecessors will have zero demand in their corresponding periods with a lead-time correction. According to Property 4, the production setup in these periods must be zero (it cannot be 1). Thus, Property 5 holds, and if Yit
                           
                           =1, Property 5 holds straightforwardly.□


                           Lot size relationship: The relationships below hold for a feasible solution of an MLLS problem:
                              
                                 (12)
                                 
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             t
                                             =
                                             1
                                          
                                          
                                             n
                                          
                                       
                                    
                                    (
                                    
                                       
                                          X
                                       
                                       
                                          it
                                       
                                    
                                    ·
                                    
                                       
                                          Y
                                       
                                       
                                          it
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             t
                                             =
                                             1
                                          
                                          
                                             n
                                          
                                       
                                    
                                    
                                       
                                          D
                                       
                                       
                                          it
                                       
                                    
                                    ,
                                    
                                    ∀
                                    i
                                    ,
                                 
                              
                           
                           
                              
                                 (13)
                                 
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             
                                                
                                                   t
                                                
                                                
                                                   ′
                                                
                                             
                                             =
                                             1
                                          
                                          
                                             t
                                          
                                       
                                    
                                    (
                                    
                                       
                                          X
                                       
                                       
                                          
                                             
                                                it
                                             
                                             
                                                ′
                                             
                                          
                                       
                                    
                                    ·
                                    
                                       
                                          Y
                                       
                                       
                                          
                                             
                                                it
                                             
                                             
                                                ′
                                             
                                          
                                       
                                    
                                    )
                                    ⩾
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             
                                                
                                                   t
                                                
                                                
                                                   ′
                                                
                                             
                                             =
                                             1
                                          
                                          
                                             t
                                          
                                       
                                    
                                    
                                       
                                          D
                                       
                                       
                                          
                                             
                                                it
                                             
                                             
                                                ′
                                             
                                          
                                       
                                    
                                    ,
                                    
                                    ∀
                                    i
                                    ,
                                    t
                                    ,
                                 
                              
                           
                           
                              
                                 (14)
                                 
                                    
                                       
                                          Y
                                       
                                       
                                          it
                                       
                                    
                                    ·
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             
                                                
                                                   t
                                                
                                                
                                                   ′
                                                
                                             
                                             =
                                             t
                                          
                                          
                                             n
                                          
                                       
                                    
                                    (
                                    
                                       
                                          X
                                       
                                       
                                          
                                             
                                                it
                                             
                                             
                                                ′
                                             
                                          
                                       
                                    
                                    ·
                                    
                                       
                                          Y
                                       
                                       
                                          
                                             
                                                it
                                             
                                             
                                                ′
                                             
                                          
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          Y
                                       
                                       
                                          it
                                       
                                    
                                    ·
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             
                                                
                                                   t
                                                
                                                
                                                   ′
                                                
                                             
                                             =
                                             t
                                          
                                          
                                             n
                                          
                                       
                                    
                                    
                                       
                                          D
                                       
                                       
                                          
                                             
                                                it
                                             
                                             
                                                ′
                                             
                                          
                                       
                                    
                                    ,
                                    
                                    ∀
                                    i
                                    ,
                                    t
                                    .
                                 
                              
                           
                        

The VNS is a top-level heuristic strategy that can be used to conduct an efficient stochastic search for better solutions using predefined and systematic changes of neighborhoods. This approach implies a general principle of from near to far for exploration of the neighborhoods of the incumbent solution. The underlying basis for this principle is that, in most cases, there is always a higher probability of finding better solutions in nearby neighborhoods rather than farther ones. Therefore, the definition of the neighborhood structures will be highly important for the implementation of the VNS strategy to solve an optimization problem. Xiao et al. (2011a) first introduced a distance-based definition of neighborhood structures for the MLLS problem and verified this approach with experiments in later study (Xiao et al., 2012), which showed that the distance and the other two indices (changing range and changing level) are highly effective factors for conducting a variable neighborhood search. Below, we introduce the definition of distance-based neighborhood structures for the MLLS problem according to Xiao et al. (2011a).
                           Definition 1
                           The distance metric: For a set of feasible solutions of an MLLS problem (i.e., {Y}), the distance between any two solutions in {Y} (e.g., Y and Y′) can be measured by the following:
                                 
                                    
                                       ρ
                                       (
                                       Y
                                       ,
                                       
                                          
                                             Y
                                          
                                          
                                             ′
                                          
                                       
                                       )
                                       =
                                       |
                                       Y
                                       ⧹
                                       
                                          
                                             Y
                                          
                                          
                                             ′
                                          
                                       
                                       |
                                       =
                                       |
                                       
                                          
                                             Y
                                          
                                          
                                             ′
                                          
                                       
                                       ⧹
                                       Y
                                       |
                                       ,
                                       
                                       ∀
                                       Y
                                       ,
                                       
                                          
                                             Y
                                          
                                          
                                             ′
                                          
                                       
                                       ∈
                                       {
                                       Y
                                       }
                                       ,
                                    
                                 
                              where 
                                 
                                    |
                                    ·
                                    ⧹
                                    ·
                                    |
                                 
                               denotes the number of different bits between Y and Y′, e.g., 
                                 
                                    |
                                    Y
                                    ⧹
                                    
                                       
                                          Y
                                       
                                       
                                          ′
                                       
                                    
                                    |
                                    =
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          m
                                       
                                    
                                    
                                    
                                       
                                          ∑
                                       
                                       
                                          t
                                          =
                                          1
                                       
                                       
                                          n
                                       
                                    
                                    |
                                    
                                       
                                          Y
                                       
                                       
                                          it
                                       
                                    
                                    -
                                    
                                       
                                          Y
                                       
                                       
                                          it
                                       
                                       
                                          ′
                                       
                                    
                                    |
                                 
                              .

Next, based on Definition 1, the neighborhood structure of the incumbent solution can be defined as follows:
                           Definition 2
                           The neighborhood structures: A solution Y′ belongs to the kth-neighborhood of the incumbent solution Y (i.e., Nk
                              (Y)) if and only if it satisfies
                                 
                                    
                                       ρ
                                       (
                                       Y
                                       ,
                                       
                                          
                                             Y
                                          
                                          
                                             ′
                                          
                                       
                                       )
                                       =
                                       k
                                       ,
                                    
                                 
                              which can be simply expressed as 
                                 
                                    
                                       
                                          Y
                                       
                                       
                                          ′
                                       
                                    
                                    ∈
                                    
                                       
                                          N
                                       
                                       
                                          k
                                       
                                    
                                    (
                                    Y
                                    )
                                    ⇔
                                    ρ
                                    (
                                    Y
                                    ,
                                    
                                       
                                          Y
                                       
                                       
                                          ′
                                       
                                    
                                    )
                                    =
                                    k
                                 
                              , where k is a positive integer.

The application of the basic VNS scheme to solve the MLLS problem consists of two general phases. The first phase includes defining a neighborhood structure, selecting an exploring sequence, initiating a solution Y
                        0 as the incumbent (Y
                        ←
                        Y
                        0), and defining a stopping condition. The second phase involves a neighborhood search loop in which better solutions in the targeted neighborhood (from near to far) are repeatedly explored and accepted. In this loop process, the nearest neighborhood N
                        1(Y) is explored first (k
                        ←1); if no better solution can be found in neighborhood Nk
                        (Y), then the algorithm shifts to explore a farther neighborhood by setting k
                        ←
                        k +1 until k
                        =
                        K
                        max. While searching for a better solution, a shaking procedure is used to randomly generate a candidate Y′, and a local search procedure is applied to look for the best solution Y″ near Y′. If Y″ is better than Y, then Y is moved to Y″ by Y
                        ←
                        Y″ and the algorithm continues to search Nk
                        (Y) after setting k
                        ←1; otherwise, we set k
                        ←
                        k
                        +1. In Fig. 1
                        , we present the basic scheme of the VNS algorithm introduced by Hansen and Mladenovic (2001).

Previously, Xiao et al. (2011b) proposed a reduced variable neighborhood search (RVNS) algorithm for solving the uncapacitated multi-level lot-sizing problem. The term “reduced” means that the local search component of the basic VNS scheme was removed to increase the computational efficiency. However, in this work, we suggest an effective local search method and add it to the RVNS algorithm because the local search method enables a significant improvement in the search for higher-quality solutions with an acceptable increase in the computational load.

The local search method is known as the Ancestors Depth-first Traversal Search (ADTS). After shaking out a new candidate Y′ in the neighborhood Nk
                        (Y) of the incumbent Y, we use the ADTS method to find the best solution Y″ near Y′ and perform a comparison between Y and Y″ (instead of Y′ in RVNS) to facilitate the “move or not” decision accordingly. In this process, the incumbent Y will tentatively move to Y′ and use the ADTS to find a Y″. If Y″ is better than Y′, then the algorithm moves to Y″; otherwise, it returns to Y. When Y moves to Y′, a total of k bits of Y must be reversed. Once one bit (i.e., Yit
                        ) is reversed, the ADTS procedure is launched to seek the largest cost reduction by setting (one at a time) each of the immediate and non-immediate predecessors of item i with the same setup value in the corresponding periods of t (with a lead-time correction if the lead time is not zero). In other words, after setting Yit
                        
                        ←1−
                        Yit
                        , we use a depth-first traversal recursion to set all of the setup values of the ancestors of item i to 1−
                        Yit
                         and record the path that leads to the largest cost reduction. The recorded path is exactly the method used to change Y′ to Y″, which is the best solution found from a local search around Y′.

In Fig. 2
                        , we present a simple MLLS example involving a six-item product structure over a six-period planning horizon to illustrate the principle of ADTS. Fig. 2(a) describes a general product structure, and Fig. 2(b) and (c) present two solutions with zero lead time and one period of lead-time, respectively. Because the solution moves into its kth neighborhood, it must reverse the values of the k bits that are selected randomly by the shaking procedure. The ADTS procedure will follow each change in these bits. For example, in Fig. 2(b), once bit Y
                        33 is reversed from 1 to 0, the ADTS procedure acts to recursively change all of its ancestors to zero and to restore the original value through the path Y
                        43(1→0)→
                        Y
                        63(unchanged)→
                        Y
                        43(0→1)→
                        Y
                        53(1→0)→
                        Y
                        53(0→1). In Fig. 2(c), once bit Y
                        24 is reversed to 1, the ADTS procedure changes and restores the bits through the path Y
                        43(0→1)→
                        Y
                        62(0→1)→
                        Y
                        62(1→0)→
                        Y
                        43(1→0) using the lead-time of 1 to correct the corresponding periods. Throughout the entire process, the largest cost reduction and its corresponding path will be recorded, and all of the changes that the ADTS has enacted to Y′ will be restored in the end. Finally, if the largest cost reduction is positive such that the best solution Y″ found in the local search is better than Y′, then the procedure moves Y′ to Y″ through the recorded path. The formal description of the ADTS procedure is detailed in Fig. 3
                        .

The concept of ADTS arose from the new formulation of the mathematical model for the uncapacitated MLLS problem. According to Eqs. (1)–(5), when an item i changes its production setup value in period t (i.e., Yit
                        
                        ←1−
                        Yit
                        ), this operation will only affect the setup and the inventory holding cost of item i and its ancestors in the affected periods. Most of the items and periods remain in the unaffected zone. Therefore, a local search that focuses only on the affected items and related periods will be effective for searching for the best solution close to Y′. Our computational experiment in the latter section confirmed this conjecture and verified the effectiveness of the ADTS method.

We now describe the framework of the VNS algorithm with the ADTS local search procedure, as shown in Fig. 4
                        . This framework is based on the RVNS algorithm introduced by Xiao et al. (2011b) and was combined with the ADTS local search procedure proposed in this study. The ideas introduced by Xiao et al. (2012) to restrain the changing ranges and changing level in the shaking procedure were also adopted in this algorithm. This approach represents the latest version of the neighborhood search algorithm that we developed for the MLLS problem, and we have verified its effectiveness and efficiency through the computational experiments reported in Section 5. In Table 1
                        , we list the interpretations of the parameters and variables that appear in the VNS algorithm.

The initial method is a modified RCWW (randomized cumulative Wagner and Whitin) algorithm that was first introduced by Dellaert and Jeunet (2000) and represents a sequential use of the Wagner and Whitin (WW) algorithm from an end item to the raw materials with a randomized cumulative production setup cost. The detailed WW algorithm can be found in the study published by Wagner and Whitin (1958; republished in 2004). Xiao et al. (2011b) extended the RCWW algorithm using both the modified setup cost and the modified inventory holding cost, which are also adopted in the new VNS algorithm of this paper.

The shaking procedure in Step 7 (a) randomly produces a candidate Y from the incumbent Y′, and the constraints on distance, changing range of item/period, and changing level are controlled by the input parameters K
                        max, Δt, and α, respectively. For additional details on these constraints, please see the manuscript by Xiao et al. (2012). Other techniques, i.e., limiting the changed bits between the first period and the last period that have positive demand described by Dellaert and Jeunet (2000), trigger recursive changes described by Dellaert and Jeunet (2000) and Jeunet & Jonard, (2005), setup shifting rule described by Xiao et al. (2011a), and the no demand no setup principle described by Xiao et al. (2011b), are included in the shaking procedure. All of these techniques will help improve the quality of the solution or the efficiency of the algorithm.

The stop condition in Step 9 can be either a maximum span between two consecutive improvements or a fixed computing time elapsed. The first stop condition, i.e., a maximum improvement span P
                        max, can be used for a small-sized problem because finding the optimal solution requires only a few (or even less than 1) seconds in this case. For medium/large-sized problems, the second stop condition, i.e., a fixed computing time, is suggested because different problems may require considerably different computing times if using the first stop condition. When a fixed computing time is used as the stop condition, the maximum improvement span (i.e., max(P)) can be treated as an evaluation index that indicates the extent to which the current solution has been optimized. A small value of max(P) indicates that the current solution still has much potential for optimization, whereas a large value indicates that the current result is an already optimized solution of the algorithm obtained with great effort.

Although the ADTS local search is quite effective in searching for better solutions, it is also a notably time-consuming procedure because it must recalculate the objective cost of the incumbent solution at each step of the recursive path (in theory). To improve the computational load, we propose an efficient method to calculate the cost variation of a solution change using the advantages of the new formulation for the MLLS problem shown in Eqs. (1)–(5). The new approach is a recursive accumulation of the cost reduction (or variation) (RACR in short) for recalculation of the objective function when the incumbent solution is changed. The RACR is highly efficient because it directly calculates the cost reduction from the changed point and the lot size in related periods.

It can be observed from Eq. (1) that the setup cost is determined only by the decision variable Y, and the inventory holding cost is determined only by the dependent variables P and D. When a bit in Y (i.e., Yit
                     
                     ∊
                     Y) is changed, the objective cost will increase by one unit of the setup cost of item i if Yit
                      moves from 0 to 1 or decrease by one unit if Yit
                      moves from 1 to 0. This change also triggers subsequent changes in Pit
                      and Xit
                      according to Eqs. (3) and (4), respectively. Therefore, the inventory holding cost for item i will be influenced: it will increase if Yit
                      moves from 1 to 0 or decrease if Yit
                      moves from 0 to 1. Subsequently, the varying lot size of item i (i.e., Xit
                     ) continues to recursively influence the demands for its immediate and non-immediate predecessors (components) in the corresponding periods (with a lead-time correction if Li
                      is non-zero). This scenario also causes an inventory holding cost variation. Note that all of these changes are occurring only in the area related to the ancestors of item i (not to all of the items) during a few periods (not in all of the periods). Therefore, the calculation of the cost reduction that results from the related area is much faster than the complete recalculation of the objective cost for all of the items throughout all of the periods. This insight helps us develop an efficient method for calculating the cost reduction in the neighborhood search and may also be applicable for any other generate-and-test algorithms to enhance their computing efficiencies. In Fig. 5
                     , we provide the detailed steps of the RACV procedure when the incumbent Y moves to Y′.

We note that the efficiency of the algorithms shown in Figs. 3 and 5 can be reciprocally related to the complexity of the product structure because the algorithm must call the recursive function to traverse the entire ancestor tree of the changed item. Therefore, the number of ancestor tree nodes can be used to measure the complexity of the product structure, which will considerably influence the computing efficiency. We use the notation Ti
                      to denote the number of ancestor tree nodes of item i and accordingly define a complexity index (CI) of the product structure using the average number of ancestor tree nodes for all of the items as follows:
                        
                           (15)
                           
                              CI
                              =
                              
                                 
                                    1
                                 
                                 
                                    n
                                 
                              
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       m
                                    
                                 
                              
                              
                                 
                                    T
                                 
                                 
                                    i
                                 
                              
                              .
                           
                        
                     
                  

Three classes of uncapacitated MLLS problems of different scales (small, medium, and large) were used to test the performance of the new VNS algorithm with the ADTS local search. Class 1 consists of 96 small-sized MLLS problems involving a five-item assembly structure over a 12-period planning horizon, as developed by Coleman and McKnew (1991) based on the work of Veral and LaForge (1985) and Benton and Srivastava (1985). Class 2 consists of 40 medium-sized MLLS problems involving 40/50-item product structures over a 12/24-period planning horizon, based on the product structures proposed by Afentakis et al. (1984), Afentakis and Gavish (1986), and Dellaert and Jeunet (2000). Class 3 covers 40 large-sized problems with a size of 500 items over 36/52 periods, as synthesized by Dellaert and Jeunet (2000) and used as a benchmark by Pitakaso, Almeder, Doerner, and Hartl (2007), Homberger (2008, 2010), and Xiao et al. (2011b, 2012). Thus, a total of 176 problem instances will be used to test the VNS algorithm. For the 136 problems classified as Class 1 and Class 2, the lead times of the items were all set to 0, whereas the lead times for all of the items in the 40 large-sized problems were set to 1.

The VNS algorithm was coded in VC++6.0 and run on a PC computer equipped with an Intel(R) Core(TM) 3i-2100 3.1G CPU operating under a Windows 7 system. We used the maximum improvement span as the stop condition for the small-sized problems in Class 1 and a fixed computing time as the stop condition for all of the medium and large-sized problems in Class 2 and Class 3. We compared the performances of the VNS with those of six other existing algorithms in the literature: the hybrid genetic algorithm (HGA) developed by Dellaert and Jeunet (2000), the MAX–MIN ant system (MMAS) developed by Pitakaso et al. (2007), the parallel genetic algorithm (PGA) developed by Homberger (2008), the ant colony algorithm (SACO) developed by Homberger and Gehring (2009), the RVNS developed by Xiao et al. (2011b), and the ISN algorithm developed by Xiao et al. (2012).

Interested readers can download all of our experimental data, including the benchmark problems, experimental outcomes, best solution found, executable program, and source code in VC++6.0, from the following website: http://semen.buaa.edu.cn/Teacher/Templet/DefaultDownLoadAttach.aspx?cwid=1224.

First, we ran the VNS algorithm once for the Class 1 problems using the control parameters P
                        max
                        =100, K
                        max
                        =5, N
                        max
                        =100, α
                        =0.5, and ΔT
                        =6. The parameter P
                        max
                        =100 indicates that the algorithm stops after 100 consecutive attempts to improve Y
                        best without success. The average results are shown and compared with those of the existing algorithms in Table 2
                        . It can be observed that the VNS algorithm is able to discover the optimal solutions for 100% of the 96 small-sized problems within a rather short computing time (much less than one second).

Second, we ran the VNS algorithm once for the 40 problems classified as Class 2 using the control parameters T
                        max
                        =30 (seconds), K
                        max
                        =5, N
                        max
                        =1000, α
                        =0.5, and ΔT
                        =6. The parameter T
                        max
                        =30seconds indicates that the algorithm stops after 30seconds have elapsed. The average results are shown and compared with those of the existing algorithms in Table 3
                        , and the detailed results are listed in Table 4
                         and compared with the previous best-known solutions (to date). It can be observed from Table 3 that the VNS algorithm delivers the best performances among all of the compared algorithms with respect to the Average Cost, Deviation from the best-known (%), and the best-known found (%). The RCWW in the second row shows the average initial solutions resulting from the RCWW method. The VNS
                        − in the third row indicates the VNS algorithm without use of the ADTS procedure, which demonstrates the advantages of the local search method. The fourth row (VNS (P
                        max
                        =100)) is the performance of the VNS algorithm with the stop condition P
                        max
                        =100 (where RACR is used) and can be compared with that of the fifth row (VNS∗
                         (P
                        max
                        =100)) (where RACR is not used, and the objective value is always fully recalculated from the changing item toward top materials). A AA comparison of these two rows reveals that, when the same solution quality is reached, the RACR method saves a marked amount of computational time. The detailed list in Table 4 indicates that five problems were updated with new best-known solutions, and 75% of the problems (30 of 40) reached their best-known solutions in this run. The column Max(P) in Table 4 indicates that certain problems with lower values, such as Problems 20 and 38, could be further optimized if the maximum computing time was extended. The latest list of best-known solutions from Class 2 problems has been updated in Table A1 in the Appendix.

Third, we ran the VNS algorithm once on the 40 problems classified as Class 3 using the control parameters T
                        max
                        =1800 (seconds), K
                        max
                        =6, N
                        max
                        =2000, α
                        =0.5, and ΔT
                        =8. The results are listed in Table 5
                        . As shown in Table 5, the VNS algorithm performs quite well on large-sized MLLS problems because 75% (30 out of 40) of the tested problems were updated with new best-known solutions. We summarize the results in Table 6
                         and compare them with those from the existing methods in the literature. It can be observed that the VNS algorithm delivers the best performances among all of the algorithms with respect to Deviation from Best (%) and Best solutions found (%) and delivers the second-best performance in terms of Average Cost. It is notable that the Average Cost is the arithmetic mean of the solutions for the 40 tested problems and is thus primarily influenced by those problems with large objective values, e.g., Problems 15 and 35, which have the largest and second-largest objective values. If Problem 15 or 35 has a deviation from the best-known solution of merely 0.2% (increase in the objective value), this deviation will cover up all of the good performances (decrease in the objective value) from the remaining 38 problems in terms of Average Cost for the entire problem class. For this reason, although the VNS algorithm updated 30 of the 40 problems with new best-known solutions (Problems 15 and 35 were not updated), the Average Cost is not the lowest among all of the existing methods. The numbers in column CI of Table 5, as defined in Eq. (15) in Section 4, indicate the complexity of the product structures of the problems. The experimental results show that a notably large CI can greatly decrease the efficiency of the ADTS local search method and will also result in a low value for Max(P), which may indicate that the current solution is still far from the optimal solution and has potential for further optimization. For example, Problems 15 and 35 are associated with a large CI and therefore output a small Max(P); it is therefore assumed that better solutions would be very likely to be obtained if the computational times are extended. The latest version of the best-known solutions for Class 3 problems has been updated in Table A2 in the Appendix. An example of the new best solution of the NO.39 instance of Class 3 can be found at http://semen.buaa.edu.cn/Teacher/Templet/DefaultDownLoadAttach.aspx?cwid=1224.

In Table 7
                        , we show a comparison of the total number of evaluated candidate solutions throughout the search process, which demonstrates the logic and computational effort defined by Alba, Nebro, and Troya (2002) that a generate-and-test algorithm uses to reach the best-found solution. The penultimate row in Table 7 (VNS’s shaking solutions) is the total number of candidate solutions generated by the shaking procedure, i.e., the number of Y′. The last row (VNS’s bit changes) in Table 7 shows the total number of times that one bit change (including 1→0 and 0→1) occurred in the incumbent solution, which are the shifts that primarily conducted by the shaking procedure of VNS (Y
                        →
                        Y′) and the ADTS procedure (Y′→
                        Y″). The comparison in Table 7 shows that, although the VNS algorithm delivered the best performance in terms of solution quality, it required much more logic and computational effort than other methods. This result demonstrates the benefits of the proposed RACR method for the rapid calculation of the objective function when a bit change occurs. In other words, RACR enables the VNS algorithm (and other generate-and-test algorithms, in our opinion) to test a larger number of candidate solutions within the same computational time.

Finally, a parameter N
                        max was added to our VNS algorithm to indicate the extent to which a neighborhood should be explored before moving to the next neighborhood by k
                        ←
                        k
                        +1; in contrast, in the basic scheme of the classical VNS algorithm, the parameter N
                        max is fixed to 1. We then performed the following experiment to verify the role of N
                        max in the VNS algorithm. We used the 40 Class 2 medium-sized problems as the benchmark problem set to test the parameter N
                        max, which was varied from 1, 10, 50, 100, 200, 500, 1000, to 2000. The other parameters were fixed at T
                        max
                        =30seconds, K
                        max
                        =5, ΔT
                        =6, and α
                        =0.2, the stop condition was fixed at 30seconds of computing time for each problem. The Average Cost, Deviation from best (%), and best solutions found (%) are recorded in the experiments with respect to every value of N
                        max. The experimental results are shown in Table 8
                        . It can be observed that, with the same computing time, the VNS algorithm tends to obtain a better quality of solutions as the parameter N
                        max increases. The best value for N
                        max is approximately 1000, and a continued increase of N
                        max to values higher than 1000 will not improve the solutions. It is notable that the solution quality is quite poor when N
                        max
                        =1, which is exactly the case for the basic VNS scheme shown in Fig. 1 without the N
                        max parameter. In Table 9
                        , we performed similar experiments for parameter K
                        max, which was increased from 1 to 10 at intervals of 1, and the other parameters were fixed at T
                        max
                        =30seconds, N
                        max
                        =1000, ΔT
                        =6, and α
                        =0.2. The experimental results are shown in Table 9. It can also be observed that the VNS algorithm tends to obtain better solutions as the parameter K
                        max increases, and an optimal value of K
                        max
                        =8, which gave even better outcomes than those observed in the previous experiments with K
                        max
                        =5 shown in Table 3, was found. We believe that a proper K
                        max will also improve the solutions shown in Table 5 for the large-sized problems, because the K
                        max
                        =6 used is relatively small. Interested readers can download our program to perform their own experiments (http://semen.buaa.edu.cn/Teacher/Templet/DefaultDownLoadAttach.aspx?cwid=1224). Note that when K
                        max
                        =1, the VNS algorithm will degrade to the single-point stochastic search (SPSS) studied by Jeunet & Jonard, (2005). Several new best-known solutions were observed in the experiments, but we did not add them to the best-known list of Table 10 because they were found using multiple runs.

@&#CONCLUSIONS@&#

The uncapacitated MLLS model represents a basic form of many extended versions of the MLLS problem under various constraints. The solution approach is foundational and plays a basic role in the solution of many other extended problems. We recently conducted studies on this problem using a variable neighborhood search and presented certain effective and efficient techniques and algorithms. In this paper, we continue our contributions to this topic and focus primarily on two points. First, we suggest an effective local search method that can be added to our previously presented RVNS algorithm; as a result, the solution quality can be markedly improved because many of the well-known benchmark problems were updated with new best-known solutions obtained from our computational experiments. Second, a highly efficient approach for the rapid calculation of the cost variation of the objective function when the incumbent solution is changed was presented. This approach can significantly improve the efficiency of the VNS algorithm and, more importantly, may be helpful in improving the efficiency of many other generate-and-test algorithms for MLLS problems that have been historically presented in the literature.

@&#ACKNOWLEDGEMENTS@&#

This work was supported by the National Natural Science Foundation of China under Projects No. 71271009, 71271010, and 71071007 and the Japan Society of Promotion of Science (JSPS) under Grant No. 24510192.

See Fig. A1
                      and Tables A1 and A2
                     
                     .

@&#REFERENCES@&#

