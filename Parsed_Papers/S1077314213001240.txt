@&#MAIN-TITLE@&#A multimodal temporal panorama approach for moving vehicle detection, reconstruction and classification

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           An effective multimodal temporal panorama approach for moving vehicle detection and classification using a novel sensing system.


                        
                        
                           
                           A new audio-visual vehicle (AVV) dataset, which features audio-visual alignment, vehicle detection and reconstruction.


                        
                        
                           
                           Multimodal audio-visual feature extraction and selection with systematically study for multimodal feature integration.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Laser-Doppler vibrometry

Multimodal

Panoramic imaging

Audio-visual integration

@&#ABSTRACT@&#


               
               
                  Moving vehicle detection and classification using multimodal data is a challenging task in data collection, audio-visual alignment, data labeling and feature selection under uncontrolled environments with occlusions, motion blurs, varying image resolutions and perspective distortions. In this work, we propose an effective multimodal temporal panorama approach for moving vehicle detection and classification using a novel long-range audio-visual sensing system. A new audio-visual vehicle (AVV) dataset is created, which features automatic vehicle detection and audio-visual alignment, accurate vehicle extraction and reconstruction, and efficient data labeling. In particular, vehicles’ visual images are reconstructed once detected in order to remove most of the occlusions, motion blurs, and variations of perspective views. Multimodal audio-visual features are extracted, including global geometric features (aspect ratios, profiles), local structure features (HOGs), as well various audio features (MFCCs, etc.). Using radial-based SVMs, the effectiveness of the integration of these multimodal features is thoroughly and systematically studied. The concept of MTP may not be only limited to visual, motion and audio modalities; it could also be applicable to other sensing modalities that can obtain data in the temporal domain.
               
            

@&#INTRODUCTION@&#

Moving vehicle detection and classification, in applications such as traffic monitoring [11,12,29], surveillance [24,25], and checkpoint inspection [7], can be very challenging if the data are noisy, especially from a sensor system at a large standoff distance. Occlusions, motion blurs and variations of perspective views always make the task difficult in both feature extraction and object classification. Using visual-only sensors may not be sufficient; audio information can provide complementary acoustic signatures, such as loudness and sharpness, for distinguishing different types of vehicles [15]. Typical acoustic sensors are microphones or microphone arrays; however, laser Doppler vibrometers (LDVs) [30,16] have started to show promise in long-range acoustic detection. Microphones or microphone arrays need to be placed at fixed location and near to the target, otherwise all sounds in between are captured. An LDV can be used to listen to the target at very long distance when the laser beam is reflected from a good vibration surface, and only sounds close to the vibration surface are captured. For example, using a Polytec LDV with a
                     <1mW red laser, we have found that the distance can be as far as 300m [30]; using a newly available Polytec LDV with a 10mW infrared laser, the distance could be much larger than that. In this work, we use a novel integrated audio-visual sensor system with a pair of PTZ cameras and a LDV for automatically acquiring video, range and audio information of moving vehicles at a large distance.

Using the multimodal sensor platform, new audio-visual vehicle (AVV) datasets are collected in both local road and highway scenarios. Multimodal data are then represented in a multimodal temporal panorama (MTP) interface for audio-visual alignment, real-time automatic vehicle detection, vehicle speed estimation, accurate vehicle image reconstruction, and efficient vehicle labeling. Vehicle detection using multimodalities reduces some false target detection with online data alignment so that the feature combination and classification can be more efficient. With the detection results, we further apply a visual image reconstruction technique on moving vehicles so that the images of extracted vehicles are invariant to perspective views, and free from occlusions from static objects as well as motion blurs. Since vehicles’ images are reconstructed with the same side-views, many local and global features can be applied more efficiently for classification. Along with audio features, the classification performance can further be improved. We analyze various types of visual features and audio features individually; then integrate them systematically. The visual features include aspect ratio and size (ARS), histograms of oriented gradients (HOG), and shape profiles (SP), representing simple global geometric features, statistical features, and global structure features, respectively. The audio features include short time energy (STE), spectral energy, entropy, flux and centroid feature, and Mel-frequency cepstral coefficients (MFCC), which are grouped into three types: temporal feature (STE), spectral feature (SPEC) and perceptual feature (PERC). We provide a thorough study on the selection and combinations of features for vehicle classification using support vector machines (SVMs). This research can find applications in traffic monitoring, check point security inspection, and transportation management. The general idea can also be applied to other applications with multimodal sensing and target detection and classification.

There are three main contributions of this work. First, a new audio-visual vehicle dataset is created, and a multimodal temporal panorama approach is proposed for real-time detection and fast data labeling. Second, a novel method for vehicle image reconstruction is designed using multiple images, and the reconstructed images are both view-invariant and occlusion and motion-blur free. Third, we show the effectiveness of multimodal feature selection and combination through an empirical approach, particularly of the importance of audio-visual integration. We also provide detailed observations for future research.

The rest of paper is organized as follows. Section 2 discusses some related work. Section 3 presents the AVV dataset. Section 4 describes the multimodal temporal panorama representation and the visual reconstruction method. Section 5 describes multimodal feature extraction. Section 6 provides the experiment results of multimodal vehicle classification and analyzes the multimodal integration issues based on the experiments. Conclusions and discussions are provided in Section 7.

@&#RELATED WORK@&#

Prior works in vehicle detection (e.g., [11,12] assumed that the complete image of a vehicle was available, generally detected by image differencing. Median filtering and blob extraction are commonly used in vehicle detection [8]. Several environmental variations, however, significantly affected the accuracy of vehicle classification after detection. An analysis of time-spatial image (TSI) obtained from a virtual line on the frames of a video presented by [18,23] showed that the dependencies of pixel intensities of still and moving objects of the video may be reduced. While a 1D slit scanning approach has been used for scene reconstruction [28] and vehicle detection in traffic monitoring [29], helping to overcome environmental variations, the influence of vehicle reconstruction on classification has not been studied. While wide-angle surveillance cameras can be used to perform detection and classification [24,25], the resolution on the targets by using these systems limits the capabilities of follow-on classification systems. Therefore, we have created a vehicle image detection reconstruction system that is robust to environmental variations, provides a dataset with view-invariant and occlusion-free vehicle images, and study its effects on vehicle classification algorithms.

The combination of both acoustic and visual information can provide classification results that are more reliable in surveillance [6,2,4]. Using different types of sensors, such as video and acoustic sensors, a rough estimate of target direction-or-arrival (DOA) is obtained using acoustic data and then refined using visual cues [2]. Those typical acoustic sensors are a microphone or microphone arrays, and lately laser Doppler vibrometers [15,30,16]. Work using both audio and video for vehicle detection can be found in [6]. In their approaches, the full video images are processed, which are sometimes computationally expensive but unnecessary. In addition, environmental variations have not been handled, such as the changes of the entire background or presence of other stationary objects in occlusion. The multimodal data acquired may be noisy and not aligned for efficient feature extraction and classification. Therefore, there is a need to represent multimodal data in an aligned representation and study multimodal features for the classification task.

To test vehicle classification algorithms, we have built an audio-visual dataset from long range surveillance of moving vehicles. The data are collected mainly at two locations, one at a two-way local road and the other at a multi-lane highway, using a multimodal sensor platform consisting of a pair of PTZ cameras and a LDV as shown in Fig. 1
                     . The system is an improvement to an early prototype described in [16], with a second PTZ camera added in order to measure the distance of the laser spot on a reflecting surface. Forming a PTZ stereo pair is useful to focus the laser beam even when the laser spot cannot be seen due to large distances or unfocussed laser beam, a necessary prerequisite for audio data collection [19]. The local road has a number of occluded static objects, such as trees, parked vehicles, mailbox, etc. (Fig. 2
                     a and b). This situation is very common in an urban environment where a lot of parked vehicles and trees on the roadsides. Fortunately, the traffic of the passing vehicles in the local road is sparse, thus, the audio signals of a vehicle can be effectively separated from other vehicles. The video data acquired uses two zooming levels of the cameras, one with large field of view but low details of vehicles, and the other with high details of the vehicles but only part of vehicles’ bodies are in the field of view. The standoff distance of the multimodal sensor platform at the local road is about 25–30m. For the highway location (Fig. 2c and d), there is less occlusion on the roadside, but the traffic of fast-passing vehicles are dense. The standoff distances for the highway data collected vary from 50 to 70m. For both locations, the image resolution is 640×480 and video frame rate is 30fps, the acoustic signals are collected using the mono sound track of a sound card at 22.5kHz of 16bit with its input directly obtained from the LDV. The LDV’s laser beam points to a vibrating surface very close to the road side (in Fig. 2a red
                        1
                        For interpretation of color in Figs. 2, 4 and 8, the reader is referred to the web version of this article.
                     
                     
                        1
                      laser spot can be seen in the bottom of the tree on image a, or the trunk of the tree on the left of image b).

The data were collected in different days from Monday through Sunday in the summer and with various weather conditions. The audio listening position was consistent through all days and times. The shortest video clip is about 1min and the longest is about 3h. Each clip may contain zero, one, or many passing-by vehicles. There are about 3000 vehicle samples in total. Therefore, due to the large variations in recording durations, traffic volumes, camera setups, and scene locations, it is hard to manually label the data and process the raw clips. It is necessary to have an efficient and effective data processing and representation technique for visualizing, searching and labeling the audio, visual and motion data of moving vehicles. Meanwhile, it is also desirable to remove perspective distortions, occlusions and motions blurs of vehicle images, and align them with corresponding acoustic signatures. This required a novel technical approach to vehicle reconstruction, labeling, and cross-sensor synchronization, which will be discussed in the next section.

During data collection, both visual and audio data are captured simultaneously. Multimodal data including visual, motion, and audio information from moving vehicles are represented into a multimodal temporal panorama (MTP) as we first described in [20]. The MTP consists of three synchronized 2D spatial–temporal panoramas (Fig. 3
                        ). The first panorama is the panoramic view image (PVI) concatenated from the same 1D vertical detection lines across all image frames [28,29]. The least occluded line in the scene, particularly when there is a significant amount of occlusions such as trees, parked vehicles or others, is selected initially to detect any vehicles crosses the line (the vertical red line in Fig. 3, right). Using a single line approach ensures a consistent background subtraction result since there is little variation in consecutive background lines over time in the video sequence. The line can be reselected if the scene is changed or a new location is picked. The second panorama is the epipolar plane image (EPI) [1,29] that has same time axis as the PVI. The EPI is generated from concatenating 1D horizontal epipolar lines along the direction of a vehicle’s motion. The purpose of this epipolar line is to track the motion of a vehicle on the road, after an initial target location on the road is selected. This location connects with the vanishing point of two parallel lines on the roadside to form an epipolar line. However, if the road is wide (i.e., a two way street, or a multi-lane road), a single fixed epipolar line may not be sufficient to trace the motion of a vehicle in various lanes/directions. A multi-look-up table (mLUT) is used to store multiple epipolar lines that correspond with all possible moving paths of a vehicle. Once a vehicle is detected from the PVI, the right row index of the mLUT is selected for constructing the EPI. For both PVI and EPI, we do not require the whole body of a vehicle in the field of view. A partially viewed moving vehicle by the camera is sufficient. Last, a 1D audio wave scroll can be easily represented along with the PVI and the EPI in the same temporal axis. The first use of the audio information is to improve the robustness of vehicle detection using the PVI representation. The short time energy of a window of signals can distinguish a sounding target with silent background thus removing some false target detection from the PVI.


                        Fig. 4
                         shows a segment of a MTP for a clip of multimodal vehicle collection on a local road. Here we want to note that due to the interlacing mechanism of the PTZ cameras, the images of moving vehicles are quite blurry. Therefore, we use fields as the unit for both PVI and EPI in the time direction instead of frames (consisting of even and odd fields). This reduces the spatial resolution of images in the vertical direction to half of the full resolution, but later in vehicle reconstruction, we will recover the original resolution.


                        Fig. 5
                         shows a segment of a MTP of multimodal vehicle collection on a highway. This scenario is more complicated than that of a local road where traffic is relatively sparse. In the first image shot, one vehicle is occluded by another visually; the sounds of the two moving vehicles next to each other with a very small headway are also mixed. For this case an image shot and a period of audio that contain these two vehicles are extracted together. Vehicles moving on the other side of the highway divider are also partially detected by the PVI. We detect them in the PVI but do not extract them for later classification based on the EPI and audio energy information.

The MTP facilitates the synchronization and integration of the information across the three modalities, both for automatic and interactive vehicle and traffic analysis, thus providing more succinct and reliable information for tasks like moving vehicle detection and classification using visual, motion, and audio information.

Because our system allows us to collect multimodal data at different locations and selecting various detection zones, the visual detection and audio detection of vehicles may not be aligned. In other words, depending on the viewing angles of the camera and the directions of the moving vehicles, the system may hear the sound before or after it actually sees them. Also, noise from the background subtraction and ambient sounds may also cause invalid alignment. Here, we present a systematic way to align the multimodal data using the multimodal temporal panorama. Let 
                           
                              
                                 
                                    I
                                 
                                 
                                    i
                                 
                                 
                                    D
                                 
                              
                           
                         denote intensity map of a vehicle whose center body is detected at the time i in the appearance panorama D. Let 
                           
                              
                                 
                                    I
                                 
                                 
                                    i
                                 
                                 
                                    M
                                 
                              
                           
                         denote the intensity map displayed at the time i in the motion panorama M. We want to select a correct range (j
                        −
                        m, j
                        +
                        m) in an audio clip that corresponds to the detected vehicle, as:
                           
                              (1)
                              
                                 
                                    
                                       argmax
                                    
                                    
                                       j
                                    
                                 
                                 
                                    
                                       1
                                    
                                    
                                       N
                                    
                                 
                                 
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             
                                                I
                                             
                                             
                                                i
                                             
                                             
                                                D
                                             
                                          
                                          +
                                          
                                             ∑
                                          
                                          
                                             
                                                I
                                             
                                             
                                                i
                                             
                                             
                                                M
                                             
                                          
                                          +
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   j
                                                   -
                                                   m
                                                
                                                
                                                   j
                                                   +
                                                   m
                                                
                                             
                                          
                                          A
                                       
                                    
                                 
                              
                           
                        where j is the center of the audio clip and m is the half-duration of the audio signal of a vehicle. N is the normalization factor, and A is the energy of the audio signals. Unlike human speech signals, the sound of a vehicle is much consistent during a period of time, so usually an audio clip of 5–10s (for 2m) can describe the signature of a vehicle sufficiently. There are three main terms in this MTP base on Eq. (1): visual detection-
                           
                              ∑
                              
                                 
                                    I
                                 
                                 
                                    i
                                 
                                 
                                    D
                                 
                              
                           
                        , motion detection-
                           
                              ∑
                              
                                 
                                    I
                                 
                                 
                                    i
                                 
                                 
                                    M
                                 
                              
                           
                        , and audio detection - 
                           
                              ∑
                              
                                 
                                    I
                                 
                                 
                                    j
                                    -
                                    m
                                 
                                 
                                    j
                                    +
                                    m
                                 
                              
                              A
                           
                        . A constraint that integrates these three terms will be adjusted according to a specified task. For moving vehicle detection and classification, the motion of a vehicle has to be detected, together with either strong visual detection or strong sound detection. Here the either-or operation is used in case the vehicle could be very silent (such as an electric car). Thus, a constraint Ψ in subject to the Eq. (1) is set as:
                           
                              (2)
                              
                                 Ψ
                                 =
                                 
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             
                                                I
                                             
                                             
                                                i
                                             
                                             
                                                M
                                             
                                          
                                          >
                                          τ
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      
                                                         I
                                                      
                                                      
                                                         i
                                                      
                                                      
                                                         D
                                                      
                                                   
                                                   >
                                                   τ
                                                
                                             
                                          
                                          
                                          or
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            ∑
                                                         
                                                         
                                                            j
                                                            -
                                                            m
                                                         
                                                         
                                                            j
                                                            +
                                                            m
                                                         
                                                      
                                                   
                                                   A
                                                   >
                                                   φ
                                                
                                             
                                          
                                       
                                    
                                 
                                 =
                                 1
                              
                           
                        where τ, φ are thresholds to penalize the visual background noise and the ambient sound. Note that the detection results do not rely on restricted thresholds. Indeed, a clean background subtraction with a filtered audio signal can guarantee a good detection results using very small τ, φ.


                        Fig. 4 shows the detected objects in red rectangle boxes and their aligned audio clips in black boxes. For the first object (vehicle), the detection time of the visual appearance and the audio signals are different; however, they can be aligned using Eq. (1) by finding an audio region that yields the highest total energy with respect to the visual detecting region. In addition, false targets in the appearance panorama can be removed if there is no motion presented to indicate a moving vehicle. In other words, if there is a vehicle detected in all the three panoramas (appearance, motion and audio), the result should be 1 in Eq. (2); otherwise, the result should be 0. Note that the constraint is task dependent; and we assume a moving vehicle could be detected at both video and audio. It is definitely possible to hear the sound of a moving vehicle without actually seeing it, then the constraint needs to be redesigned to fit in this situation.

Using the MTP, we can reconstruct complete visual images of vehicles with no occlusion, less motion blur and the same side views for better vehicle classification. The general idea of reconstruction is demonstrated in (Fig. 6
                        top-left); a basic algorithm was presented in [21]. The detected region in the PVI consists of consecutive detection lines that correspond to the time frames in the original video sequences from starting time ts
                         to finishing time tf
                        . The slope of the locus of the vehicle in the corresponding time in the EPI shows the speed of a moving vehicle, the method to detect loci can be found in [29]. The slope m of the vehicle’s locus at the corresponding time i in the EPI shows the relative speed vi of a moving vehicle as:
                           
                              (3)
                              
                                 
                                    
                                       v
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 m
                                 =
                                 
                                    
                                       ∂
                                       x
                                    
                                    
                                       ∂
                                       t
                                    
                                 
                              
                           
                        In other words, it equals the number of pixels in the motion direction in the original image that need to be extracted if t represents the frame number. The sign of the slope indicates the direction the vehicle’s motion. So, if the vehicle moves from left to right, the image piece to the left of the vertical detection line (vl) is extracted. If the vehicle moves from right to left, the image piece to the right of the vl is used. Then an image piece Si
                         contains vi
                         of pixels from the right direction is sliced from the original video frame Ii
                         at time i. A rotation matrix Aγ
                         is used to rectify every Si
                         in order to solve rotation of the camera. Note that the angle γ can be estimated from calibration or calculated using epipolar lines with the vanishing point. Then the reconstructed image IR
                         for a vehicle is the integration of all the image pieces from multiple images. The first image shot of in Fig. 5 shows two vehicles moving closely next to each other, so the reconstruction result contains both of them, overlapped. The motion slopes of those two are mixed together so only the best fitting line is selected to estimate the speed. So we can handle dense traffic, however we assume two or more vehicles moving at least at the same direction with a similar speed in order to make the reconstruction work.

The motion blur is mostly caused by the interlacing of the camera. Similarly, by knowing the speed of the vehicle, we can accurately align the even and odd fields of the image pieces into a single image piece at the frame i, thus significantly reducing the image blur, and restoring the original image resolution in the vertical direction. Fig. 6 bottom shows an example before and after motion blur removal.

The reconstruction is based on the detection panorama and the motion panorama: the detection panorama shows the current position of the target pass across the vertical detection line, and the motion panorama shows how many pixels need to be sliced from the corresponding original image shot. So it is the combination of image patches sliced from the original image at consecutive frames based on the relative speed. It is irrelevant to the distance and the camera since this is basically an image alignment problem, assuming the slices are of a planar surface of the car. We tested on data at different distances and different zoom levels, the shape of vehicles can mostly be reconstructed. Although a little shutter effect is still left, most conventional feature descriptors can be applied to distinguish the shapes of vehicles on the reconstructed images.

The method we proposed is a real-time implementation for automatic vehicle reconstruction. Thanks to the large stand-off distance of the multimodal sensor platform, the vehicles on the road will not create vibration to the sensors. The detection window mechanism in our temporal panorama approach also partially solves the problem of occlusions of vehicles by objects between the location of sensor platform and the road. The best detection zone can mostly be selected since it is only a small slit in the entire image. Furthermore the LDV as a remote “microphone” can also effectively pick up the acoustic signals of a passing-by-vehicles if we set a reflective surface close the visual detection zone. The reconstruction error analysis for this has appeared in one of our previous papers [21].

In this section, we describe various visual and audio features that we could extract from the multimodal data, for preparing the analysis of feature selection and multimodal vehicle classification.

Thanks to the reconstruction results from the MTP approach, vehicles’ visual images are invariant to perspective views, and with the distance information obtained from the PTZ stereo pair, they are also invariant to distances, therefore both scale and metric features as well as view and scale invariant features can be used. The first one that can be used is simply the aspect ratio and size (ARS) feature, as fARS
                        
                        =[w,
                        h,
                        w/h], where w is the width and h is the height. It can classify vehicles into various sizes. The other visual feature is the shape profile (SP), which is a curve that indicates the top boundary of a vehicle, a strong indicator of the vehicle’s type. To create the SP, we first apply the background subtraction on image pieces of the reconstructed vehicle image to obtain a clean shape of a vehicle. Only the top half of the images is used since only the top boundary contains significant differences among different types of vehicles, and the bottom part is harder to segment from the background due to shadows and motion blurs of the wheels. Then, the top boundary curves of all the vehicle images are sampled into the same number of bins with each bin Bi
                         presents the average of height of the current shape boundary, and to form a feature vector fSP
                         of the same dimension after normalization as:
                           
                              (4)
                              
                                 
                                    
                                       f
                                    
                                    
                                       SP
                                    
                                 
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       max
                                       
                                       B
                                    
                                 
                                 [
                                 
                                    
                                       B
                                    
                                    
                                       1
                                       ,
                                    
                                 
                                 
                                    
                                       B
                                    
                                    
                                       2
                                       ,
                                    
                                 
                                 …
                                 ,
                                 
                                    
                                       B
                                    
                                    
                                       N
                                    
                                 
                                 ]
                              
                           
                        where N is the number of bins for the SP. Note this normalization loses the size information. Note that this normalization loses the size information, but it has been captured by the aspect-ratio and size feature.

Histograms of oriented gradients (HOGs) [5] are a statistical feature that preserves some texture and local structure. It counts occurrences of gradient orientation in localized dense grid cells uniformly, thus, forming a feature vector of histogram H as fHOG
                        
                        =[H
                        1,
                        H
                        2,…,
                        HM
                        ], where M is the number of bins for the HOGs. Since it uses local contrast normalization, it is invariant to illumination changes, thus, it is good at people detection as well as vehicle detection [14]. We also extract HOGs for both reconstructed vehicle images with and without background removal for comparison of classification performance.

In general, audio features can be categorized into three groups: time-series features, spectral features and perceptual features. The time-series features represent audio samples in their raw waveforms. Short time energy (STE) is used to calculate the energy over a time [13]. It is usually good at distinguishing a vehicle sound with a silent background. Since the audio signals of a moving vehicle are much consistent over a short time period, we form the STE feature vector using only its mean and standard deviation as fSTE
                        
                        =[μSTE
                        ,
                        σSTE
                        ].

In the second group, the spectral features (SPEC) represent spectral moments and flatness [17]. Spectral energy, entropy, flux and centroid are composed together into a spectral feature vector fSPEC
                        
                        =[Eng,
                        Ent,
                        Flux,
                        Cent]. The spectral energy Eng calculates the energy of the power spectrum defined as:
                           
                              (5)
                              
                                 Eng
                                 =
                                 
                                    ∑
                                 
                                 |
                                 F
                                 {
                                 x
                                 (
                                 t
                                 )
                                 }
                                 
                                    
                                       |
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        where x(t) is the audio signal and F{} is the Fourier transform. The spectral entropy Ent measures the energy changes and defined as
                           
                              (6)
                              
                                 Ent
                                 =
                                 -
                                 
                                    ∑
                                 
                                 |
                                 F
                                 {
                                 x
                                 (
                                 t
                                 )
                                 }
                                 |
                                 
                                    
                                       log
                                       |
                                       F
                                       {
                                       x
                                       (
                                       t
                                       )
                                       }
                                       |
                                    
                                    
                                       E
                                    
                                 
                              
                           
                        The spectral flux Flux measures how quickly the power spectrum of a signal is changing and defined as:
                           
                              (7)
                              
                                 Flux
                                 =
                                 
                                    ∑
                                 
                                 
                                    
                                       (
                                       |
                                       F
                                       {
                                       x
                                       (
                                       t
                                       )
                                       }
                                       |
                                       -
                                       |
                                       F
                                       {
                                       x
                                       (
                                       t
                                       -
                                       1
                                       )
                                       }
                                       |
                                       )
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        The spectral centroid Cent indicates the center of the spectrum defined as:
                           
                              (8)
                              
                                 Cent
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       w
                                       |
                                       F
                                       {
                                       x
                                       (
                                       t
                                       )
                                       }
                                       |
                                    
                                    
                                       
                                          ∑
                                       
                                       |
                                       F
                                       {
                                       x
                                       (
                                       t
                                       )
                                       }
                                       |
                                    
                                 
                              
                           
                        where w is the weighted mean vector of the same dimension as the F.

In the third group, the perceptual features (PERC) represent the spectral variation and sharpness. Mel-frequency cepstral coefficients (MFCCs) [27] are commonly used to perceptually represent the frequency band responses of the human auditory system. The mel-frequency cepstrum (MFC) equally spaces the frequency band on the mel scale of F{x(t)}, and then transformed using the DCT after log of powers at each mel frequency. Then the coefficients of the results forms the perceptual feature fPERC
                        
                        =[μMFCC
                        ,
                        σMFCC
                        ], where μMFCC
                        , σMFCC
                         are the mean and the standard deviation vectors of all coefficients, respectively.

Feature selection is a task dependent problem. Given two different tasks, the classification results may be different using the same features or feature combinations. We’d like to evaluate a large number of features and select only a few of representative features or feature combinations. Such problem can be formulated as: given a feature set F
                        ={fi|i
                        =1,
                        …
                        ,
                        N}, find a subset SM
                         with M
                        <
                        N, that maximizes an objective function J(S),
                           
                              (9)
                              
                                 
                                    
                                       S
                                    
                                    
                                       M
                                    
                                 
                                 =
                                 {
                                 
                                    
                                       f
                                    
                                    
                                       i
                                       1
                                    
                                 
                                 ,
                                 
                                    
                                       f
                                    
                                    
                                       i
                                       2
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    
                                       f
                                    
                                    
                                       iM
                                    
                                 
                                 }
                                 =
                                 
                                    
                                       argmax
                                    
                                    
                                       M
                                       ,
                                       iM
                                    
                                 
                                 J
                                 {
                                 
                                    
                                       f
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 i
                                 =
                                 1
                                 ,
                                 …
                                 ,
                                 N
                                 }
                              
                           
                        The commonly used selection strategy is sequential forward selection (SFS) [10], which starts from the empty set and sequentially adds the feature that maximizes J(S) in each step, then the process is repeated testing each remaining feature combinations with those previously preserved until all features have been evaluated. The problem of the SFS algorithm is that only a single best feature is selected at each round so that it has a tendency to become trapped in local maxima. To alleviate this problem, we design a multi-branching sequential forward selection (MB-SFS) algorithm which selects a number of good features at each round that satisfy some maximal J(S) above a threshold ω. The algorithm is described as:


                        
                           
                              
                                 
                                 
                                 
                                    
                                       1.
                                       Start with the empty set SM
                                          
                                          ={∅}
                                    
                                    
                                       2.
                                       Select the next features {fi
                                          }, i
                                          =,…,
                                          M, s.t.
                                    
                                    
                                       
                                       
                                          
                                          J(Yk
                                          
                                          +
                                          fi
                                          )⩾
                                          ω, where
                                    
                                    
                                       
                                       
                                          
                                          fi
                                          
                                          
                                          
                                          Sk
                                          , and ω
                                          =
                                          max(J(Yk
                                          
                                          +
                                          fi
                                          )) – offset
                                       
                                    
                                    
                                       3.
                                       Update Sk
                                          
                                          +1
                                          =
                                          Sk
                                          
                                          +{fi
                                          }; k
                                          =
                                          k
                                          +1
                                    
                                    
                                       4.
                                       4. Go to step 2.
                                    
                                 
                              
                           
                        Last, the feature with the best accuracy among all levels of feature combination in SM
                         is selected.

To evaluate the individual features or a feature set, the radial based support vector machines (SVMs) [3]. are used. For the multi-class problem, one-against-one technique is used by fitting all binary sub-classifiers and finding the correct class using a voting mechanism. To evaluate the classifier for a given feature or feature set, confusion matrix C is generated and its error ε
                        =1−
                        trace(Diag(C)/sum(C)) is calculated to indicate what percentage the true labels and expected labels are off the diagonal.

To perform our experiments, we used the data acquired on the local, 2-way road, with 667 different vehicles in the dataset. Note that this is similar to our previous work [21], but the number of vehicle samples is almost tripled and more multimodal features are extracted and analyzed (previously we used just HOGs for visual and MFCCs for audio modalities). Of the 667 vehicle samples, 400 are used for training and 267 for testing. All vehicles’ visual images are reconstructed so that the image results are invariant to perspective views and the occlusions and motion blurs are removed. In our experiments, the vehicles are labeled into four categories: sedans, vans, pickup trucks and buses. There are more variations in each categorythan the small dataset of 150 vehicle samples that we used in our previous paper [21]. For example, sedans contain sports cars and economic 2 door or 4 door cars some with fastback or hatchback; vans include mini vans, regular size vans and long size vans, note that SUVs are categorized into vans as well; pickup trucks some may have wagons or trailers at rear parts; and buses include both school buses and transportation buses. Examples of the four types are shown in Fig. 7
                        . Note that the entire process of data collection, vehicle extraction, feature extraction, and vehicle classification is performed automatically. The only exception is during the training phase when the vehicle types are labeled manually. A video demo may be downloaded at [22], which shows the basic steps in MTP representation, automatic vehicle detection, vehicle image reconstruction, audio-visual feature extraction, and data labeling.

Note that in the local road scenario, similar or same vehicles often pass through the monitoring areas several times. For example, the same school buses or senior transportation buses appear at about the same time every day in the week, but may have different moving directions. However, the MTP construction ensures that all of them are represented in the same side views. All features are integrated and evaluated using the multi-branching sequential forward selection algorithm. Given the set of heterogeneous features f
                        ={fARS
                        ,
                        fHOG
                        ,
                        fSP
                        ,
                        fSTE,
                        fSPEC
                        ,
                        fPERC
                        }, we first perform classification on every individual kth feature in f. The training accuracy of a classifier fk
                         less than 55% is discarded. Next, we perform classification on combination of several good kth and jth (j
                        >
                        k) feature pairs in f that satisfy the threshold ω. Then, add new feature from f to the feature pair that is not k and j, repeat until all features combinations that derived from the good feature are evaluated.

The AVV dataset for moving vehicle detection and classification has made public available (http://visionlab.engr.ccny.cuny.edu/~tao/dataset/AudioVisualVehicleDataset.htm). It contains 961 sets of multimodal vehicle samples from both a local road and a highway locations, using our Vision Aided Automated Vibrometry (VAAV) multimodal sensor system, which consists of a laser Doppler vibrometer (LDV) and a pair of PTZ cameras.

Each set of vehicle sample has three files: an audio clip captured by the LDV, an original image shot from one of the PTZ cameras (the main PTZ cam), and a reconstructed visual image produced by our vehicle detection and reconstruction algorithm. Samples are divided into several categories, including: bikes, buses (school buses and mini buses), motorcycles, 2-door sedans, 4-door sedans, pickup trucks, regular trucks, mini-vans (include Jeeps and SUVs), regular vans, and mixtures of other different types of vehicles. There are three top directories. Two for data collected at the local road using zoom-in and zoom-out views of the main PTZ camera. The other one for data collected at the highway location. Each directory may contain folders corresponding to all or part of the aforementioned categories.

The original image shots include many real-world problems, such as occlusions, motion blurs, perspective distortion or vehicles out of camera’s views. The reconstruction results attempt to deal with all those issues. Note that some vehicles after reconstruction may look almost identical in their appearances among different samples; this is because the same vehicles may be captured multiple times at different time or days, particular for data collected at the local road.

The dataset has been used in a series of our studies in moving vehicle detection and classification. An early study with 140 vehicle samples was reported in [20] and more recent results on 667 samples of these dataset were presented in [21]. The dataset could also be used for vehicle recognition and anomaly detection as well as other potential applications, such as traffic monitoring, check-point inspection, and surveillance.

First, for showing the effectiveness of vehicle reconstruction and background removal, we apply the HOG feature extraction on three sets of images of the same vehicles: original raw images, reconstructed images without background removal, and reconstructed images with background removed (see Fig. 6). The original images are directly cut out from the original video frames that best correspond to the reconstructed results. Note that the original images may include partial occlusions, various side views and motion blurs. Table 1
                         shows the comparison in three confusion matrices on the testing data, where the rows indicate ground truth and columns are the estimations (same below in Table 2
                         and Fig. 8
                        ). The training size, testing size and training parameters are all the same for the three sets of data. Applying the same classifiers, the reconstruction without background removal improves the performance by 15.73%, and reconstruction plus background removal improves the performance by more than 18.10%. Therefore, from this point on, the HOG features are extracted only from reconstructed images with background removal.


                        Fig. 8 shows the classification results of all the individuals and combinations of multimodal features, including single modalities, bi-modalities, and multimodalities (⩾3). The yellow shading boxes indicate the good features that are selected at each level of combinations, and the bold blue lines show their derived branches. Confusion matrices of four meaning single-modal features and the best multimodal features on the same training data are presented in the figure.

At the bottom level, none of the unimodal features achieve a testing accuracy of over 75%. However, we can observe specific strengths of different features by looking into their confusion matrices. The ARS feature is the simplest visual feature, therefore it obtains the worst individual performance, but it can help to distinguish vehicles with different sizes and aspect ratios, for example sedans from trucks and buses. Since HOG feature counts the interior structure of vehicles (e.g., windows), its overall performance is the best, but individually it is not the best. The shape profile features analyze the global shape of vehicles and are therefore seems to be the best at distinguishing trucks and other types (particularly vans, since their top rear parts are usually quite different). The PERC feature individually has slightly better performance than the simple ARS feature; from their confusion matrices we can see that it does much better in separating vans from sedans, probably because their sound is more distinct than their aspect ratios and sizes. We will further see how this will make a difference in multimodal integration. The SPEC and STE are not good in combining with other modalities since their training accuracies are less than 55%. Because each modality has its own advantages and disadvantages, the combination among them becomes important to provide complementary information, which we will see next.

In bi-modal classification, we experimented on both visual only and visual and audio cases. In visual feature combinations, HOG and SP are applied on size-normalized images, but their combination includes both interior and exterior information of vehicles, thus providing some classification improvement. ARS feature preserves the size information of vehicle, and therefore providing complementary information to HOG or SP; when combined with either HOG or SP, we also see improvement in testing accuracies, particularly in ARS+SP. The PERC feature adds acoustic signatures of vehicles in addition to their visual information, thus providing significant improvement over the audio-only results. The testing accuracy using PERC with HOG is slightly better than using HOG itself, indicating features from two different sources (audio and visual) are better than the single source, even though individually, visuals do better than audio.

In the multimodal level, combining 3 or more than 3 features improve the classification. For example, the combination of ARS, HOG and SP (all visual features) increases the accuracy since each of them inherits distinct signature of vehicles. When combining visual features with audio features, the results are also improved. Based on the results, the accuracies with three modalities, between two different visual-audio combinations (ARS+HOG+PERC and ARS+SP+PERC) are very close; the former is slightly better in training and the latter in testing However, SP feature has only 30 dimensions whereas HOG uses 162 dimensions. Therefore, if the reconstructed images are accurate, the SP can be used to replace HOG while combining with other features to reduce computational costs for the vehicle classification task. In fact, the total feature size of ARS+SP+PERC is 63, which is even smaller than the size of the HOG feature vector (162). Between the visual-audio combinations (ARS+HOG+PERC and ARS+SP+PERC) and the visual-only combinations (ARS+HOG+SP), heterogeneous multimodal combinations seem to win, by 3% with the testing set used in this experiment.

At the very top in Fig. 8, the combination of all useful features has testing accuracy (73.78%), which may not be the best in performance. Therefore, in selecting best combination of feature modalities, only the one with highest training accuracy is finally chosen. In this experiment, ARS+HOG+PERC is selected with training accuracy 88.50%. Note that its testing accuracy is the second best of the available options.

Here we show an example on how the best multimodal combination wins with the training data. Looking into the training confusion matrices in Fig. 8, we have found that buses are misclassified for every one of the four features: ARS(4 misclassification), HOG(1 misclassification), SP(2 misclassification), and PERC(4 misclassification). However, the combination of these four features has 0 misclassifications. This is possible that each feature type has wrong classification on different samples, but by combining all or part of the feature types, those wrong classification results could be corrected. For example, ARS may misclassify samples 2, 3, 4, HOG may misclassify samples 7, 8, 9, and PERC may misclassify samples 10, 11, 12. When combined together there could be no misclassification.

Because the classes the task we used is designed based on visual appearance, the visual feature gave much better results than audio features. Nevertheless, adding audio features to visual feature would provide some improvement, say HOG+SPEC had 2.0% and 0.25% improvement than HOG (the best single modality) itself on both the training set and the testing set, respectively. The best combination, ASR+HOG+PERC, outperformed the best single modality HOG by 6% and 2.12% on the training set and the testing set, respectively.

Based on our experiment results (Fig. 8), we select the best combination of feature modalities ARS+HOG+PERC based on the training accuracy. Its training and testing confusion matrices and accuracies are show in Table 2. Because we use one-against-all for the multi-class classification, we will show the receiver operating characteristic (ROC) curves on each type of vehicles separately in Fig. 9
                        . In a ROC curve, the true positive rate (sensitivity) is plotted against the false positive rate (1-specificity) for different cut-off points. The sensitivity is the probability that a test result will be positive when the corresponding vehicle type is present; whereas the specificity is the probability that a test result will be negative when the corresponding vehicle type is not present. So, each cut-off point on the ROC curve represents a sensitivity/specificity pair corresponding to a particular decision threshold, which is a selected value of the cost function (0–1) of the SVM classifier for a testing sample to be positive. In generating the ROC curve, the thresholds (i.e., the cut-off values of the cost function in making a positive decision) vary from the smallest (⩾0) to the largest (⩽1). A test with perfect discrimination (no overlap in the two distributions) has a ROC curve that passes through the upper left corner (100% sensitivity, 100% specificity). Therefore, the closer the ROC curve is to the upper left corner, the higher the overall accuracy of the test. The area under the ROC curve (AUC) is used to measure the training and testing accuracies of one type against the rest. The Bus class has AUC 1.0 (100%) and all 22 buses are classified correctly in the training, meaning this feature combination can significantly distinguish buses with other types. However, the testing AUC is the lowest for the busses, comparing to the others. That may because the number of samples for buses is much smaller compare to the others. The AUCs for classes with large samples such as sedans and vans also indicated the classifiers are good to separate those types against the rest.

In this paper, we first describe the new audio-visual dataset acquired for moving vehicle detection and classification. Noisy multimodal data are represented efficiently in a multimodal temporal panorama interface for automatic moving target detection and fast vehicle labeling. A visual image reconstruction technique is provided to improve vehicle classification. Various multimodal features are systematically integrated and studied for vehicle classification. Results show that using multimodal features can have significant improvement than using single modality. We also make a number of important observations on the strengths and weakness of various features and their combinations. In addition, for some types of features the combination is computationally faster than a complicated feature, with similar classification performance.

However, we realize that the observations, even though justifiable, are made only on 667 vehicles samples at local road with possibilities of the same passing by vehicles at different time. Nevertheless, it is the first piece of work providing automated data labeling for multimodal vehicle classification, and to offer detailed observations of multimodal features. The ongoing work includes the labeling of the rest of vehicle samples in our data collections (about 3000) for further training and testing. We are working on noisy highway data collection to investigate performance issues in improvement with multimodal features.

Second, we also show that both the visual reconstruction and the audio-visual integration really improve performance in classification; and we believe it will be even more important for other tasks such as vehicle identification and anomaly detection over time and at different locations. The MTP interface on the AVV datasets provides a convenient way to label the data for different tasks.

Finally, so far we only provide empirical combinations of feature selection and integration, the next natural step could be the use of an ensemble of classifiers with various features and their combinations. For example, Adaboost could be used to train various combinations of features and select the most important features [9,26]. The results presented in this paper can be used as guidelines to extract features; but an automatic feature selection mechanism is highly desired with considerations in both classification performance and computational times (in both training and online testing).

@&#ACKNOWLEDGMENTS@&#

This work has been supported by the Air Force Office of Scientific Research (AFOSR) under Award #FA9550-08-1-0199 and the 2011 Air Force Summer Faculty Fellow Program (SFFP), by the National Collegiate Inventors and Innovators Alliance (NCIIA) under an E-TEAM Grant (No. 6629-09), and by a PSC-CUNY Research Award. The work is also partially supported by National Science Foundation (NSF) under Award #EFRI-1137172 and Award # CNS-0551598, and Army Research Office (ARO) under Award #W911NF-08-1-0531. We thank Dr. Kevin L. Priddy for his advising during our summer research at the Air Force Research Laboratory, WPAFB. This paper cleared through public release by 88 ABW/PA on 28-09-2012 as Document Number 88ABW-2012-5174.

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.cviu.2013.02.011.


                     
                        
                           Supplementary data 1
                           
                        
                     
                     
                        
                           Supplementary data 2
                           
                        
                     
                     
                        
                           Supplementary data 3
                           
                        
                     
                  

@&#REFERENCES@&#

