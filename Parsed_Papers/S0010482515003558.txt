@&#MAIN-TITLE@&#Unsupervised learning assisted robust prediction of bioluminescent proteins

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Combination of unsupervised learning with SMOTE for imbalance learning problems.


                        
                        
                           
                           Effective handling of between class and within class imbalance.


                        
                        
                           
                           Diversification of the training set with optimal class distribution.


                        
                        
                           
                           Does not require evolutionary information for prediction.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Class imbalance

Training set diversity

Optimal class distribution




                     K-Means

SMOTE

@&#ABSTRACT@&#


               
               
                  Bioluminescence plays an important role in nature, for example, it is used for intracellular chemical signalling in bacteria. It is also used as a useful reagent for various analytical research methods ranging from cellular imaging to gene expression analysis. However, identification and annotation of bioluminescent proteins is a difficult task as they share poor sequence similarities among them. In this paper, we present a novel approach for within-class and between-class balancing as well as diversifying of a training dataset by effectively combining unsupervised K-Means algorithm with Synthetic Minority Oversampling Technique (SMOTE) in order to achieve the true performance of the prediction model. Further, we experimented by varying different levels of balancing ratio of positive data to negative data in the training dataset in order to probe for an optimal class distribution which produces the best prediction accuracy. The appropriately balanced and diversified training set resulted in near complete learning with greater generalization on the blind test datasets. The obtained results strongly justify the fact that optimal class distribution with a high degree of diversity is an essential factor to achieve near perfect learning. Using random forest as the weak learners in boosting and training it on the optimally balanced and diversified training dataset, we achieved an overall accuracy of 95.3% on a tenfold cross validation test, and an accuracy of 91.7%, sensitivity of 89. 3% and specificity of 91.8% on a holdout test set. It is quite possible that the general framework discussed in the current work can be successfully applied to other biological datasets to deal with imbalance and incomplete learning problems effectively.
               
            

@&#INTRODUCTION@&#

There are mainly two phenomena, bioluminescence and bioflourescence which are responsible for the emission of visible light from the living organisms. The mechanisms of these two processes are distinct as the former involves a chemical reaction, and the latter involves absorption of light from external sources and its emission after transformation. Bioluminescence is observed in both terrestrial and marine habitats. The chemical reaction, which is responsible for bioluminescence, generates very less heat and can be categorized into oxygen dependent (luciferin-luciferase system) and oxygen independent types (ex. Photoproteins). The colour of the emission is governed by the amino acid sequence, and by accessory proteins like yellow fluorescent proteins (YFP) and green fluorescent proteins (GFP) [1]. Diverse systems for bioluminescence exist in nature, for ex. in Dinoflagellates, specialized organelles known as Scintillons [2,3] exhibit bioluminescence. Bioluminescence plays an important role in bacterial intracellular chemical signalling and in symbiosis: a common example of which is shown by Epryme scolopes and Vibrio fishcri 
                     [4,5], in attracting for a mate and repelling the predators. The independent evolution of bioluminescence in different organisms has been discussed in Hastings et al. [1]. In some organisms, the usefulness of bioluminescence is still unknown.

In silico prediction of bioluminescent proteins (BLP) was first carried out by Kandaswamy et al. [6]. They developed Blprot, which is an SVM based method. Their prediction model was trained by using 544 amino acid physicochemical properties. The prediction of bioluminescent proteins was further improved by Zhao et al. (BLPre) [7] using evolutionary information in the form of PSSM (Position Specific Scoring Matrices) obtained from PSI-BLAST. Fan et al. [8] used a balanced dataset (equal number of positive and negative samples for training) with average chemical shift and modified pseudo amino acid composition for prediction of bioluminescent proteins. Recently, Huang [9] proposed a scoring card method (SCBM) for their prediction.

The imbalanced class ratios are often encountered in the protein family classification problems. This causes the overrepresentation of instances belonging to majority class and under representation of instances belonging to minority class in the training set. The machine learning models trained with the imbalance training dataset have classification bias towards majority class and behave like a majority class classifier. This issue of imbalanced dataset has not been given the required attention in the bioinformatics community as it deserves.

In the current prediction problem, bioluminescent proteins (BLPs) are the positive minority class (which is the class of interest) and the majority class consists of all the non-bioluminescent proteins (NBLPs) belonging to different other protein families. The negative class is naturally very large as compared to the number of BLPs. So, the bioluminescent prediction problem training dataset is one of the classic examples of imbalanced dataset. This imbalance in class distribution greatly affects the accuracy in predicting the positive class instances (as the prediction models tends to act as a majority class classifier) and it is also quite evident from the previous studies [6–9].

When we use any machine learning algorithm to build a prediction model, the major motive is to maximize the generalization ability of the model. This insures that the trained predictive model will yield good prediction accuracy on the future unseen data. Ideally the training dataset that is presented to the learning algorithm should be properly diversified by covering the representatives from the entire input instance space to achieve the maximum possible generalization ability. If the training data are composed of a large number of very similar instances, it may get biased towards those instances. This notion holds true in the cases of both between-class (inter-class) and within-class (intra-class) instances. So the diversification of the training set is essential to gain enhanced generalization. Both between-class imbalance and within-class imbalance have a negative influence on the performance of machine learning algorithms [10].

In the present study, we have created a diversified and balanced training dataset by using unsupervised K-Means clustering algorithm (to deal with the within class imbalance where each class contains subgroups of similar instances of varying numbers) and then using SMOTE [11] (to selectively amplify the representative minority class sequences for balancing the between-class imbalance). The boosted random forest algorithm which has performed considerably better than the other machine learning algorithms was used to create our prediction model.

As the next part of this study, we have investigated the effect on prediction performances by varying the balancing ratio from ideal ratio (that is 1:1) to the original imbalance ratio. Analyzing the experimental results has revealed that the best prediction performance can be achieved at an optimal balancing ratio rather than at ideal balancing ratio. It was found that another performance factor (diversity) gets affected at the ideal balancing ratio of 1:1. This has motivated us to probe for optimal class distribution which is required to achieve superior accuracy (provides the best trade-off between inter-class balancing ratio and the diversity). The optimal class distribution is seldom explored in bioinformatics.

Finally individual features are ranked using the Relieff feature ranking algorithm and investigated the performance of the classifier by varying the number of features starting from 5 most discriminating features up to 40 (according to their rank) and recorded the calculated performance evaluation metrics obtained for RARF. The prediction performance increases with the increasing number of features (according to their ranks). This has authenticated the presence of large diversity among BLPs and there is a need for finding the optimal class distribution in order to achieve the best prediction performance. The superiority of the proposed framework as compared to random sampling is also discussed.

We used the dataset of Kandaswamy et al. [6] which consists of 441 positive class sequences (bioluminescent proteins) having less than 40% sequence identity and 18202 negative class sequences (non-bioluminescent protein NBLP) having more than 40% sequence identity. The redundant sequences in the dataset may result in bias and overestimation of model evaluation parameters. So we have used CD-HIT [12] to reduce the redundancy by removing sequences having more than 40% sequence identity, which resulted in 13,446 negative sequences. The final dataset consisted of approximately 1:30 positive to negative instances ratio. The data imbalance is intrinsically present in most of the protein family classification problems and affects the accuracy of predicting the members of a particular protein family. So the datasets are needed to be appropriately balanced to achieve the true performance of the classifiers.

The input vectors were created by extracting the following three types of features from every protein sequence.
                           
                              (i)
                              
                                 Amino acid composition: We used the percentage composition of amino acid residues (aa) as one of the feature vectors. This feature was selected on the assumption that there are some specific avoidances and preferences of certain amino acids in the formation of a protein family to perform a common functionality, which resulted in distinguishable frequency compositions (f
                                 
                                    res
                                 ).
                                    
                                       (1)
                                       
                                          
                                             
                                                
                                                   f
                                                
                                                
                                                   r
                                                   e
                                                   s
                                                
                                             
                                             =
                                             
                                                
                                                   
                                                      
                                                         N
                                                      
                                                      
                                                         r
                                                         e
                                                         s
                                                         ,
                                                         i
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         N
                                                      
                                                      
                                                         t
                                                         o
                                                         t
                                                         a
                                                         l
                                                         _
                                                         r
                                                         e
                                                         s
                                                         ,
                                                         i
                                                      
                                                   
                                                
                                             
                                             ×
                                             100
                                          
                                       
                                    
                                 
                              

where
                                    
                                       
                                          
                                             
                                                res
                                             
                                           stands for one of the 20 different amino acid residues


                                          
                                             f
                                          
                                          
                                             
                                                res
                                             
                                           denotes the amino acid percentage frequency of the specific residue in ith Sequence.
                                       


                                          
                                             N
                                          
                                          
                                             
                                                res,i
                                             
                                           denotes the total count of amino acid of the specific type in the ith sequence.
                                       


                                          
                                             N
                                          
                                          
                                             
                                                total_res,i
                                             
                                           denotes the total count of all residues in the ith sequence (i.e. sequence length).


                                 Amino acid property group composition: The percentage frequency counts of amino acid property groups were used as the second component in the feature vector. The different amino acid property groups [13] that are selected for this study are given in 
                                 Table 1. This is a refinement over amino acid frequency composition where specific property group count is computed instead of the individual amino acid count.

where
                           
                              
                                 
                                    
                                       pg
                                    
                                  denotes one of the 11 different amino acid property groups


                                 
                                    f
                                 
                                 
                                    
                                       pg
                                    
                                  denotes the percentage frequency of the specific amino acid property group in the ith sequence.
                              


                                 
                                    N
                                 
                                 
                                    
                                       pg,i
                                    
                                  denotes the total count of the specific amino acid property group in the ith sequence.


                                 
                                    N
                                 
                                 
                                    
                                       total_res,i
                                    
                                  denotes the total count of all residues in the ith sequence.


                                 Physicochemical n-grams: Physicochemical properties of amino acid residues play an important role in determining the protein’s function. There are many effective ways to incorporate the physicochemical properties of amino acid residues for representing protein sequences which can be used for discrimination of proteins of interest from other proteins. For calculating physicochemical n-grams, we used a technique of sliding window of length n (where n is an integer). If all the amino acid residues inside the sliding window share the same physicochemical group then the frequency of that physicochemical group is counted. If the amino acid residues are having more than one similar physicochemical group then corresponding counts are made for all those physicochemical groups. The physicochemical groups mentioned in table 1 were retained for calculation of physicochemical n-grams.
                                    
                                       (3)
                                       
                                          
                                             P
                                             h
                                             y
                                             s
                                             i
                                             c
                                             o
                                             c
                                             h
                                             e
                                             m
                                             i
                                             c
                                             a
                                             l
                                             
                                             2
                                             −
                                             g
                                             r
                                             a
                                             m
                                             :
                                             S
                                             m
                                             a
                                             l
                                             l
                                             =
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   N
                                                   −
                                                   1
                                                
                                             
                                             C
                                             
                                                (
                                                
                                                   i
                                                   ,
                                                   i
                                                   +
                                                   1
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             N
                                           denotes the length of the protein sequence


                                          
                                             I
                                           denotes the position of the amino acid residue along the protein sequence


                                          C(i,i+1) if the condition
                                             
                                                
                                                   
                                                      
                                                         
                                                      
                                                      (
                                                      
                                                         aa
                                                      
                                                   
                                                   
                                                      i
                                                   
                                                
                                                
                                                   ∈
                                                
                                                
                                                   
                                                      S
                                                   
                                                   
                                                      *
                                                   
                                                
                                                
                                                   ,
                                                   
                                                      
                                                   
                                                
                                                
                                                   
                                                      aa
                                                   
                                                   
                                                      i
                                                      +
                                                      1
                                                   
                                                
                                                
                                                   ∈
                                                
                                                
                                                   
                                                      S
                                                   
                                                   
                                                      *
                                                   
                                                
                                                )
                                             
                                          
                                          is satisfied then C(i,i+1)= 1 otherwise C(i,i+1)=0. The set of small-amino-acids S
                                             ⁎
                                          ={Ala,Cys,Asp,Gly,Asn,Pro,Ser,Thr,Val}

In the similar way the physicochemical 2-grams for the other ten physicochemical property groups were calculated. This feature captures the most important positional related information. So our input vector is a judicious combination of amino acid frequency compositions, physiochemical properties and positional features that are extracted from every sequence from the dataset.

For any supervised learning algorithm, the presentation of a diverse set of labelled data from the entire input space belonging to different classes is very important for its proper learning of all the concepts and the sub-concepts. Moreover, a dataset is said to be imbalanced when there is a large difference between the numbers of examples belonging to different classes. Normally every classifier tends to be the majority class classifier. The Bioluminescent protein classification problem is one of the classical examples of the class imbalance problem. In this present classification problem, the minority examples are the bioluminescent proteins and majority examples belongs to the non-bioluminescent proteins.

Most of the learning algorithms are designed to optimize the accuracy as the evaluation metric during the process of learning the concepts and the sub-concepts from the dataset. When accuracy is taken as the evaluation metric, it gets too strongly biased towards the majority class by predicting correctly most of the majority class instances as compared to minority class instances. So the accuracy does not prove to be the true indicator for better performance as it is a weighted average of accuracies in predicting both the majority and minority classes. Often in cases, when there is imbalanced data and the class of interest is the minority class, it is very important to gain better predictive accuracy and generalization ability for the minority class than for the majority class. When the imbalance ratio is high between the majority and minority class instances, the accuracy in predicting the minority class is low, as the learning algorithm will have less opportunity to learn all the minority class sub-concepts as compared to majority class sub-concepts due to the overwhelming number of instances from the dominant class. This may also result in misclassification of some of the minority class instances into the majority class.

Apart from between-class imbalance, the within-class imbalance in the training data may also result in a lower generalization of the learned models. Presence of rare cases or less common cases results in the within-class imbalance. If the common cases are present in more numbers in the training data, then learning algorithm will have less opportunity to learn the rare case sub-concepts. Ideally, there should be an adequate representation of common as well as rare cases from both majority and minority sub-classes in the training data. Also the class distribution of the training and testing samples should be similar otherwise the model may not generalize well on the unseen test set examples. Here we propose a hybrid sampling method using K-Means clustering and SMOTE for creating a balanced and diversified training dataset. Diversification of the training set is important as it includes as many distinct training samples as possible to maximize the generalization ability.

One of the popular methods to handle class imbalanced data is by using a sampling of the dataset. Sampling can be done in a random manner like random downsampling; random oversampling; and in an intelligent manner by using SMOTE or its variants [14]. There is a good chance of losing important instances in random downsampling of the majority class. While in random oversampling there is a good chance that some instances of the minority class may get overrepresented. Both these situations results in an incomplete learning. Random sampling results in the reduction of data variation in the training set and consequently results in a low generalization of the prediction model. Jo et al. [15] in their work addressed the problem of small disjuncts (disjuncts can be defined as those regions in the input space that covers only a few training examples). These small disjuncts are difficult to learn and ideally there should be an adequate representation of them in the training data.

This clustering technique aims to find homogeneous groups that occur naturally in a dataset. It is an unsupervised method of clustering, where with a given similarity measure (clustering criterion); it tries to find hidden patterns in the dataset and groups together the more similar entries. We have applied K-Means clustering separately on positive and negative class instances. The objective function, which is to be minimized during the each iteration of K-Means, is given as follows:
                              
                                 (4)
                                 
                                    
                                       S
                                       S
                                       E
                                       =
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          K
                                       
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             
                                                
                                                   n
                                                
                                                
                                                   j
                                                
                                             
                                          
                                       
                                       |
                                       |
                                       
                                          
                                             P
                                          
                                          
                                             i
                                          
                                          
                                             j
                                          
                                       
                                       −
                                       
                                          
                                             C
                                          
                                          
                                             j
                                          
                                       
                                       |
                                       |
                                    
                                 
                              
                           
                        

where
                              
                                 
                                    C
                                    
                                       j
                                     denotes the centroid of the jth cluster.


                                    P
                                    
                                       i
                                    
                                    
                                       j
                                     denotes the ith pattern of the jth cluster.


                                    n
                                    
                                       j
                                     denotes the number of objects in the jth cluster.


                                    K denotes predetermined number of clusters


                                    || || denotes matching distance metric used

The entire feature vectors were normalized before clustering with euclidean distance as the distance metric. To avoid sticking to the local minima, we have repeated the clustering for 10 times with a new set of centroid positions for the initial clusters. To determine the optimal value for K, (i) we have calculated the ratio of within cluster variance to between cluster variance (distortion ratio) for every value of K that varies from 1 to 441(that is the total number bioluminescent proteins in the dataset) using the K-Means algorithm for the positive dataset, (ii) a graph was plotted for K versus distortion ratio (
                           Fig.1) and (iii) the value of K after which there is no significant decrease in the corresponding value of the distortion ratio is taken as its optimal value (375). We used the optimal K value obtained from the positive dataset as the initial value of K for the negative dataset (the majority class) due to the constraints of very high time and space complexity in finding its K‐optimal as well as the need for dealing with an within-class imbalance. The experiments were carried out with the K‐optimal as well as with integral multiple of K‐optimal (from 2×K‐optimal to 7× K‐optimal) for the negative dataset to find out the best balancing factor. The rationale for using the higher value for K for the negative dataset is that the K-optimal for the positive dataset need not be optimal for the relatively larger negative dataset. Moreover, clustering with the same K-optimal for negative dataset causes tight clustering of the negative instances and may result in merging two or more distinct clusters or redistribution of unique cluster elements to different clusters. Accordingly, the higher values of K for the negative class were used to have a relax clustering so as to include vast diversity from the negative instances.

For the given K-optimal for positive dataset =375, the possible training datasets were created, such as Training Set I: 375 positive instances and 375 negative instances, Training Set II: 375 positive and 750 negative instances, Training Set III: 375 positive, 1125 negative instances and so forth.

The main purpose of clustering is to select representative samples from both the positive and negative classes to achieve diversification of the training set, so as to minimize the within-class imbalance. One instance from each cluster is selected from both the positive class clusters and negative class clusters for the training set and the rest of the cluster members are retained in the testing set.

In SMOTE, minority class is oversampled by inducing artificial instances. It is a nearest neighbour based method. It selects randomly a minority class instance and its N nearest minority class neighbours (the default value of N =5). Distance is calculated between the sample and one of the randomly chosen nearest neighbour in the feature space and then it creates a synthetic instance along the line segment between the minority sample and it’s selected nearest neighbour.

In cases where there are an unequal K (for ex. in 2×K, 3×K, etc.), we used SMOTE to selectively oversample the positive class representative instances equal to the number of negative class instances. We experimented with different % of SMOTE sampling and have examined the effect of a balanced and imbalanced dataset on the prediction evaluation metrics by creating different datasets with different proportions of positive and negative instances. The properties of different training and testing sets are presented in 
                           Table 2.

Boosting [16,17] combines many weak base learners linearly to construct a strong classifier with improved accuracy. It is an iterative procedure. During the each iteration, the incorrectly classified instances from both the positive and negative classes are given more weights so that the learning is concentrated on the hard and difficult to classify instances in the training set. It is a sequential ensemble method where the subsequent learners are evolved from the previous learners.

Random forest [18] is an ensemble learning method consisting of many individual decision trees. Classifier ensembles promote an optimal trade-off between diversity and accuracy. Ensemble classifiers usually outperform single classifiers and they are robust to the presence of noise in the data and to over fitting of inputs [19]. Different base classifiers making errors in different parts of the hypothesis space give better accuracy when properly combined together.

The concept of bagging [20] is implemented in the random forest classification algorithm. In random forest bootstrap samples from the training set with randomly selected feature subsets were evaluated at each node of the decision tree. The final decision is made by decision fusion of all the trees by majority voting. Random forests have been successfully applied to many classification and prediction tasks [21,22]. Major steps of random forest are summarized as follows: (1) a bagged sample is drawn from the training data. (2) A decision tree is grown without pruning on the bagged sample, where at each node a randomly selected subset of features from the full feature subset is evaluated. (3) Fusing the decisions from all the individual trees.

We have used random forest as weak learners for Boosting algorithm. Recently, some of the authors have also successfully applied boosted random forest for classification and prediction [23,24]. Real Adaboost is one of the popular modifications of the Adaboost algorithm. The major steps are same except that it involves the calculation of real valued class probability estimates. We experimented with both discrete and real Adaboosting algorithms. The schematic representation of the proposed methodology is shown in 
                           Fig.2.

The performance of the machine learning methods is evaluated by using threshold-dependent and threshold-independent parameters and these parameters are calculated from the values of true positives (TP), false negatives (FN), true negatives (TN) and false positives (FP).


                        Sensitivity: expresses the percentage of correctly predicted BLPs.
                           
                              (5)
                              
                                 
                                    S
                                    e
                                    n
                                    s
                                    i
                                    t
                                    i
                                    v
                                    i
                                    t
                                    y
                                    =
                                    
                                       
                                          T
                                          P
                                       
                                       
                                          (
                                          
                                             T
                                             P
                                             +
                                             F
                                             N
                                          
                                          )
                                       
                                    
                                    ×
                                    100
                                 
                              
                           
                        
                     


                        Specificity: expresses the percentage of correctly predicted NBLPs.
                           
                              (6)
                              
                                 
                                    S
                                    p
                                    e
                                    c
                                    i
                                    f
                                    i
                                    c
                                    i
                                    t
                                    y
                                    =
                                    
                                       
                                          T
                                          N
                                       
                                       
                                          (
                                          
                                             T
                                             N
                                             +
                                             F
                                             P
                                          
                                          )
                                       
                                    
                                    ×
                                    100
                                 
                              
                           
                        
                     


                        Accuracy: expresses the percentage of correctly predicted BLPs and NBLPs.
                           
                              (7)
                              
                                 
                                    A
                                    c
                                    c
                                    u
                                    r
                                    a
                                    c
                                    y
                                    =
                                    
                                       
                                          T
                                          P
                                          +
                                          T
                                          N
                                       
                                       
                                          (
                                          
                                             T
                                             P
                                             +
                                             F
                                             P
                                             +
                                             T
                                             N
                                             +
                                             F
                                             N
                                          
                                          )
                                       
                                    
                                 
                              
                           
                        
                     


                        AUC: the area under the receiver operating characteristic curve can be summarized by a single numerical quantity known as area under the curve (AUC). An AUC value close to 1 is considered good.


                        
                           g-means: It is the geometric mean of sensitivity and specificity and is calculated by
                           
                              (8)
                              
                                 
                                    g
                                    −
                                    m
                                    e
                                    a
                                    n
                                    s
                                    =
                                    
                                       
                                          (
                                          
                                             S
                                             e
                                             n
                                             s
                                             i
                                             t
                                             i
                                             v
                                             i
                                             t
                                             y
                                             ×
                                             S
                                             p
                                             e
                                             c
                                             i
                                             f
                                             i
                                             c
                                             i
                                             t
                                             y
                                          
                                          )
                                       
                                    
                                 
                              
                           
                        
                     


                        Youden׳s index (Y): this parameter measures the models ability to avoid failures and is calculated by
                           
                              (9)
                              
                                 
                                    Y
                                    =
                                    S
                                    e
                                    n
                                    s
                                    i
                                    t
                                    i
                                    v
                                    i
                                    t
                                    y
                                    −
                                    
                                       (
                                       
                                          1
                                          −
                                          S
                                          p
                                          e
                                          c
                                          i
                                          f
                                          i
                                          c
                                          i
                                          t
                                          y
                                       
                                       )
                                    
                                 
                              
                           
                        
                     

@&#RESULTS@&#

Initially, we trained the following six different machine learning algorithms using the randomly balanced training set, namely: Support vector machines with sequential minimization optimization (SMO), K Nearest Neighbour (IBK), Random Forest (RF), Rotation Forest (ROF), RARF (Real Adaboosting Random Forest) and ARF (Adaboosting Random Forest). The performance evaluation metrics on the randomly balanced training set (using tenfold stratified cross validation) and on the holdout testing set are presented in 
                     Table 3.

The overall accuracy of the tree based algorithm is better than the SMO and IBK algorithms. All the algorithms performed relatively well on the positive samples. RARF gave a comparable sensitivity among other learning algorithms with highest overall accuracy, AUC, g-means and Youden׳s index on the training set using tenfold cross validation. The same trend is also observed on the testing set

We selected the RARF algorithm for further analysis with varying balancing ratios, as it outperformed other learning algorithms on the randomly balanced dataset. The performance evaluation metrics of RARF using different training and testing sets are presented in 
                        Table 4.

It can be observed that RARF performed relatively better whenever the training set is fully balanced (with training set IDs −1, 2.2, 3.3, 4.4, 5.5, 6.6, and 7.7) and the sensitivity (accuracy of the positive class) increased with the increasing rate of SMOTE oversampling. RARF achieved higher sensitivity values on fully balanced training sets as compared to other partially balanced training sets. An opposite trend of decreasing specificity values is observed with the increasing rate of SMOTE oversampling. Though the specificity values on fully balanced training sets are least as compared to other training sets, the overall accuracy of RARF was increased with the rate of SMOTE oversampling. On all the fully balanced training sets, highest accuracy values can be observed as compared to other training sets. Full balancing of training instances between positive and negative samples also has positive effects on AUC values. Higher AUC values were observed in training sets having a lower imbalance ratio. The g-means reflects the model’s accuracy on both the positive and negative instances. Higher g-means values were observed in training sets with lower imbalance ratios. In training sets where the imbalance is more pronounced, lower accuracy for the positive class (sensitivity) was observed as compared to the accuracy for the negative class (specificity). Observing the Youden׳s index values, imbalanced training sets were having a lower fault avoidance rate as compared to fully balanced training sets. Learning of RARF on fully balanced training sets is nearly complete as is evident from the performance evaluation metrics. Partially balanced and fully balanced training sets are having comparable performance evaluation metrics for the RARF algorithm (with training set IDs 4.3, 4.4, 5.3, 5.5, 6.5, 6.6, 7.6, 7.7). The sensitivity values in the holdout testing sets also increased with the rate of SMOTE oversampling of the representatives of the minority class samples in the training sets. The best tradeoffs between the different performance-evaluation metrics were observed in the training set 5.5.

Although training set 7.7 gave 96.4% overall accuracy on tenfold cross validation, the overall accuracy on the testing set was only 92.5%. Based on g-means and AUC, which are robust to the imbalance nature of the datasets, training set 5.5 can be considered as the optimal training set for the prediction of bioluminescent proteins. Also the accuracy for predicting the positive class -bioluminescent proteins (sensitivity) did not improve further after training set 5.5.

The training datasets were created by varying the balancing ratio of positive class to negative class instances with the increasing number of training instances. The proposed prediction model trained with the training set 5.5 yielded the best performance on the testing set. This indicates that the dataset 5.5 is optimally diversified with both the positive as well as the negative class instances and providing the highest generalization ability. This is because the initial training datasets contains fully diversified positive class instances and partially diversified negative class instances, the input space of the negative class was much larger(more distinct sub-classes) than the input space of the positive class (less distinct sub-classes). Accordingly, optimally diversified (both the +ve and −ve classes) training set 5.5 provides the learning algorithm with all sorts of distinct samples for learning the sequence features of both the classes, giving the learning algorithm a chance to learn the entire input space, thereby achieving towards near perfect learning, while the balanced ratio of positive to negative class samples keeps the accuracy for the minority positive class samples optimal.

We also compared the performance of RARF using random training set with the same ratio as in the training set 5.5 with SMOTE oversampling on randomly selected positive samples and randomly undersampled majority class instances (
                        Table 5). Using random sampling we created ten random training and testing sets. The performance of RARF on the K-Means preprocessed training set on tenfold cross validation is comparable with that of randomly SMOTE oversampled training set, but the sensitivity, g-means and AUC values are lower than the K-Means preprocessed training set and the same trend is also observed in the holdout testing set.

When the dataset is imbalanced it is very trivial to get high accuracy by predicting most of the sequences as the majority class sequence. But to have a true estimation of model׳s generalization ability, parameters like sensitivity, specificity, accuracy, g‐means and AUC should also be taken into consideration. To have a complete picture of the classifier performance, the evaluation of learning algorithms should be based on appropriate evaluation metrics. The proposed approach tried to diversify the training instances, reducing the number of redundant instances from both the majority and minority classes and selection of a training set with the optimal class distribution. Overall full balancing gave perfect training and testing with optimal evaluation parameters for bioluminescent protein prediction.

Within-class imbalance is often ignored in resampling techniques. If the within class complexity is high, then learning algorithms are hard to optimize. The strength of K-Means assisted SMOTE oversampling (of the minority class) and undersampling (of the majority class) depends on the unique characteristics of the dataset and the learning algorithm used.

Undersampling the majority class with K-Means undersampling has some advantages
                           
                              (i)
                              It reduces the training time for the learning algorithms as it eliminates large quantities of redundant majority instances which may also bias the learning.

While simple undersampling results in loss of valuable information from the discarded instances, K-Means based undersampling preserves the informative sequences from the entire input space and hence prevents the information loss.


                        K-Means on the minority class samples results in:
                           
                              (i)
                              Having representative sequences from the entire positive instance space so that the learning algorithm may have a chance to learn about all the concepts and sub-concepts.

Elimination of redundant minority instances which may bias the learning.

Further, to have an optimal class distribution in the training set, SMOTE is used to oversample the representative instances to match the number of majority instances in the training set.

We compared our method with the four existing methods (
                        Table 6). Fan et al. [8] have modified the same dataset in terms of proportionality of positive and negative sequences to address the issue of huge imbalance which may lead to false model evaluation parameters.

For evaluating the generalization ability of the developed model only BLProt and BLPre have used a separate blind testing set. All of the previous methods have higher specificity values than the sensitivity values, which indicate that these models are having more power in detecting non-bioluminescent proteins than bioluminescent proteins. Due to imbalanced nature of the dataset, the trained models gets biased towards majority instances (non-bioluminescent proteins) and contributes towards higher overall accuracy values. The present method handles the class imbalance problem giving balanced evaluation metrics for both the majority and minority class instances. Our method gives experimental evidence that selective SMOTE of representative feature vectors gives the superior generalization ability to the learned models, specifically larger gains in sensitivity can be observed.

We have used Relieff [25] feature ranking algorithm for ranking the protein sequence features according to their discriminating ability. 
                        Fig. 3 shows the heatmap representation of the various features along with their ranks in discriminating between the two groups.

The importance of aromatic amino acids in bioluminescence has been stressed in some of the previous studies [26,27]. Previously Huang [9] also mentioned the importance of hydrophobic amino acids through the implementation of probabilistic method. The role of Tryptophan in fluorescence has been stressed notably in [28,29]. The contribution of hydrophobic 2‐grams and nonpolar 2‐grams in discrimination is mostly due to the presence of aromatic amino acids.

We have also investigated the performance of the classifier by varying the number of features starting from 5 most discriminating features up to 40 (according to the Relieff feature ranking) and recorded the calculated performance evaluation metrics obtained for RARF in 
                        Table 7.

This result indicates that most of the sequences are able to be predicted with only two top ranking features and the additional unique types of sequences are predicted with the help of other remaining higher ranking features.

This variation in performance metrics indicates great diversity (poor sequence similarities) among the bioluminescent protein sequences and consequently emphasizes the need for representation from each diverse group of bioluminescent protein sequences in the training set, which is essential for improving the generalization ability of the classifier. The lowest rank features contribute to either higher sensitivity or higher specificity.

As the dimension of our feature vector was not large, so it did not affected the performance evaluation metrics much, but for larger dimension feature vectors, it may be useful to find optimal dimension of the feature vector using feature ranking method (feature reduction method).

@&#CONCLUSION@&#

Previously the issue of imbalanced dataset and its effect on the prediction performance in bioinformatics have been addressed by Dobson et al. [30], Wei et al. [31]. Their study pointed out the necessity of a balanced dataset for more accurate prediction performance. The class imbalance should be given proper importance as it is almost ubiquitous in protein family classification problems. When there is a huge imbalance between the different classes, the classifier can achieve very high accuracy by simply predicting most of the test instances as the majority class instances. Creating an appropriate training dataset is not a straightforward process for any learning algorithm as the different factors influence the classification accuracy and identifying those factors as well as finding the best trade-off among them is a challenging task. Through experiments, we have studied the effect of different training sets with varying level of imbalance, on the learning of classifiers. We have analyzed the role of these factors in achieving the best classification accuracy. The proposed method effectively undersamples the majority class, well balances the within-class imbalance and attains optimal class distribution to obtain the superior classification performance. we applied K-Means to achieve this task. The current work proved that the balanced training set have performed better than randomly created training sets.

The current research addresses the issues of classifier bias due to imbalanced dataset and incomplete learning through proposing a novel method for the creation of a balanced and diversified training dataset. Most of the common resampling methods do not take into account the within-class imbalance. In the current work, we addressed the issue of between-class imbalance and within-class imbalance simultaneously. Our method of sampling gives the superior generalization ability to the learned models. We robustly tested our model against a larger blind testing set and the results are highly improved. The present method not only handles the imbalance between the majority and minority classes, but also gives a proper representation to the rare instances present in both majority and minority classes. As the time complexity of the K‐Means algorithm is quite high, the proposed preprocessing has some limitations for large dataset with high dimensional feature space. Divide and conquer approach and other suitable techniques can be explored in the future research to deal with this problem. We hope that the current framework can be implemented successfully to other imbalance datasets to achieve the true performance of a classifier.

The authors declare they have no conflicts of interest.

@&#REFERENCES@&#

