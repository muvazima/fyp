@&#MAIN-TITLE@&#Gaussian Markov random field based improved texture descriptor for image segmentation

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Distributions of GMRF local parameter estimates as improved texture descriptors.


                        
                        
                           
                           Spatially localized parameter estimation using local linear regression.


                        
                        
                           
                           Approaches to overcome inconsistencies in localized parameter estimation.


                        
                        
                           
                           Proposed descriptors capture both spatial dependencies and distributions of texture.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Gaussian Markov random field

Texture feature extraction

Local feature distributions

Local linear regression

Texture segmentation

Natural image analysis

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

Texture plays a vital role in human perception of visual objects and scenic regions, together with other visual cues such as colour, brightness and form [1]. As a result, much research in recent years has been carried out in texture feature extraction [2–6]. Segmentation of textured images has its own significance in image segmentation domain with wide spread applications in many fields including medical image processing, remote sensing, document processing, defect detection and object recognition [1]. Feature-based texture segmentation basically consists of two successive processes; texture feature extraction and a routine of feature clustering [7]. The texture feature extraction mainly aims at formulating effective discriminative texture descriptors and has been intensively studied [2–4,6].

In statistical model based texture feature extraction, texture is assumed as a realization of a random process which is governed by the model parameters. Texture features obtained from these methods, especially Markov random fields (MRF), have proved to offer a powerful framework for image analysis [8–10]. Gaussian Markov random field (GMRF) is an important subclass of MRF whose joint distribution is a multivariate Gaussian distribution [9]. A local conditional probability distribution of GMRF encapsulates spatial dependencies between a pixel and its neighbours [11]. Model parameters of the conditional distribution offer a satisfactory feature set, which successfully enables to discriminate many different textures [12]. GMRF is widely popular because it avoids the difficulties in parameter estimation and therefore makes the process analytically and computationally efficient [9,13,14]. Using a Gaussian model is a valid assumption because most of the real textures like wood, wool, water, etc. obey the Gaussian notion [14]. GMRF based features produce good results for homogeneous, fine, stochastic textures, but poorly perform when characterizing more structured and macro textures [10,15].

In this paper our contribution is to propose a method based on GMRFs which can easily capture both statistical and structural properties of a texture. It can overcome the problem of parameter smoothing occurred in traditional GMRF parameter estimation. Our method suggests simple alterations to the existing GMRF feature extraction technique and achieves significantly better results. We perform parameter estimation using least squares estimation similar to the traditional GMRF parameter estimation, however we fit localized linear models at each pixel based on local linear regression. The issues arising in the local parameter estimation process are drawn into special attention and are addressed by using the generalized inverse [16], setting constraints on estimation window size and applying regularization. The localized process of parameter estimation results in spatial variations in the parameter estimates which are repetitive with the periodic texture patterns. We propose the distributions of these local parameters as a successful discriminative texture descriptor and designate as local parameter histograms (LPHs).

LPH descriptors capture both pixel dependencies and spatial distributions in a texture and therefore give a significant discriminative performance over classical GMRF features. In this study, we perform general and natural texture image segmentation using the novel LPH descriptors and demonstrate convincing segmentation results with a simple segmentation method such as k-means clustering algorithm. Segmentation experiments on large datasets are carried out. The LPH descriptors are also compared against the state-of-the-art structural and spectral texture feature extraction methods based on local feature distributions. The results on natural image segmentation could be further improved by integrating colour information and using advanced segmentation algorithms for better boundary localization. Therefore, for texture segmentation where more discriminative features are required rather than exact modelling of texture, LPH features are a better choice over the classical GMRF features.

The remainder of this paper is organized as follows. In Section 2 we provide the background which lead to the novel texture descriptors. Section 3 presents the theoretical foundation of GMRFs and in Section 4 we discuss local linear regression and its adaptation to GMRF parameter estimation. In Section 5 we address the issues arising in localized estimation with small sample sizes. Section 6 describes the construction of local parameter histograms. Section 7 illustrates the experimental results on texture segmentation and finally, we conclude this work in Section 8.

@&#BACKGROUND@&#

Model parameters of conditional GMRF model offer descriptive features for texture analysis and have been directly used as texture features. In feature-based segmentation adaptive GMRF features are acquired by sliding window estimation. These GMRF features have been used in many studies focusing on texture segmentation [14,17–21] and are referred to as the classical GMRF features. The parameter estimation is performed either by least squares estimation (LSE) or maximum likelihood estimation (MLE) and both methods lead to the same solutions [11,18,22].

Techniques have been proposed to enhance the discriminative power of classical GMRF features. Most of these methods specially focus on improving the parameter estimation process of GMRFs [11,23–25]. Hierarchical formulations of GMRF model and mixture of Gaussian model have also been proposed to derive improved GMRF features [26–31].

During the estimation process, the estimation window size not only should be large enough to capture a homogeneous texture region but also should be small enough to achieve accurate boundary localization at the texture boundaries [11,12]. Many studies have been performed to address the boundary localization problem by proposing better adaptive segmentation algorithms and model-based segmentation approaches [7,12,18].

However, another issue of using larger estimation windows, other than the boundary localization problem, is that the large sample size for the estimation process, sampled from the large estimation window leads to over smoothed parameter estimates. This causes loss of structural information. Especially when the texture is deterministic and structured [10,15]. This parameter smoothing effect mainly occurs due to the Gaussian and linear neighbour dependency assumptions forced on the model. Comparative studies have shown that the GMRF features inherit a reduced discriminative ability in large scale empirical evaluations including many deterministic textures [10,32–34].

To find a solution to this smoothing problem, which has not been addressed before, we investigate the performance of locally estimated GMRF features based on local linear regression. Local regression also referred to as the kernel regression is a non-parametric method that depends on data itself rather than highly relying on a specific pre-selected model [35]. This framework gives a rich mechanism for computing point-wise estimates with minimal assumptions about the global model. For local regression the underlying model may remain totally unspecified. However, the local linear regression fits many localized linear models to describe any signal [35]. Here, we use the local linear regression to simplify the estimation process over the local regression and to maintain the direct link to the GMRF parameter estimation. Local estimates of GMRF model parameters obtained by local linear regression can integrate both statistical and structural texture information while minimizing the effects of parameter smoothing. The method proposed here reduces the large dependence of GMRF features on Gaussian constraints but inherits the computational simplicity of GMRF parameter estimation.

The concept of local linear regression has been used as an effective tool for image de-noising, interpolation and other image processing tasks [36–39]. However, it has not been used previously in formulating improved GMRF texture features.

Furthermore, recent studies on texture and object recognition have demonstrated that image representation based on distributions of local features is surprisingly effective [34,40]. Distributions of local features such as local binary patterns, spectral histograms and non-parametric MRF methods have demonstrated impressive results in texture classification and segmentation [2,6,32,34,40–42]. Following this in the present study, we investigate the performance of distributions of local parameter estimates. The localized parameter estimation captures the spatial dependencies in the texture and the histogram construction captures their structural spatial variations.

Another concern in classical GMRF features is that the analysis of spatial interactions is limited to a relatively small neighbourhood, i.e. the usage of small neighbourhood sizes or low model orders [2]. During the parameter estimation, model order should be approximately equal to the pattern size. Such a model order preserves the Markovianity [24]. But the pattern size is usually unknown. Also if the model order is freely increased to follow the pattern size, the number of interaction parameters in the model increases quadratically. Such an increase in the model parameters leads to a computationally more expensive estimation process. These difficulties have been observed before and many studies in the literature have been directed to choose manually fixed small neighbourhood sizes [14,17,18,23,43]. As a result, the adequacy of these features to characterize textures of various pattern scales is rarely checked [2,41]. However, the dependence of proposed localized parameter estimation method on selected neighbourhood size is small. In fact small neighbourhood sizes are generally more favourable for constructing local feature distributions. This is because it enables ease of fitting localized models onto small regions and achieves spatial variations of estimates which more closely resemble the spatial structure of the texture.

Therefore, in this study we explore the relevance and performance of distributions of local parameters estimates of GMRFs as an improved GMRF based texture descriptor.

The local conditional probability distribution of GMRF model encapsulates spatial dependencies between a pixel and its neighbours. This probability distribution associates any pixel with its neighbours in a Gaussian function [11]. Let Ω={s
                     =(i, j)|1≤
                     i
                     ≤
                     H, 1≤
                     j
                     ≤
                     W} represent the set of grid points on a H
                     ×
                     W regular lattice corresponding to an image region. The image region on Ω is pre-processed to have zero mean. The intensity value of the pixel at the location s is given by y
                     
                        s
                      and N denotes the set of relative positions of its neighbours. Then the local conditional probability density function has the form
                        
                           (1)
                           
                              
                                 
                                    
                                       
                                       p
                                       
                                       
                                          
                                             
                                                y
                                                s
                                             
                                             |
                                             
                                                y
                                                
                                                   s
                                                   +
                                                   r
                                                
                                             
                                             ,
                                             r
                                             ∈
                                             N
                                          
                                       
                                       =
                                    
                                 
                                 
                                    
                                       
                                       
                                       
                                          1
                                          
                                             
                                                2
                                                π
                                                
                                                   σ
                                                   2
                                                
                                             
                                          
                                       
                                       exp
                                       
                                          
                                             −
                                             
                                                1
                                                
                                                   2
                                                   
                                                      σ
                                                      2
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            y
                                                            s
                                                         
                                                         −
                                                         
                                                            
                                                               ∑
                                                               
                                                                  r
                                                                  ∈
                                                                  N
                                                               
                                                            
                                                            
                                                         
                                                         
                                                         
                                                            α
                                                            r
                                                         
                                                         
                                                            y
                                                            
                                                               s
                                                               +
                                                               r
                                                            
                                                         
                                                      
                                                   
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     where α
                     
                        r
                      is the interaction coefficient which measures the influence on a pixel by a neighbour intensity value at the relative neighbour position r. For simplicity only the square neighbourhoods of size n
                     ×
                     n pixels are used in this study for N and n is a positive odd integer value. The neighbourhood size n is referred to as the model order.

The pixels in symmetric positions about pixel s are assumed to have identical parameters [10,44], i.e. α
                     
                        r
                     
                     =
                     α
                     −
                        r
                      with 
                        r
                        ∈
                        
                           N
                           ˜
                        
                      where 
                        
                           N
                           ˜
                        
                      is the asymmetric neighbourhood such that if 
                        r
                        ∈
                        
                           N
                           ˜
                        
                     , then 
                        −
                        r
                        ∉
                        
                           N
                           ˜
                        
                      and 
                        N
                        =
                        
                           
                              r
                              |
                              r
                              ∈
                              
                                 N
                                 ˜
                              
                           
                        
                        ∪
                        
                           
                              r
                              |
                              −
                              r
                              ∈
                              
                                 N
                                 ˜
                              
                           
                        
                      
                     [11]. Therefore, the number of interaction parameters in the model is (n
                     2
                     −1)/2.

The model is then reduced to
                        
                           (2)
                           
                              
                                 
                                    
                                       
                                       p
                                       
                                       
                                          
                                             
                                                y
                                                s
                                             
                                             |
                                             
                                                y
                                                
                                                   s
                                                   +
                                                   r
                                                
                                             
                                             ,
                                             r
                                             ∈
                                             N
                                          
                                       
                                       =
                                    
                                 
                                 
                                    
                                       
                                       
                                       
                                          1
                                          
                                             
                                                2
                                                π
                                                
                                                   σ
                                                   2
                                                
                                             
                                          
                                       
                                       exp
                                       
                                          
                                             −
                                             
                                                1
                                                
                                                   2
                                                   
                                                      σ
                                                      2
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            y
                                                            s
                                                         
                                                         −
                                                         
                                                            
                                                               ∑
                                                               
                                                                  r
                                                                  ∈
                                                                  
                                                                     N
                                                                     ˜
                                                                  
                                                               
                                                            
                                                            
                                                         
                                                         
                                                         
                                                            α
                                                            r
                                                         
                                                         
                                                            
                                                               
                                                                  y
                                                                  ¯
                                                               
                                                               
                                                                  s
                                                                  +
                                                                  r
                                                               
                                                            
                                                         
                                                      
                                                   
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     where 
                        
                           
                              
                                 y
                                 ¯
                              
                              
                                 s
                                 +
                                 r
                              
                           
                        
                        =
                        
                           
                              
                                 y
                                 
                                    s
                                    +
                                    r
                                 
                              
                              +
                              
                                 y
                                 
                                    s
                                    −
                                    r
                                 
                              
                           
                        
                     . The model parameters are estimated using LSE or MLE.

The choice of using least squares estimation (LSE) for parameter estimation for GMRF is suggested by Ref. [14]. The main assumption behind the LSE method is that because the probability distribution in Eq. (2) is Gaussian, the estimated value of y
                        
                           s
                         is more probable to be the mean value of the function [10]. Therefore, the error or the residual will be
                           
                              (3)
                              
                                 
                                    ϵ
                                    s
                                 
                                 =
                                 
                                    y
                                    s
                                 
                                 −
                                 
                                    
                                       ∑
                                       
                                          r
                                          ∈
                                          
                                             N
                                             ˜
                                          
                                       
                                    
                                    
                                 
                                 
                                 
                                    α
                                    r
                                 
                                 
                                    
                                       
                                          y
                                          ¯
                                       
                                       
                                          s
                                          +
                                          r
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

For least squares fitting, given a stationary texture, sample neighbourhoods of the texture are extracted by linear scanning of the region. Overlapping neighbourhoods are also allowed [8]. Let the interaction parameter vector be 
                           α
                           =
                           col
                           
                              
                                 
                                    α
                                    r
                                 
                                 |
                                 r
                                 ∈
                                 
                                    N
                                    ˜
                                 
                              
                           
                         and the neighbour value vector at the location s be 
                           
                              
                                 
                                    y
                                    ¯
                                 
                                 s
                              
                           
                           =
                           col
                           
                              
                                 
                                    
                                       
                                          y
                                          ¯
                                       
                                       
                                          s
                                          +
                                          r
                                       
                                    
                                 
                                 |
                                 r
                                 ∈
                                 
                                    N
                                    ˜
                                 
                              
                           
                           .
                         Then the least squares solution is
                           
                              (4)
                              
                                 α
                                 =
                                 arg
                                 
                                    min
                                    α
                                 
                                 
                                    
                                       ∑
                                       
                                          s
                                          ∈
                                          Ω
                                       
                                    
                                    
                                 
                                 
                                 
                                    ϵ
                                    s
                                    2
                                 
                                 =
                                 arg
                                 
                                    min
                                    α
                                 
                                 
                                    
                                       ∑
                                       
                                          s
                                          ∈
                                          Ω
                                       
                                    
                                    
                                 
                                 
                                 
                                    
                                       
                                          
                                             
                                                y
                                                s
                                             
                                             −
                                             
                                                α
                                                T
                                             
                                             
                                                
                                                   
                                                      y
                                                      ¯
                                                   
                                                   s
                                                
                                             
                                          
                                       
                                       2
                                    
                                 
                                 .
                              
                           
                        
                     

By setting the first derivative to zero the parameter values can be obtained as
                           
                              (5)
                              
                                 α
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   ∑
                                                   
                                                      s
                                                      ∈
                                                      Ω
                                                   
                                                
                                                
                                             
                                             
                                             
                                                
                                                   
                                                      y
                                                      ¯
                                                   
                                                   s
                                                
                                             
                                             
                                                
                                                   
                                                      y
                                                      ¯
                                                   
                                                   s
                                                   T
                                                
                                             
                                          
                                       
                                       
                                          −
                                          1
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             ∑
                                             
                                                s
                                                ∈
                                                Ω
                                             
                                          
                                          
                                       
                                       
                                       
                                          
                                             
                                                y
                                                ¯
                                             
                                             s
                                          
                                       
                                       
                                          y
                                          s
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

The variance parameter of the model is then calculated by
                           
                              (6)
                              
                                 
                                    σ
                                    2
                                 
                                 =
                                 
                                    1
                                    
                                       Ω
                                    
                                 
                                 
                                    
                                       ∑
                                       
                                          s
                                          ∈
                                          Ω
                                       
                                    
                                    
                                 
                                 
                                 
                                    
                                       
                                          
                                             
                                                y
                                                s
                                             
                                             −
                                             
                                                α
                                                T
                                             
                                             
                                                
                                                   
                                                      y
                                                      ¯
                                                   
                                                   s
                                                
                                             
                                          
                                       
                                       2
                                    
                                 
                                 .
                              
                           
                        
                     

The model parameters 
                           α
                         and σ are constant over the domain Ω for a particular stationary texture. Therefore, the model parameter vector f
                        =[
                           α
                        
                        
                           T
                        ,
                        σ]
                           T
                         characterizes the texture and can be used as texture features.

In a texture segmentation problem Ω will be a size w
                        ×
                        w pixel estimation window and the features are obtained at each pixel by sliding the estimation window. These GMRF features are referred to as classical adaptive GMRF features [19]. To obtain consistent estimates the number of samples obtained from the estimation window should be sufficiently greater than the number of parameters [10]. Hence, using comparatively larger estimation window sizes, which capture sufficiently large homogeneous texture regions, is more common in extracting classical adaptive GMRF features [11,12].

The benefits of a GMRF model are its simplicity and ease of parameter estimation. GMRF represents the conditional expectation via a linear model which is the weighted sum of neighbour values (see Eq. (2)). The parameter estimation procedure therefore becomes a linear least squares regression problem where analytical solutions simply exist.

However, in the case where a linear regression function does not well capture the underline true model of data, estimates are biased and are over-smoothed leading to poor results [38]. This happens in texture modelling with GMRFs, especially where texture inside the estimation window is more structured and deterministic [10]. This is a major drawback in GMRF linear regression parameter estimation.

As a solution, the localized version of model fitting can be explained by using local linear regression technique, a non-parametric method with more flexibility in estimating regression functions [35]. Local linear regression exploits the fact that, over a small enough subset of the domain, any sufficiently nice function can be well approximated by locally straight lines or hyper-planes (in higher dimensions) [38], i.e. relaxation from a globally linear model to one that is locally linear. Local linear regression closely models the underlying function by fitting a different but simple model separately at each pixel. This method belongs to a category of regression techniques known as kernel smoothing techniques [35]. Here, the kernels are mostly used as a device for localization.

In adaptive GMRF feature extraction (for segmentation), the difference between linear regression and the local linear regression (sometimes referred to as small model estimation here) is the selection of the estimation window size. This is explained by an example in Fig. 1
                      for a model with one predictor. The continuous blue line shows the linear regression fit and dashed red line illustrates the local linear regression fit. It can be clearly seen from Fig. 1 that the coefficients of ordinary linear regression represent an over-smoothed model. In a texture feature extraction task this is a great disadvantage because it misses out some important structural information about the texture. Unlike in a synthesis problem, discriminative features are required rather than reliable modelling of the texture in classification and segmentation problems. Therefore, in this study we explore the performance difference that can be achieved by non-parametric localized linear regression models.

Here, rather than fitting a linear model to the entire set of observations in Ω, local linear regression fits a simple model to only a small subset of observations in a region Ω
                        
                           s
                         local to each pixel minimizing the local error
                           
                              (7)
                              
                                 
                                    α
                                    s
                                 
                                 =
                                 arg
                                 
                                    min
                                    
                                       α
                                       s
                                    
                                 
                                 
                                    
                                       ∑
                                       
                                          s
                                          ∈
                                          
                                             Ω
                                             s
                                          
                                       
                                    
                                    
                                 
                                 
                                 
                                    ϵ
                                    s
                                    2
                                 
                                 =
                                 arg
                                 
                                    min
                                    
                                       α
                                       s
                                    
                                 
                                 
                                    
                                       ∑
                                       
                                          s
                                          ∈
                                          
                                             Ω
                                             s
                                          
                                       
                                    
                                    
                                 
                                 
                                 
                                    
                                       
                                          
                                             y
                                             s
                                          
                                          −
                                          
                                             
                                                α
                                                s
                                             
                                             T
                                          
                                          
                                             
                                                y
                                                ¯
                                             
                                             s
                                          
                                       
                                    
                                    2
                                 
                              
                           
                        leading to
                           
                              (8)
                              
                                 
                                    α
                                    s
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   ∑
                                                   
                                                      s
                                                      ∈
                                                      
                                                         Ω
                                                         s
                                                      
                                                   
                                                
                                                
                                             
                                             
                                             
                                                
                                                   
                                                      y
                                                      ¯
                                                   
                                                   s
                                                
                                             
                                             
                                                
                                                   
                                                      y
                                                      ¯
                                                   
                                                   s
                                                   T
                                                
                                             
                                          
                                       
                                       
                                          −
                                          1
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             ∑
                                             
                                                s
                                                ∈
                                                
                                                   Ω
                                                   s
                                                
                                             
                                          
                                          
                                       
                                       
                                       
                                          
                                             
                                                y
                                                ¯
                                             
                                             s
                                          
                                       
                                       
                                          y
                                          s
                                       
                                    
                                 
                              
                           
                        and the variance parameter of the model
                           
                              (9)
                              
                                 
                                    σ
                                    s
                                    2
                                 
                                 =
                                 
                                    1
                                    
                                       
                                          Ω
                                          s
                                       
                                    
                                 
                                 
                                    
                                       ∑
                                       
                                          s
                                          ∈
                                          
                                             Ω
                                             s
                                          
                                       
                                    
                                    
                                 
                                 
                                 
                                    
                                       
                                          
                                             
                                                y
                                                s
                                             
                                             −
                                             
                                                
                                                   
                                                      α
                                                      s
                                                   
                                                   T
                                                
                                             
                                             
                                                
                                                   
                                                      y
                                                      ¯
                                                   
                                                   s
                                                
                                             
                                          
                                       
                                       2
                                    
                                 
                                 .
                              
                           
                        
                     

The estimation window is now Ω
                        
                           s
                         instead of Ω. When Ω
                        
                           s
                         approaches Ω, classical GMRF parameters can be achieved and parameter dependence on the linear model assumption increases. The classical adaptive GMRF parameter estimation can be easily adjusted into local linear regression parameter estimation by reducing the estimation window size w while addressing computational inconsistencies that can arise by small estimation windows. Note that this is equivalent to using a uniform kernel or in other words a weighting function where samples inside Ω
                        
                           s
                         are weighted by a factor one and samples outside Ω
                        
                           s
                         are weighted by a factor zero [35].

From the viewpoint of a homogeneous texture, classical adaptive GMRF feature extraction assumes that the linear regression coefficients are constant over a homogeneous texture. However, parameters estimated at each pixel by local linear regression have a higher spatial variance and suggest that there are multiple spatially adjacent distinct linear models contributing to form a texture.

Also the spatial variations in parameter estimates are much higher when a selected model order n is inadequate to describe the texture. For example, in Fig. 2
                         the chosen neighbourhood size n is smaller than the actual texel size. The pixel A and pixel B have different interactions with their corresponding n
                        =7 neighbourhoods (Fig. 2b). For that reason interaction parameters should be able to have some degree of spatial variations when low order GMRFs are involved. In such a situation the localized modelling can better describe the underlying process which minimizes the smoothing incurred by parameter estimation with larger sample sizes. Localized parameter estimation overcomes the strong dependence on choice of the model order, n. It enables the usage of low order GMRF models to be more relevant and also preserves structural spatial variations in large texture patterns.

The model parameter vector 
                           
                              f
                              s
                           
                           =
                           
                              
                                 
                                    
                                       
                                          α
                                          s
                                       
                                       T
                                    
                                 
                                 
                                    σ
                                    s
                                 
                              
                              T
                           
                         at a pixel corresponds to a locally fitted GMRF model simply referred here as the small model.

To overcome the smoothing problem associated with ordinary linear regression in GMRF context other alternative techniques such as the non-linear regression can also be used. Also the autoregressive conditional heteroskedasticity (ARCH) model is another approach to model more complex processes. However, the local linear regression does not strongly depend on the assumption about the regression function and is very flexible and simple to employ in modelling complex deterministic structures. The generalized case of local linear regression is the local regression (LOESS) where instead of the linear model the flexibility of using non-linear regression is also allowed [35]. In this study we use the local linear regression by altering the estimation window size w to transfer from classical adaptive GMRF parameter estimation. We also show that the features based on local linear regression can bring a significant improvement to the GMRF based texture feature extraction task.

The size of Ω
                     
                        s
                      region plays a significant role in the local estimation results. The region Ω
                     
                        s
                      corresponds to the estimation window of size w
                     ×
                     w pixels or a uniform kernel with weights equal to one. In kernel regression literature different methods of selecting the kernel and its parameters have been discussed [35]. However, in our study we keep the direct relation to GMRF linear regression and only reduce the size of the estimation window w, and most importantly address the issues arising from the use of a smaller sample size in estimation process.

When reducing the size of the estimation window, the number of samples for estimation process becomes limited. This causes two major related problems. One is that the estimation process can become inconsistent or an ill-posed problem because there is a matrix inversion involved with the estimation of interaction parameters (see Eq. (8)). The other is that too few training samples can result in regressions with incorrectly steep extrapolations [38].

To overcome the first problem generalized inverse also known as Moore–Penrose inverse [16] is used in this study. When a small number of samples are involved, there might be correlations among these samples. Hence the matrix 
                        
                           
                              
                                 
                                    ∑
                                    
                                       s
                                       ∈
                                       
                                          Ω
                                          s
                                       
                                    
                                 
                                 
                              
                              
                              
                                 
                                    
                                       y
                                       ¯
                                    
                                    s
                                 
                              
                              
                                 
                                    
                                       y
                                       ¯
                                    
                                    s
                                    T
                                 
                              
                           
                        
                      can become rank deficient. The generalized inverse acquires an approximated inverse by singular valued decomposition (SVD) when a matrix becomes rank deficient [16]. The SVD approximates the matrix inverse by eliminating relatively small singular values and calculating a truncated Moore–Penrose inverse [16,45]. When the matrix inversion in Eq. (8) exists the generalize inverse produces the same results as the ordinary matrix inversion.

Next we make sure the number of samples is always more than the number of model parameters. The number of interaction parameters depends on the size of the neighbourhood and is equal to (n
                     2
                     −1)/2. Since the samples are extracted from inside region of w
                     ×
                     w estimation window, the number of overlapping samples that can achieve from the window is equal to (w
                     −
                     n
                     +1)2. When the number of samples is less than the number of model parameters infinite number of solutions can exist. Therefore, we use the following rule to find a value for w. The number of samples must be greater than or equal to the number of interaction parameters, i.e. (w
                     −
                     n
                     +1)2
                     ≥(n
                     2
                     −1)/2. Therefore we select the case which is well above the equality criteria, that is number of samples is equal to n
                     2. This leads to the value of w
                     =2n
                     −1. The value for size w selection is illustrated in Fig. 3
                     . Hence in this study for local linear regression the estimation window size is selected as w
                     =2n
                     −1.

However, still incorrect large values for parameter estimates can occur due to outliers among the few samples. This can be overcome by regularization, for example by using ridge regression [35] which minimizes the L2 norm of the interaction parameter vector 
                        α
                     
                     
                        s
                     .


                     
                        
                           (10)
                           
                              
                                 α
                                 s
                              
                              =
                              arg
                              
                                 min
                                 
                                    α
                                    s
                                 
                              
                              
                                 
                                    ∑
                                    
                                       s
                                       ∈
                                       
                                          Ω
                                          s
                                       
                                    
                                 
                                 
                              
                              
                              
                                 
                                    
                                       
                                          
                                             y
                                             s
                                          
                                          −
                                          
                                             
                                                
                                                   α
                                                   s
                                                
                                                T
                                             
                                          
                                          
                                             
                                                
                                                   y
                                                   ¯
                                                
                                                s
                                             
                                          
                                       
                                    
                                    2
                                 
                              
                              +
                              
                                 c
                                 2
                              
                              
                                 
                                    
                                       α
                                       s
                                    
                                    T
                                 
                              
                              
                                 α
                                 s
                              
                           
                        
                     where c is a constant which controls the trade-off between minimizing the error and penalizing the magnitude of the parameters. This regularized local linear regression estimation results in
                        
                           (11)
                           
                              
                                 α
                                 s
                              
                              =
                              
                                 
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   s
                                                   ∈
                                                   
                                                      Ω
                                                      s
                                                   
                                                
                                             
                                             
                                          
                                          
                                          
                                             
                                                
                                                   y
                                                   ¯
                                                
                                                s
                                             
                                          
                                          
                                             
                                                
                                                   y
                                                   ¯
                                                
                                                s
                                                T
                                             
                                          
                                          +
                                          
                                             c
                                             2
                                          
                                          I
                                       
                                    
                                    +
                                 
                              
                              
                                 
                                    
                                       
                                          ∑
                                          
                                             s
                                             ∈
                                             
                                                Ω
                                                s
                                             
                                          
                                       
                                       
                                    
                                    
                                    
                                       
                                          
                                             y
                                             ¯
                                          
                                          s
                                       
                                    
                                    
                                       y
                                       s
                                    
                                 
                              
                           
                        
                     where 
                        I
                      is the identity matrix and [.]+ is the generalized inverse notation.

By 1) using generalize inverse, 2) ensuring the number of samples is greater than the number of model parameters and 3) applying regularization, we can well assure a promising local parameter estimation process with local linear regression. After estimating interaction parameters, variance parameter is obtained according to Eq. (9).

Distributions of locally extracted features are a popular choice in state-of-the-art texture feature extraction [2,6,40]. Local linear regression leads to spatial variations in the parameter space capturing local structures of a texture. Therefore we use the distributions of local parameters as a novel improved texture descriptor.

The estimation of parameters at each pixel by fitting local small models results in local parameter image stack. A parameter image corresponds to the spatial domain of one specific parameter in the vector f
                     
                        s
                     . These local parameter estimates contain more spatial variations than classical adaptive GMRF parameters. Also these local parameters inherit spatial repetitiveness according to the repetitive pattern of a given homogeneous texture.

On each parameter image, the normalized parameter histograms are constructed by sliding another window of size b
                     ×
                     b pixels to formulate the distribution of local parameters. These features are referred to as local parameter histograms (LPHs). The LPH construction is illustrated in Fig. 4
                     .

The number of bins is manually fixed. For example, if n
                     =3 is selected, there are five different model parameters governing the GMRF model ((n
                     2
                     −1)/2=4 — four interaction parameters and the variance parameter). If 10 bins for the PL histogram are fixed, then there will be a 50 dimensional feature vector. However, this does not suffer from the curse of dimensionality in this problem, mainly because the addition of more dimensions brings more discriminative information and enriches the discriminative power of the descriptors.

When the estimation window size w is smaller, estimated local parameters have more spatial variations. This gives a higher information content to the histograms of such local parameters. However, to meet the consistency criteria we have bound the minimum size of w to w
                        =2n
                        −1 which depends on the model order, n, as explained in Section 5.

In Fig. 5
                        , the entropy of the LPH with the estimation window size is shown. Here, the entropy of the histogram depicts its ability to capture spatial variations in the local parameters associated with the texture. Higher entropy values suggest that more spatial variations are captured and vice versa. When w increases, entropy gradually drops implying that spatial variations are progressively smoothed out by the estimation process. Therefore, smaller estimation windows are also preferable to construct local feature distributions.

@&#RESULTS AND DISCUSSION@&#

In this section, we present the results of our improved texture descriptor addressing the texture segmentation problem. We formulate our experiments around analysing three main aspects. First the improvement in discriminative ability of LPH is tested against the classical adaptive GMRF method which is the direct ancestor of LPH features. Next the LPH features are compared against some standard state-of-the-art efficient texture descriptors based on local feature distributions to find out its place among other texture descriptors. Finally, we apply the proposed features in solving the challenging problem of natural image segmentation. The results are compared with the current state-of-the-art segmentation algorithms for natural images. Prior to presenting the results, the data and parameter setting used in the experiments are explained.

In our general texture segmentation experiments we use mosaic data sets for quantitative evaluation of the proposed features. The mosaic images are constructed using the four texture databases namely, BRODATZ [46], PRAGUE [47], CURET (grey) [48] and UIUC [49]. Here, grey-scale mosaic images comprising four different texture regions are considered. Some representative mosaic images from each dataset are shown in Fig. 6
                        . BRODATZ, PRAGUE, CURET and UIUC mosaic datasets are comprised of 15, 60, 1380 and 240 mosaic images respectively. Grey-scale textures are considered here because if the texture descriptor works for this setup then it can be easily extended to colour texture segmentation with promising performances.

The texture feature extraction method proposed here involves some global variables such as n,w,b and number of bins. We fix these variables manually or based on empirical evaluations.

In order to construct LPH descriptors n,w,b and bins should be pre-defined. The neighbourhood size is kept small and set to n
                        =5 in this study. The estimation window size is selected as w
                        =2n
                        −1 to achieve consistency as discussed in Section 5. Therefore, w
                        =9 is used when n
                        =5.

When the histogram construction window size b is large, spatial structures of larger texture patterns can be well captured however, when it is small boundary localization is improved. Our experiments indicate that a window size in the range of b
                        =15 to b
                        =25 works well. Therefore, b
                        =21 is selected in the following experiments. The number of bins for constructing a parameter histogram is fixed to bins
                        =10. For extracting classical adaptive GMRF features n
                        =5 and w
                        =21 are used similar to Ref. [11].

For unsupervised segmentation, the k-means clustering algorithm is used with the general texture segmentation experiments. The k-means clustering algorithm is used here to emphasize the discriminative power of the texture descriptor alone, without inquiring much reinforcement from segmentation method to the final segmented image. The L1 norm distance metric between histograms is used to measure the difference of two local parameter histograms. The percentage error rate se
                         is calculated as the ratio between the number of incorrectly segmented pixels and the total pixels in the image.

The regularization parameter value c
                        =100 is chosen for feature extraction of natural images. For general texture segmentation regularization is not employed (c
                        =0). We have found that the regularization does not make much difference in performance for general textures where the texture patterns are more regular without much disturbances of outliers. But for the natural images, where textures have large intra-class variations produced by texture contaminations, the regularization can significantly contribute to achieve better segmentation performances. This is discussed in Section 7.5 with natural image processing.

We begin with the qualitative evaluation of the features extracted from classical adaptive GMRF method and LPH descriptors. Fig. 7
                         illustrates LPH of horizontal and vertical interaction parameters and the variance parameter of GMRF model belonging to two different homogeneous textures. Note that the features are from one randomly chosen estimation window. The estimated values from the classical adaptive GMRF method are shown by dash lines in Fig. 7. These estimates have small differences between two textures even though the displayed textures have large visual dissimilarities. In contrast, LPH descriptors demonstrate a significant difference in their distributions.


                        Fig. 8
                         presents the pixel-wise adaptive features extracted from both methods for some given mosaic images. The dimensions of the original feature vectors are reduced to two dimensions using principal component analysis (PCA) for illustration purposes. Each data point in Fig. 8 represents the reduced-dimension feature vector at a pixel.

In the classical adaptive GMRF estimation process, the texture contained in the estimation window should be homogeneous. However near texture boundaries, non-homogeneity can occur leading to inaccurate features which cause boundary localization problems [7,41]. The classical adaptive GMRF method involves a large estimation window size compared to that of LPH parameter estimation. This means that in classical adaptive GMRF parameter estimation the boundary localization problem is more prominent. Therefore, LPH has a reduced overlap and a higher separability (inter-class distance) between the two texture classes (Fig. 8c,d). Also LPH contains more local information about the texture than classical adaptive GMRF method leading to a tighter dispersion in class features, conveying lower intra-class variation (Fig. 8d).

Next we perform an empirical study on segmentation with mosaic datasets, for a quantitative evaluation of the performance improvement. The segmentation error se
                         of all the mosaic images in each dataset is measured. The median of segmentation error and the interquartile range (IQR) is shown in Table 1
                        . As observed from Table 1, LPH features substantially increase the discriminative power of classical adaptive GMRF features. The IQR value is comparatively small for LPH suggesting that the results are more robust. Note that the median error here is a percentage value between 0 and 100%. We have not taken any special precautions for boundary localization here (for example, using an advanced segmentation algorithm [7]). Hence the median error of LPH is between 5 and 8%. However, for the comparison purposes we do not depend on achieving perfection because we adapt the same setup for the other method in the comparison process.

A few examples of the best segmented mosaic images are displayed in Fig. 9
                        .

In the previous section, it is clearly illustrated that there is a significant improvement in discrimination power of adaptive GMRF by formulation of LPH descriptors. In this section, we compare the segmentation performance of proposed features with other standard texture descriptors. Our interest is on the texture features rather than the segmentation algorithm. Therefore, k-means clustering is again chosen. However, other improved clustering and optimization methods could be used to further improve the final segmentation results.

The local binary patterns (LBP) texture descriptor is one of the popular state-of-the-art efficient texture descriptors in texture classification and segmentation [6]. On the other hand, filter based Gabor texture descriptors are another well known method in texture analysis which closely relates to the biological vision system [2]. These methods are extensively used in texture feature extraction to this day in many object recognition and image segmentation studies [3–5,50–55]. Some studies have pointed out that these texture features can perform better than classical adaptive GMRF features [2,32–34].

Therefore, rotational invariant uniform local binary patterns (LBP) [6] and spectral histograms (SH) [2], representing structural and spectral texture features respectively, are used to compare the segmentation performance. The histogram calculation window size is kept as b
                        =21 and the contrast information is not included similar to the LPH construction process.

The comparison results are shown in Fig. 10
                         as boxplots of segmentation errors. LPH features have the best performances for PRAGUE, CURET and UIUC datasets except for the BRODATZ dataset where SH features perform better. The UIUC dataset has large texture patterns compared to other textures (see Fig. 6). However, LPH descriptors have achieved comparatively good performance on this dataset as well, considering the other two methods. The CURET dataset comprises of mostly fine textures. Therefore, in Fig. 10c it can be observed that the LPH performs well on fine textures compared to other datasets. These results show that the LPH descriptors are reliable discriminative texture descriptors for segmentation which achieve better results compared to the state-of-the-art methods.

Unlike mosaic texture, natural textures tend to be more random and contain global inhomogeneities. They also contain textured regions as well as smoothed and near piece-wise constant intensity regions. LPH features are used to segment natural images from the Berkeley dataset [57]. The parameter setting is kept as n
                        =3 and b
                        =21. Here also k-means clustering algorithm is used as the default segmentation method. The k-means clustering algorithm disregards the spatial dependency property of adjacent neighbour labellings. Therefore, smoothing of bin images by an average filter is carried out to reduce the isolated misclassified pixels. Segmentation using more advanced segmentation algorithms also has been considered later.

The grey-scale images are used to extract the texture features. Employing texture features from the grey-scale version of the image is efficient and sufficient to capture the required properties of the texture. In human perception of natural scenes both colour and texture are efficiently processed to give an accurate understanding of the image regions. The dominating property of the region recognition interchanges between colour and texture. For example, smoothed regions may be predominantly identified by their colour and regions with patterns are predominantly recognized by texture. Therefore, we integrate the colour information of the image to the texture features obtained from the grey-scale image. Colour and texture features are extracted separately.

For the colour feature extraction the dominant RGB colours inspired by the study in Ref. [58] are used. A window of size 11×11 is used to scan over each of the three RGB colour planes. Next the first three dominant colours are found based on histogram frequencies and stored individually. Therefore at a pixel a nine dimensional colour feature vector is constructed based on the three RGB colour planes. Finally, the colour feature stack is adjoined to the texture feature stack and PCA is performed to reduce the dimensionality. The complete colour image segmentation process is illustrated in Fig. 11
                        . The dimensionality of the full feature vector is reduced to a 25 dimensional feature vector which is sufficient to achieve good segmentation performances. The integration of colour information also further improves the region boundary localization [58].

The results of colour image segmentation are shown in Fig. 12
                        . For comparison, the results from a state-of-the-art natural image segmentation method using lossy compression which is available from Ref. [59] are also presented [56]. The images containing more textured regions are specially considered. Our segmentation results are comparable with their results. However the boundary localization is better in lossy compression technique. This is because in our method the segmentation method (k-means clustering algorithm) does not provide any additional advantage for boundary localization. For the cases where background and foreground textures have little colour differences our method has performed better implying our texture feature extraction method is stronger and more robust than that of lossy compression technique [56].

It is interesting to analyse the segmentation performance solely based on texture features without integrating colour. Grey-scale image segmentation is performed only using LPH texture features and compared against a grey-scale texture segmentation method, the integrated active contours, proposed in Ref. [5]. Here in our method, we also use the integrated active contour method as the segmentation method instead of the k-means clustering algorithm. The results are shown in Fig. 13
                        . The proposed features perform well and achieve comparable results with Gabor filters based texture features used in Ref. [5] and they are also comparative with the results reported in Ref. [60].

Finally, we discuss the effect of changing the regularization parameter c and selecting the value of c. The experiments performed here suggest that it is essential to have the regularization in processing natural images. The general texture images in standard datasets are acquired under certain predefined controlled environments. However, natural images are acquired under diversely varying settings of illumination, focus, background, etc. Natural images therefore contain considerable variations from overly smoothed regions to higher amounts of disturbances to normal homogeneous flow of a texture. Therefore, a method like LPH which includes a localized model estimation stage has to be regularized to have the best performance with natural image processing.


                        Fig. 14
                         shows the segmentation behaviours with changing values of c. It can be seen that for general texture images such as img3 in Fig. 14(a), the regularization does not make much difference in the performance, although for natural images it is significant. Therefore, for natural images which contain more abrupt texture variations which are possibly considered as outliers, regularization in parameter estimation is important.

@&#CONCLUSIONS@&#

In this paper, we have introduced a novel improved texture descriptor based on GMRFs. Although much research have been proposed to improve discriminative power of GMRFs based features, localized feature formulation and their distributions have not been given proper consideration. In this study with simple alterations and posing carefully selected restrictions to the classical GMRF parameter estimation process, the localized parameter estimates are obtained. These localized parameter estimates are able to well preserve structural information and their distributions form an effective texture feature. Also these features are less dependent on model order selection and Gaussian restrictions, however, they harness the advantage of simplicity of parameter estimation in GMRFs. The distributions of local parameters named as LPH descriptors significantly improve the discriminative power of classical GMRF features. Also they show robust effective results compared to the state-of-the-art texture descriptors. Impressive natural image segmentation results have been achieved using LPH descriptors with k-means clustering algorithm. However, the boundary localization of segmentation process can be further improved by integrating colour information and using advanced segmentation methods.

@&#REFERENCES@&#

