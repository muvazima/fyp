@&#MAIN-TITLE@&#An improved distance-based relevance feedback strategy for image retrieval

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           The performance of existing nearest neighbor approaches is studied in a CBIR context.


                        
                        
                           
                           Several issues on the application of NN methods to the CBIR problem are analyzed.


                        
                        
                           
                           An improved relevance feedback algorithm also based on the NN paradigm is proposed.


                        
                        
                           
                           The accuracy of the estimates is improved by considering locality of labeled samples.


                        
                        
                           
                           Experimental results evidence significant improvements in most cases.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

CBIR

Image retrieval

Relevance feedback

Nearest neighbor

@&#ABSTRACT@&#


               
               
                  Most CBIR (content based image retrieval) systems use relevance feedback as a mechanism to improve retrieval results. NN (nearest neighbor) approaches provide an efficient method to compute relevance scores, by using estimated densities of relevant and non-relevant samples in a particular feature space. In this paper, particularities of the CBIR problem are exploited to propose an improved relevance feedback algorithm based on the NN approach. The resulting method has been tested in a number of different situations and compared to the standard NN approach and other existing relevance feedback mechanisms. Experimental results evidence significant improvements in most cases.
               
            

@&#INTRODUCTION@&#

Content based image retrieval (CBIR) refers to the application of techniques to retrieve digital images from large databases, by analyzing the actual content of the image rather than the metadata associated with it.

In general, a CBIR system represents each image in the repository as a set of features (usually related to color, texture and shape), and uses a set of distance functions defined over this feature space to estimate similarity between pictures.

A query can be understood as the intention of a user to retrieve a certain kind of images, and it is usually materialized as one or more sample pictures. The goal of a CBIR system is to retrieve a set of images that is best suited to the user's intention. Obviously, the potential results of such a system will strongly depend not only on the particular features of the representation space but also on the implicit or explicit distance functions used to measure similarity between pictures [1–3].

This way of assessing similarity comes along with the implicit assumption that image resemblance is related to a distance defined over a particular feature space. This leads to the so-called semantic gap, between the semantics induced from the low level features and the real high level meaningful user interpretation of the image. To reduce this gap, relevance feedback has been adopted by most recent CBIR systems [4]. When relevance feedback is used, the search is considered an iterative process in which the original query is refined interactively, to progressively obtain a more accurate result. At each iteration, the system retrieves a series of images according to a pre-defined similarity measure, and requires user interaction to mark the relevant and non-relevant retrievals. This data is used to modify some system parameters and produce a new set of results, repeating the process until a satisfying enough result is obtained. In this context, the relationship between any image in the database and the user's desire is usually expressed in terms of a relevance value. This value is aimed at directly reflecting the interest that the user may have in the image and is to be refined at each iteration.

Most relevance feedback algorithms use the user's selection to search for global properties which are shared by the relevant samples available at each iteration [4]. From a Pattern Recognition viewpoint, this can be seen as obtaining an appropriate estimate of the probability of (subjective) relevance. Many different approaches exist to model and progressively refine these estimates. But relevance feedback faces a small sample problem whose models cannot be reliably established because of the semantic gap. In this context, nonparametric distance-based methods using neighbors are particularly appealing [5–8]. The aim of these methods is to assess relevance of a given image by using distances to relevant and non-relevant neighbors. In particular, an image is considered as much relevant as its distance from the nearest relevant image is small compared to the distance of its nearest non-relevant image.

In the present work, all these considerations about distance-based CBIR approaches are taken into account to derive a novel way of reliably estimating relevance from distances. The algorithm is then evaluated exhaustively in three databases and in a variety of contexts, including both query by example and refinement of a textual search.

The paper is organized as follows. The next section explains the model used; outlines the assumptions made; presents the name conventions used throughout the article; and describes the plain nearest neighbor approach. Section 6 exposes several key facts about the way relevance is estimated and introduces a novel alternative. In the experimental section the proposed algorithm is compared against the original one [5,6], some extensions [7,8], and other representative relevance feedback methods. Finally, the main conclusions of the proposed approach are outlined along with some work in progress.

@&#RELATED WORK@&#

Relevance feedback in CBIR has been an active topic of research for the last two decades. In general, the performance of CBIR algorithms depends critically on the (dis)similarity measure used to rank the images in the repository. This measure is commonly built/adapted at each iteration, by using the information made available by the user. In this section, we summarize some previous work in relevance feedback, with the intention of contextualizing the method presented in the paper. For a more comprehensive survey, the reader is referred to recent reviews on the topic [1,9].

First approaches were aimed at progressively adapting the similarity measure and/or move the query point so that it becomes closer to positive samples and farther from negative ones. Query point movement and axis re-weighting methods fall within this category of techniques [10–12]. In general, these approaches model the query as a point in a (possibly deformed) feature space, and retrieve results according to their distance to the query. A major advantage of these techniques is that they are relatively fast, and scale reasonably well with the size of the repository. On the negative side, they usually ignore dependencies between features [13], treat features globally [14] and are only effective if the query concept consists of a convex region in the feature space.

A different way to approach the definition of an adequate similarity function is from a pattern recognition perspective. Relevance feedback can be considered as a classical machine learning problem, in which the user feedback is used as an input to a learning algorithm to address the classification of images as relevant images [9]. This opens the scope for the application of a large diversity of popular algorithms in this context. For example, labeled samples can be used to build a projection into a subspace of a lower dimensionality where relevant samples appear close to each other [15,16]; or to learn a Mahalanobis metric based on pairwise (dis)similarity constraints [17] or small subsets of points that are known to belong to the same class [18]. Closely related, support vector machines (SVM) methods attempt to find the hyperplane which achieves a maximum separation between two classes [19–22]. One class, two classes and other extensions have been adapted to overcome some of the inherent limitations of standard SVMs, e.g., imbalanced training set, high computational burden [9]. A major problem with some of these methods refers to the high sensibility of the parameters required for fine tuning the algorithms [7]. Other related approaches include the use of neural networks, e.g., radial basic functions (RBF) networks [23], self-organizing maps (SOM) [14], fuzzy sets [24] or regression methods [13]. Despite the many efforts performed in this direction, many of the algorithms suffer from the small sample size problem, caused by the relatively scarce information provided at each relevance feedback iteration.

Other strategies also include Bayesian approaches. In this case, posterior probability distributions are estimated according to the data gathered through the relevance feedback process. In particular, the probability densities of the relevant and non-relevant classes are usually approximated by using different types of estimators [25–29]. Then, the probability of being relevant is used as a similarity measure, as in the case of using a soft classifier. In this way, relevance values are implicitly modeled as a probability distribution, rather than as a single point in the feature space. Nearest neighbor methods can be classified in this category, and used in this context to estimate the posterior probabilities of the relevant and non-relevant classes. In addition, they are compatible with other distance metric learning approaches and can also be used as a framework to determine relevance by using other distances learned from the user feedback. These methods have previously been applied in the CBIR field [5–8], showing a good comparative performance superior to other Bayesian frameworks [5] and classical SVM techniques [7]. In this paper, we build on some of these previous works, by proposing series of strategies to face some fundamental shortcomings of NN-based approaches.

Assume that there is a repository of images 
                           
                              X
                              =
                              
                                 
                                    x
                                    1
                                 
                                 ⋯
                                 
                                    x
                                    m
                                 
                              
                           
                         conveniently represented in a metric feature space, 
                           F
                        , whose associate distance measure is 
                           
                              d
                              :
                              F
                              ×
                              F
                              →
                              
                                 R
                                 
                                    ≥
                                    0
                                 
                              
                           
                        . This particular representation space is assumed to be the D-dimensional space 
                           
                              R
                              D
                           
                         in this work as in much other closely related works. In some cases and specially in the image retrieval context, the representation space may embrace multiple low level descriptors (e.g., color, texture or shape) and the distance d is constructed by a combination from simpler distance measures over each descriptor [1].

When a particular user is interested in retrieving images from 
                           X
                        , his/her intention can be thought of as a semantic concept which can be more or less objectively specified (as e.g., images of bicycles, domestic animals, etc.). Regardless of the scope and specificity of this semantic concept, it can be modeled in the feature space as a probability function over the corresponding repository, P(relevant|x), which can be extended to the whole representation space.

This probability of relevance over the space can be conveniently simplified and tackled. For example, single point query approaches assume that this probability function can be appropriately represented by a single (ideal) point 
                           
                              c
                              ∈
                              F
                           
                         possibly along with a convenient axis or feature re-weighting [30]. This can be seen as equivalent to considering uncorrelated Gaussian distribution functions. This approach can be extended to use a set of representative points or mixtures of Gaussians instead [31].

Single point methods use a distance measure to rank images. Multiple point methods usually combine distances (or rankings) to each representative in a set of (ideal) points 
                           C
                        . In any case, all methods end up using a particular ranking as a final tool to retrieve images regardless of the way they model the query in the feature space. Fig. 1
                         graphically illustrates this situation.

In this paper we assume the most usual case in relevance feedback in which the user marks or labels some images as relevant or non-relevant. In this setup, the available information or user feedback is given by the set of images from 
                           
                              Q
                              ⊂
                              X
                           
                         already seen by the user and marked either as relevant (positive), 
                           
                              Q
                              +
                           
                         or as non-relevant (negative), 
                           
                              Q
                              −
                           
                        . Both disjoint subsets, 
                           
                              Q
                              +
                           
                         and 
                           
                              f
                              
                                 Q
                                 −
                              
                           
                        , can be seen as samples corresponding to the distribution functions that determine P(relevant|x) as in a typical two-class classification setting. This information is then used to produce a particular ranking over the database.

If the set of images shown to the user for labeling was large and representative enough and the user was infallible, we would be facing a standard learning problem that could be tackled with many different methodologies. But the sets involved are very small and poorly representative. Moreover, the user labeling in practice may contain several types of errors. Hence, the learning problem involved is very hard to solve effectively, not only because of small sample size effects and unrepresentativeness, but also because of the strong dependences introduced by the way in which new evidence is progressively taken into account.

Nearest neighbor (NN) methods have been extensively used in the context of learning, vision and pattern recognition due to their well-known, convenient and well studied practical and asymptotic behavior [32,33]. In general, and regardless of whether they are used for classification or estimation, they show a very consistent behavior ranging from the large scale to the small sample size case.

NN approaches base probability estimation on the ratio of neighbors of a certain kind to the volume of the hypersphere that contains them. This is known to be a reasonable estimate for the corresponding probability distributions [34], and has previously been used in the context of image retrieval as a way to compute scores by using the relative distance of an image to its nearest relevant and non-relevant neighbors [9].

In [7], a single nearest neighbor approximation was proposed to estimate the density of the relevant and non-relevant classes as inversely proportional to the volume of the corresponding 1-Neighborhoods, V
                     
                        R
                     (d
                     
                        R
                     (x)) and V
                     
                        N
                     (d
                     
                        N
                     (x)), where the subscripts refer to the nearest relevant (R) and non-relevant (N) neighbors, respectively; and dR
                      and dN
                      are the corresponding distances to each neighbor from x. From the separate estimates and obviating some constant terms and the exponent in the volume formulae, the following expression can be arrived at [7]:
                        
                           (1)
                           
                              
                                 P
                                 
                                    
                                       relevant
                                       |
                                       x
                                    
                                 
                                 =
                                 
                                    
                                       
                                          d
                                          N
                                       
                                       
                                          x
                                       
                                    
                                    
                                       
                                          d
                                          R
                                       
                                       
                                          x
                                       
                                       +
                                       
                                          d
                                          N
                                       
                                       
                                          x
                                       
                                    
                                 
                                 .
                              
                           
                        
                     
                  

For convenience, the ratio of distances in the above formula can be substituted by other equivalent ones from the point of view of ranking relevant images. The most used one is 
                        
                           
                              
                                 
                                    d
                                    N
                                 
                                 
                                    x
                                 
                              
                              
                                 
                                    d
                                    R
                                 
                                 
                                    x
                                 
                              
                           
                        
                     . In a recent work, this ratio has been smoothed by introducing a moderating term that decreases with the distance to the closest relevant image. The modified ratio, 
                        
                           
                              
                                 
                                    d
                                    N
                                 
                                 
                                    x
                                 
                              
                              
                                 
                                    d
                                    R
                                 
                                 
                                    
                                       x
                                    
                                    2
                                 
                              
                           
                        
                     , has been shown to improve the basic one in some cases [8].

In all NN methods, and regardless of the particular expression used to compute the scores (or corresponding probability estimate), this ratio is calculated for each image in the database. Then, results are used to rank the images and the top w (window size) is presented to the user for evaluation.

When multiple descriptors are available (e.g., color, texture or shape), both distance and score combination approaches are compatible with the NN methods. When distance combination methods are used, the distance function d is constructed by a combination from simple distance measures over each descriptor space [1], and scores are computed according to the corresponding ratio expression (e.g., 
                        
                           
                              
                                 
                                    d
                                    N
                                 
                                 
                                    x
                                 
                              
                              
                                 
                                    d
                                    R
                                 
                                 
                                    x
                                 
                              
                           
                        
                     ). Score combination methods would use this ratio expression in each representation space and then combine the results to produce a new score which gathers the contribution of the multiple representations. A common approach for both cases is the use of a linear combination of normalized distances/scores, in which weights reflect the relevance of the each feature representation to the user's need [7]. The following expression is used in [5] to normalize the scores obtained for each descriptor:
                        
                           (2)
                           
                              
                                 relevanc
                                 
                                    e
                                    NN
                                 
                                 
                                    x
                                 
                                 =
                                 1
                                 −
                                 
                                    e
                                    
                                       −
                                       
                                          
                                             
                                                
                                                   d
                                                   N
                                                
                                                
                                                   x
                                                
                                             
                                             
                                                
                                                   d
                                                   R
                                                
                                                
                                                   x
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     where d
                     
                        R
                      and d
                     
                        N
                      represent the distances to the nearest relevant and non-relevant sample respectively.

NN-based relevance feedback gives surprisingly good results in practice, in comparison to other state-of-the-art techniques [7]. Nevertheless and apart from other improvements related to more meaningful or robust representations (g using dissimilarity spaces) or using hybridization techniques (e.g., with Bayesian relevance feedback), there is still room for improvement in the NN approach related to relevance feedback itself.

In this section, several issues on the application of the non parametric NN density estimation to the CBIR problem are carefully analyzed.

A first problem, initially reported in [7], is caused by the relative sizes of the 
                           
                              
                                 Q
                              
                              +
                           
                         and 
                           
                              
                                 Q
                              
                              −
                           
                         sets. In general, the number of relevant items is by far smaller than the number of non-relevant ones, even in the surroundings of the elements in 
                           C
                        . This causes typically the number of elements in 
                           
                              
                                 Q
                              
                              +
                           
                         to be also lower than in 
                           
                              
                                 Q
                              
                              −
                           
                        . When a relevant selection is surrounded by non-relevant ones, the above rankings produce high values in a very small closed region around it. But from the images outside this region, the top ranked ones are those which are far from both relevant and non-relevant samples. This undesirable effect is illustrated in Fig. 2
                         (left). The chances of this type of situation increase with the relevance feedback iterations, as areas around positive selections tend to be explored more in depth.

This problem has been treated in [7,8]. In [7], the score of the conventional NN technique is combined with another obtained by using a Bayesian query shifting (BQS) approach [27]. A linear combination is used, and weights are computed dynamically at each iteration by considering the number of relevant and non-relevant samples. This leads to a stabilized score that combines an exploitation (NN) with an exploration term (BQS). In [8], a conveniently smoothed NN estimate (SNN) was defined by proposing an alternative formulation of the approach that increased the importance of 
                           
                              Q
                              +
                           
                        . In particular, Eq. (2) was modified by introducing a moderating term that rewards pictures which are close to positive selections and penalizes others which are far away. To this end, the following expression was used to compute the relevance scores:
                           
                              (3)
                              
                                 
                                    relevanc
                                    
                                       e
                                       SNN
                                    
                                    
                                       x
                                    
                                    =
                                    1
                                    −
                                    
                                       e
                                       
                                          −
                                          
                                             
                                                
                                                   d
                                                   N
                                                
                                                
                                                   x
                                                
                                             
                                             
                                                
                                                   d
                                                   R
                                                
                                                
                                                   
                                                      x
                                                   
                                                   2
                                                
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

The effect of introducing this modification in the plain NN approach is illustrated in Fig. 2 (right).

Differently populated regions in the feature space cause relevance estimates to work with significantly different sample densities [8]. As the ratio of distances is defined in a global way, densely populated regions with high relevance values will tend to dominate the ranking. This problem is caused both by the possibly uneven distribution of images in the repository, 
                           X
                        , and also because of the complex relationship between perceptual similarity and the distance used to find neighbors, which may in turn be different in different regions of the feature space. That is, the probability of relevance may scale differently with distance in different regions.

A way to overcome this problem consists of building independent rankings for each relevant selection and combining them to produce a more consistent final ordering [8]. As a result, a set of r local searches (one per relevant selection q
                        
                           i
                        
                        + in the set 
                           
                              
                                 Q
                              
                              +
                           
                        ) is carried out. Each of these searches is performed by using Eq. (3), but considering the picture q
                        
                           i
                        
                        + as the only relevant sample. This results in a set of r independent rankings R
                        ={R
                        1 ⋯ R
                        
                           r
                        }, one for each local search. Finally, each picture is assigned a score which is inversely proportional to its best ranking position in the set of rankings R. This technique makes the approach robust against different density areas.

Another problem that was identified in [7] is related to the reliability of the nearest neighbor density estimation under a small sample size. This problem is implicitly associated with the problem at hand, and it is shared with most methods based on machine learning. The use of the k-th distance instead of the first distance used in Eq. (2) could potentially contribute to alleviate this problem [6]. However, the number of relevant samples in the usual case is not sufficient to allow for a reliable probability estimate by using a k-nearest neighbor approach.

Another major limitation is imposed by the fact that the NN density estimation, as most other non-parametric density estimation methods, relies on a labeled set of random samples. This principle is violated in the CBIR problem, in which user judgments (the labeled samples) concentrate on regions of a (probably deformed) feature space. This bias is introduced by the relevance feedback mechanism itself, which does not sample the feature space randomly but rather according to the probability of finding relevant pictures. This yields considerably different sampling densities across the feature space. Hence, the reliability of the scores obtained for each sample also varies across the feature space. On the one hand, for images which lay far apart from populated regions, the samples provide little information about the density function. On the other hand, density estimations for images lying within populated regions are more accurate. Despite the importance and implications of this problem, to the best of our knowledge, it has not even been mentioned in the CBIR literature. In the next section we extend the standard NN method to compensate for this effect.

Common relevance feedback mechanisms are based on processing user feedback on a set of images 
                        
                           Q
                           ⊂
                           X
                        
                     . In most CBIR systems, the elements in 
                        Q
                      are the first w elements in the ranking produced at the previous iteration, and these were computed as those with the highest probability P(relevant|x) to be relevant to the user query. This process leads to a severe biased estimate, and the reliability of the scores obtained varies dramatically across the feature space.

Let us assume that there is a new random variable reliable, which can take a value in the set {true, false}, depends on x and represents whether the posterior probability of relevance, P(relevant|x), is actually certain.

The corresponding estimate of relevance at a given point x will be trusted if reliable is true. But if reliable is false, then the only information about relevance is given by the prior P(relevant) regardless of x. If information about reliability is available and assuming independence, it is possible to obtain a corrected probability of relevance at x as
                        
                           (4)
                           
                              
                                 
                                    
                                       
                                       
                                          P
                                          ′
                                       
                                       
                                          
                                             relevant
                                             |
                                             x
                                          
                                       
                                       
                                       =
                                       P
                                       
                                          
                                             reliable
                                             |
                                             x
                                          
                                       
                                       ⋅
                                       P
                                       
                                          
                                             relevant
                                             |
                                             x
                                          
                                       
                                       +
                                    
                                 
                                 
                                    
                                       
                                       
                                       
                                          
                                             1
                                             −
                                             P
                                             
                                                
                                                   reliable
                                                   |
                                                   x
                                                
                                             
                                          
                                       
                                       ⋅
                                       P
                                       
                                          relevant
                                       
                                       .
                                    
                                 
                              
                           
                        
                     
                  

The use of P(reliable|x) allows facing both the small sample size and the locality of labeled sample problems described above. The definition of such a probability function is domain dependent, but it is certainly related to the density of samples around the estimation point x. One way to approximate it in a CBIR context is to define it as a function 
                        
                           f
                           :
                           
                              R
                              
                                 ≥
                                 0
                              
                           
                           →
                           
                              0
                              1
                           
                        
                      that maps the distance to the nearest sample from x to an estimate of the probability P(reliable|x).

The application of Eq. (4) assumes that P(relevant|x), P(reliable|x) and the prior, P(relevant) are known. The first of these terms was already given in Eq. (1). Although other estimates of the probability P(reliable|x) are possible, one simple estimate when distances are already normalized in the range [0,1] is given by
                           
                              (5)
                              
                                 
                                    
                                       
                                          P
                                          
                                             
                                                reliable
                                                |
                                                x
                                             
                                          
                                          ≈
                                          f
                                          
                                             
                                                min
                                                
                                                   
                                                      
                                                         d
                                                         R
                                                      
                                                      
                                                         x
                                                      
                                                      ,
                                                      
                                                         d
                                                         N
                                                      
                                                      
                                                         x
                                                      
                                                   
                                                
                                             
                                          
                                          =
                                       
                                    
                                    
                                       
                                          
                                          1
                                          −
                                          min
                                          
                                             
                                                
                                                   d
                                                   R
                                                
                                                
                                                   x
                                                
                                                ,
                                                
                                                   d
                                                   N
                                                
                                                
                                                   x
                                                
                                             
                                          
                                          .
                                       
                                    
                                 
                              
                           
                        
                     

This distance-based approximation is of a similar nature as that of Eq. (1). The first elements in the ranking determined by Eq. (4) are the ones in which P(reliable|x) and P(relevant|x) are simultaneously close to one. In addition, the second term at the right hand side in Eq. (4) tends to zero as P(reliable|x) tends to one. This makes it possible to neglect this term without significantly affecting the first elements in the final ranking produced. By doing this and considering Eqs. (1) and (5), Eq. (4) can be written as:
                           
                              (6)
                              
                                 
                                    
                                       P
                                       ′
                                    
                                    
                                       
                                          relevant
                                          |
                                          x
                                       
                                    
                                    ≈
                                    
                                       
                                          1
                                          −
                                          min
                                          
                                             
                                                
                                                   d
                                                   R
                                                
                                                
                                                   x
                                                
                                                ,
                                                
                                                   d
                                                   N
                                                
                                                
                                                   x
                                                
                                             
                                          
                                       
                                    
                                    ⋅
                                    
                                       
                                          
                                             
                                                d
                                                N
                                             
                                             
                                                x
                                             
                                          
                                          
                                             
                                                d
                                                R
                                             
                                             
                                                x
                                             
                                             +
                                             
                                                d
                                                N
                                             
                                             
                                                x
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

The effect of the proposed estimate is graphically illustrated in Fig. 3
                        . By comparing this with the estimates in Fig. 2 it is possible to see that the new proposal leads to a smoothing effect similar to the one proposed in [8]. Nevertheless, the fundamental difference lies in the fact that the amount of smoothness varies across the representation space by the use of the reliability estimate.

A number of comparative experiments using all nearest neighbor-based approaches considered in this work have been carried out. The original NN approach [5] without any other independent extensions has been taken as a starting point. This has allowed us to appropriately evaluate the impact of the improvements and extensions proposed in this work on several performance measures.

In particular, the results obtained with the method proposed and with the following NN-based methods are reported: a) the basic nearest neighbor approach (NN) as in [5]; b) the composite NN approach using Bayesian query shifting (NN+BQS) [27]; and c) the NN approach enhanced with a smoothed estimator as in [8] (SSN). In addition, other relevance feedback algorithms representative of the approaches described in Section 2 have been considered, namely: a) a simple query point movement (QPM) implementation to be used as a baseline; b) the SVM method in [19]; c) the probabilistic framework presented in [28]; d) the self-organizing map (SOM) method introduced in [14]; and e) an approach based on the fuzzy sets [24]. An exhaustive experimentation has been carried out using three different databases that have been previously used in similar studies. A summary of the details of each database is given in Table 2.
                        
                           Small:
                           is a small repository which was intentionally assembled for testing, using some images obtained from the Web and others taken by the authors.
                                 1
                              
                              
                                 1
                                 Available in http://www.uv.es/arevalil/dbImages/.
                               The 1508 pictures it contains are classified as belonging to 29 different themes such as flowers, horses, paintings, skies, textures, ceramic tiles, buildings, clouds, trees, etc. In this case, the 50 dimensional feature vectors include a 10×3 HS color histogram and texture information in the form of two granulometric cumulative distribution functions [35].

has been manually compiled from the commercial collection “Art Explosion”, distributed by the company Nova Development. This collection is composed of 5476 images organized in 63 categories [1]. The 10×3 HS color histogram and six texture features have been computed for each picture in this database, namely Gabor convolution energies [36], Gray level co-occurrence matrix, [37], Gaussian random Markov fields, [38], the coefficients of fitting the granulometry distribution with a B-spline basis, [39] and two versions of the spatial size distribution, [40], one using a horizontal segment and another with a vertical segment [35]. The total number of features used is 104.

is a subset of the Corel database used in [5]. This is composed of 30000 images which were manually classified into 71 categories (Table 1
                              ). The 89 dimensional representation uses the descriptors in the KDD-UCI
                                 2
                              
                              
                                 2
                                 Available in http://kdd.ics.uci.edu/databases/CorelFeatures.
                               repository, namely: A nine component vector with the mean, standard deviation and skewness for each hue, saturation and value in the HSV color space; a 16 component vector with the co-occurrence in horizontal, vertical and the two diagonal directions; a 32 component vector with the 4×2 color HS histograms for each of the resulting sub-images after one horizontal and one vertical split; and a 32 component vector with the HS histogram for the entire image.

All algorithms have been implemented according to the principles described in the original publications. In the case of the SVM approach, Gaussian kernels have been used and parameters for the soft margin strength and class imbalance have been appropriately tuned. The SOM-based approach uses SOM sizes of 16×16, 32×32 and 64×64 for the Small, Art and Corel repositories, respectively. In all nearest neighbor approaches, distances between features have been estimated using the histogram intersection [41] on the color histograms and the Euclidean distance for the other descriptors, and they have been combined as suggested in the original publication [5]. In particular, the distances for different descriptors have been normalized in the range [0,1] and then summed up prior to applying the formulas in Section 5. Other options as using weighted linear combination of the scores or using Gaussian normalization [42] have also been tried but the results obtained were similar and, more importantly, they did not affect the relative merits of the proposals in this work.

In the experiments carried out and reported, a similar setup to the one in [5,7] has been employed. The available categories have been used as concepts, and user judgments about similarity have been simulated considering that only pictures under the same category represent the same concept.

In order to systematically characterize the pros and cons of each method, two different kinds of queries have been simulated. On one hand, a query by example has been considered by randomly choosing a picture from a random category. Having this image as the unique positive example, that is |Q
                     +|=1 and Q
                     −
                     =∅, a given number of pictures are retrieved from the database according to the plain distance to the only picture in Q
                     +. On the other hand, semantic queries aiming at simulating content-based searches initiated by a textual query have also been considered. These have been implemented by randomly choosing several non-clustered positive and negative examples in the following way. First, a category is initially chosen at random. Then a small number of relevant images, k, are randomly taken from the database (between 3 and 5, also at random). Accordingly, other w
                     −
                     k non-relevant images are selected in the same way.

Both kinds of queries use the general relevance feedback algorithm to proceed from their initial configuration. In the first case, the algorithm has a very local positive information about the concept sought. In the second, the information about relevant and non-relevant images is potentially spread throughout the whole representation space.

The following measures have been used to compare the performance of the algorithms considered:
                        
                           1.
                           Precision. The proportion of relevant pictures amongst the ones retrieved.

Recall. The proportion of relevant pictures retrieved with regard to the ones in the whole database.

Average precision. If precision is plotted as a function of recall, it measures the average value of precision for each different recall value.

The first two measures have been used to evaluate the performance of the techniques on the top positions of the ranking, as it is generally the interest in CBIR. Both have been measured on a typical window size w
                     =20 (the number of images provided for judgment at each iteration). To measure recall, relevant images ranked in between the top w at each iteration are marked as already recovered and not considered as candidate pictures for the next iterations. For precision, images recovered at one iteration, are still considered as candidate images for the next. Average precision is used to evaluate the entire ranking produced by each method. Although this may be of less interest from a CBIR perspective, it is an adequate measure to explore the potential of using the proposed approach in other different contexts.

To obtain more reliable data, each technique has been evaluated with 500 random searches on each repository, using the same categories and initial picture order for all algorithms. The presented results are averaged values in all cases.

The performance results using the smallest of the databases considered are shown in Fig. 4
                      for the two kinds of queries that have been considered: query by example (left) and semantic queries (right). In this figure and the following ones, the NN method is not shown for clarity reasons and also because it was always equal or worse than all of its variants. The fuzzy approach has been also removed from the plots as it was always equal or worse than the SOM approach.

As it can be seen in Fig. 4, the proposed method is among the best behaved ones according to all performance measures. Nevertheless, the composite method NN+BQS is significantly the best one for this database when queries start from the very local information. On the contrary, when queries start with more spread information, this method gives significantly worse results in the first iterations (specially when measuring precision). Also remarkable is the poor performance of the SVM method in the first iterations which can be attributed at the difficulty of training an appropriate relevance model when data is scarce. Accordingly, the SVM method is the one whose relative improvement as iterations increase is the best, specially when measuring precision and average precision on semantic queries.


                     Fig. 5
                      shows the performance results on both types of queries using the Art database. This is a medium size and relatively well behaved repository according to the absolute values of the performance measures shown in the figure.

From these, very similar conclusions can be drawn with regard to the behavior of the proposed method with respect to most of its competitors.

Contrary to the previous database, the NN+BQS method ranks among the worst for both types of queries and according to all performance measures. Also, the SVM method exhibits a very similar behavior but in this occasion its relative improvement is so dramatic that the method gives clearly and significantly the best precision and, specially, average precision results in the last iterations. The huge difference in average precision is not totally unexpected as it measures the whole ranking and not only the top positions and clearly the SVM is by far the most powerful classifier when enough training data is available.

A detail worth remarking is that the SNN method that shares part of the rationale of the proposed algorithm gives slightly better precision and average precision results in the case of queries by example on this database.

The results on the largest of the databases considered in this work are shown in Fig. 6
                     . As it can be seen, very similar results as in the previous databases are obtained. The proposed method and SNN give very similar results in precision and average precision.

In this database, differences are more significant in recall. The proposed method is significantly the best and the difference increases with iterations. On the contrary, SNN performance gets worse with iterations.

In the case of measuring average precision along the whole ranking, the probabilistic approach gives the best results when performing semantic queries. The SVM approach follows a very similar behavior than in the previous databases and ranks among the best at the last iterations. Apart from its behavior with regard to average precision, the probabilistic approach also improves the results of the proposed approach but only in the last iterations.

The performance measures shown in the previous figures are averaged on a particular set of 500 random queries of each type. These measures have a very high variability due to the fact that queries are very different depending on the particular class and also depending on the starting information given to the algorithms. To effectively rank the different algorithms according to their general performance and measure the statistical significance of this, the different performance measures have been measured separately for each single query at each particular iteration and for all three databases considered. Measures for iterations 1 and 2 (where variability is even higher) have not been considered in this study. With all these measures, a multiple comparison Friedman test [43] has been performed for each measure and kind of query. The resulting average ranks are shown in Table 2
                      for all methods including the ones not shown in the previous figures. The average rank that corresponds to the best method for each performance measure on each kind of query is shown in bold. Shaded cells indicate that there is no statistically significant difference among these and the proposed method according to a post-hoc Holm test at significance level α
                     =0.05. The adjusted p-values according to this test that correspond to comparing each method to the proposed one are shown in the same table in brackets.

According to this statistical test, only the proposed method performs the best in all measures and kinds of queries. The previously proposed smoothed estimate, SNN, gives also very similar results with the exception of recall. With regard to recall, the proposed method is very significantly the best according to the p-values shown. It is also worth mentioning the fact that the SVM method is as good as the best with regard to average precision. Also, the probabilistic approach is as good as the best with regard to precision and average precision on semantic queries.

A comparative study considering several different proposals to improve the basic nearest neighbor approach for CBIR has been carried out in this paper. In particular, the ideas behind the approach along with their benefits and disadvantages have been analyzed and an improved score using a reliability estimate has been proposed. According to the experimentation carried out and the corresponding performance measures, the proposed approach has several significant advantages with regard to the other nearest neighbor based alternatives, and also outperforms a series of representative relevance feedback approaches.

Even though the proposed method continues a series of improvements on the plain NN method and arrives to very similar results to other improved methods, the way in which the estimate has been derived makes it more robust and easy to integrate with other approaches. For example, the previous SNN method needed the combination of two strategies (a moderating term in the score and a local search for images) to beat the NN method. With the proposal in this work we are able to improve this by using only one better and more reliable way of estimating relevance, avoiding the computational burden associated with building a ranking per relevant selection.

Related work on hybridizing these distance-based estimates with other ways of estimating relevance is now under development. Even though it is difficult to improve further the results presented here, we aim at being able to adapt the way relevance is estimated to different kinds of situations.

@&#ACKNOWLEDGMENTS@&#

We would like to thank Dr. G. Giacinto for his help on the evaluation of this paper, facilitating the manually performed classification of the 30000 images repository. We would also like to thank Dr. M. Ortega for providing the thumbnails of these images. This work has been partially funded by FEDER and the Spanish Government through projects TIN2011-29221-C03-02 and TIN2009-14205-C04-03, and Consolider Ingenio 2010 
                  CSD07-00018.

@&#REFERENCES@&#

