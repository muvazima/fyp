@&#MAIN-TITLE@&#Stabilization of panoramic videos from mobile multi-camera platforms

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Stabilization of panoramic and stabilization of single camera videos are separate problems.


                        
                        
                           
                           Panoramic videos suffer from global and inter-camera vibrations.


                        
                        
                           
                           Blend-masks are useful for dealing with inter-camera vibrations.


                        
                        
                           
                           Our survey suggests that viewers prefer this scheme over prior works.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Panoramic Videos

Video Stabilization

Video Stitching

Mobile multi-camera platforms

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

Multimedia technology has seen a rapid evolution in recent years in terms of both quality and quantity of the information delivered through multimedia displays. Wide Field of View (FOV) panoramic images and videos provide ultra-high definition content that when displayed on large high definition displays, provide an immersive experience to the viewers. Panoramic videos are generated either by using fish-eye lens or by stitching together synchronized video frames coming from multiple cameras arranged on a rig [1,2]. The use of stitching-based panorama generation and display systems once limited to geological surveys and surveillance applications [3] has already been extended for entertainment purposes [4–6]. With decreasing cost of commodity cameras, this trend is expected to be more ubiquitous [7] in the entertainment industry, closely followed by household consumer market in the future. Thus, instead of static panorama acquisition systems [3] that arrange closely coupled sensors in a dedicated closed unit [2], affordable panoramic acquisition systems are emerging that employ commodity cameras mounted on a platform. When set on a mobile platform, such a panoramic video acquisition system may suffer from mechanical vibrations that are global to the camera rig or independent to a particular constituent camera. The global vibrations appear as global jitter in the panoramic video frames while the inter-camera vibrations result in jitter in the spatial region of panoramic video contributed by the particular affected cameras. This results in an unpleasant experience for viewers. This is illustrated conceptually in Fig. 1
                      which shows a single frame of a panoramic video (‘Man video’) that was formed by stitching frames acquired using three cameras. Please refer to the accompanying video to watch this panorama sequence. The tail of the arrows in Fig. 1 represents the location of a few salient features tracked using Kanade–Lucas–Tomasi (KLT) feature tracker [8,9]. The yellow, red and green vectors represent the direction of motion of these features at a particular instance in time and are color coded to signify the independent direction of motion for the contents captured from camera 1, camera 2 and camera 3 respectively. Fig. 1 illustrates an example where camera 3 is experiencing a different vibration and hence its motion vectors (green) appear to have a different direction as compared to camera 1 and camera 2. In the panoramic video (Man video), this effect appears as jitter in the region of panoramic video frames that is contributed by camera 3. Since this jitter appears in a sub-section of panoramic frame, we term it as sub-frame jitter. The ‘Man video’ demonstrates this effect for an actual setup and thus supports our motivation for the requirement of a stabilizing scheme particularly aimed at panoramic videos.

The problem of single camera video stabilization has been extensively researched and has reached a certain level of maturity [10–14]. However, for stabilization of panoramic videos captured from multi-camera platforms, little work has been reported. Furthermore, the existing schemes for such systems [15,16] do not account for the inter-camera vibrations experienced in mobile platforms. In this paper, we treat stabilization of panoramic multi-camera videos as a distinct problem from that of a single camera video. We achieve this by classifying the effects of these vibrations as global, sub-frame and local jitter, and by proposing a method to deal with each one of these in a systematic fashion. In summary, global stabilization is achieved by estimating 2D motion models using the tracked feature trajectories over the complete panoramic frame. Sub-frame stabilization is achieved by making use of the information available in the blend masks that are generated by the stitching application [1]. Blend masks are the intensity weights that are used to blend together the images acquired from multiple cameras to generate a seamless panorama. Finally, the local stabilization tackles the residual jitter that might appear in parts of the panoramic scene in the video due to differences in the scene depth. To the best of our knowledge, this is the first scheme that acknowledges that panoramic videos need to be stabilized temporally as a sequence of stitched frames, as well as spatially to handle the sub-frame jitter.

This paper is organized as follows. In Section 2, we provide a brief account of predominant approaches for video stabilization. In Section 3, we provide the necessary background for panoramic video stitching process followed by a description of the proposed stabilization scheme. In Section 4, we discuss the comparison of the results of the proposed scheme with that of two recent stabilization schemes [14,15] for a number of videos. Finally in Section 5, we present our conclusion.

@&#LITERATURE REVIEW@&#

Single camera video stabilization problem has been thoroughly addressed in previous works over the last decade. Most video stabilization methods comprise of three main steps: Estimating a 2D motion model between subsequent video frames, computing a smooth motion model that removes the unpleasant jitter and finally applying stabilizing geometric transforms to the video frames [11,12,14,17–29]. These methods differ in their approach of 2D motion model estimation which mostly relies on tracking the position of a certain number of features among adjacent video frames or correlating similar regions in subsequent video frames to estimate the motion vectors. Kanade–Lucas–Tomasi tracker (KLT) is one such popular method for tracking salient features in a video sequence [8,9,30]. The information from the tracking is then used to estimate a 2D motion model for the video. Such 2D motion models depend on the behavior of majority content in the scene and hence fail to accurately model scenes with large moving objects or significant parallax. For most single camera videos, this limitation does not pose a serious problem but for wide angle panoramic videos, there are almost always significant variations in the scene in terms of scene depth and moving content. Another class of stabilization algorithms are based on constructing a 3D [10] model of the scene and camera motion using Structure-From-Motion (SFM) method [31]. Such methods then perform video reconstruction on an estimated low frequency 3D camera path. A relatively recently proposed subspace video stabilization approach [13] tracks feature point trajectories and restricts them to a low-dimensional subspace. Feature trajectories corresponding to moving objects are discarded and corresponding smooth trajectories are computed for the remaining feature trajectories. Instead of estimating global geometric transforms as in the previous schemes, Liu et al. use content preserving warping [32]. Both original and smooth feature trajectories are used as handles in order to cater for the local nature of parallax induced jitter. Liu et al. demonstrated greater robustness than SFM-based schemes while handling parallax more effectively than 2D video stabilization.

Certain schemes for stabilization of multi-camera camera systems have also been proposed. In [15] Haq et al. propose an efficient stabilization system for panoramic video acquisition. It extracts the vibration parameters from the output of one of the cameras and extends it to the output of other cameras based on the assumption that all the cameras have the same optical center and the cameras are mutually static. However, as evident from Fig. 1 this may not be true for a number of cases. In [16] Kamali et al. discuss a scheme for stabilization of spherical videos captured by the Ladybug omnidirectional image acquisition system [2]. It makes use of SFM for modeling spherical scene content. Parameters of camera motion model are then smoothed separately and a new smooth camera path is computed. Stabilized frames are rendered by using a technique similar to content preserving warps [32]. This method however, treats each panoramic frame of the video as a single video frame and assumes no individual motion exhibited by each camera. For a large panoramic video acquisition platform (Fig. 2
                     ), enforcing such a constraint may not be feasible. Nevertheless, for a wide angle, high resolution panoramic video, computing a 3D model shall involve complex modeling of multiple cameras for SFM and shall be a difficult task to accomplish. To summarize, neither the single camera stabilization schemes nor the ones that are aimed at panoramic videos particularly account for the effects caused due to inter-camera vibrations.

In this section we present a brief overview of panoramic video generation followed by a detailed description of the proposed video stabilization technique.

A panoramic video acquisition system consists of a number of cameras mounted on a platform in such a way that the collective viewing angle of the platform is larger than that of a single camera. These cameras are placed and directed in such a way that the view of each camera partially overlaps with the view of at least one other camera. To minimize parallax, an attempt is made to align the optical centers of the cameras either by placing them carefully in a circular array or by using a conical reflecting mirror rig [6]. Furthermore, the overlapping region may also be minimized to limit the visual effects of parallax to a minimum. Three such panoramic video acquisition systems that were used to test the robustness of our scheme are shown in Fig. 2.

Considering C number of cameras forming the camera rig, let I
                        
                           j
                        
                        
                           i
                         be the jth
                         frame captured from the ith
                         camera. Thus for a three camera system, j
                        th Panorama frame Pj
                         is generated by geometrically warping and blending the camera frames (I
                        
                           j
                        
                        1,
                        I
                        
                           j
                        
                        2
                        and I
                        
                           j
                        
                        3) such that the features in the overlapping region of neighboring frames (I
                        
                           j
                        
                        
                           i
                        
                        and I
                        
                           j
                        
                        
                           i
                           +1) are aligned. This is typically achieved by extracting SIFT based feature points [33] in the overlapping portion of the neighboring frames and estimating the inter-frame homography [1]. This homography provides the required geometric transformation to align the extracted feature points. The transformation is then applied on the input frames to generate the geometrically warped frames (Ig
                        
                           j
                        
                        1,
                        Ig
                        
                           j
                        
                        2,
                        Ig
                        
                           j
                        
                        3). As an example, Fig. 3
                         shows I
                        
                           j
                        
                        1,
                        I
                        
                           j
                        
                        2,
                        I
                        
                           j
                        
                        3 for a three-camera system as well as their corresponding geometrically transformed frames Ig
                        
                           j
                        
                        1,
                        Ig
                        
                           j
                        
                        2 and Ig
                        
                           j
                        
                        3. To generate a seamless panorama frame Pj
                        , these geometrically transformed frames have to be smoothly faded into frames from their neighboring cameras. This is achieved by multiplying each (Ig
                        
                           j
                        
                        1,
                        Ig
                        
                           j
                        
                        2
                        and Ig
                        
                           j
                        
                        3) by its corresponding blend mask (Bm
                        1,
                        Bm
                        2,
                        Bm
                        3) and finally adding the blended frames together. Thus, 60×3×30=5400 camera frames come together to produce a minute long, three camera panoramic video at 30 fps. In the interest of temporal coherence and computational efficiency, stitching homographies and blend masks are typically computed for initial frames (I
                        1
                        1,
                        I
                        1
                        2,..,
                        I
                        1
                        
                           C
                        ) and the rest of the frames in the video are stitched using these pre-computed homographies and blend masks.

When captured from a mobile camera platform, these stitched videos show unpleasant jitter due to mechanical vibrations in the panoramic video acquisition system. We assume that these panoramic videos may suffer from three types of unwanted motions. The most prominent among them is global jitter, which is the overall high frequency vibration exhibited by the mechanical structure of the camera rig and is transmitted equally to all the cameras in the video acquisition system. Single camera videos also exhibit this kind of jitter. Secondly, inter-camera vibration is the high frequency motion exhibited by each camera independently due to differences in mechanical constraints on each camera. It appears in the panoramic frame as sub-frame jitter. This was illustrated in Fig. 1. Parallax induced jitter is caused due to the fact that camera motions translate to content motion partially depending upon depth of the content in the physical world. Hence content in different depth planes appear to exhibit different motion. This effect is also present in single camera videos and is particularly visible when there are major variations in the depth of the scene being recorded. Most 2D video stabilization methods ignore this effect under the consideration that most single camera scenes do not have significant variations in depth. This consideration, while sufficiently plausible for single camera videos, is not plausible for a panoramic video since a panoramic video has a much greater view angle and hence contains greater scene information which makes depth variations a more probable occurrence.

Our method takes stitched video frames and respective blend masks as input and minimizes the visual degradation in the video caused by all three types of jitter. A flowchart of the proposed scheme is provided in Fig. 4
                        , followed by a detailed description of the method. Assuming a C camera panoramic video comprising K frames, our stabilization scheme requires the panorama frames (P0, P1 … PK
                        ) and C blend masks (Bm
                        1,
                        Bm
                        2,….
                        Bm
                        
                           C
                        ) as an input. We begin by extracting salient feature trajectories from the panoramic video frames and use them to estimate various transformations over several steps in order to render a smooth video. Firstly, the extracted feature trajectories from the complete panorama region are smoothed and used to estimate global geometric transformations for the panoramic frame. While the computed transform can be applied on each pixel at this stage, these geometric transforms only deal with the global jitter and further transformations are required to deal with other two types of jitter. Since subsequent steps require processing to be done on feature trajectories, the application of intermediate transforms is deferred till last step to avoid re-computation of trajectories after each intermediate stabilizing step. The global transform for each panoramic frame is thus applied only on feature trajectories rather than applying on all the pixels. In the second step, these transformed trajectories are grouped by camera (illustrated by colored arrows in Fig. 1) and used to estimate a geometric transform for each of the C camera regions separately. Trajectories in each camera group are then transformed according to their corresponding estimated transform. Trajectories in each of the panorama overlapping region have two estimated transforms, one for each of the neighboring camera. Therefore, a blend-mask weighted transform is applied to these trajectories to ensure smooth spatial transition between the two cameras. At this point, the remaining major source of residual jitter in the transformed trajectories is the parallax induced jitter. To handle this, each trajectory is smoothed independently followed by a clustering scheme that forms groups of features that experience similar jitter and hence belong to the same depth-plane or are moving together. The feature trajectories that do not conform to any of these groups are tagged erroneous. The original and the transformed locations of the non-erroneous feature trajectories are then used to finally warp the original frames, effectively applying all the three stabilizing transformations collectively to produce a stabilized panoramic video.

The proposed global stabilization step is inspired from the 2D video stabilization technique described in [27]. It begins by tracking features through the panoramic video. Next the features representing moving objects that disappear or get occluded are discarded. The remaining feature trajectories are then filtered to compute smooth trajectories. These smoothened trajectories are used as a reference to estimate global motion and geometric transforms are computed for motion compensation [14]. The estimated geometric transformations are then applied to the feature trajectories resulting in a set of trajectories that are free of global jitter. A detailed description of this process follows.

A fixed number (N) of most salient features are tracked through the sequence of panoramic frames. N is chosen manually depending upon the resolution of the acquired panoramic frames. We use Kanade–Lucas–Tomasi (KLT) feature tracker [8,9,30] for this purpose. Each of the features represents some content in the scene; therefore, if a part of the scene escapes camera view or gets occluded, the features representing it are lost. If a feature is lost, it is replaced by a new feature which is tracked in the subsequent frames effectively maintaining a constant number of feature trajectories being tracked through each frame while simultaneously accommodating new scene content entering the FOV. These feature trajectories are then used to estimate the actual motion of the panoramic video acquisition system and create new smooth motion trajectories. This process is called motion estimation and compensation. Trajectories of the features that are not lost for α number of frames are flagged as sufficiently long feature trajectories. The rest of the feature trajectories are discarded and play no further part in the process. This is done in order to remove the trajectories of small moving objects in the scene that do not last for a long time [32]. The criterion for a feature trajectory to be sufficiently long depends on the content of the video. In theory, we want to include as many frames as we can for accurate motion estimation and compensation but videos with quickly changing scenes do not contain a large number of sufficiently long trajectories which puts a quantitative upper bound on the value of α. We define this upper bound as the largest value of α for which at least 50% of the total number of features (
                              N
                           ) being tracked exist throughout the entire length of α number of frames. α is computed by initializing it to the total number of frames and iteratively optimizing it using a simple gradient descent strategy until the number of trajectories spanning all α frames is at least 0.5
                           N:
                              
                                 (1)
                                 
                                    α
                                    :
                                    =
                                    α
                                    −
                                    η
                                    
                                       
                                          0.5
                                          N
                                          −
                                          k
                                       
                                    
                                 
                              
                           where η is the update rate and k is the number of trajectories that span all α frames. Some methods fix the criterion for a trajectory to be sufficiently long to a certain number of frames [13]. However, for videos in which a specific portion does not have enough trajectories that last the said fixed number of frames, such schemes are unable to proceed and later parts of the video remain un-stabilized. Since our method uses a dynamic criterion for selection of α that adapts with the changing contents of the video, it thus allows the stabilization to recover after portions of video that are characterized by low number of sufficiently long trajectories.

After selection of sufficiently long feature trajectories, we are left with two matrices, Fx
                            and Fy
                            respectively, containing the x and y coordinates of M
                           =0.5
                           N feature locations through α frames. In these matrices, the elements of a column represent the x and y locations of M features at an instance in time and thus each row represents a feature trajectory.
                              
                                 (2)
                                 
                                    F
                                    x
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   x
                                                   1
                                                   1
                                                
                                             
                                             
                                                ⋯
                                             
                                             
                                                
                                                   x
                                                   α
                                                   1
                                                
                                             
                                          
                                          
                                             
                                                ⋮
                                             
                                             
                                                ⋱
                                             
                                             
                                                ⋮
                                             
                                          
                                          
                                             
                                                
                                                   x
                                                   1
                                                   M
                                                
                                             
                                             
                                                ⋯
                                             
                                             
                                                
                                                   x
                                                   α
                                                   M
                                                
                                             
                                          
                                       
                                    
                                    ,
                                    F
                                    y
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   y
                                                   1
                                                   1
                                                
                                             
                                             
                                                ⋯
                                             
                                             
                                                
                                                   y
                                                   α
                                                   1
                                                
                                             
                                          
                                          
                                             
                                                ⋮
                                             
                                             
                                                ⋱
                                             
                                             
                                                ⋮
                                             
                                          
                                          
                                             
                                                
                                                   y
                                                   1
                                                   M
                                                
                                             
                                             
                                                ⋯
                                             
                                             
                                                
                                                   y
                                                   α
                                                   M
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

Each row of matrices Fx
                            and Fy
                            is then smoothed either by passing through a simple low pass filter or fitting a lower order polynomial in order to remove the high frequency unwanted jitter. At this point, for each original feature trajectory represented by Fx
                            and Fy
                           , we have a smoothed feature trajectory represented by 
                              
                                 
                                    F
                                    ¯
                                 
                                 x
                              
                            and 
                              
                                 
                                    F
                                    ¯
                                 
                                 y
                              
                            respectively, and have the same dimensions.
                              
                                 (3)
                                 
                                    
                                       
                                          
                                             
                                                F
                                                ¯
                                             
                                             x
                                          
                                          
                                             
                                                F
                                                ¯
                                             
                                             y
                                          
                                       
                                       n
                                    
                                    =
                                    
                                       
                                          
                                             
                                                F
                                                x
                                             
                                             *
                                             h
                                             ,
                                             
                                             
                                                F
                                                y
                                             
                                             *
                                             h
                                          
                                       
                                       n
                                    
                                    …
                                    …
                                    …
                                    .
                                    
                                       
                                          1
                                          ≤
                                          n
                                          ≤
                                          M
                                       
                                    
                                 
                              
                           
                        

Here n represents rows, h is a 1-D smoothing kernel and * represents convolution operation.

The original feature locations (Fx
                           , Fy
                           ) represent the location of content in the original video frame and the smoothed feature locations (
                              
                                 
                                    F
                                    ¯
                                 
                                 y
                              
                           , 
                              
                                 
                                    F
                                    ¯
                                 
                                 y
                              
                           ) represent where this content should lie for the video to be smooth and free of any unwanted global motion. For frame j, the two sets of corresponding trajectory points are used to estimate a geometric transformation model 
                              H
                              j
                           . The geometric transformation model we use is non-reflective similarity i.e. scaling, translation and rotation required to transform the original frame to the corresponding smooth frame defined by the filtered trajectories and is given by:
                              
                                 (4)
                                 
                                    
                                       H
                                       j
                                    
                                    =
                                    
                                       
                                          
                                             
                                                s
                                                cos
                                                θ
                                             
                                             
                                                −
                                                s
                                                sin
                                                θ
                                             
                                             
                                                t
                                                x
                                             
                                          
                                          
                                             
                                                s
                                                sin
                                                θ
                                             
                                             
                                                s
                                                cos
                                                θ
                                             
                                             
                                                t
                                                y
                                             
                                          
                                          
                                             
                                                0
                                             
                                             
                                                0
                                             
                                             
                                                1
                                             
                                          
                                       
                                    
                                 
                              
                           where s, θ, tx, and ty are the estimated scale, rotation angle, translation in x axis and translation in y axis respectively. 
                              H
                              j
                            is estimated using RANdom SAmple Consensus (RANSAC) [34]. We begin by randomly choosing two corresponding sets of points Q and 
                              
                                 Q
                                 ¯
                              
                            from F and 
                              
                                 F
                                 ¯
                              
                            respectively and computing a transformation matrix 
                              H
                              =
                              
                                 Q
                                 ¯
                              
                              
                                 Q
                                 
                                    −
                                    1
                                 
                              
                           . Number of point-pairs that are approximately consistent with H is counted and this process is repeated until a significant majority of the point-pairs are consistent with H. These points are called inliers. These inliers are then used to estimate an overall transform H
                           j for j
                           th frame.

The transformation 
                              H
                              j
                            is then applied to the trajectories (Fx, Fy
                           )
                              j
                            of all the feature points in j
                           th frame to generate (S
                           
                              G
                           
                           F
                           
                              x
                           ,
                           S
                           
                              G
                           
                           F
                           
                              y
                           )
                              j
                           , the globally stabilized trajectories of x and y coordinates of tracked feature points. Let us assume a vector Un,j
                            representing the location of nth
                            feature in jth
                            panoramic frame in homogenous coordinates and given by:
                              
                                 
                                    
                                       U
                                       
                                          n
                                          ,
                                          j
                                       
                                    
                                    =
                                    
                                       
                                          x
                                          
                                             n
                                             ,
                                             j
                                          
                                       
                                       
                                          y
                                          
                                             n
                                             ,
                                             j
                                          
                                       
                                       1
                                    
                                 
                              
                           
                        

The location of nth
                            globally stabilized feature in jth
                            frame is then given by
                              
                                 (5)
                                 
                                    
                                       V
                                       
                                          n
                                          ,
                                          j
                                       
                                    
                                    =
                                    
                                       
                                          
                                             S
                                             G
                                          
                                          
                                             F
                                             x
                                          
                                          
                                             n
                                             j
                                          
                                          ,
                                          
                                             S
                                             G
                                          
                                          
                                             F
                                             y
                                          
                                          
                                             n
                                             j
                                          
                                          ,
                                          1
                                       
                                    
                                    
                                    =
                                    
                                       H
                                       j
                                    
                                    
                                       U
                                       
                                          n
                                          ,
                                          j
                                       
                                    
                                    …
                                    …
                                    …
                                    .
                                    
                                       
                                          1
                                          ≤
                                          n
                                          ≤
                                          M
                                       
                                    
                                    
                                       
                                          1
                                          ≤
                                          j
                                          ≤
                                          α
                                       
                                    
                                 
                              
                           
                        

The transform estimation in the previous step uses the component of motion that is common to a majority of the feature trajectories. Therefore, the stabilized trajectories from the previous step are only free of the global jitter and still contain independent camera jitter. To account for the inter-camera vibrations, independent stabilization transformations (
                              H
                           
                           
                              i,j
                           
                           
                              sub
                           ), for each of the ith camera (i
                           =1,2,…C), have to be estimated for every panoramic frame (P
                           j). This step begins by passing the S
                           
                              G
                           
                           F
                           
                              x
                           ,
                           S
                           
                              G
                           
                           F
                           
                              y
                            through a low pass filter to get a new set of smoothed feature trajectories: 
                              
                                 
                                    
                                       S
                                       G
                                    
                                    
                                       F
                                       x
                                    
                                 
                                 ¯
                              
                              ,
                              
                              
                                 
                                    
                                       S
                                       G
                                    
                                    
                                       F
                                       y
                                    
                                 
                                 ¯
                              
                           . For each panoramic frame, all the feature points S
                           
                              G
                           
                           F
                           
                              x
                           ,
                           S
                           
                              G
                           
                           F
                           
                              y
                            and their corresponding smoothed points 
                              
                                 
                                    
                                       S
                                       G
                                    
                                    
                                       F
                                       x
                                    
                                 
                                 ¯
                              
                              ,
                              
                              
                                 
                                    
                                       S
                                       G
                                    
                                    
                                       F
                                       y
                                    
                                 
                                 ¯
                              
                            are arranged into C sets depending upon the camera they are captured from. In other words, all the feature points lying in the region of Pj
                            that is contributed by camera i will be grouped together. A non-reflective similarity transformation matrix (
                              H
                           
                           
                              i,j
                           
                           
                              sub
                           ) is then estimated for each set (i
                           =1,2,…C) of features using RANSAC, that defines the transformation of all the (S
                           
                              G
                           
                           F
                           
                              x
                           ,
                           S
                           
                              G
                           
                           F
                           
                              y
                           ) to (
                              
                                 
                                    
                                       S
                                       G
                                    
                                    
                                       F
                                       x
                                    
                                 
                                 ¯
                              
                              ,
                              
                              
                                 
                                    
                                       S
                                       G
                                    
                                    
                                       F
                                       y
                                    
                                 
                                 ¯
                              
                           ) that belong to the set i. Since common features in the overlapping regions are contributed by two neighboring cameras (i and i
                           +1), they experience different stabilization transformations (
                              H
                           
                           
                              i,j
                           
                           
                              sub
                            and 
                              H
                           
                           
                              i
                              +1,j
                           
                           
                              sub
                           ) which may produce visual artifacts near the overlapping region in the panoramic frame. The blend masks of panoramic videos define the pixel-wise contribution of video frames from each camera to the stitched panorama, therefore, we use them as weights for blending the stabilization transform parameters (s, θ, tx and ty) for each feature-point location lying in different sections of the frame. For this purpose, an M
                           ×α×
                           C matrix B(n,j,i) is generated such that 
                              B
                           (n,j,i) represents the value of ith
                            blend mask at the location represented by Un,j
                           . Let the transform parameters for camera i be represented as [s,
                           θ,
                           tx,
                           ty]
                              i
                           , then the effective transformation parameters for nth
                            feature in jth
                            panoramic frame are weighted as follows:
                              
                                 (6)
                                 
                                    
                                       
                                          s
                                          θ
                                          
                                             t
                                             x
                                          
                                          
                                             t
                                             y
                                          
                                       
                                       
                                          n
                                          ,
                                          j
                                       
                                    
                                    =
                                    
                                       Σ
                                       
                                          i
                                          =
                                          1
                                       
                                       C
                                    
                                    B
                                    
                                       n
                                       j
                                       i
                                    
                                    .
                                    
                                       
                                          
                                             s
                                             ,
                                             θ
                                             ,
                                             t
                                             x
                                             ,
                                             t
                                             y
                                          
                                       
                                       i
                                    
                                 
                              
                           
                        

Parameters from Equation-6 define the sub-frame stabilization transformation 
                              H
                           
                           
                              n,j
                           
                           
                              sub
                            for each feature in jth panorama. Thus, for a feature falling in the overlapping region of camera i and i
                           +1, all the terms except B(n,
                           j,
                           i) and B(n,
                           j,
                           i
                           +1) become zero and the resulting transformation is a weighted average of the motions depicted by both camera i and camera i
                           +1. The sub-frame stabilized feature trajectories (S
                           
                              GS
                           
                           F
                           
                              x
                           ,
                           S
                           
                              GS
                           
                           F
                           
                              y
                           )
                              n,j
                            are thus computed by applying the following transformation:
                              
                                 (7)
                                 
                                    
                                       
                                          
                                             
                                                S
                                                
                                                   G
                                                   S
                                                
                                             
                                             
                                                F
                                                x
                                             
                                             ,
                                             
                                                S
                                                
                                                   G
                                                   S
                                                
                                             
                                             
                                                F
                                                y
                                             
                                          
                                       
                                       
                                          n
                                          ,
                                          j
                                       
                                    
                                    =
                                    
                                       H
                                       
                                          n
                                          ,
                                          j
                                       
                                       
                                          s
                                          u
                                          b
                                       
                                    
                                    
                                       V
                                       
                                          n
                                          ,
                                          j
                                       
                                    
                                    …
                                    …
                                    …
                                    .
                                    
                                       
                                          1
                                          ≤
                                          n
                                          ≤
                                          M
                                       
                                    
                                    
                                       
                                          1
                                          ≤
                                          j
                                          ≤
                                          α
                                       
                                    
                                 
                              
                           
                        

The previous two steps were based on frame and sub-frame based motion estimation respectively. However, there can be feature trajectories that may not conform to global or sub-frame stabilization transformation and may require independent local warping of each trajectory for stabilization. This is further elaborated in Fig. 5
                            which illustrates global transformation (Fig. 5a) and local warping (Fig. 5b) on three sets of trajectories (dotted red), their corresponding smoothed trajectories (dotted green) and their corresponding transformed trajectories (blue). In Fig. 5a, a global transform is estimated using the original and smoothed trajectories and applied to the trajectories. The resulting transformed trajectories (blue) are very close to the computed smoothed trajectories in case of trajectory 1 and 2 because they resemble the overall motion of the frame. However, trajectory 3 depicts motion that is not similar to the overall motion of the frame hence the resulting transformed trajectory does not resemble the smoothed trajectory. In Fig. 5b, local warping was applied which forced the transformed content to lie on the smoothed trajectory for each trajectory and signifies the motivation for our local stabilization step. It is however important to note that local warping may cause unnatural artifacts in the scene due to erroneous feature tracking or absence of sufficient features in a part of the scene as discussed in Section 4. Therefore, we limit the use of local warps to the removal of parallax induced jitter which is generally smaller in magnitude.

This step begins by smoothing the global and sub-frame stabilized feature trajectories (S
                           
                              GS
                           
                           F
                           
                              x
                           ,
                           S
                           
                              GS
                           
                           F
                           
                              y
                           ) in a fashion similar to the one described in Section 3.2.1 in order to compute the final stabilized feature trajectories:
                              
                                 (8)
                                 
                                    
                                       
                                          
                                             
                                                S
                                                
                                                   G
                                                   S
                                                   L
                                                
                                             
                                             
                                                F
                                                x
                                             
                                             ,
                                             
                                                S
                                                
                                                   G
                                                   S
                                                   L
                                                
                                             
                                             
                                                F
                                                y
                                             
                                          
                                       
                                       
                                          n
                                          ,
                                          j
                                       
                                    
                                    =
                                    
                                    
                                       
                                          (
                                          
                                             S
                                             
                                                G
                                                S
                                             
                                          
                                          
                                             F
                                             x
                                          
                                       
                                       ¯
                                    
                                    ,
                                    
                                       
                                          
                                             S
                                             
                                                G
                                                S
                                             
                                          
                                          
                                             F
                                             y
                                          
                                       
                                       ¯
                                    
                                    )
                                    
                                       
                                       
                                          n
                                          ,
                                          j
                                       
                                    
                                 
                              
                           
                        

Finally (F
                           
                              x
                           ,
                           F
                           
                              y
                           ) and (S
                           
                              GSL
                           
                           F
                           
                              x
                           ,
                           S
                           
                              GSL
                           
                           F
                           
                              y
                           ) are used as the control points for the local warping which shifts all the content represented by original feature locations in the panoramic frames to the locations defined by (S
                           
                              GSL
                           
                           F
                           
                              x
                           ,
                           S
                           
                              GSL
                           
                           F
                           
                              y
                           ). For our testing, we used Moving Least Square Deformation (MLSD) [35] technique for local warping because it smoothly spreads out local warps into neighboring content.

Some of the features tracked by the KLT feature tracker are erroneous and since each of the trajectories is used as control point for warping, it may cause unpleasant local artifacts to appear in the stabilized panoramic video. This problem is generally tackled by fitting a rigid geometric model on the trajectories and discarding outlying trajectories [8,9], however, the scene cannot be considered rigid in case of panoramic videos with sub-frame jitter. Moreover, this solution does not handle jitter for distinct depth planes. Therefore, we consider a deformable model [36] for the scene in a panoramic video and use clustering methods to group trajectories corresponding to motion of the content in distinct depth planes. Trajectories that do not conform to any of the trajectory clusters are considered erroneous. Further description of outlier removal follows.

For each of α frames, we map the feature trajectories (S
                           
                              GS
                           
                           F
                           
                              x
                           ,
                           S
                           
                              GS
                           
                           F
                           
                              y
                           ) to G Gaussian clusters, where G
                           ≈
                           
                              
                                 
                                    M
                                    /
                                    2
                                 
                              
                           . Intuitively, each cluster formed is a set of trajectories that roughly represent a part of the scene at approximately the same depth. Formally, for j
                           th frame, each Gaussian is defined in the normalized feature space X
                           =[(S
                           
                              GS
                           
                           F
                           
                              x
                           )
                              n,j
                           ,(S
                           
                              GS
                           
                           F
                           
                              y
                           )
                              n,j
                           ,
                           d
                           
                              n,j,j
                              −1,
                           θ
                           
                              n,j,j
                              −1], where dn,j,j−1
                            and θn,j,j−1
                            are the magnitude and direction of motion of n
                           th trajectory from j
                           −1th to j
                           th frame, respectively. Formally, gth
                            cluster is represented by a mean vector μg
                            of size 4 and a 4×4 covariance matrix ∑
                              g
                           . Expectation Maximization (EM) algorithm [37] is used on feature space X with training weights W
                           =
                           [4,4,1,2]. The rationale for these training weights is that most common-mode high-frequency motions get damped in the previous steps, therefore, it is less likely that feature trajectories spaced far apart will have the same motion even if they are in the same depth plane. Fig. 6
                            shows the output of our clustering scheme for a test case.

We estimate G such Gaussian clusters for each of α frames and cumulative association probability ρ
                           n,g of n
                           th trajectory to g
                           th cluster is computed by summing association probabilities over all frames. If normalized cumulative association probability of a trajectory is lower than a certain threshold (we use 1.5/
                           G), it is discarded. Estimating Gaussian mixtures for every frame may seem computationally expensive because of the number of iterations required by EM to estimate a mixture. However, the complete iterations are run for initial frame only. For subsequent frames, EM is initialized with previously computed Gaussian mixture which generally enables termination of EM after one or two iterations. This formulation works because the feature space X of a valid trajectory only varies slightly from one frame to the next.

@&#RESULTS AND DISCUSSION@&#

To evaluate our proposed scheme for panoramic videos, we developed a custom panoramic video acquisition system that consists of 5 Sony FS100 cameras, mounted on a wheeled metal rig in a mirror based setting as shown in Fig. 2a. Each camera captured 1920×1080 resolution frames at 30 frames per second. For stitching the videos, we used the method described in [1] using OpenCV. In the following, we provide a thorough discussion and experimental analysis of our three step stabilization scheme followed by our comparison results with prior schemes.

In order to provide a demonstration of the efficacy of the three step process, we generated outputs for each of the stabilization steps proposed in Section 3 for three video sets: ‘Man’, ‘Cart1’ and ‘Cart2’ video. All these outputs have been provided in the results section of the accompanying video. Since the unpleasant jittery effects are a result of large movements between successive frames, we analyze the effect of our stabilization scheme on the instantaneous motion of a strong static feature point of ‘Cart1 video’. This is obtained by differentiating its trajectory along the x and y directions (dF
                     
                        x
                     /dx,
                     dF
                     
                        y
                     /dy), thus providing a useful measure of overall instantaneous scene motion from one frame to the next. In Fig. 7
                      (b, c, d and e), we compare the vertical instantaneous motion of the trajectory of a strong background feature for the case of original, globally stabilized, sub-frame stabilized and completely stabilized video. Please note that while global stabilization step removes a significant amount of jitter (Fig. 7c), it makes the individual jitter of each camera (sub-frame jitter) more discernible. Plots for cameras 2 and 3 (Fig. 7c) show residual jitter that is not observable in camera 1 (Fig. 7c). The sub-frame stabilization step removes most of this jitter (Fig. 7d) by applying a correction based upon the estimated motion of majority content for each camera. The residual local jitter is then removed by the third stabilization step (Fig. 7e). To further elaborate these three types of jitter, the vertical instantaneous motions of Fig. 7 are shown in Fig. 8
                      by superimposing these plots for each camera for each stabilization step. A careful view of the accompanying video confirms that the proposed scheme successfully stabilizes the three panoramic videos.

Please note that the dominant common mode global jitter overshadows sub-frame jitter. Once the global jitter is removed, sub-frame jitter becomes more discernible in the second row. This analysis has also been verified with experiments. In the results section of accompanying video, we compare the globally stabilized ‘Man video’ to that of its sub-frame stabilized video. The comparison confirms that the application of a global transform visually highlights the inter-camera jitter of the right most camera. Please note that in special cases, the method can be optimized to only handle any permutation of the three types of jitters proposed in this paper. For instance, in case of no parallax, the last step can be skipped and the output of second stage can be applied to the frames to obtain the final result.

A possible alternative to the proposed three step stabilization scheme may have been to rely on local warping only without performing global and sub-frame stabilization. This shall leave removal of all the forms of jitter to local warping, and may cause local shaking effects in the videos. Our method, on the other hand, handles the biggest components of jittery motion i.e. global and sub-frame jitter through affine or geometric models. The residual parallax induced jitter is comparatively smaller in magnitude and hence the errors caused are relatively small and do not violate the visual plausibility requirements. A comparison of ‘local-only’ stabilization to that of our three step stabilization scheme is also provided in the results section of accompanying video for ‘Man video’. It can be observed that since all the motion is being compensated locally the stabilization is not coherent globally.

To evaluate our stabilization scheme, we compared our results for a number of videos with those of [14], commercially available Deshaker [39], iMovie [40] and the panoramic video stabilization scheme proposed by [15]. We choseYeon et al. [14] for comparison because at the time of experimentation, it was the latest work on video stabilization that used full-frame warps. We selected a random subset of these results and performed a subjective qualitative evaluation survey according to the Pair Comparison Method (PC) of the ITU-T recommendation for video quality assessment methods for multimedia applications [38]. For PC, the participants were shown the original video followed by side by side results of two stabilization schemes: ours and one of the previous works. They were then asked to rate the better of the two results or declare ‘difference not discernible’. To ensure the fidelity of the responses, each side by side comparison was repeated in an opposite order. The results of PC survey are given as a bar chart in Fig. 9
                      and are discussed in the text that follows.

For all the videos, our method was able to remove the sub-frame jitter and produce visually plausible videos as concluded by the subjective qualitative survey (Fig. 9). All these comparison videos have been provided in the comparison section of the accompanying video. The ‘Cart1 video’ in the comparison section compares the result of our proposed scheme with that of Yeon et al. for a three camera panoramic video. By observing the white hut towards the left portion of the video it can be concluded that [14] manages to remove the global jitter only, which makes the sub-frame jitter more observable. The accompanying video provides a thorough comparison for ‘Walk video’ panorama to compare Deshaker [39], iMovie [40], Haq et al. and the proposed scheme. The analysis of stabilized output by Deshaker and iMovie reveals that these schemes make some use of local warping that results in a wobbling effect observable in the structure of the buildings in the background. 94% of participants in the survey suggested that the proposed scheme is more effective as compared to [15] for the relevant dataset. A careful observation of the Cart1 video stabilized by the scheme of Haq et al. confirms the subjective evaluation. Markers have been placed in the comparison videos to assist the reader in observing the inter-camera jitter that could not be removed by other schemes. The accompanying video also provides the stabilization results of Haq et al. for ‘Cart2’ and ‘Man’ panoramic video that provide a side-by-side comparison with our results and further illustrate the benefit of handling inter-camera jitter separately. In ‘Cart2 video’ please observe the jitter in the center and right most camera for the output by Haq [15]. Similarly, for the ‘Man video’ video the jitter is particularly observable in the panoramic content contributed by right most camera for the video stabilized using [15]. The shortcoming of [15] is due to the assumption that cameras are experiencing coherent jitter which does not hold true for large panoramic scenes.

We also designed several stress cases to test the performance of our method under extreme conditions. For this purpose, we built a panoramic video acquisition system that consists of a Sony HDR-SR11 and a Canon Vixia HF100 camera mounted on an acrylic sheet supported by a moderately flexible aluminum frame as shown in Fig. 2c. The flexible frame ensures greater amount of inter-camera vibrations. Stitching parameters, including homography and blend masks are computed once in the beginning. Greater amount of jitter causes higher misalignment between neighboring frames from different cameras resulting in ghosting in the overlapping region. For our stress test videos, we reduced the blending width and turned off the color correction of the videos in order to pronounce the seams. We captured 30 such videos in a variety of settings i.e. walking forward, walking sideways, from a moving vehicle, pan etc. A random subset of these videos was selected for an online user survey that confirmed to the Degradation Category Rating (DCR) evaluation scheme of ITU Recommendations [37]. For DCR, the participants were shown the original and corresponding stabilized panoramic videos and asked to rate the quality of stabilization on a scale of 1–5, with a higher scale value representing a better stabilized output. The results for DCR survey are given in Table 1
                     . A link to the survey has been provided in the supplementary material. The mean opinion score (MOS) for all the outputs was above four, indicating the success of our scheme for stabilization of panoramic videos.

The last section of accompanying video also provides stabilization results of our scheme for three stress test panoramic videos: ‘Soccer’, ‘Lawn’ and ‘Speedbreaker’. For the purpose of evaluating the performance of our method with respect to sub-frame jitter, one strong feature from approximately the same depth plane (found on a stationary background building) was selected from either side of the overlapping region in one of the two-camera panoramic video (‘Lawn video’). Their instantaneous motions in vertical and horizontal co-ordinates were calculated. In the absence of any sub-frame jitter, their relative motion computed by taking the difference of these two calculated instantaneous motions should be zero. Fig. 10
                      shows the plots of vertical (Fig. 10a) and horizontal (Fig. 10b) instantaneous motion of content on the right side of the blending region relative to the content on the left. The blue plots represent the motion of original video whereas the red plots represent our stabilized video. The Soccer video provides another supporting case where our proposed stabilization scheme is able to align the background building structure in the presence of moving players.

During our stress testing, we came across one video (‘Speedbreaker’) for which our method did not produce satisfactory results. The video, ‘Speedbreaker’ was captured from a vehicle moving at a significant speed. There is a portion in the video where an electricity pole sweeps across the scene at a high speed and close distance from the cameras. Our method manages only a slight reduction in the jitter in the said part of the video. Further analysis revealed that the pole wiped out most of the feature trajectories as it swept across the scene at high speed. Lack of sufficiently long trajectories resulted in erroneous estimation of stabilizing transforms hence that section of the video was not stabilized properly. Owing to the dynamic criterion of Eq-1 for sufficiently long trajectories, the method was able to proceed further and successfully stabilize the next parts of the video. While we disabled the color correction for the stress tests, we also noted that considerable color difference can result into a similar ‘sweeping pole’ effect which may result in noticeable local artifacts. Such an artifact can be observed towards the latter half of ‘Lawn’ video. Thus, geometric and photometric stitching imperfections can also adversely affect the quality of stabilization.

It may be noted that severe inter-camera vibrations result in stitching errors which result in misalignments in the overlapping region of the video. Beyond a certain limit, the misalignment becomes too large and ghosting artifacts appear in the blending region. This in turn makes it hard to track features and hence affects the quality of stabilization in the blending region. The aforementioned limitation on the solvable magnitude of sub-frame jitter led us to explore the possibility of stabilizing individual video streams prior to the stitching process and generating the panoramic videos afterwards using the initially computed homography. However, since the stabilized video frames are warped independently, they do not conform to the initially computed stitching homography which causes severe ghosting and misalignment. Theoretically, this problem can be solved by computing stitching parameters for each set of frames separately throughout the video sequence but despite being computationally expensive, such an approach showed induction of additional jitter in the video. Slight changes in the scene cause stitching homographies to vary from frame to frame which induces jitter. It may also be noted that the proposed scheme requires the information of the blend masks from the stitching process. If it is required to apply the stabilization scheme for panoramic videos for which blend masks are not available, a manual rough estimate of the location of the blending seams can be used to generate approximate linear blend masks with each pixel location having a weight from 0 to 1

@&#CONCLUSIONS@&#

In this paper we propose a video stabilization scheme specifically for stitched panoramic videos generated using mobile multi-camera rigs. The proposed scheme acknowledges that each camera in the panoramic video acquisition system may exhibit independent motion due to which, parts of the panorama contents coming from a camera may jitter differently from the rest. It was demonstrated that by classifying the vibrations for such systems into three categories: global, sub-frame and local vibrations, the problem can be handled efficiently and effectively. For global stabilization, a dynamic trajectory selection criterion was introduced to ensure continuity of the process pipeline. In sub-frame stabilization, for the tracked features found in the overlapping regions of two neighboring cameras, the estimated 2D smoothing transform was weighted by the blending weights generated during the stitching process. In the final step, clustering was used to ensure that erroneous feature trajectories are unable to create artifacts in the video. The performance of the proposed scheme has been compared with two recent stabilization schemes [14,15] by performing rigorous testing for over 40 panoramic videos captured in a variety of settings. Our method successfully stabilizes the videos where the traditional stabilization methods fail. A subjective evaluation was also performed that demonstrated the success of our formulation and technique.

The following are the supplementary data related to this article.
                        
                           
                              Supplementary material.
                           
                           
                        
                     
                     
                        
                           
                              Supplementary video.
                           
                           
                        
                     
                  

Supplementary data to this article can be found online at http://dx.doi.org/10.1016/j.imavis.2015.02.002.

@&#ACKNOWLEDGEMENT@&#

We would like to thank IT R&D program of MKE/ETRI (14ZR1110, HCI based UHD Panorama Technology Development) for their generous funding.

@&#REFERENCES@&#

