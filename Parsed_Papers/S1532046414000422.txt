@&#MAIN-TITLE@&#Identifying scientific artefacts in biomedical literature: The Evidence Based Medicine use case

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Classification of sentences in Evidence Based Medicine abstracts, using a standard abstract structure.


                        
                        
                           
                           Supervised sentence-oriented classification using the PIBOSO scheme.


                        
                        
                           
                           Lexical, statistical and sequential features, independent of external sources.


                        
                        
                           
                           Increased efficiency of around 25 percentage points in F-score when compared to state of the art.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Evidence Based Medicine

Text classification

PICO

PIBOSO

Machine Learning

@&#ABSTRACT@&#


               
               
                  Evidence Based Medicine (EBM) provides a framework that makes use of the current best evidence in the domain to support clinicians in the decision making process. In most cases, the underlying foundational knowledge is captured in scientific publications that detail specific clinical studies or randomised controlled trials. Over the course of the last two decades, research has been performed on modelling key aspects described within publications (e.g., aims, methods, results), to enable the successful realisation of the goals of EBM. A significant outcome of this research has been the PICO (Population/Problem–Intervention–Comparison–Outcome) structure, and its refined version PIBOSO (Population–Intervention–Background–Outcome–Study Design–Other), both of which provide a formalisation of these scientific artefacts. Subsequently, using these schemes, diverse automatic extraction techniques have been proposed to streamline the knowledge discovery and exploration process in EBM. In this paper, we present a Machine Learning approach that aims to classify sentences according to the PIBOSO scheme. We use a discriminative set of features that do not rely on any external resources to achieve results comparable to the state of the art. A corpus of 1000 structured and unstructured abstracts – i.e., the NICTA-PIBOSO corpus – is used for training and testing. Our best CRF classifier achieves a micro-average F-score of 90.74% and 87.21%, respectively, over structured and unstructured abstracts, which represents an increase of 25.48 percentage points and 26.6 percentage points in F-score when compared to the best existing approaches.
               
            

@&#INTRODUCTION@&#

Evidence Based Medicine (EBM) represents a framework that encompasses decision making in the healthcare domain based on the best existing evidence, with the goal of providing treatment options for individual patients. In order to provide patients with judicious decisions, practitioners must access the current best evidence in relevant published medical research, such as Randomised Control Trials (RCTs). The rhetorical structure of these publications generally follows the Population/Problem–Intervention–Comparison–Outcome (PICO) scheme [1]. For instance, based on the NICTA-PIBOSO corpus [2], the sentence “The authors describe the case of a 13-year-old girl who was admitted with a history of back pain and acute-onset lower-extremity weakness.” is an instance of Population class, since it gives some information about the individuals involved in the study. Similarly, the sentence “One year postoperatively, the residual cyst had gradually shrunk and had almost disappeared.” discusses the result of the study, and hence can be considered an Outcome.

In the EBM process, clinicians search for PICO elements as evidence when making their judgments. Although most of today’s domain-specific text mining approaches [3,4] are able to identify and recognise concepts in the content of scientific publications (e.g., genes, proteins, chemical elements), they are still unable to capture and retrieve scientific artefacts. These vary in scope and granularity and can be framed within a particular area or domain, such as those comprised by the PICO structure, or may have more generic roles, like Aim, Method or Results. For example, some of the existing rhetorical schemes provide a fine-grained perspective of the narrative (e.g., [5]), which leads to mixed classification results but to an increased potential to realise fine-grained linking across rhetorical types (e.g., relate a Motivation statement to a Goal, and the Goal to a Observation). Other approaches model this knowledge at a more coarse grained level (e.g., [6]), which leads to better classification results (since the types follow a rather uniform distribution), but do not enable linking – except at a very high level (e.g., relate a Scientific statement to a Methodology).

Independently of the underlying scheme, scientific artefacts articulate the essential knowledge emerging from the described research. The granularity of the scheme becomes, however, important when trying to observe and analyse evolving patterns and trends, for example from one Goal to a set of Observations, and in particular when such relations span multiple publications. Moreover, in order to gain a deeper understanding of this knowledge, the types of scientific artefacts represented by the scheme should map, as close as possible, to domain-specific aspects. Hence, while one could use generic models to represent and extract artefacts in the EBM domain, by using PICO one addresses directly the specific application and domain requirements. Finally, our focus on the PICO scheme is also motivated by the lack of well-established annotated corpora. To our knowledge the NICTA-PIBOSO corpus is currently the only such resource available for EBM, the other existing corpora being targeted towards other domains (e.g., biochemistry in the case of the corpus described in [5]) or being completely generic – for example, the Wilbur corpus [6].

Recognising scientific artefacts and their complex relationships within a publication and across multiple publications is extremely difficult. Furthermore, in order to capture their rhetorical nature and semantics, one needs to bridge the gap between unstructured text and some structured formalism (e.g., [1,7,5]). Finally, once formalised, they require integration, consolidation and linking, in order to create a comprehensive and interlinked overview of a domain. Our ultimate goal is to provide a holistic solution to the lifecycle of EBM scientific artefacts, from unstructured text to an enriched, consolidated and linked network. Achieving this goal would enable clinicians to gain a deeper understanding in the role of different PICO elements in similar studies and to discover new relations and trends within them. For instance, this would allow us to analyse better the impact of the variation of the number of participants (i.e., Population) on the Outcomes of two similar studies with the same Intervention, e.g., using the following two statements from two different publications: “A total of 23 patients with mandibular gingival cancer were treated with docetaxel by intra-arterial infusion and systemic chemoradiotherapy with cisplatinum.” and “In all, 34 patients (21 men and 13 women) with squamous cell carcinoma of the gingiva underwent radiation therapy with concurrent intra-arterial infusion chemotherapy with cisplatinum and docetaxel.”
                  

In this paper, we focus on the first step of the above-described goal, i.e., automatic extraction of rhetorical artefacts (or PICO elements) from abstracts in the EBM domain. Research on automatic recognition of scientific artefacts in biomedical publications has been reported from the early 2000s, but has only recently become more prominent, with most approaches employing Machine Learning techniques guided by specific schemes. As shown in [5], three major research directions can be distinguished: (i) sentence or zone classification according to a predefined annotation scheme [8,9,5]; (ii) detection and analysis of speculative language and hedging [10,11]; and (iii) sentence classification according to a multi-dimensional scheme [6,7,12].

In the context of the EBM domain, all existing approaches that aim at extracting rhetorical artefacts are designed to work on publication abstracts [13–15,2]. Furthermore, some of them have been developed around an extended, fine-grained PICO scheme – PIBOSO [2]. PIBOSO refines PICO by categorising scientific artefacts in six categories, rather than four: (i) Population – the group of individuals participating in a study; (ii) Intervention – the act of interfering with a condition to modify it or with a process to change its course; (iii) Background – material that places the current study in perspective, e.g. work that preceded the current study; information about disease prevalence; etc.; (iv) Outcome – a summarisation of the consequences of an intervention; (v) Study Design – the type of study that is being described; and (vi) Other – other information provided in the publication.

From a technical perspective, the underlying task is represented by sentence classification according to the PICO/PIBOSO scheme. Hence, an associated challenge is the encoding of the characteristics of a sentence in a format able to preserve both token and sentence features together. This would enable us to take advantage of all the information that can be inferred from the sentence tokens in a setting that deals with a sentence as the meaningful unit. The Machine Learning features used so far in this context vary greatly, yet they can be grouped into two major categories: structural and sequential features [14,2,16]. Structural features capture the position of the sentence in the context of the given abstract, while sequential features leverage token-based information, such as bag of words or nearest neighbours using sliding windows.

Our approach follows the same design principles as the existing solutions and employs Machine Learning techniques to perform sentence classification. In terms of features, we combine token-level and sentence-level features that capture both the positional (e.g., placement in the abstract), as well as the sequential (e.g., predicted classes of adjacent neighbours) aspects of the target classes. In addition, we introduce a new category of features – statistical features – that joins the two levels of feature granularity by computing sentence-wide token statistics. The intuition behind these new features is that each target class may be characterised by a specific statistical distribution of the types of tokens composing it – e.g., based on the verb or on other low-level linguistic information (exact details are provided in Section 3). Also, inferred sequential features derived from the co-occurrence of similar types of sentences are proposed. The overall combination of these features results in a representative feature vector that enables the training of an accurate classifier.

We have performed an extensive set of experiments using the NICTA-PIBOSO corpus [2] – described in detail in Section 3.1. Our results show that classifiers trained with a mixture of the above-presented features are able to achieve an accuracy comparable to the state of the art. Four different classification methods have been investigated: Conditional Random Fields (CRF) [17], Support Vector Machines (SVM) [18], Naive Bayes [19], and Multinomial Logistic Regression [20]. Among these, CRF has achieved a micro-average F-score of 90.74% and 87.21%, respectively, over structured and unstructured abstracts, which represents an increase of 25.48 percentage points and 26.6 percentage points in F-score when compared to best existing approach. In summary, the main contributions of this manuscript are: (i) a selection of features that enables an increased accuracy of sentence classification based on the PIBOSO scheme and (ii) a comprehensive experimental setup that provides a good overview of the behaviour of different classification mechanisms using diverse feature configurations.

The remainder of the paper is structured as follows: Section 2 describes relevant existing research. In Section 3 we detail the classification features and the data used within our experiments, as well as the experimental setup. Section 4 presents the evaluation results of achieving based on different feature configurations – (i.e. the presence or absence of particular feature sets in the experiments), and Section 5 discusses the experimental results and provides a thorough comparative analysis of the state of the art. Finally, we conclude in Section 6.

@&#RELATED WORK@&#

In order to reach an efficient and effective judgement in EBM, Richardson et al. [1] concluded that clinicians require an addition fundamental skill, i.e., to ask well-built clinical questions. Consequently, they propose the PICO criteria for formulating these questions. The elements of this scheme (or of the refined ones, e.g., PIBOSO) represent scientific artefacts, intrinsically captured by scientific publications and require human interpretation in order to make the explicit. From a computational perspective, this can be represented as a classification task that aims to either support humans in their annotation process or fully automate the extraction of such scientific artefacts.

The first approach to automatically identify outcome-related information and PICO elements in medical text was proposed by Demner-Fushman et al. [21,13]. Their solution employed an ensemble of six base classifiers, including a rule-based classifier, a Naive Bayes classifier, an n-gram-based classifier, a position classifier, a document length classifier, and a semantic classifier. The rules required by the rule-based classifier were handcrafted, while the rest of the classifiers were trained on their corpus containing 275 abstracts. Their final accuracy of identifying outcome elements ranged between 88% and 93%. At a later stage, the authors have focused on the PICO structure and developed a Naive Bayes classifier using most of the features mentioned above, in addition to semantic information derived via MetaMap [22]. MetaMap provides a framework to annotate biomedical text with Unified Medical Language System (UMLS) Metathesaurus [23] concepts.

Chung [14], on the other hand, combined generic scientific artefacts (or rhetorical roles) in conjunction with PICO elements (Intervention, Outcome, Participants) to perform sentence classification. The approach consists of two steps. Firstly, a classifier is trained with the goal of recognising generic rhetorical roles such as Aim, Method, Results, and Conclusions. Secondly, standard token-based features are combined with the predicted rhetorical roles (from the first classifier) to train and test two classifiers (one CRF and one SVM) that target individual PICO class. More concretely, for every PICO element a new classifier is built by adjoining the four generic roles as classes to each individual PICO class, hence resulting in a 5-way classification. The results are, however, focused on the actual PICO element and not the rest of the classes and vary between 0.80% and 0.88% F1. In addition to the rhetorical roles, the features vector consists of a unigram bag-of-words, POS tags, positional information, and windowed features of the previous and the following sentences. In terms of classification models, CRF outperforms SVM in their experimental results.

The rhetorical structure of a biomedical abstract has, usually, a fairly well-established flow that starts from some background information and details gradually the research before concluding with some outcomes. Boudin et al. [15] have leveraged this aspect and proposed a solution that uses the location of sentences and the distribution of PICO elements from an Information Retrieval point of view. They have also applied a weighting model based on the PICO information, which assigned different weights to different parts of a document. However, their rigid partitioning in ten equal parts, in addition to their classification method without enforcing any restriction in the size of instances (i.e. instances could be words, phrases, or sentences) did not improve the classification efficiency.

The two approaches closest to our solution and goals are those of Kim et al. [2] and Verbeke et al. [16]. Kim et al. perform classification in two steps using the refined PIBOSO scheme. In the first step, a classifier identifies the sentences that contain PIBOSO concepts, while in the second step, a different classifier assigns PIBOSO classes to the sentences found to be relevant by the previous classifier. The annotation is performed at sentence level and one sentence may have more than one class. They also employ the CRF implementation of MALLET as their classifier model, using features derived from the context, semantic relations, structure and the sequence of sentences in the text. Domain-specific information is obtained via Metamap. Their final feature vector includes a combination of the following features: bag-of-words, bigrams, POS tags, semantic information, section headings, sentence position, and windowed features of the previous sentences. As opposed to their approach, we do not make use of any external resources.

Verbeke et al. [16], on the other hand, apply a statistical relational learning approach using a kernel-based learning (kLog) framework to perform classification using the NICTA-PIBOSO corpus. They exploit the relational and background knowledge in abstracts, but take into account only the sequential information at word level. More concretely, their feature set includes a sequence of class labels of the four previous sentences as well as of the two following ones, the lemma of the dependency root of the current sentence and the previous sentence, the position of the sentence, and the section information.

Finally, Sarker et al. [24] use a set of binary SVM classifiers in conjunction with feature sets customised for each classification task to attain the same goal. Using the same NICTA-PIBOSO corpus, they use MetaMap to extract medical concepts, and in particular UMLS Concept Unique Identifiers (CUIs) and Semantic Types, to be then considered as domain-specific semantic features. The rest of the features they employ consist of n-grams, POS tags, section headings, relative and absolute sentence positions and sequential features adapted from Kim et al. [2], as well as class-specific features for the Outcome class only. As we discuss in Section 5, their results are comparable to the previous approaches and outperform them especially on unstructured abstracts for the Population and Study Design classes.

Apart from the above discussed approaches, which are directly relevant to our work, there are many other similar methods proposed to identify scientific artefacts from publications, however with a focus on different aspects (e.g. [5,6,8,25]). While they are relevant from a high-level overview perspective, their use of a different annotation scheme makes it impossible for us to provide a direct comparison of the experimental results. We will, however, discuss them later in the manuscript in the context of the possible mappings one could envision between these schemes and the one we have employed.

In this section we describe the corpus, the feature sets and the classification models used by our approach and conclude with the experimental setup.

In order to train and test classifiers that satisfy our goal of recognising scientific artefacts according to the PIBOSO scheme, we have used the NICTA-PIBOSO corpus [2], kindly provided by Kim et al. A general overview of the corpus statistics can be found in Table 1
                        . The corpus consists of 1000 abstracts, 500 retrieved from MEDLINE by querying for varied aspects related to traumatic brain injury and spinal cord injury and 500 randomly sampled by querying for various other medical issues. The Evidence Based Medicine focus of the corpus has been ensured by considering only systematic reviews of the literature, such as Randomised Controlled Trials, comparative studies and case reports. To this end, the collection of abstracts has been performed by narrowing the search to the publications provided by two institutions: The Global Evidence Mapping Initiative (GEM), and The Agency for Healthcare Research and Quality (AHRQ). The corpus was annotated manually by a medical student with the continuous collaboration of a senior medical expert using the Annotex tool, designed especially for this task [2]. In order to measure agreement, one of the authors has blindly annotated 60 abstracts [2]. The class-based inter-annotator agreement based on Cohen’s Kappa values is also provided in Table 1.

From a structural perspective, the corpus contains two types of abstracts: (i) structured, and (ii) unstructured. The difference between the two categories is given by the presence (or absence) of appropriate section headings within the abstracts (i.e., Background, Methodology, Results, etc.). Tables 1 and 2 in the Supplementary Information provide the distribution of section headings in the entire corpus, as well as mapped onto the underlying sentence type. This separation has been created to gain a better understanding of the role carried by the section headings in the sentence classification (since they can be used as features). Each sentence within the corpus is tagged with multiple annotation categories, based on the PIBOSO scheme. As shown in Table 1, the coverage of the different classes varies from 2.24% for Study design class to 43.57% for Outcome. A better overview of the class distribution can be seen in Fig. 1
                        . Consequently, from a model learning perspective, the set of features chosen for classification needs to cater for both well-represented, as well as under-represented classes. In addition, it is worth mentioning that sentences in the NICTA-PIBOSO corpus may have multiple labels assigned (around 5% of the sentences are in this category). Hence, addressing this ambiguity aspect in a comprehensive manner may require either a series of binary classifiers combined with a set of post-processing rules or a chunker that would enable the segmentation of the sentences into the elementary components that correspond to the multiple labels. Currently, our solution does not focus on dealing with ambiguous sentences.

We used four sets of features to build sentence classifiers. The actual feature values range from signalling flags (indicating the presence/absence of a feature) to discrete encodings of numeric or symbolic values – for example, the position of the sentence in the paragraph, or counting the co-occurrence of features of words that characterise a sentence, and then transforming the resulting count into a set of discrete pre-established values. Features that build on linguistic aspects have been extracted using the GATE Natural Language Processing toolkit [26], while the rest have been computed in a direct manner. In the context of GATE, we employed the ANNIE (A Nearly-New IE system) standard pipeline consisting of: Tokeniser, POS Tagger, in addition to a Verb Phrase Chunker. The POS tag output of ANNIE contains lexical information as well as the orthographic characteristic of each token. In order to retain the corpus consistency and since the sentences boundaries were available in the corpus, we did not use an external sentence splitter. Morphological attributes, e.g., lemmas, were extracted using the GATE Morphological Analyser. Verb-related attributes (e.g., tense, voice, etc.) used in some of the proposed statistical features were obtained from the ANNIE VP Chunker component. Finally, as mentioned above, the rest of the features, such as those comprised in the statistical feature set, were computed by counting the frequency of each target attribute within a particular sentence, while the positional and sequential features were compiled by capturing structural specifications and by analysing the prior outcome of a trained model.

For clarification purposes, throughout the description of the feature sets we use a series of statistics on different types of tokens present within particular types of sentences to justify some of our design decisions. These corpus-wide statistics are provided solely for information purposes and were not used directly as features in the learned models – i.e., we have tried to avoid creating an obvious bias of the resulting model towards the training corpus. The classification models have been trained using discrete features, similar to those used by other relevant approaches.

This set of features builds on the most basic meaning-encoding elements of a sentence to leverage distinct higher-level valuable patterns.
                              
                                 
                                    Part of speech (POS) tags (Also used in
                                    
                                    [2,14,16,24]): We maintained and used the POS tags of each individual sentence token as a feature. The tags include nouns, adjectives, adverbs, verbs, coordinating conjunctions, determiners, prepositions, and pronouns. The POS tags for each token were preserved as provided by GATE ANNIE. For example, “patient” would have assigned an NN feature.


                                    Orthographic case: For each token we have computed and maintained its orthographic shape – i.e., allCaps (only capital letters), lowerCase (only lowercase letters), mixedCaps (a mixture of capital and lowercase letters) and upperInitial (signalling a capitalised token).


                                    Lemma (Also used in
                                    
                                    [16]): In order to cater for various variants of a token, we have also used its lemma, acquired during linguistic pre-processing.

Although POS tags have been extensively used as training features in ML approaches for sentence classification, the orthographic forms of the tokens and the lemmas have not been commonly applied for this task [2,14,16,24]. The only previous solution that makes use of lemmas is that of Verbeke et al. [16], however, they consider only the lemma of the dependency root of the previous and current sentences, as opposed to all sentence tokens in our case. As a note, bigrams have occasionally been used as features in previous approaches, however, Kim et al. [2] show that unigrams lead to better results than bigrams in the context of our goal.

As mentioned in the previous section, we propose the use of statistical features to capture token-based sentence-wide information, with the assumption that each target class may be characterised by certain statistical distributions of the types of tokens composing it. More concretely, we took into account detailed verb-related information, in balance to non-verb-related information.
                              
                                 
                                    Verb-related statistics: The number of occurrences of different verb tenses, types and voices in a particular sentence. This includes a count of verbs at past or present tense, as well as a count of verbs having a passive or active voice. In addition, we also capture verb negation (i.e., whether the verb has been negated or not). As mentioned earlier, this information has been extracted by means of the ANNIE VP Chunker component in GATE. Existing studies show that verb information represents a good indicator of the different types of scientific artefacts in biomedical literature [27]. Furthermore, certain patterns seem to emerge from verb attributes, as shown in Table 2
                                    , where Population or Intervention sentences seem to feature predominantly past tense verbs, while negation seems to be present mostly in Outcome sentences. From a rhetorical perspective, the polarity of expressed opinions usually occurs in argumentative statements within publications and may be helpful in finding answers for some clinical questions [28]. Hence, including the presence of verb negation could be beneficial and discriminative in identifying certain types of sentences, such as Outcome and Background sentences. It is, however, worth mentioning that this set of features is not individually representative of types of sentences, but rather can be used as a discriminative factor between groups of sentences. For example, 58% of verbs in Background sentences are at present tense, which is considerably higher than in any other type of sentence. Similarly, Intervention and Population sentences use passive voice as well as past tense more than the rest of the classes.


                                    Non-verb related statistics: Counts of nouns and adjectives in each sentence, in addition to the total sentence length in tokens. Similar to the verb-related information, however less selective, the use of nouns or adjectives may be associated more with certain types of sentences than with others. Table 3
                                     lists the statistics of the average occurrence of non-verb features in each type of sentence. Adjectives seem to be more present in Background and Study Design sentences, while adverbs more prominent in Background and Outcome.

To our knowledge, none of the previous approaches make use of sentence-wide statistical features.

In general, the rhetorical structure of scientific publication abstracts in the biomedical domain follows a fairly standard form, which starts with some background information, continues with the problem statement and then describes the core aspects before concluding with a set of outcomes. Each of these levels of the structure can be easily associated with the target classes of the PIBOSO scheme. Consequently, the position of a sentence in the abstract represents a good indicator of its associated class. In most cases, a certain level of ambiguity remains, however, this is usually lowered to two classes, rather than the entire scheme. In this category we have used the following positional features:
                              
                                 
                                    Sentence position (Also used in 
                                    [2,14,16,24]): The position of each sentence from the beginning of the abstract.


                                    Section heading (Also used in 
                                    [2,16,24]): In structured abstracts (only), all the sentences between two headings are enriched with a feature that consists of the content of the heading under scrutiny.

Taking the positional information a step further, we can assume that, in a coherent abstract, sentences belonging to the same class are usually adjacent, except for the cases where there is a transition between classes. Consequently, providing the classifier with the history of the past occurrences of specific classes would have a positive impact on the classification task. Most of the previous approaches used as sequential information windows of adjacent instance features [14] or the predicted class labels of preceding and subsequent instances [2,29]. Similar to these approaches, we take into account the predicted labels of the two preceding sentences, however, instead of recording them directly, we compare them to the predicted label of the sentence under scrutiny. Subject to the level of confidence of the classifier and the outcome of the comparison, we record the equality between the resulting classes. For example, if the two preceding sentences are labelled with the same label as the current sentence with a high confidence, we record a “double previous support” feature.

In order to populate this feature, a learner model is first trained on the rest of the feature sets and then by performing a selection strategy based on the highest probability of the model in predicting target labels, the value of this feature would be inferred and attached to each sentence in the data set. As a concrete example, let us consider the following three sentences and their predicted labels from a previously trained model:
                              
                                 •
                                 <S1 Background>: “Spinal hydatid disease is a rare entity that frequently yields to severe, acute-onset neurological deficits.”

<S2 Background>: “Although the gold standard treatment is total surgical removal of the cysts without inducing any spillage, it may not be possible to perform this in patients with multiple and fragile cysts.”

<S3 Population>: “The authors describe the case of a 13-year-old girl who was admitted with a history of back pain and acute-onset lower-extremity weakness.”

The value of this feature is computed in the following manner:
                              
                                 •
                                 S1: Empty value – since it is the first sentence in the abstract.

S2: The label of S2 is the same as S1, hence if the confidence attached to both labels is high enough, a “single previous support” value is assigned to the feature.

S3: Using the same inference as in the previous case, since both previous labels are different than the current one, a “different” value is assigned to the feature.

The entire feature vector is presented in Table 4
                           . In the following section we report on the experimental results achieved by using different parts of this feature vector in the classification task.

We have conducted experiments with four types of models. Firstly, as it has been widely and successfully used in the literature [2,14,5,25], we have trained a Conditional Random Fields (CRFs) [17] tagger. CRFs are a probabilistic framework for labelling and segmenting sequential data and have the advantage that, in addition to supporting a large number of features, they are also able to intrinsically model the sequential form of the data. A conditional random field is an undirected graphical model that calculates the conditional probability of output values based on given input values. The graphical model may take various shapes, such as linear or skip-chain. Linear-chain CRFs capture local (near) dependencies in a given sequence, while skip-chain CRFs cater for dependencies located at arbitrary distances in the sequence [30]. In contrast to traditional classification algorithms in Machine Learning, CRF not only considers the attributes of the current element when determining the class, but also attributes of preceding and succeeding items. Furthermore, in the context of sentence classification, CRF classifiers have been shown to have several advantages over other sequence models such as Hidden Markov Models (HMMs) [14]. From a development perspective, we have used the MALLET [31] implementation of the linear-chain CRF with default settings, i.e., Gaussian prior variance for training.

In order to provide a comprehensive experimental setting, we have trained three additional classifiers using the WEKA toolkit [32]: Support Vector Machines (SVM) [18], Naive Bayes [19], and Multinomial Logistic Regression [20]. As in the case of CRF, we have used the standard configuration parameters, in addition to Sequential Minimal Optimisation (SMO), which is an efficient algorithm for training a support vector classifier with a polynomial kernel.

Using the data and features described above, we have trained sentence classifiers using stratified 10-fold cross-validation – stratification eliminates the possible bias introduced by a skewed distribution of the classes in particular folds. We report the results using the standard class-based (or micro) precision, recall and F1 scores [33]. This enables us to provide a clear view over the behaviour of the classifier in each class, in addition to comparing our results to prior approaches. Furthermore, to gain a deeper understanding of the results, each experiment is presented and discussed in the context of the two types of abstracts comprised in the corpus (i.e., structured and unstructured) and using the complete “6-way” PIBOSO scheme, as well as the “5-way” PIBOS scheme. This last aspect emerges from the interpretation of the instances that do not belong to any of the meaning-carrier classes. More concretely, in PIBOSO (Problem–Intervention–Background–Outcome–Study design–Other), such instances are considered under the Other class, and labelled accordingly. However, one may also choose to ignore them (from a classification perspective – and leave them out of the data set), which leads to classifying sentences according to the first five categories of PIBOSO – i.e., PIBOS.

@&#EXPERIMENTAL RESULTS@&#

In order to have a comprehensive understanding of the impact each feature set has on the classification model we have performed experiments using various configurations, including single feature sets, all feature sets and leave-one-out (i.e., iterating over the full feature set and testing models that leave one feature set out at any point in time). Below we start by presenting and discussing the full model and then iterate over a series of other configurations, in particular in the leave-one-out setting. Except for the full model where we present a comparison between CRF, SVM, Naive Bayes, and Multinomial Logistic Regression (MLR), all other classifiers reported below have been trained using CRF.


                        Table 5
                         lists the results achieved by the CRF classifier covering all feature sets, while Tables 6–8
                        
                        
                         present the results achieved by the other classifiers using the same configuration. We can observe that in the 6-way classification setting over structured abstracts CRF achieved a F-score of more than 95% on three classes (Background, Outcome, Other), with the best result reaching 99.07% for the Outcome class. The results on the unstructured abstracts are similar, with a F-score of more than 95% achieved for the same three classes and the best score of 97.68% reported for the Outcome class. The most problematic classes in both types of abstracts have been Intervention and Study design, which are also the most under-represented classes in the corpus. The results in the 5-way classification setting follow the same pattern as in 6-way classification but they are relatively higher. Comparing the two settings reveals an increase in F-score of around 5 percentage points in the case of the Population and Intervention classes in both structured and unstructured abstracts. This leads to the conclusion that Population and Intervention share discriminative features with Other, and hence the 5-way classifier is able to take advantage of this missing sixth class to improve its classification accuracy.

A comparison between the results achieved by the other classification models shows SVM to be superior on most classes in the 6-way classification scenario and MLR to dominate the 5-way classification over both structured and unstructured abstracts (Tables 6 and 8). Although MLR achieves good results for some classes, such as Outcome in 6-way and Background and Outcome in 5-way (both over structured abstracts), its overall performance has been fairly poor in comparison to CRF. The difference in F-score between the two classifiers ranges from 1–7 percentage points on the classes mentioned above, to a maximum of 46.5 percentage points on Study Design in 6-way classification over unstructured abstracts. Naive Bayes (Table 7) had a mixed performance as well, achieving good results in 5-way classification when compared to SVM and poor results in 6-way classification when compared to the same classifier. In conclusion, while most methods performed average over the well-represented classes, CRF proved to be superior across all classes, independently of their distribution.

As mentioned above, in order to understand the impact of each feature set on the overall performance of the classifier, we performed additional experiments iteratively leaving one feature set out (LOOF) from the full model. Table 9
                         lists the results achieved by leaving the sequential feature set out. CRF intrinsically uses sequential information during the training process, however, as we can observe from these results, discarding the explicit sequential feature set leads to a negative effect. All F-scores have decreased, with the under-represented classes being the most affected. For example, while Background has suffered a loss of around 4% F-score across both classification schemes over structured and unstructured abstracts, Study design has lost around 30% v in the 6-way classification and around 20% in the 5-way classification over both types of abstracts. The best F-score is achieved by the Outcome class in both 6-way and 5-way classifications; Outcome has been also the class with the minimal loss of F-score in this setting.


                        Table 10
                         lists the results of leaving the statistical features out. Multi-class classification forces a classifier to learn discriminative patterns associated with each individual class, with the final model representing an optimum across all classes. Consequently, by discarding some features, one may experience an improvement in the classification results in those classes that were disadvantaged by the presence of these features. An example of this phenomenon (although at a very low scale) can be observed in this feature set configuration, where the Outcome class, in the 5-way classification over structured abstracts, achieves minimally better results than in the full model configuration (Table 5 – i.e., 99.67% F-score against 99.38%. From a rhetorical perspective, Outcome instances usually occur in the last segment of an abstract and are fairly well grouped. This positional uniformity (also shown in Table 11
                        ), combined with the important representation in the corpus appears to be enough for this class to be accurately identified – the rest of the features being able to improve only marginally the classification results. Moreover, these statistical features introduce confusion rather than aid the discrimination – probably because the distribution of the elements taken into account by them does not stand out from the entire group of classes.

Most of the other classes have suffered minimal losses in F-score in the 5-way classification over both types of abstracts – up to maximum 10%. In the 6-way classification, on the other hand, Population and Intervention have been affected in both structured and unstructured abstracts, while Study design has been affected only in the case of unstructured abstracts – the average F-score loss was of 30%.

A side experiment we have performed revealed that the positional feature set, in conjunction with the original tokens and their POS tags, is able to achieve a high accuracy classification across most classes. For example, as shown in Table 11, the F-scores for the Background and Outcome classes are above 90% in both 6-way and 5-way classification over both types of abstracts, while F-score for the rest of the classes are in the range of 40% to 70%.

The significance of this feature set is to a large extent observable also in the leave-one-out configuration that excludes it from the model – see Table 12
                        . All classes suffer major decreases in F-score across all classification options, with the exception of the Outcome class, which is very well represented by the rest of the feature sets and has lost only around 10–15 percentage points in F-score. Another observation is that the Study design class is more dependent on this feature set in the context of unstructured abstracts than the rest of the under-represented classes. The difference in F-score between structured and unstructured abstracts in both 6-way and 5-way classification has been minimal for the Population and Intervention classes, i.e., 0–3 percentage points, while Study design has lost around 30 percentage points when changing from one type of abstract to the other. This shows that Study design is highly dependent on the positional features and, moreover, it often co-occurs with particular section headings, which support its accurate identification.

Finally, Table 13
                         lists the results achieved by the fourth possible LOOF combination, i.e., discarding the token-based feature set. The trend is largely similar to the first two configurations, with the well-represented classes suffering minimal or no F-score losses and the others being heavily affected. As in the case of the previous configuration, Study design experiences a significant drop in F-score when changing from structured to unstructured abstracts in the 6-way classification (from 58.91% to 15.17%), however, in this case Population and Intervention have a similar behaviour, both loosing around 15% in F-score – as opposed to the LOOF positional feature set where the difference was up to 3%.

There are a series of general observations that can be derived from the LOOF classification experiments. Firstly, as expected the full model outperformed the LOOF models in all experiments, with one exception – the LOOF statistical feature set configuration, where we have witnessed a minimal increase in performance for one class in one particular setting (5-way classification over structured abstracts). Secondly, positional features have an important role in classifying these scientific artefacts, which strengthens the hypothesis that, at least in the EBM domain, abstracts follow a fairly rigid structure (even at this fine-grained level and when the actual section headings are missing). Thirdly, the 5-way classification setting always yields better results than the 6-way classification. This is, in particular, coupled with the observation that the Population and Intervention classes share features with the Other class, and by ignoring the latter leads to a positive effect on the first two. Finally, the same two classes are heavily dependent on statistical and token-based features, which leads to the conclusion that shallow linguistic patterns seem to exist within them, in addition to some key trigger tokens that enable a more accurate classification. In practice, as we discuss in the following section, such trigger tokens are also the cause of some of the mis-classification errors.

@&#DISCUSSION@&#

As presented and discussed in the previous section, our approach achieves good results across all classification settings over both structured and unstructured abstracts. Furthermore, we have noted a slightly better behaviour in the 5-way classification context and a fairly strong dependency on certain feature sets (e.g., positional) of the under-represented classes. Overall, the results show that independently of the approach, high efficiency is achieved primarily on structured abstracts. This may serve as a lesson learned and leads to the conclusion that if we are to achieve significant progress in this area, we should support the extraction process by providing structured abstracts for all publications. From a different perspective, this represents a good example of “A small amount of effort goes a long way”, since the presence of the section headings within the abstracts improved significantly the classification accuracy, in particular for the more problematic classes.

In this section, we take a step further and try to understand some of the common mistakes that led to mis-classification errors and compare our results against the state of the art.

In order to gain a deeper understanding of the errors made during the classification, we have compiled corresponding confusion matrices for all possible classification settings, i.e., 6-way and 5-way classification over structured and unstructured abstracts. Tables 14 and 15
                        
                         present the confusion matrices for the 6-way classification. In the case of structured abstracts, we can observe, in principle, two major types of confusion errors performed by our classifier: Intervention sentences classified as Population (and vice versa) – 19 out of 313 (6%) and 12 out of 369 (3.25%) – as well as Study design classified as Population (11 of 149 – 7.38%). We have considered these to be more significant because the ratio of errors to the total number of sentences of those particular types is higher. It can be observed that these errors are correlated with the inter-annotator agreement (presented in Table 1 – i.e., they dominate the classes on which the human annotators had the highest disagreements.

Another error presented by this confusion matrix is Background sentences classified as Intervention, however, these errors represent a mere 1.49% of the total number of Background sentences. On the other hand, all these errors are amplified in the case of unstructured abstracts, where some of the positional features are unable to aid the classification (Table 15). For example, both Intervention classified as Population and vice versa increase in error to 8.22% and 4.96%, respectively. Moreover, additional types of errors emerge, such as Population sentences classified as Outcome (15 of 443 – 3.38%).

A closer look at the mis-classified sentences has revealed three general observations. Firstly, the Population class seems to be discriminated based on a series of trigger tokens, such as patient, age, percentage, as well as number tokens. This can be clearly seen in the example sentence “These studies cited an 11–13% incidence of low T3 syndrome (LT3S) in SCI patients, with an increased incidence in tetraplegics (20–36%).”, which is according to the gold standard a Background sentence, but has been classified as Population. Secondly, Population and Intervention classes are highly ambiguous from a classification perspective, because usually, they carry both roles. For example, “The overall efficacy and tolerance of a new skeletal muscle relaxant DS 103–282 was evaluated by treating 10 patients with chronic spinal spasticity.” is an Intervention that has been classified as Population, or “Between April 1985 and 2005 orthotopic bladder substitution with an ileal low pressure reservoir was performed in 482 patients (including 40 women) after radical and, if possible, nerve sparing cystectomy.” is a Population classified as Intervention. In practice, these sentences should be associated with both classes and according to the gold standard they are, however, the classifier is only able to choose one single classification result – the one with the highest confidence. This leads to the need to segment sentences into elementary units and assigning classes to these units, rather than to the entire sentence – as done, for example, by Shatkay et al. [7]. We would, hence, create a clear and unequivocal association between a class and a span of text and lower the granularity of the knowledge to its atomic form. From a recognition perspective, the task is transformed from classification to chunking or segmentation.

Finally, a very small number of classification errors were actually errors in the gold standard. For example, “Intraperitoneal or intracerebroventricular injections of agmatine rapidly elicit antidepressant-like behavioural changes in the rodent forced swim test and tail suspension test.” is a Population sentence according to the gold standard, while our classifier assigned it to the Intervention class, which is, in reality, the correct one.

The 5-way classification retains some of the errors made in the 6-way classification (as seen in Tables 16 and 17)
                        
                        , e.g., Intervention and Population classified as Background or Intervention classified as Population, but at a much lower rate. This again shows that by ignoring sentences that are out of the scope of EBM, better classification results would be attained. Furthermore, the specific errors made in classification are the same as in the 6-way classification setting.

As mentioned previously, several other approaches have been proposed to address the goal of recognising scientific artefacts according to the PIBOSO scheme. Some have followed the same design aspects, i.e., sentence classification using Machine Learning, others employed different techniques, e.g., statistical relational learning. Among these existing solutions, we were able to create a comparative overview against three of them, because they all rely on the same corpus for training and testing: Kim et al. [2], Verbeke et al. [16] and Sarker et al. [24] (the latter only in the context of 6-way classification).


                        Tables 18 and 19
                        
                         present this comparative overview using the same structure as our experimental results, i.e., 6-way and 5-way classification over structured and unstructured abstracts. We can observe that our approach outperforms all other approaches at all levels. In the context of structured abstracts (for both classification settings) the difference in F-score between our solution and the next best one are relatively small, ranging from 3% to 11%. However, this stands only for the well represented classes (Background and Outcome), the rest displaying significant differences that range from around 20% on Study design (Kim vs. ours) to 53.19% on Population (Sarker vs. ours) – for the 6-way classification. The major improvements are, on the other hand, better observed in the context of unstructured abstracts. In the 6-way classification, the problematic classes achieved reasonable scores when employing our approach (min. 77.65% for Study design) and low scores as reported by the other approaches: min. 4.4% or 6.67% for Study design by Kim and Verbeke (although Sarker achieved 60.3%), or 12.68% and 16.14% for Intervention by Kim and Verbeke (although Sarker, again, achieved a higher score of 38.9%).

The literature also contains approaches that have addressed similar goals – i.e., sentence classification in abstracts of scientific publications using other annotation schemes, e.g., Hirohata et al. [25] Although some similarities exist, e.g., the focus on structured abstracts or possible mappings between classes – i.e., Results and Conclusions classes to Outcome – we are, however, unable to draw an appropriate comparison between them due to the lack of a common foundation. The target classes do not overlap completely and the underlying data is different. Nevertheless, there are aspects that are worth noting, for example, a similar set of F-scores achieved on those classes that could be partly mapped: 95% and 94.2% on Results and Conclusions respectively, vs. our 99.07% on Outcome.

@&#CONCLUSION@&#

In this paper, we proposed a Machine Leaning approach for recognising scientific artefacts in biomedical abstracts, in the context of Evidence Based Medicine. Our solution relied on a sentence classification according to the PIBOSO scheme (Population–Intervention–Background–Outcome–Study design–Other). The feature sets used in conjunction with a CRF tagger, covered from token-based observations to positional, statistical and sequential aspects. At the same time, unlike some of the previous research, our approach did not use any external resources (e.g., domain-specific concepts) as classification features. Experimental results have shown a significant improvement over the state of the art, leading to a micro-average F-score of 90.74% and 87.21%, respectively, over structured and unstructured abstracts, i.e., an increase of 25.48 percentage points and 26.6 percentage points in F-score when compared to best existing approaches.

As stated in the introduction, our overarching goal is to develop a holistic solution for the lifecycle of scientific artefacts in the context of EBM, by integrating and linking them based on their conceptual semantics. Consequently, our future work will focus on the next step in this process, i.e., consolidation of recognised scientific artefacts, or more concretely on clustering PIBOSO elements that share the same type according to their underlying meaning. This will support a more efficient and meaningful retrieval of abstracts that discuss, for example, studies targeting the same population or having the same outcomes.

@&#ACKNOWLEDGMENTS@&#

This research is funded by the Australian Research Council (ARC) under the Discovery Early Career Researcher Award (DECRA) – DE120100508.

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.jbi.2014.02.006.


                     
                        
                           Supplementary material
                           
                              Tables 1 and 2.
                           
                           
                        
                     
                  

@&#REFERENCES@&#

