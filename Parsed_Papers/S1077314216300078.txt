@&#MAIN-TITLE@&#Gender and gaze gesture recognition for human-computer interaction

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We introduce a gender recognition algorithm that adopts Fisher Vectors.


                        
                        
                           
                           We propose an unsupervised modular approach for eye centre localisation.


                        
                        
                           
                           We design gaze gestures intended for controlling a HCI system remotely.


                        
                        
                           
                           We develop a HCI system as a type of assistive technology.


                        
                        
                           
                           All the proposed methods are highly accurate, efficient and robust.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Assistive HCI

Gender recognition

Eye centre localisation

Gaze analysis

Directed advertising

@&#ABSTRACT@&#


               
               
                  The identification of visual cues in facial images has been widely explored in the broad area of computer vision. However theoretical analyses are often not transformed into widespread assistive Human-Computer Interaction (HCI) systems, due to factors such as inconsistent robustness, low efficiency, large computational expense or strong dependence on complex hardware. We present a novel gender recognition algorithm, a modular eye centre localisation approach and a gaze gesture recognition method, aiming to escalate the intelligence, adaptability and interactivity of HCI systems by combining demographic data (gender) and behavioural data (gaze) to enable development of a range of real-world assistive-technology applications.
                  The gender recognition algorithm utilises Fisher Vectors as facial features which are encoded from low-level local features in facial images. We experimented with four types of low-level features: greyscale values, Local Binary Patterns (LBP), LBP histograms and Scale Invariant Feature Transform (SIFT). The corresponding Fisher Vectors were classified using a linear Support Vector Machine. The algorithm has been tested on the FERET database, the LFW database and the FRGCv2 database, yielding 97.7%, 92.5% and 96.7% accuracy respectively.
                  The eye centre localisation algorithm has a modular approach, following a coarse-to-fine, global-to-regional scheme and utilising isophote and gradient features. A Selective Oriented Gradient filter has been specifically designed to detect and remove strong gradients from eyebrows, eye corners and self-shadows (which sabotage most eye centre localisation methods). The trajectories of the eye centres are then defined as gaze gestures for active HCI. The eye centre localisation algorithm has been compared with 10 other state-of-the-art algorithms with similar functionality and has outperformed them in terms of accuracy while maintaining excellent real-time performance.
                  The above methods have been employed for development of a data recovery system that can be employed for implementation of advanced assistive technology tools. The high accuracy, reliability and real-time performance achieved for attention monitoring, gaze gesture control and recovery of demographic data, can enable the advanced human-robot interaction that is needed for developing systems that can provide assistance with everyday actions, thereby improving the quality of life for the elderly and/or disabled.
               
            

@&#INTRODUCTION@&#

With the emergence of personal computing in the late 1970s, the concept of Human-Computer Interaction (HCI) was pushed rapidly and steadily into the environment where the individual role of science or engineering was not sufficient for addressing the urgent need for increasing the usability of computer software and operating systems (Grudin, 2011). The potential of increased accessibility to personal computers and demand for higher usability (Karray et al., 2008) of computer platforms called for the practical need for HCI and a synthesis of science and engineering. As personal computers present more digital information to people, the way people perceive, process and respond to such data has largely altered. HCI has therefore not only become central to information science theoretically and professionally, but also has stepped into peoples’ lives, offering multiple types of communication channels, i.e. modalities. These modalities gain their input via various types of sensors, mimicking human sensors including visual, audio and haptic sensors (Karray et al., 2008). They individually or combined give rise to a wide array of HCI systems. Among the various types of sensors, the visual sensor and the resulting visual based interaction modality are the most widespread, taking visual signals as inputs, which include but are not limited to images/videos of faces, bodies and hands (Jaimes and Sebe, 2007). The corresponding research spans the areas of gaze tracking, gait analysis, and recognition of face, gender, age, gesture and facial expression. While face recognition reveals an individual's identity, gender/age recognition aims to gather human demographics for the system to understand human characteristics. Facial expression recognition further estimates human affection states and gathers emotional cues while gaze tracking serves to extract users’ attentive information and to predict their intentions.

These modalities, independent or combined, have played different assistive roles in their corresponding HCI applications. As an example of gaze tracking, smart solutions are available that monitor the gaze direction of a driver in order to identify driver distraction/drowsiness and provide timely alerts (Tawari et al., 2014). These driver assistance systems, capable of detecting and acting on driver inattentiveness, are of great value to road safety. In the area of gesture recognition, a sterile browsing tool, `Gestix’, has been designed for doctor-computer interaction. This assistive technology provides doctors the sterility needed in an operation room where radiology images can be browsed in a contactless manner. A doctor's hand is tracked by a segmentation algorithm using colour model back-projection and motion cues from image frames (Wachs et al., 2008). Moreover, when expression recognition is concerned, an intelligent tutoring system, namely a Learning Companion, is proposed to predict when a learner might be frustrated, initiate interaction depending on the user's affective state and provide support accordingly (Kapoor et al., 2007).

With regard to the visual modality, we categorise these tasks as `demographic recognition’ (e.g. age and gender recognition) or `behavioural recognition’ (e.g. gaze analysis), according to the source and the utilisation of visual signals. In this paper, we propose a HCI strategy by combining demographic and behavioural recognition such that more natural, interactive and user-centred HCI environments can be created. More specifically, although demographic data reveal user characteristics and help define the ‘initial state’ and the `general theme’ of a HCI session, they cannot address the dynamic nature of a HCI session where user behaviours are constantly changing. Therefore, on the one hand, behavioural recognition reflects user attentions and intentions; on the other hand, it allows a user to issue commands and to actively interact with a HCI system through various means. As a result, the combination of demographic and behavioural recognition provides fused knowledge throughout a HCI session and better mimics a natural face-to-face interaction.

Although research in HCI has become increasingly active and sophisticated, a few issues remain unresolved that restrict most works from being transformed into assistive HCI systems that can benefit the daily life of human beings. We summarise the three major general issues that undermine the practicability of these works as follows:

                        
                           (1)
                           Lack of accuracy in real-world scenarios. Many research works are tested on controlled databases where ideal illuminations, high-resolution images and desirable viewpoint are available. When tested under various types of scenes with dynamic environmental factors, their performance will drop severely.

Undesirable real-time performance. As powerful as they might be, sophisticated algorithms often incur large computational cost, rendering them unsuitable for real-time implementation.

High dependence on expensive or inconvenient hardware configuration. The cost and the complexity of algorithm implementation will limit the usability and applicability of any method. Cheap yet effective methods are in high demand in order to boost assistive technologies.

In order to fill these gaps, in Section 3, we present an accurate gender recognition method that exhibits high accuracy and robustness under controlled and uncontrolled environments. This method puts Fisher Vectors at its core and encodes low-level local features (e.g. greyscale values, Local Binary Pattern (LBP), LBP histograms and the Scale Invariant Feature Transform (SIFT)) into more discriminative features for gender classification. State-of-the-art accuracy is achieved by only a linear Support Vector Machine (SVM), which further confirms the superiority of Fisher Vectors.

In Section 4, we propose a modular eye centre localisation method that makes use of isophote and gradient features extracted by its two modules. The first module performs an initial estimation of eye centre locations using isophote features from face images and filters eye centre candidates for the second module. The second module then updates the eye centre locations using only local gradient features. This coarse-to-fine and global-to-regional scheme ensures that this method is fast and accurate.

To further explore the localised eye centres, in Section 5, we introduce gaze gesture recognition, i.e. classification of the trajectory patterns of eye centre locations in consecutive frames. These gaze gestures provide contactless substitutes for mouse/keyboard input. Gaze gesture recognition therefore offers great assistance to the elderly and the disabled in accessing a variety of digital systems.

As our algorithms only require a standard webcam to capture image data, we implemented all the above mentioned algorithms and integrated them in a directed advertising system – which is one of the assistive technology tools our algorithms can bring to real-world applications. The directed advertising system combines the interpretation of demographic data (gender) and behavioural data (gaze) in order to deliver customised advertisements to its users and allow them to remotely browse advertising messages.

In summary, the main contribution of this paper is fourfold.

                        
                           (1)
                           A novel gender recognition method utilising the Fisher Vector encoding method – a generic method that can encode almost all types of features but still maintains low complexity in implementation. It also proves to have high accuracy and robustness against head poses. To the best of our knowledge, this is the first time that Fisher Vectors have been employed for gender recognition.

A modular eye centre localisation method consisting of two modules and a Selective Oriented Gradient (SOG) filter. The eye centre localisation method has proved to be accurate, fast and, more importantly, robust to in-plane and out-of-plane head rotations. The SOG filter is specifically designed to detect and remove strong gradients from eyebrows, eye corners and self-shadows that many other methods suffer. The SOG filter also resolves general tasks regarding the detection of curved shapes.

Design of gaze gestures and a gaze gesture recognition algorithm. The algorithm captures the relative attention of the users and enables them to control HCI systems by issuing gaze gestures. This will largely enhance the interactivity of current HCI systems.

Development of an intelligent and assistive case study system. By combining demographic data and behavioural data, this system makes digital out-of-home advertising more adaptive and interactive so that it is of great value to both advertising agencies and consumers. This system also provides enabling technology for assisting diverse groups of people (including the elderly and the disabled), with accessing HCI systems with ease and convenience, in a wide range of applications.

@&#RELATED WORK@&#

A number of assistive technologies that utilise vision modalities have been introduced in the preceding section. In this section, we explore the combination of gender recognition and eye/gaze analysis and review a number of related state-of-the-art methods. We provide a summary of the current research state for gender recognition, eye centre localisation and gaze/gaze gesture analysis; and we discuss their potential benefits for assistive HCI applications, and identify the limitations that will critically hinder advancement in this area.

Gender recognition from facial images, i.e. gender classification, is a challenging task in that a face exhibits a wide range of intra-class variations due to facial attributes or environmental factors. The former complications mainly include age, ethnicity and makeup while the latter include illumination condition, head pose, facial occlusion and camera quality.

Most high-performance gender recognition methods involve machine learning and follow four stages: face detection, facial image pre-processing, feature extraction and classification (Ng et al., 2012).

For the face detection stage, the Viola-Jones face detector (Viola and Jones, 2004) has been widely adopted due to its ease of implementation and relatively high accuracy. It is essentially a face detector that employs Haar-like features, a classifier learning with AdaBoost and a cascade structure (Viola and Jones, 2001). It can operate in real time and has allowed many practical applications to boom in the last decade.

For the image pre-processing stage, normalisation, i.e. contrast and brightness adjustment; image resizing and face alignment are commonly considered useful despite their varied implementation details. Among them, face alignment has been reported to be able to guarantee an increase in the classification accuracy by a number of studies. For example, in a research (Mäkinen and R., 2008) evaluating a number of gender classification methods, it is concluded that Support Vector Machine (SVM) outperformed other classification methods with 86.54% accuracy on 36×36 aligned images, and that higher accuracy could be achieved by improving the implementation of the automatic alignment methods. Another research (Mäkinen and Raisamo, 2008) with regard to gender classification illustrated that face alignment brought an increase to the classification accuracy for various methods including use of neural networks, SVM, and Adaboost. Different pre-processing methods are experimented with in our method with the results reported in Section 3.

For the feature extraction and selection stage, a wide range of features are experimented with and evaluated in the literature. They include intensity values from greyscale images (Moghaddam and Yang, 2000), LBP (Shan, 2012, Ullah et al., 2012), facial strips (Lee et al., 2010), Haar-like features (Viola and Jones, 2001), SIFT features (Wang et al., 2010), etc. These features can be extracted globally from complete face images or locally from defined sub-regions of the face images.

For the classification stage, SVMs and neural networks have been the most popular classifiers. SVMs with different kernels were investigated in (Moghaddam and Yang, 2000) and convolutional neural networks were adopted by (Tivive and Bouzerdoum, 2006) and (Phung and Bouzerdoum, 2007) as gender classifiers.

Following the four major stages, a number of approaches have reported relatively high classification rate on publicly available datasets. Some representative works on gender classification are reviewed here, with a detailed comparison to the proposed method in Section 3.

A decision-fusion based method is presented by (Alexandre, 2010) that utilises multiple SVMs to classify intensity values, local binary patterns and histogram of edge directions as features. The three types of features are extracted from images of various sizes. Finally all classification results are integrated to make the final decision by means of majority voting, leading to 99.07% accuracy. However this result is obtained from a small subset of the FERET database and their validation method is not sophisticated enough to reflect the performance of their approach objectively. In addition, only controlled databases are used for training and testing in this research so that the applicability of this approach to dynamic environments remains unevaluated. Similarly, another study (Lee et al., 2010) employs 10 regression functions to conduct region-based classifications and feed the vector of classification results into an SVM to generate the final decision. Despite the 98.8% accuracy with the FERET database they reported, they did not illustrate their evaluation method and the split of training and testing data. The reappearance of the same subject in both the training and testing data may account for the high accuracy they obtained. A face alignment scheme is compulsory to their approach, the absence of which leads to a 6% drop in the classification rate, bringing 98.8% down to 92.8%. This is an inherent limitation of conventional region-based approaches where defined facial regions have to be perfectly aligned. A fusion-based method (Hu et al., 2010) proposed to integrate different facial regions for gender recognition using the `matcher weighing fusion’ method. The facial landmarks for segmenting the face into its sub-regions are detected by a profile-based method and a curvature based method. Interestingly they prove experimentally that the fusion of multiple facial sub-regions is superior to the complete face region alone and that the upper face contains more discrimination ability regarding gender classification. Apart from classification fusion, feature fusion provides an alternative way to boost gender classification rates. In (Wang and Kambhamettu, 2013), two types of appearance features, i.e. the LBP features and the shape index features, were fused to characterise facial textures and shapes. The resulting classification rate on the FRGCv2 dataset was up to 93.7%.

With individual works reviewed, we draw conclusions from the literature regarding the preferences in gender classification. 1) SVM and neural networks are the most popular classifiers. 2) LBP and its variations are the most popular features. 3) Most studies use the FERET database as the standard evaluation database. 4) Most works are carried out under a well-controlled environment while real-world implementation and evaluation lack exploitation. 5) Most works incorporate face alignment in the pre-processing stage. 6) Most works employ the 5-fold cross validation for accuracy estimation.

In most gender classification studies, limitations and complications are seen as both intrinsic factors and extrinsic factors. The former type is mainly the consequence of large amount of variation in facial appearance due to aging, makeup, facial occlusion, ethnicity and accessories. The latter type is largely due to environmental variations such as camera viewpoint (head pose), illumination condition, etc. Variations incurred by undesirable illumination conditions in particular are difficult to address by employment of powerful classifiers, but rather should be tackled by seeking for more robust and reliable facial features. This has defined the trend for gender recognition researches – the exploitation of 3D features that are independent of lighting conditions. 3D features have been employed individually (Hu et al., 2010, Fagertun et al., 2012) or fused (Wang and Kambhamettu, 2013, Huynh et al., 2012) with 2D features to better characterise face shapes and textures. This will be reflected in our future works aiming to extend the proposed gender recognition algorithm by incorporating 3D features.

Eye/gaze analysis is receiving an increasing amount of attention for implementing HCI by utilising the visual modality. Compared to gender and age recognition, it reveals more personal information by estimating the attention and intention of an individual. With eye/gaze analysis, a HCI system can not only observe its user passively, but it allows its user to take control of the system actively with eye movement. Therefore eye/gaze analysis excels in remote and contactless interaction and provides an ideal channel for elderly people and those with motor disabilities to access HCI systems.

According to the features extracted, eye centre localisation methods fall into two main categories: inherent feature based methods and additive feature based methods. An additive feature based method actively projects infrared illumination toward the eyes that result in reflections on the corneas, which are referred to as `glints’ in the literature (Zhu and Ji, 2005). Being highly reliant on dedicated devices, this method essentially alters the primary task of eye centre detection into corneal reflection detection as a simplified detection task. A passive inherent feature based method is more generalizable since it employs characteristic features from the eye region itself and therefore becomes the method we explore in this paper. It can be further divided into 1) eye geometry or morphology based methods that utilise gradient, isophote or curvature features to estimate the eye centre that comply with geometrical or morphological constraints, 2) model and machine learning based methods where distinct features are extracted to train a model to search for the eye region that best matches the model representation, and 3) hybrid methods which normally follow a multi-stage scheme that comprises the previously summarised. While several methods have achieved interesting results, they have also exhibited their respective limitations.

One geometrical feature based method (Timm and Barth, 2011) localises eye centres by means of gradients. In this approach, the iris centre obtains the maximised value in the objective function that peaks at the centre of a circular object. Its performance declines in the presence of strong gradients from eyelids, eyebrows, shadows and occluded pupils that overshadow iris contours. Another unsupervised method employing geometrical features that was investigated is Self-Similarity Space. Here image regions that can maintain peculiar characteristics under geometric transformations receive high self-similarity scores (Leo et al., 2014).

Regarding model and machine learning based methods, for all algorithms that utilise extracted features to train a model, it holds that the training data are of critical influence on the performance of the algorithms (Zhu and Ramanan, 2012). More specifically, variations posed by illumination and head rotation have a huge impact on the accuracy and robustness of the algorithm for most types of features. Inspired by Fisher Linear Discriminant (FLD) (Duda et al., 2012), (Kroon et al., 2008) designed a linear filter trained by the image patches extracted from normalized face images. This method not only considers the high response from the filtered image, but also examines a rectangular neighbourhood around the estimated eye centre positions. This is based on the observation that a pupil in an image is formed by a collection of dark pixels within a small region. Another machine learning based method (Niu et al., 2006) focuses on the design of a novel classifier rather than the extraction of representative features. This method introduces a 2D cascade AdaBoost classifier that combines bootstrapping positive samples and bootstrapping negative samples (Viola and Jones, 2001). The final localisation of an eye can be achieved either by weighting all classifier results for high precision or by cascading all classifiers (i.e. only adopting the result from the first classifier that detects an eye window) for increased efficiency. In addition, a number of studies are only effective for frontal faces. For example, (Asadifard and Shanbezadeh, 2010) employed a cumulative distributed function (CDF) for adaptive centre of pupil detection on frontal face images. Their approach firstly extracts the top-left and top-right quarters of a face image as the regions of interest and then filters each region of interest with a CDF. An absolute threshold is defined for the filtering process given the fact that the pixels in the pupil region are darker than the rest of the eye region. Another study on frontal faces (Türkan et al., 2007) explored edge projections for eye localisation. With a face image available, their method firstly defines a rough horizontal position for the eye region according to facial anthropometric relations. After the eye band is cropped, it gathers eye candidate points that are extracted by a high-pass filter of a wavelet transform. A Support Vector Machine (SVM) based classifier (Chang and Lin, 2011) is then used to estimate the probability value for every eye candidate. This type of method normally requires that all face images are perfectly aligned so that the facial geometry agrees with facial anthropometric relations as the prior knowledge.

Although recent studies have shown promising results in accurately localising the eye centres, the estimation error increases at relatively long distances and is also affected by shadows and specularities. (Drewes et al., 2007) carried out a study on eye-gaze interaction for mobile phone use following two methods, the standard dwell-time based method and the gaze gesture method. This study concludes that gaze gesture is robust to head movement since it only captures relative eye movement rather than absolute eye fixation points. Calibration is also unnecessary and this therefore makes eye gesture more suitable for real-world applications. The two interaction methods are further compared by (Hyrskykari et al., 2012) which suggested that ``gaze gestures are not only a feasible means of issuing commands in the course of game play, but they also exhibited performance that was at least as good as or better than dwell selections”. Another study (Rozado et al., 2012) achieved gaze gesture recognition for HCI under more general circumstances. It employs the hierarchical temporal memory pattern recognition algorithm to recognise predefined gaze gesture patterns. 98% accuracy is achieved for 10 different intentional gaze gesture patterns. Some other works on gaze gestures dedicated to HCI have similar limitations. Firstly, they all depend on active NIR lighting for eye centre localisation. Secondly, the eye centre localisation algorithms work at only relatively short distance.

As seen from the literature, most methods regarding gender recognition and gaze analysis lack robustness and suffer from various limitations in real-world scenarios. To bridge these gaps, we explore novel methods that can assist HCI implementations robustly and efficiently while maintaining high accuracy. In this section, we introduce a gender recognition method that utilises Fisher Vectors as discriminative features.

A Fisher Vector (FV) is an encoded vector that applies Fisher kernels on visual vocabularies where the visual words are represented by means of a Gaussian Mixture Model (GMM). The Fisher kernel function is derived from a generative probability model, and provides a generic mechanism that combines the advantages of generative and discriminative approaches.

As a core component of a FV, a GMM is a parametric probability density function represented as a weighted sum of Gaussian component densities as given by Eq. (1) (Reynolds, 2009)

                           
                              (1)
                              
                                 
                                    p
                                    
                                       (
                                       
                                          x
                                          |
                                          λ
                                       
                                       )
                                    
                                    =
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       B
                                    
                                    
                                       ω
                                       i
                                    
                                    
                                    N
                                    
                                       (
                                       x
                                       |
                                       
                                          
                                             μ
                                          
                                          i
                                       
                                       ,
                                       
                                          
                                             σ
                                          
                                          i
                                       
                                       )
                                    
                                 
                              
                           
                        where 
                           x
                         is a L-dimensional data vector, 
                           
                              λ
                              =
                              {
                              
                                 
                                    ω
                                    i
                                 
                                 ,
                                 
                                 
                                    
                                       μ
                                    
                                    i
                                 
                                 ,
                                 
                                 
                                    
                                       σ
                                    
                                    i
                                 
                                 ,
                                 
                                 i
                                 =
                                 1
                                 ,
                                 2
                                 ,
                                 …
                                 ,
                                 B
                              
                              }
                           
                         is the collective representation of the GMM parameters – ωi
                         the mixture weights, 
                           μ
                        
                        
                           
                              i
                           
                         the mean vector and 
                           
                              
                                 
                              
                              
                                 
                                    σ
                                 
                                 i
                              
                           
                         the covariance matrix. B is the number of Gaussians. The component 
                           
                              N
                              (
                              x
                              |
                              
                                 
                                    μ
                                 
                                 i
                              
                              ,
                              
                              
                                 
                                    σ
                                 
                                 i
                              
                              )
                           
                         is further described in Eq. (2).

                           
                              (2)
                              
                                 
                                    N
                                    
                                       (
                                       
                                          
                                             x
                                             |
                                          
                                          
                                             
                                                μ
                                             
                                             i
                                          
                                          ,
                                          
                                          
                                             
                                                σ
                                             
                                             i
                                          
                                       
                                       )
                                    
                                    =
                                    
                                    
                                       
                                          e
                                          
                                             {
                                             
                                                −
                                                
                                                   1
                                                   2
                                                
                                                
                                                   
                                                      (
                                                      
                                                         x
                                                         −
                                                         
                                                            μ
                                                            i
                                                         
                                                      
                                                      )
                                                   
                                                   ′
                                                
                                                
                                                   
                                                      
                                                         σ
                                                         i
                                                      
                                                   
                                                   
                                                      −
                                                      1
                                                   
                                                
                                                
                                                   (
                                                   
                                                      x
                                                      −
                                                      
                                                         μ
                                                         i
                                                      
                                                   
                                                   )
                                                
                                             
                                             }
                                          
                                       
                                       
                                          
                                             
                                                
                                                   (
                                                   
                                                      2
                                                      π
                                                   
                                                   )
                                                
                                             
                                             
                                                L
                                                /
                                                2
                                             
                                          
                                          
                                             
                                                
                                                   |
                                                   
                                                      
                                                         σ
                                                      
                                                      i
                                                   
                                                   |
                                                
                                             
                                             
                                                1
                                                /
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The mixture weights are subject to the constraint in Eq. (3).

                           
                              (3)
                              
                                 
                                    
                                       ∑
                                       1
                                       B
                                    
                                    
                                       ω
                                       i
                                    
                                    =
                                    1
                                 
                              
                           
                        
                     

The covariance matrices are assumed to be diagonal since any distribution can be decomposed into a number of weighted Gaussians with diagonal covariances.

Let 
                           
                              X
                              =
                              {
                              
                                 
                                    X
                                    t
                                 
                                 ,
                                 
                                 t
                                 =
                                 1
                                 ,
                                 2
                                 ,
                                 …
                                 ,
                                 T
                              
                              }
                           
                         be the set of descriptors of low-level features extracted from an image, and it is assumed that all the descriptors are independent. Eq. (4) can be found:

                           
                              (4)
                              
                                 
                                    log
                                    p
                                    
                                       (
                                       
                                          X
                                          |
                                          λ
                                       
                                       )
                                    
                                    =
                                    
                                    
                                       ∑
                                       1
                                       T
                                    
                                    log
                                    p
                                    
                                       (
                                       
                                          X
                                          t
                                       
                                       |
                                       λ
                                       )
                                    
                                 
                              
                           
                        
                     

The descriptors 
                           X
                         can be described by the gradient vector:

                           
                              (5)
                              
                                 
                                    
                                       ψ
                                       λ
                                       X
                                    
                                    =
                                    
                                    
                                       
                                          
                                             ∇
                                             λ
                                          
                                          
                                          l
                                          o
                                          g
                                          p
                                          
                                             (
                                             
                                                X
                                                |
                                                λ
                                             
                                             )
                                          
                                       
                                       T
                                    
                                 
                              
                           
                        
                     

A natural kernel on these gradients is:

                           
                              (6)
                              
                                 
                                    ϰ
                                    
                                       (
                                       
                                          X
                                          ,
                                          Y
                                       
                                       )
                                    
                                    =
                                    
                                    
                                    ψ
                                    
                                       
                                          
                                          λ
                                          X
                                       
                                       
                                          
                                          ′
                                       
                                    
                                    
                                    
                                       F
                                       λ
                                       
                                          −
                                          1
                                       
                                    
                                    
                                    
                                       ψ
                                       λ
                                       Y
                                    
                                 
                              
                           
                        where 
                           
                              
                                 F
                                 λ
                              
                              =
                              
                              
                                 
                                    
                                       L
                                       λ
                                    
                                 
                                 ′
                              
                              
                                 L
                                 λ
                              
                              
                           
                        is the Fisher information matrix and 
                           
                              
                                 ψ
                                 λ
                                 X
                              
                              =
                              
                              
                                 L
                                 λ
                              
                              
                              
                                 ψ
                                 λ
                                 X
                              
                           
                         is referred to as the Fisher Vector of 
                           X
                        .

Let γt
                        (i) denotes the soft assignment of descriptor 
                           X
                        
                        
                           
                              t
                           
                         to the Gaussian component i:

                           
                              (7)
                              
                                 
                                    
                                       γ
                                       t
                                    
                                    
                                       (
                                       i
                                       )
                                    
                                    =
                                    p
                                    
                                       (
                                       
                                          
                                             i
                                             |
                                          
                                          
                                             x
                                             t
                                          
                                          ,
                                          λ
                                       
                                       )
                                    
                                    =
                                    
                                    
                                       
                                          
                                             ω
                                             i
                                          
                                          N
                                          
                                             (
                                             
                                                
                                                   x
                                                   t
                                                
                                                
                                                   |
                                                
                                                
                                                   
                                                      μ
                                                   
                                                   i
                                                
                                                ,
                                                
                                                
                                                   
                                                      σ
                                                   
                                                   i
                                                
                                             
                                             )
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             B
                                          
                                          
                                             ω
                                             j
                                          
                                          N
                                          
                                             (
                                             
                                                
                                                   x
                                                   t
                                                
                                                
                                                   |
                                                
                                                
                                                   
                                                      μ
                                                   
                                                   j
                                                
                                                ,
                                                
                                                
                                                   
                                                      σ
                                                   
                                                   j
                                                
                                             
                                             )
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The gradients of Gaussian component i with respect to the mean 
                           μ
                        
                        
                           i
                         and the covariance 
                           σ
                        
                        
                           i
                         respectively are:

                           
                              (8)
                              
                                 
                                    
                                       Ψ
                                       
                                          μ
                                          ,
                                          
                                          i
                                       
                                       X
                                    
                                    =
                                    
                                    
                                       1
                                       
                                          T
                                          
                                             
                                                ω
                                                i
                                             
                                          
                                       
                                    
                                    
                                       ∑
                                       1
                                       T
                                    
                                    
                                       γ
                                       t
                                    
                                    
                                       (
                                       i
                                       )
                                    
                                    
                                       (
                                       
                                          
                                             
                                                x
                                                t
                                             
                                             −
                                             
                                                
                                                   μ
                                                
                                                i
                                             
                                          
                                          
                                             
                                                σ
                                             
                                             i
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        
                        
                           
                              (9)
                              
                                 
                                    
                                       Ψ
                                       
                                          σ
                                          ,
                                          
                                          i
                                       
                                       X
                                    
                                    =
                                    
                                    
                                       1
                                       
                                          T
                                          
                                             
                                                2
                                                
                                                   ω
                                                   i
                                                
                                             
                                          
                                       
                                    
                                    
                                       ∑
                                       1
                                       T
                                    
                                    
                                       γ
                                       t
                                    
                                    
                                       (
                                       i
                                       )
                                    
                                    
                                       [
                                       
                                          
                                             
                                                
                                                   (
                                                   
                                                      x
                                                      t
                                                   
                                                   −
                                                   
                                                      
                                                         μ
                                                      
                                                      i
                                                   
                                                   )
                                                
                                                2
                                             
                                             
                                                
                                                   
                                                      
                                                         σ
                                                      
                                                      i
                                                   
                                                
                                                2
                                             
                                          
                                          −
                                          1
                                       
                                       ]
                                    
                                 
                              
                           
                        
                     

Finally a FV is represented as:

                           
                              (10)
                              
                                 
                                    Φ
                                    =
                                    
                                       {
                                       
                                          
                                             Ψ
                                             
                                                μ
                                                ,
                                                
                                                1
                                             
                                             X
                                          
                                          ,
                                          
                                          
                                             Ψ
                                             
                                                σ
                                                ,
                                                
                                                1
                                             
                                             X
                                          
                                          ,
                                          
                                          …
                                          ,
                                          
                                          
                                             Ψ
                                             
                                                μ
                                                ,
                                                
                                                N
                                             
                                             X
                                          
                                          ,
                                          
                                          
                                             Ψ
                                             
                                                σ
                                                ,
                                                
                                                N
                                             
                                             X
                                          
                                       
                                       }
                                    
                                 
                              
                           
                        
                     

Therefore, when images from a certain database produce a large number of feature descriptors, a GMM can be trained by them using the Maximum Likelihood (ML) estimation. The difference between every individual descriptor and every Gaussian distribution is captured by Eq. (8) and (9), which is further stacked into a FV by Eq. (10). Following this manner, a FV can be computed for every descriptor, which encodes the difference between a single descriptor and all descriptors described by the GMM. This means that the difference between an image and the entire training dataset is preserved. As a result, a FV is provided with contextual definition and enhanced saliency for classification.

Fisher Vectors have been used for face recognition and have proved to be an excellent encoding method (Simonyan et al., 2013). The FV encoding approach consists of five main stages: 1) face pre-processing, 2) low-level feature and face descriptor computation, 3) dimensionality reduction/feature selection, 4) FV encoding and 5) classifier training. In more detail, we illustrate the five primary stages adopted by our method as follows.

                           
                              (1)
                              Face pre-processing.

The techniques experimented at this stage include face detection, image resizing, histogram equalisation and face alignment. In the experiment, the Viola-Jones face detector was used to obtain the face region in the first place. As well as aiming to achieve high recognition accuracy, this study intends to investigate the most discriminative facial parts, i.e. regions on a face that can best characterise and differentiate male and female groups. To this end, we reshaped the face regions obtained by the face detector so that they could incorporate the hair region and the chin. Practically, this was managed by excluding the background margins from the width by a ratio of rw and expanding the length by a ratio of rl. Let the side of a square region detected by the face detector be fs, the length of a reshaped face region be fl and the width be fw. A reshaped face region can be defined as:
                           
                              (11)
                              
                                 
                                    {
                                    
                                       
                                          
                                             
                                                f
                                                w
                                                =
                                                f
                                                s
                                                ×
                                                
                                                   (
                                                   
                                                      1
                                                      −
                                                      r
                                                      w
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                f
                                                l
                                                =
                                                f
                                                w
                                                ×
                                                
                                                   (
                                                   
                                                      1
                                                      +
                                                      r
                                                      l
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

where
                           
                              
                              r
                              w
                              
                              =
                              
                              0.12
                              
                           
                        and
                           
                              
                              r
                              l
                              
                              =
                              
                              0.33
                           
                         in our experiments. This ensures that the reshaped face region has a uniform aspect ratio (i.e. 3:4), while keeping its size relative to that originally detected by the face detector. Facial regions were further resized to the same size in the next step with or without histogram equalisation. Face alignment was also experimented with and its impact was investigated.

An optional step at this stage is face alignment, which is intended to compensate for different head poses that are likely to sabotage most gender recognition algorithms. To evaluate the robustness of our method against different head poses, we conducted experiments (see Section 3.3) with or without face alignments. Eye centre coordinates obtained by the proposed eye centre localisation method (introduced in Section 4) were employed as facial landmarks for the alignment. However, eye centres accurately localised by other methods or other types of facial landmarks (e.g. eye corners and nose tip) should also suffice to perform the alignment. Fig. 1
                         further illustrates the face pre-processing stage with an example.


                        
                           
                              (2)
                              Low-level feature and face descriptor computation.

Conventionally, one face image produces only one feature vector, namely a descriptor (e.g. the LBP features) or a few descriptors around keypoints (e.g. the SIFT features). In both cases, feature descriptors are sparsely extracted. Different from both methods, we extract dense descriptors at every pixel location. Firstly, we segment a face image into a number of overlapping patches of the same size. Specifically, these patches are obtained by sliding a r × r window across an image horizontally and vertically with a predefined stride
                           
                              
                              s
                              
                              (
                              
                                 s
                                 ∈
                                 Z
                              
                              )
                           
                        . One descriptor per patch rather than one descriptor per image is calculated. The geometry of the patch-based descriptors is shown in Fig. 2
                        . For example, vector (pxc
                        , pyc
                        ) records the centre position of the ath
                         (
                           
                              a
                              ∈
                              N
                              ,
                              
                              a
                              ≤
                              
                              p
                              n
                           
                        ) patch in the image. The centre position of the first patch is therefore (r/2, r/2). For an m × n . image, the total number of patches is:
                           
                              (12)
                              
                                 
                                    p
                                    n
                                    =
                                    
                                    
                                       
                                          m
                                          −
                                          r
                                          +
                                          1
                                       
                                       s
                                    
                                    ×
                                    
                                       
                                          n
                                          −
                                          r
                                          +
                                          1
                                       
                                       s
                                    
                                    ,
                                    
                                    
                                    
                                    
                                    
                                       
                                          
                                             
                                                r
                                                <
                                                min
                                                
                                                   (
                                                   
                                                      m
                                                      ,
                                                      n
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                1
                                                ≤
                                                s
                                                ≤
                                                min
                                                
                                                   (
                                                   
                                                      m
                                                      ,
                                                      n
                                                   
                                                   )
                                                
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Although a number of pixels at the margins of an image cannot be the centres of a sliding window, they are incorporated by at least one image patch and are therefore involved in the formation of image descriptors. Low-level dense features can be visualised by mapping the top three principal components to the RGB space. A similar process can be found in (Liu et al., 2011). As an example, dense SIFT features extracted with different parameters can be seen in Fig. 3.
                        
                     

The visualisation implies that facial structures can be captured by densely sampled local features. It should be noted that this visualisation only reflects the top three principal components of the SIFT vectors which originally had 128 dimensions. Visualisation of low-level dense features under a particular colour space is only a feature-to-colour mapping process and therefore has no impact on the actual feature used for gender recognition.


                        
                           
                              (3)
                              Dimension reduction and feature selection

As one image produces multiple descriptors, linearly increasing the total number of observations as opposed to conventional methods, dimension reduction is essential in that it cuts down memory consumption and that it potentially compresses raw features into more discriminative presentations.

In our experiments, Principal Component Analysis (PCA) was implemented to reduce the dimensionality to 64 features per descriptor. As Fig. 3 demonstrates that the top 3 principal components of SIFT features can capture the main facial structure, 64 principal components should be able to reflect sharp edges and fine facial structures. By reducing the dimensionality of facial descriptors, highly correlated information that does not contributed to discriminability could be removed. In order that a descriptor can be located on the original image after being encoded into a FV, the centre position of a patch was appended to the end of a corresponding compressed descriptor in the form of a 2D vector (Simonyan et al., 2013), increasing the dimensionality to 66 from 64. This 2D vector is rescaled to [–0.5, 0.5] so that it would have minimal effect compared to the other 64 features. As a result, although spatial information is embedded into feature vectors, the overall features are still relatively independent of global facial geometry and are therefore robust to head pose variation.


                        
                           
                              (4)
                              FV encoding

The aggregate of all descriptors from training images trains a GMM and yields model parameters which, according to Eq. (8) – (10), lead to FVs as encoded features. In the experiments, we utilised a publicly available toolbox (Vedaldi and Fulkerson, 2010) for GMM training and SIFT feature extraction. Among GMM parameters, different numbers of Gaussian components were experimented with, amongst which 512 was deemed a suitable value (see results in Section 3.3 for more details). It can be calculated from Eq. (10) that the dimension of a FV is
                           
                              
                              2
                              ×
                              B
                              ×
                              L
                              =
                              2
                              ×
                              512
                              ×
                              66
                              =
                              67584
                           
                        . In this stage, decomposed patches are reunited to characterise a complete image, in the form of derivatives of all Gaussian components. In the computation of FVs, ℓ2 normalisation and power normalisation are applied to the vectors since they were reported to improve classification performance (Perronnin et al., 2010).


                        
                           
                              (5)
                              Classifier training

Our algorithm learns a SVM classifier from all training FVs. In our experiments (see Section 3.3), a linear SVM and a SVM with a Radial Basis Function (RBF) kernel were both tested. By comparing their respective classification rate, we show that the FV encoding method only requires a linear SVM to function accurately and robustly. The employment of a SVM classifier has a twofold purpose. Firstly, it naturally fulfils the classification task by defining a hyper-plane, with or without a kernel. Its output gives the predicted labels for all testing images. Secondly, when a linear SVM is concerned, it reveals the discriminative power of each variable/feature in FVs. From the perspective of a SVM classifier, a SVM learns a hyper-plane that separates two classes with maximum margin. As the hyper-plane is defined by a decision hyper-plane normal vector
                           
                              
                              
                                 
                                    w
                                    →
                                 
                              
                              
                           
                        that is perpendicular to the hyper-plane, as well as an intercept term b, the absolute values of the elements in 
                           
                              
                                 w
                                 →
                              
                           
                         imply the significance of the corresponding elements in the FVs. From the perspective of metric learning, the diagonal linear transformation matrix 
                           W
                         to be learnt has its diagonal values as in 
                           
                              
                                 w
                                 →
                              
                           
                        . It can be considered as projecting the original data so that they are located on each side of a fixed hyper-plane, different from the former perspective where the data are fixed and the hyper-plane is unknown (Do et al., 2012). Both methods state that 
                           
                              
                                 w
                                 →
                              
                           
                        , also known as the weight vector, reflects the discriminative power of individual features.

From the manner in which a FV is constructed (Eq. (10), a mapping can be obtained between the features and the Gaussian components. Therefore the relationship between the discriminative power and each Gaussian component can be established. When visualised with regard to its spatial location, a Gaussian component can be used to signify the discriminability of a corresponding facial region.

In order to evaluate the performance of our gender recognition algorithm under controlled environments and real-world conditions respectively, the Grey FERET database (Phillips et al., 1998) (referred to as the FERET database in the rest of the paper), the Labelled Face in the Wild (LFW) database (Huang et al., 2007) and the FRGCv2 database (Phillips et al., 2005) are employed.

The FERET database consists of 14051 greyscale images of frontal and profile face images. It is further divided into several partitions where the fa and fb partitions contain near-frontal facial images. Since the two partitions significantly overlap, only the fa partition of 1152 male patterns and 610 female patterns were employed. Although captured under controlled environment, the FERET database still poses great challenge as it accommodates different ethnicities, facial expressions, facial accessories, facial makeup and illumination conditions. The LFW database is considered one of the most challenging databases and has become the evaluation benchmark for face recognition under unconstrained environment. The 13233 colour facial images of 5749 subjects collected from the web include all types of variation and interference (illumination, head poses, occlusion, image blur, chromatic distortion, etc.), and come in inconsistent image quality. Only one image per subject (the first image) was used in the experiment so that the same subject could not appear in both the training set and the testing set. The FRGCv2 database includes 4007 depth images belonging to 466 subjects. These data also include different ethnicities and age groups. Some sample images from the three databases are shown in Fig. 4.
                        
                     

The 5-fold cross validation technique was adopted for evaluation. More specifically, the database was partitioned into five splits of similar size, four of which were used for training in each repetition with the remaining 1 split for testing. After 5 repetitions, the average classification rate was calculated as the final result.

In the first group of experiments, different types of features are explored including the greyscale values, the SIFT features, the LBP features (extracted using the circularly symmetric neighbour sets (Ojala et al., 2000) with 8 neighbouring pixels and radius of 1), and LBP histogram with uniform pattern. Evaluated on the FERET database, their respective performances are summarised in Fig. 5.
                        
                     

The size of facial images is 160 × 120, the sampling stride is four pixels, the window size for local patches is 24 × 24 and the number of Gaussian components in the GMM is 512. The classification results suggest that dense SIFT features perform the best. Therefore this feature type was selected for a further inspection regarding a number of parameters. Note that our dense SIFT features were only extracted at one scale since we did not observe higher accuracy when multiple scales were used. In addition, we only employed a linear SVM since the RBF kernel in the experiments did not contribute to higher classification rate.

In the second group of experiments, parameters inspected concerned image size, size of the sliding window, sampling stride and Gaussian number. One of these parameters was altered at a time while the others remained fixed as in the last group of experiments (if not otherwise specified). Misaligned and non-normalised face images were used in this group of experiments.


                        
                           
                              (1)
                              Image size

The maximum size of selected face regions from the FERET database is 160 × 120. With the aspect ratio fixed, each face region was resized to between 30% and 90% of the maximum size, with an interval of 10%. Fig. 6
                        (a) reflects the declined performance as the size decreases. The results suggest that larger images produce higher classification rates. This coincides with the intuition that more details reside in larger images which should generate more effective features for classification.


                        
                           
                              (2)
                              Sampling stride for local patches

Increasing the sampling stride in extracting local descriptors is comparable to decreasing the sampling rate in sub-sampling an image. Fig. 6(b) summarises the impact of sampling stride. To avoid high memory usage, we firstly employed all the facial images resized to 96 × 72. Strides from 2 to 7 were experimented with in this setting. We then employed the first 500 male and female images (1000 in total) and further resized them to 80 × 60, in order that experiments with stride of 1 could be conducted. It can be seen from Fig. 6(b) that although a smaller stride tends to produces a higher classification rate in general, it will only have a dramatic influence when it goes beyond four pixels. Therefore, a stride of 3 or 4 can be deemed an appropriate value without incurring high memory usage at the training stage.


                        
                           
                              (3)
                              Window size of local patches

Our algorithm is appearance-based and densely extracts descriptors at local level. Broadly speaking, a larger window implicitly maintains more global and geometric information. An extreme case concerns a window as large as the entire image which removes the advantage of using local features. Hence there is a critical need for the window to be defined with an appropriate size that allows sufficient tolerance to global variations. The window size ranged from 12 × 12 to 40 × 40 pixels in the experiments, with an interval of four pixels. The respective classification rate is summarised in Fig. 6(c). It can be concluded that a window 15% to 20% the size of the entire image is optimal.


                        
                           
                              (4)
                              GMM component number

Fitting a generative model, the GMM, to the features is a key step in this method. Fig. 6(d) shows that the classification rate fluctuates as the number of Gaussians changes. The size of the facial images used was 128 × 96 (80% of the maximum size). The result agrees with the statement (Sánchez et al., 2012) that an appropriate number of Gaussian components is needed since too many components result in very few `per Gaussian’ statistics that are pooled together, i.e. sparse Gaussian representation. Too few Gaussians, on the other hand, are not sufficient enough to capture the uniqueness and reflect the separability of local descriptors.


                        
                           
                              (5)
                              Face alignment, histogram equalisation and root SIFT

Face alignment has been reported in the literature to have a substantial impact on classification rates. Without alignment, one may experience a drop in classification rates as much as 6% (Lee et al., 2010). In addition, applying histogram equalisation is a common pre-processing procedure that may increase classification rate. When the SIFT features are concerned, a variation of the SIFT, the root SIFT is recommended by (Arandjelovic and Zisserman, 2012), which uses the Hellinger kernel instead of the standard Euclidean distance to measure the similarity between SIFT descriptors. The impacts of the three factors were explored in this experiment, summarised in Fig. 7.
                        
                     

It should be noted that each time only one type of adjustment was made so that their impacts could be evaluated independently from other factors. As seen from Fig. 7, only face alignment plays a positive role while the other two types of adjustment decrease the classification rate. It can be claimed that the proposed algorithm is relatively robust against misalignment which caused only a 0.6% drop in the classification rate. In addition, the impact of PCA was also explored by reducing the original facial descriptors to dimensionality of 32, 64 and 96, respectively. When the first 64 principal components are employed, the highest classification rate (97.7%) can be achieved, in comparison to 96.4%, 97.2% and 95.7% for 32, 96 and 128 principal components, respectively.

As SIFT features yielded the highest classification accuracy, it was further tested on the LFW database (real data) and the FRGCv2 database (3D data). The optimal parameters identified by the previous experiments were employed in an attempt to achieve the highest classification result, except that the facial images were resized to 112 × 84 for the LFW database and 240 × 180 for the FRGCv2 database. The classification rates for aligned and misaligned images in the LFW database are 92.5% and 92.3%, respectively; the classification rate for misaligned images in the FRGCv2 database is 96.7%.

The endeavour to align faces in the LFW database sees very limited benefit (0.2% increase) and is therefore not worthwhile, since in this case and in general cases, it incurs large amount of computation. The reason behind the reduced improvement brought by face alignment on this database, compared to the 0.6% increase on the FERET database, is possibly the large extent of out-of-plane head rotation that cannot be compensated by conventional alignment algorithms.

It is stated in
Section 3.2 that during the FV encoding process, spatial information of every image patch is implicitly embedded in a GMM. Therefore it is possible to restore the spatial coordinates from the Gaussian means (where the last two variables stand for the x and y coordinates). Similarly, the spatial variances can be restored from the Gaussian covariances, which indicate how well the spatial coordinates can represent individual patch locations. By visualising the Gaussians that correspond to the most discriminative image patches, it is possible to localise the facial regions that are most powerful in distinguishing male and female faces. Visualisation of facial discriminability is shown in Fig. 8
                        . Note that the face image displayed is only a generic representation of facial geometry and therefore is not gender-specific.

The first visualisation format (Fig. 8(a)) shows that the most discriminative Gaussians agree with intuitive feature points (e.g. edges and corners), and therefore are deemed a suitable representation of face appearance. The Gaussians are further visualised in the form of dense image patches whose rankings in the discriminative power are indicated by the numbers centred at each patch (Fig. 8(b)). It can be noticed that the patches overlap significantly, making it difficult to distinguish the most discriminative ones. One simple solution is to construct an energy map for all pixel locations, stacking up all the patches in the previous format. The energy map agrees with the following two rules: 1) image patches with higher rankings hold more energy; 2) A pixel location where overlap exists draws energy from all the patches that cause this overlap. It can be seen that the mouth region (where male adults may have beard and moustache), the nasolabial furrows and the forehead region (where females are more likely to have bangs) are the most discriminative. This result provides insight into region-based facial discriminability so that future research on gender recognition can better target on facial regions with high discriminative power. To further validate our method, we compare the accuracy of our method to eight other state-of-the-art methods with an evaluation of their respective limitations, detailed in Table 1.
                        
                     

The accuracy of the proposed method is only slightly lower than (Ullah et al., 2012) and (Lee et al., 2010) on the FERET database and (Shan, 2012) on the LFW database, but outperforms the others in comparison. However it should be noted that (Ullah et al., 2012) used both fa and fb subsets of the FERET database containing replication of most subjects, while (Lee et al., 2010) used a large number of classifiers and did not specify the evaluation method. Using images of the same subjects for both training and testing should be avoided in an objective evaluation process. On the other hand, (Shan, 2012) removed images with large head rotation and those with ambiguous ground truth. This imposed manual intervention on the database and selected only the `good’ data that were more frontal and easier to classify. It can be therefore concluded that the accuracy of the proposed gender classificaiton algorithm is among one of the highest in the literature, tested on both controlled and uncontrolled databases. The high accuracy of gender classification brought by our study allows a HCI system to understand human charactersitics. As a result, assistive systems are enabled to create user-centred HCI environments that bring more comfort and naturalness to users. In addition, the high robustness and efficiency ensure that it is applicable to practical applications rather than being limited to theoretical studies.

While our gender recognition method accurately and robustly gathers demographic data such that assistive HCI systems can better understand human characteristics, a hybrid method is further proposed that can perform accurate and efficient localisation of eye centres in low-resolution images and videos in real time. Therefore demographic data from gender recognition and behavioural data from eye/gaze analysis can be fused by assistive technologies in order to gain higher intelligence and interactivity. Our algorithm is summarised in Fig. 9
                      as an overview of the eye centre localisation chain.

The algorithm includes two modalities. The first modality performs a global estimation of eye centres over a face region detected by the Viola-Jones face detector (Viola and Jones, 2004) and extracts corresponding eye regions. Results from the first modality are fed into the second modality as prior knowledge and lead to a local and more precise estimation of eye centres. The two energy maps generated by the two modalities are fused to produce final estimation of eye centres.

Human eyes can be characterised as radially symmetrical patterns which can be represented by contours of equal intensity values in an image, i.e. isophotes (Lichtenauer et al., 2005). Due to the large contrast between the iris and the sclera as well as that between the iris and the pupil, the isophotes that follow the edges of the iris and the pupil reflect the geometrical properties of the eye. Therefore centre of these isophotes will be able to represent estimated eye centres. One study (Valenti and Gevers, 2008) proposed an isophote based algorithm that enables pixels to vote for isophote centres they belong to. The displacement vector pointing from a pixel to its isophote centre follows Eq. (13).

                           
                              (13)
                              
                                 
                                    
                                       {
                                       
                                          
                                             D
                                             x
                                          
                                          ,
                                          
                                          
                                             D
                                             y
                                          
                                       
                                       }
                                    
                                    =
                                    
                                    −
                                    
                                       
                                          
                                             {
                                             
                                                
                                                   I
                                                   x
                                                
                                                ,
                                                
                                                
                                                   I
                                                   y
                                                
                                             
                                             }
                                          
                                          
                                             (
                                             
                                                
                                                   I
                                                   x
                                                   2
                                                
                                                
                                                +
                                                
                                                
                                                   I
                                                   y
                                                   2
                                                
                                             
                                             )
                                          
                                       
                                       
                                          
                                             I
                                             y
                                             2
                                          
                                          
                                             f
                                             
                                                x
                                                x
                                             
                                          
                                          
                                          +
                                          2
                                          
                                             I
                                             x
                                          
                                          
                                             I
                                             
                                                x
                                                y
                                             
                                          
                                          
                                             I
                                             y
                                          
                                          +
                                          
                                          
                                             I
                                             x
                                             2
                                          
                                          
                                             I
                                             
                                                y
                                                y
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where Ix
                         and Iy
                         are first-order derivatives of the luminance function I(x, y) in the x and y directions. Ixx, Ixy
                         and Iyy
                         are the second-order partial derivatives. The weight of each vote is indicated by the curvedness of an isophote since the iris and pupil edges that are circular obtain high curvedness values as opposed to flat isophotes. The curvedness (Koenderink and Doorn, 1992) is calculated as:

                           
                              (14)
                              
                                 
                                    c
                                    d
                                    
                                       (
                                       
                                          x
                                          ,
                                          y
                                       
                                       )
                                    
                                    =
                                    
                                    
                                       
                                          
                                             I
                                             
                                                x
                                                x
                                             
                                             2
                                          
                                          +
                                          
                                          2
                                          ×
                                          
                                             I
                                             
                                                x
                                                y
                                             
                                             2
                                          
                                          +
                                          
                                          
                                             I
                                             
                                                y
                                                y
                                             
                                             2
                                          
                                       
                                    
                                 
                              
                           
                        
                     

We also consider the brightness of the isophote centre in the voting process based on the fact that the pupil is normally darker than the iris and the sclera. Therefore, an energy map Ea
                        (x, y) is constructed that collects all the votes to reflect the eye centre position following Eq. (15), where α is the maximum greyscale in the image (
                           
                              α
                              =
                              255
                           
                         in the experiments).

                           
                              (15)
                              
                                 
                                    
                                       E
                                       a
                                    
                                    
                                       (
                                       
                                          x
                                          +
                                          
                                             D
                                             x
                                          
                                          ,
                                          
                                          y
                                          +
                                          
                                             D
                                             y
                                          
                                       
                                       )
                                    
                                    =
                                    
                                       [
                                       
                                          α
                                          −
                                          
                                          I
                                          
                                             (
                                             
                                                x
                                                +
                                                
                                                   D
                                                   x
                                                
                                                ,
                                                
                                                y
                                                +
                                                
                                                   D
                                                   y
                                                
                                             
                                             )
                                          
                                       
                                       ]
                                    
                                    
                                    
                                    ×
                                    
                                    c
                                    d
                                    
                                       (
                                       
                                          x
                                          ,
                                          
                                          y
                                       
                                       )
                                    
                                 
                              
                           
                        
                     

Though isophotes have been employed in the literature, related works have only extracted isophote features from eye regions which are either cropped according to anthropometric relations (which are interrupted by head poses) or found by an eye detector (which largely increases the complexity of algorithm). Our method is different, in that it extracts isophote features for a whole face and constructs a global energy map Ea
                        (x, y). The energy points below 30% of the maximum value are removed. The remaining energy points therefore become the new eye centre candidates that are fed to our second modality for further analysis. Ea
                        (x, y) is then split into the left half Eaul
                        (x, y) and the right half Eaur
                        (x, y), corresponding to the left and right half of the face, where the mouth region (the lower half of the energy map) is simply removed since it is unlikely to concern any eye information regardless of normal head poses. We further calculate the energy centre, i.e. the first moment of the energy map divided by the total energy, which is selected instead as the optimal eye centre. Taking Eaul
                        (x, y) as an example, this can be formulated as Eq. (16)
                        
                           
                              (16)
                              
                                 
                                    
                                       {
                                       
                                          c
                                          
                                             x
                                             
                                                a
                                                u
                                                l
                                             
                                          
                                          ,
                                          
                                             
                                             c
                                          
                                          
                                             y
                                             
                                                a
                                                u
                                                l
                                             
                                          
                                       
                                       }
                                    
                                    =
                                    
                                    
                                       
                                          
                                             ∑
                                             
                                                x
                                                =
                                                1
                                             
                                             m
                                          
                                          
                                             ∑
                                             
                                                y
                                                =
                                                1
                                             
                                             n
                                          
                                          
                                             {
                                             
                                                x
                                                ,
                                                
                                                y
                                             
                                             }
                                          
                                          ·
                                          
                                             E
                                             
                                                a
                                                u
                                                l
                                             
                                          
                                          
                                             (
                                             
                                                x
                                                ,
                                                
                                                y
                                             
                                             )
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                x
                                                =
                                                1
                                             
                                             m
                                          
                                          
                                             ∑
                                             
                                                y
                                                =
                                                1
                                             
                                             n
                                          
                                          
                                             E
                                             
                                                a
                                                u
                                                l
                                             
                                          
                                          
                                             (
                                             
                                                x
                                                ,
                                                
                                                y
                                             
                                             )
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 C
                                 
                                    a
                                    u
                                    l
                                 
                              
                              =
                              
                              
                                 {
                                 
                                    c
                                    
                                       x
                                       
                                          a
                                          u
                                          l
                                       
                                    
                                    ,
                                    
                                       
                                       c
                                    
                                    
                                       y
                                       
                                          a
                                          u
                                          l
                                       
                                    
                                 
                                 }
                              
                           
                         is the optimal estimation of the left eye centre, m and n are the maximum row and column number in Eaul
                        . The eye region to be analysed by our second modality is then selected, which centres at the optimal eye centre estimation (its width being 1/10 of the face size and its height being 1/15 of the face size). As a result, our method does not require an eye detector and is robust to head rotations since global isophotes are investigated. This process is shown in Fig. 10.
                        
                     

The first modality performs the initial eye centre estimation, filtering eye centre candidates and selecting local eye regions for the second modality. Based on an objective function (Eq. (17)), we further design a radius constraint and a Selective Oriented Gradient (SOG) filter to re-estimate the eye centre positions with enhanced accuracy and reliability.

The radially symmetrical patterns of an eye generate isophotes around the iris and pupil edges that can effectively vote for the eye centre. When simply modelled as circular objects, the iris and the pupil can produce gradient features that give an accurate estimation of the eye centre. This is based on the idea that the prominent gradient vectors on circular iris/pupil boundaries should agree with the radial directions and therefore the dot product of each gradient vector with its corresponding radial vector is maximised.

This is formulated by (Timm and Barth, 2011) as an objective function:

                           
                              (17)
                              
                                 
                                    
                                       
                                          c
                                       
                                       *
                                    
                                    =
                                    
                                       
                                          
                                             
                                                arg
                                                m
                                                a
                                                x
                                             
                                          
                                       
                                       
                                          
                                             c
                                          
                                       
                                    
                                    
                                       {
                                       
                                          
                                             1
                                             N
                                          
                                          
                                             ∑
                                             
                                                x
                                                =
                                                1
                                             
                                             m
                                          
                                          
                                             ∑
                                             
                                                y
                                                =
                                                1
                                             
                                             n
                                          
                                          
                                             I
                                             c
                                          
                                          
                                             (
                                             
                                                x
                                                ,
                                                
                                                y
                                             
                                             )
                                          
                                          ·
                                          
                                             
                                                (
                                                
                                                   
                                                      d
                                                   
                                                   t
                                                
                                                
                                                   (
                                                   
                                                      x
                                                      ,
                                                      
                                                      y
                                                   
                                                   )
                                                
                                                ·
                                                ∇
                                                
                                                   I
                                                   c
                                                
                                                
                                                   (
                                                   
                                                      x
                                                      ,
                                                      
                                                      y
                                                   
                                                   )
                                                
                                                )
                                             
                                             2
                                          
                                       
                                       }
                                    
                                 
                              
                           
                        
                        
                           
                              (18)
                              
                                 
                                    
                                       
                                          
                                             d
                                             
                                                (
                                                
                                                   x
                                                   ,
                                                   
                                                   y
                                                
                                                )
                                             
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                             
                                                
                                                   p
                                                   
                                                      (
                                                      
                                                         x
                                                         ,
                                                         
                                                         y
                                                      
                                                      )
                                                   
                                                   −
                                                   c
                                                
                                                
                                                   ∥
                                                   p
                                                   
                                                      (
                                                      
                                                         x
                                                         ,
                                                         
                                                         y
                                                      
                                                      )
                                                   
                                                   −
                                                   
                                                      c
                                                      2
                                                   
                                                   ∥
                                                
                                             
                                             ,
                                             
                                             
                                             
                                             
                                             ∀
                                             x
                                             ∀
                                             y
                                             :
                                             
                                             ∥
                                             d
                                             
                                                (
                                                
                                                   x
                                                   ,
                                                   
                                                   y
                                                
                                                )
                                             
                                             ∥
                                             
                                                
                                                2
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                          =
                                       
                                       
                                          
                                             1
                                             ,
                                             
                                             
                                             
                                             ∥
                                             ∇
                                             
                                                I
                                                c
                                             
                                             
                                                (
                                                
                                                   x
                                                   ,
                                                   
                                                   y
                                                
                                                )
                                             
                                             ∥
                                             
                                                
                                                2
                                             
                                             =
                                             1
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           c
                         is a centre candidate that remained from the isophote based modality, 
                           c
                        * is the optimal centre to be calculated, N is the total number of pixels in an eye region to be analysed, 
                           d
                        (x, y) is the displacement vector connecting a centre candidate 
                           c
                         and a different pixel 
                           p
                        (x, y). ∇I
                        
                           
                              c
                           
                        (x, y) is the gradient vector and I
                        
                           
                              c
                           
                         is the intensity value for an eye centre candidate, meaning that the objective function only considers pixels that have been previously selected as eye centre candidates. The displacement vectors and gradient vectors are normalised to unit vectors. This objective function is modified such that a gradient vector is only considered if its direction is reverse to the displacement vector. This is based on the observation that the pupil is always darker than its neighbouring regions and thus generates outward gradients. A sample implementation of this approach on an eye image is illustrated in Fig. 11
                        . We tested this modality on a number of images with varied head poses, gaze directions and partial eye/pupil occlusions. The raw face images (right halves), gradients from the eye regions, and the corresponding energy maps are shown in Fig. 12.
                        
                     

While this method provides an effective solution to eye centre estimation, it has a number of inherent limitations that would incur error or even failure in the estimation. First of all, the objective function is formulated given that the pupil and the iris are circular objects. When the edges around eye corners and shadows surpass the pupil and the iris in circularity measure, they will cause eye centre estimates to be located on themselves rather than the centre of the pupil. Secondly, gradients on eyelids, eye corners and eyebrows will also vote for eye centre estimations. We define these as `non-effective gradients’, as opposed to `effective gradients’ that are located on edges of the iris and pupil. In more severe cases where makeup and shadows are prominent, the iris and the pupil, by contrast, generate weak `effective gradients’ such that the energy map is prone to erroneous energy response. This will further escalate estimation errors.

To resolve the above problems shared by most methods that utilise geometrical features for eye centre localisation, we introduce a radius constraint and design a SOG filter that can effectively deal with circularity measure and can resolve problems posed by eye corners, eyebrows, eyelids and shadows.

A radius constraint is introduced such that the Euclidean norms of displacement vectors, which are related to estimated iris radius, have more influence on the calculation of an eye centre location. This is based on the assumption that shadows and eyebrow segments have random radius values, while the iris radii are more constant relative to the size of a face. This provides a way to differentiate circular clusters of various radiuses and to determine their weights in energy map accumulation. The function for the significance measure emulates the frequency response of a Butterworth low pass filter:

                           
                              (19)
                              
                                 
                                    
                                       w
                                       r
                                    
                                    
                                       (
                                       
                                          x
                                          ,
                                          
                                          y
                                       
                                       )
                                    
                                    =
                                    
                                    
                                       
                                          1
                                          
                                             1
                                             +
                                             
                                                
                                                   
                                                      (
                                                      
                                                         
                                                            
                                                               
                                                                  ∥
                                                                  
                                                                     d
                                                                     
                                                                        (
                                                                        
                                                                           x
                                                                           ,
                                                                           
                                                                           y
                                                                        
                                                                        )
                                                                     
                                                                  
                                                                  ∥
                                                               
                                                               2
                                                            
                                                            −
                                                            ρ
                                                         
                                                         ω
                                                      
                                                      )
                                                   
                                                
                                                
                                                   2
                                                   σ
                                                
                                             
                                          
                                       
                                       2
                                    
                                 
                              
                           
                        where 
                           d
                        (x, y)2 is the L
                        2 norm of a displacement vector without being normalised to unit vector. ρ is the estimated radius of the iris. σ and ω correspond to the order and the cutoff frequency of the filter. The curves corresponding to varying σ and ω following Eq. (19) are shown in Fig. 13
                        . It should be noted that in each subfigure only one parameter is variable while the other remains constant.

The radius weight function is maximally flat around the estimated centre ρ and drops rapidly when the radius is out of the flatness band whose range is controlled by σ. The roll-off rate is controlled by ω, indicating the decreasing rate in weight. Increasing ω while decreasing σ will enhance the rigidity of the constraint which could be assumed for circumstances where strong shadows are present. The value for ρ can be set according to the size of the face in an image. Consequently, this constraint can effectively alleviate problematic issues posed by dark and circular pixel clusters.

With the inspiration drawn from the histogram of oriented gradients (HOG) feature descriptor (Dalal and Triggs, 2005), a SOG filter is introduced that discriminates gradients of rapid change in orientation from those of less change. We specifically design this novel SOG filter and introduce it into the modular eye centre localisation scheme so that it is perfectly tailored to reinforce the two main modalities despite its versatile applicability. The basic idea takes the form of a statistical analysis of the gradient orientations within a window centred at a pixel position. For each Kx
                         × Ky
                         window (12×8 pixels in the experiment) centred at pixel i, the gradients in x and y directions are calculated, whose orientations follow:

                           
                              (20)
                              
                                 
                                    θ
                                    =
                                    
                                    
                                       tan
                                       
                                          −
                                          1
                                       
                                    
                                    
                                       (
                                       
                                          
                                             I
                                             y
                                          
                                          
                                             I
                                             x
                                          
                                       
                                       )
                                    
                                    ·
                                    
                                       
                                          180
                                          ∘
                                       
                                       π
                                    
                                 
                              
                           
                        
                     

The gradient orientations are then accumulated into k (k  <  360) orientation bins, where each bin contains the count of orientations from 
                           
                              b
                              ·
                              
                                 
                                    360
                                    ∘
                                 
                                 k
                              
                           
                         to 
                           
                              
                                 (
                                 
                                    b
                                    +
                                    1
                                 
                                 )
                              
                              ·
                              
                                 
                                    360
                                    ∘
                                 
                                 k
                              
                              
                              
                                 (
                                 
                                    0
                                    ≤
                                    b
                                    ≤
                                    k
                                    −
                                    1
                                 
                                 )
                              
                           
                         within the window. If the recorded count in a bin exceeds a threshold, the corresponding pixels that accumulate the bin will have their gradient vectors halved, i.e. their weights reduced. As a result, the objective function becomes:

                           
                              (21)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   c
                                                
                                                *
                                             
                                             =
                                             
                                                
                                                   
                                                      
                                                         arg
                                                         m
                                                         a
                                                         x
                                                      
                                                   
                                                
                                                
                                                   
                                                      c
                                                   
                                                
                                             
                                          
                                       
                                       
                                       
                                          
                                             {
                                             
                                                1
                                                N
                                             
                                             
                                                ∑
                                                
                                                   x
                                                   =
                                                   1
                                                
                                                m
                                             
                                             
                                                ∑
                                                
                                                   y
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                w
                                                g
                                             
                                             
                                                (
                                                
                                                   x
                                                   ,
                                                   y
                                                
                                                )
                                             
                                             ·
                                             
                                                w
                                                r
                                             
                                             
                                                (
                                                
                                                   x
                                                   ,
                                                   
                                                   y
                                                
                                                )
                                             
                                             ·
                                             
                                                [
                                                
                                                   α
                                                   −
                                                   I
                                                   
                                                      (
                                                      
                                                         x
                                                         ,
                                                         
                                                         y
                                                      
                                                      )
                                                   
                                                
                                                ]
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                       
                                          
                                             ·
                                             
                                                
                                                   (
                                                   
                                                      
                                                         d
                                                      
                                                      t
                                                   
                                                   
                                                      (
                                                      
                                                         x
                                                         ,
                                                         
                                                         y
                                                      
                                                      )
                                                   
                                                   ·
                                                   ∇
                                                   
                                                      I
                                                      c
                                                   
                                                   
                                                      (
                                                      
                                                         x
                                                         ,
                                                         
                                                         y
                                                      
                                                      )
                                                   
                                                   )
                                                
                                                2
                                             
                                             }
                                          
                                       
                                    
                                 
                              
                           
                        where wg
                        (x, y) is the weight of a gradient adjusted by the SOG filter (i.e.
                        
                           
                              
                                 w
                                 g
                              
                              
                                 (
                                 
                                    x
                                    ,
                                    y
                                 
                                 )
                              
                              =
                              0.5
                           
                         
                        to halve the gradient vectors and
                        
                           
                              
                                 w
                                 g
                              
                              
                                 (
                                 
                                    x
                                    ,
                                    y
                                 
                                 )
                              
                              =
                              1
                           
                         
                        otherwise). The threshold for the counts is determined by an absolute value (6 in the experiment) as well as a value (40% in the experiment) relative to the number of pixels with non-zero gradients within the window. As a result, pixels that maintain similar gradient orientations to their neighbours will have their weights reduced and they are referred to as `impaired pixels’ in the rest of the paper. When a curve has low curvature, it comprises more `impaired pixels’. Therefore the SOG filter can be used for general curvature discrimination tasks. It has the advantage that it does not require an explicit function for the curve and that it is effective in dealing with curves forming irregular shapes. Fig. 14
                         demonstrates the effectiveness of a SOG filter applied to an image containing irregular curves and an image of an eye region.

It is shown in Fig. 14 that the SOG filter has successfully distinguished curves with low and high curvatures and that it is effective in dealing with intersected and occluded curves or curve segments. This approach allows both magnitude and orientation of gradients to serve for gradient filtering. It resolves the challenges brought by shadows, facial makeup and edges on the eyelids, eyebrows and other facial parts outside the iris that are most interfering in geometrical feature based eye centre localisation approaches.

In the final stage, two energy maps Ea
                         and Eb
                         are integrated into Ef
                        (x, y) so that they both contribute to election of an eye centre. It is critical, prior to the integration, to determine the confidence of each modality, to estimate the complexity of the eye image, and thus to determine their weights in the fusion mechanism.

The left eye region is taken as an example to illustrate the fusion mechanism. If the equivalent centroid Caul
                         calculated by Eq. (16) is close to the pixel position Cmaxl
                         that has the maximum value in the first energy map Eaul
                        (x, y). , Caul
                         is considered confident since the isophote centre and the equivalent centroid coincide. In this case, more `effective gradients’ are present, allowing the second modality to be more robust and precise. The two modalities are then utilised and fused following Eq. (22). When Caul
                         and Cmaxl
                         disagree and have a large Euclidean distance, the first energy map will have high energy clusters sparsely distributed, potentially caused by severe shadows and specularities. The second modality will be fluenced by `impaired pixels’ and produce erroneous centre estimates. Therefore only the equivalent centroids Caul
                         and Caur
                         are selected as final eye centres.

                           
                              (22)
                              
                                 
                                    
                                    
                                       E
                                       f
                                    
                                    
                                       (
                                       
                                          x
                                          ,
                                          
                                          y
                                       
                                       )
                                    
                                    =
                                    
                                    
                                       1
                                       
                                          
                                             ∥
                                             
                                                
                                                   C
                                                   
                                                      a
                                                      u
                                                      l
                                                   
                                                
                                                −
                                                
                                                
                                                   C
                                                   
                                                      E
                                                      a
                                                      m
                                                      a
                                                      x
                                                   
                                                
                                             
                                             ∥
                                          
                                          2
                                       
                                    
                                    ·
                                    
                                       E
                                       a
                                    
                                    
                                       (
                                       
                                          x
                                          ,
                                          
                                          y
                                       
                                       )
                                    
                                    +
                                    
                                    
                                       E
                                       b
                                    
                                    
                                       (
                                       
                                          x
                                          ,
                                          
                                          y
                                       
                                       )
                                    
                                 
                              
                           
                        where 
                           ϵ
                         takes a value relative to the width of the eye region 
                           
                              ϵ
                              f
                           
                         and 
                           
                              
                              0
                              <
                              ∥
                              
                                 C
                                 
                                    a
                                    u
                                    l
                                 
                              
                              −
                              
                              
                                 C
                                 
                                    E
                                    a
                                    m
                                    a
                                    x
                                 
                              
                              ∥
                              
                                 
                                 2
                              
                              ≤
                              ϵ
                           
                        . In our experiments, 
                           
                              ϵ
                              =
                              
                              0.3
                              
                                 ϵ
                                 f
                              
                           
                         pixels. The maximum response in the final energy map will represent the estimated eye centre. The estimate for the final right eye centre follows the same approach.

The public dataset used is the BioID database (BioID., 2014), consisting of 1520 images. It has been popular in the literature for evaluations of other eye centre localisation algorithms. The variations in the database include illumination, face scale, head pose and presence of glasses. Applied to this database, the proposed algorithm was tested against the others in the literature in resolving challenges introduced by the wide range of variations. The relative error measure proposed by (Jesorsky et al., 2001) was used to evaluate the accuracy of the algorithm. This method firstly calculates the absolute error, i.e. the Euclidian distance between the centre estimates and the ground truth provided by the database; it then normalises the Euclidian distance relative to the pupillary distance. This is formulated by Eq. (23).

                           
                              (23)
                              
                                 
                                    e
                                    =
                                    
                                    
                                       
                                          max
                                          
                                             (
                                             
                                                
                                                   d
                                                   
                                                      l
                                                      e
                                                      f
                                                      t
                                                   
                                                
                                                ,
                                                
                                                
                                                   d
                                                   
                                                      r
                                                      i
                                                      g
                                                      h
                                                      t
                                                   
                                                
                                             
                                             )
                                          
                                       
                                       
                                          ɛ
                                       
                                    
                                 
                              
                           
                        where dleft
                         and dright
                         are the absolute errors for the eye pair, and ɛ is distance between the left and the right eye, i.e. the pupillary distance, measured in pixels. The maximum of dleft
                         and dright
                         after normalisation is defined as `max normalised error’ emax
                        . Similarly, the accuracy curve for the minimum normalised error emin
                         and the average normalised error eavg
                         are calculated. A relative distance of 
                           
                              e
                              =
                              0.25
                           
                         corresponds to half the width of an eye. The accuracy of the proposed algorithm is evaluated by this error measure technique. Its accuracy curves are shown in Fig. 15
                        . The proposed algorithm is further compared with 10 state-of-the-art methods in the literature, summarised in Table 2.
                        
                     

The proposed method gains the best results for the accuracy measure emin
                         ≤ 0.10 as well as emax
                         ≤ 0.25, and the second best for emax
                         ≤ 0.05 and emax
                         ≤ 0.10. Except for the accuracy measure for emin
                         ≤ 0.25 where very similar results are achieved, a score of 2 is assigned to every first rank and a score of 1 is assigned to every second rank. The proposed method gains a total score of 6, outperforming all the other methods when comparing the classification accuracy.

The simplicity and efficiency of the proposed method in computation is demonstrated by comparing it to (Timm and Barth, 2011) which claims to have achieved excellent real-time performance as one of its key features. Take an image containing a 41 × 47 eye region, i.e. 1927 pixels, as an example (Fig. 11), the method in comparison performs per-pixel estimation of the eye centre, assuming that every pixel is an eye centre candidate. Therefore 1927 iterations are needed before the optimal candidate is selected. The proposed method, on the other hand, resolves the problem by utilising prior knowledge drawn from the first modality which, through an initial estimation, avoids the per-pixel candidate assumption. The removal of the low-energy pixels in the first modality largely reduced the number of candidates, i.e. number of iterations in the second modality. In the same sample image of the 41 × 47 eye region, the iterations are decreased to only 67 from 1927, making our algorithm 29 times faster.

In summary, our eye centre localisation algorithm follows a coarse-to-fine, global-to-regional approach, making use of both isophote and gradient features. The high accuracy and efficiency of the proposed algorithm is achieved by two complementary modalities with a SOG filter designed to deal with the strong edges and shadows in the eye regions. The proposed method does not need any training or learning stages and is easy to implement.

Gaze gestures are predefined sequences of eye movements which hold great potential in HCI. It has been proposed in the literature for disability assistance and other HCI purposes. Gaze gestures are derived from consecutive image frames where eye centre coordinates are collected. Therefore, an efficient and robust eye centre localisation algorithm becomes the key to successful gaze gesture recognition and real-time assistive technologies. Unfortunately, as reviewed previously, most eye centre localisation algorithms require dedicated devices (e.g. NIR illuminator for active methods or head-mounted devices) which render the resulting systems unrealistic for extensive use due to the high cost and inconvenience. Others, often being short-ranged, lack accuracy and robustness in real-world scenes. In comparison, our eye centre localisation method provides an efficient and accurate means for collecting eye centre coordinates whose patterns, namely gaze gestures, can be further analysed.

The realisation of remote control of a HCI system via gaze gestures is non-invasive, low-cost and efficient. We introduce our method as a four-stage process: 1) accurate eye centre localisation in spatial-temporal domain, 2) eye movement encoding (Hyrskykari et al., 2012), 3) gaze gesture recognition and 4) HCI event activation.

We first employ the algorithm introduced in the preceding subsection for accurate and robust eye centre localisation. Two eye centre positions 
                           
                              
                                 E
                                 l
                              
                              
                                 (
                                 f
                                 )
                              
                              =
                              
                                 {
                                 
                                    
                                       e
                                       
                                          l
                                          x
                                       
                                    
                                    ,
                                    
                                    
                                       e
                                       
                                          l
                                          y
                                       
                                    
                                 
                                 }
                              
                           
                         and 
                           
                              
                                 E
                                 r
                              
                              
                                 (
                                 f
                                 )
                              
                              =
                              
                                 {
                                 
                                    
                                       e
                                       
                                          r
                                          x
                                       
                                    
                                    ,
                                    
                                    
                                       e
                                       
                                          r
                                          y
                                       
                                    
                                 
                                 }
                              
                           
                         are estimated and recorded for each frame. The mean value of them is calculated as
                           
                              
                                 
                                    E
                                    ¯
                                 
                              
                              
                                 (
                                 f
                                 )
                              
                              =
                              
                                 {
                                 
                                    
                                       
                                          e
                                          ¯
                                       
                                       x
                                    
                                    ,
                                    
                                       
                                          e
                                          ¯
                                       
                                       y
                                    
                                 
                                 }
                              
                              =
                              
                                 {
                                 
                                    
                                       (
                                       
                                          e
                                          
                                             l
                                             x
                                          
                                       
                                       +
                                       
                                          e
                                          
                                             r
                                             x
                                          
                                       
                                       )
                                    
                                    /
                                    2
                                    ,
                                    
                                       (
                                       
                                          e
                                          
                                             l
                                             y
                                          
                                       
                                       +
                                       
                                          e
                                          
                                             r
                                             y
                                          
                                       
                                       )
                                    
                                    /
                                    2
                                 
                                 }
                              
                           
                        
                        , where f is the frame number and the x and y notations stand for the horizontal and vertical components.

In the second stage, the first order derivatives of the vectors 
                           E
                        
                        
                           
                              l
                           
                        
                        , E
                        
                        
                           
                              r
                           
                         and 
                           
                              
                                 E
                                 ¯
                              
                           
                         are calculated as 
                           
                              
                                 G
                                 l
                              
                              
                                 (
                                 f
                                 )
                              
                              =
                              
                                 {
                                 
                                    
                                       
                                          
                                             e
                                             
                                                l
                                                x
                                             
                                          
                                       
                                       ′
                                    
                                    ,
                                    
                                    
                                       
                                          
                                             e
                                             
                                                l
                                                y
                                             
                                          
                                       
                                       ′
                                    
                                 
                                 }
                              
                           
                        , 
                           
                              
                                 G
                                 r
                              
                              
                                 (
                                 f
                                 )
                              
                              =
                              
                                 {
                                 
                                    
                                       
                                          
                                             e
                                             
                                                r
                                                x
                                             
                                          
                                       
                                       ′
                                    
                                    ,
                                    
                                    
                                       
                                          
                                             e
                                             
                                                r
                                                y
                                             
                                          
                                       
                                       ′
                                    
                                 
                                 }
                              
                           
                         and 
                           
                              
                                 
                                    G
                                    ¯
                                 
                              
                              
                                 (
                                 f
                                 )
                              
                              =
                              
                                 {
                                 
                                    
                                       
                                          
                                             
                                                e
                                                ¯
                                             
                                             x
                                          
                                       
                                       ′
                                    
                                    ,
                                    
                                    
                                       
                                          
                                             
                                                e
                                                ¯
                                             
                                             y
                                          
                                       
                                       ′
                                    
                                 
                                 }
                              
                           
                        , respectively. A threshold (4.5 % in the experiments) is then set to remove any small values in the two vectors, which might be caused by unintentional saccadic movements. It should be noted that the threshold is normalised by the pupillary distance ɛ so that it is independent of user-to-camera distance. Additionally, the movements of the two eyes are compared to each other with regard to their magnitudes according to Eq. (24) and (25). This logarithm function accounts for the fact that the two eyes in natural behaviours always move together, i.e. the left and the right eye move toward the same direction and shift by similar amount.

                           
                              (24)
                              
                                 
                                    
                                       R
                                       g
                                    
                                    =
                                    
                                       {
                                       
                                          
                                             
                                                
                                                   
                                                      log
                                                      10
                                                   
                                                   
                                                      (
                                                      
                                                         
                                                            
                                                               
                                                                  e
                                                                  
                                                                     l
                                                                     x
                                                                  
                                                               
                                                               
                                                                  
                                                                  
                                                                     ′
                                                                     2
                                                                  
                                                               
                                                               +
                                                               
                                                                  e
                                                                  
                                                                     l
                                                                     y
                                                                  
                                                               
                                                               
                                                                  
                                                                  
                                                                     ′
                                                                     2
                                                                  
                                                               
                                                            
                                                         
                                                         
                                                            
                                                               
                                                                  e
                                                                  
                                                                     r
                                                                     x
                                                                  
                                                               
                                                               
                                                                  
                                                                  
                                                                     ′
                                                                     2
                                                                  
                                                               
                                                               +
                                                               
                                                                  e
                                                                  
                                                                     r
                                                                     y
                                                                  
                                                               
                                                               
                                                                  
                                                                  
                                                                     ′
                                                                     2
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                      )
                                                   
                                                   ,
                                                
                                             
                                             
                                                
                                                   i
                                                   f
                                                   
                                                      
                                                         
                                                            e
                                                            
                                                               l
                                                               x
                                                            
                                                         
                                                         
                                                            
                                                            
                                                               ′
                                                               2
                                                            
                                                         
                                                         
                                                         +
                                                         
                                                         
                                                            e
                                                            
                                                               l
                                                               y
                                                            
                                                         
                                                         
                                                            
                                                            
                                                               ′
                                                               2
                                                            
                                                         
                                                      
                                                   
                                                   
                                                   ≥
                                                   
                                                   1
                                                   
                                                   &
                                                   
                                                   
                                                      
                                                         
                                                            e
                                                            
                                                               l
                                                               x
                                                            
                                                         
                                                         
                                                            
                                                            
                                                               ′
                                                               2
                                                            
                                                         
                                                         
                                                         +
                                                         
                                                         
                                                            e
                                                            
                                                               l
                                                               y
                                                            
                                                         
                                                         
                                                            
                                                            
                                                               ′
                                                               2
                                                            
                                                         
                                                      
                                                   
                                                   ≥
                                                   1
                                                
                                             
                                          
                                          
                                             
                                                
                                                   0
                                                   ,
                                                
                                             
                                             
                                                
                                                   o
                                                   t
                                                   h
                                                   e
                                                   r
                                                   w
                                                   i
                                                   s
                                                   e
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (25)
                              
                                 
                                    
                                       
                                          
                                             
                                                D
                                                
                                                   g
                                                   x
                                                
                                             
                                             =
                                             
                                             
                                                e
                                                
                                                   l
                                                   x
                                                
                                             
                                             ·
                                             
                                                e
                                                
                                                   r
                                                   x
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                D
                                                
                                                   g
                                                   y
                                                
                                             
                                             =
                                             
                                             
                                                e
                                                
                                                   l
                                                   y
                                                
                                             
                                             ·
                                             
                                                e
                                                
                                                   r
                                                   y
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

When both 
                           
                              D
                              
                                 g
                                 x
                              
                           
                         and 
                           
                              D
                              
                                 g
                                 y
                              
                           
                         take positive values, we consider the directions of eye movements consistent. Ideally, when the two eyes shift by the same amount, Rg
                         should have value 0. To allow for a margin of error, we set a threshold of 0.6 and consider the eye movement consistent when the absolute value of Rg
                         falls below the threshold. Only when these two conditions are satisfied are the eye centre positions updated by those from the subsequent frame.

To encode voluntary eye saccades, we decompose 
                           
                              
                                 
                                    G
                                    ¯
                                 
                              
                              
                                 (
                                 f
                                 )
                              
                           
                         into 
                           
                              
                                 
                                    G
                                    X
                                 
                                 ¯
                              
                              
                                 (
                                 f
                                 )
                              
                              =
                              
                              
                                 
                                    
                                       
                                          e
                                          ¯
                                       
                                       x
                                    
                                 
                                 ′
                              
                           
                         and 
                           
                              
                                 
                                    G
                                    Y
                                 
                                 ¯
                              
                              
                                 (
                                 f
                                 )
                              
                              =
                              
                              
                                 
                                    
                                       
                                          e
                                          ¯
                                       
                                       y
                                    
                                 
                                 ′
                              
                           
                        . Then a positive value in 
                           
                              
                                 
                                    G
                                    X
                                 
                                 ¯
                              
                              
                                 (
                                 f
                                 )
                              
                           
                         is denoted by `1’, a negative value by `2’; a positive value in 
                           
                              
                                 
                                    G
                                    Y
                                 
                                 ¯
                              
                              
                                 (
                                 f
                                 )
                              
                           
                         is denoted by `4’ and a negative value by `7’. Therefore `1’, `2’, `4’, `7’ are encoded gaze shifts representing saccadic strokes `left’, `right’, `up’ and `down’. We further summate the two gaze shift vectors 
                           
                              
                                 
                                    G
                                    X
                                 
                                 ¯
                              
                              
                                 (
                                 f
                                 )
                              
                           
                         and 
                           
                              
                                 
                                    G
                                    Y
                                 
                                 ¯
                              
                              
                                 (
                                 f
                                 )
                              
                           
                         and produce 
                           
                              
                                 
                                    G
                                    s
                                 
                                 ¯
                              
                              
                                 (
                                 f
                                 )
                              
                           
                        , the integrated gaze shift vector. As a result, voluntary gaze shifts are recorded as a combination and repetition of the four digits, while a `0’ represents an unchanged eye position or an involuntary saccade. We define in the experiments for HCI seven types of gaze gestures, shown in Table 3
                        . Other saccadic codes (`5’=`1’+`4’, `6’=`2’+`4’, `8’=`1’+`7’, `9’=`2’+`7’) denote diagonal saccadic strokes that are reserved for our future work.

In the third stage, the gaze gesture patterns are recognised by searching for specific gesture sequences in a segment of 
                           
                              
                                 
                                    G
                                    s
                                 
                                 ¯
                              
                              
                                 (
                                 f
                                 )
                              
                           
                         (every segment being a two-second time slot in our experiments), which will trigger pre-defined HCI events in the last stage.

To validate our method from an applied perspective, we designed an experimental directed advertising system (Fig. 16
                        ) that consisted of a high-definition (HD) 47-inch display, a webcam operating at 640×480 resolution and a PC in the cabinet for camera control and data storage/processing. Digital signage systems have been prevalent for years in this digital era and the information age. Their use for advertising has become ubiquitous and can be found at venues such as restaurants, shopping malls, airports and other public spaces. Often referred to as digital out-of-home (DOOH) advertising (Lasinger and Bauer, 2013), this advertising format aims to extend the exposure and the effectiveness of marketing messages by engaging consumers to an increased extent, compared to conventional print based billboards. Following the idea of switching from print media to digital media, it is only intuitive to reform a conventional DOOH advertising system toward a HCI system for enhanced interactivity and adaptability. This system further plays assistive roles by allowing the elderly and the disabled to browse information remotely and creating a user-friendly atmosphere according to the user characteristics it gathers and predicts.

When a user approaches the system, his/her face images are captured by the camera. The FV encoding method then output the predicted gender label. As a result, advertisement thumbnails that are highly relevant to the predicted gender group will be displayed in replacement to previous advertising messages. The eye centre localisation algorithm then detects eye centres in every image frame and supplies the information to the gaze gesture recognition algorithm. When a gaze sequence matches one of the seven pre-defined gaze gesture patterns (see Table 3), the advertising messages displayed on the screen can be manipulated correspondingly. For example, when gaze gesture No. 2 is detected, the top-right advertisement thumbnail will be displayed at the screen centre. If gaze gesture No. 6 is detected as a subsequent gesture, an enlarged view of the centred thumbnail will become available. This puts the user in an active role for being able to receive the recommended advertisements from the system, as well as being able to browse or switch the advertisements oneself. In the tests, the system could robustly and accurately perform advertisement selection (gaze gesture type 1, 2, 3, 4, 7), reset (gaze gesture type 5) and zoom-in (gaze gesture type 6) operations. Any gaze gesture could be recognised as soon as the last eye saccade in a gaze gesture sequence was issued by a user. When no face appears in a frame, randomly selected advertisement thumbnails will be brought to circulation on the screen. Apart from enabling active advertisement browsing, the system can also passively construct an attentive energy map to reflect relative user attention by accumulating eye centre positions over time. Higher energy points correspond to directions where a user gazes at for longer time.

A video demonstration (see Supplementary Material directed_advertising.mp4) has been made to illustrate this HCI process where the display of advertisements responds to the gender and gaze of a user. A representative frame from the demonstration is displayed in Fig. 17
                        . This demonstration shows that the proposed method can accurately localise eye centres on well-illuminated faces (in a normal indoor environment) and on poorly-illuminated faces (only illuminated by two near-infrared (NIR) LEDs). The predicted gender label and the gaze gestures can be utilised collectively to change the advertisement display. Another video demonstration (see Supplementary Material eye_centre_localisation.mp4) has been made to show that eye centres can be accurately localised on over-exposed images of a face with different head poses and eye occlusions (by a glass frame and a finger). A representative frame of this demonstration is shown in Fig. 18.
                        
                     

It should be noted that although the proposed HCI strategy combines gender and gaze analysis, the two respects have been implemented as individual algorithms such that the results from automatic gender recognition and gaze gesture recognition are not correlated in general. The highly accurate and efficient gaze gesture recognition benefits from the two complementary modules in the eye centre localisation algorithm; the gender recognition method benefits from the encoded discriminant FVs and a simple linear classifier. However, the localised eye centres on a face image can be utilised for face alignment which increases gender recognition accuracy. The fused knowledge drawn from demographic data and behavioural data can enrich functionality of HCI systems while maintaining the merits of individual recognition algorithms.

Our directed advertising system has manifested excellent accuracy and robustness in the real-world tests. Tested with Microsoft Visual Studio 2012 on a computer with an Inter(R) Core(TM) i5-4570 CPU and 12GB RAM, the proposed gender and gaze gesture recognition method has exhibited excellent real-time performance with an average frame rate of 32 frames per second. It should be noted that during the face detection stage, we simply employed the Viola-Jones face detector which consumed nearly 65% of the computational time in our tests. This indicates that much higher frame rates can be achieved by employing a more efficient face detector.

User-camera distance was set at 0.5 metres, 1 metre and 1.5 metres, respectively. The alteration of user-camera distance saw negligible impact on the system performance. The reason behind this is that each face image is resized to a standard size before eye centre localisation, and that eye saccadic signals are relative values, normalised by the pupillary distance. The illumination in the tests was provided by overhead lamps (at around seven metres high), which could represent common indoor settings. NIR illumination was also employed to create a controllable under-illuminated environment. Outdoor environments with strong sunshine are simulated by over-exposed images.

The combined demographic recognition and behavioural recognition have brought higher functionality and usability to this case study HCI system. Without the gender recognition module, advertising messages cannot be tailored to user characteristics. On the other hand, without the gaze gesture recognition module, although personalised advertisements can be delivered to suit different gender groups, they cannot respond to the level of user satisfaction and attentiveness, but can only present digital content in a passive manner. More importantly, the functionality of the system is not restricted to a particular context but is applicable to the broader theme of assisting the elderly and the disabled by creating a user-centred HCI environment. Assistive systems can be designed to provide service for patients in hospitals, personal care for the elderly who live alone and the disabled in public venues, knowing their gender and knowing more about their attentions/intentions. Therefore the care and service delivered will be less general but more personalised.

@&#DISCUSSION AND CONCLUSIONS@&#

This paper explores the two popular visual modalities in HCI – gender and gaze. Three novel algorithms have been proposed which enable HCI system to fulfil assistive roles in a wide range of scenarios.

We introduce a gender recognition algorithm that adopts a type of discriminative encoded feature, namely the Fisher Vectors, to reliably predict gender from facial images. Comprehensive tests have been carried out that evaluate: 1) the FVs encoded from four different types of low level features – greyscale, LBP, LBP histogram and SIFT, 2) the impacts of algorithm parameters – image size, sampling window size, sampling stride, GMM component number and principal component number 3) pre-processing techniques – histogram equalisation and face alignment and 4) algorithm performance on controlled image data and uncontrolled image data. As a result, we conclude that the SIFT feature yields the highest gender recognition rate, 97.7% on the FERET dataset, 92.5% on the LFW database and 96.7% on the FRGCv2 database. In addition, we compare our method with eight other state-of-the-art approaches and prove the superiority of our method. We further prove the robustness of our algorithm against head pose by showing that misaligned facial images have an insignificant negative impact on the recognition accuracy (less than 1% decrease). Another merit of our gender recognition method is its ability to identify the most discriminative facial region, i.e. the regions on a face that characterise male and female groups. As found by our algorithm, the mouth, the nasolabial furrows and the forehead regions are the most discriminative. The only disadvantage of our algorithm is high memory consumption in the phase of GMM training. This is incurred by low level features that are densely sampled such that the number of descriptors is relatively large. However the prolonged training phase will not have an impact on the classification phase since we only employ a linear SVM. Although the proposed FV encoding method has yielded promising results for gender recognition, further accuracy boost can be expected by incorporating more robust and intrinsically discriminative features. To this end, in our future works, novel 3D imaging systems, 3D reconstruction methods and 3D gender recognition strategies will be explored.

As for gaze analysis, we first propose an unsupervised modular approach for eye centre localisation as the preliminary stage. This approach utilises gradient and isophote features and follows a coarse-to-fine and global-to-regional scheme. We further design a SOG filter that specifically deals with the prominent gradients from the eyelids, eyebrows and shadows which sabotage most geometrical feature based methods. Our eye centre localisation method is free from classifier training and absolute facial anthropometric relations so that it is efficient and robust. This approach has been tested on the BioID dataset and compared to 10 other state-of-the-art methods in six accuracy measures. It outperforms all the other methods in comparison by gaining the highest accuracy measure score. Apart from its high accuracy, the algorithm exhibits superior real-time performance as the two modules interact with each other and largely reduce eye centre candidates.

Building on this algorithm, we design seven gaze gestures that can be used to control a HCI system in a remote and contactless manner. We tested the gaze gesture recognition algorithm by designing a directed advertising system that, upon receiving gaze gestures issued by a user, displays advertisements in different manners. This type of system combines demographic recognition (gender) and behaviour analysis (gaze) and therefore is able to create a user-centred HCI environment by better understanding the needs and intentions of its users. We consider that these capabilities can enable advanced functionality that would offer potential to commercially implement many new advertising applications. Regarding assistive roles, the system offers huge potential in various situations and is especially valuable for enabling assistance and communication for the elderly and people with motor disabilities.

In our future works, the Fisher Vector encoding method will be extended such that reconstructed 3D faces can be explored as the source of more robust features. Novel 3D reconstruction algorithms will also be explored, accompanied by the development of other types of 2D and 3D based HCI systems suitable for use in real-world environments.

Supplementary material associated with this article can be found, in the online version, at doi:10.1016/j.cviu.2016.03.014.


                     
                        
                           video 1
                           Image, video 1
                           
                        
                     
                     
                        
                           video 2
                           Image, video 2
                           
                        
                     
                  

@&#REFERENCES@&#

