@&#MAIN-TITLE@&#Comparison of UMLS terminologies to identify risk of heart disease using clinical notes

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Our submission was the best rule-based system at the 2014 i2b2 challenge.


                        
                        
                           
                           Combining UMLS concepts and regular expressions is effective.


                        
                        
                           
                           Terminology-restricted version of our system evaluates a specific terminology.


                        
                        
                           
                           Performance comparison conclude that the Consumer Health Vocabulary was most useful.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Natural language processing

Electronic health records

Rule-based system

Unified Medical Language System

@&#ABSTRACT@&#


               
               
                  The second track of the 2014 i2b2 challenge asked participants to automatically identify risk factors for heart disease among diabetic patients using natural language processing techniques for clinical notes. This paper describes a rule-based system developed using a combination of regular expressions, concepts from the Unified Medical Language System (UMLS), and freely-available resources from the community. With a performance (F1=90.7) that is significantly higher than the median (F1=87.20) and close to the top performing system (F1=92.8), it was the best rule-based system of all the submissions in the challenge. We also used this system to evaluate the utility of different terminologies in the UMLS towards the challenge task. Of the 155 terminologies in the UMLS, 129 (76.78%) have no representation in the corpus. The Consumer Health Vocabulary had very good coverage of relevant concepts and was the most useful terminology for the challenge task. While segmenting notes into sections and lists has a significant impact on the performance, identifying negations and experiencer of the medical event results in negligible gain.
               
            

@&#INTRODUCTION@&#

Electronic medical record-based phenotyping has been a topic of great interest in the biomedical informatics community. The annual i2b2 natural language processing (NLP) challenges play a key role in the advancement of NLP for this task. These challenges also serve as a unique source of publicly-available de-identified clinical data, overcoming traditional barriers of privacy and data access that prevent comparison of different NLP approaches. The task for the second track of the 2014 i2b2/UTHealth challenge [1] asked participants to automatically identify risk factors for heart disease among diabetic patients by applying NLP techniques on longitudinally-ordered cropus of clinical notes. Participants were provided with exhaustive annotation guidelines to extract these risk factors and the temporal information associated with them. The risk factors were divided into six categories (referred to as tags, in the annotation guidelines): medications, hypertension, diabetes, coronary artery disease (CAD), hyperlipidemia, family history, smoking status and obesity. There have been i2b2 challenges in NLP based on the identification of most of these tags individually in the past [2–4]. The second track of the 2014 challenge brings these together for one comprehensive task.

We submitted a novel rule-based system as an entry for this track of the challenge. This system leverages freely-available resources such as the Unified Medical Language System (UMLS) [5], MetaMap (version 2013AB) [6], and ConText [7]. The performance of our system is largely dependent on UMLS concepts extracted from the challenge corpus of clinical notes. We used our system to evaluate the utility of different terminologies for the task of identifying risk factors for heart disease among diabetic patients. To achieve this goal, we restricted the performance of our system to concepts from specific terminologies. The performance of our terminology-restricted systems was used to measure the utility of a terminology for the task. The paper is organized as follows: The corpus used in this work is described in Section 2. A brief review of rule-based approaches using a combination of regular expressions and UMLS concepts in previous i2b2 challenges is discussed in Section 3. We describe different components of the system in Section 4 and the results in Section 5. Sections 6 and 7 compare terminologies based on concept coverage and contribution to performance. We present ablation tests analyzing the impact of different components of our system in Section 8 and a brief error analysis in Section 9. This is followed by a discussion of our results and the conclusion.

The corpus provided for the challenge consists of longitudinally ordered clinical notes from three cohorts of diabetic patients: (1) Patients who have a coronary artery disease (CAD) in all the notes, (2) Patients who never had CAD, and (3) Patients who develop CAD over the course of their notes. There are three to five notes corresponding to different times of care for each patient record. These notes belong to different categories of clinical narratives such as discharge summaries, admission notes, discharge notes and letters of communication between physicians. The corpus was divided into a training set of 790 notes (178 patients) and a testing set of 514 notes (118 patients). The participants were supplied with the training data in two stages: 521 notes (119 patients) at the beginning of the challenge, followed by 269 notes (59 patients) a month later, to generate their NLP systems. The unlabeled test set was made available at the end of two months for two days to generate and submit the system outputs.

@&#RELATED WORK@&#

Rules derived from UMLS concepts and regular expressions have been used by multiple NLP systems in the past. Here, we briefly describe top performing systems that took this approach in previous i2b2 challenges. The 2006 challenge [2] asked participants to classify patients into five categories of smoking status. Clark et al. [8] developed simple regular expressions to identify sections in notes, instances of smoking related mentions and temporal expressions. Further they used a SVM for smoking category classification. A similar approach was also used by Cohen [9]. The 2007 challenge [2] aimed at identification of obesity status of patients and associated co-morbidities using discharge summaries. Ware et al. [10] developed a system where a principal concept of interest was identified using regular expressions and then surrounding context was searched for secondary concepts. Final decisions were made using these two sets of concepts. Yang et al. [11] used lexical patterns to identify sections followed by a lexicon of UMLS concepts and associated confidence scores for obesity and disease status. The 2008 challenge [4] aimed at identification of medication information such as drug name, dosage, frequency, reason for medication, etc. from clinical notes. Deleger et al. [12] created a lexicon for drug names from various sources such as medical websites and UMLS ontologies to identify medication names. Separate lexicons were also created for each field by filtering UMLS concepts associated with specific semantic types (e.g. concepts from ‘Signs and Symptoms’ for reason). Hamon and Grabar [13] created a lexicon of medication names using RxNorm and created regular expressions to identify associated information. Meystre et al. developed a system called Textractor [14] that uses cTAKES for routine NLP tasks such as tokenization, chunking, and segmentation. Further, they used Metamap to identify drug names and their reasons and regular expressions for other fields of interest. Mork et al. [15] used systems such as Metamap, MERKI, Gopher and to extract concepts and create dictionaries for different fields of interest. Spasic et al. [16] created rules that made use of syntatic information such as noun phrases involving medication names and wrote regular expressions over them. The 2010 challenge [17] presented three tasks: concept extraction, assertion classification and relation classification. Almost all the top-performing systems for this challenge were machine learning systems. The 2011 challenge [18] asked participants to resolve co-references of noun phrases in medical records. Hinote et al. [19] created rules using dates, locations, and descriptive modifiers. They used UMLS concepts to determine closely related medical mentions. In addition, the system also used Wordnet synonyms to match words within the mentions, and results from internet search engines for identifying mentions of medical personnel. The 2012 challenge [20] focused on the identification of clinical events, and the relative ordering of the events with respect to each other and with respect to time expressions included in the records. While most of the top performing systems used rule based approaches to identify and normalize temporal expressions, the tasks of event extraction and temporal relation classification were performed using machine learning approaches. Many of these systems adapted the popular HeidelTime [21] rule-based system.

@&#METHODS@&#

Our system (Fig. 1
                        ) is an NLP pipeline, the main component of which is a rules engine that uses a combination of regular expressions and UMLS concepts. These concepts and regular expressions are semi-automatically created following the annotation guidelines provided by the challenge organizers. A clinical note is divided into sections and then into sentences using section identification and sentence chunking modules that are a part of the pre-processing stage. This is followed by application of rules to generate candidate annotations. These candidate annotations are then filtered by detecting the presence of negation and checking whether the identified rules are applicable to the patient in context (experiencer detection). Negation and experiencer detection are a part of the post-processing module. Temporal annotations for these filtered annotations are generated towards the end of the pipeline. The following sections describe each component in detail.

We created a set of UMLS concepts relevant to the annotations for each tag. The annotation guidelines provided by challenge organizers served as a resource to identify concepts of interest for every tag. For example, consider the tag DIABETES. One of the guidelines states that a mention of a diagnosis for Type 1 or Type 2 diabetes, or a pre-existing diagnosis, should generate an annotation for the tag DIABETES. However, multiple concepts in the UMLS such as ‘diabetes mellitus’ (C0011849), ‘insulin-dependent diabetes mellitus with multiple complications’ (C0348916), or ‘type II diabetes mellitus with ophthalmoplegia of right eye’ (C2171215), conforming to varying level of detail for diabetes, correspond to this description. Thus, multiple concepts in the UMLS are candidates for a single gold standard annotation. This is true for all six tags specified in the challenge.

We developed a graph-search module to traverse the UMLS graph from a given concept to its children, along the IS-A relationship, until a leaf node was reached. This gave us the set of all concepts relevant to a guideline. Following the previous example, traversing the graph from the concept diabetes mellitus (C0011849) gave us candidates for mentions of Type 1 and Type 2 diabetes. This search covered all concepts with varying granularity of detail for diabetes.

However, the IS-A relationship also connects two concepts that are not of the same semantic type and this can induce false positives. Continuing with the above example: the UMLS concepts, ‘gestational diabetes’ and ‘complication during pregnancy’ are connected through the IS-A relationship. This is simply because gestational diabetes is a complication during pregnancy. However, the concept ‘complication during pregnancy’ is not of interest for annotating mentions of diabetes. To eliminate such false positives, only concepts with the same semantic type as its parent were retained during a search. Since the semantic type of ‘complication during pregnancy’ is ‘Pathologic Function,’ which is different than ‘Disorder,’ the semantic type of ‘gestational diabetes,’ we omit this concept from the graph search. We refer to the set of all concepts from a single search as a CUI-List. Multiple CUI-Lists for each tag were created as a result of this search module, by referring to the annotation guidelines. Approximately, each instruction in the annotation guideline resulted in a CUI-List. For example: There were CUI-Lists for different revascularization procedures (PTCA, CABG, etc.), types of obesity (hypertrophic, maternal, childhood, etc.). The collection of these CUI-Lists contributes to what we refer to as the CUI-Bank henceforth. The CUI-Bank was developed in a semi-automated fashion by manually choosing different annotation-relevant concepts that were fed to the automated graph search module. We also manually annotated the ‘medication types’ for MEDICATION annotations. The final set of regular expressions and the CUI-Bank was fine-tuned over multiple runs of our system on the training data. Automating these manual steps in the development process can be an interesting line of research. Our CUI-Bank consisted of 6766 unique CUIs. Of the 9363 unique CUIs identified by MetaMap across the entire training set of 790 notes, 226 concepts had an overlap with our CUI-Bank.

Our system applies rules for extracting candidate annotations, to all notes, one sentence at a time. Therefore, a note was first divided into sections and then into constituent sentences. We identified section headings relevant to the annotation guidelines using regular expressions based on simple observations such as the length of a string when it is a section header, the usage of camel-case of upper-case and finally the set of common words that constitute a section header. This was useful in the identification of most sections.

However, there were some sections that did not have an explicit section heading. These were identified by frequent concepts occurring in such sections. For example: a medication section although not explicitly identified by the note-author as a medication section would comprise of many concepts that correspond to medication names. These concepts could be easily extracted from a CUI-List of medication names. A module was built around this intuition that helped us to identify sections without explicit section headings. A similar approach was also used to identify lists. For example, part of a note was identified as a ‘vertical medication list’ if multiple concepts from the medication CUI-List occurred on consecutive lines of a note. Similarly, part of a note was identified as a ‘horizontal medication list’ if multiple concepts from the medication CUI-List occurred as consecutive words in a sentence. Identification of medication lists in particular was very useful because the guidelines had specific instructions for temporal aspect of the annotation. For example: One of the instructions in the annotation guidelines states that if a medication is mentioned in a list, the temporal marker for that medication is always marked as “continuing” even if it occurs in sections such as “medication on admission,” or “medication on discharge.”

Identifying sections gives a coarse idea of the context for each concept. This helped us in building some section informed rules. However, to use post-processing modules such as negation and experiencer detection and other apply the rules engine the system needs segmentation at the sentence level. Therefore, each section was further chunked into sentences by using the Ling-Pipe library [22], using a model trained on the MEDLINE biomedical abstracts. Every chunk of text identified as a sentence by our text segmentation module was analyzed by the rules engine in order to identify candidate annotations.

The CUI-Bank and its constituent CUI-Lists described in Section 4.2 are leveraged by the rules engine to generate candidate annotations. The rules engine consists of three types of rules to identify occurrences of risk factors.
                           
                              1.
                              
                                 UMLS concept-based rules: The CUI-Bank is a repository of concepts that are relevant to the task. Presence of these concepts in a note under certain conditions can result in generation of candidate annotations. MetaMap [6] is a tool to identify medical concepts from text. Concept based rules generate candidate annotations if concepts identified by MetaMap are present in the CUI-Bank. These rules are completely based on UMLS concepts and do not rely on regular expressions. For example: the occurrence of any concept from the CUI-List described in Section 4.2 is a candidate annotation for the mention of diabetes. These concept-based rules may not be limited to a single concept.

For example: occurrence of the concept ‘deny’ followed by any concept from a CUI-List for the SMOKER tag in the same sentence, within a predefined word window, can be used to generate annotations for the SMOKER tag. The CUI-List for SMOKER tag could contain concepts such as ‘smoking,’ ‘tobacco,’ ‘cigarette’ and ‘pipe.’ A single rule can thus capture instances such as “denies smoking,” “denies use of tobacco,” “denies consuming cigarettes,” and “denies smoking a pipe.” Such rules benefit from capabilities of MetaMap that can account for lexical variations of words and map ‘denies’ and ‘denied’ to the same concept or ‘Cigarette’ and ‘cigarettes’ to same concept respectively. This concept-based rule can be easily extended to other rules by using the same CUI-List (smoking, tobacco, cigarette, pipe) and replacing the concept for ‘deny’ by concepts for ‘quit,’ ‘continue,’ or ‘accept.’ Concepts which are identified by MetaMap but are not present in the CUI-Bank, never satisfy a rule and hence do not generate annotations.


                                 Regular expression-based rules: Although MetaMap can associate a number of variations of text for a given concept; it can fail in the event of irregular language, such as non-standard abbreviations and misspellings, which are a characteristic of clinical notes. For example, healthcare providers often refer to type 2 diabetes by expressions such as “DM2” or “DMT2;” MetaMap fails to associate a UMLS concept with such texts. We wrote regular expressions to account for such cases. Text matching such regular expressions generated additional candidate annotations. Note that these rules are purely based on regular expressions and do not rely on any UMLS concepts.


                                 Rules using both UMLS concepts and regular expressions: The expressive power of a rule can be broadened by using a combination of UMLS concepts and regular expressions. We demonstrate this through an illustrative example. One of the indicators for the DIABETES tag is an A1c test value of more than 6.5. This can be captured using a combination of multiple concepts from a UMLS sub-graph followed by multiple regular expressions. Fig. 2
                                  shows one such sub-graph with concepts in blue color. In addition regular expressions are required to capture the value of 6.5 or more. Two such regular expressions are shown in red color. The edges in the concept graph represent relations specific to a terminology. These are shown in green and blue for Columbia Presbyterian Medical (CPM) dictionary and the MEDCIN terminology respectively. Thus textual occurrences that map to any of these four concepts, followed by any of the two regular expressions will result in the rule being satisfied. It should be noted that Fig. 2 is just an illustration to convey the idea in limited space constraints of the paper. In reality, the number and size of actual sub-graphs as well as the number of regular expressions for different rules is quite large.

Candidate annotations generated by rules described in the previous section may generate false positives since the concept of interest may be reported as a negated finding in the note (e.g. “The patient is ruled out for diabetes”). The concept may also be associated with someone other than the patient, as is commonly found in the family history section of a typical note. We used ConText [7] to identify the experiencer of the concept and filtered the candidate annotations for false positives. In addition, we created custom rules to identify negated findings to cover cases not covered by ConText. We also created rules using ConText to identify true positives for the FAMILY_HISTORY tag.

An important aspect of the task description was the identification of a temporal marker for every annotation. Each temporal marker in the annotation was relative to the document creation time (DCT) for that note and could be one of the following: “before DCT,” “after DCT,” “during DCT,” “continuing” or “not mentioned.” Temporal analysis is the last step in our NLP pipeline. The rules engine generates candidate annotations on pre-processed notes, which are analyzed for negation and experiencer detection to remove false positives in the post-processing module. All annotations at this stage of the pipeline are considered final system annotations and a temporal marker is assigned to each annotation. The challenge organizers provided guidelines for temporal annotations. These were either based on the language used in the notes or on the structure of the document. Language based temporal annotations were handled using regular expression-based rules. While creating these rules, we also factored in the output of ConText, which identifies temporality of a concept in a given sentence. This was particularly useful for the “during DCT” annotation. Temporal annotations based on document structure were identified by leveraging the output of the pre-processing step. For example: Mentions of diabetes, CAD, hypertension in the history section of a note account for the temporal annotation “continuing.” Similarly lab values account for the annotation “before DCT.” The annotation guidelines for the MEDICATION tag specify that “if a medication is mentioned in a list, the assumption has to be that the patient was on them before and during the DCT.” We wrote rules for temporal annotations using identified sections and lists as described in Section 4.3.

@&#RESULTS@&#

As mentioned in Section 2, the training data consisted of 790 notes across 178 patients, while the testing data consisted of 514 notes across 118 patients. There were 2–5 notes for every patient. Each gold standard annotation belonged to one of six tags: medications, hypertension, diabetes, CAD, hyperlipidemia, family history, smoking status and obesity.


                     Table 1
                      summarizes the number of annotations for every tag in the corpus. The annotation for every tag has one, two or three attributes. Medications account for more than half of the annotations in both the training and the testing data. Annotations for the MEDICATION tag involved specifying the type (one of 22 possible types) of drug as an attribute. Combination drugs had two types. Annotations for SMOKER and FAMILY_HIST tags were sparse and had only one attribute. Most annotations for the SMOKER tag were “Unknown” and for FAMILY_HIST were “Not present.” The reader is referred to [1] for a detailed description of the annotations.

The competition results were graded based on the manually annotated gold standard using a micro-averaged F-Score. Table 2
                      summarizes the overall performance of our system on the training and test data and across all tags. We can observe that the performance numbers on the training data are very close to that of testing data. This demonstrates that the rules in our system are robust and generalize well.

Owing to the domain-specific nature of tasks, biomedical NLP systems today rely on mapping text to concepts from UMLS terminologies, as a part of their pre-processing. Most of these terminologies have been constructed for specific applications and are hence often complementary rather than being comprehensive. Although, there are a few comprehensive terminologies, restricting concept search to them favors precision over recall [23] of the identified concepts and result in a compromised performance of the NLP system. Therefore, identifying all possible concepts from most terminologies is an approach used by NLP systems although it is very inefficient (Systematized Nomenclature of Medicine – Clinical Terms: SNOMEDCT_US, contains more than 300,000 concepts) and consumes a significant amount of time. However, the eventual goal of systems developed through shared tasks such as the i2b2 challenges is to use them for practical applications of cohort discovery in hospitals. If objectives such as - identifying risk of heart disease in diabetic patients, are to be achieved on a large scale, it is necessary to ensure that all steps in the NLP pipeline scale well and execute efficiently. Research studies in the past have examined utility of individual terminologies [24,25] for specific issues such as coding medical problems and analyzing completeness. Terminologies have also been examined for aspects of structure and relationships [26]. Very few studies [27–29] have compared multiple terminologies for coverage and utility for an end to end NLP task.

In the following two sections we address this issue and present a comprehensive comparison of UMLS terminologies for coverage and utility in clinical notes for the challenge task. We used MetaMap to identify medical concepts in the corpus. Concepts that are a part of the concept-based rules are responsible for contributing towards the performance of the system. However, there are many other concepts identified in the corpus. We considered all concepts identified by MetaMap and found the terminologies to which they belong. Of the 155 sources in the 2013AB version of the UMLS, we found that only 36 (23.22%) had a concept in the training data. This suggests that it is unnecessary to search the full UMLS for the NLP task of identifying risk of heart disease in diabetic patients.

Further, coverage of a terminology was calculated as a percentage value normalizing the count of all concepts in that terminology by the count of all concepts in the training data.
                        
                           
                              
                                 
                                    
                                       Terminology Coverage
                                    
                                    
                                       unrestricted
                                    
                                 
                                 =
                                 
                                    
                                       count
                                       (
                                       Terminology concepts
                                       )
                                    
                                    
                                       count
                                       (
                                       All concepts
                                       )
                                    
                                 
                              
                           
                        
                     
                  

Note that the denominator in the above equation is the count of all possible concepts in the corpus. The calculated coverage thus represents the generic utility of a terminology for a corpus of clinical notes similar to that the one made available for the challenge. Table 3a
                      lists the top five terminologies ranked based on their coverage in the test data. It should be noted that many concepts overlap across terminologies. We illustrate this through a Venn diagram in Fig. 3
                     .

The CUI-Bank (Section 4.2) is a set of concepts curated by searching the graph of UMLS concepts and hand curation based on instructions in the annotation guidelines. In addition to the calculation of coverage where the denominator is all possible concepts in the corpus, we also calculated coverage of concepts restricted to our CUI-Bank (Table 3b
                     ).
                        
                           
                              
                                 
                                    
                                       Terminology Coverage
                                    
                                    
                                       CUI
                                       _
                                       Bank
                                    
                                 
                                 =
                                 
                                    
                                       count
                                       (
                                       Terminology concepts in CUI
                                       _
                                       Bank
                                       )
                                    
                                    
                                       count
                                       (
                                       Concepts in CUI
                                       _
                                       Bank
                                       )
                                    
                                 
                              
                           
                        
                     
                  

Since concepts in the CUI-Bank are restricted by their relevance to the task of the challenge, this quantity demonstrates possible utility of concepts from a terminology for the task. For example, RxNorm is a terminology specific to medications. Although it does not appear in Table 3a, it does appear in Table 3b. This shows that medication concepts identified by RxNorm are useful for the task. This is also evident from Table 1, which shows that more than half of the annotations belong to the MEDICATION tag. Although the Consumer Health Vocabulary (CHV) is comprised of fewer concepts than the others in the top five, it has a very good coverage. Wu et al. [27] compared different terminologies based on the occurrence of UMLS concepts on a large-scale corpus of clinical notes (51million documents) from the Mayo Clinic. However, they used a different metric to measure utility of a terminology. They defined degree of utilization by the number of unique term strings from a terminology identified in a given corpus. They reported that CHV followed by SNOMEDCT had the highest degree of utilization in their corpus. Our observations about most terminologies (and hence concepts) never having a representation in clinical notes, and CHV being a very useful terminology are consistent with their study. As observed by Brennan and Aronson [30] in their study of consumer vocabulary in email messages by 6 nursing terminologies, we believe this is primarily because the CHV address a particular part of the patient experience not addressed in other health care vocabularies.

The previous section analyzes possible utility of terminologies in the challenge task. In this section, we calculated the actual utility of a terminology for the task. We restricted the concepts used in our rules to a specific terminology and evaluated the performance of our system that would use only the UMLS concept based rules. For example: Consider a simple concept-based rule that identifies mention of diabetes using a CUI-List as described in Section 4.2. The graph search will traverse the UMLS graph along the IS-A relation and generate a CUI-List to yield concepts that are descriptions of diabetes at varying levels of detail. These concepts belong to different terminologies in the UMLS. If we restrict our search to a particular terminology, then concepts from other terminologies would not be captured in the search. Elaborating on the example in Fig. 2, if we restrict the terminology only to MEDCIN, the rule will capture only 2 of the 4 concepts in the sub-graph. Thus, the power of concept-based rules is reduced by restricting the search to a particular terminology and impacts performance accordingly.

This performance serves as an indicator of the utility of that ontology towards the goal of the task. We found that CHV outperformed other ontologies. Table 4
                      summarizes the performance of terminology-restricted systems with highest coverage in the CUI-Bank, ranked by their micro-averaged F-score. Values in bold for a column indicate the highest performance for that tag. We can observe that concept-based rules did not perform well for the task of identifying family history. Obviously, RxNorm performs extremely well for identifying medications, but not for any other tags. SNOMEDCT_US, although considered as the most comprehensive terminology, does not perform well in identifying medications and hence ranks fourth.

We performed ablation analysis of the various components we used in system. This allowed us to analyze the contribution of different components of the system. Table 5
                      summarizes the impact of various components on the performance of our system based on its various components. Bold faced values indicate highest precision, recall and F1-scores in corresponding columns. We found that section identification and list identification had a significant impact on the performance of the system. Surprisingly, ConText and its constituent modules did not have a major impact on the system performance. Our system relies heavily on MetaMap. This is evident through the last entry in Table 5. This is the performance of our system using only regular expression-based rules. While such a system is very precise, it has low recall. The role of concepts is thus to increase recall of the system.

The system had a total of 54 rules to generate candidate annotations and 18 rules for post-processing. A total of 93 regular expressions were written for generating candidate annotations, with most of them accounting for the MEDICATION tag and least for the OBESITY tag. We performed an error analysis of the system and found that most errors in the test set originated from the MEDICATION tag. This is because the number of annotations for the MEDICATION tag is substantially larger compared to other tags. Misspelling of common medications was one of the leading causes of false negatives, e.g. “simavstatin” instead of “simvastatin.” In other cases, medicines not included in our CUI-Bank also caused false negatives (e.g. Lopid 600mg). Some extraneous entries in our CUI-Bank (e.g., Potassium Citrate as a diuretic) also resulted in false positives. Negations were tricky to capture in some cases for ConText e.g., “We discussed insulin and he really is quite against it” was marked as insulin and resulted in a false positive.

As per the annotation guidelines, each tag has a set of indicators that help in its identification when it is present. For example: The tag DIABETES has “high glucose,” “high A1c,” and a diagnosis “mention” in the note as its indicators. CAD and SMOKER tags rank a distant second and third in terms of number of errors. On comparing the errors across individual indicators for the CAD tag, we observed that the system did not perform well for detecting “symptom” and “test” indicators but captured annotations for “event” and “mention” indicators. In particular, our rules failed to capture ischemic evidence associated with tests resulting in false negatives. Multiple false positives resulted for CAD events and tests. e.g. “Will continue qd ASA for CAD/stroke prevention” was marked as a mention of CAD, which has been negated by the word “prevention.” False negatives were a result of incompleteness with our CUI-Bank. For example: Although “chest pain” is present in the CUI-Bank, the concept “chest tightness” is absent. Occasionally, some values are mentioned in pseudo tables in plain text. In that case, there is no context in the given sentence to identify what the numbers mean: for e.g. in “211/45/4.69 4/84.” where 211/45 is the heart rate was not captured and resulted in a false negative. Annotations for the SMOKER tag are very sparse in the test dataset. While the system could correctly capture the ’ever’ indicator, it underperformed in the ’current’ and ’never’ indicators. This was because of sentences such as “smokes but wants to quit” which are detected by the system as the ’past’ indicator due to the word ’quit.’ Our system did not captured any instances where the patient had a positive family history.

For the DIABETES, HYPERTENSION and HYPERLIPIDEMIA tags, there are errors with the regular expressions used to detect high A1c. e.g. “HGBHA1C on 8/17=6.4” was marked as high A1c due to the date being misinterpreted by the rules as an actual A1c value. The failure to identify the proper temporal attribute is also a cause of false positives and corresponding false negatives across all tags. For example: the temporal identifier for “11/95 complaints of chest tightness and blood pressure above 230mmHg systolic.” is incorrectly categorized as “during DCT” instead of “before DCT.” The de-identified date “11/95” is not captured by the temporal rules in our system. Errors also stemmed from incorrect identification of negation. For example: Consider the sentence: “This is a 59-year-old man with metabolic syndrome, diabetes type 2, hyperlipidemia, hypertension, obesity, and mild coronary artery disease that has been medically managed.” The phrase “has been medically managed,” negates multiple terms in the sentence, which are identified as false positives by the system. Errors also resulted from chunking in MetaMap. For example: In the sentence “DIAGNOSIS: Non-Q-wave, non-ST-elevation myocardial infarction.” MetaMap correctly identified the longer and more specific term “Non-Q-wave, non-ST-elevation myocardial infarction.” However, the system missed on the generic term “myocardial infarction” which is also annotated in the test set.

As can be inferred from the description of errors above, most of the errors can be addressed by addition of rules into the system. However, certain negations and contrapositives will require investigation of sentence structure, which is not currently a part of our system. The system currently builds CUI-Bank only using the training data made available for the challenge. This could be bootstrapped by using additional unlabeled data or even data available from the Web. Using a spell-checker could make the system more robust and result in slight improvements. Addition of rules to the temporality and negation identification modules of ConText could also result in improvements.

@&#CONCLUSION@&#

In this paper, we described a rule-based system designed to participate in the 2014 i2b2 challenge for identifying risk factors of heart disease in diabetic patients. With a performance (F1=90.7) that is significantly higher than the median (87.20) and close to the best performing system (F1=92.8), it was the best rule-based system of all the submissions in the challenge. The rules in our system use a combination of concepts from the UMLS and regular expressions. While regular expressions add to precision, the use of UMLS concepts increased recall. Amongst the different terminologies in the UMLS, the Consumer Health Vocabulary (CHV) had a good coverage of concepts that are relevant to the task of the challenge. A terminology-restricted version of our system using CHV also performed better than other terminologies. We believe that such analysis comparing usefulness of different terminologies would be helpful for other researchers building similar systems in future.

The authors declare that they have no competing interest.

@&#ACKNOWLEDGMENTS@&#

Research reported in this publication was supported by the National Library of Medicine of the National Institutes of Health under Award Number R01LM011116. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.jbi.2015.08.025.


                     
                        
                           Supplementary data 1
                           
                        
                     
                  

@&#REFERENCES@&#

