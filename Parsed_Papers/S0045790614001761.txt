@&#MAIN-TITLE@&#Fast and efficient lossless adaptive compression scheme for wireless sensor networks

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A fast and low memory data compression scheme is proposed for WSNs.


                        
                        
                           
                           The scheme performs compression losslessly using 8 variable-length code options.


                        
                        
                           
                           The scheme which uses a very simple data model is fast and computationally simple.


                        
                        
                           
                           The scheme is efficient and requires no coding dictionary.


                        
                        
                           
                           The scheme achieve better compression performance than previously proposed schemes.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

@&#ABSTRACT@&#


               
               
                  The number of wireless sensor network deployments for real-life applications has rapidly increased in recent years. However, power consumption is a critical problem affecting the lifetime of wireless sensor networks (WSNs). A number of techniques have been proposed to solve this power problem. Among the proposed techniques, data compression scheme is one that can be used to reduce the volume of data to be transmitted. This paper therefore proposes a fast and efficient lossless adaptive compression scheme (FELACS) for WSNs. FELACS was proposed to enable a fast and low memory compression algorithm for WSNs. FELACS generates its coding tables on the fly and compresses data very fast. FELACS is lightweight, robust to packet losses and has very low complexity. FELACS achieved compression rates of 4.11 bits per sample. In addition, it achieved power savings up to 70.61% using the real-world test datasets.
               
            

@&#INTRODUCTION@&#

Wireless sensor networks (WSNs) are very large scale deployments of tiny smart wireless sensor devices working together to monitor a region and to collect data about the environment. Sensor nodes are generally self-organized and they communicate with each other wirelessly to perform a common task. The nodes are deployed in large quantities (from tens to thousands) and scattered randomly in an ad-hoc manner in the sensor field (a large geographic area). Through advanced mesh networking protocols, these sensor nodes form a wide area of connectivity that extends the reach of cyberspace out into the physical world. Data collected by each sensor node is transferred wirelessly to the sink either directly or through multi-hop communication. WSNs have unlimited applicability. They find application in areas such as environmental monitoring, industrial monitoring, health and wellness monitoring, seismic and structural monitoring, inventory location monitoring, surveillance, power monitoring, factory and process automation, object tracking, precision agriculture, disaster management, and equipment diagnostics [1–6]. These monitoring applications involve the collection of large volumes of raw data over time. Thus, collecting high-fidelity data for these monitoring applications considering the limitations of WSNs presents a key challenge.

WSNs operate under tight energy budgets and have other limitations that include limited radio bandwidth, limited memory, limited computational capability, limited packet size and high packet loss rates. Data compression is one tool that can be used to effectively manage these limited resources of the nodes to deliver high-fidelity high data return to the sink node over low rate and unreliable radio links. Saving a byte of data through data compression has been shown to worth spending between four thousand (using Chipcon CC2420 transceiver) to two million (using MaxStream XTend transceiver) cycles of computation [7]. The amount of energy required for transmission over time can be greatly minimized if we can reduce the volume of data to be transmitted. The reduction in data size will further lead to considerable savings in power. Therefore, compressing data before transmission is one of the major strategies for energy-efficient WSNs.

In this paper, we present a lightweight block-based lossless adaptive compression scheme called fast and efficient lossless adaptive compression scheme (FELACS) based on Golomb–Rice codes for WSNs. Since sensor nodes have relatively low computational power, tight energy budgets and limited hardware resources, our focus in the design of FELACS is that, it must have very low complexity. FELACS compresses each block of source data independently and thus provide significant robustness to packet losses that may occur due to congestion, interference and multipath effects. Naturally, Golomb–Rice codes are fast to implement and requires no coding table. As such, they have been used in many application areas [8]. For each application, a different Golomb–Rice parameter estimation technique is normally proposed to suit the particular application. Therefore, in this paper, we devised a simple, fast and efficient Golomb–Rice parameter estimation method that makes the implementation of Golomb–Rice codes very suitable for WSNs.

The rest of the paper is organized as follows. In Section 2, we review related work. In Section 3, we discuss the original Rice coding algorithm. In addition, we introduce our proposed FELACS algorithm. Next, we discuss Lossless Entropy Compression (LEC) algorithm. In Section 4, we present a detailed performance evaluation of our proposed scheme in comparison to other schemes including state-of-the-art Rice algorithm and other lossless schemes proposed specifically for WSNs. Finally, in Section 5, we present our conclusions.

@&#RELATED WORK@&#

In the current literature, we find two main categories of data compression schemes: (i) distributed data compression schemes, and (ii) local data compression schemes. The distributed data compression approach exploits the high spatial correlation in data from fixed sensors node in dense networks. Some of the main techniques under this approach include distributed source coding (DSC) [9], distributed transform coding (DTC) [10], distributed source modeling (DSM) [11] and compressed sensing (CS) [12]. The distributed compression approach however conserves energy at the expense of information loss in the source data and for this reason will not be considered. Instead the local data compression approach that takes advantage of the temporal correlation that exist in sampled sensor data to perform its compression locally on each sensor node will be considered. Some of the proposed local data compression algorithms based on temporal correlation in WSNs include: lossy algorithms (Lightweight Temporal Compression (LTC) [13], Differential Pulse Code Modulation-based Optimization (DPCM-Optimization) [14]); lossless algorithms (Sensor-Lempel–Ziv–Welch (S-LZW) algorithm [7], Lossless Entropy Compression (LEC) [2], Modified Adaptive Huffman Compression Scheme [3]). The precision required by some application domains demands sensor nodes with very high accuracy that cannot tolerate measured data being corrupted by the compression process. Thus, in this section, we will focus on lossless local data compression algorithms.

The authors in [7] introduced a lossless compression algorithm called S-LZW which is an adapted version of Lempel–Ziv–Welch (LZW) [15] designed specifically for resource constrained sensor nodes. S-LZW is a dictionary-based low complexity lossless compression algorithm that divides the uncompressed input bit streams into fixed size blocks of 528 bytes (two flash pages) and compresses each block separately. The dictionary structure allows the algorithm to adapt to changes in the input and to take advantage of repetition in the sensed data. However, the algorithm suffers from the growing dictionary problem and its compression efficiency still needs to be improved. Also, it is less robust to packet losses because it encodes 528 bytes of source data into 10 or more dependent packets. Hence, once a packet is lost in transmission, the remaining packets following in the same group cannot be decoded.

In [2], the authors introduced Huffman coding into wireless sensor nodes. Their simple LEC algorithm which was based on static Huffman coding, exploits the temporal correlation that exists in sensor data to compute a compressed version using an extended coding table that was used in the baseline JPEG (Joint Photographic Experts Group) algorithm for compressing the DC coefficients. The algorithm was particularly suitable for computational and memory resource constrained sensor nodes. The algorithm gives good compression performance. However, the algorithm is static and cannot adapt to changes in the source data statistics. Also, another fundamental drawback of the LEC scheme is the interdependence of one packet on the other. As such, once a packet is lost in transmission, all other packets following cannot be decoded.

In paper [3], the proposed algorithm was a modified version of the classical adaptive Huffman coding. The algorithm does not require prior knowledge of the statistics of the source data and compression is performed adaptively based on the temporal correlation that exists in the source data. The drawback of this algorithm is that it is computationally intensive and it is not robust to packet losses.

FELACS is therefore proposed to address all the drawbacks of these algorithms. FELACS compresses data very fast since it is based on Golomb–Rice coding which have been shown in the literature to encode data faster than Arithmetic, Huffman and adaptive Huffman coding. FELACS gives good compression better than that obtained by all the algorithms discussed earlier in this section. FELACS requires very low computational resource and it encodes each block of source data independently. Thus, the algorithm is very robust to packet losses.

In this section, we discussed three lossless compression schemes namely the state-of-the-art Rice algorithm (original Rice coding algorithm), the proposed FELACS algorithm and the LEC algorithm.

Rice codes which are also referred to as Golomb–Rice codes consist of an infinite family of prefix codes, which are optimal for encoding symbols of exponential probability distribution. Golomb–Rice codes is a subset of an infinite family of exponential prefix codes called Golomb codes which were first discussed by Golomb [16]. Golomb codes are parameterized by a non-negative integer parameter m. Thus, given the value of m, the mth Golomb code [16] encode non-negative integer δ by encoding 
                           
                              ⌊
                              δ
                              /
                              m
                              ⌋
                           
                         in unary, then encoding δ mod m using an adjusted binary code for the range [0, m
                        −1]. Golomb codes are shown to be optimal for geometrically distributed sources. Golomb–Rice codes were independently rediscovered by Rice [17] as the special case of Golomb codes where m
                        =2
                           k
                         for some non-negative integer k. As noted in [16], coding becomes exceptionally simple when m is a power of 2. Given k, the codeword prefix for the integer δ consists of the unary representation of 
                           
                              ⌊
                              δ
                              /
                              
                                 
                                    2
                                 
                                 
                                    k
                                 
                              
                              ⌋
                           
                         which is 
                           
                              ⌊
                              δ
                              /
                              
                                 
                                    2
                                 
                                 
                                    k
                                 
                              
                              ⌋
                           
                         zeros followed by a one and the codeword suffix consists of the k least significant bits of the binary representation of δ (i.e. δ mod 2
                           k
                         using k-bits binary representation). Generating codeword for Golomb–Rice codes is very simple, since instead of the division δ/2
                           k
                         we just shift δ right k bits, and instead of modulo reduction (δ mod 2
                           k
                        ), we simply take the k least significant bits of δ and output them directly [8]. Our focus in this paper is on the use of Golomb–Rice codes in WSNs, since it has been used in a variety of applications because of its coding simplicity, efficiency and speed.

In the original Rice coding algorithm [18], encoding a given sequence of N-bits symbols requires Golomb–Rice family of N
                        −1 codes (k
                        =0, 1, 2…
                        N
                        −2) plus one natural binary code (that transmits the original bits to ensure that the symbol is not expanded). Thus, for any symbol represented by N-bits, at least one optimal k
                        ∊{0, 1, 2…
                        N
                        −2, N} can be found that gives the minimal code length. Note that whenever k
                        =
                        N, the natural binary code option is used. In Rice coding, these N code options are adaptively used to encode blocks of the source samples. The original Rice coding implementation uses 16 samples per block based on its initial area of application [18,19]. Note that, for each optimal k selected for encoding a block of samples, an identification (ID) bit pattern which is the representation of k in natural binary code using 
                           
                              ⌈
                              
                                 
                                    log
                                 
                                 
                                    2
                                 
                              
                              N
                              ⌉
                           
                         bits is transmitted together with the block’s encoded bit stream to indicate to the decoder which code option was used to encode that block of samples. Thus, Rice coding can adapt to changing source statistics since different code option can be selected for each block. A data model is used for the selection of the optimal k value for each block. In the original Rice coding algorithm, data modeling is done through an exhaustive search among the allowable k values (brute-force coding approach). Consequently, the complexity of the original Rice coding algorithm was generally high since its performance depends on the individual evaluation of each code option for every block of the source samples. Thus, the key factor behind the effective use of Golomb–Rice codes is the estimation of the coding parameter k to be used for a given sample or block of samples. In the next section, we devise a simple method for the estimation of Golomb–Rice coding parameter k that was very suitable for implementation in WSNs.

In this section, we describe the fast and efficient lossless adaptive compression scheme (FELACS). FELACS is a block-based lossless adaptive compression scheme based on the Golomb–Rice coding method for WSNs. We assumed that each original sample value is in the range [0, 2
                           N
                        
                        −1] where N is the analog-to-digital converter (ADC) resolution of the sensor node. In designing FELACS, we made the following modifications to the original Rice coding (Golomb–Rice coding) algorithm to make it suitable for implementation in WSNs.
                           
                              1.
                              After a careful study of the frequency distribution plots of the residues of some wireless sensor network data sets, we discovered that the source alphabet size can be truncated to 28. Consequently, in our implementation of FELACS, we limit our code options choice to Golomb–Rice family of 8 codes (with k
                                 =0, 1, 2…7) only. This modification helps us to keep the overhead cost due to the ID bit pattern as minimal as possible (3-bits each). In addition, using only these Golomb–Rice family of 8 codes (with k
                                 =0, 1, 2…7), data sets with source entropy in the range 
                                    
                                       1.5
                                       ⩽
                                       
                                          
                                             
                                                
                                                   H
                                                
                                                
                                                   ‾
                                                
                                             
                                          
                                          
                                             δ
                                          
                                       
                                       ⩽
                                       9.5
                                    
                                  
                                 [18] will be effectively compressed.

The radio links may experience frequent packet losses due to interference, congestion, and multipath effects. Thus, to provide significant robustness to packet losses, we encode each block of source data independently. Hence, each block is preprocessed independently. The first sample in each block is the reference sample and is transmitted directly in N-bits. The remaining samples in each block are then encoded together using the best code option and appended to the N-bit natural binary code of the reference sample. This ensures that there is significant robustness to packet losses. This way, each packet is uniquely decodable.

An ID bit pattern using 
                                    
                                       ⌈
                                       
                                          
                                             log
                                          
                                          
                                             2
                                          
                                       
                                       8
                                       ⌉
                                       =
                                       3
                                    
                                  bits binary code is used to indicate to the decoder which code option was used in encoding the block. For example, if the code option with k
                                 =2 is selected, the corresponding ID bit pattern is 010.

Unlike the original Rice algorithm that determines the optimal k value through individual evaluation of each code option for every block of source samples, we devise a simple method that uses the sum of the samples in each block of source samples to determine the optimal k value. Note that in taking the sum of the samples in a block, the first sample being a reference sample is excluded from the summation. Our proposed simple optimum code parameter selection method is efficient and fast, requiring only at most 7 bit-shift operations for each block of source samples to determine the optimal k value.

This simple optimum code parameter selection method is at the heart of our proposed FELACS algorithm and thus, it is discussed in details in the next section. Sensor nodes have relatively low computational power, tight energy budgets and limited hardware resources. Our focus in the design of FELACS is that it must have very low complexity. The block scheme of FELACS is given in Fig. 1
                        .

In this section, using a heuristic approach, we devised a simple means of estimating the non-negative Golomb–Rice coding parameter k that requires minimal computation in its determination so as to make it very suitable for implementation in WSNs. In Golomb–Rice coding, encoding a non-negative integer variable δ using Golomb–Rice code of parameter k requires k bits for the split-bit part and 
                              
                                 ⌊
                                 δ
                                 /
                                 
                                    
                                       2
                                    
                                    
                                       k
                                    
                                 
                                 ⌋
                                 +
                                 1
                              
                            bits for the unary part. Thus, if we denote the code length of the Golomb–Rice code of parameter k for any non-negative integer variable δ by ℓ
                              k
                           , then
                              
                                 (1)
                                 
                                    
                                       
                                          ℓ
                                       
                                       
                                          k
                                       
                                    
                                    =
                                    ⌊
                                    δ
                                    /
                                    
                                       
                                          2
                                       
                                       
                                          k
                                       
                                    
                                    ⌋
                                    +
                                    1
                                    +
                                    k
                                    .
                                 
                              
                           
                        

Our problem now is to determine the optimum k (k
                           *) that yields the minimum code length for every fixed non-negative integer δ. Using (1), the minimum code length and the corresponding optimum coding parameter k
                           * of some non-negative integer variable δ were computed and recorded in Table 1
                           . A critical study of Table 1 reveals the followings:
                              
                                 1.
                                 That given δ, any non-negative integer p satisfying


                           
                              
                                 2.
                                 Denoting the minimum code length of Golomb–Rice code at the optimum coding parameter k
                                    * for the non-negative integer δ by 
                                       
                                          
                                             
                                                ℓ
                                             
                                             
                                                
                                                   
                                                      k
                                                   
                                                   
                                                      ∗
                                                   
                                                
                                             
                                          
                                       
                                    , we observe from Table 1 that

What this means is that 
                              
                                 
                                    
                                       ℓ
                                    
                                    
                                       
                                          
                                             k
                                          
                                          
                                             ∗
                                          
                                       
                                    
                                 
                              
                            is a global minimum and any non-negative integer k
                           * that satisfies (3) is the optimum choice of Golomb–Rice coding parameter. ℓ−1 should be treated as ∞.

Suppose δ
                           1, δ
                           2, δ
                           3
                           …
                           δJ
                            are the J samples in a block of data. For a highly correlated data set and small block size J, we assume that the entire δi
                           s in a block are within the same range defined by (2). That is:
                              
                                 (4)
                                 
                                    
                                       
                                          2
                                       
                                       
                                          p
                                       
                                    
                                    <
                                    
                                       
                                          δ
                                       
                                       
                                          i
                                       
                                    
                                    ⩽
                                    
                                       
                                          2
                                       
                                       
                                          p
                                          +
                                          1
                                       
                                    
                                    ,
                                 
                              
                           where i
                           =1, 2, 3…
                           J.

By summing the entire δi
                           s in the block together, (4) then becomes
                              
                                 (5)
                                 
                                    J
                                    ×
                                    
                                       
                                          2
                                       
                                       
                                          p
                                       
                                    
                                    <
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             J
                                          
                                       
                                    
                                    
                                       
                                          δ
                                       
                                       
                                          i
                                       
                                    
                                    ⩽
                                    J
                                    ×
                                    
                                       
                                          2
                                       
                                       
                                          p
                                          +
                                          1
                                       
                                    
                                    .
                                 
                              
                           
                        

For example, if in a block of 4 samples we have δ
                           1, δ
                           2, δ
                           3 and δ
                           4 given as 9, 11, 13 and 15 respectively. From (4) the samples are all within the range 
                              
                                 
                                    
                                       2
                                    
                                    
                                       3
                                    
                                 
                                 <
                                 
                                    
                                       δ
                                    
                                    
                                       i
                                    
                                 
                                 ⩽
                                 
                                    
                                       2
                                    
                                    
                                       4
                                    
                                 
                                 .
                              
                            Their sum 9+11+13+15=48 according to (5) should be within the range 
                              
                                 4
                                 ×
                                 
                                    
                                       2
                                    
                                    
                                       3
                                    
                                 
                                 <
                                 48
                                 ⩽
                                 4
                                 ×
                                 
                                    
                                       2
                                    
                                    
                                       4
                                    
                                 
                              
                           , that reduces to 
                              
                                 32
                                 <
                                 48
                                 ⩽
                                 64
                              
                            which is true.

Thus, even though the assumption that we made is not very accurate for some data sources that we came across in practice, it still leads to a very simple Golomb–Rice parameter estimation method that gives good performance. The good performance is made possible by virtue of (3). For instance, even though some of the δi
                           s in a block might not be within the range defined by (2) but in neighbouring ranges (either above or below), some of them are still being encoded efficiently by virtue of (3) and which is also obvious from the second column of Table 1. Thus, to select Golomb–Rice code for a block of J samples δ
                           1, δ
                           2, δ
                           3
                           …
                           δJ
                           , we simply compute the sum of the sample values in that block. That is, if the sum is defined as
                              
                                 (6)
                                 
                                    D
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             J
                                          
                                       
                                    
                                    
                                       
                                          δ
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                 
                              
                           then (5) becomes
                              
                                 (7)
                                 
                                    J
                                    ×
                                    
                                       
                                          2
                                       
                                       
                                          p
                                       
                                    
                                    <
                                    D
                                    ⩽
                                    J
                                    ×
                                    
                                       
                                          2
                                       
                                       
                                          p
                                          +
                                          1
                                       
                                    
                                    .
                                 
                              
                           
                        

Next, we compare the sum D with a list of predefined decision regions that were computed and recorded in Table 2
                            using (7). For example, whenever 
                              
                                 D
                                 ⩽
                                 2
                                 J
                              
                            then the optimum coding parameter k
                           *
                           =
                           p
                           =0.

Using our proposed method, the optimum coding parameter k
                           * can be computed on the fly without the need for lookup table using only bit-shift operations since the decision regions boundaries occur at J multiplied by a power of 2 as can be seen from (7) and Table 2. Note that (7) can be implemented using simple C source code as
                              
                                 
                                    for
                                    (
                                    p
                                    =
                                    0
                                    ;
                                    (
                                    J
                                    ≪
                                    p
                                    )
                                    ⩽
                                    D
                                    ;
                                    p
                                    +
                                    +
                                    )
                                 
                              
                           
                        

Our proposed Golomb–Rice code estimation method is very efficient and fast compared to Rice’s method that determines the optimum k
                           * through an exhaustive search among the allowable k values (brute-force approach). Simulation results using the six test data sets show that the increased compression bit rate that results due to suboptimal code selection (i.e. selecting the wrong code option parameter) is negligible.

FELACS depends on the principle of entropy coding of geometrically distributed source. However, in practice, most of the sources encountered are not geometrically distributed. As such, the original sources have to be preprocessed and transformed to ensure that they fit the geometric distribution. To this end, we apply the principle of predictive coding since prediction errors are often times modeled using Laplacian distribution which can easily be transformed into an approximate geometric distribution. To ensure low levels of computations since sensor nodes in WSNs have relatively low computational power, we used a linear prediction model that is limited to taking the differences between consecutive sampled data. For our intended applications, the prediction model proves to be simple and efficient. In addition, it also ensures that the computational complexity of our compression scheme is as low as possible. Thus, the predicted sample 
                              
                                 
                                    
                                       
                                          
                                             x
                                          
                                          
                                             ˆ
                                          
                                       
                                    
                                    
                                       i
                                    
                                 
                              
                            is given by
                              
                                 (8)
                                 
                                    
                                       
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                       
                                       
                                          ˆ
                                       
                                    
                                    =
                                    
                                       
                                          x
                                       
                                       
                                          i
                                          -
                                          1
                                       
                                    
                                    .
                                 
                              
                           
                        

That is, the predicted sample is equal to the last observed sample. The residue (i.e. the error term) is then calculated by subtracting the predicted sample from the current sample. Hence, the residue di
                            is the difference
                              
                                 (9)
                                 
                                    
                                       
                                          d
                                       
                                       
                                          i
                                       
                                    
                                    =
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    -
                                    
                                       
                                          x
                                       
                                       
                                          i
                                          -
                                          1
                                       
                                    
                                    .
                                 
                              
                           
                        

In order to compute the first residue d
                           1, we assume that
                              
                                 (10)
                                 
                                    
                                       
                                          x
                                       
                                       
                                          0
                                       
                                    
                                    =
                                    0
                                    .
                                 
                              
                           
                        

Eq. (10) ensures that the first sample of each block of residues is the reference sample (original sample value). The residues di
                            are now Laplacian distributed. We then use the Rice mapping function [18] to transform (map) the Laplacian distributed error terms (residues) to approximate geometrically distributed non-negative integer δi
                            which serves as input to the entropy encoder. The Rice mapping function is defined by
                              
                                 (11)
                                 
                                    
                                       
                                          δ
                                       
                                       
                                          i
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      2
                                                      
                                                         
                                                            d
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                   
                                                   
                                                      0
                                                      ⩽
                                                      
                                                         
                                                            d
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                      ⩽
                                                      θ
                                                   
                                                
                                                
                                                   
                                                      2
                                                      |
                                                      
                                                         
                                                            d
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                      |
                                                      -
                                                      1
                                                   
                                                   
                                                      -
                                                      θ
                                                      ⩽
                                                      
                                                         
                                                            d
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                      <
                                                      0
                                                   
                                                
                                                
                                                   
                                                      θ
                                                      +
                                                      |
                                                      
                                                         
                                                            d
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                      |
                                                   
                                                   
                                                      otherwise,
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 θ
                                 =
                                 min
                                 (
                                 
                                    
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                    
                                    
                                       ˆ
                                    
                                 
                                 ,
                                 
                                    
                                       2
                                    
                                    
                                       N
                                    
                                 
                                 -
                                 1
                                 -
                                 
                                    
                                       
                                          
                                             x
                                          
                                          
                                             ˆ
                                          
                                       
                                    
                                    
                                       i
                                    
                                 
                                 )
                              
                           .

The mapping function in (11) is reversible and ensures that the dynamic range of the non-negative integer δi
                            is the same with that of the original source. Since smaller magnitude of prediction errors (residues) occurs more frequently than larger magnitude, the mapping functions maps smaller magnitude xi
                            to smaller magnitude of δi
                           . The non-negative integer δi
                            then serves as input to an adaptive entropy encoder. That is, δi
                            is used to losslessly encode xi
                            using the coding schemes described in Section 3.2.3.

In order to achieve energy conservation and also to maximize data return over unreliable and low rate radio links, we propose to implement FELACS algorithm that compresses blocks of sampled data independently at a time using Golomb–Rice family of 8 code options (k
                           =0, 1, 2,…,7). Our proposed algorithm which dynamically adjusts to changing source statistics operates in one pass and can be applied to multiple data types. Our entropy coding problem is how to efficiently and independently encode a block of n non-negative integer δi
                            at a time using the best code option. To this end, in Section 3.2.1, we have devised a simple, efficient and fast Golomb–Rice parameter estimation method that gives good performance. Using our proposed method, the optimum coding parameter k
                           * can be computed on the fly without the need for lookup table using only bit-shift operations since the decision regions boundaries occur at J multiplied by a power of 2 as can be seen from (7) and Table 2. Thus, to independently encode blocks of n non-negative integer δi
                           , our entropy coding procedure simplifies to the repetition of the following steps:
                              
                                 1.
                                 Encode the first sample (reference sample) in the block of n non-negative integer δi
                                     using N-bit natural binary code.

Using (6), compute the sum D of the remaining J (where J
                                    =(n
                                    −1)) samples in the block.

Using the sum D, compute on the fly the optimum coding parameter k
                                    * using (7). Note, at most, only 7 bit-shift operations and comparison is required.

Generate the ID bit pattern using 
                                       
                                          ⌈
                                          
                                             
                                                log
                                             
                                             
                                                2
                                             
                                          
                                          8
                                          ⌉
                                       
                                     bits of the natural binary code representation of k
                                    * which is used to indicate to the decoder which code option was used in encoding the remaining J samples in the block.

Encoding the remaining J samples in the block using Golomb–Rice code with code option parameter k
                                    * that was computed in step 3.

Concatenate the ID bit pattern, the N-bit natural binary code of the first sample and the encoded bitstream of the remaining J samples in the block together and send to the decoder (sink).

As will be seen in Section 4, by using Golomb–Rice family of 8 code options and following the simple coding steps listed above, and by using our proposed simple Golomb–Rice parameter estimation method, FELACS achieved slightly better compression performance than the original Rice algorithm.

FELACS can be applied in area such as body area networks (BANs) in which sensor nodes permanently monitor and log vital signs with high accuracy so that every small variation of these important signals are captured to help provide crucial information to make a diagnosis [2]. In addition, FELACS can also find application in environmental monitoring tasks, particularly in new science discoveries where the accuracy of observations is very critical for understanding the underlying physical processes.

In this section, we present a numerical example to illustrate the encoding and decoding steps of the FELACS algorithm. Suppose a block of incoming source samples with ADC (N) resolution of 14 is xi
                           
                           ={x
                           1, x
                           2
                           …
                           xn
                           }={5555, 5583, 5548}.
                              
                                 1.
                                 We predict and map the resulting residues to non-negative integer values by applying (9)–(11). Thus we have, di
                                    
                                    ={5555, 28, −35} and δi
                                    
                                    ={5555, 56, 69}.

Next, the decision regions in Table 2 are computed on the fly using 7 left bit-shift operations. The first left bit-shift operation is equivalent to 2J where J
                                    =(n
                                    −1)=2. The second left bit-shift operation is equivalent to 4J and so on.

Next, since δ
                                    1 will be transmitted directly, using (6) we compute the sum D for the remaining two samples. 
                                       
                                          
                                             D
                                             =
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   
                                                      J
                                                   
                                                
                                             
                                             
                                                
                                                   δ
                                                
                                                
                                                   i
                                                
                                             
                                             =
                                             
                                                
                                                   δ
                                                
                                                
                                                   2
                                                
                                             
                                             +
                                             
                                                
                                                   δ
                                                
                                                
                                                   3
                                                
                                             
                                             =
                                             56
                                             +
                                             69
                                             =
                                             125
                                             .
                                          
                                       
                                    
                                 

Next, using the sum D, we compute the optimum coding parameter p (k
                                    *) using either (7) or Table 2. Since D
                                    ⩽128, k
                                    *
                                    =5 and the ID bit pattern of this code option is generated using 3-bit binary representation as 
                                       
                                    .

Next, δ
                                    1 (5555) is encoded using 14-bit natural binary code as 
                                       
                                    .

Next, δ
                                    2 (56) is encoded using Golomb–Rice code with code option parameter k
                                    =5. The codeword is generated as 
                                       
                                     as explained in Section 3.1. Similarly, δ
                                    3 (69) is encoded as 
                                       
                                     using Golomb–Rice code with code option parameter k
                                    =5.

The ID bit pattern, the 14-bit natural binary code of δ
                                    1 and the encoded bitstream of δ
                                    2 and δ
                                    3 are concatenated and send to the sink (decoder) as 
                                       
                                    .

Next, in the decoding procedure of FELACS, the first parameters at the beginning of each packet are decoded first. Note, the following are known by the decoder: the number of ID bit pattern, N (the ADC resolution) and the number of samples in each packet. First, the decoder reads the first 3 bits 
                              
                            and converts it to decimal (5) to determine the Golomb–Rice code option used in encoding samples in that packet. Second, the decoder reads the next 14 bits 
                              
                            and convert it to decimal (5555) and uses it as a reference sample for that packet. Third, the decoder counts the number of ‘zeros’ preceding a ‘one’ 
                              
                           , converts the number of zeros to binary 
                              
                            and appends the next 5 bits to it 
                              
                           . The resulting binary number 
                              
                            is converted to decimal (56) and this represents the second sample. Fourth, the decoder again counts the number of ‘zeros’ preceding a ‘one’ 
                              
                           , converts the number of zeros to binary 
                              
                            and appends the next 5 bits to it 
                              
                            The resulting binary number 
                              
                            is converted to decimal (69) and this represents the third sample. The fourth procedure is repeated until the end of the packet is reached. Thus, δi
                            is decoded as δi
                           
                           ={5555, 56, 69}. Fifth, the original source samples are restored losslessly by the application of the inverse operation of (11), (9), and (10).

The basic LEC [2] algorithm is a simple differential compression scheme that uses a single fixed small size coding table to losslessly compress data. The scheme divides the alphabet of symbols into groups whose size increases exponentially. LEC exploits a modified version of the Exponential–Golomb code of order 0. Thus, a codeword in LEC is a hybrid of entropy codes and binary codes. The groups are entropy coded while the index positions of each residue in its group are binary coded. This way, the entropy codes can be re-calculated to best fit the particular probability distribution of the sources being compressed. LEC compresses data on the fly and does not suffer from growing dictionary size like S-LZW. See [2] for detailed description of the LEC scheme. Although the LEC scheme is simple, its compression performance is good and far better than those obtained by earlier schemes like S-LZW [7] among others. In fact, to date, to the best of our knowledge, LEC is the best lossless compression scheme proposed for WSNs. However, one fundamental drawback of the LEC scheme is the interdependence of one packet on the other. As such, once a packet is lost in transmission, other packets following cannot be decoded. The LEC algorithm will be used to evaluate the compression performance of FELACS.

To demonstrate the effectiveness of our proposed FELACS algorithm, we tested it against various real-world environmental data sets. We considered temperature and relative humidity datasets. The performance of our proposed data compression algorithm is computed by using compression rate which is defined as the ratio of the number of bits to represent the compressed data to the number of samples of the original data. That is:
                        
                           (12)
                           
                              Compression rate
                              =
                              
                                 
                                    CDS
                                 
                                 
                                    NSOD
                                 
                              
                              bits per sample,
                           
                        
                     where CDS is the compressed data size in bits, and NSOD is the number of samples of the original data. Note that the smaller the value of the compression rate, the better the compression performance of the compression algorithm.

Real-world environmental monitoring WSN datasets from SensorScope [20] were used in our simulations. We used relative humidity and temperature measurements from three SensorScope deployments: Le Gènèpi Deployment, HES-SO FishNet Deployment and LUCE Deployment. Publicly accessible data sets were used to make the comparison as fair as possible. These deployments use a TinyNode node [21] which comprises of a TI MSP430 microcontroller, a Xemics XE1205 radio and a Sensirion SHT75 sensor module [22]. Both the relative humidity and temperature sensors are connected to a 14 bit analog to digital converter (ADC). The default measurement resolution for raw relative humidity (raw_h) and raw temperature (raw_t) is 12 bits and 14 bits respectively. Each ADC output raw_h and raw_t is converted into measure h and t in percentage and degree Celsius respectively as described in [22]. The data sets that are published on SensorScope deployments correspond to physical measures h and t. But the compression algorithms work on raw_h and raw_t. Therefore, before applying the compression algorithm, the physical measures h and t are converted to raw_h and raw_t by using the inverted versions of the conversion functions in [22]. Table 3
                         summarizes the main characteristics of the data sets. We compute the information entropy H of the original data sets and the information entropy Hδ
                         of the preprocessed samples of each data set using (13) and (14) respectively. R is the number of possible values of xi
                         (or δi
                        ) and p(xi
                        ) (or p(δi
                        )) is the probability mass function of xi
                         (or δi
                        ). These are all recorded in Table 4
                        . By preprocessing the data sets as can be seen from Table 4, the compressibility of the data sets has been enhanced since information entropy represents an absolute limit on the best possible lossless compression of any source, under certain constraints (i.e. under the assumption that the source is a memoryless source).
                           
                              (13)
                              
                                 H
                                 =
                                 -
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          R
                                       
                                    
                                 
                                 p
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 ·
                                 
                                    
                                       log
                                    
                                    
                                       2
                                    
                                 
                                 p
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 ,
                              
                           
                        
                        
                           
                              (14)
                              
                                 
                                    
                                       H
                                    
                                    
                                       δ
                                    
                                 
                                 =
                                 -
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          R
                                       
                                    
                                 
                                 p
                                 (
                                 
                                    
                                       δ
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 ·
                                 
                                    
                                       log
                                    
                                    
                                       2
                                    
                                 
                                 p
                                 (
                                 
                                    
                                       δ
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 .
                              
                           
                        
                     

In this section, we discussed the compression performance of our proposed FELACS algorithm in comparison with other lossless block-based compression algorithms using the real-world data sets discussed in Section 4.1.

In this subsection, we compare the compression performance of FELACS with the state-of-the-art Rice algorithm presented in [18] and discussed in Section 3.1. Fig. 2
                            shows the variation of the compressed bit rates versus block size achieved on all the six data sets using the FELACS algorithm and the original Rice algorithm. For both algorithms, each block of source samples is independently compressed and as such the first samples in each block are transmitted directly using N-bit natural binary code (N
                           =14 for temperature data sets and N
                           =12 for relative humidity data sets). While Rice coding parameter k
                           ∊{0, 1, 2,…,
                           N
                           −2, N}, FELACS coding parameter k is limited to k
                           ∊{0, 1, 2,…,7}. From the plots, the compression rate achieved by both algorithms decreases with increase in block size. For very small block sizes, the compression rate achieved by both algorithms is very poor (high values) due to the ID bit pattern overhead cost. It is important to note that, as the block size increases (i.e. from block size of n
                           =49 upwards), the compressed bit rates achieved on all the six data sets were in the neighbourhood of Hδ
                           , which is the absolute limit on the best possible lossless compression rate of any source achievable under the assumption that the source is a memoryless source. Thus, both FELACS and the original Rice algorithm are efficient. However, FELACS outperforms the original Rice algorithm up to a block size of about 113 for some data sets (LU84 temp (temperature data set), LU84 RH (relative humidity data set), FN101 RH and LG20 RH) and up to a block size of 225 and 273 for FN101 and LG20 temperature data sets respectively. The better performance of FELACS as against the original Rice is as a result of the limitation of the code options in FELACS to 8 and efficient data modeling despite the simplicity of our proposed simple Golomb–Rice parameter estimation method. Furthermore, in terms of time complexity, FELACS is about N (N
                           =
                           default ADC resolution) times faster than Rice algorithm. While Rice algorithm determines the optimum code option through individual evaluation of each code option for every block of source samples, FELACS determines the optimum code option on the fly using additions and only 7 bit-shift operations and comparisons. In fact, for applications where the block size is fixed, the performance of FELACS is even faster. The decision region boundaries are computed on the fly once using only 7 bit shift operations before the start of coding. Subsequently, for each block of source samples, only additions and at most 7 comparisons are required to determine the optimum code option. For example, if it takes Rice algorithm 7s to compress a certain data set, FELACS will only take about 0.5s to compress the same data set for a default ADC resolution (N) of 14 bits. In addition, in terms of space complexity, Rice algorithm requires more resources (e.g. coding memory) than FELACS. Consequently, the complexity of the original Rice coding algorithm is generally higher than FELACS.

In this subsection, we evaluate the compression performance of FELACS with LEC algorithm. In implementing the LEC algorithm, we followed the description of the algorithm given in [2] in detail to compress each data set using fixed block size (the size of each data source). The compressed bit rates achieved for all the six data sets are plotted in Fig. 2.

Note that, the LEC algorithm is not adaptive to changing statistics of the source data since it is only one coding table that is used. Consequently, FELACS being efficient and adaptive outperforms the LEC algorithm and this is clearly seen from the plots in Fig. 2. FELACS takes very less time to compress data than the LEC algorithm which spend lots of time accessing the coding table. FELACS requires less coding memory and is more robust to packet losses than LEC.

In this subsection, we evaluate the compression performance of FELACS with S-LZW algorithm proposed in [7] for WSNs. S-LZW is a dictionary-based low complexity lossless compression algorithm that divides the uncompressed input bit streams into fixed size blocks of 528 bytes (two flash pages) and compresses each block separately. From S-LZW coding mechanism, it is much less robust to packet losses than FELACS. Table 5
                            gives the compressed bit rates achieved on all the six data sets by the S-LZW algorithm with the following fixed parameters: Mini-Cache Entries=32, Maximum Dictionary Entries=512, Block Size=528 bytes, and Dictionary Strategy=Frozen [7]. From these fixed parameters, it can be seen that S-LZW uses significantly more memory space for its dictionary entries and mini-cache entries than our proposed FELACS. Also, the block size of 528 bytes suggests that S-LZW can only be applied in delay-tolerant applications while FELACS can be applied in both near real-time and delay-tolerant applications. To make the compression performance comparison between FELACS and S-LZW as fair as possible, we compress temperature and relative humidity data sets at block size of 302 (14 bits×302≈528 bytes) and 352 (12 bits×352=528bytes) respectively using FELACS. These compression performances are also recorded in Table 5. From the compression results in Table 5, FELACS is 161–214% as efficient as S-LZW. The worst compression performance of S-LZW is recorded when it is used to compress the LG20RH data set. The samples of the LG20RH data set are originally represented by 12-bit and after compression by S-LZW, the resulting compressed bit rate is 12.4915 bits/sample (i.e. data expansion results). In addition, while S-LZW algorithm suffers from the growing dictionary problem FELACS on the other hand requires no dictionary for its operation.

In this subsection, we evaluate the compression performance of FELACS with mLEC (modified Lossless Entropy Compression) [5], Simple-algorithm (a simple data compression algorithm) [6] and ALDC (adaptive lossless data compression) [4] algorithms. From their coding mechanisms, mLEC, Simple-algorithm and ALDC compression schemes all requires more hardware (memory) for storing their coding dictionaries and are also slow since for each source sample, the schemes have to access memory (using table lookup) to get the needed code. Consequently, a lot of energy is consumed accessing memory. These schemes are also not robust to packet losses. FELACS however compresses data very fast and requires no coding table. FELACS compresses each block of source data independently and thus provide significant robustness to packet losses that may occur due to congestion, interference and multipath effects. Table 6
                            gives the performance comparison between FELACS, mLEC, Simple-algorithm and ALDC in Bits/sample at block size of 48 samples. FELACS clearly outperforms mLEC, Simple-algorithm and ALDC algorithms for all the data sets.

To show that the proposed FELACS algorithm can be applied to multiple data types, we evaluate its compression performance on seismic data. The seismic dataset used was that collected by the OhioSeis Digital Seismographic Station located in Bowling Green, Ohio, for the time interval of 2:00–3:00PM on 21 September 1999 (UT) [23]. FELACS compresses the seismic data down to its entropy value as shown in Table 6 and Fig. 3
                           . Hence, it can be applied to multiple data types.

Sensor nodes in WSNs consume energy during sensing, processing and transmission. But typically, the energy spent by a sensing node in the communication module for data transmission and reception is more than the energy for processing [2].

This is because the radio transceiver on board the communication module is the most aggressive energy consumer in the sensor node. However, WSNs are battery operated and they show a strong dependence on battery lifetime. It is therefore important to carefully manage the energy consumption of each sensor node subunit in order to maximize the network lifetime of WSNs. Reduced data transmission implies less energy consumption as well. Data compression schemes help to reduce data size before transmitting in the wireless medium which translates to reduced total power consumption. This savings due to compression directly translate into lifetime extension for the network nodes [13]. Improved compression yields reduced transmission that translates into lower power consumption from the radio transceivers. Our proposed FELACS algorithm provides improved compression using very minimal computational resources and this result in high power saving for the sensor node. At the data preprocessing stage, FELACS uses the simple unit-delay predictor and Rice mapper that requires only basic operations like subtraction, addition, bit-shift operation (in place of multiplication) and comparison and thus execute the data preprocessing task very fast. At the encoding stage, FELACS uses a very simple data model and entropy encoder. While the data model function is executed using basic operations like addition, bit-shift operation and comparison, the entropy encoder on the other hand executes its coding function using only basic operations like bit-shift operation and masking. The encoding task is therefore also executed very fast. Thus, our proposed FELACS algorithm compresses data very fast. Consequently, the computational time and by extension the computational power consumed is very small and negligible when compared to the power consumed by the communication module. Next, we determine the percentage power saving of the sensor node due to compression using FELACS. The percentage power saving of the sensor node is defined as:
                           
                              (15)
                              
                                 Power saving
                                 =
                                 
                                    
                                       
                                          
                                             p
                                          
                                          
                                             s
                                          
                                       
                                    
                                    
                                       
                                          
                                             p
                                          
                                          
                                             c
                                          
                                       
                                    
                                 
                                 ×
                                 100
                                 ,
                              
                           
                        where ps
                         is the power saved through compression and pc
                         is the power consumed without compression. For FELACS under the assumption that the computational power consumed is negligible compared to the communication module power consumption as discussed above, the power saving of the sensor node can be approximated to be equal to the power saving of the communication module. Eq. (15) therefore becomes
                           
                              (16)
                              
                                 Power saving
                                 ≈
                                 
                                    
                                       
                                          
                                             N
                                          
                                          
                                             s
                                          
                                       
                                    
                                    
                                       N
                                    
                                 
                                 ×
                                 100
                                 ,
                              
                           
                        where Ns
                         is the average number of bits saved per sample and N is the number of bits per sample of the original source data (ADC resolution). Eq. (16) simplifies to
                           
                              (17)
                              
                                 Power saving
                                 ≈
                                 
                                    
                                       
                                          1
                                          -
                                          
                                             
                                                Compression rate
                                             
                                             
                                                N
                                             
                                          
                                       
                                    
                                 
                                 ×
                                 100
                                 .
                              
                           
                        
                     

Note that, the power saving of each sensor node depends on the statistics (dynamics) of the source data. Eq. (17) will therefore be used for the calculation of the percentage power savings achieved by FELACS when it is used to compress the six test data sets. For our calculations, we set the packet size to 56 bytes (which is equal to 32 14-bit samples and about 37 12-bit samples) of payload. Fig. 2 shows that using FELACS reduces the bit rate of the original source data sets to less than half of their original bit rate. Thus, by setting the packet size to 56 bytes of payload, FELACS can compress data at a block size of 64 and 74 samples per block for temperature and relative humidity data sets respectively. For simplicity, we use block size of 64 samples for both temperature and relative humidity data sets in our calculations. Table 7
                         shows the power savings achieved by FELACS at block size of 64 samples for the six test data sets. From Table 7 it can be seen that on the average tremendous power saving was achieved using FELACS. By using FELACS to compress the LU84 temp data set, the lifetime of the sensor node involved is improved by a factor of 3.40. However, in practice, the power savings achieved and by extension the node’s lifetime improvement attained will slightly be lower because of the minimal computational power consumed. Furthermore, the increased compression achieved by FELACS will lead to reduced network load which in turn will lead to fewer collisions and retransmissions.

@&#CONCLUSION@&#

In this paper, we proposed a lightweight block-based adaptive compression algorithm based on Golomb–Rice codes for WSNs. The algorithm is fast and so takes only little amount of time to compress data. It requires only a small amount of computational power which makes it suitable for battery operated WSN nodes. The algorithm is highly robust to packet losses since each packet is decompressed independently. In addition, the improved compression achieved by the algorithm helps to reduce network load which leads to fewer collisions and retransmissions. Being fast and efficient, the algorithm is suitable for high-fidelity data gathering in high rate applications. From the results, on the average, the algorithm achieved more than 55% power saving which translate to node’s lifetime improvement by a factor of 2.22. The fast and relatively low computational requirements of the algorithm make it suitable for near real-time compression. From the analysis and simulation results, the algorithm outperforms S-LZW, LEC, mLEC, Simple-algorithm and ALDC algorithms both in terms of compression performance and compression speed. We expect in the future to implement our proposed algorithm in real WSN hardware.

@&#REFERENCES@&#

