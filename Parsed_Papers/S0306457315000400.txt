@&#MAIN-TITLE@&#Multi-lingual opinion mining on YouTube

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We designed the first model for effectively carrying out opinion mining on YouTube comments.


                        
                        
                           
                           We propose kernel methods applied to a robust shallow syntactic structure, which improves accuracy for both languages.


                        
                        
                           
                           Our approach greatly outperforms other basic models on cross-domain settings.


                        
                        
                           
                           We created a YouTube corpus (in Italian and English) and made it available for the research community.


                        
                        
                           
                           Comments must be classified in subcategories to make opinion mining effective on YouTube.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Natural Language Processing

Opinion mining

Social media

@&#ABSTRACT@&#


               
               
                  In order to successfully apply opinion mining (OM) to the large amounts of user-generated content produced every day, we need robust models that can handle the noisy input well yet can easily be adapted to a new domain or language. We here focus on opinion mining for YouTube by (i) modeling classifiers that predict the type of a comment and its polarity, while distinguishing whether the polarity is directed towards the product or video; (ii) proposing a robust shallow syntactic structure (STRUCT) that adapts well when tested across domains; and (iii) evaluating the effectiveness on the proposed structure on two languages, English and Italian. We rely on tree kernels to automatically extract and learn features with better generalization power than traditionally used bag-of-word models. Our extensive empirical evaluation shows that (i) STRUCT outperforms the bag-of-words model both within the same domain (up to 2.6% and 3% of absolute improvement for Italian and English, respectively); (ii) it is particularly useful when tested across domains (up to more than 4% absolute improvement for both languages), especially when little training data is available (up to 10% absolute improvement) and (iii) the proposed structure is also effective in a lower-resource language scenario, where only less accurate linguistic processing tools are available.
               
            

@&#INTRODUCTION@&#

The increasing prevalence of social media like Twitter, Facebook and YouTube, which enable millions of users to share information and opinions quickly, has urged the need for new tools that robustly and automatically process the sheer amounts of user-generated content produced every day. Of particular importance is the fact that such information units, e.g., tweets in the case of Twitter or comments in case of YouTube, often carry opinions (or sentiment), i.e., they express subjective opinions of a particular user. In particular, we estimated that roughly 60–80% of the YouTube comments do actually contain opinions. Therefore, social media provide a key source that raises the importance of automatic extraction of opinions affecting the reputation of a person, and organization, or a specific product.

In this study we focus on YouTube. It is a platform that hosts videos uploaded by users (companies, private persons, etc.). YouTube is a unique environment with many facets: it is multi-modal, multi-lingual, multi-domain and multi-cultural, since people from different regions of the world can upload videos including textual information about different topics, they can rate videos, and comments on videos in different languages. Therefore, work promoting the success of sentiment analysis systems in such an environment is of high interest for both the industry and the research community.

Most prior research on opinion mining has been carried out on well-edited texts (Pang & Lee, 2008), and there has been some recent effort for sentiment analysis on Twitter (Nakov et al., 2013). In contrast, YouTube comprises several new challenges, which need to be tackled: (i) words expressing polarity can refer to either the video content itself (“the girl is cute”) or the advertised product (“I hate the G2”), or may even contain contrasting sentiment; (ii) many comments are unrelated and are spam (“go to http to win an iPad”); (iii) YouTube is a large resource covering a variety of domains, thus it is not clear how well a supervised system trained on, say, the tablets domain, fares on a different domain, e.g., videos about automobiles; and (iv) it covers a large variety of languages, both in terms of the video and the comments for a certain video, thus approaches that handle the multilinguality aspect are of particular interest.

Still, the majority of systems for sentiment analysis rely on the simple bag-of-words (BOW) representation. That is, the input text is split into n-grams of words (or characters). These are used in machine learning algorithms, e.g., Support Vector Machines (SVM) or logistic regression, to induce a model that can classify new instances. In fact, the winning system (Mohammad, Kiritchenko, & Zhu, 2013) of the SemEval 2013 shared task (Nakov et al., 2013) used a BOW representation together with a sentiment lexicon in SVMs. However, opinions usually involve complex interactions between lexical items (e.g., variations in sentiment scope and target, modality and negations, etc.). The standard BOW representation cannot take those into consideration, since by definition it abstracts away from many important clues. For example, consider a comment from a YouTube video, where a person reviews a specific product, namely the Motorola xoom tablet:
                        
                           this guy really puts a negative spin on this, and I’m not sure why, this seems crazy fast, and I’m not entirely sure why his pinch to zoom his laggy all the other 
                           xoom 
                           reviews
                        
                     
                  

The comment contains the product name (
                        xoom
                     ) and a list of negative expressions, thus, a bag-of-words model would derive a negative polarity for this product. In contrast, the opinion towards the product is neutral as the negative sentiment is expressed towards the video. Similarly, the following Italian comment on an iPad video expresses positive sentiment about another product (galaxy note), but is neutral with respect to the topical product.
                        
                           Questo video fa un presentazione interessante dell’ iPad, ma ho preso il galaxy note:) ha uno schermo fantastico. veramente bello e fluido. te lo consiglio. (This video gives a nice introduction of iPad but I took the galaxy note:) it has a fantastic screen. very nice and fluent. I recommend it).
                     
                  

The following short English comment illustrates the main problem even better:
                        
                           iPad 2 is better. the superior apps just destroy the 
                           
                              xoom
                           
                           .
                        
                     
                  

It contains two positive and one negative word, yet the sentiment towards the product is negative (the negative token destroy refers to Xoom). Thus, it is important to distinguish if the sentiment on YouTube is directed either towards the source video itself, or the product described in that video or another product. This cannot be captured by a bag-of-words model, which lacks the needed structural information for linking the sentiment with the target product.

In this paper, we present the results of the first research effort on the systematic analysis of opinion mining (OM) for YouTube comments capitalizing on our previous work Uryupina, Plank, Severyn, Rotondi, and Moschitti (2014) and Severyn, Moschitti, Uryupina, Plank, and Filippova (2014). The contributions of our research are:
                        
                           1.
                           
                              User comment type and polarity classification: to solve the issues outlined above, we devise a classification scheme that separates unrelated and spam comments from informative ones, which are, in turn, further categorized into product- or video-related (type classification). Moreover, we learn classifiers to assign polarity (positive, negative, neutral) to each type of informative comment. This allows us to filter out irrelevant comments, providing accurate OM distinguishing comments about the video and the target product.


                              A novel structural representation, based on shallow syntactic trees enriched with conceptual information, i.e., tags generalizing the specific topic of the video, e.g., Fiat Panda, xoom, Toyota Aygo. In particular, we define an efficient tree kernel derived from the Partial Tree Kernel (Moschitti, 2006), suitable for encoding structural representation of noisy user-generated comments into Support Vector Machines (SVMs).


                              Creation and annotation of a corpus of YouTube comments: it contains 50k manually labeled (by an expert coder) comments for two product domains: tablets and automobiles.
                                 1
                                 The corpus and the annotation guidelines are publicly available at: http://projects.disi.unitn.it/iKernels/projects/sentube/.
                              
                              
                                 1
                               It is the first manually annotated corpus that enables researchers to use supervised methods on YouTube for comment classification and opinion analysis. The comments from different product domains exhibit different properties (cf. Section 5.2), which give the possibility to study the domain adaptability of the supervised models by training on one category and testing on the other (and vice versa).


                              Multi-lingual experiments: in contrast to our and other prior work focused exclusively on one language (mainly, English), we show that our structural representation also works well for a less-resourced language, namely, Italian. This is of particular interest since it tests the proposed representation under limiting conditions: the performance of linguistic preprocessors is inferior to tools that were developed for English, or we might not even have access to all required analyzers. This allows us to gauge how well our structural models fare in a setup that can lead to inaccurate and noisy representations.

Our evaluation shows that our models are adaptable and thus robust across domains. Structural models generally improve on both tasks – polarity and type classification – yielding up to 30% relative improvement, when little data is available. Hence, the impractical task of annotating data for each YouTube category can be mitigated by the use of models that adapt better across domains. Moreover, our evaluation on the Italian data confirms that the proposed representation works well also on another rather different and less-resourced language.

The remainder of the paper is structured as follows. Section 2 discusses related work, Section 3 introduces our baseline models and structured representation. Section 4 introduces our corpus, Section 5 describes our experiments. Section 6 concludes and provides directions for future research.

@&#RELATED WORK@&#

In the past decade, automatic sentiment analysis of texts has attracted attention from both industry and academia. Such interest has produced a large body of research work, mainly focusing on the use of machine learning algorithms for opinion classification (Montoyo, Martínez-Barco, & Balahur, 2012; Pang & Lee, 2008).

Most prior work on opinion mining has been performed on more standardized forms of text, such as consumer reviews or newswire. The most commonly used datasets include: the MPQA corpus of news documents (Wilson, Wiebe, & Hoffmann, 2005), web customer review data (Hu & Liu, 2004), Amazon review data (Blitzer, Dredze, & Pereira, 2007), the JDPA corpus of blogs (Kessler, Eckert, Clark, & Nicolov, 2010), etc. However, these corpora are only partially suitable for developing models on social media, since the informal text poses additional challenges for Information Extraction and Natural Language Processing.

Opinion mining on social media has started to receive a lot of attention from the scientific community only very recently (Cambria & Hussain, 2012; Nakov et al., 2013). Several annotation projects have been proposed to support the development of sentiment analysis models for social media, focusing mainly on Twitter—one of the biggest initiatives being the SemEval 2013 task on the sentiment analysis (Nakov et al., 2013). While most of these datasets contain only English documents, several corpora cover other languages. In particular, the SentiTUT project focuses on annotating a Twitter-based corpus for sentiment analysis in Italian (Bosco, Patti, & Bolioli, 2013).

Similar to Twitter, most YouTube comments are very short, the language is informal with numerous accidental and deliberate errors and grammatical inconsistencies (Baldwin, Cook, Lui, MacKinlay, & Wang, 2013), which make previous corpora less suitable to train models for OM on YouTube. Nevertheless, YouTube is a much less explored social media, with almost no work on sentiment analysis published so far. Siersdorfer, Chelaru, Nejdl, and San Pedro (2010) focus on exploiting user ratings (the counts of ‘thumbs up/down’ as flagged by other users) of YouTube video comments to train classifiers to predict the community acceptance of new comments. Their goal is thus different: predicting comment ratings, rather than predicting the sentiment expressed in a YouTube comment or its information content. Exploiting the information from user ratings is a feature that we have not exploited thus far, but we believe that it is a valuable feature to use in future work.

Early studies on opinion mining focused on the document polarity classification problem: for a given document, the algorithm assigns a label determining its general attitude (positive, negative or neutral). This formulation, however, is often too simplistic and thus the most recent studies address more fine-grained tasks, including identifying subjective vs. objective parts of a document (Pang & Lee, 2004; Yessenalina, Yue, & Cardie, 2010), opinion holders (Johansson & Moschitti, 2013) or more complex sentiments and emotions (Cambria & Hussain, 2012), in particular, as irony or sarcasm (Carvalho, Sarmento, Silva, & de Oliveira, 2009; Reyes, Rosso, & Veale, 2013). In our work, we refine the traditional polarity classification formulation, distinguishing between different sentiment targets (video vs. product). This allows us to provide a better understanding of user-generated comments that may address several topics, expressing different emotions.

Most of the previous work on supervised sentiment analysis use feature vectors to encode documents. Several studies provide in-depth analysis of lexical features for opinion mining (Pang, Lee, & Vaithyanathan, 2002; Riloff, Patwardhan, & Wiebe, 2006). Such features can be effectively combined with external information, for example, with personalized co-occurence statistics (Panicheva, Cardiff, & Rosso, 2013). Our feature-based baseline model (cf. Section 3) is very similar to the best performing system from the SemEval 2013 shared task on Twitter (Mohammad et al., 2013).

While a few successful attempts have been made to use more involved linguistic analysis for opinion mining, such as dependency trees (Poria, Cambria, Winterstein, & Huang, 2014; Täckström & McDonald, 2011) and constituency trees with vectorized nodes (Socher, Pennington, Huang, Ng, & Manning, 2011), recently, a comprehensive study by Wang and Manning (2012) showed that a simple model using bigrams and SVMs performs on par with more complex algorithms.

In contrast, we show that adding structural features (cf. Section 3) from syntactic trees is particularly useful for the cross-domain setting (cf. Section 5.4) and our findings carry over to our experiments on Italian (cf. Section 5.6). The structural features help to build a system that is more robust across domains. Therefore, rather than trying to build a specialized system for every new target domain, as it has been done in most prior work on domain adaptation (Blitzer et al., 2007; Daumé, 2007), the domain adaptation problem boils down to finding a more robust system (Plank & Moschitti, 2013; Søgaard & Johannsen, 2012). This is in line with recent advances in parsing The Web (Petrov & McDonald, 2012), where participants were asked to build a single system able to cope with different yet related domains.

Our approach, which we will describe in detail in the next section, relies on robust syntactic structures to automatically generate patterns that, given our empirical findings, have shown to adapt better. These representations were inspired by the semantic models developed for Question Answering (Moschitti, 2008; Severyn & Moschitti, 2012; Severyn & Moschitti, 2013) and Semantic Textual Similarity (Severyn, Nicosia, & Moschitti, 2013). Moreover, we introduce additional tags, e.g., video concepts, polarity and negation words, to achieve better generalization across different domains, where the word distribution and vocabulary changes.

Most studies on opinion mining, especially for social media data, focus on English. Thus, several algorithms have been proposed for detecting opinions in English tweets within the recent SemEval evaluation campaign (Nakov et al., 2013). To our knowledge, only one study has addressed the task for Italian so far (Basile & Nissim, 2013). There are several important differences between this work and our research. We have created a manually annotated corpus (cf. Section 4) that can be used by the scientific community for experiments on supervised opinion mining. The previous work was based on automatically extracted data and a small manually tagged test collection. We propose structural models for our data representation and show that they yield superior performance. The previous work adopted a more baseline methodology, relying on term-based opinion scores.

In this section, we describe our learning systems exploiting innovative computational representations. First, we will introduce and discuss standardly used representation based on simple bag-of-words features. Then, we will introduce our novel structured representation, its advantages and requirements and the suitable learning machinery based on tree kernels.

As mentioned in Section 2, traditional opinion mining systems mainly use the bag-of-word representation in conjunction with sentiment lexicons and simple negation handling (Mohammad et al., 2013; Nakov et al., 2013; Pang & Lee, 2008). Following prior work, we use the features belowin our baseline model (FVEC):
                           
                              •
                              
                                 word n-grams: unigrams and bigrams over lower-cased word lemmas, i.e., a binary feature that indicates the presence/absence of a given item.


                                 lexicon: a sentiment lexicon is a list of words associated with positive or negative sentiment. The lexicon features represent the count of the number of positive and negative tokens in a user comment, respectively.


                                 negation: this feature counts the number of negation words, e.g., {don’t, never, not, etc.} in a given comment.
                                    2
                                    The list of English negation words is adopted from http://sentiment.christopherpotts.net/lingstruc.html. The list of Italian negation words has been compiled by the authors and consists of: “no”, “non”, “mai”, “nessuno”, “nessuna”, “nessun”, “niente”, “nulla”, “neppure”, “neppur”, “neanche”, ”mica”, “né”, “nemmeno”, “manco”, “giammai”.
                                 
                                 
                                    2
                                  As we will discuss in the next Section, our structural representation enables a more powerful treatment of negation.


                                 video concept: the cosine similarity using word features between a user comment and the title/description of the video. Since many of the videos come with a title and a short description, we use this feature as a proxy to encode the topicality of each comment in relation to the video.

Our experiment section will show that models using the above feature representation are already very powerful. Yet, the goal of this study is to gauge the effectiveness of a more informed model that takes structure into consideration, as introduced next.

In order to go beyond traditional feature vectors we use structural models (STRUCT), which encode each comment into a shallow syntactic tree. Each user comment is transformed into a tree structure, which constitutes the input of a tree kernel function. This, in turn, can generate structural features from such tree. Social media text, which is less well-edited and in general very noisy data, requires special attention in handling. Therefore, we propose a structure that is specifically adapted to the noisy user-generated text:
                           
                              1.
                              The structural representation should encode structural dependencies, yet be robust to the noisy text input.

It is important that the structure encodes important concepts of the comments.

Therefore, we here propose to use a shallow syntactic tree representation with enriched tags. Our structure (cf. Figs. 1 and 2
                        
                        , for English and Italian, respectively) encodes not only words from the sentiment lexicons, but also important concepts about the product and negation words, which specifically target the sentiment and comment type classification tasks.

In particular, our proposed shallow syntactic tree is a two-level tree built from word lemmas (leaves) and part-of-speech tags that are further grouped into chunks (Fig. 1). As full syntactic parsers such as constituency or dependency tree parsers would significantly degrade in performance on noisy texts, e.g., Twitter or YouTube comments, we opted for shallow structures, which rely on simpler and more robust components: a part-of-speech tagger and a chunker.

We address the second point above by enriching the syntactic trees with semantic tags, which encode: (i) central concepts of the video (e.g. the product name itself), (ii) sentiment-bearing words expressing positive or negative sentiment and (iii) negation words. To automatically identify concept words relevant for the video we use nouns detected by the part-of-speech tagger in the video title or video description and match them in the tree. For the matched words, we enrich labels of their parent nodes (part-of- speech and chunk) with the PRODUCT tag (cf.the parent nodes for xoom in Fig. 1). In the same way, nodes associated with words found in the sentiment lexicon are enriched with their respective polarity (either positive or negative).

The advantage of the structured representation over the features vector-based (FVEC) model is the fact that it encodes powerful contextual syntactic features in terms of tree fragments. The latter are automatically generated and learned by SVMs with expressive tree kernels. On the downside, it needs linguistic processors such as a part-of-speech tagger and chunker – however, as we will see in the experimental setup, our proposed shallow representation is robust and handles noisy data well, works also on another language and in both cases outperforms the FVEC model. In fact, the FVEC model relies only on feature counts of simple features that do not take structure into consideration.

To illustrate the advantage of the structured representation, consider the English comment shown in Fig. 1. It contains two positive and one negative token as found in the sentiment lexicon. This would strongly bias the FVEC sentiment classifier to assign a positive label to the comment. However, the polarity of this comment is negative with respect to the target product. In contrast, the STRUCT model relies on the fact that the negative word, destroy, refers to the product xoom, where the latter is in object relation to the verb destroy. Consider the excerpt of tree fragments generated by the tree kernel shown in Fig. 3
                         (the kernel function generates many more subtrees, as described in more detail in the next section). The tree fragment on the left is a strong feature that helps the classifier to discriminate in such difficult cases. In general, tree kernels generate all possible subtrees, thereby producing generalized (back-off) features. For instance, the middle and right subtree shown in Fig. 3 generalize the one on the left. The following section will depict in more detail how the tree kernel machinery works.

We rely on supervised machine learning, i.e., SVMs, for performing opinion mining on YouTube. The goal is to learn a model for automatically detecting the sentiment and type of each comment. The tasks will be described in detail in Section 5.1, while here we focus on introducing the learning algorithm.

We build a multi-classifier using the one-vs-all scheme for detecting the sentiment or comment type: a binary classifier is trained for each class, and, at testing time, the class obtaining the maximum prediction score is chosen. Our back-end binary classifier is SVM- light-TK,
                           3
                           
                              http://disi.unitn.it/moschitti/Tree-Kernel.htm.
                        
                        
                           3
                         which encodes structural kernels in the SVM- light (Joachims, 2002) solver. We define a novel and efficient tree kernel function, namely, the 
                           Sh
                        allow syntactic 
                           T
                        ree 
                           K
                        ernel (SHTK), which is as much expressive as the Partial Tree Kernel (PTK) (Moschitti, 2006) to handle feature engineering over the structural representations of the STRUCT model.

A typical kernel machine, e.g., SVMs, classifies a test input 
                              x
                            using the following prediction function: 
                              
                                 h
                                 
                                    (
                                    x
                                    )
                                 
                                 =
                                 
                                    ∑
                                    i
                                 
                                 
                                    α
                                    i
                                 
                                 
                                    y
                                    i
                                 
                                 K
                                 
                                    (
                                    x
                                    ,
                                    
                                       x
                                       i
                                    
                                    )
                                 
                              
                           , where αi
                            are the model parameters estimated from the training data, yi
                            are target variables, 
                              x
                           
                           
                              i
                            are support vectors, and K( · , · ) is a kernel function. The latter computes the similarity between two comments.

In our FVEC model, the similarity between two comments is measured with a polynomial kernel of degree 3 between the two comment feature vectors 
                              v
                           
                           
                              i
                           : 
                              
                                 (1)
                                 
                                    
                                       
                                          
                                             
                                                K
                                                
                                                   (
                                                   
                                                      x
                                                      1
                                                   
                                                   ,
                                                   
                                                      x
                                                      2
                                                   
                                                   )
                                                
                                                =
                                                
                                                   K
                                                   
                                                      v
                                                   
                                                
                                                
                                                   (
                                                   
                                                      v
                                                      1
                                                   
                                                   ,
                                                   
                                                      v
                                                      2
                                                   
                                                   )
                                                
                                                ,
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

The STRUCT model treats each comment as a tuple 
                              
                                 x
                                 =
                                 〈
                                 T
                                 ,
                                 v
                                 〉
                              
                            composed of a shallow syntactic tree 
                              T
                            and a feature vector 
                              v
                           . Hence, for each pair of comments 
                              x
                           
                           1 and 
                              x
                           
                           2, we define the following comment similarity kernel:
                              
                                 (2)
                                 
                                    
                                       
                                          
                                             
                                                K
                                                
                                                   (
                                                   
                                                      x
                                                      1
                                                   
                                                   ,
                                                   
                                                      x
                                                      2
                                                   
                                                   )
                                                
                                                =
                                                
                                                   K
                                                   
                                                      TK
                                                   
                                                
                                                
                                                   (
                                                   
                                                      T
                                                      1
                                                   
                                                   ,
                                                   
                                                      T
                                                      2
                                                   
                                                   )
                                                
                                                +
                                                
                                                   K
                                                   
                                                      v
                                                   
                                                
                                                
                                                   (
                                                   
                                                      v
                                                      1
                                                   
                                                   ,
                                                   
                                                      v
                                                      2
                                                   
                                                   )
                                                
                                                ,
                                             
                                          
                                       
                                    
                                 
                              
                           where K
                           TK computes SHTK (defined next), and K
                           v is a kernel over feature vectors, e.g., linear, polynomial, Gaussian, etc.

Following the convolution kernel framework, we define the new SHTK function from Eq. 2 to compute the similarity between tree structures. It counts the number of common substructures between two trees T
                           1 and T
                           2 without explicitly considering the whole fragment space. The general equationfor Convolution Tree Kernels is:
                              
                                 (3)
                                 
                                    
                                       
                                          
                                             
                                                TK
                                                
                                                   (
                                                   
                                                      T
                                                      1
                                                   
                                                   ,
                                                   
                                                      T
                                                      2
                                                   
                                                   )
                                                
                                                =
                                                
                                                   ∑
                                                   
                                                      
                                                         n
                                                         1
                                                      
                                                      ∈
                                                      
                                                         N
                                                         
                                                            T
                                                            1
                                                         
                                                      
                                                   
                                                
                                                
                                                   ∑
                                                   
                                                      
                                                         n
                                                         2
                                                      
                                                      ∈
                                                      
                                                         N
                                                         
                                                            T
                                                            2
                                                         
                                                      
                                                   
                                                
                                                
                                                   Δ
                                                
                                                
                                                   (
                                                   
                                                      n
                                                      1
                                                   
                                                   ,
                                                   
                                                      n
                                                      2
                                                   
                                                   )
                                                
                                                ,
                                             
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 N
                                 
                                    T
                                    1
                                 
                              
                            and 
                              
                                 N
                                 
                                    T
                                    2
                                 
                              
                            are the sets of the T
                           1’s and T
                           2’s nodes, respectively and Δ(n
                           1,n
                           2) is equal to the number of common fragments rooted in the n
                           1 and n
                           2 nodes. Such number can vary according to several possible definitions of the atomic fragments.

To improve the speed computation of TK, we consider pairs of nodes (n
                           1,n
                           2) belonging to the same tree level. Thus, given H, the height of the STRUCT trees, where each level h contains nodes of the same type, i.e., chunk, POS, and lexical nodes, we define SHTK as the following
                              4
                              To have a similarity score between 0 and 1, a normalization in the kernel space, i.e., 
                                    
                                       
                                          SHTK
                                          (
                                          
                                             T
                                             1
                                          
                                          ,
                                          
                                             T
                                             2
                                          
                                          )
                                       
                                       
                                          
                                             SHTK
                                             
                                                (
                                                
                                                   T
                                                   1
                                                
                                                ,
                                                
                                                   T
                                                   1
                                                
                                                )
                                             
                                             ×
                                             
                                             SHTK
                                             
                                                (
                                                
                                                   T
                                                   2
                                                
                                                ,
                                                
                                                   T
                                                   2
                                                
                                                )
                                             
                                          
                                       
                                    
                                 , is applied.
                           
                           
                              4
                           :
                              
                                 (4)
                                 
                                    
                                       
                                          
                                             
                                                SHTK
                                                
                                                   (
                                                   
                                                      T
                                                      1
                                                   
                                                   ,
                                                   
                                                      T
                                                      2
                                                   
                                                   )
                                                
                                                =
                                                
                                                   ∑
                                                   
                                                      h
                                                      =
                                                      1
                                                   
                                                   H
                                                
                                                
                                                   
                                                      0.35
                                                      e
                                                      m
                                                   
                                                   
                                                      0
                                                      e
                                                      x
                                                   
                                                
                                                
                                                   ∑
                                                   
                                                      
                                                         n
                                                         1
                                                      
                                                      ∈
                                                      
                                                         N
                                                         
                                                            
                                                               T
                                                               1
                                                            
                                                         
                                                         h
                                                      
                                                   
                                                
                                                
                                                   
                                                      0.35
                                                      e
                                                      m
                                                   
                                                   
                                                      0
                                                      e
                                                      x
                                                   
                                                
                                                
                                                   ∑
                                                   
                                                      
                                                         n
                                                         2
                                                      
                                                      ∈
                                                      
                                                         N
                                                         
                                                            
                                                               T
                                                               2
                                                            
                                                         
                                                         h
                                                      
                                                   
                                                
                                                
                                                   Δ
                                                
                                                
                                                   (
                                                   
                                                      n
                                                      1
                                                   
                                                   ,
                                                   
                                                      n
                                                      2
                                                   
                                                   )
                                                
                                                ,
                                             
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 N
                                 
                                    
                                       T
                                       1
                                    
                                 
                                 h
                              
                            and 
                              
                                 N
                                 
                                    
                                       T
                                       2
                                    
                                 
                                 h
                              
                            are sets of nodes at height h.

The above equation can be applied with any Δ function. To have a more general and expressive kernel, we use Δ previously defined for PTK. More formally: if n
                           1 and n
                           2 are leaves then 
                              
                                 
                                    Δ
                                 
                                 
                                    (
                                    
                                       n
                                       1
                                    
                                    ,
                                    
                                       n
                                       2
                                    
                                    )
                                 
                                 =
                                 μ
                                 λ
                                 
                                    (
                                    
                                       n
                                       1
                                    
                                    ,
                                    
                                       n
                                       2
                                    
                                    )
                                 
                              
                           ; else
                              
                                 
                                    
                                       
                                          Δ
                                       
                                       
                                          (
                                          
                                             n
                                             1
                                          
                                          ,
                                          
                                             n
                                             2
                                          
                                          )
                                       
                                       =
                                       μ
                                       
                                          (
                                          
                                             
                                                λ
                                             
                                             2
                                          
                                          +
                                          
                                             ∑
                                             
                                                
                                                   
                                                      I
                                                      →
                                                   
                                                   1
                                                
                                                ,
                                                
                                                   
                                                      I
                                                      →
                                                   
                                                   2
                                                
                                                
                                                   ,
                                                   |
                                                
                                                
                                                   
                                                      I
                                                      →
                                                   
                                                   1
                                                
                                                
                                                   |
                                                   =
                                                   |
                                                
                                                
                                                   
                                                      I
                                                      →
                                                   
                                                   2
                                                
                                                
                                                   |
                                                
                                             
                                          
                                          
                                             
                                                λ
                                             
                                             
                                                d
                                                
                                                   (
                                                   
                                                      
                                                         I
                                                         →
                                                      
                                                      1
                                                   
                                                   )
                                                
                                                +
                                                d
                                                
                                                   (
                                                   
                                                      
                                                         I
                                                         →
                                                      
                                                      2
                                                   
                                                   )
                                                
                                             
                                          
                                          
                                             ∏
                                             
                                                j
                                                =
                                                1
                                             
                                             
                                                
                                                   |
                                                
                                                
                                                   
                                                      I
                                                      →
                                                   
                                                   1
                                                
                                                
                                                   |
                                                
                                             
                                          
                                          
                                             Δ
                                          
                                          
                                             (
                                             
                                                c
                                                
                                                   n
                                                   1
                                                
                                             
                                             
                                                (
                                                
                                                   
                                                      I
                                                      →
                                                   
                                                   
                                                      1
                                                      j
                                                   
                                                
                                                )
                                             
                                             ,
                                             
                                                c
                                                
                                                   n
                                                   2
                                                
                                             
                                             
                                                (
                                                
                                                   
                                                      I
                                                      →
                                                   
                                                   
                                                      2
                                                      j
                                                   
                                                
                                                )
                                             
                                             )
                                          
                                          )
                                       
                                       ,
                                    
                                 
                              
                           where λ,μ ∈ [0,1] are decay factors; the large sum is adopted from a definition of the subsequence kernel (Shawe-Taylor & Cristianini, 2004) to generate children subsets with gaps, which are then used in a recursive call to Δ. Here, 
                              
                                 
                                    c
                                    
                                       n
                                       1
                                    
                                 
                                 
                                    (
                                    i
                                    )
                                 
                              
                            is the ith
                            child of the node 
                              
                                 
                                    n
                                    1
                                 
                                 ;
                                 
                                    
                                       I
                                       →
                                    
                                    1
                                 
                              
                            and 
                              
                                 
                                    I
                                    →
                                 
                                 2
                              
                            are two sequences of indexes that enumerate subsets of children with gaps, i.e., 
                              
                                 
                                    I
                                    →
                                 
                                 
                                    =
                                    (
                                 
                                 
                                    i
                                    1
                                 
                                 ,
                                 
                                    i
                                    2
                                 
                                 
                                    ,
                                    ⋯
                                    ,
                                    |
                                    I
                                    |
                                    )
                                 
                              
                           , with 1 ≤ i
                           1 < i
                           2 < ⋅⋅⋅ < i
                           |I|; and 
                              
                                 d
                                 
                                    (
                                    
                                       
                                          I
                                          →
                                       
                                       1
                                    
                                    )
                                 
                                 =
                                 
                                    
                                       I
                                       →
                                    
                                    
                                       1
                                       l
                                       (
                                       
                                          
                                             I
                                             →
                                          
                                          1
                                       
                                       )
                                    
                                 
                                 −
                                 
                                    
                                       I
                                       →
                                    
                                    11
                                 
                                 +
                                 1
                              
                            and 
                              
                                 d
                                 
                                    (
                                    
                                       
                                          I
                                          →
                                       
                                       2
                                    
                                    )
                                 
                                 =
                                 
                                    
                                       I
                                       →
                                    
                                    
                                       2
                                       l
                                       (
                                       
                                          
                                             I
                                             →
                                          
                                          2
                                       
                                       )
                                    
                                 
                                 −
                                 
                                    
                                       I
                                       →
                                    
                                    21
                                 
                                 +
                                 1
                              
                           , which penalize the subsequences with larger gaps.

It should be noted that: firstly, the use of a subsequence kernel makes it possible to generate child subsets of the two nodes, i.e., it allows for gaps, which makes matching of syntactic patterns less rigid. Secondly, the resulting SHTK is essentially a special case of PTK (Moschitti, 2006), adapted to the shallow structural representation STRUCT (see Section 3.2). When applied to STRUCT trees, SHTK exactly computes the same feature space as PTK, but in faster time (on average). Indeed, SHTK requires to be only applied to node pairs from the same level (see Eq. (4)), where the node labels can match – chunk, POS or lexicals. This reduces the time for selecting the matching-node pairs carried out in PTK (Moschitti, 2006). The fragment space is obviously the same, as the node labels of different levels in STRUCT are different and will not be matched by PTK either. Finally, given its recursive definition in (Eq. 4) and the use of subsequence (with gaps), SHTK can derive useful dependencies between its elements.

As discussed above, the SHTK function defines a very general type of the tree fragments imposing very little constraints on their shape, i.e., a fragment can start at any non-terminal node, and nodes in the fragment can contain children with gaps. According to the recursive definition of the Δ function for the SHTK, it will generate various tree fragments as shown in Fig. 3. Note that each of the tree fragment defined by SHTK encodes both syntactic structure and information about the sentiment (via additional node tags).

To sum up, to take advantage of automatic feature engineering offered by kernels, we define SHTK function to mine all possible tree fragments from a STRUCT model which encodes comments into a shallow syntactic tree. The mined tree fragments (implicitly generated the tree kernel) represent features used by an SVM to learn a predictive model. Hence, the tree kernel learning framework applied to our STRUCT representation results in a more expressive feature representation than, for example, typical bag-of-words as it integrates both syntactic and sentiment information in a structured way.

In order to construct the YouTube comments corpus, we compiled a list of products (Apple iPad, Motorola xoom, Fiat 500, etc.). Our target videos are either commercials or review videos of the products, therefore we queried the YouTube API (gdata)
                        5
                        
                           https://developers.google.com/youtube/v3/.
                     
                     
                        5
                      to retrieve the initial set of links to YouTube videos.
                        6
                        By appending either “commercial” or “review” to the query term which is product name. For the Italian data the links were collected manually through the web interface.
                     
                     
                        6
                      Since we aimed at collecting a multi-lingual corpus, i.e., English and Italian videos, we use the same list of products for the two languages, which allows future cross-lingual experiments. The initial set of links was then manually inspected in order to ensure that the videos were indeed of the target language and that they contained at least several comments. We focused on two particular product domains: automobiles (AUTO) and tablets (TABLETS).

For each video, we downloaded all available comments (limited to a maximum of 1k comments per video). They were then manually annotated with comment type and polarity. We distinguish between the following types:
                        
                           •
                           
                              product: about products in general or some of their aspects;

                                 
                                    –
                                    positive/negative/neutral (toward the product)


                              video: about videos or some of their details;

                                 
                                    –
                                    positive/negative/neutral (toward the video)


                              spam: advertisements and/or malicious links; and


                              off-topic: comments that have almost no content (“lmao”) or content that is not related to the video (“Thank you!”).

With respect to comment polarity, we follow prior work and use the standard categories, i.e., expressing {positive, negative, neutral} sentiment, but distinguishing whether a comment refers to the product or the video. Moreover, if a comment contains several statements of different polarities, it is annotated as both positive and negative. For example, the following is positive towards the video but negative with regard to the target product: “Awesome clip but waiting for the Kindle paperwhite”.

In total we have 217 annotated English videos with around 43k comments (139 videos on TABLETS and 78 videos for the AUTO domain). Moreover, we have 198 annotated Italian videos on the same products with around 10k comments (100 videos on TABLETS and 98 for AUTO). For several products, there was no corresponding Italian commercial or review video. Table 1
                      highlights different corpus statistics for both languages and both domains. There are several notable differences between the two languages.

First, the Italian videos contain fewer comments in total (around 50 comments per video in Italian, as opposed to 200 comments per video for English). This reflects the fact that the English-speaking YouTube audience is much larger and therefore Italian videos do not receive the same number of comments. Thus, our Italian corpus is approximately 1/3 in size of the English corpus.

Second, a much smaller percentage of tokens in Italian can be found in our sentiment dictionary. This can be explained by the lower coverage of the Italian sentiment lexicon (see Section 5 for more details on the sentiment dictionaries we have used in our experiments). It should also be stressed that for both languages, the percentage of sentiment tokens is low (0.7–2.5tokens per comment, around 3–11% of all the tokens). This highlights the importance of a more general approach, that is able to learn task-relevant sentiment patterns going beyond specific lexical.

Finally, for all the domains, we get a very large number of unique tokens, mainly due to a large amount of noise in user-generated comments. This limits the applicability of bag-of-words approaches.

For our experiments, we have split the entire SenTube corpus into several parts, as described in Section 5 below. Summary statistics over our train and test partitions (in number of comments and the distribution over the different labels) are given in Tables 2 and 3
                      for English and Italian, respectively.

To gauge the quality of the labels, in an initial experiment we asked five annotators to label a sample set of one hundred comments and measured the agreement. The resulting annotator agreement α value (Krippendorf, 2004, chap. 11; Artstein & Poesio, 2008) scores are 60.6 (AUTO), 72.1 (TABLETS) for the sentiment polarity task and 64.1 (AUTO), 79.3 (TABLETS) for the type classification task. For the remaining comments, the entire annotation task was assigned to a single coder. Further details on the corpus, including detailed annotation guidelines, can be found in Uryupina et al. (2014).

@&#EXPERIMENTS@&#

In this section, we first introduce our tasks, data and experimental setup. Then, we report on: (i) experiments on the individual subtasks of opinion and type classification; (ii) the full task of predicting both type and sentiment. Moreover, we test the models (iii) to study their adaptability across domains, i.e., train on one domain and test on another. We here provide also learning curves that give an indication on the required amount and type of data and the scalability to other domains. Finally, we report on (iv) the applicability of the proposed structured model to another language, namely Italian, where we see that even in such a less-resourced setup our models outperform the commonly used feature-based methods.

This is a three-way classification task. We treat each comment as expressing positive, negative or neutral sentiment.

Since the challenge of sentiment analysis on YouTube data lies in the fact that a comment may express the sentiment not only towards the product shown in the video, but also the video itself, it is of important to distinguish between the opinion target, whether it is the video or product. For instance, users may post positive comments to the video while being generally negative about the product and vice versa. Hence, type classification is important.

Additionally, many comments are irrelevant for both the product and the video (off-topic) or may even contain spam. However, given that the main goal of sentiment analysis is to select sentiment-bearing comments and identify their polarity, we here do not consider distinguishing between off-topic and spam and conflate them into a single category (uninformative). Therefore, the comment type classification task is again a three-way decision: video, product and uninformative.

While the previously discussed sentiment and type identification tasks are useful to model and study in their own right, our end goal is: given a stream of comments, to jointly predict both the type and the sentiment of each comment.

Therefore, we cast the problem to a single multi-class classification task
                              7
                              We exclude comments annotated as both video and product. This enables the use of a simple flat multi-classifiers with seven categories for the full task, instead of a hierarchical multi-label classifiers (i.e., type classification first and then opinion polarity). The number of comments assigned to both product and video is relatively small (8% for TABLETS and 4% for AUTO).
                           
                           
                              7
                            with seven classes: the Cartesian product between {product, video} type labels and {positive, neutral, negative} sentiment labels, and the additional class for uninformative.


                        Tables 2 and 3
                         show the datasets for English and Italian, respectively. For both languages, we split the videos into 50% training (TRAIN) and 50% test set (TEST), such that each video contains all its comments. This ensures that all comments from the same video appear in either TRAIN or TEST. Since the number of comments per video varies, the resulting sizes of each set are different (we use the larger split for TRAIN). Table 2 shows the data distribution across the task-specific classes – sentiment and type classification. For the sentiment task, we exclude off-topic and spam comments as well as comments with ambiguous sentiment, i.e., annotated as both positive and negative.

For both languages, in the sentiment task the majority of comments have neutral polarity, while the negative class is the least frequent. Approximately 50% of the comments are neutral for the English dataset, and 42% in the Italian one. Interestingly, the ratios between polarities expressed in comments from AUTO and TABLETS are very similar across both TRAIN and TEST. This actually holds for both languages.

Conversely, for the type task, we observe that comments from AUTO are uniformly distributed among the three classes, while for the TABLETS the majority of comments are product related. This holds also for the Italian dataset, although it is slightly less balanced for AUTO (a bit more product related comments). However, also there we observe the majority of comments (69%) about the product. It is likely due to the nature of the TABLETS videos, which are more geek-oriented, where users are more prone to share their opinions and enter involved discussions about a product. In contrast, videos from the AUTO category (both commercials and user reviews) are more visually captivating and, being generally oriented towards a larger audience, generate more video-related comments. Regarding the full setting, where the goal is to have a joint prediction of the comment sentiment and type, we observe that video-negative and video-positive are the least frequent classes, which makes them the most difficult to learn and predict.

For English, several models were developed recently (Gimpel et al., 2011; Ritter, Clark, Mausam, & Etzioni, 2011), specifically tailored to process noisy inputs. They yield significant reductions in the error rate on user-generated texts, e.g., Twitter. Hence, we use the CMU Twitter pos-tagger (Gimpel et al., 2011; Owoputi et al., 2013) in our setup for English to obtain the part-of-speech tags. Our second component – the chunker – is the one developed by Ritter et al. (2011), which also comes with a model trained on Twitter data.
                           8
                           The chunker from Ritter et al. (2011) relies on its own POS tagger, however, in our structural representations we favor the POS tags from the CMU Twitter tagger and take only the chunk tags from the chunker.
                        
                        
                           8
                         It has been shown to perform better on noisy data such as user comments. Since the linguistic conventions used on Twitter and YouTube show similarities (Baldwin et al., 2013), we here exploit the existence of such tools to process YouTube data in contrast to tools trained on more well-edited text.

For Italian, however, no specific tools are available at the moment to process social media data. We therefore opted for general-purpose modules. This way, we can gauge how well our model still works in such a suboptimal setup. We obtain tokens, lemmas and part-of-speech tags through TextPro (Pianta, Girardi, & Zanoli, 2007)—a state-of-the-art NLP suite for Italian. To generate our shallow structural representations, we use the Berkeley parser trained on the Torino Treebank (Bosco & Lombardo, 2006). Given that we reconstruct chunks from a full syntactic parse tree, we opted for a simple flattening heuristic to obtain the chunk nodes. In particular, we remove all the intermediate nodes from a parse tree that are at the height greater than two. An example tree (where chunk nodes are obtained by flattening a full syntactic parse) is shown in Fig. 2.

Regarding sentiment lexicons: for English, we merge two manually constructed sentiment lexicons that are freely available, the MPQA Lexicon (Wilson et al., 2005) and the lexicon of Hu and Liu (2004). For Italian, we use the SentiStrength lexicon (Thelwall, Buckley, & Paltoglou, 2012).

For system evaluation, we measure accuracy (the proportion of correctly predicted labels) as well as the per-label F1, the balanced mean between precision (the proportion of correct predictions for that label) and recall (the proportion of correct predictions for that label compared to the gold standard).

We first present the results for the traditional in-domain setup, where both TRAIN and TEST come from the same domain, e.g., AUTO or TABLETS, respectively. Next, we show the learning curves to analyze the behavior of the FVEC and STRUCT models when the training size increases. Fnally, we perform a set of cross-domain experiments that describe the enhanced adaptability of the patterns generated by the STRUCT model.

We compare the FVEC and STRUCT models on the three tasks described in Section 5.1: sentiment, type and full. Table 4
                            reports the per-class performance and the overall accuracy of the multi-class classifier. We note that: first of all, the performance on TABLETS is much higher than on AUTO across all tasks. This can be explained by the following: (i) TABLETS contains more training data and (ii) videos from the AUTO and TABLETS categories draw different types of audiences – well-informed users and geeks expressing better-motivated opinions about a product for the former vs. more general audience for the latter. This results in the different quality of comments with the AUTO being more challenging to analyze.

Secondly and most importantly, we observe that the STRUCT model provides 1.5–3% absolute improvement in accuracy over the FVEC. This actually holds for all tasks. If we examine individual categories and the F1 scores, we see that F1 also improves with the STRUCT model, except for the negative classes for AUTO, where we see a small drop. We conjecture that the sentiment prediction for the AUTO category is largely driven by one-shot phrases and statements where it is hard to improve upon the bag-of-words and sentiment lexicon features. In contrast, comments from the TABLETS category tend to be more elaborated and well-argumented, thus, full benefiting from the extended expressiveness of the structural representation.

Finally, if we consider the per-class performance breakdown, we observe that correctly predicting negative sentiment is the most difficult task for both AUTO and TABLETS. This again is probably caused by the fact that there is a smaller proportion of the negative comments in the training set. For the type task, the video-related class is substantially more difficult than the product-related one for both domains. For the full task, the class video-negative accounts for the largest error. This is confirmed by the results from the previous sentiment and type tasks. Also there we saw that handling negative sentiment and detecting video-related comments turns out to be the most difficult.

In this section we examine what happens if we have different amounts of training data at our disposal. We examine the learning curves for both the FVEC and STRUCT models for increasing training set sizes. Intuitively, the STRUCT model relies on more general syntactic patterns and may overcome the sparseness problems incurred by the FVEC model when little training data is available.

As shown in Fig. 4
                           , the STRUCT model consistently outperforms the FVEC across all training sizes. This also holds for the case very little training data is available, although both learning curves for sentiment and type classification tasks across both product categories show that there is not that large of an advantageas expected for very little data. The STRUCT model outperforms the FVEC model by an almost constant margin. As we will see next, this picture considerably changes when we apply the model across domains.

To examine the performance of our classifiers on other YouTube domains, we perform a set of cross-domain experiments. This means that we train a model on the data from one product category and test it on data from the other. This is important as it allows us to examine the adaptability of our models and estimate how much and whether we need training data for a new domain.


                           Table 5
                            reports the accuracy for the three tasks when we use all comments (TRAIN+TEST) from AUTO to predict on the TEST from TABLETS, and the other way around (TABLETS
                           
 →

                           AUTO). When we use AUTO as a source domain, the STRUCT model provides an 1–3% absolute improvement in accuracy, except for the sentiment task where it reaches baseline performance. All improvements are significant, thus showing that our structure representation is way more robust across domains than the vector based model.

Similar to the in-domain experiments, we studied the effect of the source domain size on the target test performance. This is important to assess the adaptability of features exploited by the FVEC and STRUCT models with the change in the number of labeled examples available for training. Additionally, we considered a setting including a small amount of training data from the target data (i.e., supervised domain adaptation).

The learning curves of the FVEC and STRUCT models applied to the sentiment and type tasks are shown in Fig. 5
                           : AUTO is used as the source domain to train models, which are tested on TABLETS.
                              9
                              Results for the other direction (TABLETS
                                 
 →

                                 AUTO) show similar behavior and are thus omitted.
                           
                           
                              9
                            The plot shows that when little training data is available, the features generated by the STRUCT model are much more robust and show better adaptability, up to 10% absolute improvement over FVEC (30% of relative improvement). The bag-of-words model seems to be affected by the data sparsity problem, which becomes a crucial issue when only a small training set is available. This difference becomes smaller as we add data from the same domain (see the shaded area in the Fig. 5). This is an important advantage of our structural approach, since we cannot realistically expect to obtain manual annotations for 10k
                              +
                            comments for each (of the many thousands) product domains present on YouTube.

The Italian data poses several additional challenges for our approach: firstly, such data is much smaller, about 1/5–1/3 of the corresponding English one depending on the task (cf. Table 3). Note that we collected the same set of products for both languages. For some products, however, we were not able to find any corresponding Italian videos. Even for well-represented products, the average number of comments per video is much lower for Italian (50 comments per video) than for English (about 200 comments per video). Table 3 describes the Italian corpus.

Secondly, the Italian preprocessing is less robust. In particular, we do not have any social media-specific models for Italian. Our way to generate chunks is error-prone as well: we rely on a parser (trained on a relatively small corpus) and some flattening heuristics to obtain Italian chunk trees. English trees, on the contrary, are created through a toolkit trained on social media specifically for the chunking task on a much larger dataset.

Finally, to our knowledge, there are no high-coverage sentiment lexical resources for Italian available yet. We did run some experiments on inducing an Italian sentiment dictionary from the SentiWordNet (Baccianella, Esuli, & Sebastiani, 2010) via the MultiWordNet synset mappings (Pianta, Bentivogli, & Girardi, 2002), following the approach of Basile and Nissim (2013). This induced dictionary, however, is too noisy (for example, the word “avere” (“to have”) is marked as positive) and, at the same time, has a limited coverage. The SentiStrength lexicon showed a better performance in our pilot experiments and was therefore adopted for the current study. However, SentiStrength has a much lower coverage (our English lexicon is 4× larger).

The latter two issues are crucial for the STRUCT model: since our structural representations integrate different types of linguistic (part-of-speech, chunks) and task-specific (sentiment clues) evidence, they can get noisy when the preprocessing becomes less accurate. We therefore expect the difference between the models to be less pronounced for Italian than for English.


                        Table 6
                         reports the results for the in-domain and cross-domain experiments on Italian. Despite the suboptimal setup, our evaluation experiments actually show that the structural representation works also for Italian. In both experiments (in-domain and across-domain) we do see an improvement up to 4.2% absolute in accuracy (for type classification from AUTO to TABLETS). Thus, the same trend holds for both Italian and English, with the only exceptions of (i) the in-domain type classification of TABLETS, where both models for Italian reach the same performance; and (ii) AUTO to TABLETS sentiment classification, where STRUCT slightly decreases the system accuracy (not statistically significant result), and (iii) only minor improvements were obtained for the in-domain TABLETS setup.

@&#DISCUSSION@&#

Our STRUCT model is more accurate that the FVEC model since it is able to induce structural patterns of sentiment. Consider the following comment: optimus pad is better. this xoom is just to bulky but optimus pad offers better functionality. The FVEC bag-of-words model misclassifies it to be positive, since it contains two positive expressions (better, better functionality) that outweigh a single negative expression (bulky). The structural model, in contrast, is able to identify the product of interest (xoom) and associate it with the negative expression through a structural feature and it thus correctly classifies the comment as negative.

However, our model has its limitations as well. For instance, the largest group of errors are implicit sentiments. Thus, some comments do not contain any explicit positive or negative opinion, but provide detailed and well-argumented criticism, for example, this phone is heavy. Such comments might also include irony. To account for these cases, a deep understanding of the product domain is necessary, for instance, also by linking product aspects to the actual product.

@&#CONCLUSIONS AND FUTURE WORK@&#

This paper presents the results of a research effort targeting opinion mining on YouTube comments. We tackled the problem as multi-class supervised classification task, where the goal is to detect the comment type and polarity. A peculiarity of our approach is that we distinguish between video and product related opinions. We proposed a novel structural representation based on shallow syntactic trees enriched with additional conceptual information and show that it outperforms traditional approaches that are based on bag-of-words models.

To sum up, our research effort comprises the following contributions: (i) it shows that effective OM can be carried out with supervised models trained on high quality annotations; (ii) it introduces a novel manually annotated multilingual corpus of YouTube comments, which we make available for the research community; (iii) it defines new structural models and kernels, which can improve on feature vectors, e.g., up to 30% of relative improvement in type classification, when little data is available, and demonstrates that the structural model scales well to other domains; (iv) it addresses the opinion mining task in a multilingual setting, showing that our structural models are robust enough to be used for languages other than English.

For future work, we plan to work on a joint model to classify all the comments of a given video, such that it is possible to exploit latent dependencies between entities and the sentiments of the comment thread. Additionally, we plan to experiment with hierarchical multi-label classifiers for the full task (in place of a flat multi-class learner) and perform cross-lingual experiments, in order to exploit the data from one language to predict opinions on the other.

@&#ACKNOWLEDGMENTS@&#

The authors are supported by a Google Faculty Award 2011, the Google Europe Fellowship Award 2013 and the European Community’s Seventh Framework Programme (FP7/2007-2013) under the grant #288024: LiMoSINe. We thank the HLT group at Fondazione Bruno Kessler for providing us with the syntactic parsing model for Italian.

@&#REFERENCES@&#

