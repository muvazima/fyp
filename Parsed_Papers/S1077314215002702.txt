@&#MAIN-TITLE@&#Spontaneous micro-expression spotting via geometric deformation modeling

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A probabilistic framework is proposed to detect spontaneous micro-expression clips.


                        
                        
                           
                           The geometric deformation captured by ASM model is utilized as features.


                        
                        
                           
                           The features are robust to subtle head movement and illumination variation.


                        
                        
                           
                           The Adaboost algorithm is used to estimate the initial probability for each frame.


                        
                        
                           
                           The random walk algorithm computes the transition probability by deformation similarity.


                        
                        
                           
                           Extensive experiments are performed on two spontaneous datasets.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Micro-expression spotting

Random walk

Active shape model

Geometric deformation

Adaboost

@&#ABSTRACT@&#


               
               
                  Facial micro-expression is important and prevalent as it reveals the actual emotion of humans. Especially, the automated micro-expression analysis substituted for humans begins to gain the attention recently. However, largely unsolved problems of detecting micro-expressions for subsequent analysis need to be addressed sequentially, such as subtle head movements and unconstrained lighting conditions. To face these challenges, we propose a probabilistic framework to detect spontaneous micro-expression clips temporally from a video sequence (micro-expression spotting) in this paper. In the probabilistic framework, a random walk model is presented to calculate the probability of individual frames having micro-expressions. The Adaboost model is utilized to estimate the initial probability for each frame and the correlation between frames would be considered into the random walk model. The active shape model and Procrustes analysis, which are robust to the head movement and lighting variation, are used to describe the geometric shape of human face. Then the geometric deformation would be modeled and used for Adaboost training. Through performing the experiments on two spontaneous micro-expression datasets, we verify the effectiveness of our proposed micro-expression spotting approach.
               
            

@&#INTRODUCTION@&#

Emotion exists in humans’ life, which can be revealed by external behaviors, such as vocal expression, facial expression, and sign expression. Among these behaviors, the facial expression plays a vital role in analyzing human emotions [1]. Therefore, the facial expression has attracted much attention in psychological studies [2]. Besides, with the development of computer science, the facial expression analysis and recognition become popular in fields of computer vision and pattern recognition [3]. Various machine learning techniques have been employed to automatically analyze and recognize facial expression from visual images or videos, such as local binary pattern (LBP) [4] and hidden Markov model (HMM) [5]. Most of these works are devoted to the macro-expression while the micro-expression manifests more affective information [6].

In contrast to macro-expression, the micro-expression is a brief, involuntary facial expression shown on the face of humans, which usually sustains from 1/25 to 1/5 s and has a period of onset, apex, and offset [7]. The psychological studies have shown the importance of micro-expression revealing the suppressed affect of humans, which helps to understand humans’ deceitful behaviors. Consequently, there are large amounts of areas to apply the micro-expression analysis and recognition, such as lie detection, police case diagnosis, business negotiation, psychoanalyzing, and so on. Due to the short duration and involuntariness of micro-expressions, it is very difficult for untrained people to detect and analyze micro-expressions. Even trained by professional tools, such as the micro expression training tool (METT) [8], numerous works might be accomplished manually by professionals to detect and analyze micro-expressions from videos. Therefore, the automated detection and analysis of micro-expressions would be very valuable and help people promote the performance of analyzing large amounts of video sequences.

The automated micro-expression detection from temporal video sequences attracts few attentions while some works have been devoted to the micro-expression recognition based on well-segmented video sequences containing the micro-expressions. Although the micro-expression detection is more fundamental to subsequent micro-expression analysis and recognition, however, few works have been presented to detect micro-expressions [9,10]. To apply the detection in real-life scenarios, several problems of micro-expression context would be addressed. The very small head movements and heterogeneous ambient lighting conditions induce the context complexity. So the head movements and lighting variation have potentially significant effects on subtle changes of micro-expressions. To face these challenges, we present a framework to detect consecutive frame clips having micro-expressions from video sequences, which would be robust for small head movement and lighting variation. In addition, the deliberate micro-expressions differ greatly from the spontaneous ones as they are controlled by different motor pathways [11]. Since spontaneous micro-expressions can be observed frequently in real life and reveal more affective information of humans, we focus on the problem of detecting spontaneous micro-expression frame clips temporally from video sequences, which is called as spontaneous micro-expression spotting in this context.

To be dedicated to the problem of micro-expression spotting, we propose a random walk framework to detect frame clips having micro-expressions in video sequences. In the probabilistic model, the main contributions of our proposed approach are summarized as follows

                        
                           •
                           The random walk (RM) model is applied to compute the probability of frames having micro-expressions. The model can leverage the deformation correlation between frames and spot the consecutive frame clips with micro-expressions.

The Adaboost algorithm is utilized to compute the initial probability of individual frames having micro-expressions in the RW procedure. The thresholding weak classifiers have been trained for obtaining the best geometric features for micro-expression spotting.

In order to prevent influences of the head movement and lighting variation, a revised active shape model (ASM) is used to locate the landmarks and then the Procrustes analysis is presented to align these landmarks. The geometric deformations of landmarks are modeled as the features of classifier training and used to compute the transition probability in RW model.

The rest of this paper is organized as follows. Section 2 reviews the related work briefly and our proposed probabilistic framework for micro-expression spotting is presented in Section 3. Then we discuss the experimental results for algorithm evaluation in Section 4. Finally, Section 5 describes our conclusions.

@&#RELATED WORK@&#

In this section, the research on micro-expression is summarized in psychology and computer science. The psychology studies are presented briefly to demonstrate the characteristics of micro-expressions. The techniques for micro-expression analysis in computer science are described to indicate the shifted focus of research.

In psychology, micro-expression has been studied for many decades since it was first discovered in motion picture films of psychotherapy hours [12]. The micro-expression was used to indicate the non-verbal communication between patient and therapist. As a pioneer work, Ekman et al. developed the facial action coding system (FACS) to describe human facial movements for facial expressions by facial action units [13]. Then the micro-expression was treated as complex combinations of facial action units. Through the thorough studies by Gottman [14] and Ekman [15], micro-expressions can be observed in psychological experiments and training programs for learning them have been presented. According to studies of psychology, the humans are not good at detecting and recognizing micro-expressions. The experiments of micro-expression testing performed by Frank et al. [16] have shown that participants cannot achieve good detection accuracy even if these participants were US undergraduates. The difficulties of human analysis lead to the promising research in computer science.

In computer science, some primary studies have been done for micro-expression analysis, which contains micro-expression spotting and recognizing. In the beginning, the scholars in computer vision constructed few datasets for detecting and recognizing acted micro-expressions. Polikovsky et al. [17] proposed a recognition algorithm with the gradient orientation histograms descriptors, which collected data by simulating micro-expressions of undergraduates on high-speed cameras (Polikovsky’s dataset). In [18], the authors presented the strain patterns for detecting the macro-expressions and micro-expressions through gathering acted data from some subjects (USF-HD dataset). However, the acted micro-expressions are greatly different from the natural facial expressions [19]. Therefore, several works have been done on the natural facial micro-expressions (spontaneous micro-expressions), which are induced by watching specific videos. Li et al. [20] constructed a Spontaneous micro-expression corpus (SMIC) dataset with 16 subjects and 164 spontaneous micro-expressions. Based on the SMIC dataset, Pfister et al. presented a recognition algorithm using the LBP-TOP feature and SVM, multiple kernel classifiers or random forests classifiers to recognize negative or positive micro-expressions [21]. In [10], the feature difference based on LBP is used to spot the frames containing spontaneous micro-expressions. The Chinese Academy of Sciences Micro-Expression database (CASME) has been reported by Yan et al. [22] containing 19 subjects and more than 190 micro-expressions. And an improved version of CASME has been published with more classes of micro-expressions [23]. Wang et al. extended the LBP-TOP feature into the tensor independent color space and then recognized micro-expressions in the tensor subspace [24]. In [25], feature points have been tracked and used to recognize the specific micro-expressions, such as happiness and disgust.

In tasks of recognizing micro-expressions, so far, it is unclear how many micro-expressions can be recognized. Aforementioned algorithms were proposed to aim at different tasks, such as positive/negative micro-expressions, happiness/disgust micro-expressions. This needs to be further studied in both psychology and computer science. Moreover, these recognition algorithms are appearance-based methods and use the representations based on LBP and its variants.

The task of detecting micro-expressions is prior to micro-expression recognition as the recognition depends on the detection. For detecting the micro-expressions, two types of detection tasks have been defined. The detection task in [21,24,25] is defined to classify videos containing micro-expressions among videos while the other detection task is to find temporal frames having micro-expressions from a video sequence in [9,10]. To distinguish these two types of detection tasks, we call the latter detection task as micro-expression spotting, which is focused on the problem of detecting the temporal locations with micro-expressions. Compared to the spotting task in [9,10], we further spot the consecutive frame clips having micro-expressions from video sequences, more than discrete frames. The research of micro-expression spotting is in an initial stage and has not fully considered the problem of head movements and lighting variation. In this paper, we propose a novel framework modeling geometric deformation to alleviate them.

Given N frames in a video sequence F, these frames are denoted as 
                        
                           
                              f
                              1
                           
                           ,
                           ⋯
                           ,
                           
                              f
                              i
                           
                           ,
                           ⋯
                           ,
                        
                      
                     fN
                     . The spotting task is to split the frame clip Fe
                      with micro-expressions, assuming 
                        
                           
                              f
                              
                                 i
                                 1
                              
                           
                           ,
                           ⋯
                           ,
                           
                              f
                              
                                 i
                                 2
                              
                           
                        
                      (i1 > 1, i2 < N), from the sequence F. The index range [i1, i2] is a consecutive interval.


                     Fig. 1
                      presents the procedure of our proposed approach for spotting micro-expression frame clips, in which an RW model is constructed to compute the probability of frames having micro-expressions. To estimate the initial possibility in RW model, we learn an Adaboost model for computing the probability of individual frames with micro-expressions. Then we utilize the geometric deformation correlation of frames in a temporal window between sequences to measure the probability of these frames with micro-expressions, which would be used as the transition probability in the RW model. To avoid the impact of head movements and lighting variation, an advised ASM face model is utilized to describe the geometric shape of faces, and the Procrustes analysis is used to align these shape points. After alignment, the subtle geometric deformations are calculated for the random walk model.

To describe the geometric shapes of faces in video sequences, we utilize the promoted ASM model to extract the landmarks of face shapes. As the faces are detected by the face detector [26], a shape of one face is then modeled by a point distribution. With collecting shapes from faces manually, the ASM model analyzes the statistics of shape points by a principle component analysis (PCA) procedure. Then, the point distribution is modeled as the template for new face shape matching.

Many modified ASM algorithms have been presented since the classical ASM algorithm [27] was proposed. These algorithms attempt to fit the template shape using more accurate searching orientation and multi-solution technique. Here, we utilize the STASM algorithm [28] to detect the shapes of face images. In the fitting procedure, the STASM algorithm
                           1
                        
                        
                           1
                           
                              http://www.milbo.users.sonic.net/stasm/.
                         uses the simplified SIFT features as descriptors and matches local landmarks with the multivariate adaptive regression splines (MARS) method. For matching the face shape model robustly and rapidly, the multi-resolution approach is employed and the searching of landmarks is implemented in the image pyramid. Totally, 77 landmarks are located in one facial image.

Due to the local SIFT feature, the STASM algorithm is robust for the lighting variation. However, small head movements would induce the changes in the shape points of STASM algorithm for a video sequence. Therefore, we use the Procrustes analysis approach to align these shape points in frames.

The Procrustes analysis aligns the shape points to one shape (usually the first shape) by normalizing the orientation, scale, and translation parameters. The aligned point distribution of a face shape in ith frame is denoted as 
                           
                              
                                 z
                                 i
                              
                              =
                              
                                 
                                    (
                                    
                                       x
                                       
                                          i
                                          1
                                       
                                    
                                    ,
                                    
                                       y
                                       
                                          i
                                          1
                                       
                                    
                                    ,
                                    ⋯
                                    ,
                                    
                                       x
                                       
                                          i
                                          k
                                       
                                    
                                    ,
                                    
                                       y
                                       
                                          i
                                          k
                                       
                                    
                                    ,
                                    ⋯
                                    ,
                                    
                                       x
                                       
                                          i
                                          n
                                       
                                    
                                    ,
                                    
                                       y
                                       
                                          i
                                          n
                                       
                                    
                                    )
                                 
                                 T
                              
                              ,
                           
                         where n is the number of shape points. Let M(z; s, θ, t) be the rotation, scaling, translation transformation for one shape z (a point distribution vector). The alignment of two shapes, z
                        
                           i
                         and z
                        
                           j
                        , is equal to the mapping of shape z
                        
                           j
                         to shape z
                        
                           i
                        . It is achieved by minimizing the weighted sum

                           
                              (1)
                              
                                 
                                    E
                                    =
                                    
                                       
                                          (
                                          
                                             z
                                             i
                                          
                                          −
                                          M
                                          
                                             (
                                             
                                                z
                                                j
                                             
                                             ;
                                             s
                                             ,
                                             θ
                                             ,
                                             t
                                             )
                                          
                                          )
                                       
                                       T
                                    
                                    W
                                    
                                       (
                                       
                                          z
                                          i
                                       
                                       −
                                       M
                                       
                                          (
                                          
                                             z
                                             j
                                          
                                          ;
                                          s
                                          ,
                                          θ
                                          ,
                                          t
                                          )
                                       
                                       )
                                    
                                 
                              
                           
                        where W is the weight matrix and M(z
                        
                           j
                        ; s, θ, t) is the transformation of shape vector z
                        
                           j
                         to z
                        
                           i
                        . It is defined as follows

                           
                              (2)
                              
                                 
                                    
                                       
                                          
                                             M
                                             (
                                             
                                                z
                                                j
                                             
                                             ;
                                             s
                                             ,
                                             θ
                                             ,
                                             t
                                             )
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             (
                                             
                                                
                                                   
                                                      
                                                         s
                                                         ·
                                                         cos
                                                         
                                                         θ
                                                         ·
                                                         
                                                            x
                                                            
                                                               j
                                                               k
                                                            
                                                         
                                                         −
                                                         s
                                                         ·
                                                         sin
                                                         
                                                         θ
                                                         ·
                                                         
                                                            y
                                                            
                                                               j
                                                               k
                                                            
                                                         
                                                         −
                                                         
                                                            t
                                                            
                                                               j
                                                               x
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         s
                                                         ·
                                                         sin
                                                         
                                                         θ
                                                         ·
                                                         
                                                            x
                                                            
                                                               j
                                                               k
                                                            
                                                         
                                                         +
                                                         s
                                                         ·
                                                         cos
                                                         
                                                         θ
                                                         ·
                                                         
                                                            y
                                                            
                                                               j
                                                               k
                                                            
                                                         
                                                         −
                                                         
                                                            t
                                                            
                                                               j
                                                               y
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                             )
                                          
                                       
                                    
                                    
                                       
                                          
                                             z
                                             i
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                (
                                                
                                                   x
                                                   
                                                      i
                                                      1
                                                   
                                                
                                                ,
                                                
                                                   y
                                                   
                                                      i
                                                      1
                                                   
                                                
                                                ,
                                                ⋯
                                                ,
                                                
                                                   x
                                                   
                                                      i
                                                      k
                                                   
                                                
                                                ,
                                                
                                                   y
                                                   
                                                      i
                                                      k
                                                   
                                                
                                                ,
                                                ⋯
                                                ,
                                                
                                                   x
                                                   
                                                      i
                                                      n
                                                   
                                                
                                                ,
                                                
                                                   y
                                                   
                                                      i
                                                      n
                                                   
                                                
                                                )
                                             
                                             T
                                          
                                       
                                    
                                    
                                       
                                          
                                             z
                                             j
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                (
                                                
                                                   x
                                                   
                                                      j
                                                      1
                                                   
                                                
                                                ,
                                                
                                                   y
                                                   
                                                      j
                                                      1
                                                   
                                                
                                                ,
                                                ⋯
                                                ,
                                                
                                                   x
                                                   
                                                      j
                                                      k
                                                   
                                                
                                                ,
                                                
                                                   y
                                                   
                                                      j
                                                      k
                                                   
                                                
                                                ,
                                                ⋯
                                                ,
                                                
                                                   x
                                                   
                                                      j
                                                      n
                                                   
                                                
                                                ,
                                                
                                                   y
                                                   
                                                      j
                                                      n
                                                   
                                                
                                                )
                                             
                                             T
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 t
                                 j
                              
                              =
                              
                                 
                                    (
                                    
                                       t
                                       
                                          j
                                          x
                                       
                                    
                                    ,
                                    
                                       t
                                       
                                          j
                                          y
                                       
                                    
                                    )
                                 
                                 T
                              
                           
                         represents the translation vector of shape z
                        
                           j
                         along the axis x and y. The Procrustes analysis algorithm aligns shapes by iteratively normalizing the parameters s, θ, t 
                        [27]. Fig. 2
                         shows the effectiveness of Procrustes analysis to align shape points.

In this paper, we utilize the geometric deformation as features for Adaboost model training. Based on the point distribution of faces, we define the geometric deformations to describe the local motion between frames. In this context, two types of deformations are considered. Firstly, the local motion 
                           
                              v
                              i
                              b
                           
                         between current frame fi
                         and baseline frames is modeled as

                           
                              (3)
                              
                                 
                                    
                                       v
                                       i
                                       b
                                    
                                    =
                                    
                                       (
                                       
                                          z
                                          i
                                       
                                       −
                                       
                                          z
                                          b
                                       
                                       )
                                    
                                 
                              
                           
                        where z
                        
                           b
                         represents the point distribution of baseline face. z
                        
                           b
                         could be valued by the point distribution of starting frame or the average point distribution of first few frames as we assume the expressions in first few frames are neutral.

Then, the temporal motion of shape points is built as the second type of motions. The temporal motion 
                           
                              v
                              i
                              d
                           
                         in a temporal window can be described by temporal difference. So we compute the difference approximately as follows

                           
                              (4)
                              
                                 
                                    
                                       v
                                       i
                                       d
                                    
                                    =
                                    
                                       (
                                       
                                          z
                                          i
                                       
                                       −
                                       
                                          z
                                          a
                                       
                                       )
                                    
                                 
                              
                           
                        where z
                        
                           a
                         represents the average point distribution in an interval 
                           
                              [
                              i
                              −
                              L
                              /
                              2
                              ,
                              i
                              +
                              L
                              /
                              2
                              ]
                           
                        .

These two types of geometric deformations describe different motion patterns of shape points. 
                           
                              v
                              i
                              b
                           
                         represents the motion of frames with micro-expressions compared to the neutral frames while 
                           
                              v
                              i
                              d
                           
                         describes the small motions within frames having micro-expressions. Consequently, we combine these two geometric deformations and compute the final geometric deformation v
                        
                           i
                         as

                           
                              (5)
                              
                                 
                                    
                                       v
                                       i
                                    
                                    =
                                    β
                                    
                                       v
                                       i
                                       b
                                    
                                    /
                                    σ
                                    
                                       (
                                       
                                          v
                                          b
                                       
                                       )
                                    
                                    +
                                    
                                       (
                                       1
                                       −
                                       β
                                       )
                                    
                                    
                                       v
                                       i
                                       d
                                    
                                    /
                                    σ
                                    
                                       (
                                       
                                          v
                                          d
                                       
                                       )
                                    
                                 
                              
                           
                        where σ(v
                        
                           b
                        ) and σ(v
                        
                           d
                        ) are the variances of two types of motions in a video sequence, respectively, which are used to normalize the variation.

After the geometric deformation vector v
                        
                           i
                         is calculated, an Adaboost algorithm using thresholding weak classifiers could be trained. Given the label ci
                         ∈ {0, 1} (0 for no micro-expressions, 1 for having micro-expressions) for frame fi
                        , the model training is processed as in [29]. The jth weak classifier we used in Adaboost model is defined as

                           
                              (6)
                              
                                 
                                    
                                       h
                                       j
                                    
                                    
                                       (
                                       x
                                       )
                                    
                                    =
                                    
                                       {
                                       
                                          
                                             
                                                
                                                   −
                                                   1
                                                
                                             
                                             
                                                
                                                   
                                                      if
                                                      
                                                      |
                                                   
                                                   
                                                      v
                                                      
                                                         j
                                                         k
                                                      
                                                   
                                                   
                                                      |
                                                      <
                                                   
                                                   
                                                      δ
                                                      j
                                                   
                                                
                                             
                                          
                                          
                                             
                                                1
                                             
                                             
                                                otherwise
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where vjk
                         denotes the kth feature of vector v
                        
                           j
                        . In each round, the weak classifier could be trained by minimizing the weighted classification error with an optimal threshold. The final strong classifier can be linearly concatenated as 
                           
                              H
                              
                                 (
                                 x
                                 )
                              
                              =
                              sign
                              
                                 (
                                 
                                    ∑
                                    
                                       j
                                       =
                                       1
                                    
                                    M
                                 
                                 
                                    γ
                                    j
                                 
                                 
                                    h
                                    j
                                 
                                 
                                    (
                                    x
                                    )
                                 
                                 )
                              
                              ,
                           
                         where M is the number of weak classifiers.

The random walk process has been used in many applications, such as image annotation and retrieval [30]. In this paper, we utilize the RW model to compute the probability of frames having micro-expressions, considering the deformation correlation between frames in a temporal window.

For ith frame fi
                         in a video sequence F, we extract L frames as a temporal window centered in this frame. The width of temporal window is 
                           
                              L
                              +
                              1
                           
                         and the frames in range 
                           
                              [
                              i
                              −
                              L
                              /
                              2
                              ,
                              i
                              +
                              L
                              /
                              2
                              ]
                           
                         would be extracted. Therefore, there are 
                           
                              L
                              +
                              1
                           
                         nodes in the RW process, in which each node corresponds to one frame in the temporal window. Then the RW process is formulated as

                           
                              (7)
                              
                                 
                                    
                                       p
                                       
                                          t
                                          +
                                          1
                                       
                                    
                                    
                                       (
                                       i
                                       )
                                    
                                    =
                                    α
                                    
                                       ∑
                                       
                                          
                                             
                                                
                                                   j
                                                   ∈
                                                   
                                                      Ω
                                                      
                                                         f
                                                         i
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       p
                                       t
                                    
                                    
                                       (
                                       j
                                       )
                                    
                                    ϕ
                                    
                                       (
                                       i
                                       ,
                                       j
                                       )
                                    
                                    +
                                    
                                       (
                                       1
                                       −
                                       α
                                       )
                                    
                                    
                                       p
                                       0
                                    
                                    
                                       (
                                       i
                                       )
                                    
                                 
                              
                           
                        where 
                           
                              Ω
                              
                                 f
                                 i
                              
                           
                         is the temporal neighbor set being adjacent to the ith frame fi. ϕ(i, j) is the transition probability from frame fi
                         to fj
                        , and p
                        0(i) is the initial probability for the ith frame having micro-expressions. α ∈ [0, 1] is a linear weight between two terms. The probability of ith frame at the tth iteration is denoted as pt
                        (i).

In our proposed method, the initial probability p
                        0(i) is estimated from the Adaboost model, which would be introduced in Section 3.2. In this context, we define the transition probability using the deformation similarity of two frames, that is

                           
                              (8)
                              
                                 
                                    ϕ
                                    
                                       (
                                       i
                                       ,
                                       j
                                       )
                                    
                                    =
                                    
                                       
                                          
                                             p
                                             t
                                          
                                          
                                             (
                                             j
                                             )
                                          
                                          
                                             s
                                             
                                                i
                                                j
                                             
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                k
                                                ∈
                                                
                                                   Ω
                                                   
                                                      f
                                                      i
                                                   
                                                
                                             
                                          
                                          
                                             p
                                             t
                                          
                                          
                                             (
                                             k
                                             )
                                          
                                          
                                             s
                                             
                                                i
                                                k
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where sij
                         can be computed according to the geometric deformation between frames. According to (5), it is defined as 
                           
                              
                                 s
                                 
                                    i
                                    j
                                 
                              
                              =
                              
                                 e
                                 
                                    
                                       −
                                       ∥
                                    
                                    
                                       v
                                       i
                                    
                                    −
                                    
                                       v
                                       j
                                    
                                    
                                       
                                          ∥
                                       
                                       2
                                    
                                 
                              
                           
                        .

Through the random walk process, the probability of individual frames having micro-expressions could be computed. Then the binary operation would be performed on probabilities of frames by thresholding to predict the presence of micro-expressions. To eliminate the isolated frame with micro-expressions and obtain consecutive frame clips, we regard the frame predicted negative as positive if there are at least 
                           
                              L
                              −
                              2
                           
                         frames predicted positive around it in the 
                           
                              L
                              +
                              1
                           
                         temporal window. After adjusted, the frame clip with micro-expressions can be spotted.

Finally, the spotting algorithm based on RW model is presented in Algorithm 1. It consists of three main procedures: geometric deformation computation, Adaboost training, and the random walk process.
                     

In this section, we present the details of our experiments, including the data sets we used and approaches for comparison.

Two spontaneous micro-expression datasets are used to evaluate the performance of our proposed approach in our experiments: SMIC dataset [20] and CASME dataset [22]. Both of them are designed to detect and recognize micro-expressions, which are constructed by inducing subjects’ micro-expressions. These two corpuses have following characteristics:

                           
                              •
                              The SMIC dataset contains 164 spontaneous micro-expressions from 16 subjects. These participants undergo high emotional arousal and suppress their facial expressions in an interrogation room setting with a punishment threat and highly emotional clips. The database collectors cooperate with the psychologists and achieve micro-expressions under the psychological rules.

The CASME dataset has 195 micro-expressions from 19 subjects. It uses similar procedures like SMIC to elicit micro-expressions from subjects. Because of the creators with the psychological background, these expressions are obtained from more strict lab situations and labeled more accurately. More emotions and action units (AUs) are labeled by psychologists.

For spotting the frames with micro-expressions, we utilize the raw video sequences of two datasets, rather than the cropped sequences. To train an Adaboost model, we split each dataset into the training and testing subset by a proportion of 30%/70%. In training subset, we use 5-folds method to determine appropriate model parameters.

To evaluate the performance of our micro-expression spotting algorithm, we utilize the receiver operating characteristic (ROC) curve and area under the ROC curve (AUC) of spotting micro-expression clips as the experimental criteria. Denoted spotted video clip as fs
                         and the ground-truth clip fg
                         for one video sequence, the positive spotting results are decided to be true positive or false positive by

                           
                              (9)
                              
                                 
                                    
                                       
                                          
                                             true
                                             
                                             
                                             
                                             positive
                                             :
                                             
                                             
                                                
                                                   
                                                      f
                                                      i
                                                      s
                                                   
                                                   ∩
                                                   
                                                      f
                                                      i
                                                      g
                                                   
                                                
                                                
                                                   
                                                      f
                                                      i
                                                      s
                                                   
                                                   ∪
                                                   
                                                      f
                                                      i
                                                      g
                                                   
                                                
                                             
                                             >
                                             η
                                          
                                       
                                    
                                    
                                       
                                          
                                             false
                                             
                                             
                                             
                                             positive
                                             :
                                             
                                             
                                                
                                                   
                                                      f
                                                      i
                                                      s
                                                   
                                                   ∩
                                                   
                                                      f
                                                      i
                                                      g
                                                   
                                                
                                                
                                                   
                                                      f
                                                      i
                                                      s
                                                   
                                                   ∪
                                                   
                                                      f
                                                      i
                                                      g
                                                   
                                                
                                             
                                             ≤
                                             η
                                          
                                       
                                    
                                 
                              
                           
                        This criterion measures
                         the overlap ratio of spotted video clip divided with ground-truth video clip. Then the true positive rate rp
                         is computed as 
                           
                              
                                 r
                                 p
                              
                              =
                              
                                 K
                                 p
                              
                              /
                              K
                              ,
                           
                         where Kp
                         and K are the numbers of true positive clips and total ground truth clips for evaluation.

Furthermore, we perform experiments to determine the optimal parameters for models. We conduct experiments to observe the impact of the number of weak classifiers and tuning parameters. The true positive rate is used to measure the impact of different parameters fixing the other parameters.

@&#EXPERIMENTAL RESULTS@&#

The experimental results of spotting frame clips with micro-expressions are shown in Table 1, Figs. 3 and 4
                        
                        
                        
                        
                        . For each sequence (generated from a video), consecutive frame clips with micro-expressions are spotted and used to calculate the ROC curve compared to the ground truth. The threshold η of overlap ratio in (9) is set to 0.5.

In this context, we compute ROC curves by varying the thresholding value of probability p(i) in (7). According to experiments, some frames without micro-expressions might have high probability as those shape points cannot be located very accurately. So we choose 0.8 as the upper bound of probabilities. As the ground truth datasets do not provide the true negatives for video clips, using false positives for ROC curves is more reasonable than the false positive rate. To compute the AUC in interval [0, 1], we normalize the false positives by dividing the maximum of false positives.


                        Table 1 summarizes the AUCs of different spotting approaches on SMIC and CASME datasets. In the strain pattern (SP) spotting algorithm [9], the optical flow algorithm is computed according to the implementation in [31]. The feature difference analysis (FDA) [10] utilizes the LBP difference in average to spot the micro-expressions. In contrast to our Adaboost algorithm, we use the support vector machine (SVM) classifier as a baseline approach to estimate the initial probability in our proposed RW model, where the LIBSVM
                           2
                        
                        
                           2
                           
                              http://www.csie.ntu.edu.tw/~cjlin/libsvm/ .
                         is used as its implementation. Moreover, the experiments of merely using Adaboost algorithm are used to demonstrate the impact of our RW model.

The experiments performed on CASME dataset are performed similarly to the experiments on SMIC dataset, respectively shown in Figs. 3 and 4. From two experiments, it is observed that our algorithm (RW-Adaboost) is more effective than other approaches. The random walk procedure could boost the classifiers by considering the deformation correlation between frames. Combined with experiments on two datasets, the classifier component in RW framework for estimating initial probability plays an important role for improving the ultimate performance. For the micro-expression spotting task, the Adaboost algorithm outperforms the SVM classifier in our RW model.

In the experiments, we achieve poorer performance of SP algorithm on SMIC. Compared to the SMIC dataset, the main difference between the two datasets is that the shooting conditions in CASME are more controlled than SMIC. The videos in CASME dataset are recorded in a uniform lighting environment and have more simple background while videos in SMIC dataset are in lower light. Besides, the face size of SMIC dataset is smaller relative to the total image size than the CASME dataset. From the results, it is concluded that the strain patterns based on optical flow could be impacted by lighting variation and face size. Because the head movements are handled elaborately by Procrustes analysis in our method and more deformation correlations are considered, our approach outperforms better than the FDA algorithm.

To quantitatively measure the impact of landmarks detection, the random displacement of landmarks’ location is used to disturb the detection of landmarks. We randomly select several landmarks and shift the coordinates of these landmarks in a random way. The magnitudes of horizontal and vertical shifting are respectively controlled by a random function under the uniform distribution. From Fig. 5, it can be seen that the performance of landmarks detection influences the micro-expression spotting. More unfaithful landmarks can dramatically degrade the spotting performance.


                        Fig. 6 demonstrates the influence of weak classifier amount to the performance of micro-expression spotting. To train Adaboost classifiers, we randomly select 1000 frames having micro-expressions as positive samples and 1000 frames without micro-expressions as negative samples. Then we split the training set into five folds and use four folds to train. The remaining one fold is used to test the Adaboost classifiers. It is concluded that the performance would be improved with the increased number of weak classifiers. According to the experiments on the training set, the best amount of weak classifiers would be set to 40.

To tune appropriate model parameters, we perform experiments on changing values of α in (7) and β in (5). The experimental results are shown in Fig. 7. From the experiments, the best values of α and β are set to 0.4 and 0.7 respectively. In other words, the initial probability of RW procedure is more important than the transition probability, and the motion v
                        
                           b
                         is more effective than v
                        
                           d
                         in geometric deformation measurement.

Finally, two video sequences with micro-expressions spotted by our algorithm are shown in Fig. 8. The difference between images is not visually perceptible but distinctive from the computational perspective. Although the contextual changes are not visually visible, our proposed method proves to be more robust to small head movements and lighting variation in computational way.

@&#CONCLUSION@&#

In this paper, we proposed a random walk model to spot frame clips with micro-expressions. The RW model could calculate the probability of individual frames having micro-expressions considering the geometric deformation correlation between frames. In the RW procedure, the Adaboost algorithm is utilized to estimate the initial probability of frames, and the deformation similarity is utilized to calculate the transition probability between frames. For a video sequence, the STASM algorithm is used to describe the geometric shape of human face, which is robust to the head movement and light variation. Then the shape points are aligned by Procrustes analysis approach and the geometric deformation would be modeled. These deformations are used for training Adaboost classifiers.

Although the proposed micro-expression spotting algorithm in this paper achieves promising results, the performance of micro-expression spotting can further be promoted. The feature used in the framework might be improved if more accurate alignment of landmarks or landmarks localization method can be developed. Besides, if the correlation among multiple frames can be modeled, the micro-expression frame clips might be spotted more accurately.

@&#ACKNOWLEDGMENTS@&#

The authors would like to thank Jun Wu for language polish and all reviewers for helpful suggestions. This work is partly supported by the National Aerospace Science Foundation of China (no. 20131353015).

@&#REFERENCES@&#

