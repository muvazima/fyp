@&#MAIN-TITLE@&#Application of a two-stage fuzzy neural network to a prostate cancer prognosis system

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Propose a two-stage fuzzy neural network (FNN) for prediction.


                        
                        
                           
                           The results for three benchmark functions show that the proposed FNN has better performance.


                        
                        
                           
                           The proposed algorithm is applied to prostate cancer prognosis.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Optimization version of an artificial immune network

Particle swarm optimization algorithm

Fuzzy neural network

Prostate cancer

Prognosis

@&#ABSTRACT@&#


               
               
                  Objective
                  This study intends to develop a two-stage fuzzy neural network (FNN) for prognoses of prostate cancer.
               
               
                  Methods
                  Due to the difficulty of making prognoses of prostate cancer, this study proposes a two-stage FNN for prediction. The initial membership function parameters of FNN are determined by cluster analysis. Then, an integration of the optimization version of an artificial immune network (Opt-aiNET) and a particle swarm optimization (PSO) algorithm is developed to investigate the relationship between the inputs and outputs.
               
               
                  Results
                  The evaluation results for three benchmark functions show that the proposed two-stage FNN has better performance than the other algorithms. In addition, model evaluation results indicate that the proposed algorithm really can predict prognoses of prostate cancer more accurately.
               
               
                  Conclusions
                  The proposed two-stage FNN is able to learn the relationship between the clinical features and the prognosis of prostate cancer. Once the clinical data are known, the prognosis of prostate cancer patient can be predicted. Furthermore, unlike artificial neural networks, it is much easier to interpret the training results of the proposed network since they are in the form of fuzzy IF–THEN rules. These rules are very important for medical doctors. This can dramatically assist medical doctors to make decisions.
               
            

@&#INTRODUCTION@&#

Prostate cancer is one of the most common causes of death for men in most industrialized countries [1]. Many studies have considered the introduction of diagnoses for prostate cancer, but few studies have aimed at prognoses for the prostate. Other studies have evaluated the use of artificial neural networks (ANNs) to increase prostate cancer detection rates and reduce unnecessary biopsies [2] and used a neuro-fuzzy system based on both serum data (total prostate-specific antigen; tPSA, percent free PSA; %fPSA), and clinical data (age) to enhance the performance of tPSA in discriminating prostate cancer [3]. However, none of these studies have discussed prognoses of prostate cancer. In Taiwan, the rates of morbidity and mortality of prostate cancer have been rising in recent years. Patients often wonder how much time they have left and how serious their disease is. Doctors can only point to a five-year survival rate ratio depending on patients’ clinical data. Therefore, this study intends to build a prognosis system for prostate cancer to estimate how many years patients have remaining, not just classify patients into two categories (those who can survive more than five years and those who cannot).

Nowadays, many business decisions are made based on the results of data analysis. Another important application area of this type of analysis is in medical diagnoses. Models obtained from data analysis that are applied in practice require transparency and interpretability in terms of the attributes they process. Only then can the user can understand what the results mean easily. Thus, employing appropriate algorithms to build a prognosis system is a very important issue. This kind of algorithm can enable more accurate prediction of prostate cancer prognoses and translate the results of the proposed scheme into comprehensible information for users, or doctors.

In view of the excellent performance of artificial immune systems (AIS) in many fields [4], this study proposes a novel two-stage FNN and applies it to prostate cancer prognoses. The proposed approach includes the following steps: (1) cluster the features and get the initial means and variances of FNN and (2) integrate Opt-aiNET and PSO-based FNN (IOAP-FNN) to forecast the prognoses of prostate cancer patients. The solution is first found through Opt-aiNET due to its good searching capability and then fine-tuned by using PSO. Since the proposed algorithm combines both the advantages of Opt-aiNET and PSO, it has good searching capability, avoids getting stuck to the local optimal solutions and is able to converge rapidly. Unlike an artificial neural network, it is much easier to interpret the training results when using the fuzzy neural network since they are in the form of fuzzy IF–THEN rules.

In order to verify the proposed FNN, three benchmark functions are first employed. Then, the proposed network is further applied to prostate cancer data in order to build a prognosis system for medical doctors who can use these results to improve the diagnoses of prostate cancer patients.

The remainder of this study is organized as follows. Section 2 provides a review of the related literatures. Section 3 presents the proposed prognosis system which employs two-stage FNN, while the simulation results are presented in Section 4. Section 5 presents the model evaluation results for the prostate cancer prognosis system. Discussions and concluding remarks are finally made in Sections 6 and 7.

@&#BACKGROUND@&#

This section presents some background related to the current study, including discussions of prostate cancer, fuzzy theory, FNNs, artificial immune networks, PSO and applications of soft computing techniques to FNN.

The prostate gland is one organ of male reproduction. It can be found below the bladder and in front of the rectum. The definition of prostate cancer is a disease in which cancer develops in the prostate gland. Cancer occurs when the cells of the prostate mutate and begin to multiply out of control. These cells may metastasize from the prostate to other organs, especially the bones and lymph nodes. Prostate cancer is a significant cause of morbidity and mortality in Western countries [5]. In the United States, prostate cancer is the most common cancer and the second most common cause of cancer related death [6]. In 2011, an estimated 240,890 new cases and 33,720 deaths from prostate cancer were recorded. Similar data have been reported in Europe and Canada. In Taiwan, the morbidity of prostate cancer is the fifth for men with cancer, and the mortality ranks seventh. According to the statistical analysis published by the Department of Health, Executive Yuan, there were 3603 cases and 1052 deaths from prostate cancer recorded in 2008. The mean age of prostate cancer patients is 75 years old. This indicates that older men have a higher possibility of getting prostate cancer.

Prostate cancer is one of the most common types of cancer found in men. Risk factors for prostate cancer include age, the family's cancer history and ethnic background [7]. The increasing rate of prostate cancer for men in Taiwan has led doctors and patients seek more information regarding the probability of prostate cancer diagnosis, and the prognosis of prostate cancer is important to patients as well. Definitive diagnosis of prostate cancer can be made through a biopsy. An initial diagnosis is made after patients’ transrectal ultrasonography, rectal examination results and the amount of prostate-specific antigen (PSA) are assessed by a specialist doctor. The PSA level in the blood has become one of the most common methods as a result of studies conducted in recent years for early diagnosis of prostate cancer [7].

PSA levels which are below 4ng/ml in the blood are considered normal, while levels between 4 and 10ng/ml are considered limit values and levels above 10ng/ml are high. It has been stated that the higher the PSA level is, the higher the prostate cancer risk is [8]. However, PSA values may not yield conclusive results about existence of prostate cancer because PSA levels can be increased by inflammation of the prostate and benign prostate hyperplasia (BPH). Therefore, patients are also given rectal examinations. If anomalies are observed at the end of a rectal examination, even if PSA results may seem normal, it is recommended that a prostate biopsy be performed and definitive diagnosis be made [7].

The probability of prostate cancer can be estimated by logistic regression analysis and ANNs, which can be trained to predict diagnostic outcomes. However, none of these tools have resolved the problem of low specificity for prostate cancer diagnosis [3].

Since prostate cancer is one of the most common causes of cancer death among men in most industrialized countries, people are becoming more concerned about this disease. There already have been some studies regarding the classification of prostate cancer by using different methods and features. ANNs which have been applied in many different areas due to their promising results have also been employed to solve medical diagnoses problems recently. In 2007, Keles et al. [1] applied the neuro-fuzzy classification (NEFCLASS) tool to classify whether the patients have prostate cancer or benign prostatic hyperplasia (BPH) since these two illnesses have similar symptoms. Çınar et al. [9] presented a classifier-based expert system for early diagnosis of the prostate in the constraint phase to facilitate informed decision making without biopsy by using some selected features without biopsy examination. These features include weight, height, body mass index (BMI), PSA, free PSA, age, prostate volume, density, smoking, systolic, diastolic, pulse, and Gleason score. Saritas et al. [7] applied an artificial neural network to predict whether patients have cancer or not. The features used are free PSA, total PSA and age data. Though the system does not diagnose cancer conclusively, yet it helps doctors decide whether a biopsy is necessary or a waiting policy should just be carried to see whether the patient has prostate cancer or according to the information provided [7].

Benecchi [3] developed a neuro-fuzzy system to predict the presence of prostate cancer. The proposed neuro-fuzzy system uses tPSA, %fPSA and clinical data (age) as the input features. This can enhance the performance of tPSA in discriminating prostate cancer. The predictive accuracy of the neuro-fuzzy system is superior to that of tPSA and %fPSA. Ecke et al. [2] evaluated the use of the ANN program “Prostate Class” in a daily routine to increase the prostate cancer detection rate and reduce unnecessary biopsies. A total of 204 patients were included in the study. Individual ANN predictions were generated with the use of the ANN application for the Beckman Access PSA and free PSA assays, relying on the features of age, PSA, %fPSA, prostate volume, and DRE. The results show that ANN is a very helpful parameter in the daily routine to increase the prostate cancer detection rate and reduce unnecessary biopsies.

An ANN is a system derived from neurophysiology models. In general, an ANN consists of a collection of simple, nonlinear computing elements, whose inputs and outputs are tied together, to form a network [10]. However, a disadvantage of ANNs, which is an impediment to their more widespread acceptance, is the absence of capability to explain to the user, in a form comprehensible to humans, how the network arrives at a particular decision. Neither can one comment about the knowledge encoded within the black box [11]. On the other hand, fuzzy modeling [12], which is used to fuse decisions from different variables, requires an approach that learns from experience (i.e., data collected in advance). In an innovative approach, ANN learning algorithms have been applied to enhance the performance of fuzzy systems. Fuzzy IF–THEN rules are generated and adjusted by these learning methods, using numerical data [13].

Fuzzy control, based on the Takagi–Sugeno (TS) fuzzy model [14] has been used widely to control nonlinear systems, since a TS fuzzy model can efficiently represent a nonlinear system with a set of linear subsystems. Lin and Lee [15] proposed the so-called neural-network-based fuzzy logic control system (NN-FLCS) as shown in Fig. 1
                        . They introduced the low-level learning power of ANNs into fuzzy logic systems and imbued the normal connectionist architecture with a high-level, human-understandable meaning.

Jang [16] also proposed a different FNN, called an adaptive network-based fuzzy inference system (ANFIS). ANFIS implements a Sugeno-like fuzzy system, in a five-layer network structure. Back-propagation is used to learn the antecedent membership functions, while a least mean squares algorithm determines the coefficients of the linear combinations in the consequences of the rule. In ANFIS, the rule-base must be known in advance. ANFIS adjusts only the membership functions of the antecedent and consequent parameters [17]. Kuo and Cohen [18] applied a feed-forward ANN in fuzzy inference, represented by a TS model. The previously mentioned FNNs are only appropriate for numerical data. However, expert knowledge is always fuzzy, so some researchers have attempted to address this dichotomy. Ishigami et al. [19] proposed learning methods for ANNs that utilize not only numerical data, but also expert knowledge, represented by fuzzy IF–THEN rules. Meanwhile, Buckley and Hayashi [20] surveyed recent results for learning algorithms and applications for FNNs and Buckley introduced several methods for error back-propagation learning algorithms. In addition, Feng [21] also provided a survey for modeling fuzzy control systems.

In 1974, in “Towards a network theory of the immune system,” the first AIS was proposed by Jerne [22]. From then on, many studies related to AIS have been published in a wide variety of fields. AISs are a class of computationally intelligent systems inspired by the principles and processes of the vertebrate immune system. The algorithms typically exploit the immune system's characteristics of learning and memory to solve a problem.

de Castro and Timmis [23] proposed the Opt-aiNET based on an artificial immune network (aiNET). This common technique was inspired by specific immunological theories that explain the function and behavior of the mammalian adaptive immune system. It has become a well-known immune inspired algorithm for function optimization [23].

PSO is also a population-based search algorithm. In PSO, individuals referred to as particles are “flown” in the search space with their own velocities. Their velocities are dynamically adjusted according to their historical behaviors. Therefore the particles fly toward the better search area. The velocity and position of each particle is calculated according to Eqs. (1) and (2):
                           
                              (1)
                              
                                 
                                    
                                       
                                          v
                                          ⇀
                                       
                                       i
                                       
                                          t
                                          +
                                          1
                                       
                                    
                                    =
                                    w
                                    ×
                                    
                                       
                                          v
                                          ⇀
                                       
                                       i
                                       t
                                    
                                    +
                                    
                                       r
                                       1
                                    
                                    
                                       c
                                       1
                                    
                                    (
                                    
                                       
                                          x
                                          ⇀
                                       
                                       
                                          p
                                          B
                                          e
                                          s
                                          t
                                       
                                    
                                    −
                                    
                                       
                                          x
                                          ⇀
                                       
                                       i
                                       t
                                    
                                    )
                                    +
                                    
                                       r
                                       2
                                    
                                    
                                       c
                                       2
                                    
                                    (
                                    
                                       
                                          x
                                          ⇀
                                       
                                       
                                          g
                                          B
                                          e
                                          s
                                          t
                                       
                                    
                                    −
                                    
                                       
                                          x
                                          ⇀
                                       
                                       i
                                       t
                                    
                                    )
                                 
                              
                           
                        and
                           
                              (2)
                              
                                 
                                    
                                       
                                          x
                                          ⇀
                                       
                                       i
                                       
                                          t
                                          +
                                          1
                                       
                                    
                                    =
                                    
                                       
                                          x
                                          ⇀
                                       
                                       i
                                       t
                                    
                                    +
                                    
                                       
                                          v
                                          ⇀
                                       
                                       i
                                       
                                          t
                                          +
                                          1
                                       
                                    
                                    ,
                                 
                              
                           
                        where w is the inertia weight which is usually between 0 and 1 for controlling the influence of the previous velocity. This is a mechanism to control the exploration and exploitation abilities of the particle and also to eliminate the need for velocity clamping [24]. The cognitive component 
                           
                              
                                 r
                                 1
                              
                              
                                 c
                                 1
                              
                              (
                              
                                 
                                    x
                                    ⇀
                                 
                                 
                                    p
                                    B
                                    e
                                    s
                                    t
                                 
                              
                              −
                              
                                 
                                    x
                                    ⇀
                                 
                                 i
                                 t
                              
                              )
                           
                         resembles the individual memory of the best particle's position, and 
                           
                              
                                 r
                                 2
                              
                              
                                 c
                                 2
                              
                              (
                              
                                 
                                    x
                                    ⇀
                                 
                                 
                                    g
                                    B
                                    e
                                    s
                                    t
                                 
                              
                              −
                              
                                 
                                    x
                                    ⇀
                                 
                                 i
                                 t
                              
                              )
                           
                         is the social best position, and the acceleration coefficients, c
                        1 and c
                        2 together with random vectors r
                        1 and r
                        1 control the stochastic influence of the cognitive and social components on the particle's velocity. The concept of velocity is the main difference between PSO and other meta-heuristic methods.

Due to the complex training process for FNNs, soft computing techniques, including genetic algorithms, particle swarm optimization, artificial immune systems and ant colony systems, have been employed to determine their weights and fuzzy membership function parameters [11].

Ishigami et al. [19] proposed an auto-tuning method, for FNNs, using GA. The new tuning method constructs the minimal and optimal structures of the fuzzy model. It can solve the problem of the convergence of tuning, using conventional methods, depending on the initial conditions of the fuzzy model. Shimojima et al. [25] presented a genetic algorithm based FNN, with an adaptive membership function, rules and structure.

Kumar and Garg [26] developed methodologies to learn and optimize fuzzy logic controller parameters, based on ANN and GA. These were used to control an inverted pendulum. The results for three different fuzzy logic controllers, developed with the help of iterative learning, from operator experience, GA and ANN, were compared. As well as the application for a controller, a similar concept was also applied to design [27]. The evaluation of each individual design candidate, in terms of its ability to meet the demands of the marketplace, is a crucial step in the conceptual design stage. Consequently, Hsiao and Tsai [28] proposed a method that enables an automatic product form search, or product image evaluation, by means of GA-based FNN.

Östermark [29] proposed a multi-group classification algorithm, based on a hybrid genetic fuzzy neural net (GFNN) framework. It aggregates the signal inherent in the fuzzifier, using a suitable T-norm, and transmitted it to a defuzzifier. The defuzzifier aggregates the predicted group membership, using a suitable conform. If misclassifications occur during training, the membership functions of both the fuzzifier and the defuzzifier are adapted using a systematic, robust procedure. The algorithm was successfully tested, with real economic data.

PSO is inspired by observations of the social behavior of birds and it has been applied in solving optimization problems. The advantages of PSO are fast convergence, fewer parameters, ease of discovery of the global optimum, its ease of understanding and simplicity of implementation. One of the FNN's biggest problems is that too many rules result in extended computation time. He et al. [30] used PSO to extract rules from FNN. The network parameters, including the necessary membership functions of the input variables and the consequent parameters, were tuned and identified using a modified PSO, which uses the best current performance of each particle's neighbors to replace the best previous one and uses a non-accumulative rate of change to replace the accumulative one, to accelerate the search procedure. The trained network is then pruned, so that the general rules can be extracted and explained.

Lin et al. [31] proposed a functional-link-based neural fuzzy network (FNFN), combined with immune particle swarm optimization (IPSO), to solve prediction and control problems. IPSO employs the advantages of PSO to improve the mutation mechanism of the immune algorithm. Another study, which examined hybrid AIS, used Learning Vector Quantization (LVQ) and fuzzy set theory to present a new supervised learning method (HFNINME) [32]. This model was compared with other algorithms. The results of the experiments revealed that the proposed method produced a parsimonious classifier that can classify data more accurately and more efficiently.

Ant colony systems have also been employed to generate appropriate fuzzy control inputs. Cai et al. [33] used ant colony optimization to tune the parameters of a FNN. Using computer simulation, the experimental results show the feasibility and effectiveness of the proposed method. In one application of this research, Bao and Lin [34] used ant colony optimization control for a FNN, for a freeway entrance ramp. The advantages of the proposed model are fast convergence, reduced computation time, good quality of control, and stabilization of the traffic flow density for the main line.

@&#METHODOLOGY@&#

In the area of prostate cancer, few studies have been devoted the prognosis of prostate cancer. A number of studies [3,7] related to diagnosis of prostate cancer have only classified patients into two categories, patients who survive more than five years or less. Thus, this study aims to propose a more accurate prognosis system for prostate cancer. In order words, we will provide a more precise estimation of survival.

The proposed prognosis system comprised four steps: (1) prostate cancer data collection, (2) feature selection, (3) data clustering, and (4) forecasting of model development. The proposed forecasting model is an improved fuzzy neural network which employs an integrated Opt-aiNET and PSO algorithm with cluster analysis (two-stage FNN) to predict the prognosis of prostate cancer patient. Unlike traditional ANN which is a black box, the advantage of FNN is that the fuzzy inference rules can be explained because they are given in the form of fuzzy IF–THEN rules. This allows the causal relationship to be more easily understood by users, or doctors. Using the trained two-stage FNN, the prognosis of a prostate cancer patient can be estimated as new data are obtained by the doctor. Then, the system can use this information to infer how many years remain for the patient, not just whether they have cancer or not.

The data which are used in the prognoses of prostate cancer patients in this study were collected from a well-known teaching oriented hospital located in Taipei. The history prostate cancer database contains many pathological data of patients and the treatment techniques that that doctors have used. Since the goal of this study is to build a prognosis system for prostate cancer, the patients who died due to prostate cancer and the patients who survived over five years from when they found out they had prostate cancer were selected.

Basically, many different factors affect prognosis including age, BMI, PSA, DRE, biopsy, clinical T-stage, clinical N-stage, clinical M-stage and treatment methods. Thus, this study employs stepwise regression analysis to find out the important factors. Then, these important factors are applied to a fuzzy neural network.

The purpose of features clustering is to find a good initial solution for the membership functions at the beginning of training. In this subsection, the aiNET-K algorithm [35] is employed to implement feature clustering. The aiNET-K algorithm is a hybrid clustering method which integrates and artificial immune network and K-means. In an AIS, an antigen is any foreign substance that causes the immune system to produce an antibody against it. The memory cells are activated to produce the antibodies which effectively killed the antigen in the past. This corresponds to recalling a past successful solution to a similar problem. The remaining cells, of the antibody which matched the antigen, are dispersed to the memory cells and suppressor cells. This process can be seen as keeping a good solution as a memory for the search in the next step [36]. The aiNET-K algorithm flowchart is illustrated in Fig. 2
                            and the detailed procedure is shown as follows:


                           Step 1: Set up relative parameters
                        

The relative parameters in the aiNET-K algorithm include the number of iterations, the number of memory cells (M), the number of remaining cells (R), clone times (N
                           
                              c
                           ), error threshold and suppression threshold (σ
                              s
                           ).


                           Step 2: Generate initial population randomly
                        

In the initial iteration, the algorithm generates M memory cells and R remaining cells randomly. Then, they are combined as the initial population set P where P is the sum of M and R. They evolve iteratively in order to obtain the optimal solution according to pre-specified objective. Each antibody is concatenated with K centroid vectors, thus:
                              
                                 (3)
                                 
                                    
                                       
                                          P
                                          
                                             i
                                             d
                                          
                                       
                                       =
                                       (
                                       
                                          q
                                          
                                             i
                                             1
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          q
                                          
                                             i
                                             j
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          q
                                          
                                             i
                                             k
                                          
                                       
                                       )
                                       ,
                                        
                                       i
                                       ∈
                                       P
                                       ,
                                        
                                       d
                                       ∈
                                       K
                                       ,
                                    
                                 
                              
                           where P
                           
                              id
                            is the ith population set and q
                           
                              ij
                            is the ith dimension vector of the jth cluster's centroid.


                           Step 3: Calculate the fitness value of antibodies
                        

The fitness function used for aiNET-K is Euclidean distance, and the steps to calculate the fitness value are shown as follows:
                              
                                 (1)
                                 Calculate the Euclidean distance between data point X
                                    
                                       i
                                     and each centroid. The Euclidean distance is calculated as follows:
                                       
                                          (4)
                                          
                                             
                                                d
                                                (
                                                
                                                   X
                                                   i
                                                
                                                ,
                                                
                                                   C
                                                   
                                                      i
                                                      j
                                                   
                                                
                                                )
                                                =
                                                |
                                                |
                                                
                                                   X
                                                   i
                                                
                                                −
                                                
                                                   C
                                                   
                                                      i
                                                      j
                                                   
                                                
                                                |
                                                |
                                                ,
                                             
                                          
                                       
                                    where X
                                    
                                       i
                                     is the vector of the ith data point, C
                                    
                                       ij
                                     is the centroid's vector of the jth cluster in the ith antibody and d is the distance between X
                                    
                                       i
                                     and C
                                    
                                       ij
                                    .

The data point X
                                    
                                       i
                                     will be assigned to the cluster with the shortest Euclidean distance between data point X
                                    
                                       i
                                     and the corresponding centroid.

Calculate the fitness value, (Ab)
                                       i
                                    .
                                       
                                          (5)
                                          
                                             
                                                
                                                   
                                                      (
                                                      A
                                                      b
                                                      )
                                                   
                                                   i
                                                
                                                =
                                                
                                                   1
                                                   
                                                      1
                                                      +
                                                      
                                                         D
                                                         i
                                                      
                                                   
                                                
                                                ,
                                                 
                                                0
                                                ≤
                                                
                                                   
                                                      (
                                                      A
                                                      b
                                                      )
                                                   
                                                   i
                                                
                                                ≤
                                                1
                                                ,
                                                 
                                                i
                                                ∈
                                                P
                                                ;
                                             
                                          
                                       
                                    
                                    
                                       
                                          (6)
                                          
                                             
                                                
                                                   D
                                                   i
                                                
                                                =
                                                S
                                                E
                                                
                                                   D
                                                   i
                                                
                                                =
                                                K
                                                
                                                   ∑
                                                   
                                                      j
                                                      =
                                                      1
                                                   
                                                   K
                                                
                                                
                                                   
                                                      ∑
                                                      
                                                         ∀
                                                         
                                                            X
                                                            i
                                                         
                                                         ∈
                                                         
                                                            n
                                                            
                                                               i
                                                               j
                                                            
                                                         
                                                      
                                                   
                                                   
                                                      |
                                                      |
                                                      
                                                         X
                                                         i
                                                      
                                                      −
                                                      
                                                         C
                                                         
                                                            i
                                                            j
                                                         
                                                      
                                                      |
                                                      |
                                                   
                                                
                                                ,
                                             
                                          
                                       
                                    where SED is the sum of the Euclidean distance, X
                                    
                                       i
                                     is the vector of the ith data point, n
                                    
                                       ij
                                     is the number of the data point which belongs to the jth cluster in the ith antibody and C
                                    
                                       ij
                                     is the centroid's vector of the jth cluster in the ith antibody.


                           Step 4: Clone the population
                        

Clone the antibodies by a pre-specified number, N
                           
                              c
                           . This study employs the Taguchi method to determine the N
                           
                              c
                            value. Thus, the total cell number will be equal to N
                           ×
                           N
                           
                              c
                           .


                           Step 5: Maturate
                        

In a biological immune system, mutation refers to the changing proteins of antibodies. For aiNET, mutation is the change of the centroid vectors of antibodies. However, this change may result in better or worse fitness. Worse mutation is eliminated by selection. The original cells are replaced by mutated cells which have better fitness. However, the original population is kept and then mutates the rest of cells. The purpose is to avoid cases where mutated cells are worse than the original population. The maturation steps are described as follows:
                              
                                 (1)
                                 Calculate the maturate rate-

The mutation rate is relative to the corresponding fitness. The antibody which has higher fitness will have a lower mutation rate. The maturation rate is calculated as follows:(7)
                                       
                                          M
                                          a
                                          t
                                          u
                                          r
                                          a
                                          t
                                          i
                                          o
                                          n
                                             
                                          r
                                          a
                                          t
                                          e
                                          =
                                          
                                             α
                                             i
                                          
                                          =
                                          
                                             
                                                
                                                   1
                                                   ρ
                                                
                                             
                                          
                                          
                                             e
                                             
                                                −
                                                (
                                                A
                                                
                                                   b
                                                   ∗
                                                
                                                )
                                                i
                                             
                                          
                                          ,
                                       
                                    where (Ab*)
                                       i
                                    
                                    =(Ab)
                                       i
                                    /(Ab)
                                       max
                                    , which means normalized affinity, ρ is a constent, and i
                                    ∈(P
                                    ×
                                    Nc)−
                                    P
                                 

Maturation-

The formula for maturation is shown as follows:
                                       
                                          (8)
                                          
                                             
                                                
                                                   c
                                                   ′
                                                
                                                =
                                                c
                                                +
                                                α
                                                i
                                                ×
                                                N
                                                (
                                                0,1
                                                )
                                                ,
                                             
                                          
                                       
                                    where c′ is the antibody which has been maturated, and c
                                    =(c
                                    1, …, c
                                    2, …, c
                                    
                                       x
                                    ) which means for the antibodies which have been cloned, αi is the maturation rate and N(0,1) is the standard normal distribution.


                           Step 6: Get new antibody clusters via he K-means algorithm
                        

The first two steps are the same as in Steps 3-(1) and (2), but not the third step of Step 3. The following formula shows how to calculate each new centroid vector and how to get new antibodies.
                              
                                 (9)
                                 
                                    
                                       
                                          C
                                          
                                             i
                                             j
                                          
                                          
                                             n
                                             e
                                             w
                                          
                                       
                                       =
                                       
                                          1
                                          
                                             
                                                n
                                                
                                                   i
                                                   j
                                                
                                             
                                          
                                       
                                       
                                          ∑
                                          
                                             ∀
                                             
                                                X
                                                i
                                             
                                             ∈
                                             
                                                n
                                                
                                                   i
                                                   j
                                                
                                             
                                          
                                       
                                       
                                          
                                             X
                                             i
                                          
                                       
                                       ,
                                    
                                 
                              
                           where 
                              
                                 
                                    C
                                    
                                       i
                                       j
                                    
                                    
                                       n
                                       e
                                       w
                                    
                                 
                              
                            is the new vector of the jth cluster for the ith antibody and n
                           
                              ij
                            is the number of data points which belong to the jth cluster for the ith antibody.


                           Step 7: Calculate the fitness of new antibody clusters
                        

Since mutation and the K-means algorithm change the centroid's vector, therefore, it is necessary to recalculate the fitness of the offspring before implementing selection.


                           Step 8: Replace the population
                        

There are P new clusters, and each cluster has N
                           
                              c
                            antibodies. In order to get the best antibody of each cluster, the proposed system ranks the fitness of each antibody from high to low and selects the first antibody (it has the highest fitness value) of each cluster to replace the original antibodies. However, the best antibody of each cluster might be a worse antibody when ranked with other cluster's antibodies. This does not seem to limit local optimization.


                           Step 9: Determine the generation error
                        

This step is to determine whether the fitness is improved or not. If there is an error between the current generation and previous generation, it implies that the population cells have not been searched fully. There is chance that the population cells can be evolved to achieve better solutions. Thus, it is necessary to go back to Step 4. If the error is smaller than a pre-specified threshold, then go to the next step. The average error function is calculated as follows:
                              
                                 (10)
                                 
                                    
                                       P
                                       o
                                       p
                                       u
                                       l
                                       a
                                       t
                                       i
                                       o
                                       n
                                          
                                       e
                                       r
                                       r
                                       o
                                       r
                                       =
                                       
                                          ∑
                                          i
                                          p
                                       
                                       
                                          
                                             
                                                S
                                                E
                                                
                                                   D
                                                   i
                                                
                                             
                                             n
                                          
                                       
                                       ,
                                    
                                 
                              
                           where n is the size of the population in the current generation.


                           Step 10: Suppression
                        

The purpose of suppression is to delete similar antibodies. This is done by calculating the Euclidean distance between two antibodies. If the distance is smaller than a pre-specified threshold, σ
                           
                              s
                           , then the former antibody is eliminated. But this cannot ensure that a better solution will be retained. Antibodies with potential may be deleted. In addition, the larger the threshold is, the higher the probability. To solve this problem, the antibodies can be sorted by their corresponding fitness values before suppression. Since this study is a minimization problem, antibodies are sorted from large to small. This can ensure that better antibodies are retained.


                           Step 11: Determine whether to stop or not
                        

If the number of iterations reaches the pre-specified value, then stop and output the best solution. Otherwise, go back to Step 3.

By using the aiNET-K algorithm, collected data after feature selection are clustered into pre-specified number clusters. Basically, the number of clusters is equal to the number of membership functions of FNN. Thereafter, for each cluster, the mean and variance are computed and become a portion of initial solutions, or antibodies, mean and variance of membership functions. This procedure can accelerate the training speed of FNN. The arrangement of antibodies which are generated by using aiNET-K algorithm is illustrated in Fig. 3
                           . Some antibodies obtained using the solution obtained from aiNET-K algorithm are added to the initial antibodies, but the rest are still generated randomly. For the connecting weights, connections between rules and outputs are all randomly generated. In Fig. 3, m_1_1 is the mean of the “first” cluster of the “first” feature, and σ_1_1 is the variance of the “first” cluster of the “first” feature.

In this study, we integrate the optimization artificial immune network and particle swarm optimization for training a fuzzy neural network. Opt-aiNET, which has been applied to optimization problems, views the problems which we want to solve as foreign antigens and feasible solutions as antibodies. Through iterative evolution, the algorithm tries to find the optimal antibody where a memory cell can store the current good solutions. Fig. 4
                            illustrates the flowchart of the proposed two-stage FNN and the procedure is described as follows.


                           Step 1: Generate the initial antigen and antibody randomly and then combine the clustered antibodies together
                        

Some of the initial antibodies, or solutions, are generated by the features’ cluster results, and others are generated randomly. They evolve in order to obtain the optimal solution according to the pre-specified objective. For antibodies, the common encoding methods are binary encoding and floating-point encoding. Since the fitness function is continuous for the current problem, this study adopts floating-point encoding for antibodies. Thus, all fuzzy neural network parameters, including the mean and variance of each membership function and the connecting weights, are concatenated as antibodies.


                           Step 2: Calculate the fitness of antibodies
                        

The fitness function used in this study is mean square error. Thus, for every antibody, we use the inputs of training data for FNN and calculate the corresponding outputs. Then we can obtain the mean square error by using the output from FNN and target outputs. For all the procedures, refer to [15].


                           Step 3: Clone the population
                        

Clone the antibodies according to a pre-specified number, N
                           
                              c
                           . This study employs the Taguchi method to determine the N
                           
                              c
                            valuable. Thus, the total cell number is equal to N
                           ×
                           N
                           
                              c
                           .


                           Step 4: Mutation
                        

In biological immune system, mutation refers to the changing of the protein of antibodies. For Opt-aiNET, mutation is the changing of the fuzzy numbers and weights of antibodies. However, this change may result in better or worse fitness. Worse mutation is eliminated by selection. The original cells are replaced by mutated cells which have better fitness. However, the original population is kept and then mutates the rest of cells. The purpose is to avoid cases where mutated cells are worse than the original population. The maturation steps are represented as follows:
                              
                                 (11)
                                 
                                    
                                       
                                          c
                                          ′
                                       
                                       =
                                       c
                                       +
                                       α
                                       N
                                       (
                                       0,1
                                       )
                                    
                                 
                              
                           and
                              
                                 (12)
                                 
                                    
                                       α
                                       =
                                       
                                          
                                             
                                                1
                                                β
                                             
                                          
                                       
                                       exp
                                       (
                                       −
                                       
                                          f
                                          ∗
                                       
                                       )
                                       ,
                                    
                                 
                              
                           where α is the mutation rate, f is the antibody fitness, c is the cloned cells, and c′ is the mutated cells. The fitness can be transformed to the mutation rate by using an exponential function. β is the self-defined variable which can narrow down the mutation rate to a preferred range. In addition, it can avoid cases where f is equal to 0, and α becomes 1. This implies that the mutation rate will equal to 100%. Then, multiply the mutation rate with a random variable which falls within N(0,1). We can then get the amount of adjustment.


                           Step 5: Calculate the fitness of offspring after mutation
                        

Since mutation cause changes of antibodies, therefore, we have to recalculate the fitness of the offspring after mutation before implementing selection.


                           Step 6: Replace the population
                        

Replace the population with the best solution of each group.


                           Step 7: Calculate the average fitness
                        

Calculate the average fitness of the replaced population.


                           Step 8: Determine the generation error
                        

This step is to determine whether the fitness is improved or not. If there is error between the current generation and previous generation, it implies that the population cells have not been searched fully. There is chance that the population cells can be evolved to better solutions. Thus, it is necessary to go back to Step 3. If the error is smaller than a pre-specified threshold, then go to the next step. The average error function is calculated as follows:
                              
                                 (13)
                                 
                                    
                                       d
                                       e
                                       l
                                       t
                                       a
                                       =
                                       1
                                       +
                                       
                                          
                                             a
                                             v
                                             e
                                             f
                                             i
                                             
                                                t
                                                
                                                   o
                                                   l
                                                   d
                                                
                                             
                                          
                                          
                                             a
                                             v
                                             e
                                             f
                                             i
                                             
                                                t
                                                
                                                   n
                                                   e
                                                   w
                                                
                                             
                                          
                                       
                                       ,
                                    
                                 
                              
                           where avefit
                           
                              old
                            and avefit
                           
                              new
                            are the average fitness values of the previous and current generations, respectively. Basically, the decision criteria can be set as follows:
                              
                                 
                                    
                                       If
                                          
                                       d
                                       e
                                       l
                                       t
                                       a
                                       <
                                       0.0001
                                       ;
                                          
                                       t
                                       h
                                       e
                                       n
                                          
                                       g
                                       o
                                          
                                       t
                                       o
                                          
                                       
                                          suppression
                                       
                                       .
                                    
                                 
                              
                           This is suitable for a maximization problem. However, for a minimization problem, the average error function is as follows:
                              
                                 (14)
                                 
                                    
                                       d
                                       e
                                       l
                                       t
                                       a
                                       =
                                       1
                                       +
                                       
                                          
                                             a
                                             v
                                             e
                                             f
                                             i
                                             
                                                t
                                                
                                                   o
                                                   l
                                                   d
                                                
                                             
                                          
                                          
                                             a
                                             v
                                             e
                                             f
                                             i
                                             
                                                t
                                                
                                                   n
                                                   e
                                                   w
                                                
                                             
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        


                           Step 9: Suppression
                        

The purpose of suppression is to delete similar antibodies. This is done by calculating the Euclidean distance between two antibodies. If the distance is smaller than a pre-specified threshold, σ
                           
                              s
                           , then the former antibody is eliminated. But this cannot ensure a better solution will be retained. The antibodies with potential may be deleted. In addition, the larger the threshold is, the higher the probability. To solve this problem, the antibodies can be sorted by their corresponding fitness values before suppression. Since this study addresses a minimization problem, antibodies are sorted from big to small. In this way, we can make sure that the better antibodies are retained.


                           Step 10: Replace the memory cell
                        

After suppression, the size of cells is changed. Then, the suppressed cells become memory cells. The size of memory cells varies from one iteration to another.


                           Step 11: Add in new cells
                        

In order to avoid falling into local optimal solutions, new antibodies are added in this step. The number of newcomers is the number of memory cells times d%. d is a self-defined variable.


                           Step 12: Generate particle velocity
                        

PSO starts from this step. According to the size of the combination of memory cells and newcomers, generate the corresponding velocity for every antibody randomly.


                           Step 13: Calculate the fitness
                        

Use the feed-forward part of FNN to calculate the fitness of the population, as in Step 2.


                           Step 14: Find the 
                           
                              pBest
                            
                           and 
                           
                              gBest
                           
                        

According to the fitness obtained from the previous step, the best solution is gBest. Because Opt-aiNET adopts elite strategy, the best antibody must be preserved. Therefore, gBest is the best solution of the current iteration. It does not need to be compared with previous iterations. Regarding pBest, the sizes of the cells of every iteration may not be the same, so we cannot know the historical best solution of every particles. Furthermore, since Opt-aiNET adopts elite strategy, the current solution must be better than the previous one. The population cells are their own pBest.


                           Step 15: Renew velocity
                        

The updating of velocity is shown in Eq. (1).


                           Step 16: Renew the positions of particles
                        

The new positions of particles are the original values plus the renewed velocity, as shown in Eq. (2).


                           Step 17: Determine whether to stop
                        

If the number of iterations reaches the pre-specified value, then stop and output the best solution. Otherwise, go back to Step 2.

For the purpose of testing the proposed two-stage FNN, this study uses Matlab to program code. Three different benchmark functions are adopted to verify the proposed model. This study compares the proposed two-stage FNN with other algorithms. Mean square error (MSE) is applied as the criteria. Lower MSE implies better efficiency.

In addition, this study uses K-fold cross-validation to assess the proposed model. In K-fold cross-validation, the original sample is randomly partitioned into K subsamples. Of the K subsamples, a single subsample is retained as the validation data for testing the model, and the remaining K
                           −1 subsamples are used as training data. The cross-validation process is then repeated K times, or folds, with each of the K subsamples used exactly once as the validation data. The K results from the folds then can be averaged to produce a single estimation. The advantage of this method over repeated random sub-sampling is that all observations are used for both training and validation, and each observation is used for validation exactly once. 10-fold cross-validation is commonly used, so K is set as 10. Then, we implement hypothesis testing with a confidence interval of 95%. The goal is to determine whether two-stage FNN is significantly better than other algorithms or not.

Based on the proposed two-stage FNN, Matlab is employed to design a computer program for simulating different cases so as to demonstrate the feasibility of the proposed FNN. Basically, two-stage FNN can effectively help prevent the FNN from falling into a partial optimal solution, and can quickly converge to a global domain solution. The performance is compared with those of IOAP-FNN and NN-FLCS presented by Lin (1991) and a genetic algorithm-based FNN (GA-FNN), AIS-based FNN (AIS-FNN), PSO-based FNN (PSO-FNN), Opt-aiNET-based FNN (Opt-aiNET-FNN), and AIS and PSO-based FNN (AIS-PSO-FNN).

The benchmark functions employed for simulation include the Ackley function, Hartmann function and Mackey–Glass time series [37]. Each test function has its own characteristics and corresponding input number. Training results are utilized to demonstrate convergence of testing data in order to understand the integrality of the proposed FNN.


                        
                           
                              (1)
                              Benchmark function 1 – Ackley function

(Number of variables: n
                                 =2)


                                 
                                    
                                       (15)
                                       
                                          
                                             A
                                             (
                                             x
                                             )
                                             =
                                             −
                                             20
                                             ×
                                             exp
                                             
                                                
                                                   −
                                                   0.2
                                                   ×
                                                   
                                                      
                                                         
                                                            1
                                                            n
                                                         
                                                         
                                                            ∑
                                                            
                                                               i
                                                               =
                                                               1
                                                            
                                                            n
                                                         
                                                         
                                                            
                                                               x
                                                               i
                                                               2
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                             −
                                             exp
                                             
                                                
                                                   
                                                      1
                                                      n
                                                   
                                                   
                                                      ∑
                                                      
                                                         i
                                                         =
                                                         1
                                                      
                                                      n
                                                   
                                                   
                                                      cos
                                                      (
                                                      2
                                                      π
                                                      ⋅
                                                      
                                                         x
                                                         i
                                                      
                                                      )
                                                   
                                                
                                             
                                             +
                                             20
                                             +
                                             
                                                e
                                                1
                                             
                                          
                                       
                                    
                                 
                              

Benchmark function 2 – Hartmann Function

(Number of variables: 3)


                                 
                                    
                                       (16)
                                       
                                          
                                             
                                                H
                                                
                                                   3,4
                                                
                                             
                                             =
                                             −
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                4
                                             
                                             
                                                
                                                   c
                                                   i
                                                
                                             
                                             exp
                                             
                                                
                                                   −
                                                   
                                                      ∑
                                                      
                                                         j
                                                         =
                                                         1
                                                      
                                                      n
                                                   
                                                   
                                                      
                                                         α
                                                         
                                                            i
                                                            j
                                                         
                                                      
                                                      
                                                         
                                                            (
                                                            
                                                               x
                                                               j
                                                            
                                                            −
                                                            
                                                               p
                                                               
                                                                  i
                                                                  j
                                                               
                                                            
                                                            )
                                                         
                                                         2
                                                      
                                                   
                                                
                                             
                                             .
                                          
                                       
                                    
                                 
                              

Benchmark function 3 – Mackey–Glass time series

(Number of variables: 4)


                                 
                                    
                                       (17)
                                       
                                          
                                             
                                                
                                                   d
                                                   x
                                                   (
                                                   t
                                                   )
                                                
                                                
                                                   d
                                                   t
                                                
                                             
                                             =
                                             a
                                             
                                                
                                                   x
                                                   (
                                                   t
                                                   −
                                                   τ
                                                   )
                                                
                                                
                                                   1
                                                   +
                                                   x
                                                   
                                                      
                                                         (
                                                         t
                                                         −
                                                         τ
                                                         )
                                                      
                                                      
                                                         10
                                                      
                                                   
                                                
                                             
                                             −
                                             b
                                             x
                                             (
                                             t
                                             )
                                             .
                                          
                                       
                                    
                                 
                              

The parameters determined include the number of clustered antibodies (C-anti), suppression, ratio of new cells (d), and β (beta), population cells (N) and clone times (N
                        
                           c
                        ). If full factorial design is implemented, then the number of combinations will be very large. Thus, in this study, Taguchi design is employed in order to overcome the above mentioned problem. Taking these six parameters as the factors for Taguchi design, each factor contains two levels. The controlled parameters and their corresponding levels are listed in Table 1
                        . Minitab is used to implement Taguchi analysis. Every parameter combination is run five times and each time the proposed two-stage FNN is implemented for 500 iterations.

Through Taguchi design, the appropriate training parameters can be determined. This study uses MSE as the criterion since it is the target value for FNN. Basically, the smaller the MSE, the better the result. In Taguchi design, the signal to noise ratio (S/N ratio) is the criterion of robust quality. When the S/N ratio is higher, it means that the noise is less and the quality is better. Table 2
                         indicates that the best parameter combination of each benchmark is as follows.

After training for 500 iterations, the computational results of each benchmark are illustrated in Figs. 4 and 5
                        . They show the training/testing MSE values and the learning curves for different algorithms, respectively. We can see that the proposed two-stage FNN has better convergence, followed by IOAP-FNN and AIS-FNN (Fig. 6
                        ).

Furthermore, K-fold cross-validation (K
                        =10) is applied to conform to the statistical independent random process, and every experiment is implemented for 500 iterations. Three replications are conducted. Thus, there are 30 runs for every experiment. Table 3
                         shows the summarized experimental results for every algorithm of each benchmark. It reveals that the proposed two-stage FNN has the smallest MSE value.


                        Table 4
                         shows the computational time for every algorithm; it indicates that NN-FLCS had the best computational time performance. Since the proposed two-stage FNN is a two-stage hybrid algorithm, it requires more time for computation.

In order to further testify whether the proposed algorithm is really better than the other algorithms or not, statistical hypothesis testing is conducted. The hypothesis testing formula is shown below:
                           
                              (18)
                              
                                 
                                    Z
                                    =
                                    
                                       
                                          (
                                          
                                             
                                                
                                                   X
                                                   1
                                                
                                             
                                             ¯
                                          
                                          −
                                          
                                             
                                                
                                                   X
                                                   2
                                                
                                             
                                             ¯
                                          
                                          )
                                          −
                                          (
                                          
                                             μ
                                             1
                                          
                                          −
                                          
                                             μ
                                             2
                                          
                                          )
                                       
                                       
                                          
                                             S
                                             
                                                
                                                   
                                                      
                                                         X
                                                         1
                                                      
                                                   
                                                   ¯
                                                
                                                −
                                                
                                                   
                                                      
                                                         X
                                                         2
                                                      
                                                   
                                                   ¯
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        and
                           
                              (19)
                              
                                 
                                    
                                       S
                                       
                                          
                                             
                                                
                                                   X
                                                   1
                                                
                                             
                                             ¯
                                          
                                          −
                                          
                                             
                                                
                                                   X
                                                   2
                                                
                                             
                                             ¯
                                          
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   S
                                                   1
                                                   2
                                                
                                             
                                             
                                                
                                                   n
                                                   1
                                                
                                             
                                          
                                          +
                                          
                                             
                                                
                                                   S
                                                   2
                                                   2
                                                
                                             
                                             
                                                
                                                   n
                                                   2
                                                
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

Since this is one-tailed hypothesis testing (left-tailed), it rejects H
                        0 when Z
                        ≤−Z
                        
                           α
                        . The Z-distribution table points out that Z
                        
                           α
                         is 1.64485 when α equals 0.05. Table 5
                         shows the hypothesis testing results. It reveals that two-stage FNN is significantly better than any other algorithm listed in the table.

The simulation results in Section 4 demonstrate that the proposed two-stage FNN is able to learn the continuous functions successfully. Thus, this section intends to apply two-stage FNN to a prostate cancer prognosis system. This application consists of four parts: (1) data collection, (2) factor selection, (3) parameter determination, and (4) prognosis. Each part is discussed in the following sections.

Prostate cancer related data were collected from a well-known teaching-oriented hospital located in Taipei, Taiwan. 100 sets of data qualified for the current study. All prostate cancer data collected with no leaking data. The data can be divided into two categories, 1 or 0. Category 1 indicates that the patients died due to prostate cancer. 61 patients belong to this category. Category 0 represents the patients are still alive. Thirty-nine patients are in this category. The patients in category 0 must be followed up after 5 years. The reason is that the patients who survive more than 5 years after finding out they have prostate cancer often pass away due to reasons unrelated to prostate cancer.

In this study, the original data collected from the hospital contain many variables including age, height, weight, initial prostate specific antigen (iPSA), digital rectal examination (DRE), biopsy Gleason score, clinical tumor stage (C-Stg-T) and so on. Six important factors are first selected based on doctors’ advice, and then stepwise regression is employed to pick the critical factors from these six factors. After conducting stepwise regression, the computational results reveal that biopsy, iPSA and DRE are critical factors which affect the accuracy of prognosis. Thus, these three factors are used to develop the prognosis system using two-stage FNN.

For the aiNET-K algorithm, some parameters must be determined. Though the Taguchi method, Table 6
                         indicates the best parameter combination of the prostate cancer data set. Then, aiNET-K is employed to cluster the features into a pre-specified number of clusters which is equal to 3 for the current data. The clustering results of prostate cancer data set are listed in Table 7
                        .

In this section, the preprocessed data is used to train the proposed two-stage FNN. Four trials are designed to determine the number of antibodies which will not be generated randomly at the beginning of training. These antibodies use the cluster analysis result as the solution. In these trials, 3 (one-tenth of N), 5 (one-sixth of N), 10 (one-third of N) and 15 (a half of N) antibodies using the cluster analysis result as the solutions added at the beginning. Then, the efficiency is compared.

In these trials, the prostate cancer data are ranked randomly, and then K-fold cross-validation (K
                        =10) is employed to conform the statistical independent random process and every experiment is implemented for 500 iterations. Three replications are conducted. Thus, there are 30 runs for every experiment. After all of trials are completed, sensitivity analysis is applied to examine model behavior. The sensitivity analysis results are summarized diagrammatically in Fig. 7
                        .

This diagram indicates that when more antibodies were added at the initial stage, lower MSE it contributed until the initially clustered antibodies exceed half of the entire initial antibodies (15/30). And this figure also indicates that trial 3 (one-third of initial clustered antibodies) has the best performance among all trials. In trial 4, 15 antibodies are added to the original antibodies, but the performance is even worse than IOAP-FNN, NN-FLCS and BP, and it is not significantly better than the regression result.

In trial 3, 10 antibodies using the cluster analysis results as the solution are added to the initial antibody group. This means that there are 10 antibodies using the cluster analysis result as the solution and 20 antibodies which are generated randomly in the beginning of training. After training for 500 iterations, the computational results are illustrated in Figs. 7 and 8
                        . They show the membership functions and training/testing MSE, respectively. The corresponding means and variances, as shown in Table 8
                        , are used as intervals to build fuzzy membership functions.


                        Figs. 9 and 10
                        
                         illustrate the learning and testing curves of different algorithms, respectively. Although the BP neural network has the lowest MSE during the training part, two-stage FNN still has the best performance during the testing part, which means that the BP neural network might result in the overfitting phenomenon (Fig. 11
                        ).


                        Table 9
                         shows the summarized experimental results for every algorithm. It reveals that two-stage FNN has the smallest MSE value, 0.068907, while NN-FLCS has the smallest standard deviation according to testing.

In order to further testify whether that the proposed algorithm is really better than the other algorithms or not, statistical hypothesis testing is conducted in Trail 3. Table 10
                         shows the hypothesis testing results. It reveals that two-stage FNN is significantly better than IOAP-FNN, NN-FLCS, BP and regression (Table 11
                        ).

@&#DISCUSSION@&#

This study has proposed a novel two-stage framework for FNN. The first stage employs the aiNET-K algorithm to cluster the data and then determines the initial membership functions for the FNN. Because of the first stage, FNN can start learning from a good initial solution. This can dramatically speed up the learning process. This can be revealed by the learning curves illustrated in Fig. 6 for three benchmark functions. In addition, if FNN can start from a good solution, basically, the chance to get stuck to the local minima is lower. Thus, for most cases, the proposed two-stage FNN has lower MSE values, as shown in Table 3. The proposed two-stage FNN also has smaller standard deviation. This means that integration of Opt-aiNET and PSO can obtain stable results. However, the drawback of a hybrid method is the computation time. This is why the proposed method needs longer computation time compared with individual methods. However, the number of iterations is smaller for the proposed method. The result of the original NN-FLCS is nearly the worst. The reason is that original NN-FLCS employs the steepest descent method, which is very easy to get stuck to the local minima. However, like most meta-heuristics, it is necessary to determine suitable values for Opt-aiNet parameters. Through the Taguchi method, we can save a lot of computation time.

In prostate cancer prognosis, collecting data is very difficult. This is the reason why we only have 100 data. The original data collected from the hospital includes many variables such as age, height, weight, iPSA, DRE, biopsy Gleason score, C-Stg-T and so on. However, it is not efficient to use all the variables. Thus, after taking into account doctors’ advice and stepwise regression, only biopsy, iPSA and DRE are taken as the critical factors. Only these three factors are used to develop the proposed FNN model.

For two-stage FNN using prostate cancer data, the result is similar for these three benchmark functions. The two-stage FNN also has the best prediction capability compared to other individual methods and original NN-FLCS. In addition to accurate prediction, FNN also provides some fuzzy rules. These rules are interpretable [38,39] and are very important for medical doctors. They can explain the reasoning behind the results. This can dramatically assist medical doctors to make decisions.

@&#CONCLUSIONS@&#

This study has presented a novel two-stage FNN for a prostate cancer prognosis system. The proposed two-stage FNN can learn the relationship between the clinical features of prostate cancer and the prognosis of prostate cancer. Its performance is better than those of other soft computing-based techniques including AIS, GA, PSO and Opt-aiNET. The two-stage FNN results, which are in the form of fuzzy IF–THEN rules, can be easily interpreted. Once the clinical data are known, the prognosis of prostate cancer patient can be forecasted, thus providing doctors with a different way to make prognoses.

In the future, other soft computing techniques might be integrated into the heuristics in order to provide better estimation. Rule pruning might also be considered. In addition, increasing the amount of data model evaluation might increase the accuracy of the prognosis of prostate cancer. Since it is quite difficult to collect prostate cancer cases, it might be feasible to combine different hospitals’ prostate cancer cases together for the current study.

@&#REFERENCES@&#

