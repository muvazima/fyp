@&#MAIN-TITLE@&#Unsupervised segmentation of the vocal tract from real-time MRI sequences

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Vocal tract segmentation considering sequential nature of the RT-MRI data.


                        
                        
                           
                           Explicit consideration of vocal tract configurations with an open and closed velum.


                        
                        
                           
                           Single, high level segmentation initialisation per speaker, unsupervised operation thereafter.


                        
                        
                           
                           Small set of images for training: small manual annotation overhead.


                        
                        
                           
                           Evaluation of precision and accuracy over large image set and considering annotated images by four observers.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Vocal tract

Segmentation

Real-time MRI

@&#ABSTRACT@&#


               
               
                  Advances on real-time magnetic resonance imaging (RT-MRI) make it suitable to study the dynamic aspects of the upper airway. One of the main challenges concerns how to deal with the large amount of data resulting from these studies, particularly to extract relevant features for analysis such as the vocal tract profiles. A method is proposed, based on a modified active appearance model (AAM) approach, for unsupervised segmentation of the vocal tract from midsagittal RT-MRI sequences. The described approach was designed considering the low inter-frame difference. As a result, when compared to a traditional AAM approach, segmentation is performed faster and model convergence is improved, attaining good results using small training sets. The main goal is to extract the vocal tract profiles automatically, over time, providing identification of different regions of interest, to allow the study of the dynamic features of the vocal tract, for example, during speech production. The proposed method has been evaluated against vocal tract delineations manually performed by four observers, yielding good agreement.
               
            

@&#INTRODUCTION@&#

During speech production, the vocal tract configuration continuously changes over time. This dynamic nature has long been recognised as of paramount importance to study speech production (Saltzman and Munhall, 1989) and should cover the characterisation of the position and movement of the different articulators involved, such as the tongue, lips and velum (Fig. 1
                     ).

Several techniques have been proposed and used to gather data to enable the study of these dynamic aspects such as electromagnetic midsagittal articulography (EMMA) (e.g., Oliveira and Teixeira, 2007) or ultrasound (e.g., Wrench et al., 2011). Even though they provide high frame rates, they present some limitations by focusing in a restricted set of regions of the vocal tract. In recent years, real-time magnetic resonance imaging (RT-MRI) has been used for speech studies (e.g., Narayanan et al., 2011; Teixeira et al., 2012), allowing enough frame rate to provide useful data regarding the position and coordination of the different articulators, over time, while potentially avoiding the hyperarticulation effect observed in sustained productions (i.e., the speaker sustains vowel production while a single static image is acquired) (Engwall, 2003). Although typically acquired at the midsagittal plane, any other plane of interest may be considered (Silva et al., 2012).

A wide range of applications can profit from these dynamic studies (Höwing et al., 1999), such as the assessment of swallowing disorders (Kumar et al., 2013) or the characterisation of the articulatory properties of speech (Oliveira et al., 2012; Shosted et al., 2012; Silva et al., 2013). A notable example of the latter is the study of nasal and oral vowels (for example, considering European Portuguese, the second sound in “canto” ([I] sing) and “cato” (cactus)). These have traditionally been considered to differ essentially on the lowering of the velum, for nasal vowels, without any additional articulatory adjustment, but a few studies have recently shown evidence of modifications occurring also at the tongue and lips (Engwall et al., 2006; Carignan, 2011; Shosted et al., 2012a).

The articulation of European Portuguese (EP) nasal vowels has been addressed by the authors and colleagues (Martins et al., 2008; Oliveira et al., 2009), focusing on velum dynamics or using limited tongue information obtained from EMMA studies. Real-time MRI has been acquired (Teixeira et al., 2012) to extend these studies with a characterisation of the oral configuration of EP nasal vowels, important, for example, for articulatory synthesis (Teixeira et al., 2005; Birkholz et al., 2011).

The often used visual comparison among different image frames is of limited use due to the subjective nature of the comparison and the difficulty to cross compare among multiple images. Therefore, relevant data regarding the vocal tract and different articulators must be extracted for analysis. Real-time MRI studies rapidly result in several thousands of images and one of the main challenges concerns how to deal with such a large amount of data in order to extract relevant features and provide researchers with the materials and visualisations that allow systematic analysis (e.g., Silva et al., 2013).

In this scenario, manual segmentation of each image is not only unthinkable, given the large number of images, but also inadvisable in a scenario where a large number of observers might be available to perform the annotation, since the noisy nature of the images makes it difficult to maintain consistency intra- and inter-observer. Therefore, a systematic (semi-)automatic method to extract the relevant data should be used.

@&#RELATED WORK@&#

Analysis of dynamic MRI vocal tract data has typically been performed looking into pattern variations at pixel level, with no segmentation of the anatomical structures of interest (Demolin et al., 2002), or focusing on specific regions, e.g., tongue dorsum (Stone et al., 2001). In recent years, a few authors have presented methods to segment the vocal tract (or specific articulators) from MRI.


                        Avila-Garcia et al. (2004) perform automatic tongue segmentation on dynamic MRI studies by combining active shape models (ASM), trained from 39 manually annotated images, with the dynamic Hough transform. To constrain the search in Hough space, the full image sequences are considered in order to find a global optimum. Although the proposed method seems promising, the authors report a high computational complexity of this approach to the point that only a single variation mode for the ASM could be used. This fact, and the lack of quantitative data regarding its performance and the effects of considering the Hough transform, preclude any consideration regarding its merits.


                        Bresch and Narayanan (2009) proposed a method that uses data on the spatial frequency domain, extracted from k-space, to perform segmentation of the vocal tract from RT-MRI. Each image in the sequence is processed independently and explicit identification of the different articulators is provided based on an anatomically informed geometric model.


                        Peng et al. (2010) use a shape-based method to segment the tongue contour in MRI images of sustained productions of different sounds. Shape priors are obtained from a set of 39 images from one speaker and then used to define a curve representation which is the base for segmenting 64 images from three other speakers. Prior to segmentation, the images are roughly aligned and scaled to match the pose of the speaker considered for the training set. Enhancements to this method are proposed by Eryildirim et al. (2011) to allow a correspondence between the contours and include an automated method to detect the tongue extremities (lingual frenulum and epiglottis).


                        Proctor et al. (2010) describe a vocal tract segmentation method applied to RT-MRI. Based on the works of Maeda (1979) a grid is superimposed on each image frame. To perform segmentation, the pixel intensity profiles along the line grids are analysed looking for local minima and the vocal tract defined by an optimal path between the relevant minima. A post-processing is applied to the extracted vocal tract profile to smooth the contour segment corresponding to the tongue.


                        Vasconcelos et al. (2011) have presented an approach to vocal tract segmentation using active shape/appearance models applied to MRI images of the vocal tract for several relevant sustained productions of EP sounds. Nevertheless, the application scope is limited as segmentation is only applied to images from a single speaker and validation of the proposed method is performed using four images (with 21 images in the training set).


                        Katsamanis et al. (2011) also propose an active shape model trained from 460 utterances of a single speaker (circa 30,000 vocal tract images). Note, however, that the purpose of the presented work was not segmentation, which was performed using the method by Bresch and Narayanan (2009). The main goal was to obtain a model that could provide data on the different/relevant shape properties of the vocal tract during speech production to distinguish between different types of articulation.


                        Raeesy et al. (2013) perform vocal tract segmentation from RT-MRI images by using oriented active shape models on each image frame separately. They also propose an automatic method to set the landmarks used for training. Since these are computed from manually segmented contours, it is not yet clear how much is gained from such method and it introduces further variability in landmark position, regarding anatomical landmarks. This results in a lack of landmarks in important points, such as the upper lip, leading to poor segmentation results as depicted in Fig. 4 of Raeesy et al. (2013). Furthermore, careful positioning of landmarks might be very useful for easier contour partitioning (e.g., identifying different regions of interest, such as the tongue or the velum).

The authors (Silva et al., 2013) presented a first segmentation method to tackle vocal tract segmentation from RT-MRI, based on region growing propagation over time and considering different regions of the vocal tract, with different intensity properties. Even though this method generally provides very good segmentations, it poses some difficulties when a strong contact between the tongue dorsum and the hard palate is observed. In these situations the segmentation can be edited manually, to separate both structures, but it requires a considerable amount of work and often affects tongue shape. Furthermore, even considering that, at most times, user intervention was not needed, it was still important to supervise the segmentation outputs in case it was necessary to tweak seed point position or adjust threshold intervals.

Regarding image acquisition, no standard protocol exists for RT-MRI of the vocal tract. Even though, for static images, the final results might be comparable in terms of image resolution and quality, for RT-MRI acquisitions there is a lot of variability among research groups regarding the used acquisition protocols, resulting in different frame rates with significant impact on image resolution and noise levels.

It is also important to highlight that none of the works presented in the literature performs thorough evaluation of the proposed methods. Evaluation is often qualitative (Bresch and Narayanan, 2009), performed over a small image set (Vasconcelos et al., 2011; Raeesy et al., 2013) or considering annotations by a single observer (Proctor et al., 2010).

A novel segmentation approach is proposed, based on active appearance models (AAMs), to address the task of segmenting a large database of RT-MRI images acquired to study EP vowels (for which k-space data is not available and avoiding the frequency domain analysis complexity). Its main purpose is to tackle the issues detected in our previous work (Silva et al., 2013) and to build a more versatile framework, providing suitable data for parametric vocal tract analysis (Silva et al., 2013)

It is important to clarify that these RT-MRI databases, acquired for articulatory studies, involve a fair amount of resources and do not typically include data for many speakers. Instead, they include extensive data for each speaker (in our case, around 4000 images/speaker). Therefore, our aim is not to provide a segmentation method that can be used, off-the-shelf, with new speakers. We aim, instead, to propose a segmentation method that, following clearly defined criteria for selecting a small number of manually annotated training images, allows unsupervised segmentation of the full database and later extension to other speakers, following the same criteria. Since we need to rely on the segmentations provided by the proposed method to perform automatic articulatory analysis, proper evaluation of the method is of the utmost importance and must be carried out to assess its precision and accuracy. Regarding performance, it was not a major goal to provide a very fast method since it can be left running offline, generating the data for later analysis.

The work presented in this paper, considering the surveyed literature, provides several notable contributions:
                           
                              (a)
                              Consideration of the sequential nature of the data, exploring the proximity between adjacent frames to minimise convergence problems, which, to the best of our knowledge, has never been successfully considered for the segmentation of the vocal tract in this kind of image.

Explicit consideration of two configurations of the vocal tract with an open and closed velum. This can have a major impact on automated parametric analysis of velum height if, as happens for some speakers, velum aperture does not necessarily result in a considerable height variation and automated analysis must rely on other factors to assess velar differences. Furthermore, it is a requirement for velar aperture assessment and very important if the extracted contours are to be inspected by observers, for exploratory analysis of the data.

Clear separation between the contour segments corresponding to different articulators (e.g., tongue and hard palate) is guaranteed at all times, even when they are in contact (coincide).

Single, high level, user defined initialisation of the method for each speaker and unsupervised operation thereafter.

Automatic identification of different regions of interest in the vocal tract based on the criterial landmark placement during training.

The proposed method has also been evaluated regarding the influence of different initialisations and by comparing its outputs against segmentations performed manually by four observers on 50 images, yielding very good results. The evaluation procedure and the number of observers and images considered for the evaluation setting are also a notable difference regarding related works where evaluation, when performed, has been mostly done qualitatively or considering a very limited number of images and/or observers.

The remainder of this paper is organised as follows: Section 2 presents a brief description of the basic aspects involved in using active shape/appearance models for segmentation; Section 3 describes how the AAM models were created and applied to segment the vocal tract from a large set of RT-MRI image sequences; Section 4 presents the evaluation protocol used to assess the quality of the resulting segmentations and discusses evaluation data regarding precision, accuracy and performance; finally, Section 5 presents some conclusions and ideas for further work.

A brief description of the main aspects of active shape/appearance models is presented in order to provide a context to the presented work. For additional details concerning these methods, the reader is forwarded to Cootes et al. (1995, 2001), Van Ginneken et al. (2002).

To build active shape models (ASM) (Cootes et al., 1995; Van Ginneken et al., 2002) shapes need to be described by a set of L points (landmarks) which are initially defined manually over a set of N training images. For shape i, with i
                        =1, …, N, the landmark vector is given by:
                           
                              (1)
                              
                                 
                                    
                                       
                                          x
                                       
                                    
                                    i
                                 
                                 =
                                 
                                    
                                       
                                          
                                             (
                                             
                                                x
                                                
                                                   i
                                                   1
                                                
                                             
                                             ,
                                             
                                                y
                                                
                                                   i
                                                   1
                                                
                                             
                                             )
                                             ,
                                             …
                                             ,
                                             (
                                             
                                                x
                                                iL
                                             
                                             ,
                                             
                                                y
                                                iL
                                             
                                             )
                                          
                                       
                                    
                                    T
                                 
                              
                           
                        
                     

After alignment of the different shapes, the mean shape 
                           
                              x
                              ¯
                           
                         is given by:
                           
                              (2)
                              
                                 
                                    
                                       
                                          x
                                       
                                    
                                    ¯
                                 
                                 =
                                 
                                    1
                                    N
                                 
                                 
                                    ∑
                                    
                                       i
                                       =
                                       0
                                    
                                    N
                                 
                                 
                                    
                                       
                                          
                                             x
                                          
                                       
                                       i
                                    
                                 
                              
                           
                        
                     

The covariance can then be computed by
                           
                              (3)
                              
                                 S
                                 =
                                 
                                    1
                                    
                                       N
                                       −
                                       1
                                    
                                 
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    N
                                 
                                 
                                    (
                                    
                                       
                                          
                                             x
                                          
                                       
                                       i
                                    
                                    −
                                    
                                       
                                          
                                             
                                                x
                                             
                                          
                                       
                                       ¯
                                    
                                    )
                                    
                                       
                                          (
                                          
                                             
                                                
                                                   x
                                                
                                             
                                             i
                                          
                                          −
                                          
                                             
                                                
                                                   
                                                      x
                                                   
                                                
                                             
                                             ¯
                                          
                                          )
                                       
                                       T
                                    
                                 
                                 .
                              
                           
                        
                     

The eigenvectors, ϕ
                        
                           i
                        , also known as variation modes, corresponding to the t largest eigenvalues, λ
                        
                           i
                        , of the covariance matrix are considered, Φ
                        =(ϕ
                        1|ϕ
                        2|…|ϕ
                        
                           t
                        ), and used to express the shape of a new object (i.e., its landmarks vector) as a point distribution model (PDM):
                           
                              (4)
                              
                                 
                                    
                                       x
                                    
                                 
                                 ≈
                                 
                                    
                                       
                                          
                                             x
                                          
                                       
                                    
                                    ¯
                                 
                                 +
                                 Φ
                                 b
                                 ,
                              
                           
                        where b
                        =(b
                        1, b
                        2, …, b
                        
                           t
                        )
                           T
                         is a vector of weights associated to each variation mode.

The number of variation modes considered, t, is chosen as the minimum that allows explaining a certain proportion, 
                           
                              f
                              v
                           
                        , of the total variance in the training shapes and typically ranges from 90% to 99.5%:
                           
                              (5)
                              
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    t
                                 
                                 
                                    
                                       λ
                                       i
                                    
                                 
                                 ≥
                                 
                                    f
                                    v
                                 
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       2
                                       L
                                    
                                 
                                 
                                    
                                       λ
                                       i
                                    
                                 
                              
                           
                        
                     

To attain the ASM, a grey-level appearance model must also be built. This is performed by considering the neighbourhood of each landmark: the normal direction at each landmark is determined (based on neighbour landmarks) and a grey-level profile is obtained sampling k pixels to either side. Similarly to what was performed with landmark coordinates, the mean grey-level profile and covariance can be computed leading to an expression analogous to Eq. (4). The grey-level appearance model used in conjunction with the PDM described in Eq. (2) can then be used to segment objects in new images by positioning the mean model over it and then searching for the best landmark positions that satisfy both the geometric characteristics defined by the PDM and the expected grey-level profiles. This search operation can be optimised if a multi-resolution scheme is used as proposed by Cootes et al. (1994). Instead of searching over the image at the original resolution, search can be performed at different resolution levels, starting at a coarser resolution and finishing at the original resolution. This improves speed and, since the grey-level profiles are also adjusted according to resolution, it also avoids that, during the first search iterations, the model is caught by fine features near its target structure.

The ASM method can be further enhanced by building an active appearance model (AAM) (Cootes et al., 1998; Gao et al., 2010) which uses all the available data instead of just the landmark neighbourhoods. A PDM also needs to be built, but appearance is now modelled by distorting each of the training images so that the landmarks defined in each match the mean shape. Next, the region covered by the mean shape is sampled and the grey values used to build a texture model. Similar to what was performed for the PDM, the appearance model can then be expressed by:
                           
                              (6)
                              
                                 g
                                 ≈
                                 
                                    
                                       g
                                       ¯
                                    
                                 
                                 +
                                 
                                    Φ
                                    g
                                 
                                 
                                    b
                                    g
                                 
                                 ,
                              
                           
                        where 
                           
                              g
                              ¯
                           
                         is the mean texture, Φ
                        
                           g
                         is the matrix of eigenvectors (texture variation modes derived from the training set), and b
                        
                           g
                         is the weight vector.

@&#METHODS@&#

The research work carried out by the authors, concerning the articulatory characterisation of EP, includes all stages from RT-MRI image acquisition (Teixeira et al., 2012) to the extracted data analysis (Oliveira et al., 2012; Silva et al., 2013). To provide context, Fig. 2
                      depicts the main stages of the work highlighting, at the centre, the focus of this paper, image segmentation. In what follows details are provided regarding image acquisition and the different aspects concerning the development and application of the proposed segmentation method are described.

The articulation of European Portuguese (EP) nasal vowels has been studied by our group using modalities such as EMMA (Oliveira et al., 2009) and static MRI Martins et al. (2008). More recently, RT-MRI has been acquired to extend these studies with further characterisation of the oral configuration of EP nasal vowels (Oliveira et al., 2012; Martins et al., 2012). It is important to note, as previously mentioned, that this imaging modality provides adequate data regarding the position and coordination of the different articulators over time (Hagedorn et al., 2011) and might provide a good choice to tackle the hyperarticulation effect observed in sustained productions (Engwall, 2003). Additionally, it might also help reduce the gravity effect on articulators for acquisitions in supine position (Tiede and Vatikiotis-Bateson, 2000). Therefore, beyond providing the data regarding the dynamic aspects of speech, RT-MRI can also support static studies, tackling some of the associated issues.

Image sequences were acquired containing: (a) the five European Portuguese (EP) nasal vowels uttered in three word positions: initial, internal and final (e.g., the nonsense words “ampa, pampa, pan” [
                           
                        p
                           
                        , p
                           
                         p
                           
                        , p
                           
                        ]); and (b) the eight EP oral vowels (e.g., “papa” or “pupa”).

Acquisition was performed at the midsagittal plane of the vocal tract. The images were acquired on an unmodified 3.0T MR scanner (Magneton Tim Trio, Siemens, Erlanger, Germany) equipped with high performance gradients (Gmax = 45mT/m, rise time=0.2s, slew Rate=200T/m/s, and FOV=50cm). Custom 12-channel head and 4-channel neck phased-array coils were used for data acquisition. Parallel imaging (GRAPPA 2) and magnetic field gradients operating at FAST mode were used to speed up the acquisition. After localisation images, a T1 W 2D-midsagittal MRI slice of the vocal tract was obtained, using an Ultra-Fast RF-spoiled Gradient Echo (GE) pulse sequence (Single-Shot TurboFLASH), with a slice thickness of 8mm and the following parameters: TR/TE/FA=72ms/1.02ms/5°, Bandwidth=1395Hz/pixel, FOV (mm2)=210×210, reconstruction matrix of (128×128) elements with 50% phase resolution, in-plane resolution (mm2)=3.3×1.6, yielding a frame rate of 14images/s.

Typically, each recorded sequence contained 75 images (taking around 5s to acquire) although some longer sequences (300 images) were also acquired, mostly with the speakers producing sequences of isolated vowels (e.g., [
                           
                        ], [˜e], [ĩ], [õ], [ũ], [
                           
                        ], etc.).

Audio was recorded simultaneously with the RT-MRI images inside the MR scanner, at a sampling rate of 16,000Hz, using a fibreoptic microphone and manually annotated, using the software tool Praat (http://www.fon.hum.uva.nl/praat/), in order to identify the time intervals corresponding to different sounds. The time intervals allow the determination (because both data are aligned) of the corresponding image frames.

Data was acquired for three female speakers (CM, CO and SV), aged between 21 and 33, phonetically trained, with no history of hearing or speech disorders.

It is important to note that RT-MRI acquisitions require a large amount of resources. A greater diversity of speakers, at this moment, could have been accomplished by reducing the number of sequences acquired per speaker, but this, beyond increasing the overhead regarding speaker preparation, would have a negative impact on the range of studies that can be performed using the data.

The obtained frame rate of 14 frames/s, for this dataset, although not as high as reported by other authors (e.g., Lammert et al., 2010; Raeesy et al., 2013), has already been useful to support articulatory studies (e.g., Oliveira et al., 2012; Martins et al., 2012) and research on velar movement detection using surface electromyography (Freitas et al., 2014). Therefore, it constitutes an adequate dataset to demonstrate the proposed segmentation method.

Further details concerning the image acquisition protocol and corpus can be found in Teixeira et al. (2012).

There are two different kinds of vocal tract configurations that result in slightly different segmentations: those with an open velum and those with a closed velum (refer to Fig. 3
                         for an example of each case). From an analysis point of view by, for instance, a linguist, when looking just to the extracted vocal tract profile, it is easier to distinguish between an open and closed velum if the segmentation for the closed velum does not include the nasal cavity. Furthermore, velar aperture assessment requires that the velar passage is segmented. Quantitative assessment of velar differences might also profit from this approach (Silva et al., 2013): for some speakers velum height does not vary much between the open and closed velum configurations and the assessment of the bottom side of the velum is not enough to detect the difference.

Building a single model to accommodate both oral and nasal options resulted in a less stable option as it often failed to converge to a proper solution. This happened because such a model encompasses much more variability in the velar region which, considering the noise in the images, was often a problem. An approach based on bifurcating contours (Seise et al., 2007) might be suitable, but added complexity which we considered avoidable. We opted for building a different model for each situation (hereafter referred to as the nasal model, which contains a nasal cavity, and the oral model) and define rules to decide, at each image frame, which model to use.

The choice of which images to use for training the model was based on three main aspects: (a) there is significant variability among speakers concerning some characteristics of the vocal tract (e.g., size, angle of the pharynx) and therefore it would be important to include images from all three speakers; (b) training images should cover the most notably different vocal tract configurations present, relying on the model to adapt to intermediate configurations; and (c) as manual segmentation of the vocal tract is a tiresome, time consuming task, the number of images included should not be unnecessarily large. The goal is to build an AAM model with a small set of annotated images that allows proper segmentation of the entire database.

Both oral and nasal models have been built using the same procedure, with the sole differences residing on the images used for training and the inclusion of the nasal cavity in the nasal model. Twenty-six landmark points have been defined manually along the vocal tract and covering the main anatomical structures of interest.


                        Fig. 4
                         shows the landmarks depicted over a vocal tract image: 3 for each lip, 2 for the lingual frenulum, 1 at the tongue tip, 6 evenly distributed over tongue dorsum and back and 1 at tongue root, 4 along the pharynx, 3 on the velum and 3 at the hard palate and alveolar ridge. Interpolation was performed, adding 18 points between landmarks. The method used to manually define the landmarks also allowed setting secondary points, between landmarks, in order to guide how interpolation was performed between them. For the nasal model a few secondary points were used to include the nasal cavity.

The training set contained a total of 51 images, 30 images for the nasal model, covering the three speakers and all EP nasal vowels and 21 images for the oral model, covering all speakers and oral vowels. These were selected from all occurrences available in the database, identified according to the annotations of the audio signal and, for each occurrence, selecting the most representative frame of the interval (e.g., velum clearly open). Images presenting acquisition artefacts that might hinder proper creation of the statistical model, e.g., unclear separation between tongue and velum, were not considered. Vocal tract configurations with the lips completely closed were also not included as preliminary experiments seemed to show better results when these were not used. Fig. 5
                         shows some of the images used and defined landmarks and Fig. 6
                         presents the first four variation modes for each model (oral and nasal). Although not the subject matter of this paper it is important to note that these variation modes are also an important result as they describe the most important articulatory characteristics of the vocal tract and might be an important tool for articulatory recognition (Vasconcelos et al., 2011; Katsamanis et al., 2011).

The AAM models created were used to perform segmentation of the vocal tract along the image sequences.

An initialisation stage, requiring user supervision, was performed once for each speaker and for the nasal and oral models separately. Initialisation consisted in two tasks: the definition of the initial mean model position, over one image frame, and verification if a proper segmentation was obtained with such positioning. Mean model positioning is performed interactively: the user moves the mean model over the image and clicks on the desired position. Then, the segmentation method takes over and the model is automatically adjusted to the image using a four level multi-resolution search approach (Cootes et al., 1994). To assess if a proper segmentation resulted the user is required to validate it by simply checking if the model converged to the speaker's vocal tract size and if it reached the lips and nasal cavity. If any major adjustment problem is detected the mean model can be repositioned and the process repeated.

This is the only step of the presented method requiring user intervention and the complete initialisation step takes around 10–15s. When compared with the initialisation of the methods presented by Bresch and Narayanan (2009) and Proctor et al. (2010), it is simpler, faster and performed at a higher level, since a single click is needed and no manual annotation of the vocal tract is required.

The initialisation is performed over images of the vocal tract for [
                           
                        ] and [a], since these configurations are closer to those depicted by the mean models.

The two segmentations obtained in this stage were thereafter used as the initial models for each speaker (hereafter known as the speaker-specific initial models), instead of the mean models, to perform the segmentation of the first oral/nasal image frame in each sequence. Since this initial model is already close to several speaker dependant features (e.g., dimension, hard palate and pharynx configurations) search for the optimum solution could be performed at the highest image resolution and the model generally converged better/faster than when the mean model was used.

For the remaining images beyond the first, in each sequence, using only the highest resolution for search, segmentation was performed using the final segmentation of the previous frame (Cootes et al., 1994; Ionita et al., 2011) as the starting condition. This approach was chosen since neighbour image frames typically presented vocal tract configurations which did not strongly differ and allowed for faster segmentation as it required a smaller number of iterations to converge. As noted by Ionita et al. (2011), when small displacements occur, it is inefficient to discard the current solution, at high resolution, and start the search back at the lowest resolution as this increases the chance of error (e.g., in the presence of image noise or contact between articulators). On the other hand, for large displacements, this approach might fail, but no severe case has been observed for our data. At most, the contour might not adjust immediately and take a couple of frames to do it.

It might also be argued that using one segmentation as the initialisation of the next frame might pose problems if the segmentation starts to diverge. We observed that this does not occur often, but it might start to happen for configurations with strong contact between articulators (e.g., between tongue and hard palate), particularly if the contact lasts for a few frames and the lips are blurry. In this situation, the model typically returns to normal once the vocal tract configuration changes, a few frames ahead (e.g., the tongue or the lips move).

For each segmented image frame, the proper model (oral or nasal) needed to be selected. One possibility would be to perform an a priori analysis of the image region concerned with the velar port (e.g., Silva et al., 2013), to extract velum aperture data, choosing the model accordingly. Taking into account that the change between models was not expected to be very frequent, i.e., the same model would be used in 5–6 frames before the other model was chosen, we apply one of the models and then perform analysis of the image in the vicinity of the landmarks located at the velum tip and at the top of the pharynx. Fig. 7
                         shows a diagram depicting the main steps considered to choose the proper model. The region between the two landmarks is analysed in order to compute the mean image intensity value (see Fig. 8
                        ).

When the mean intensity between the landmarks exceeds half the maximum intensity of the image, the velum is considered closed and the oral model should be applied. If this is not the current model, then the oral model is applied to the same image frame. The error associated with this segmentation is compared with that obtained with the nasal model to check if it is smaller. If, even though the region between velum tip and top of the pharynx is above the intensity threshold, the error got larger, then the nasal model is kept. A similar procedure is followed for the case when the region between velum and pharynx is below the intensity threshold leading to the choice of the nasal model.


                        Fig. 9
                         shows several examples of the obtained segmentations, for all speakers, and covering a wide variety of vocal tract configurations. These show smooth contours presenting very good adjustment to the different articulators.

One important aspect to note is that, due to the positions chosen for the different landmarks, when creating the models, it is easy to automatically identify different regions of interest from the segmented contours (as depicted, in different colours and line styles, in Fig. 9) namely, the lips, tongue body and tip, velum, hard palate and pharynx, important to support analysis (e.g., Silva et al., 2013).

Considering other ASM/AAM based vocal tract segmentation methods (Vasconcelos et al., 2011; Raeesy et al., 2013) our approach adds to them by: (1) requiring a single manual initialisation step per speaker (as opposed to one initialisation per image); (2) using two models to explicitly tackle oral and nasal configurations; (3) addressing data from multiple speakers (only one speaker considered in Vasconcelos et al., 2011); (4) using criterial positioning of landmarks allowing subsequent identification of different regions of interest in the vocal tract; and (5) starting the segmentation of each frame with the segmentation of the previous frame.

@&#EVALUATION@&#

The evaluation of segmentation methods should typically consider three main aspects (Udupa et al., 2006): precision, accuracy and performance. To assess how the proposed segmentation method performs on these three aspects, a set of evaluation tasks was devised and carried out as described in what follows.

Precision, or reliability, deals with the similarity among segmentations obtained for the same data at different times. In our case, one important aspect that might affect segmentation is the mean model initial position, defined by the user, regarding how it affects repeatability (Chang et al., 2009; McGuinness and O’Connor, 2010; Seshadri and Savvides, 2012). The proposed method includes an initialisation stage in which the initial position for the mean model is set and the segmentation is performed for a single image frame to attain the speaker-specific initial model. Therefore, since the initialisation step is supervised by an observer, the initial mean model localisation will always result in a proper initialisation (otherwise, the localisation is changed until it does). Nevertheless, two aspects deserve attention: (1) how the position chosen for the mean model influences the obtained speaker-specific initial model and (2) how different speaker-specific initial models influence image sequence segmentation.


                        Effect of position in speaker-specific model initialisation – considering the image used for initialisation, for each speaker, a proper vocal tract segmentation (as judged by an observer) was used as the reference initialisation. Considering a position where the mean model was roughly aligned with the vocal tract, translations from that position, on both axes, were applied in the (empirically set) range −15 to 15 pixels in 5 pixels steps. The initial speaker-specific model was then obtained for each of these locations. The reason by which the position used as a starting point for the displacements is not of paramount importance is that our main interest is not to find the optimal initial position (or all positions yielding good segmentations) for the model, but to characterise the effects of changing that position. The maximum mean model displacement values were chosen in order to encompass the region where a user could typically position the mean model. Fig. 10
                         presents two examples of the region covered by moving the mean model on the chosen intervals. For the sake of simplicity, the initial mean model positions for the oral and nasal models were kept the same.

The new speaker-specific initial models thus obtained were compared with the reference initialisation using the Dice similarity coefficient (DSC) (Popovic et al., 2007):
                           
                              (7)
                              
                                 DSC
                                 =
                                 
                                    
                                       2
                                       
                                          
                                             
                                                A
                                                ∩
                                                B
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             A
                                          
                                       
                                       +
                                       
                                          
                                             B
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The DSC measures the amount of overlap between two segmentations (A and B) normalised by the summed areas of both: therefore, it is 1 when the regions contained inside both contours coincide and 0 (zero) when they are completely different. The literature suggests that a value of DSC
                        >0.7 represents a good overlap (Zijdenbos et al., 1994).


                        Fig. 11
                        a and b presents two examples of DSC values distribution over the tested regions, obtained for two of the speakers.

The brighter regions depict locations where the obtained speaker-specific initial models were closer to the reference model, while darker regions depict increasing differences between both. The plot in Fig. 11b, for example, shows how, for that speaker, initialisation needed more care, noticeable by a smaller region leading to good initialisation than for the speaker in Fig. 11a. Nevertheless, the gathered data shows evidence that the region where the user can place the mean model (and will result in a good initialisation) is not very restricted.


                        Effect of different speaker-specific models on image sequence segmentation – in the evaluation step described above several mean model initial positions were tested resulting in different model initialisations (per speaker). Then, we analysed which positions provided good initialisations by comparing with a reference initialisation using the DSC. We now wanted to assess how these different initialisations resulted in differences during the segmentation of image sequences.

First, the initialisations used as reference were considered for the segmentation of four image sequences per speaker (30 frames per sequence) providing reference segmentations. Then, all initialisations evaluated earlier which obtained a DSC above 0.8 were considered (Zijdenbos et al., 1994). The rationale behind the choice of only these initialisations for testing was that, in our method, the model initialisation is supervised by the user and, therefore, a good initialisation will always be used for segmentation.

Next, the chosen initialisations were used to perform the segmentation of the four image sequences per speaker and the resulting segmentations compared with the reference segmentations. Fig. 11c shows the overall box plots drawn considering all sequences and speakers depicting how the segmentation generally evolves along the sequences. One notable aspect observed is that several initial locations yield a segmentation that differs more from the reference on the first few frames, depicted by the wider box plots (although generally keeping high DSC values), but then comes closer to the reference on the remaining frames. This shows that the different initialisations have a small influence on the final segmentation results.

Accuracy, or validity, concerns how the proposed segmentations compare with true segmentation or, in most cases, a surrogate of true segmentation (Udupa et al., 2006).

Several aspects of accuracy are assessed for the proposed method: (a) the overall comparison with manual segmentations performed by observers; (b) regional assessment of which parts of the vocal tract were the main source of existing differences; (c) agreement between the proposed method and the group of observers; and (d) the influence of design choices on accuracy.


                        Experimental setting – To assess the accuracy of the proposed segmentation method, image frames of every oral/nasal vowel available on the database were randomly chosen from all speakers by selecting the last frame of the vowel occurrence based on the audio annotation. Furthermore, additional images where considered covering intermediate vocal tract configurations, namely presenting closed lips and the first frame for the nasal congeners of the EP cardinal vowels ([
                           
                        , ĩ, ũ]), as some difference is expected on the vocal tract configuration over their production for EP (Oliveira et al., 2012). In total, 50 images were considered, none of which was part of the training sets for the AAMs.

A supervised evaluation method was adopted (Zhang et al., 2008). A set of four observers, two radiographers (OBS1 and OBS3), and two phoneticists (OSB2 and OBS4) were asked to segment the vocal tract using a completely manual contour delineation tool (Fig. 12
                        ).

The first radiographer (OBS1) and the first phoneticist (OBS2) had previous experience with RT-MRI images and speech studies, while the second element of both groups (OBS3 and OBS4) had general experience in their fields of work. None of these observers was involved in the vocal tract annotations of the training image set. The session started with a brief explanation regarding what was required, how the tool worked and each observer was allowed to experiment with contour delineation (by clicking points along the desired contour) over a training image which was not considered for evaluation. When the observers felt comfortable working with the delineation tool, they could proceed with the 50 segmentations. Each observer segmented the images in a different order to minimise the effects of fatigue or previous segmentations on the overall results. All observers performed the segmentations using the same desktop computer and similar ambient light conditions. Fig. 13
                         presents some of the segmented images, representative of the overall results, showing the manual and automatic segmentations.

The vocal tract contours, obtained using the proposed segmentation method, were compared with the manual segmentations using the DSC, sensitivity (p) and specificity (q) (Jayender et al., 2013). Overall comparison (method vs. observers) – Table 1
                         presents the mean and standard deviation values for DSC, p and q obtained by comparing between the proposed method and the manual segmentations performed by each observer.

It can be noted that the values for each observer, along with the overall values, express good agreement between the segmentations provided by the proposed method and those performed by observers. The specificity value is always 99% which was expected and explained by the fact that the size of the vocal tract is relatively small compared to the whole image (Zhang et al., 2012). To minimise this effect on specificity, the bounding box enclosing the corresponding segmentations (observer and segmentation method) was computed and a new specificity value computed (q′). These new specificity values are lower, but do not fall below 92%.


                        Regional difference assessment – Even though the differences between the manual segmentations and those obtained using the proposed method were small, we also assessed if any particular region of the vocal tract had a more frequent discrepancy between the manual segmentations and those provided by the proposed segmentation method.

To perform regional difference assessment, the contours provided by the segmentation method were divided into different regions exhibiting anatomical/articulatory relevance and the Euclidean distance computed from each point in the region towards the observer contours. Fig. 14
                        a shows box plots of the distances computed for each of the regions considered.

A mask of the difference between the regions contained inside each manual segmentation and its corresponding automatic segmentation was computed and all masks added, by speaker. This resulted in an image in which the regions where differences are more frequent appear with a higher pixel value. These masks should not be looked at as an absolute measure of error incidence, but as a complementary view to that provided by the box plots. Fig. 14b shows an example of the overall difference mask for speaker CM.

Excluding the difference denoted at the lips, which was expected as there is no clear feature delimiting where to start/stop the segmentation, it is possible to note that there is a stronger difference for the lingual frenulum and velar regions and that the lingual frenulum is where differences occur more frequently. In the case of the lingual frenulum, difference originated mostly from the automatic segmentation not adjusting properly (or as tightly as the observers) or different segmentation criteria used by the observers. Regarding the velar region, segmentation was more difficult due to the frequent presence of motion artefacts or an unclear passage between the oral and nasal cavities resulting in some observers assuming a closed velum.


                        Fig. 14c shows examples of different situations which originated discrepancy between segmentations. On the top left, the observer considered a closed velum, while the segmentation method (and remaining observers) considered it open; on the top right, the observer, due to artefacts, did not fully segment the pharynx; on the bottom left image, the segmentation method did not segment the region between the lower lip and the tongue as tightly as the observer; and, on the bottom right, the observer considered that the epiglottis was clearly separated from the tongue while the segmentation method included it with the tongue.


                        Agreement with observers group – Similarity measures such as the DSC allow to compare pairs of segmentations. Since we have a set of surrogate truths (provided by qualified observers) it is also interesting to assess the agreement of the proposed segmentation method with the group of observers.

To determine a consensus among all observers is not trivial and several methods have been discussed in the literature (Vanbelle and Albert, 2009), which might even lead to different conclusions about the same data. To address these issues some measures have been proposed (Williams, 1976; Chalana and Kim, 1997; Kouwenhoven et al., 2009) such as the Williams agreement index. This index, widely used in the literature (Bouix et al., 2007; Babalola et al., 2009), can be expressed as:
                           
                              (8)
                              
                                 
                                    WI
                                    i
                                 
                                 =
                                 
                                    
                                       (
                                       n
                                       −
                                       2
                                       )
                                       
                                          ∑
                                          
                                             j
                                             ≠
                                             i
                                          
                                          n
                                       
                                       
                                          
                                             D
                                             ij
                                          
                                       
                                    
                                    
                                       2
                                       
                                          ∑
                                          
                                             j
                                             ≠
                                             i
                                          
                                          n
                                       
                                       
                                          
                                             ∑
                                             
                                                k
                                                ≠
                                                i
                                             
                                             
                                                j
                                                −
                                                1
                                             
                                          
                                          
                                             
                                                D
                                                jk
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where D
                        
                           ij
                         is a similarity/discrepancy measure between segmentations i and j (e.g., DSC) and n is the number of segmentations. The value provided by the Williams index expresses how a particular segmentation i compares with the group of the remaining segmentations and can be interpreted as follows: if greater than 1, the remaining segmentations are more similar to i than to each other; if close to 1, segmentations are as similar to i as they are to each other; and if below 1, remaining segmentations are more similar to each other than each is to i.

The Williams index was computed for the proposed method versus the observers group and for each observer considering the remaining observers. Fig. 15
                         shows box plots depicting agreement as provided by the Williams index and presenting notches marking the 95% confidence interval.

It can be noted that, for the segmentation method proposed, the Williams index attains a median slightly above 1. This provides some evidence that the segmentation method produces segmentations that are closer to those performed by the observers than the observers are among themselves.


                        Assessment of alternative segmentation approaches – The evaluation data gathered allows to conclude that the proposed segmentation method provides very good accuracy results. Nevertheless, the different design choices made, such as initialising the segmentation of each frame with the segmentation of the previous frame, or using two models with different nasal cavity configurations, were performed considering the application scenario and on the base of preliminary experiences. In order to confirm that, as expected, these design choices actually improved the overall results of the segmentation, when compared with a more traditional AAM approach, additional assessment was performed.

All the 50 image frames, manually segmented by the four observers, were also segmented using the traditional AAM approach, segmenting each image frame individually and considering four different alternatives to the proposed method which mainly differ in the training data used to create the models (main features summarised in Table 2
                        ):


                        Alternate oral/nasal – similarly to the proposed method, this alternative also uses two models, each trained for one of the considered configurations (oral or nasal). The model to use, at each frame, is also chosen as described in Fig. 7. The major differences towards the proposed method are that no speaker specific initialisation is performed and, for each frame, the segmentation starts from the mean model and uses a four-stage multi-resolution search strategy (Cootes et al., 1994).


                        Single oral+nasal – this alternative uses a single model trained using all the annotated images (oral and nasal). It does not include speaker specific initialisation and search is also performed using a multi-resolution strategy.


                        Oral – this alternative uses a single model trained using the images annotated with no nasal cavity (oral model). No speaker specific initialisation is performed and search is also performed using a multi-resolution strategy.


                        Nasal – this alternative is similar to the previous, but the model is trained using only the annotated images including the nasal cavity.

The initial mean model position for all alternatives was set to the one used previously for the initialisation of the proposed method.

The resulting segmentations were compared with the observer segmentations and Table 3
                         shows the overall comparison results for each of the alternatives.

The comparison values show that the proposed segmentation method provides the best results. Notice also that the single model trained considering both oral and nasal configurations yielded the poorest results. This is explained by the greater instability created by using a single model to deal with the different configurations at the velum and nasal cavity. Furthermore, the box plots presented show that the proposed method consistently obtains good results while the tested alternatives present more dispersion.

Our main goals, at this stage, did not consider any specific computational performance requirement other than allowing unsupervised segmentation in reasonable time (e.g., image data for one speaker processed overnight). To provide a reference against which to assess the computational performance of the proposed method we also measured segmentation times per image frame for the manual segmentations performed by the observers and for the other (more traditional) AAM alternatives tested in the previous section. The segmentations were performed on a quad-core i7-3520M CPU, 8GB RAM, running Matlab 2011.


                        Table 4
                         presents the different segmentation times measured. As expected, the proposed method performs a lot faster (3s/frame) than the manual segmentations (54s/frame) and, despite the added features, such as the automatic selection of the proper model to use (oral or nasal), it still performs better than the tested AAM alternatives (7s/frame), mostly due to fact that a single search stage, at full resolution, is performed.

@&#DISCUSSION@&#

In summary, the evaluation of different aspects of the proposed segmentation method provides evidence that:
                           
                              1.
                              The segmentations resulting from the proposed method show good accuracy when compared with segmentations performed manually by observers. Agreement, as measured by the Williams index, is stronger between the proposed method and the observers than among observers.

The choice of using two different models (oral and nasal), and initialisation of each frame segmentation with the segmentation of the previous frame, improved accuracy when compared with alternatives using individual image frame segmentation. This also reduced the number of search stages from four to a single stage performed at the highest resolution.

Positioning of the mean model, to build the speaker-specific initialisation, works well on a reasonably sized region. Nevertheless, as this varies from speaker to speaker, user supervision at this stage is still needed, to check if initialisation is done properly.

For the different speaker-specific initialisations possible, the resulting segmentations do not present much variability among them, with an exception to the first few frames of the image sequence where differences are more prominent. This is evidence that the only user dependant task, on our method, is prone to have little influence on the resulting segmentations.

Although not an important goal at this stage, the proposed method, beyond including automatic choice of the proper model (oral or nasal) to use, and being more accurate than the tested alternatives, also runs faster.

Considering the works discussed in the related work, no quantitative comparison with those methods is performed due to a lack of common image databases that could be used for that purpose. Note that all proposed methods address the problem dealing with databases containing images obtained using different acquisition protocols and frame rates, yielding very different image resolution and quality, which might influence the resulting segmentations. Nevertheless, based on what can be observed in the literature, for full vocal tract segmentations, when compared to Vasconcelos et al. (2011) and Raeesy et al. (2013), dealing with static images (expected to have better quality), our method provides better segmentations resulting in smoother contours, correctly covering all articulators. Compared to Bresch and Narayanan (2009), our approach provides similar results with the main difference of being applied in the image rather than the frequency domain and taking less time (Bresch and Narayanan (2009) report 20min/per frame). Furthermore, the works presented in the literature limit evaluation of the proposed methods to a qualitative evaluation (e.g., Bresch and Narayanan, 2009) or to the assessment of segmentation results in a very small number of images (Vasconcelos et al., 2011; Raeesy et al., 2013) with quantitative evaluations applied to no more than six images or using a single human observer as reference (Proctor et al., 2010).

@&#CONCLUSIONS@&#

In this paper we propose a model based method to perform the segmentation of the vocal tract from midsagittal RT-MRI image sequences. While segmentation of this kind of data has been previously addressed, that was performed without attending to the sequential nature of the data, i.e., by processing each image without accounting for its neighbours.

The proposed method allowed tackling the segmentation of a large RT-MRI database by building a model-based approach using a small set of annotated images. Although not specifically included in the training set, the vocal tract configurations along the sequences, such as those exhibiting tightly closed lips, were no particular challenge to the proposed method. This was possible by exploring the small inter-frame variability, in each image sequence, using the segmentation of one image as starting point for the next. Evaluation results, comparing the segmentations provided by the proposed method with those performed by four observers, show that the presented approach performs well, providing good levels of precision, accuracy and performance. Our approach, using two different models (nasal and oral) to address the different velum configurations (open and closed), allowed accurate segmentations that provide important extra data regarding velum aperture assessment, whether it is performed visually, by a phoneticist, or using computational tools. This is also a notable difference to the works of Proctor et al. (2010), Vasconcelos et al. (2011) and Raeesy et al. (2013). One of the main positive implications resulting from the proposed method is that it allows innovative approaches regarding automatic quantitative analysis of vocal tract data, as presented by the authors in Silva et al. (2013), considering the whole database available instead of a few chosen occurrences.

The image processing is performed sequentially, hindering parallel processing of the images inside a sequence which, as advocated by Bresch and Narayanan (2009), might improve performance on a cloud computing scenario. Nevertheless, we do not consider it a problem since parallel processing can still be performed for multiple sequences at once. The dominant factor in our database is not the size of each sequence, but their number. Besides, although, at this moment, we do not aim for a scenario where on-the-fly segmentation must be performed, the performance of the current implementation (in Matlab) is fast enough to provide, upon request, the segmentation of a particular image sequence (75 images) in less than four minutes.

Considering the different characteristics of the images gathered in RT-MRI studies of the upper airway, due to the different acquisition protocols used, the specific AAM models created for our database are most probably not directly applicable to other upper airway RT-MRI databases. Nevertheless, we consider that the proposed methodology is general enough to be used to deal with any upper airway RT-MRI database. The only requirement is that the models are retrained using a new training set, chosen according to the same criteria used here, i.e., selecting the frames for the most distinctive vocal tract configurations in the database. This, of course, is not limited to oral and nasal vowels as is the case of our database. For example, if an RT-MRI study of the upper airway is to include the articulation of lateral sounds (Teixeira et al., 2012) (e.g., /l/ as in “sal” (salt)), since the configuration of such sounds presents very distinctive characteristics on the range of movements of the tongue tip, when compared to vowels, frames showing that configuration should also be included in the training set.

@&#FUTURE WORK@&#

The difficulty of imaging the hard palate using MRI is well known. This results in considerable variability in identifying its contour. For the proposed method not much variability has been observed in this region. Nevertheless, as the hard palate is a rigid structure, it is expected that it generally stays the same for every image. One simple approach that can be used to minimise the variability is to choose the hard palate segment of one of the contours (e.g., the one in the speaker specific initialisation), for each speaker, and replace the hard palate in every segmentation with that segment (Proctor et al., 2010).

The addition of more speakers to the database was not tested, given the scarcity of speakers. Nevertheless, based on the obtained results, we consider that the proposed methodology can easily encompass additional speakers through the inclusion of a small set (no more than five per model) of annotated images of that speaker in the training set, chosen based on the same criteria. Enlarging the training set with data from additional speakers also iteratively improves the model in such a way that, in the future, it might be able to deal properly with new speakers without needing to be retrained. Although, at this moment, the training set is small and retraining the models can still be done in reasonable time, in the future, and to avoid completely retraining the models whenever a new speaker is added, incremental approaches for model evolution and learning might be considered (Sung and Kim, 2009; Chen et al., 2013).

Regarding the possibility that using one segmentation as starting point for the next might cause problems, if the segmentation starts to diverge, we did not observe any severe situation for our data. Nevertheless, in case this problem becomes an issue, for other datasets, there are several solutions that might be adopted. For example, since the speaker-specific initialisations provide a reference of vocal tract dimension and location of different regions of interest, it is possible to test for divergence and then act accordingly (e.g., reverting to the speaker-specific initialisations).

Contour self intersections were not addressed. As far as we could observe, self intersections do not happen often, and when they occur it is usually at the lips, when they are closed, and at the hard palate, when there is strong tongue contact. These do not pose critical problems for analysis and will be addressed in the future, for example, by post-processing the extracted contours.

Considering the work being carried out regarding speech production, a validated segmentation framework, as the one presented, providing complete segmentations of the vocal tract, is an important tool for the systematic study of speech production, not only on normal speakers, but also on those presenting pathologies such as a cleft palate, allowing our research work to follow that path.

@&#ACKNOWLEDGEMENTS@&#

The authors thank the observers involved in the evaluation study and the anonymous reviewers for their helpful comments and suggestions. Research partially funded by FEDER through the Program COMPETE and by National Funds (FCT) in the context of HERON II (PTDC/EEA-PLP/098298/2008), IEETA Research Unit funding FCOMP-01-0124-FEDER-022682 (FCT-PEst-C/EEI/UI0127/2011) and project Cloud Thinking (funded by the QREN Mais Centro program, ref. CENTRO-07-ST24-FEDER-002031).

@&#REFERENCES@&#

