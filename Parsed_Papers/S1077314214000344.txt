@&#MAIN-TITLE@&#Semantic video scene segmentation and transfer

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           New semi-supervised method to semantically segment a scene based on video activity.


                        
                        
                           
                           Learned functional categories are used to segment different scenes (scene transfer).


                        
                        
                           
                           We introduce new trajectory features useful for semantic segmentation.


                        
                        
                           
                           Only a small subset of relevant features leads to high classification accuracy.


                        
                        
                           
                           Proposed method achieves state-of-the-art scene classification and transfer results.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Video analysis

Semi-supervised learning

Scene segmentation

Functional modeling

@&#ABSTRACT@&#


               
               
                  In this paper we present a new approach to semantically segment a scene based on video activity and to transfer the semantic categories to other, different scenarios. In the proposed approach, a user annotates a few scenes by labeling each area with a functional category such as background, entry/exit, walking path, interest point. For each area, we calculate features derived from object tracks computed in real-time on hours of video. The characteristics of each functional area learned in the labeled training sequences are then used to classify regions in different scenarios. We demonstrate the proposed approach on several hours of three different indoor scenes, where we achieve state-of-the-art classification results.
               
            

@&#INTRODUCTION@&#

Semantic scene segmentation is the task of labeling areas of a scene according to their functional use. Modern surveillance and ambient intelligence systems increasingly exploit knowledge about the functional usage of the environment to improve their performance and provide more advanced functionalities. In this work, we present a new semantic scene segmentation solution targeted to a low cost, flexible system used in indoor environments with ceiling-mounted cameras. This scenario imposes a number challenging requirements that drive the main design choices of the proposed approach: limited cost of sensors, processing units and network infrastructure; compliance with privacy regulations; ease of deployment. To satisfy these requirements, the system uses low-cost cameras mounted on the ceiling, facing towards the floor and equipped with a wide angle lens, so that each node can cover as much surface as possible. Besides, no video streaming and centralized storage is allowed for both economic and privacy reasons. This implies that the video analysis algorithm has to work in real-time on a simple (embedded) platform. Finally, to simplify deployment, the system should limit or possibly avoid tuning of parameters for each installation.

An overview of the method proposed in this work is shown in Fig. 1
                     . First, we propose to adopt a simple and flexible real-time tracker and a series of post-processing steps to mitigate common tracking errors. The resulting trajectories are used to build features that are employed in a machine learning framework to classify areas into functional categories. A user annotates one or more scenes by labeling each area with a functional category such as background, entry/exit, walking path, interest point. For each area, we compute statistics of features derived from object tracks computed in real-time on hours of video, and select the most discriminative ones. The characteristics of each functional area learned in the labeled training sequences are then used to classify regions into functional categories both in the same scene or in different scenes.

One major contribution of this paper is the thorough analysis and selection of a large number of tracking features with different level of complexity and abstraction. We will show how feature selection is crucial to succeed in the scene categorization task. Another important contribution is the introduction of a semi-supervised approach to semantic video scene classification. To the best of our knowledge, such an approach has not been proposed before. We will demonstrate it on several hours of three different indoor scenes, where we achieve state-of-the-art results.

The remainder of the paper is organized as follows: in Section 2 we contextualize our proposed approach considering the most recent advancements in the field. In Section 3 we detail the proposed approach; the results and conclusions are presented in Section 4 and Section 5 respectively.

@&#RELATED WORKS@&#

Lately, functional scene segmentation has been approached by mainly two categories of methods: the first, builds upon algorithms of multi-target tracking (MTT), and adopts trajectories as input [1–6]. The second class avoids the complex task of MTT, and exploits low level features such as optical flow to directly learn scene topology and events [7–12].

The location at which people enter and exit a scene, named entry/exit or source and sink, is one of the most common information extracted by video analysis system based on MTT algorithms [1,2,5]. The knowledge of these locations can simplify the tracking problem, and is often used as input to handle appearance and disappearance of targets. Few methods look beyond recognition of entrances, exits, and typical paths, and attempt to segment the environment in regions according to more complex semantics [3,4,6]. In [3], the authors propose a semantic scene segmentation method that clusters scene regions based on the similarity of histograms of hierarchical, trajectory-level features. Oh et al. [4] instead, exploit high level scene context information, manually annotated for every scene, to automatically label trajectories with several features encoding the relationship between trajectories and contextual data. Fernández et al. [6] tackle the problem of semantic scene segmentation as a multi-class segmentation problem. Trajectories of moving objects are computed using the method in [13]. Each trajectory point is assigned four features, related to its movement (waiting or stopping), and type (pedestrian or vehicle). High level scene labels are modeled by defining class labels as conjunctions of required, forbidden, and irrelevant features. These constraints are embedded in a compatibility term, which takes care of modeling the probability for each cell to belong to each label. Segmentation of labels is finally obtained incorporating both terms into a statistical framework. These methods show impressive results, although they typically require complex object trackers and classifiers [3,6] and high level context information [4].

Methods relying on low level features such as optical flow [7–12], are motivated by the increased robustness over MTT-based approaches. While these approaches show remarkable results, they are typically suited for outdoor environments, with objects appearing at a relatively large distance from the camera. In these situations, each target generates a spatially consistent blob of motion, which can be well captured by optical flow. In indoor environments, the scope of our system, subjects moving in the field of view often exhibit incoherent motion, as limbs are visible and motion is articulated. In addition, because of the wide angle lens adopted in our system, the distortions at the periphery of the field of view would make the flow estimation in these regions inaccurate. In the same conditions, a tracker can be designed to follow objects with reasonable accuracy.

In this paper, we present a novel semi-supervised approach to semantically segment a scene based on video activity. While MTT algorithms have reached a high level of maturity and are deployed in many situations, they have characteristics that might not fit with requirements of our system. Most existing methods require the use of relatively complex tracking algorithms, with computational requirements unlikely to match the limited resources of low cost embedded platform. Secondly, advanced tracking algorithms that include object classification usually require to train object detectors and tune parameters to the specific dynamics of the scenario in which the system should be deployed, therefore increasing the cost and time needed in the deployment. In this work we want to use a generic real-time tracker that can be implemented in embedded platforms and does not need complex tuning.

More fundamentally, the majority of proposed systems [3,4,7–12] provide a segmentation of the environment in regions with a similar functional usage, but they do not provide a description of their semantic label, implying that a human operator would have to associate a functional tag to each label to be able to use the classification results. This process is cumbersome and little intuitive. In addition, a fair evaluation of system’s performance is difficult, as it strongly depends on this a posteriori labeling step. While it is difficult to avoid this manual annotation step, we argue that the annotation can be done before classification. If one wants to classify scene regions into a predefined set of semantic classes, we propose to exploit the human intervention by manually labeling a limited set of data and train a classifier based on the annotated data, rather than do unsupervised clustering and then label a posteriori the results, without being able to affect the classification result. The only method that does not require any training or manual annotation is presented in [6], where the authors propose a taxonomy to categorize semantic regions based on the type of objects and their activities. While the method’s performance is impressive, we decided to avoid such rule-based approach for two major reasons. Firstly, the quality of the results seems to critically depend on the ad hoc rules used to post-process the classification results. We prefer instead a data-driven approach, which is more flexible and easy to expand and improve. Furthermore, in [6] a specific tracker that classifies objects and categorizes activities is required. As already mentioned, we want to develop a flexible method that uses a generic real-time tracker.

The analysis presented in this paper is based on the output of a multi-target tracker (MTT). A large body of research has been devoted to the task of MTT. As our aim is to develop a system capable of being deployed to several indoor environments without any need of parameter tuning, we opted for a simplified tracking system, trading some degree of accuracy for lower complexity and broader applicability.

The tracking algorithm is composed of three stages. First, a detection stage generates new candidate target locations based on background modeling [14] and motion detection [15]. All candidate targets must be validated as humans. In [16], a simplified human model, based on a set of cylinders, is exploited to predict human shape for a generic camera installation. Our target shape model is inspired by that work, and approximates a human target with an elliptical shape, whose parameters are automatically computed based on the known camera field of view, and camera height from the floor. We adopted the fisheye lens model described in [17]. To validate a target, the center of mass of the blob is used to compute the parameters of the ellipse in the corresponding location, then the overlap between the targets binary mask and the ellipse gives an indication of the reliability of the target. As the aim of the tracker is not to detect and follow each subject in the scene, but to collect reliable trajectories, we apply a strict threshold to reject blobs due to groups, shadows, or partially occluded subjects. The resulting validated targets are forwarded to the data association stage where current tracking instances are updated, and the detected blobs are associated with them. Similarly to [18], we do not explicitly model merge and split events, but adopt a motion model and color histogram to match existing tracks with new detections. Differently from [18] though, in which Particle Filtering is used, we opt for a mean-shift algorithm [19]. All tracks for which the mean-shift tracker confidence is below a certain level are labeled as inactive and maintained for few frames. Finally, in the update stage the models of the active trackers are updated based on the associated targets, unassigned detections initiate new tracking instances, and inactive tracking instances are removed.

Given the simplicity of the tracking system, we observed that trajectories were often fragmented in the recorded sequences. This caused the average length of the trajectories to decrease, and the presence of trajectory starting and ending points spread across the complete scene. To reduce the issue, few pre-processing steps are applied. First, trajectories which end and start in a similar location within a short amount of time are connected. In the case of two or more candidates, the link which is minimizing the resulting trajectory local smoothness is selected. More sophisticated linking algorithms could be adopted, but we aim to keep complexity low to allow for a real-time overall system. In a second step, all trajectories which are shorter than a minimum value are removed, to reduce influence of possible false detections. Furthermore, to reduce position jitter introduced by segmentation inaccuracies, all trajectories are smoothed with a running window.

The three different environments in which the proposed method is tested and the trajectories corresponding to one of the recorded time slots are shown in Fig. 2
                        
                        .

While several methods apply learning directly to trajectory data [1,2], we further compact the available data by computing a set of features for each available video, similarly to [3,6]. The first step is the computation of feature maps, as described in this Section, which are then re-arranged into feature vectors for each image block, as detailed in Section 3.4.

We combine features inspired by existing work [1,3,6,20] with new features, as listed in Table 1
                        . Among the most popular features, source and sink carry high semantic value, as, in the case of an ideal tracker, are a one-to-one mapping to entry and exit points. We exploit the cumulative time, floor map, and floor map distance transform as a compact representation of the amount of activity in each location. Similarly, trajectory number, average dwelling time, and velocity can be correlated to either traffic or level of interest in an area. Less direct is the interpretation of velocity change, direction change, curvature, and displacement ratio, which can be seen as a measure of the complexity of the trajectory. An example of feature maps is shown in Fig. 4
                        .

Turek et al. [3] further adopt aspect ratio and size of tracked objects, useful in case in which different type of objects are present in the scene. As in our indoor scenario we focus solely on humans, we do not adopt those features. Fernández et al. [6] adopt higher level features, such as waiting, stopping, moving, and the notion of whether a track is corresponding to a pedestrian or vehicle. We avoid the use of features which require a higher semantic reasoning, such as the distinction between moving, stopping, and waiting, as we observed that the definition can be easily corrupted due to errors in the tracking algorithm when several objects are present in the scene at once.

Some of the redundancy in the features is due to the attempt of establishing which definition is the most suited to cope with high noise level of the input data. Cumulative time, average dwelling time, and average dwelling time histogram, for example, all refer to a similar notion, with increasing level of accuracy. Cumulative time cannot distinguish between two spatial bins with very different distribution of trajectories: a location for which a single trajectory would dwell for 10min would receive the same value as a location in which 5 trajectories would dwell for 1min and one trajectory for 5min. Similarly, average dwelling time, being derived from the cumulative time, would not be able to distinguish those statistics. This is the reason to adopt average dwelling time histogram. At the same time, the higher accuracy comes at the price of higher sensitivity to noise.

For all features in which a histogram is used, the details of the histogram are indicated in Table 1. Given each trajectory point, the feature value is computed, and the map corresponding to the respective histogram bin is incremented at the location of the current trajectory point. The velocity, velocity change, and the radius 
                           
                              
                                 
                                    R
                                 
                                 
                                    dwelling
                                 
                              
                           
                         used in the definition of the average dwelling time hist., are corrected for lens distortion, using the lens model described in [17].

Few steps are required to obtain feature vectors from the raw feature maps. First, we apply Gaussian smoothing with a given radius 
                           
                              
                                 
                                    R
                                 
                                 
                                    ds
                                 
                              
                           
                         to each map. In all experiments, video sequences are recorded at a resolution of 752×480 pixels and the values of the radius 
                           
                              
                                 
                                    R
                                 
                                 
                                    ds
                                 
                              
                           
                         range from 0 to 91 pixels. This allows each location to be influenced by neighboring data. Examples of the effect of different levels of smoothing applied to two feature maps are shown in Fig. 5
                        .

A second step attempts to reduce the influence of the number of trajectories and duration of a sequence on the feature value. As such distribution is vastly changing across different scenes, it is essential to adopt features which are as much as possible independent from this variation. To this aim, each feature map is locally normalized as follows: for all maps which accumulate a value for each trajectory point (
                           
                              E
                              ,
                              
                              F
                              ,
                              
                              G
                              ,
                              
                              H
                           
                        , and K), each point of the feature map is normalized by the corresponding value in the cumulative time map. For source, sink maps, as they accumulate a single value for each trajectory, each point of the feature map is normalized by the corresponding value in the trajectory number map. Finally, cumulative time is divided by the total number of frames in the sequence from which it is derived.

A third step involves collecting of normalized feature values into blocks. Specifically, the frame size is divided into equally large blocks,
                           1
                           We also experimented with blocks of constant area according to ground plane coordinates, but as blocks at the periphery are be modeled as extremely small, the features therein are subjected to high level of noise, due to the smaller amount of resolution available at the periphery. We therefore opted for a constant block size across the frame.
                        
                        
                           1
                         and the statistics indicated in the third column of Table 1 are computed for the group of values belonging to each block. After this step, each block is described by a feature vector, the length of which is indicated in the second column of Table 1, for each of the feature maps. Unless otherwise indicated, all results refer to block size of 40×40 pixels (with frame size of 752×480 pixels).

Lastly, whitening is applied for all feature vectors belonging to the same feature map and the same video sequence.

While several advanced multi-class classifiers have been successfully applied in recent years to solve complex problems (AdaBoost, Support Vector Machines, Random Forest), their application typically requires the availability of large data sets. As the amount of training samples is very limited (few tenths of samples for the class with fewest data points), and the number of dimensions comparable or larger than the number of samples, we opted for a K-Nearest Neighbor (KNN) classifier applied to the feature space obtained after Fisher’s Linear Discriminant Analysis (FDA). More specifically, given the subset of features under test, and the selected training data set, FDA is applied to reduce the initial higher dimensional space to three dimension, the maximum number of independent dimensions after FDA, given a four classes estimation problem [23]. An example of the distribution of training samples after FDA is shown on the left plot of Fig. 6
                        .

The classification method we selected does not enforce any spatial smoothness of the results, as it applies the same classifier independently to each block. While a certain degree of spatial consistency derives from the density smoothing step, we noticed the quality of the segmentation could be easily improved by a simple post-processing step.

To this aim, after obtaining a label for each image block of a test sequence, a separate binary mask is created for each of the four labels. Each of the non-zero blocks of those masks is then analyzed, and set to zero if no active neighbors are present, using four-direction neighborhood. The four processed binary masks are then combined to produce a new label image, in which blocks which do not directly receive a label are given the label of the majority of surrounding blocks.

In order to evaluate the versatility of the approach, we recorded several hours of video in three different environments, as shown in Fig. 2. The supermarket set, representing typical conditions of shops, subject to heavy traffic, long periods of no movement, and frequent occlusions, both between people, and due to the background structure. The hallway set, which combines areas of frequent movement, with sitting and standing tables, and a rotating door. Finally, the corridor set includes solely entry and exit points and passage areas.

The amount of data recorded for each environment is different: for the supermarket set, 5 sequences of 2h each; for the hallway set, 2 sequences of 3h each; for the corridor, 4 sequences of 2h each. The typical number of trajectories recorded in the sets are substantially different, ranging from less than 100 per hour for the corridor to more than 400 per hour for the supermarket set. As already mentioned, the diversity of the data composition is an essential aspect that our system aims at being able to cope with.

Each of the sequences are manually annotated, by assigning a single label for each block, as shown for some of the sequences on the rightmost column of Fig. 7
                        . We opted for 4 possible labels: entry/exit, describing areas in which people enter and exit the scene; interest point, in which people tend to reside for longer time; floor, all areas in which people transit; inactive areas which are typically not reached. Given the video resolution of 752×480 pixels, and the block size of 40 by 40 pixels, each sequence provides 228 data points. The amount of data points for each class is thus not balanced, ranging from few tenths of samples for interest point and entry/exit, to few hundreds for floor and inactive.

@&#RESULTS@&#

Several experiments were conducted to evaluate different aspects of the proposed methodology. The experiments can be separated into two classes: in intra scene experiments the training data is from the same environment as for the test sequence, albeit recorded at different time of the day. This enforces less demanding generalization properties, which we use to verify the best case performance of the method. We define scene transfer the experiments in which the training data is taken from other environments than the test sequence. In this case, the description of classes learnt in different environments is transferred to a new environment. Notice how this second case represents a more useful (and challenging) scenario, as it would allow to obtain labels in a newly installed environment without the need of any additional user intervention.

For both cases, we evaluate different combinations of features, the effect of feature map normalization, feature map density smoothing radius 
                        
                           
                              
                                 R
                              
                              
                                 ds
                              
                           
                        
                      (from 0 to 91 pixels), K neighbor number (from 10 to 90) used in the KNN classification, and label post-processing. In order to select the best performing method, we adopt a modified version of the segmentation accuracy (SA) [6], which weights the response of each class with the corresponding importance in the respective scene. This allows to account for unbalanced label distributions in the scene. More specifically, we define the modified segmentation accuracy as:
                        
                           
                              
                                 
                                    SA
                                 
                                 
                                    ∼
                                 
                              
                              =
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       c
                                       =
                                       1
                                    
                                    
                                       c
                                       =
                                       4
                                    
                                 
                              
                              
                                 
                                    TP
                                    (
                                    c
                                    )
                                 
                                 
                                    TP
                                    (
                                    c
                                    )
                                    +
                                    FP
                                    (
                                    c
                                    )
                                    +
                                    FN
                                    (
                                    c
                                    )
                                 
                              
                              ×
                              W
                              (
                              c
                              )
                              ,
                              
                              W
                              (
                              c
                              )
                              =
                              
                                 
                                    
                                       
                                          #
                                       
                                       
                                          samples
                                       
                                    
                                    (
                                    c
                                    )
                                 
                                 
                                    
                                       
                                          #
                                       
                                       
                                          B
                                       
                                    
                                 
                              
                           
                        
                     where 
                        
                           W
                           (
                           c
                           )
                        
                      is the sample distribution for each class c in the scene for which 
                        
                           
                              
                                 SA
                              
                              
                                 ∼
                              
                           
                        
                      is computed, and 
                        
                           
                              
                                 #
                              
                              
                                 B
                              
                           
                        
                      is the total number of blocks in a frame.

The classification of the best performing method for the intra scene experiment is shown in Fig. 7. The result is obtained with feature set 
                        
                           A
                           +
                           B
                           +
                           C
                           +
                           D
                           +
                           E
                        
                     , feature map normalization, 
                        
                           
                              
                                 R
                              
                              
                                 ds
                              
                           
                        
                      equal to 31, K equal to 10, and label post-processing. A qualitative evaluation of the result by visual inspection shows how the method is capable of exploiting noisy trajectories to derive the desired semantic labels. It is relevant to notice how, correctly, no interest point is detected in the corridor scene, and how all main interest point and entry/exit areas are recognized. Interestingly, only a subset of features is selected: we expected B to be a relevant feature, and 
                        
                           A
                           ,
                           
                           C
                        
                     , and D mainly relate to a rough indication of presence, while E might be exploited to discriminate between interest point and floor. Also, it appears that less detailed features, 
                        
                           A
                           ,
                           
                           C
                           ,
                           
                           D
                        
                     , are preferred over other features relating to similar properties but with higher level of detail, such as 
                        
                           J
                           ,
                           
                           K
                        
                     , and L. The effect of feature maps normalization is positive but minor, and the label post-processing indeed helps removing isolated spurious labels.

The best classification result for the scene transfer task is shown in Fig. 8
                     . The result is obtained with feature set 
                        
                           A
                           +
                           B
                           +
                           C
                           +
                           D
                           +
                           I
                        
                     , feature map normalization, 
                        
                           
                              
                                 R
                              
                              
                                 ds
                              
                           
                        
                      equal to 15, and K equal to 40, and label post-processing. As expected, the quality of the segmentation is lower. Particularly, two of the interest points in the supermarket scene are missed, as is one of the entry/exit areas in the corridor scene. A more subtle case is the rotating door in the hallway scene: it is recognized as an entry/exit, but also confused as interest point. This might be caused by people often queuing up in front of the door while waiting to exit the building, and also to the fact that the door often moving for some time after people leave the scene. Interestingly, in the intra scene task the same area was correctly classified, indicating that the availability of training data with similar characteristics would improve performance. From the point of view of features, the selected set is similar to one obtained in the intra scene task, with the difference that feature E is replaced by feature I. As feature I is a more complex feature if compared to E, exploiting several frames of the trajectory to derive its value, it seems to indicate that the scene transfer classification task requires more descriptive features to cope with the increased difficulty.

The effect of K in KNN classification on the segmentation accuracy 
                        
                           
                              
                                 SA
                              
                              
                                 ∼
                              
                           
                        
                      for the scene transfer case is shown in the left graph of Fig. 9
                     , from which it is clear that the performance degrades gracefully around the optimal value of 40. A similar plot showing the effect of density map smoothing radius in the right side of Fig. 9. An improvement of around 
                        
                           10
                           %
                        
                      is achieved thanks to the application of smoothing, comparing the performance with 
                        
                           
                              
                                 R
                              
                              
                                 ds
                              
                           
                           =
                           15
                        
                      to what achieved with 
                        
                           
                              
                                 R
                              
                              
                                 ds
                              
                           
                           =
                           0
                        
                     . Notice how performance substantially decreases for values larger than 51 pixels, implying that a too large radius of influence in the feature maps reduces the accuracy.


                     Fig. 10
                      illustrates the effect of the choice of feature sets for the scene transfer task. The graph shows the average modified segmentation accuracy for the best 20 results, obtained by maintaining constant all other parameters, while selecting different combinations of features. The importance of features 
                        
                           A
                           ,
                           
                           B
                           ,
                           
                           C
                        
                     , and D is clearly noticeable: 18 of the best 20 results include those 4 features. Also, the performance of the best performing feature set for the intra scene task, 
                        
                           A
                           +
                           B
                           +
                           C
                           +
                           D
                           +
                           E
                        
                     , appears only at 8-th place, preceded by features 
                        
                           I
                           ,
                           
                           J
                           ,
                           
                           H
                           ,
                           
                           L
                        
                     , and F, all more complex if compared to the velocity histogram E. This again illustrates how the scene transfer case requires more powerful features to resolve the classification problem. Notably, floor map C, its distance transform D and the displacement ratio I, introduced in this paper, perform best.

The segmentation accuracy for the case of scene transfer, using the best parameter combination, is reported in Table 2
                     . The visual evaluation of Fig. 8 is confirmed: feature map normalization (third column) improves average modified segmentation accuracy of nearly 
                        
                           5
                           %
                        
                     , while label post-processing (forth, rightmost column) adds more than 
                        
                           3
                           %
                        
                     . With an average segmentation accuracy of 
                        
                           58.7
                           %
                        
                      and an average modified segmentation accuracy of 
                        
                           64.7
                           %
                        
                      over the three scene, the proposed method outperforms results reported in [3,6]. A summary of the performances of the proposed approach, compared to those reported in [3,6], is shown in Table 3
                     .

In one scene transfer experiment between two urban street scenes, using five classes (Void, Doorway, Parking, Road, Sidewalk), the method in [3] achieves an average segmentation accuracy of 
                        
                           36.1
                           %
                        
                      and an average modified segmentation accuracy of 
                        
                           59.8
                           %
                        
                     . While the experiment might appear more challenging because of the higher number of classes, five versus four in our case, we would like to underline that the Doorway and Parking classes represent less than 
                        
                           2
                           %
                        
                      of all the blocks in the tested scene, and are always misclassified (hence the very low value of the average segmentation accuracy). At the same time, the labeling of the other three classes, Void, Road and Sidewalk, seems to be possible by simply using the trajectory type (vehicle or pedestrian), which is provided by the advanced trackers used in [3].

It is not possible to calculate the modified segmentation accuracy for the experiments in [6] with the data provided in the paper, but results are remarkable. The reported average segmentation accuracy with seven classes over five urban scenes is 
                        
                           55
                           %
                        
                     , which goes up to 
                        
                           80
                           %
                        
                      if the classes are reduced to three (Void, Road, Sidewalk). However, we would like to point out that the performances of the method seem to critically depend on ad hoc rules used to post-process the classification results, called geodesic interpolation in the paper. We believe that it is more fair and relevant to compare the results of our method with those obtained without the geodesic interpolation step, but using only a label post-processing step similar to the one used in our method. In this case the average classification accuracy in [6] drops to 
                        
                           46
                           %
                        
                      in the seven classes case and 
                        
                           56
                           %
                        
                      in the three classes case (as compared to 
                        
                           58.7
                           %
                        
                      in our four classes case). While a direct comparison with existing methods is difficult because of the different scenarios (outdoor in [3,6], indoor here), quantitative results demonstrate that the proposed method achieves promising scene classification and transfer results, which compare favorably to those obtained by existing solutions.

@&#CONCLUSIONS AND FUTURE WORK@&#

We have presented a novel approach to semantic scene segmentation, exploiting trajectories computed in real time by a low complexity tracker. The proposed method is capable of dealing with the tracking inaccuracies of the video tracker. The different stages of the proposed methodologies are thoroughly investigated, showing the effect of individual steps. Of particular importance is the analysis of the feature set combination, which demonstrates how the use of only a small subset of features can lead to high accuracy. The quality of the results shows how the possibility of achieving scene transfer is not only attractive, but feasible. Our approach allows to exploit one-time annotations applied to training environments, to learn the required segmentation models, and deploy the system in previously unseen environments. The proposed approach is demonstrated on three different indoor scenarios, where state-of-the-art classification and scene transfer results are achieved. Furthermore, as the method is training based, it would allow to exploit more powerful machine learning methods once the size of the training sets would become sufficiently large. For example, techniques such as AdaBoost or Random Forest, which have been successfully employed in numerous applications in recent years, would be interesting options to investigate in the proposed system.

A relevant open question is the influence of the tracking or motion feature extraction systems on the final scene segmentation results. To the best of our knowledge, this topic has not been addressed systematically in literature. We can expect that different motion features, ranging from simple optical flow up to the most advanced object trackers, will provide slightly different scene segmentation results. However, it is difficult to predict what is the combination of motion features and clustering techniques that can provide the best segmentation performances with the least overall complexity. We believe this is an interesting research question that we plan to address in future work.

@&#REFERENCES@&#

