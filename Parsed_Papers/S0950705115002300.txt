@&#MAIN-TITLE@&#Mining activation force defined dependency patterns for relation extraction

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A new trigger words identification method is defined based on word activation force.


                        
                        
                           
                           A unique representation of activation force defined dependency pattern is proposed.


                        
                        
                           
                           Two-level semantic drift suppressing strategy is adopted.


                        
                        
                           
                           Experimental result verifies the AFDDP is a good representation of relations.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Relation extraction

Pattern learning

Trigger word

Activation force

@&#ABSTRACT@&#


               
               
                  Relation extraction is essential for most text mining tasks. Existing approaches on relation extraction are generally based on bootstrapping methodology which implies semantic drift problem. This paper presents a new approach to learn semantic dependency patterns, which can significantly alleviate this problem. To this end, a unique representation of activation force defined dependency pattern is presented. It is a trigger word mediated relation between an entity and its attribute value, and the trigger word is extracted by using the statistics of word activation forces between those words. The adaptability and the scalability of the framework are facilitated by the recursive and compositional bootstrap learning of patterns and seed pairs. To obtain insights of the reliability and applicability of the method, we applied it to the English Slot Filling task of Knowledge Base Population track at Text Analysis Conference 2013. Experimental results show that the proposed method has good performance in the implementation of English Slot Filling 2013 with the overall F1 value significantly higher than the best automatic result reported. The experimental results also demonstrate that the activation force based trigger word mining method plays an essential role in improving the performance.
               
            

@&#INTRODUCTION@&#

Relation extraction is a very important task, which can potentially benefit a wide range of natural language processing (NLP) tasks such as question answering, ontology learning, and summarization [1].

In decades, many related tasks, e.g., Message Understanding Conference (MUC) [2], Automatic Content Extraction (ACE) [3], and Knowledge Base Population (KBP) [4], arose and facilitated the development of relation extraction technology. The English Slot Filling task (ESF) of KBP track in Text Analysis Conference (TAC), which involves mining information about entities from text, has been very attractive for its potential application in dealing with big data in the web. ESF systems determine from a large source collection of documents the values of specified attributes (‘slots’) of an entity, such as the age and the birthplace of a person or the top employees of a corporation [5]. Entities investigated in ESF are generally either a person or some type of organization. Extracting predefined attributes of such kind of entities is considered as a key technology to realize a practical system that can automatically mine the Web data.

Reliable relation extraction which can be used in the task mentioned above is still an open problem. One of the major challenges is how to learn selective patterns that have high coverage to represent relations [6]. In 2010, the overview of TAC-KBP slot filling showed that the bootstrapping system [7] proposed by NYU with F1 value 28.3% was much better than the CRF system [8] proposed by BUPT with F1 value 14.4%. In addition, the slot filling report [9] of CUNY in the same year also showed further experimental results that the system only using MaxEnt method gave the performance with F1 value 17.9%, which was much lower that the result of bootstrapping system. All this verified the bootstrapping method, as an effective method, is superior to the classical MaxEnt and CRF classifiers.

Recent years, more studies show that bootstrap learning methods [6,10–16] are capable for learning relation patterns starting from a small number of seed examples. But they are limited by inflexible pattern representation and semantic drift. Many patterns [6,10–12] using shallow syntactic features have poor performance in extracting relations that are ambiguous or lexically distant in their expression. Dependency patterns [17–21,15,22,16,23] have been shown to be better patterns, since they are more informative. Among these dependency patterns, the shortest dependency pattern (SDP) and the subject-verb-object (SVO) pattern are two mostly used ones [17,1,19,20]. However, because there is less semantic constraint, they gain the generality at the cost of lacking specific information and thus may produce semantic drift in bootstrapping iterations.

To address the problem of semantic drift, this paper presents an activation force based relation extraction model which can integrate the semantic constraints into patterns. This model adopts a unique representation of activation force defined dependency pattern (AFDDP). It is the shortest dependency path from an entity to its attribute value with a trigger word as the semantic anchor. The trigger word is extracted by using the statistics of word activation forces [24,25] between those words. Compared with the SDP and the SVO, the AFDDP can maintain necessary semantic information and largely suppress semantic drift of bootstrapping. During iterations, the AFDDP learning method evaluates qualities of these patterns and discards the unreliable ones.

Trigger words are words with semantic information. They activate patterns of specific relations and act as the patterns’ conceptual anchor points [26]. “John died of cancer” is an example sentence of the relation of person per:cause_of_death. Its dependency tree is shown in Fig. 1
                     . The sentence head is the main verb died which is modified by its nominal subject (nsubj) John and the of prepositional modifier (
                        
                           prep
                           _
                           of
                        
                     ) cancer. The main verb died is a trigger word of the relation per:cause_of_death and the trigger word based dependency pattern is obtained as 
                        
                           <
                           PER
                           >
                           nsubj
                           <
                           died
                           >
                           prep
                           _
                           of
                           <
                           disease
                           >
                        
                     . By matching the pattern, the entity-value pair 
                        
                           <
                        
                     John, Cancer
                        
                           >
                        
                      can be extracted from the sentence. Obviously, how well the trigger word based dependency patterns perform largely depends on how well the trigger words are identified. Unfortunately, existing trigger word identification methods are rigid and poorly portable because they are simply based on dictionary or term frequency [27,28,26,29–31]. To tackle this challenge, we proposed a unique trigger word identification method by using the statistics of word activation force, which have proved powerful in defining word relations in context [24,25]. Through word activation forces, we define a trigger force (TF) metric to identify trigger words. The TF of a word is predicted by a weighted sum of the activation force that an entity exerts on the trigger word and the activation force that the trigger word exerts on the attribute-value of the entity.

To obtain insights of reliability and applicability of this method, we applied it to the KBP-ESF task [32] to extract values of specified attributes (‘slots’) of entities. Experimental results show that we achieved a good performance with the overall precision of 46.55%, recall of 33.04%, and F1 value of 38.65%, which is significantly higher than the best automatic result reported in ESF2013.

The main contributions of this paper are as follows:
                        
                           (1)
                           We define a unique trigger force metric on trigger word identification, which identifies trigger words by using the statistics of word activation force.

A unique representation of activation force defined dependency pattern is proposed. It is a trigger word mediated relation between an entity and its attribute value. They form the shortest dependency path with a trigger word as the semantic anchor. Compared with the SDP and the SVO pattern, the new pattern is more relation oriented, and thus more noisy robust.

Two-level semantic drift suppressing strategies are adopted. At the first level, we set trigger word as the semantic anchor of a pattern to maintain the pattern’s semantic information. At the second level, we used a specialized pattern evaluation method to select the most relation revealing patterns during iterations.

The rest of this paper is organized as follows: Section 2 reviews related work. Section 3 describes activation force defined dependency patterns via trigger words. Section 4 presents the algorithm of mining activation force defined dependency patterns in large scale text. The experimental results are shown and discussed in Section 5. Finally, we conclude the paper in Section 6.

@&#RELATED WORK@&#

WordNet [33] has played an important role in trigger words extraction methods. For example, the predicates of WordNet were used as trigger words to establish relations between words and concepts of a language independent ontology [27]. A trigger word list was gathered from WordNet by checking whether a word had the semantic class “person
                           
                              |
                           
                        …
                           
                              |
                           
                        relative” to personal social relation subtypes [28]. In practice, these dictionary dependent methods generally have poor domain portability.

Alternatively, event trigger word extraction was defined as a multiclass classification problem in [26]. The trigger word extraction method [29] of social tag computed the trigger power of a word by TF-IRFw, TextRank or their product methods in a given web source. The generative model for relation extraction method [30] extracted trigger words from function words on all dependency paths. The method of learning patterns for a particular domain [31] chose the most frequent words as trigger words and clustered the rules with the same trigger words. These methods extract trigger words by simply counting frequencies of words, usually suffering from lack of syntactic and semantic constraints.

A number of machine learning approaches have recently been applied in relation extraction. One is the use of bootstrapping to learn relation patterns from a small number of seed examples [10,6,11–13,15,16].

The Snowball system [6], Question and Answer system [11], and Espresso system [12] made use of named entity tags, surface strings, and their linear orders as components in the pattern representation. These methods are limited by the use of shallow syntactic features and rigid patterns. Therefore, they are unsuitable for recognizing relations expressed via nonlocal linguistic constructions.

Alternative approaches have also been suggested in [18,17,19–21,15,16,23] for generating patterns from dependency tree, each of which allows a particular part of the dependency analysis to act as an extraction pattern. Compared with shallow feature patterns, dependency patterns can extract relations located in long complex sentences more precisely [1,34]. Among these dependency patterns, the SDP and the SVO pattern are two popular used patterns. The SDP used by [17,35,36] was defined as the shortest dependency path between two relation oriented entities in the same sentence. It offers a very condensed representation of the information need to assess relations[17]. Methods proposed in [19,20] used the basic patterns that are defined as a verb and its direct subject and/or object (SVO). The SVO pattern is suitable for extracting complex relations or events at the scenario template level [19]. However, because there is no semantic constraint, these two patterns gain the generality at the cost of lacking specific information and thus may produce semantic drift in bootstrapping iterations.

Once we move away from traditional dependency patterns, the issue of representation of dependency patterns with semantic information becomes pertinent. An ideal relation pattern should be abstract over surface word orders and can mirror semantic relations as clearly as possible. To this end, it is necessary to incorporate trigger word information with syntactic information contained in traditional dependency patterns. In this section, we introduce our activation force defined dependency patterns, which is a dependency pattern with a trigger word as the semantic anchor.

Guo et al. [24] proposed a unique link-weighting scheme, namely the activation force. It determines the strength of the links of complex networks according to the conditions of the node occurrences.

The word activation force, WAF, reveals various types of associations between words [25]. It regards the activation effect that a word exerts on another as an imaginary force. Given the frequencies 
                           
                              
                                 
                                    f
                                 
                                 
                                    i
                                 
                              
                           
                         and 
                           
                              
                                 
                                    f
                                 
                                 
                                    j
                                 
                              
                           
                        , and co-occurrence frequency 
                           
                              
                                 
                                    f
                                 
                                 
                                    ij
                                 
                              
                           
                         of a pair of words i and j, the activation force from word i to word j is defined as
                           
                              (1)
                              
                                 
                                    
                                       a
                                    
                                    
                                       ij
                                    
                                 
                                 =
                                 (
                                 
                                    
                                       f
                                    
                                    
                                       ij
                                    
                                 
                                 /
                                 
                                    
                                       f
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 (
                                 
                                    
                                       f
                                    
                                    
                                       ij
                                    
                                 
                                 /
                                 
                                    
                                       f
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 /
                                 (
                                 
                                    
                                       d
                                    
                                    
                                       ij
                                    
                                    
                                       2
                                    
                                 
                                 )
                              
                           
                        where 
                           
                              
                                 
                                    d
                                 
                                 
                                    ij
                                 
                              
                           
                         is the average distance by which word i precedes word j in their co-occurrences within a consecutive word sequence.

The WAF predicts not only the semantic activation effects, but also the syntactic and grammatical activation effects. It is verified that the activation force approach is superior in facilitating the analysis through experiments on a large-scale word network [25].

Based on the definition of activation force, for a sentence containing an entity, its attribute-value, and a trigger word (Fig. 1), we can estimate the activation force that the entity exerts on the trigger word, and the activation force that the trigger word exerts on the attribute-value. With these activation forces, we define Trigger Force (TF) of the trigger word as a criterion for trigger word identification.

Given a training data set, for entity e, candidate trigger word 
                           
                              
                                 
                                    w
                                 
                                 
                                    i
                                 
                              
                           
                        , and attribute-value v, we estimate the activation force that e exerts on 
                           
                              
                                 
                                    w
                                 
                                 
                                    i
                                 
                              
                           
                         and 
                           
                              
                                 
                                    w
                                 
                                 
                                    i
                                 
                              
                           
                         exerts on v by using the basic statistics of the data as follows,
                           
                              (2)
                              
                                 af
                                 (
                                 e
                                 ,
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 =
                                 (
                                 
                                    
                                       f
                                    
                                    
                                       e
                                       ,
                                       
                                          
                                             w
                                          
                                          
                                             i
                                          
                                       
                                    
                                 
                                 /
                                 
                                    
                                       f
                                    
                                    
                                       e
                                    
                                 
                                 )
                                 (
                                 
                                    
                                       f
                                    
                                    
                                       e
                                       ,
                                       
                                          
                                             w
                                          
                                          
                                             i
                                          
                                       
                                    
                                 
                                 /
                                 
                                    
                                       f
                                    
                                    
                                       
                                          
                                             w
                                          
                                          
                                             i
                                          
                                       
                                    
                                 
                                 )
                                 /
                                 
                                    
                                       d
                                    
                                    
                                       e
                                       ,
                                       
                                          
                                             w
                                          
                                          
                                             i
                                          
                                       
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        
                        
                           
                              (3)
                              
                                 af
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 v
                                 )
                                 =
                                 (
                                 
                                    
                                       f
                                    
                                    
                                       
                                          
                                             w
                                          
                                          
                                             i
                                          
                                       
                                       ,
                                       v
                                    
                                 
                                 /
                                 
                                    
                                       f
                                    
                                    
                                       
                                          
                                             w
                                          
                                          
                                             i
                                          
                                       
                                    
                                 
                                 )
                                 (
                                 
                                    
                                       f
                                    
                                    
                                       
                                          
                                             w
                                          
                                          
                                             i
                                          
                                       
                                       .
                                       v
                                    
                                 
                                 /
                                 
                                    
                                       f
                                    
                                    
                                       v
                                    
                                 
                                 )
                                 /
                                 
                                    
                                       d
                                    
                                    
                                       
                                          
                                             w
                                          
                                          
                                             i
                                          
                                       
                                       ,
                                       v
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    f
                                 
                                 
                                    e
                                 
                              
                              ,
                              
                                 
                                    f
                                 
                                 
                                    
                                       
                                          w
                                       
                                       
                                          i
                                       
                                    
                                 
                              
                              ,
                              
                                 
                                    f
                                 
                                 
                                    v
                                 
                              
                           
                         are the occurrence frequencies of e, 
                           
                              
                                 
                                    w
                                 
                                 
                                    i
                                 
                              
                           
                        , and v, respectively. 
                           
                              
                                 
                                    f
                                 
                                 
                                    e
                                    ,
                                    
                                       
                                          w
                                       
                                       
                                          i
                                       
                                    
                                 
                              
                           
                         is the co-occurrence frequency of e and followed by 
                           
                              
                                 
                                    w
                                 
                                 
                                    i
                                 
                              
                           
                         within L words. 
                           
                              
                                 
                                    f
                                 
                                 
                                    
                                       
                                          w
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    v
                                 
                              
                           
                         is the co-occurrence frequency of 
                           
                              
                                 
                                    w
                                 
                                 
                                    i
                                 
                              
                           
                         followed by v within L words. 
                           
                              
                                 
                                    d
                                 
                                 
                                    e
                                    ,
                                    
                                       
                                          w
                                       
                                       
                                          i
                                       
                                    
                                 
                              
                           
                         is the average distance between e and 
                           
                              
                                 
                                    w
                                 
                                 
                                    i
                                 
                              
                           
                         in their co-occurrences, and 
                           
                              
                                 
                                    d
                                 
                                 
                                    
                                       
                                          w
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    v
                                 
                              
                           
                         is the average distance between 
                           
                              
                                 
                                    w
                                 
                                 
                                    i
                                 
                              
                           
                         and v in their co-occurrence.

Here, the length of L is used to define the window width of identifying a co-occurrence of words, it is set according to specific applications. Word frequencies are counted under the condition without stemming verbs or changing nouns between plural and singular forms but with changing all upper cases into lower cases. For example, die, dies, died, dead, death terms are treated as 5 different words, but President and president are considered as the same word.

Based on activation forces among entities, words, and attribute-values, we define the TF of a trigger word as [37]:
                           
                              (4)
                              
                                 Trigger
                                 _
                                 Force
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 =
                                 λ
                                 af
                                 (
                                 e
                                 ,
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 +
                                 (
                                 1
                                 -
                                 λ
                                 )
                                 af
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 v
                                 )
                              
                           
                        where 
                           
                              λ
                           
                         is a parameter falls in the interval to [0,1]. It is used to adjust the importance of 
                           
                              af
                              (
                              e
                              ,
                              
                                 
                                    w
                                 
                                 
                                    i
                                 
                              
                              )
                           
                         and 
                           
                              af
                              (
                              
                                 
                                    w
                                 
                                 
                                    i
                                 
                              
                              ,
                              v
                              )
                           
                        .

According to this definition, the magnitude of TF is unitarily quantified in [0, 1].

Based on the TF of each word, we identify trigger words by setting a threshold of trigger force 
                           
                              
                                 
                                    tf
                                 
                                 
                                    0
                                 
                              
                           
                        . If 
                           
                              trigger
                              _
                              force
                              (
                              
                                 
                                    w
                                 
                                 
                                    i
                                 
                              
                              )
                              ⩾
                              
                                 
                                    tf
                                 
                                 
                                    0
                                 
                              
                           
                        , treat word 
                           
                              
                                 
                                    w
                                 
                                 
                                    i
                                 
                              
                           
                         as a trigger word.

After getting the trigger word set of each relation, we define a trigger word anchored pattern with high coverage to detect predefined relations and extract entity-value pairs in text. We assume that the instances containing similar relations share similar substructures in their dependency trees [18] by defining a new relation representation, activation force defined dependency pattern (AFDDP). The AFDDP is the shortest dependency path from entity to attribute-value with a trigger word as the semantic anchor. To present the AFDDP, we will first introduce the shortest dependency path.

A dependency path is represented as a sequence of words interspersed with arrows that indicate the orientation of each dependency, as illustrated in Fig. 2
                           . The SDP method assumes that the contribution of the sentence dependency graph to establishing a relationship is almost exclusively concentrated in the shortest dependency path between an entity and its attribute value [17].

For each entity-value pair in a relation mentioning sentence, the shortest dependency path form entity e to attribute value v is defined as:
                              
                                 (5)
                                 
                                    ShortestPath
                                    (
                                    e
                                    ,
                                    v
                                    )
                                    =
                                    {
                                    tag
                                    (
                                    e
                                    )
                                    ,
                                    
                                       
                                          dep
                                       
                                       
                                          1
                                       
                                    
                                    ,
                                    ⋯
                                    
                                       
                                          v
                                       
                                       
                                          i
                                       
                                    
                                    ⋯
                                    ,
                                    
                                       
                                          dep
                                       
                                       
                                          n
                                       
                                    
                                    ,
                                    tag
                                    (
                                    v
                                    )
                                    }
                                 
                              
                           where 
                              
                                 
                                    
                                       v
                                    
                                    
                                       i
                                    
                                 
                              
                            is node i in dependency tree, 
                              
                                 
                                    
                                       dep
                                    
                                    
                                       i
                                    
                                 
                              
                            is the dependency of two nodes. For a lexical chunk c, tag is a tag label function, which is
                              
                                 
                                    tag
                                    (
                                    c
                                    )
                                    ∈
                                    {
                                    PER
                                    ,
                                    ORG
                                    ,
                                    LOC
                                    ,
                                    DATE
                                    ⋯
                                    }
                                 
                              
                           
                        

The tag of chunk is relation dependent, for example, tags of entity-value pairs of org:location_of_headquarters are ORG and LOC, respectively; tags of entity-value pairs of per:date_of_birth are PER and DATE, respectively.

We use the Stanford CoreNLP [38] to create the dependencies in the “collapsedDependency” format. As noted in [39], this collapsed format often yields simplified patterns which are useful for relation extraction. For the example sentence in Fig. 2, the Stanford CoreNLP gave the following dependencies:
                              
                                 
                                    
                                    
                                    
                                       
                                          
                                             
                                                
                                                   root
                                                   (
                                                   ROOT
                                                   ,
                                                   independent
                                                   )
                                                
                                             
                                          
                                          (main verb: independent)
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   det
                                                   (
                                                   Group
                                                   ,
                                                   The
                                                   )
                                                
                                             
                                          
                                          (det: determiner)
                                       
                                       
                                          
                                             
                                                
                                                   nn
                                                   (
                                                   Group
                                                   ,
                                                   International
                                                   )
                                                
                                             
                                          
                                          (nn: noun compound modifier)
                                       
                                       
                                          
                                             
                                                
                                                   nn
                                                   (
                                                   Group
                                                   ,
                                                   Crisis
                                                   )
                                                
                                             
                                          
                                          
                                       
                                       
                                          
                                             
                                                
                                                   nsubjpass
                                                   (
                                                   independent
                                                   ,
                                                   Group
                                                   )
                                                
                                             
                                          
                                          (nsubjpass: passive nominal subject)
                                       
                                       
                                          
                                             
                                                
                                                   partmod
                                                   (
                                                   Group
                                                   ,
                                                   based
                                                   )
                                                
                                             
                                          
                                          (partmod: participial modifier)
                                       
                                       
                                          
                                             
                                                
                                                   prep
                                                   _
                                                   in
                                                   (
                                                   based
                                                   ,
                                                   Brussels
                                                   )
                                                
                                             
                                          
                                          (prep_in: in prepositional modifier)
                                       
                                       
                                          
                                             
                                                
                                                   auxpass
                                                   (
                                                   independent
                                                   ,
                                                   is
                                                   )
                                                
                                             
                                          
                                          (auxpass: passive auxiliary)
                                       
                                       
                                          
                                             
                                                
                                                   advmod
                                                   (
                                                   respected
                                                   ,
                                                   widely
                                                   )
                                                
                                             
                                          
                                          (advmod: adverbial modifier)
                                       
                                       
                                          
                                             
                                                
                                                   conj
                                                   _
                                                   and
                                                   (
                                                   independent
                                                   ,
                                                   respected
                                                   )
                                                
                                             
                                          
                                          (conj_and: conjunct and)
                                       
                                    
                                 
                              
                           where each atomic formula represents a binary dependence from dependent token to the governor token. These dependencies form a directed graph, 
                              
                                 <
                                 V
                                 ,
                                 E
                                 >
                              
                           , where V is the vertex set of tokens, and E is the edge set of dependencies.

For any pair of tokens of the sentence in Fig. 2, such as Group and Brussels, the shortest dependency path is (shown in Fig. 2):
                              
                                 
                                    ShortestPath
                                    (
                                    Group
                                    ,
                                    Brussels
                                    )
                                    =
                                    {
                                    <
                                    ORG
                                    >
                                    ,
                                    partmod
                                    ,
                                    based
                                    ,
                                    prep
                                    _
                                    in
                                    ,
                                    <
                                    LOC
                                    >
                                    }
                                 
                              
                           
                        

The shortest dependency path between an entity and its attribute value offers a condensed representation of the path. However, because there is less semantic constraint, the SDP is likely to lead to semantic drift in bootstrapping iterations.

To maintain the semantic information of the shortest dependency path, we constrain each shortest dependency path with a trigger word as the semantic node. After obtaining the dependency tree of a relation mentioning sentence, we represent the corresponding sentence by a shortest dependency path with a trigger word as the semantic anchor, which is so-called activation force defined dependency pattern (AFDDP).

For each entity-value pair (e, v) in a relation mentioning sentence, the AFDDP is defined as:
                              
                                 (6)
                                 
                                    AFDDP
                                    (
                                    e
                                    ,
                                    v
                                    )
                                    =
                                    ShortestPath
                                    (
                                    e
                                    ,
                                    
                                       
                                          w
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    +
                                    ShortestPath
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    v
                                    )
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          i
                                       
                                    
                                    ∈
                                    
                                       
                                          W
                                       
                                       
                                          s
                                       
                                    
                                    ∩
                                    
                                       
                                          W
                                       
                                       
                                          tw
                                       
                                    
                                    )
                                 
                              
                           where 
                              
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                              
                            is one of trigger words in the intersection of word set of sentence 
                              
                                 
                                    
                                       W
                                    
                                    
                                       s
                                    
                                 
                              
                            and trigger word set 
                              
                                 
                                    
                                       W
                                    
                                    
                                       tw
                                    
                                 
                              
                           .

As shown in Fig. 2, the node based is one of the trigger words of relation org:location_of_headquarters. With the node based, we can ensure the semantic relation between the two entities. Table 1
                            shows example AFDDPs of some relations in ESF task.

After generating a number of patterns from the initial seed pairs, the system scans available documents to search sentences of text that match the patterns. The Match function is defined as:
                           
                              (7)
                              
                                 Match
                                 (
                                 
                                    
                                       s
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       s
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   1
                                                
                                                
                                                   if
                                                   :
                                                   AFDDP
                                                   (
                                                   
                                                      
                                                         s
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   )
                                                   =
                                                   AFDDP
                                                   (
                                                   
                                                      
                                                         s
                                                      
                                                      
                                                         j
                                                      
                                                   
                                                   )
                                                
                                             
                                             
                                                
                                                   0
                                                
                                                
                                                   otherwise
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

As a result of this process, the system generates new entity-value pairs. Because the number of the seed pairs is limited, patterns generated from the training sentences are insufficient to cover the variety of the relation instances. To increase the extraction recall, we use a pattern bootstrapping learning method to enrich the pattern set. This bootstrapping framework is similar to the Snowball system described in [6] except that we use the deep syntactic features patterns with semantic relation information rather than the shallow feature patterns. After several iterations of bootstrapping, this system can generate much more patterns and then these patterns can be applied to extract additional entity-value pairs. The bootstrapping framework for pattern generation using seeds and trigger words is shown in Fig. 3
                        .

The evaluation of the reliability of patterns and entity-value pairs is challenging. The AFDDP representation uses the trigger words to constrain the semantic information of patterns, which can be thought as a semantic drift suppression strategy. Specially, a semantic drift analysis algorithm (SDA) [40] is used to minimize semantic drift in the generations of both entity-value pairs and patterns. Here, we improve the SDA algorithm for the evaluation of the reliability of patterns and entity-value pairs.

After finding some candidate patterns, they are ranked according to:
                           
                              (8)
                              
                                 Conf
                                 (
                                 
                                    
                                       p
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             k
                                          
                                          
                                             |
                                             
                                                
                                                   S
                                                
                                                
                                                   ′
                                                
                                             
                                             |
                                          
                                       
                                       [
                                       
                                          
                                             s
                                          
                                          
                                             k
                                          
                                       
                                       =
                                       
                                          
                                             s
                                          
                                          
                                             k
                                          
                                          
                                             ′
                                          
                                       
                                       ]
                                    
                                    
                                       |
                                       
                                          
                                             S
                                          
                                          
                                             ′
                                          
                                       
                                       |
                                    
                                 
                                 log
                                 (
                                 |
                                 
                                    
                                       S
                                    
                                    
                                       ′
                                    
                                 
                                 |
                                 )
                                 ×
                                 100
                                 %
                              
                           
                        where 
                           
                              [
                              
                                 
                                    s
                                 
                                 
                                    k
                                 
                              
                              =
                              
                                 
                                    s
                                 
                                 
                                    k
                                 
                                 
                                    ′
                                 
                              
                              ]
                           
                         is the Iverson notation that equals 1 if the seed 
                           
                              
                                 
                                    s
                                 
                                 
                                    k
                                 
                              
                           
                         of last iteration matches the extracted seed 
                           
                              
                                 
                                    s
                                 
                                 
                                    k
                                 
                                 
                                    ′
                                 
                              
                           
                        . Thus, the sum in the numerator is the number of correct pairs of the extracted seed set.

Estimation of the reliability of an entity-value pair is similar to the evaluation of the reliability of a pattern. Intuitively, a reliable pair is the one that highly associated with as many reliable patterns as possible. Hence, similar to our pattern reliability measure, we define the reliability of a seed pair as
                           
                              (9)
                              
                                 Conf
                                 (
                                 
                                    
                                       s
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             k
                                          
                                          
                                             |
                                             
                                                
                                                   P
                                                
                                                
                                                   ′
                                                
                                             
                                             |
                                          
                                       
                                       [
                                       
                                          
                                             p
                                          
                                          
                                             k
                                          
                                       
                                       =
                                       
                                          
                                             p
                                          
                                          
                                             k
                                          
                                          
                                             ′
                                          
                                       
                                       ]
                                    
                                    
                                       |
                                       
                                          
                                             P
                                          
                                          
                                             ′
                                          
                                       
                                       |
                                    
                                 
                                 log
                                 (
                                 |
                                 
                                    
                                       P
                                    
                                    
                                       ′
                                    
                                 
                                 |
                                 )
                                 ×
                                 100
                                 %
                              
                           
                        
                     

Among the patterns and seed pairs evaluated in each iteration, we use the measures of (9) and (10) to identify the reliable patterns and seed pairs for the next iteration to extract additional patterns and seed pairs.

@&#EXPERIMENTS@&#

We applied the AFDDP learning model into the KBP-ESF task of TAC. The training and testing data are formed by query relevant documents from the ESF corpus, which contains long complex sentences. The ESF corpus includes 2.3 million news docs and 1.5 million Web and other type’s docs from 2009 to 2012, and includes 1 million documents from Gigaword, 1 million web documents, and about 100,000 documents from web discussion fora in 2013. For each relation, we used all correct ESF entity-value pairs extracted from assessments results from 2009 to 2012 [41] as the seed pairs. Afterward, we matched sentences in training data to form the seed sentences. Query entities of ESF2013 were used as testing entities and assessments results of ESF2013 [42] were set as the standard answer. By choosing the top 50 relevant documents of each entity, we formed the training data with 13,900 documents. Similarly, the testing data was formed with 5000 documents. The numbers of entities and the total slots of the training and the testing data are listed in Table 2
                     .

To get statistical information of all candidate trigger words and entity-value pairs, we implemented a special processing procedure on seed sentences in the training data (an example is shown in Table 3
                           ) [37]:
                              
                                 (1)
                                 We replaced all the entity-value pairs in the corresponding sentences with the sign of E and V respectively.

We assumed that the trigger words of a specific relation are all between entities and their attribute-values in the corresponding sentences.

@&#RESULTS@&#

We carried out experiments to investigate the impact of different 
                              
                                 λ
                              
                            in the formula of TF on the performance by changing it from 0 to 1, with a step of 0.1. Fig. 4
                            presents impact of parameter 
                              
                                 λ
                              
                            for PER relation per:employee_of, per:cause_of_death, and per:charges. In addition, to guarantee the recalls, if a word mentions any relation of an entity, we judge the word as a candidate trigger word correspondingly without handling ambiguities.

From Fig. 4, we can observe that the precisions of all three relations increase before 
                              
                                 λ
                              
                            reaches 0.5, and decrease after 
                              
                                 λ
                                 =
                                 0.5
                              
                           . The results show that an even consideration of the importance of the two terms of TF is suitable for getting a good performance. Therefore, the following experiments are conducted with 
                              
                                 λ
                                 =
                                 0.5
                              
                           .

In order to investigate the statistical feature of the TF, we ordered the TF of trigger words with respect to some predefined relations in the ESF. With this operation, we found a good nature of the TF: the TF of trigger words related to a relation are very selective, forming a sharply skewed distribution over the trigger words. The major amount of all the forces is highly dependent on very few words. These words have close syntactic or semantic associations with the specific relation. Fig. 5
                            shows the TF distributions of 6 PER relations [37].

As shown in Fig. 5, the TF values descend sharply along with the ordered trigger word identifiers at the beginning, and then maintain an obvious long-tail nature (power-law-like distribution). The top 10 trigger words of PER relations per:alternate_name, per:cause_of_death, per:school_attend and per:spouse are shown in Fig. 6
                           . The visible trigger words distribution (the weighted links are higher than the threshold (1.0e–3)) of the PER relation 
                              
                                 per
                                 :
                                 alternate
                                 _
                                 name
                              
                            is shown in Fig. 7
                           . From Fig. 7, we can be observed that, in addition to high frequency words, some low frequency words also have higher TFs, such as byname, early, picked, initials, and nickname.

All these facts suggest that one can acquire the most forceful trigger words of a relation just by glancing at the top part of its TF which has been sorted in a descending order.

Compared with ESF2012, the 2013 task, with more complex queries and more uniform slot distribution, was much harder [43]. The distribution of the slots in evaluating the queries of ESF2013 is shown in Fig. 8
                        . From this figure, we can see that the top 13 slots covers 62% of data. And our experiments focused on these slots.

By using our pattern learning model, we learned AFDDPs from the training data and applied these AFDDPs on 5000 relevant docs of 2013 entities to extract their attributes defined in ESF task of KBP. Note that, we just ran three bootstrapping iterations due to the high computational cost. And we learned 494 relation patterns of person entity and 246 relation patterns of organization entity in all. To evaluate this learning, we measured the performance of the slot filling on the top 13 relations respectively, and got a result shown in Fig. 9
                        .

From Fig. 9, we can see that, our method has a good performance on precision with the average precision of 62.8% and the best precision up to 96.3%. Especially, the precisions of per:age, per:date_of_death, per:cause_of_death, and per:charges are all above 80%, which is significantly higher than the state-of-art methods investigated in this paper. The reason for the high precision is that the dependency path with a trigger word as the semantic anchor is a strong constraint for each relation, which largely improves the precision. We also noted that the performance for the slot of org:top_numbers_employees is largely lower than others, this is because fewer useful trigger words were founded in the training data.

In the experiment, our run is implemented with a bootstrapping framework of the Snowball system. It leverages relation-independent extraction patterns to automatically generate seeds of the next iteration, and then these seeds are fed into a pattern-learning component. The patterns and seed pairs used in the experiment are evaluated by the SDA method described in Section 4.2.


                        Table 4
                         lists 5 references for our method performance investigation: the human manual performance provided by the Linguistic Data Consortium (LDC), the result of DS-SVM method, the result of DS-MIML-RE method, the result of BLPs method, and the median automatic result reported by ESF2013 [43]. The DS-SVM method [44], which obtained the best result among the officially released ones, uses the distant supervision SVM classifier to extract attribute values of entities. The DS-MIML-RE method [45] makes use of a distantly supervised approach, implementing the multi-instance multi-label system (MIML-RE) [46]. The BLPs method [47] infers probabilistic rules in the form of Bayesian Logic Programs (BLPs), which is a statistical relational learning framework based on directed graphical models. From Table 4, it can be observed that our AFDDP method obtained significant improvement compared with the DS-MIML-RE method and the BLPs method in all the three measurements. Further more, the overall F1 value of our AFDDP method is significantly higher than the DS-SVM method, the best automatic result reported in English Slot Filling 2013. All comparisons mentioned above strongly demonstrate the effectiveness of our AFDDP method. Note that results of the DS-SVM and DS-MIML-RE method are above the median value and the BLPs result is below the median value.

For more insight on the method, we also compared it with the other two methods. First, we implemented the trigger word mining method [30] which chose the most frequent function words of dependency path as trigger words. We labeled the method as TWSDP. From Table 4, we can observe that the performance of our method is superior to the TWSDP method in all the three measurements.

Second, we implemented the SDP learning method. In this implementation, we chose the shortest dependency path between an entity and its attribute value as the relation representation. Table 4 shows that our AFDDP method is significantly superior to the SDP method in terms of precision and F1. This suggests that the AFDDP method can maintain more semantic constraints during the iterative process for more relation patterns without semantic drift. On the other hand, we also note that the strict semantic constraints can cause a loss of recall, because the SDP showed a significant higher measurement on that.

To show the effectiveness of our AFDDP, we compared it with the bag-of-words pattern mentioned in the Snowball system [6]. The bag-of-words pattern adopted words and dependencies as features. We labeled it as BoWVEC [48]. The similarities of two BoWVEC patterns were measured by cosine similarity measurement. In addition, to explore the similarity measurement of patterns, we also adopted dependency kernel method which was proposed in [18] to measure two patterns’ similarity. We labeled it as DPK. Table 5
                         shows the average results of five relations of PER entity: per:title, per:employee_member_of, per:cities_of_residence, per:age, and per:children. Note that, we did not use co-reference features in this comparisons due to the time complexity feature of co-reference analysis. From Table 5, we can see that even though the AFDDP and the BoWVEC use the same features, our AFDDP reaches good performance at a compensation of slight drop of recall due to the strict semantic constraint of trigger words. Because of the flexible similarity method used by DPK, our AFDDP has little lower F1 compared with the DPK method. It proves that the performance of AFDDP method can be improved by improving the similarity methods. Thus, we can improve the similarity method of our AFDDP method in the feature work.

In addition, a follow-up of our current work have been published [49],
                           1
                           This paper was submitted after the current paper, but published earlier than the current paper.
                        
                        
                           1
                         in which we trained a generative activation force model to extract trigger words. The G-AF method assumes that words which have large frequencies in relation mentioning corpus are more likely to be a trigger word compared to other words which have low frequencies. Hence, it combines these posteriori probability and features used in the current paper, and trained a generative activation force model (G-AF). The G-AF method is more comprehensive than the trigger force method proposed in the current paper. To compare the performance of the current method and the method proposed in paper [49], we applied the current method to the dataset used in paper [49] with the same preprocessing. We labeled the pattern proposed in paper [49] as GAFMSDP. The experimental results are shown in Table 6
                        .

From Table 6, we can see that, compared with GAFMSDP method, the AFDDP method has relative low performance, especially the precision measurement. This means that the semantic constraint of GAFMSDP is more effective than that of AFDDP. Since the two patterns are both using trigger words as the semantic anchor, we can conclude that our follow-up formulated trigger words mining method proposed in paper [49] is more effective than the current mining method.

@&#CONCLUSIONS@&#

This paper presented an activation force defined dependency pattern learning method for relation extraction. This method extracted trigger words by investigating the activation force between trigger words and entity-value pairs of a specific relation and integrated trigger words into the shortest paths as the semantic anchors to form the AFDDP patterns. We applied the proposed method to the KBP-ESF to extract predefined attribute values of some entities. Experimental results show that our method performs better than the best automatic result reported in ESF2013. Further experiments showed the superiority of our method over some commonly applied methods. The significant improvement on the measurement of precision suggests that the proposed method can maintain more semantic constraints during the iteration of bootstrapping for patterns’ expansion and the semantic drift suppression.

Aside from the interesting research on pattern representation and trigger words mining of relations, we intended to optimize the pattern evaluation method since we were using the simplest evaluation method. For the trigger word extraction, we did not perform the threshold optimization, leaving room for further improvement. In this study, we adopted the absolute match with the trigger words, which may lead to the relative low recall. To solve this problem, it is necessary to investigate a more flexible and effective methods to match the trigger words in the future work.

@&#ACKNOWLEDGMENTS@&#

This work is partly supported by National Natural Science Foundation of China (NSFC) grant No. 61402047, 61511130081, and 61273217, Scientific Research Foundation for Returned Scholars, Ministry of Education of China, Sweden STINT initiation grant Dnr. IB2015-5959, the Research Fund for Excellent Young and Middle-aged Scientists of Shandong Province (No. BS2013DX035), Chinese 111 program of Advanced Intelligence and Network Service under grant No. B08004, and EU FP7 IRSES MobileCloud Project (Grant No. 612212).

@&#REFERENCES@&#

