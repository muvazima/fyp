@&#MAIN-TITLE@&#Voice data mining for laryngeal pathology assessment

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           We examined pathological changes in the speech signal of vowels /a/, /i/ and /u/.


                        
                        
                           
                           Various methods of voice signal analysis were examined to detect voice pathologies.


                        
                        
                           
                           Selected features have the influence on detection accuracies of pathological voice.


                        
                        
                           
                           100% voice pathology detection is achievable using Random Forest algorithm.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Voice pathology detection

Feature selection

PCA

kPCA

Random forest

Acoustic analysis

@&#ABSTRACT@&#


               
               
                  The aim of this study was to evaluate the usefulness of different methods of speech signal analysis in the detection of voice pathologies. Firstly, an initial vector was created consisting of 28 parameters extracted from time, frequency and cepstral domain describing the human voice signal based on the analysis of sustained vowels /a/, /i/ and /u/ all at high, low and normal pitch. Afterwards we used a linear feature extraction technique (principal component analysis), which enabled a reduction in the number of parameters and choose the most effective acoustic features describing the speech signal. We have also performed non-linear data transformation which was calculated using kernel principal components. The results of the presented methods for normal and pathological cases will be revealed and discussed in this paper. The initial and extracted feature vectors were classified using the k-means clustering and the random forest classifier. We found that reasonably good classification accuracies could be achieved by selecting appropriate features. We obtained accuracies of up to 100% for classification of healthy versus pathology voice using random forest classification for female and male recordings. These results may assist in the feature development of automated detection systems for diagnosis of patients with symptoms of pathological voice.
               
            

@&#INTRODUCTION@&#

Voice and speech play an important and influential role in many professions requiring irreproachable pronunciation. It is also the easiest means of interpersonal communication. It is said that 87.5% of inhabitants in urban areas require vocal communication for their daily work [1]. Speech disorders may lead to unintelligible communication and misunderstandings. A patient׳s voice is an important source of information which is obtained non-invasively and is used for monitoring the status of the larynx. Acoustic analysis provides a physical description of the waveforms generated and emitted by the voice organ and correlates well with the phoniatric state for proper and pathological voice emission. Further audio signal processing fosters the development of increasing measurable characteristics of human voices and allows highly accurate, objective evaluation of voice and speech disorders. The patient׳s voice quality can be assessed using sustained vowel phonations and/or conversional speech. Sustained vowels are particularly useful because they circumvent linguistic artifacts and are considered sufficient for many voice assessment applications [2].

For the purpose of this paper we created a vector containing 28 parameters describing 900 female and 510 male voice recordings of sustained vowels /a/, /i/ and /u/. Half of those patients constituted a group with pathological voice symptoms and half represented healthy patients. With the use of principal component analysis we extracted the most informative principal components from the initial 28 feature vector having 90% variance. Using initial and extracted feature vectors we used the k-means clustering and random forest classification and validated this with 10-fold cross-validation. The general view on all the steps taken in this research is shown in Fig. 1
                     . In this paper, we focus on developing an efficient method for automatic voice pathology detection. We used 3 vowels /a/, /i/ and /u/ from Saarbruecken Voice Database in our experiments and extracted a set of features that were used to automatically detect the voice impairments. We implemented several classification techniques including a RF classifier which performed best and yielded 100% accuracy for /a/ characterized by kernel principal component analysis. There is no significant difference between the pathology detection accuracies of analyzed vowels using RF classifier. The rest of the paper is organized as follows: we introduce the background in Section 1 and related work in Section 2, then we go into the description of the voice database taken into the examination (Section 3), how the feature set describing the signal was constituted (Section 4), afterwards we present the classification methods (Section 5). The experiments are described in Section 6, while the discussion and conclusion of the gathered results can be found in Section 7.

@&#RELATED WORK@&#

The automated acoustic analysis of voice is increasingly used for the detection and screening of laryngeal pathologies. Fidelity recording and processing the digital audio signal motivate researchers for the further development of measurable characteristics (for different subjective characteristics) of human voices and provide good accuracy, objective assessment of discrete, unobtrusive overflow methods, voice and speech disorders [3].

In the literature there are many acoustic features that have been analyzed, with a different focus, to determine the particular characteristics of the voice. There are already several methods proposed in the literature that rely on signal statistics, including cycle-to-cycle variations in the time domain [4]. Calculations based on fundamental frequency, evaluation of the period to period variability of the pitch period (Jitter coefficient) and their statistics are the basic features, that have been employed in the analysis and pathological voice assessment. Frequency perturbation, amplitude modulation (Shimmer coefficients), have been utilized in the analysis and measurement of voice quality [5–8]. The detection of vocal fold pathology typically considers the excitation of the signal [9]. Measures based on voice perturbation have become widely available through commercial voice analysis systems [10–12]. The measurement of these perturbations is limited to the fundamental frequency or peak amplitude. These features include specific characteristics of voice signal over a long period of time i.e. static features. On the other hand, dynamic features (short-term measures) are more informative about acoustic correlates of perceptual dimensions of voice quality, which are useful for the diagnosis of different diseases [13]. Dynamic features present changes in the temporal structure of the excitation signal, while static features remove all temporal dependency. The non-parametric approaches are based on the magnitude spectrum of speech, where short-term mel frequency cepstral coefficients (MFCC) are widely used [14–16]. The analysis of those features, mentioned above, can be reliably employed for a large scale, rapid assessment of normal and pathological voices [17].

The problem of voice pathology detection from speech signals still exist. The aim of the research is to find a reliable and non-invasive tool to support clinicians to determine the voice conditions of patients. However, no universal solution has yet been found. Most of the features and algorithms are trained with limited databases, including only a few types of disorders which are classified from normal signals. In [18] the proposed method shows 100% accuracy using the Kay Elemetrics database with 281 patients. Eadi et al. present 100% accuracy in voice pathology detection using long-term average spectral measures, glottal noise measures and measures based on linear prediction modelling and conditional logistic regression analysis [19]. This experiment involved only 24 patients. The authors of [20] show how phase-based features can automatically detect voice disorders and achieved 95.92% accuracy when testing the sustained vowels of 710 patients. The paper of [21] shows an accuracy of 95%±3.54% using six features describing nonlinear dynamics, which was tested on 396 recordings of the “rainbow passage” from the Kay Elemetrics database. Akbari et al. achieved an average classification accuracy of 96.67% and 97.33% for the structure composed of wavelet packet-based energy and entropy features, using 258 data samples including normal voices and speech signals impaired by three sorts of disorders [22]. Jothilakshmi published results of 95.74% efficiency accuracy of the automatic system to detect the type of voice pathology using mel-frequency cepstral coefficients (MFCC) and linear prediction cepstral coefficients using 363 speech samples of the sustained vowel /a/ [23]. The article [24] presents the pathological voice recognition for vocal fold disease using MFCC and Gaussian mixture model testing 60s of speech, for each of 30 patients. As a result they achieved up to 98% accuracy. The publication [25] shows 91% accuracy, the papers [26,27] present 100% accuracy for the detection of Unilateral Vocal Fold Paralysis and both included up to 67 patients. Also [28] revealed 100% recognition accuracy in the case of well manifested pathologies and 96.1% recognition accuracy in the case of weakly manifested pathologies using a database of 744 patients from which 638 had various functional and organic larynx disorders. The paper written by Petrovic-Lazic et al. shows that jitter, shimmer, fundamental frequency variation, voice turbulence index, putch perturbation quotient, amplitude perturbation quotient and NHR values significantly differentiate patients with vocal fold polyps from a control group without any pathological changes in the larynx [29]. The paper [2] presents 90.7% of accuracy in the voice pathology detection using mel-frequency cepstral coefficients, harmonic-to-noise ratio, normalized noise energy and glottal-to-noise excitation ratio classified with Gaussian mixture models using the Saarbruecken Voice Database (SVD). Other researchers used a score level fusion to detect pathological voice using SVD database too and achieved 94.93% of detection accuracy of pathological voices and 95% of healthy ones [12]. 100 samples for pathological and normal subjects of vowel /a/ were used to analyze the performance of the proposed methodology. All experiments were performed using 10-folds with different numbers of Gaussian and frame length achieving a maximum of 72% accuracy [30].

The accuracy of automatic pathological voice detection is influenced by the underlying variability of the speech (different language, pathologies). What is more, combining acoustical signal and imaging of the vocal tract (kymography and high-speed digital sigmoidoscopy (HSV)) could enable a clinician to record the oscillating vocal folds in real time during phonation [31]. In recent years, machine learning and data mining techniques have brought flourishing applications [32]. These powerful tools have been utilized and have lead to better performance in pathological voice detection [33]. The idea of using RFs for voice pathology detection came from the success of RFs in classification and regression [34,35].

Voice samples of sustained vowels /a/, /i/ and /u/ were digitally recorded and published online in the Saarbruecken Voice Database (SVD) by the Institute of Phonetics of the University of Saarland [36]. SVD consists of voice recordings from more than 2000 people. Each of the patient׳s voices were recorded while the phonation of the vowels /a/, /i/ and /u/ were at high, low, low-high and normal pitch. The pitches are described and divided into three categories that were available to choose from. The length of the recordings ranges from 1 to 4s. The audio format is wav (16 bit samples) sampled at 50kHz. The whole database contains recordings of healthy patients and 71 different well-defined voice pathologies. In this paper we present the work done using 450 healthy and 450 pathological female recordings of /a/, /i/, /u/ vowel at high, low and normal pitch (167 suffered from hyperfunctional dysphonia, 139 had vocal cord paresis, 144 suffered from other pathologies listed in the database [36]) and 255 healthy and 255 pathological male recordings of the same vowels at high, low and normal pitch (46 men suffered from hyperfunctional dysphonia, 74 suffered from vocal cord paresis, 83 experienced laryngitis of which some also had leukoplakia, 52 suffered from other pathologies listed in the database [36]). Due to the essential differences in voice behavior between men and women, parameters were statistically analyzed for females and males separately.

In the first stage, the focus was placed on the initial speech signal transformation to obtain a set of parameters, whose values are the basis for the diagnosis of the patient. Registration itself and its pre-processing is not fully sufficient in the identification and evaluation of voice pathology deformation. Therefore, it is necessary to devise and describe phonetic recordings using a set of parameters, which are then arranged in an appropriate design – the feature vector. This vector is used to conduct speech deformation analysis. Such analysis can be the basis for the assessment of pathological changes. Acoustic signal analysis was carried out to determine 28 parameters: fundamental frequency, jitter and shimmer coefficients, energy, 0-, 1-, 2-, 3-order moment, kurtosis, power factor, 1-, 2- and 3-formants amplitude, 1-, 2- and 3-formants frequency, maximum and minimum value of the signal and 10 mel-frequency ceptrum coefficients (MFCC). The explanation of those parameters and their equations can be found in previous studies [37]. At this stage, the speech data set is represented by a set of high-dimensional speech features. Before classification, each feature was pre-processed by performing normalization.

Since the vector was composed of a plurality of parameters, it was important to use a method that would extract those containing the most valuable pieces of speech information. In order to reduce the number of parameters, it was firstly necessary to organize their characteristics according to their discriminative ability and as a consequence to obtain stable and consistent results. For the feature selection principal component analysis (PCA) was used [32]. Accordingly, the principal component analysis was used as a pretreatment step prior to further data analysis and to reduce the parameters to speed up and simplify subsequent calculations and to verify the quality of the algorithm.

As the result of the PCA 28 principal components were obtained, ordered from first main component, which contains the largest part of the variance to the 28th principal component, containing the smallest part of the variance. The presentation of the first three principal components of the vowel /a/ at high pitch for female is shown in Fig. 2
                     . It presents the fact that the data having 48.03% of variance from healthy and pathology recordings still interpenetrate each other at this stage. Furthermore, by adding more principal components and having a larger number of variance the data can be separated more clearly. We have experimentally chosen the number of principal components which covered more than 90% of the variance of the initial parameters. 90% of the variance included all of the signal information needed to achieve satisfactory results for every vowel and their pitches. The exact numbers of extracted principal components for each vowel and its pitch are presented in Table 1
                     .

The nonlinear generalization of PCA is kernel PCA (kPCA). It performs PCA in feature spaces of arbitrarily large (possibly infinite) dimensionality [38]. The main advantage of kernel PCA is that no nonlinear optimization is involved [38]. The nonlinear transformation, which is in use by kernel PCA, makes it possible to map the input space in a feature space via a non-linear map and afterwards compute the principal components in new feature space. It is possible that kPCA computes the principal components in higher-dimensional feature space, which is nonlinearly related to the original input dataset [32]. The algorithm of kPCA implemented in this work was formed by [39]. An important parameter that must be set in kernel PCA is σ. In order to separate different classes in the new feature space, the parameter σ should be smaller than inter-class distances and larger than inner-class distances in the domain parameters [39].

For the purpose of this work the σ parameter was chosen experimentally analyzing each vowel and each pitch for females and males separately. The mean accuracy was chosen as a selection criterion from a function depending on the number of features. The vowel /a/ was chosen due to the highest mean accuracy among other results. The results for vowel /a/ for women and men are presented in Fig. 3
                     . The data was not dependent because the calculation was done based on data which was not involved in the database used to validate the algorithms. For women the highest mean accuracy was achieved for σ=19.5 and for men σ=17.5 for 28 features. The Gaussian kernel was used for calculations of kPCA.

As the result of kPCA we achieved 28 principal components for all analyzed groups, transfer to a larger number of dimensions did not bring improvement of the classification.

The first method used for classification was the k-means clustering, which is a method commonly used to automatically partition a data set into k-groups [40]. The idea behind this algorithm is to select k initial cluster centers and then iteratively refine them. The results from the k-means algorithm were then validated with a 10-fold cross-validation.

The second method used for classification was random forest (RF). The idea behind RF is to combine many binary decision trees, which are built using different bootstrap samples of the original data and random subsets to obtain an accurate predictor. The advantage of RF is that it is robust against over-fitting as more trees are added and by this it produces a limiting value of the generalization error [41]. Having a training set X containing n observations and p features, RF trees are built as follows:
                        
                           •
                           From the original set p choose the number of trees to grow t and the subspace size. In this way you choose the number of features to provide for each tree.

In each node of the tree, the division is done by being drawn without replacement m from among p features, then the next node k among the m features, etc. 
                                 (
                                 k
                                 ⪡
                                 m
                                 ⪡
                                 p
                                 )
                              .

The tree building process without cutting continues, if possible, until the elements in the leaves are only from one class.

The observation vector is classified by all the trees and eventually classified in the class they occurred most frequently. For items which were not drawn from the original set, each ith element is subjected to classification by a tree, in which the building did not participate. Such an element is then assigned to the class, which was mostly achieved (in this way all the elements are classified from the original set). The RF algorithm in this study was used from [35]. The data that was not used to construct the trees is used to estimate the out of bag error (OOB) of the grown trees. For each of the trees the OOB error frequency is computed and the combination of parameters, which show the lowest error is selected. At the level of train data the data set is used to grow single trees and feature the following selection: the random selection of m candidate predictors considered at each split (m=
                        
                           
                              [
                           
                        
                        ]
                        p
                     , where p is the initial number of features); grow tree: the data is split using the best predictors; an estimation of the OOB error: is done by applying the tree to the OOB data; random forest as the final step includes all of the trees.

The number of trees in the RF classifier used for the calculations presented in this paper was set to t=500 and we used the recommendations from [41] for the size of the random feature subspace for the decision model 
                        (
                        m
                        =
                        
                           
                              [
                           
                        
                        ]
                        p
                        )
                     . The number of candidate predictors was set to 5 and P-value threshold was at the level of 0,95.

@&#EXPERIMENTS@&#

To evaluate the classification quality of the k-means algorithm we used the k-fold cross-validation, where k was equal to 10. The database was randomly partitioned into 10 equal in size subsamples. A single subsample constituted the validation data for testing and 
                        k
                        −
                        1
                      subsamples were used as a training data. The cross-validation process was repeated 10 times. Each of the k subsamples was used exactly once as the validation data. The k results from the folds were then averaged to produce a single estimation. All observations were used for both training and validation, each observation was used for validation exactly once [42].

Cross-validation was done separately for each vowel and for each pitch for male and female separately. The results of cross-validation show the accuracy of the classification and are expressed as the percentage of parameters from the test set that were correctly assigned to their respective group and obtained at the stage of validation (1)
                     
                        
                           (1)
                           
                              ACC
                              =
                              
                                 
                                    
                                       1
                                    
                                    
                                       k
                                    
                                 
                              
                              
                                 
                                    ∑
                                 
                                 
                                    i
                                    =
                                    1
                                 
                                 
                                    k
                                 
                              
                              
                                 
                                    TH
                                    +
                                    TP
                                 
                                 
                                    N
                                 
                              
                           
                        
                     where k is the number of subsets equal to 10 (10-fold cross validation), TH and TP are the number of cases correctly classified as healthy and pathological voices, respectively. N is the total number of cases taken into examination. The final accuracy for each vowel and its pitch is the average accuracy from 10-fold cross-validation.

The results of cross-validation for 28 initial parameters and the PCA and kPCA results for each vowel at each pitch are presented in Table 2
                     .

Using the random forest classifier we evaluated the predictive quality of 28 parameters, the reduced number of principal components and kPCA results separately and reported the accuracy of the proper classification. The RF classification, which is based on different numbers of principal components for female and male, is shown in Figs. 4 and 5
                     
                     .

We ran the k-means clustering with 10-fold cross-validation 100 times to measure the dispersion of the pathology detection accuracy. The dispersion was expressed as standard deviation (SD) and was calculated for each vowel and pitches separately. All of the results of SD did not exceed the degree of statistical significant difference, which is 5%.

Results in Table 2 show that the validation of PCA using k-means clustering and cross-validation loses 10% signal information (having 90% of variance) from the initial feature vector and gave worse results than the analysis based on the 28 initial feature vector. Comparing the results from Table 2 for females we came to the conclusion that the most accurate indications of a patient׳s healthy condition and a pathological condition were given by the analysis based on kPCA including all the analyzed pitches. The analogous analysis of male recordings showed 100% accuracy for 28 feature vectors as well as for the appropriate given number (Table 1) of principal components for each vowel at each pitch and kPCA results.

As a result of the RF classification, an accuracy of 100% was obtained for the unseen data in the two-class (healthy-pathology) tasks using 28 principal components and a specified number of principal components covering 90% of variance. Perfect separation (100% accuracy) was obtained between healthy and pathological cases when analyzing the human voices using PCA and kPCA analysis for the vowels /a/ and /i/ (Table 3
                     ). The vowel /u/ did not achieve 100% for all female recordings analysis based on 28 parameters and PCA. The kPCA analysis ensured 100% of classification.

It is seen that the k-means algorithm ensured perfect data separation for male recordings, what is the opposite to female׳s analysis based on 28 parameters and PCA. The kPCA analysis, which is non-linear data transformation, coped with this problem and achieved 99% of classification accuracy. This means that it was not sufficient to separate the data in linear way. What is more, k-means algorithm is presented as assigning objects to the nearest cluster by the distance. Because of this we have calculated the dispersion in each machine learning methodology and group (healthy vs. pathology) for all the vowels. By the dispersion we measured the Euclidean distance (ED) between the centroids and standard deviation (STD) for each group. Afterwards we calculated the ratio between each STD and ED. As the final result we summed up the mean value between healthy and pathology for each machine learning analysis and gender. Table 3 shows results for vowel /u/ and it is seen that the dispersion for female is bigger than for male. The same trend is seen for all analyzed vowels. It might be the reason, why the classification failed for this vowel for female analysis (Table 4
                     ).

@&#DISCUSSION AND CONCLUSION@&#

Quality of life may depend on the state of our vocal system, especially when it is one of the main mean of communication in our daily life. In the literature, there are many types of voice features that have been proposed. Therefore, most speech processing techniques concentrate on discovering new features that would cover more information. Gathering all the information from many features together might be a big challenge to extract useful information about a patient׳s state. We examined different classification approaches, which would uniquely separate a class of healthy and pathological patients based on multi-dimensional feature vector describing human voice.

This paper presents the effective, consist and reliable assessment of wide variety of voice pathologies reflecting vocal fold and vocal tract disorders. We present the method that applied to 1410 patients, of whom half were healthy. The database included 705 pathological patients (female and male) suffering from 71 different diseases. The data set containing 28 parameters was reduced and the results were compared to the initial data. The reduction involves leaving a minimum of 90% of the variance of the initial date set. The presented features are capable of capturing specific information relevant for discrimination between healthy and pathological classes. As the result, they performed well during the classification procedure. Finally we achieved 100% accuracy for all the analyzed material using the random forest classification in the examination, whereas the k-means algorithm for female analysis gave much worse results. This means that the proposed system can diagnose many kinds of vocal abnormalities with various states of disease progression and be a primary tool to detect possible voice diseases in patients being examined for screening tests of the voice. From the papers presenting different approaches to detect voice impairments [2,12,30] based on SVD, the highest obtained accuracy amounts to 94.93%. Our obtained accuracy is 100%, which is the highest accuracy of voice pathology detection using SVD. This result is very encouraging and it is expected that the developed tools will be of great help to preventative health care in laryngology.

None declared.

@&#ACKNOWLEDGMENTS@&#

This work was funded by the Ministry of Science and Higher Education in Poland under the Diamond Grant program, Decision number 0136/DIA/2013/42 (AGH 68.68.120.364).

@&#REFERENCES@&#

