@&#MAIN-TITLE@&#Eye detection using discriminatory Haar features and a new efficient SVM

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A discriminating feature extraction (DFE) method for two-class problems is proposed.


                        
                        
                           
                           The DFE method is applied to derive the discriminatory Haar features (DHFs) for eye detection.


                        
                        
                           
                           An efficient support vector machine (eSVM) is proposed to improve the efficiency of the SVM.


                        
                        
                           
                           An accurate and efficient eye detection method is presented using the DHFs and the eSVM.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Discriminatory feature extraction (DFE)

Discriminatory Haar features (DHFs)

Efficient support vector machine (eSVM)

Eye detection

Fisher linear discriminant (FLD)

Principal component analysis (PCA)

Face Recognition Grand Challenge (FRGC)

BioID database

@&#ABSTRACT@&#


               
               
                  This paper presents an accurate and efficient eye detection method using the discriminatory Haar features (DHFs) and a new efficient support vector machine (eSVM). The DHFs are extracted by applying a discriminating feature extraction (DFE) method to the 2D Haar wavelet transform. The DFE method is capable of extracting multiple discriminatory features for two-class problems based on two novel measure vectors and a new criterion in the whitened principal component analysis (PCA) space. The eSVM significantly improves the computational efficiency upon the conventional SVM for eye detection without sacrificing the generalization performance. Experiments on the Face Recognition Grand Challenge (FRGC) database and the BioID face database show that (i) the DHFs exhibit promising classification capability for eye detection problem; (ii) the eSVM runs much faster than the conventional SVM; and (iii) the proposed eye detection method achieves near real-time eye detection speed and better eye detection performance than some state-of-the-art eye detection methods.
               
            

@&#INTRODUCTION@&#

Accurate and efficient eye detection is important for building a fully automatic face recognition system [1–4], and the challenges for finding a robust solution to this problem have attracted much attention in the pattern recognition community [5–19]. Example challenges in accurate and efficient eye detection include large variations in image illumination, skin color (white, yellow, and black), facial expression (eyes open, partially open, or closed), as well as scale and orientation. Additional challenges include eye occlusion caused by eye glasses or long hair, and the red eye effect due to the photographic effect. All these challenge factors increase the difficulty of accurate and efficient eye detection.

We present in this paper an accurate and efficient eye detection method using the discriminatory Haar features (DHFs) and a new efficient support vector machine (eSVM). The DHFs are extracted by applying a discriminating feature extraction (DFE) method to the 2D Haar wavelet transform [20–22]. The DFE method improves upon the popular principal component analysis (PCA) and Fisher linear discriminant (FLD) methods. The PCA method is capable of extracting the optimal features for signal or image representation in the sense of mean square error [23]. However, it does not extract discriminatory features for classification [24]. An alternative to the PCA method is the FLD method, which extracts the optimal features for classification by optimizing a criterion on scatter matrices [23]. However, a significant disadvantage of the FLD method is that the maximum number of features it can derive does not exceed the number of classes minus one [23,25]. For a two-class pattern classification problem, the FLD method thus can derive at most one feature, which is usually inadequate for achieving satisfactory classification performance, especially when the problem becomes complex, such as the eye detection problem. The DFE method, based on two novel measure vectors and a new criterion, is capable of extracting multiple discriminatory features for eye detection.

The eSVM method is proposed to address the inefficiency problem of the conventional support vector machine (SVM) for eye detection. Since it was introduced, SVM has become a popular tool for two-class classification problems [26–31]. When the classification problem becomes complex, the conventional SVM tends to generate a large number of support vectors, which subsequently leads to the increase of the model complexity. As a result, SVM becomes less efficient due to the expensive computational cost of the decision function, which involves an inner product of all the support vectors for the linear SVM and a kernel computation of all the support vectors for the kernel SVM. The eSVM, by contrast, significantly reduces the number of support vectors by applying only a single slack variable. In addition, a Θ set is introduced into the definition of the eSVM, which consists of the training samples on the wrong side of their margin derived from the conventional SVM. The Θ set plays an important role in controlling the generalization performance. The eSVM therefore improves the computational efficiency upon the conventional SVM without sacrificing the generalization performance.


                     Fig. 1
                      shows the architecture of our eye detection method. First, we apply the Bayesian Discriminating Features (BDF) method [32] to detect a face from an image and normalize the detected face to a predefined size. Second, we use some geometric constraints to extract an eye strip from the upper portion of the detected face. Illumination variations are then attenuated by means of an illumination normalization procedure that consists of Gamma correction, difference of Gaussian filtering, and contrast equalization as applied in [33] and [34]. Third, the DFE method applies the 2D Haar basis functions to derive the discriminatory Haar features (DHFs). Finally, the eSVM classifier applies the DHFs features for eye detection. Usually there are multiple detections around the pupil center. The average of these multiple detections is eventually chosen as the eye location.

We evaluate our proposed eye detection method on the Face Recognition Grand Challenge (FRGC) database [35,36] and the BioID face database [10]. Experimental results show that (i) the DHFs exhibit promising classification capability for the eye detection problem; (ii) the eSVM runs much faster than the conventional SVM; and (iii) the proposed eye detection method achieves near real-time eye detection speed and better eye detection performance than some state-of-the-art eye detection methods.

We first present in this section the discriminating feature extraction (DFE) method for two-class problems. The discriminatory Haar features (DHFs) are extracted by applying the DFE method to the 2D Haar wavelet transform.

The discriminatory feature extraction (DFE) method extracts the most discriminatory features for two-class problems based on two novel measure vectors and a new criterion in the whitened PCA space.

Let 
                           X
                           ∈
                           
                              R
                              N
                           
                         be a pattern vector in an N dimensional space. Its covariance matrix 
                           
                              
                                 ∑
                                 X
                              
                              
                           
                           ∈
                           
                              R
                              
                                 N
                                 ×
                                 N
                              
                           
                         is defined as follows:
                           
                              (1)
                              
                                 
                                    
                                       ∑
                                       X
                                    
                                    
                                 
                                 =
                                 ε
                                 
                                    
                                       
                                          
                                             X
                                             −
                                             ε
                                             
                                                X
                                             
                                          
                                       
                                       
                                          
                                             
                                                X
                                                −
                                                ε
                                                
                                                   X
                                                
                                             
                                          
                                          t
                                       
                                    
                                 
                              
                           
                        
                     

where ε(⋅) is the expectation operator. The covariance matrix can be factorized into the following form according to PCA [23]:
                           
                              (2)
                              
                                 
                                    
                                       ∑
                                       X
                                    
                                    
                                 
                                 =
                                 Φ
                                 Λ
                                 Φ
                              
                           
                        
                     

where Φ=[ϕ
                        1,
                        ϕ
                        2,⋯,
                        ϕ
                        
                           N
                        ] is an orthogonal eigenvector matrix and Λ=diag{λ
                        1,
                        λ
                        2,⋯,
                        λ
                        
                           N
                        } is a diagonal eigenvalue matrix with diagonal elements in decreasing order: λ
                        1
                        ≥
                        λ
                        2
                        ≥⋯≥
                        λ
                        
                           N
                        . To prevent some principal components from dominating others for pattern classification, the whitening transformation spheres the covariance matrix. The whitening transformation matrix W thus is defined as follows:
                           
                              (3)
                              
                                 W
                                 =
                                 
                                    
                                       Φ
                                       Λ
                                    
                                    
                                       −
                                       1
                                       /
                                       2
                                    
                                 
                              
                           
                        
                     

where W
                        ∈ℝ
                           N
                           ×
                           N
                        .

We now define two measure vectors and a criterion vector in the whitened PCA space in order to form the DFE transformation matrix. The fundamental of the DFE method is to choose a smaller set of vectors from W with the most discriminatory capability. This smaller set of vectors will form the DFE transformation matrix. Towards that end, we first define the cluster-measure vector, α
                        ∈ℝ
                           N
                        , and the separation-measure vector, β
                        ∈ℝ
                           N
                        , as follows:
                           
                              (4)
                              
                                 α
                                 =
                                 
                                    P
                                    1
                                 
                                 
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          n
                                          1
                                       
                                    
                                    
                                 
                                 
                                 s
                                 
                                    
                                       
                                          W
                                          t
                                       
                                       
                                          x
                                          i
                                          
                                             1
                                          
                                       
                                       −
                                       
                                          W
                                          t
                                       
                                       
                                          M
                                          1
                                       
                                    
                                 
                                 +
                                 
                                    P
                                    2
                                 
                                 
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          n
                                          2
                                       
                                    
                                    
                                 
                                 
                                 s
                                 
                                    
                                       
                                          W
                                          t
                                       
                                       
                                          x
                                          i
                                          
                                             2
                                          
                                       
                                       −
                                       
                                          W
                                          t
                                       
                                       
                                          M
                                          2
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (5)
                              
                                 β
                                 =
                                 
                                    P
                                    1
                                 
                                 s
                                 
                                    
                                       
                                          W
                                          t
                                       
                                       
                                          M
                                          1
                                       
                                       −
                                       
                                          W
                                          t
                                       
                                       M
                                    
                                 
                                 +
                                 
                                    P
                                    2
                                 
                                 s
                                 
                                    
                                       
                                          W
                                          t
                                       
                                       
                                          M
                                          2
                                       
                                       −
                                       
                                          W
                                          t
                                       
                                       M
                                    
                                 
                              
                           
                        
                     

where P
                        1 and P
                        2 are the prior probabilities, n
                        1 and n
                        2 are the number of samples, and x
                        
                           i
                        
                        (1) and x
                        
                           i
                        
                        (2) are the pattern vectors of the first and second classes, respectively. M
                        1, M
                        2, and M are the mean vectors of the two classes, and the grand mean, respectively. The s(⋅) function defines the absolute value of the elements of the input vector. The significance of these new measure vectors is that the cluster-measure vector, α
                        ∈ℝ
                           N
                        , measures the clustering capability of the projection vectors in W, whereas the separation-measure vector, bfβ
                        ∈ℝ
                           N
                        , measures the separating capability of the vectors in W.

In order to choose the vectors from W with the most discriminatory capability, we further define a new criterion vector γ
                        ∈ℝ
                           N
                         as follows:
                           
                              (6)
                              
                                 γ
                                 =
                                 β
                                 .
                                 /
                                 α
                              
                           
                        
                     

where ./ is element-wise division. The value of the elements in γ indicates the discriminatory power of their corresponding vectors in W: the larger the value is, the more discriminatory power the corresponding vector in W possesses. We therefore choose the top m vectors, W
                        
                           i1,
                        W
                        
                           i2,⋯,
                        W
                        
                           im
                        , in W corresponding to the m largest values in γ to form the DFE transformation matrix T:
                           
                              (7)
                              
                                 T
                                 =
                                 
                                    
                                       W
                                       
                                          i
                                          1
                                       
                                    
                                    
                                       W
                                       
                                          i
                                          2
                                       
                                    
                                    ⋯
                                    
                                       W
                                       
                                          i
                                          m
                                       
                                    
                                 
                              
                           
                        
                     

where T
                        ∈ℝ
                           N
                           ×
                           m
                         and m
                        <
                        N. The DFE features are thus defined as follows:
                           
                              (8)
                              
                                 Y
                                 =
                                 
                                    T
                                    t
                                 
                                 X
                                 .
                              
                           
                        
                     

The DFE features thus reside in a low dimensional space ℝ
                           m
                         and capture the most discriminatory information of the original data 
                           X
                        .

In comparison with the PCA method, which extracts the most representative features by choosing the top eigenvectors from Φ to form the transformation matrix, the DFE method extracts the most discriminatory features by choosing the top vectors from W according to their corresponding values in γ to form the transformation matrix.

In comparison with the FLD method, the DFE method is capable of extracting multiple features in a dimensional space ℝ
                           m
                         for two-class problems. The FLD method looks for a projection matrix Q that maximizes the criterion J(Q)=|Q
                        
                           t
                        
                        S
                        
                           b
                        
                        Q|/|Q
                        
                           t
                        
                        S
                        
                           w
                        
                        Q|, where S
                        
                           w
                         and S
                        
                           b
                         are the within-class and between-class scatter matrices, respectively [23]. Mathematically, this criterion is maximized when Q consists of the leading eigenvectors of S
                        
                           w
                        
                        −1
                        S
                        
                           b
                        . According to the definition of between-class scatter matrix S
                        
                           b
                         
                        [23], the rank of S
                        
                           b
                         is at most L-1 for any L class problems. Consequently, the rank of S
                        
                           w
                        
                        −1
                        S
                        
                           b
                         is at most L-1 as well. Therefore, there are at most L-1 valid eigenvectors of S
                        
                           w
                        
                        −1
                        S
                        
                           b
                        , which means that the FLD can only derive at most L-1 valid feature for any L class problems. For two-class problems, the FLD method can only derive a single valid feature, which is significantly inadequate for achieving satisfactory performance.

The discriminatory Haar features (DHFs) are derived by applying the DFE method to the 2D Haar wavelet transform. Haar wavelet transform has broad applications in pattern recognition and data interpretation, such as cloud classification [37], object detection [38], face detection [39,40], as well as data visualization [41]. The 2D Haar wavelet transform is defined as the projection of an image onto the 2D Haar basis functions [21]. The 2D Haar basis functions display attractive characteristics such as enhancing local contrast and facilitating feature extraction. These characteristics become even more profound when the 2D Haar wavelet transform is applied to eye detection, where a dark pupil is in the center of a colored iris that is surrounded by white sclera.

The 2D Haar basis functions are defined as the tensor product of the one dimensional Haar scaling and wavelet functions [20]. The detailed information about the one dimensional Haar scaling and wavelet functions can be found in [21] and [22]. From one aspect of view, the 2D Haar basis functions include a set of scaled and shifted box type functions that encode the differences in average intensities among the regions in different scales [38]. Fig. 2
                         shows a family of 2D Haar basis functions with the size of 8×8, where white, black, and gray represent 1, −1, and 0, respectively. Fig. 2 reveals that the 2D Haar basis functions contain mainly three types of representations in different scales and locations: (i) two horizontal neighboring regions for computing the difference between the sum of the pixels within each of them, (ii) two vertical neighboring regions for computing the difference between the sum of the pixels within each of them, and (iii) four neighboring regions for computing the difference between the diagonal pairs of the regions. Note that the first basis function is for computing the average of the whole image. Since the eye regions possess remarkable intensity and gradient variations, the intensity difference encoding scheme of the 2D Haar wavelet basis functions is shown to be effective to capture the characteristic of eyes in our research.

The 2D Haar wavelet features usually reside in a high dimensional space and are highly correlated with each other. The high dimensionality renders the classifier implement such as SVM intractable, and the poor discriminating features weaken the performance of the classifier. We therefore apply the DFE method to the 2D Haar wavelet transform to derive the discriminatory Haar features (DHFs), which reside in a low dimensional space and possess the discriminatory capability. The approach of generating the DHFs is described in Algorithm 1.
                           
                              
                                 
                                 
                                    
                                       Algorithm 1. The approach of generating the DHFs
                                    
                                 
                                 
                                    
                                       
                                          Input: An image column vector 
                                             X
                                             ∈
                                             
                                                R
                                                
                                                   
                                                      n
                                                      r
                                                   
                                                   ×
                                                   
                                                      n
                                                      c
                                                   
                                                
                                             
                                           and a family of 2D Haar basis functions 
                                             Ψ
                                             ∈
                                             
                                                R
                                                
                                                   
                                                      
                                                         
                                                            n
                                                            r
                                                         
                                                         ×
                                                         
                                                            n
                                                            c
                                                         
                                                      
                                                   
                                                   ×
                                                   N
                                                
                                             
                                          , where n
                                          
                                             r
                                           and n
                                          
                                             c
                                           denote the number of rows and columns of the image, and N denotes the number of 2D Haar basis functions, respectively.
                                    
                                    
                                       
                                          Output: The discriminatory Haar features (DHFs) 
                                             Z
                                             ∈
                                             
                                                R
                                                m
                                             
                                          .
                                    
                                    
                                       
                                          Step 1: Compute the 2D Haar wavelet features 
                                             Y
                                             ∈
                                             
                                                R
                                                N
                                             
                                          :
                                          
                                             Y
                                             =
                                             
                                                Ψ
                                                t
                                             
                                             X
                                          .
                                    
                                    
                                       
                                          Step 2: Given the 2D Haar wavelet features 
                                             Y
                                             ∈
                                             
                                                R
                                                N
                                             
                                          , follow the DFE method to compute the DFE transformation matrix T
                                          ∈ℝ
                                             N
                                             ×
                                             m
                                           in Eq. (7).
                                    
                                    
                                       
                                          Step 3: Compute the DHFs 
                                             Z
                                             ∈
                                             
                                                R
                                                m
                                             
                                          :
                                          
                                             Z
                                             =
                                             
                                                T
                                                t
                                             
                                             Y
                                             =
                                             
                                                
                                                   
                                                      Ψ
                                                      T
                                                   
                                                
                                                t
                                             
                                             X
                                          .
                                    
                                 
                              
                           
                        
                     

Even though the conventional support vector machine (SVM) can achieve satisfactory performance on the eye detection problem, the SVM based eye detection methods usually have slow speed due to the expensive computational cost of the decision function and the excessive number of pixels over an image (in our experiment, each eye strip performs 440 classifications to locate the eyes even after an image downsampling). We therefore present in this section an efficient SVM (eSVM) that significantly improves the computational efficiency upon the conventional SVM without sacrificing the generalization performance. Before we present the eSVM, we first analyze the factors leading to the inefficiency problem (i.e., the large number of the support vectors) of the conventional SVM.

Let the training set be {(x
                        1,
                        y
                        1),(x
                        2,
                        y
                        2),⋯,(x
                        
                           l
                        ,
                        y
                        
                           l
                        )}, where x
                        
                           i
                        
                        ∈ℝ
                           n
                        , y
                        
                           i
                        
                        ∈{−1,1} indicate the two different classes, and l is the number of the training samples. The conventional SVM defines an optimal separating hyperplane, w
                        
                           t
                        
                        x
                        +
                        b
                        =0, by minimizing the following functional:
                           
                              (9)
                              
                                 
                                    
                                       
                                          
                                             min
                                             
                                                w
                                                ,
                                                b
                                                ,
                                                
                                                   ξ
                                                   i
                                                
                                             
                                          
                                          
                                             1
                                             2
                                          
                                          
                                             w
                                             t
                                          
                                          w
                                          +
                                          C
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                l
                                             
                                             
                                          
                                          
                                          
                                             ξ
                                             i
                                          
                                          
                                          ,
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   subject
                                                   
                                                   t
                                                   o
                                                   
                                                   
                                                      y
                                                      i
                                                   
                                                   
                                                      
                                                         
                                                            w
                                                            t
                                                         
                                                         
                                                            x
                                                            i
                                                         
                                                         +
                                                         b
                                                      
                                                   
                                                   ≥
                                                   1
                                                   −
                                                   
                                                      ξ
                                                      i
                                                   
                                                   ,
                                                
                                                
                                                   
                                                      ξ
                                                      i
                                                   
                                                   ≥
                                                   0
                                                   ,
                                                   
                                                
                                                
                                                   i
                                                   =
                                                   1
                                                   ,
                                                   2
                                                   ,
                                                   ⋯
                                                   ,
                                                   l
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

where ξ
                        
                           i
                        
                        ≥0 refers to slack variables and C
                        >0 is a regularization parameter.

The Lagrangian theory and the Kuhn–Tucker theory are then applied to optimize the functional in Eq. (9) with inequality constraints [42]. The optimization process leads to the following quadratic convex programming problem:
                           
                              (10)
                              
                                 
                                    
                                       
                                          
                                             max
                                             α
                                          
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                l
                                             
                                             
                                          
                                          
                                          
                                             α
                                             i
                                          
                                          −
                                          
                                             1
                                             2
                                          
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   ,
                                                   j
                                                   =
                                                   1
                                                
                                                l
                                             
                                             
                                          
                                          
                                          
                                             α
                                             i
                                          
                                          
                                             α
                                             j
                                          
                                          
                                             y
                                             i
                                          
                                          
                                             y
                                             j
                                          
                                          
                                             x
                                             i
                                          
                                          
                                             x
                                             j
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   subject
                                                   
                                                   t
                                                   o
                                                   
                                                   
                                                      
                                                         ∑
                                                         
                                                            i
                                                            =
                                                            1
                                                         
                                                         l
                                                      
                                                      
                                                   
                                                   
                                                   
                                                      y
                                                      i
                                                   
                                                   
                                                      α
                                                      i
                                                   
                                                   =
                                                   0
                                                   ,
                                                
                                                
                                                   0
                                                   ≤
                                                   
                                                      α
                                                      i
                                                   
                                                   ≤
                                                   C
                                                   ,
                                                
                                                
                                                   i
                                                   =
                                                   1
                                                   ,
                                                   2
                                                   ,
                                                   …
                                                   l
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The decision function of the SVM is therefore derived as follows:
                           
                              (11)
                              
                                 f
                                 
                                    x
                                 
                                 =
                                 sign
                                 
                                    
                                       w
                                       x
                                       +
                                       b
                                    
                                 
                                 =
                                 sign
                                 
                                    
                                       
                                          
                                             ∑
                                             
                                                i
                                                ∈
                                                S
                                                V
                                             
                                          
                                          
                                       
                                       
                                       
                                          y
                                          i
                                       
                                       
                                          α
                                          i
                                       
                                       
                                          x
                                          i
                                       
                                       x
                                       +
                                       b
                                    
                                 
                              
                           
                        
                     

where SV is the set of support vectors (SVs), which are the training samples with nonzero coefficients α
                        
                           i
                        . Eq. (11) reveals that the computation of the decision function involves an inner product of all support vectors (note that the computation of the decision function for the kernel SVM involves a kernel computation of all support vectors). Therefore, the computation cost of the decision function is positively correlated to the number of support vectors. When the number of support vectors is large, the computation cost of the inner product will become expensive and the computational efficiency of the conventional SVM will be compromised.

According to the Kuhn–Tucker theory, we have the following conditions for the conventional SVM:
                           
                              (12)
                              
                                 
                                    
                                       
                                          
                                             α
                                             i
                                          
                                          
                                             
                                                
                                                   y
                                                   i
                                                
                                                
                                                   
                                                      
                                                         w
                                                         t
                                                      
                                                      
                                                         x
                                                         i
                                                      
                                                      +
                                                      b
                                                   
                                                
                                                −
                                                1
                                                +
                                                
                                                   ξ
                                                   i
                                                
                                             
                                          
                                          =
                                          0
                                          ,
                                       
                                       
                                          i
                                          =
                                          1
                                          ,
                                          2
                                          ,
                                          ⋯
                                          ,
                                          l
                                          .
                                       
                                    
                                 
                              
                           
                        
                     

Eq. (12) shows that if α
                        
                           i
                        
                        ≠0, then y
                        
                           i
                        (w
                        
                           t
                        
                        x
                        
                           i
                        
                        +
                        b)−1+
                        ξ
                        
                           i
                        
                        =0. Therefore, the training samples that satisfy y
                        
                           i
                        (w
                        
                           t
                        
                        x
                        
                           i
                        
                        +
                        b)−1+
                        ξ
                        
                           i
                        
                        =0 are support vectors for the conventional SVM. The intuitive interpretation of the support vectors is that they are the training samples that lie on their boundaries or the samples pulled onto their boundaries by the slack variables ξ
                        
                           i
                         as shown in Fig. 3
                        . In fact, Fig. 3 shows that all the training samples on the wrong side of their margin become support vectors because of the slack variables, which pull the training samples onto their boundaries to make them support vectors. As a complex pattern classification problem often has a large number of the training samples on the wrong side of their margin, the number of support vectors becomes quite large, which significantly reduces the computational efficiency of the conventional SVM.

The conventional SVM usually derives a large number of support vectors, because all the training samples on the wrong side of their margin become support vectors as the slack variables pull these samples to their boundaries. The eSVM, however, reduces the number of support vectors significantly because only a small number (can be as few as one) of the training samples on the wrong side of their margin are pulled to their boundaries to become support vectors.

Specifically, the eSVM follows a two-round optimization process. The first round of the optimization, which is for the conventional SVM, is used to define a Θ set that corresponds to the training samples on the wrong side of their margin derived from the conventional SVM. The Θ set plays an important role in controlling the generalization performance in the second round of the optimization. The second round optimization, which introduces a single slack variable for all the training samples in the Θ set, finally defines the support vectors and the decision function of the eSVM.

The second optimization functional is defined as follows:
                           
                              (13)
                              
                                 
                                    
                                       
                                          
                                             min
                                             
                                                w
                                                ,
                                                b
                                                ,
                                                ξ
                                             
                                          
                                          
                                             1
                                             2
                                          
                                          
                                             w
                                             t
                                          
                                          w
                                          +
                                          C
                                          ξ
                                          
                                          ,
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   subject
                                                   
                                                   t
                                                   o
                                                   
                                                   
                                                      y
                                                      i
                                                   
                                                   
                                                      
                                                         
                                                            w
                                                            t
                                                         
                                                         
                                                            x
                                                            i
                                                         
                                                         +
                                                         b
                                                      
                                                   
                                                   ≥
                                                   1
                                                   
                                                   ,
                                                
                                                
                                                   i
                                                   ∈
                                                   Ω
                                                   −
                                                   Θ
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      y
                                                      i
                                                   
                                                   
                                                      
                                                         
                                                            w
                                                            t
                                                         
                                                         
                                                            x
                                                            i
                                                         
                                                         +
                                                         b
                                                      
                                                   
                                                   ≥
                                                   1
                                                   −
                                                   ξ
                                                   
                                                   ,
                                                
                                                
                                                   i
                                                   ∈
                                                   Θ
                                                   
                                                   ,
                                                
                                                
                                                   
                                                   ξ
                                                   ≥
                                                   0
                                                
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                       
                                    
                                    
                                       
                                          
                                       
                                    
                                    
                                       
                                          
                                       
                                    
                                 
                              
                           
                        where Θ is the set of the training samples on the wrong side of their margin derived from the conventional SVM, and Ω is the set of all the training samples. Compared with the conventional SVM that introduces the slack variables with different values, this optimization functional specifies a fixed value for all the slack variables. The first inequality constraint in Eq. (13) ensures that the training samples on the right side of their margin in the conventional SVM are still on the right side in the eSVM. The second inequality constraint in Eq. (13) ensures that only a small number of training samples in the Θ set become support vectors due to the introduction of the single slack variable that pulls most of the training samples in the Θ set beyond their margin to the right side and thus become non-support vectors. The significance of the second round optimization is thus to simulate the maximal margin separating boundaries of the conventional SVM by using much fewer support vectors.

Further analysis on the first inequality constraint in Eq. (13) reveals that this constraint makes the eSVM to maintain a similar maximal margin separating boundary with that of the conventional SVM. The definition of the separating boundary w
                        
                           t
                        
                        x
                        +
                        b
                        =0 is associated with the definition of the maximal margin w
                        
                           t
                        
                        x
                        +
                        b
                        =±1. Given the separating boundary w
                        
                           t
                        
                        x
                        +
                        b
                        =0, the maximal margin is fixed to w
                        
                           t
                        
                        x
                        +
                        b
                        =±1, and vice versa. The first inequality constraint in Eq. (13) ensures that the training samples on the right side of their margin in the conventional soft-margin SVM are still on the right side in the eSVM. If the eSVM derives a separating boundary that is significantly different from the one derived by the conventional SVM, this constraint will not be satisfied. As the eSVM derives a similar separating boundary with the conventional SVM, the maximal margin of the eSVM is thus similar with that of the conventional SVM as well — neither degrade nor upgrade much from that of the SVM. Consequently, the eSVM inherits the advantage of generalization performance of the conventional SVM, and has comparable classification performance with the SVM.

The second inequality constraint in Eq. (13) plays the significant role of reducing the number of support vectors. This constraint ensures that only a small number of training samples in the Θ set become support vectors due to the introduction of the single slack variable that pulls most of the training samples in the Θ set beyond their boundary to the right side and thus become non-support vectors. It is possible that support vectors lying on their boundaries for the eSVM are a little bit more than those for the SVM. However, majority of support vectors for the SVM come from the samples on the wrong side of their margin (i.e., the Θ set). This is because the chance that samples happen to fall on their boundaries is significantly lower than the chance that samples fall onto the right side or wrong side of their margin. Therefore, even though support vectors on their boundaries for the eSVM may increase a little bit upon those for the SVM, the total number of support vectors for the eSVM is still significantly less than that for the SVM.

As the optimization problem of the eSVM defined in Eq. (13) is different from that of the conventional SVM, its corresponding dual mathematical problem after applying the Lagrangian theory and the Kuhn–Tucker theory is also different from the ones derived in the conventional SVM. In particular, let α
                        1,
                        α
                        2,⋯,
                        α
                        
                           l
                        
                        ≥0 and μ
                        ≥0 be the Lagrange multipliers; the primal Lagrange functional is defined as follows:
                           
                              (14)
                              
                                 
                                    
                                       
                                          F
                                          
                                             w
                                             b
                                             ξ
                                             
                                                α
                                                i
                                             
                                             μ
                                          
                                          =
                                          
                                             1
                                             2
                                          
                                          
                                             w
                                             t
                                          
                                          w
                                          +
                                          C
                                          ξ
                                          −
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   ∈
                                                   Ω
                                                   −
                                                   Θ
                                                
                                             
                                             
                                          
                                          
                                             α
                                             i
                                          
                                          
                                             
                                                
                                                   y
                                                   i
                                                
                                                
                                                   
                                                      
                                                         w
                                                         t
                                                      
                                                      
                                                         x
                                                         i
                                                      
                                                      −
                                                      b
                                                   
                                                
                                                −
                                                1
                                             
                                          
                                       
                                    
                                    
                                       
                                          −
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   ∈
                                                   Θ
                                                
                                             
                                             
                                          
                                          
                                             α
                                             i
                                          
                                          
                                             
                                                
                                                   y
                                                   i
                                                
                                                
                                                   
                                                      
                                                         w
                                                         t
                                                      
                                                      
                                                         x
                                                         i
                                                      
                                                      −
                                                      b
                                                   
                                                
                                                +
                                                ξ
                                                −
                                                1
                                             
                                          
                                          −
                                          μ
                                          ξ
                                       
                                    
                                    
                                       
                                          =
                                          
                                             1
                                             2
                                          
                                          
                                             w
                                             t
                                          
                                          w
                                          +
                                          
                                             
                                                C
                                                −
                                                
                                                   
                                                      ∑
                                                      
                                                         i
                                                         ∈
                                                         Θ
                                                      
                                                   
                                                   
                                                
                                                
                                                   α
                                                   i
                                                
                                                −
                                                μ
                                             
                                          
                                          ξ
                                          −
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   ∈
                                                   Ω
                                                
                                             
                                             
                                          
                                          
                                             α
                                             i
                                          
                                          
                                             
                                                
                                                   y
                                                   i
                                                
                                                
                                                   
                                                      
                                                         w
                                                         t
                                                      
                                                      
                                                         x
                                                         i
                                                      
                                                      −
                                                      b
                                                   
                                                
                                                −
                                                1
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Next, we maximize the primal Lagrange functional 
                           F
                        (w,
                        b,
                        ξ,
                        α
                        
                           i
                        ,
                        μ) with respect to w, b, and ξ as follows:
                           
                              (15)
                              
                                 
                                    
                                       ∂
                                       F
                                    
                                    
                                       ∂
                                       w
                                    
                                 
                                 =
                                 w
                                 −
                                 
                                    
                                       ∑
                                       
                                          i
                                          ∈
                                          Ω
                                       
                                    
                                    
                                 
                                 
                                    α
                                    i
                                 
                                 
                                    y
                                    i
                                 
                                 
                                    x
                                    i
                                 
                                 =
                                 0
                                 ⇒
                                 w
                                 =
                                 
                                    
                                       ∑
                                       
                                          i
                                          ∈
                                          Ω
                                       
                                    
                                    
                                 
                                 
                                    α
                                    i
                                 
                                 
                                    y
                                    i
                                 
                                 
                                    x
                                    i
                                 
                              
                           
                        
                        
                           
                              (16)
                              
                                 
                                    
                                       ∂
                                       F
                                    
                                    
                                       ∂
                                       b
                                    
                                 
                                 =
                                 
                                    
                                       ∑
                                       
                                          i
                                          ∈
                                          Ω
                                       
                                    
                                    
                                 
                                 
                                    α
                                    i
                                 
                                 
                                    y
                                    i
                                 
                                 =
                                 0
                              
                           
                        
                        
                           
                              (17)
                              
                                 
                                    
                                       ∂
                                       F
                                    
                                    
                                       ∂
                                       ξ
                                    
                                 
                                 =
                                 C
                                 −
                                 
                                    
                                       ∑
                                       
                                          i
                                          ∈
                                          Θ
                                       
                                    
                                    
                                 
                                 
                                    α
                                    i
                                 
                                 −
                                 μ
                                 =
                                 0
                              
                           
                        
                     

and then, we derive a convex quadratic programming model by substituting Eqs. (15), (16), and (17) into Eq. (14) as follows:
                           
                              (18)
                              
                                 
                                    
                                       
                                          
                                             max
                                             α
                                          
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   ∈
                                                   Ω
                                                
                                             
                                             
                                          
                                          
                                             α
                                             i
                                          
                                          −
                                          
                                             1
                                             2
                                          
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   ,
                                                   j
                                                   ∈
                                                   Ω
                                                
                                             
                                             
                                          
                                          
                                             α
                                             i
                                          
                                          
                                             α
                                             j
                                          
                                          
                                             y
                                             i
                                          
                                          
                                             y
                                             j
                                          
                                          
                                             x
                                             i
                                          
                                          
                                             x
                                             j
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   subject
                                                   
                                                   t
                                                   o
                                                   
                                                   
                                                      
                                                         ∑
                                                         
                                                            i
                                                            ∈
                                                            Ω
                                                         
                                                      
                                                      
                                                   
                                                   
                                                   
                                                      y
                                                      i
                                                   
                                                   
                                                      α
                                                      i
                                                   
                                                   =
                                                   0
                                                   ,
                                                
                                                
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               ∑
                                                               
                                                                  i
                                                                  ∈
                                                                  Θ
                                                               
                                                            
                                                            
                                                         
                                                         
                                                         
                                                            α
                                                            i
                                                         
                                                      
                                                   
                                                   ≤
                                                   C
                                                   ,
                                                
                                                
                                                   
                                                   
                                                      α
                                                      i
                                                   
                                                   ≥
                                                   0
                                                   ,
                                                
                                                
                                                   i
                                                   ∈
                                                   Ω
                                                   .
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Furthermore, we have the following constraints for the eSVM from the Kuhn–Tucker theory:
                           
                              (19)
                              
                                 
                                    
                                       
                                          
                                             α
                                             i
                                          
                                          
                                             
                                                
                                                   y
                                                   i
                                                
                                                
                                                   
                                                      
                                                         w
                                                         t
                                                      
                                                      x
                                                      +
                                                      b
                                                   
                                                
                                                −
                                                1
                                             
                                          
                                          =
                                          0
                                          ,
                                          
                                          i
                                          ∈
                                          Ω
                                          −
                                          Θ
                                       
                                    
                                    
                                       
                                          
                                             α
                                             i
                                          
                                          
                                             
                                                
                                                   y
                                                   i
                                                
                                                
                                                   
                                                      
                                                         w
                                                         t
                                                      
                                                      x
                                                      +
                                                      b
                                                   
                                                
                                                −
                                                1
                                                +
                                                ξ
                                             
                                          
                                          =
                                          0
                                          ,
                                          
                                          i
                                          ∈
                                          Θ
                                          .
                                       
                                    
                                 
                              
                           
                        
                     

Eq. (19) shows that if α
                        
                           i
                        
                        ≠0, then either y
                        
                           i
                        (w
                        
                           t
                        
                        x
                        +
                        b)−1=0,
                        i
                        ∈Ω−Θ or y
                        
                           i
                        (w
                        
                           t
                        
                        x
                        +
                        b)−1+
                        ξ
                        =0,
                        i
                        ∈Θ. The training samples that satisfy either y
                        
                           i
                        (w
                        
                           t
                        
                        x
                        +
                        b)−1=0,
                        i
                        ∈Ω−Θ or y
                        
                           i
                        (w
                        
                           t
                        
                        x
                        +
                        b)−1+
                        ξ
                        =0,
                        i
                        ∈Θ are support vectors for the eSVM. Therefore, the intuitive interpretation of the support vectors is the training samples that lie on their boundaries or the samples pulled onto their boundaries by the slack variable ξ as shown in Fig. 4
                        . As all the slack variables in the eSVM have the same value, Fig. 4 also reveals that only a small number (can be as few as one) of the training samples on the wrong side of their margin (i.e., samples in the Θ set) are pulled onto their boundaries to become support vectors, while the others are not support vectors because they are pulled to the right side of their margin but do not fall onto the boundaries.

To further explain the advantages of the eSVM, we intuitively compare the eSVM with the conventional SVM on a two dimensional synthetic data set — the Ripley data set [43–45]. The Ripley data set defines a two-class non-separable problem, and the numbers of training and testing samples are 250 and 1000, respectively. A linear kernel and a nonlinear (RBF) kernel are applied [44,45].


                        Fig. 5
                         plots the training samples, the support vectors, and the separating boundaries of the conventional SVM and the eSVM with the linear and RBF kernels, respectively. Fig. 5 reveals that the eSVM has almost identical separating boundaries with the conventional SVM using either the linear kernel or the RBF kernel. This is consistent with the theory that the eSVM simulates the maximal margin separating boundaries of the conventional SVM in order to maintain its generalization performance. Fig. 5 also reveals that the number of support vectors of the eSVM is much smaller than that of the conventional SVM. Specifically, Fig. 5(a) and Fig. 5(c) show that the support vectors of the conventional SVM, which are represented by red crosses, are mostly the samples on the wrong side of their margin. The number of support vectors thus is large due to the fact that many training samples are on the wrong side of their margin for the non-separable problems. In contrast, for the eSVM, only a small number of the training samples on the wrong side of their margin, as shown in Fig. 5(b) and Fig. 5(d), become support vectors.


                        Table 1
                         shows the comparison of the SVM and the eSVM on the number of the support vectors, the slope and the y-intercept of the separating boundaries using the linear kernel, as well as the classification rate and running time for the testing data set. In particular, the eSVM reduces the number of support vectors by 88.76% and 75.64%, when compared with the conventional SVM using the linear and RBF kernels, respectively. Consequently, the running time of the eSVM is also reduced compared with the SVM when the same kernel is applied. The similar slope and the y-intercept values of the separating boundaries between the eSVM and the conventional SVM using a linear kernel indicate that they define similar separating boundaries, and hence have comparable generalization performance. Specifically, the classification rate of the eSVM on the testing data set is the same with that of the SVM using the linear kernel, but the classification rate of the eSVM is 0.5% higher than that of the SVM when using the RBF kernel.

@&#EXPERIMENTS@&#

We evaluate the effectiveness and the efficiency of our proposed eye detection method using 12,776 Face Recognition Grand Challenge (FRGC) images from the FRGC version 2 database [35,36]. Note that the FRGC images possess challenge properties, such as large variations in illumination, in skin color (white, yellow, and black), in facial expression (eyes open, partially open, or closed), as well as in scale and orientation. Additional challenges include eye occlusion caused by eye glasses or long hair, and the red eye effect due to the photographic effect. All these challenge factors increase the difficulty of accurate eye detection. We also implement the experiments on the BioID database [10] in order to evaluate the robustness of our proposed eye detection method and compare with some state-of-the-art methods. The experimental results on the BioID database are shown in the end of this section.

The training data collected from various sources contains 3000 pairs of eyes and 12,000 non-eye patches in our experiments. The effect of illumination variations is alleviated as described in Section 1. Fig. 6
                      shows some example of eye and non-eye training images after illumination normalization.

We evaluate the performance of the eye detection using the maximum normalized error [10]. The maximum normalized error e is the detection pixel error normalized by the interocular distance, which can be defined as:
                        
                           (20)
                           
                              e
                              =
                              
                                 
                                    max
                                    
                                       
                                          
                                             
                                                
                                                   C
                                                   l
                                                
                                                −
                                                
                                                   
                                                      
                                                         C
                                                         ~
                                                      
                                                   
                                                   l
                                                
                                                |
                                                |
                                                ,
                                                
                                                |
                                                |
                                                
                                                   C
                                                   r
                                                
                                                −
                                                
                                                   
                                                      
                                                         C
                                                         ~
                                                      
                                                   
                                                   r
                                                
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   C
                                                   ~
                                                
                                             
                                             l
                                          
                                          −
                                          
                                             
                                                
                                                   C
                                                   ~
                                                
                                             
                                             r
                                          
                                       
                                    
                                 
                              
                           
                        
                     
                  

where C
                     
                        l
                      and C
                     
                        r
                      denote the detected location of the left and right eye center respectively, and 
                        
                           
                              
                                 C
                                 ~
                              
                           
                           l
                        
                      and 
                        
                           
                              
                                 C
                                 ~
                              
                           
                           r
                        
                      denote the ground truth of the left and right eye center respectively.

We first evaluate the discriminatory capability and the detection performance of the discriminatory Haar features (DHFs) in comparison with the PCA Haar features and the FLD Haar features. Note that the PCA Haar features and the FLD Haar features are defined by applying the PCA and the FLD to the 2D Haar wavelet transform of an input image, respectively. The 2D Haar wavelet transform in our experiments consists of 1024 2D Haar basis functions, and subsequently the dimension of the 2D Haar features is 1024 as well.

If only the most two significant transformation vectors are taken into consideration, the distribution of the eye and non-eye training images can be visualized in the 2D space spanned by these two vectors so as to give an intuitive view of the discriminatory capability of the DHFs in comparison with the PCA Haar features. Note that the FLD method derives a single valid transformation vector for two class problems, and thus the FLD Haar features contain only one feature. Fig. 7
                        (a) and Fig. 7(b) show the distributions of the eye and non-eye training images using the two most significant DHFs and the two most significant PCA Haar features, respectively. Fig. 7(a) reveals that the two classes (eye and non-eye) are highly overlapping with each other when the PCA Haar features are used, whereas Fig. 7(b) reveals that the two classes are only slightly overlapping each other when the DHFs are used. The distributions of the eye and non-eye training images in Fig. 7(a) and Fig. 7(b) thus indicate that the DHFs possess better discriminatory capability than the PCA Haar features.

The features with prominent discriminatory capability may facilitate the classifier design and enhance the performance. Fig. 8
                         shows the eye detection performance of the DHFs using the simple nearest neighbor classifier in comparison with the PCA Haar features and the FLD Haar features. To evaluate the robustness of the DHFs, three similarity measures, L
                        1, L
                        2, and the cosine similarity measure, are applied. For the best eye detection performance, the dimensions of the DHFs are 60, 80, and 80 for the L
                        1, the L
                        2, and the cosine measure, respectively. The dimensions of the PCA Haar features are 150, 60, and 80 for the L
                        1, the L
                        2, and the cosine measure, respectively. The dimension of the FLD Haar features is always one, as the FLD method derives only one feature for the two-class eye detection problem. In Fig. 8, the horizontal axis represents the maximum normalized error, and the vertical axis denotes the corresponding detection rate. Fig. 8 reveals that, regardless of the measures used, the DHFs achieve the best eye detection performance throughout different normalized errors, followed in order by the FLD Haar features and the PCA Haar features.


                        Table 2
                         shows the specific eye detection performance (both detection rate and detection accuracy) of the DHFs, the PCA Haar features, and the FLD Haar features. Please note that the detection rate is given for the normalized error of 0.05. Table 2 reveals that the DHFs with the L
                        2 distance measure achieve the best eye detection rate, 85.90%. The DHFs with the L
                        2 distance measure improve the eye detection accuracy as well, by reducing the normalized error of 0.054 (from 0.101 to 0.047) when compared with the PCA Haar features with the L
                        2 distance measure, and 0.018 (from 0.065 to 0.047) when compared with the FLD Haar features with the L
                        2 distance measure.

We next evaluate the eye detection performance of the DHFs, the PCA Haar features, and the FLD Haar features using the SVM and the eSVM classifiers, respectively. The purpose of this experiment is threefold: (i) further evaluate the performance of the DHFs in comparison with the PCA and the FLD Haar features using the SVM/eSVM classifier; (ii) evaluate the performance improvement of the SVM/eSVM classifier upon the simple nearest neighbor classifier; and (iii) evaluate the efficiency improvement of the eSVM method upon the conventional SVM method.

For the best eye detection performance, the dimensions of the DHFs, the PCA Haar features, and the FLD features are 60, 80, and 1, respectively. Only the RBF kernel 
                           K
                           
                              
                                 x
                                 i
                              
                              
                                 x
                                 j
                              
                           
                           =
                           
                              e
                              
                                 −
                                 r
                                 ∥
                                 
                                    x
                                    i
                                 
                                 −
                                 
                                    x
                                    j
                                 
                                 |
                                 
                                    
                                    2
                                 
                              
                           
                         is used for SVM. The parameter r is set to be 0.0125, and the regularizing parameter C is set to be 1. In order to make a fair comparison between the SVM and the eSVM, their parameters are kept the same.


                        Fig. 9
                         shows the eye detection performance of the DHFs, the PCA Haar features, and the FLD Haar features using the SVM and the eSVM, respectively. The first observation from Fig. 9 is that the DHFs outperform both the PCA and the FLD Haar features throughout different normalized errors when the same classifier is used. This observation is consistent with that the detection performance of the DHFs, when the simple nearest neighbor classifier is used, is better than the PCA and the FLD Haar features. This observation thus may indicate the robustness of the DHFs to different classifiers. The second observation is that the SVM (or the eSVM) classifier improves the eye detection performance upon the nearest neighbor classifier. Table 3
                         shows the detection rate of the DHFs, the PCA Haar features, and the FLD Haar features using the SVM and the eSVM, respectively. By comparing the detection rates listed in Tables 2 and 3, it is observed that even the worst performance based on the SVM (or the eSVM) classifier is better than the best performance based on the nearest neighbor classifier with the L
                        1, the L
                        2, and the cosine measures.

The last observation from Fig. 9 is that the eSVM classifier has at least comparable detection performance with the SVM when the same features are applied. Table 3 shows that the eSVM has a detection rate identical to that of the SVM when the FLD Haar features are applied, has 0.33% better rate than the SVM when the PCA Haar features are applied, and has 1.49% better rate when the DHFs are applied, respectively.


                        Table 3 also compares the efficiency performance between the SVM and the eSVM in terms of the number of support vectors, eye detection time, and eye detection time per image. Table 3 reveals that the eSVM significantly reduces the number of support vectors and as a result significantly increases the detection speed. Specifically, the eSVM reduces the number of support vectors of the SVM by 97.22% when the PCA Haar features are applied, 99.86% when the FLD Haar features are applied, and 97.27% when the DHFs are applied, respectively. As the number of support vectors decreases, the eye detection time is reduced. Take the DHFs as an example. The DHFs+eSVM method, which uses 0.12seconds (8.33 images per second) on average to process each image, is over twenty times faster than the SVM, which uses 2.79seconds (0.35 images per second) on average to process each image.

In summary, the proposed DHFs+eSVM method can process 8.33 images per second on average and achieve 89.24% eye detection rate for the normalized error of 0.05. Fig. 10
                         finally shows some examples of good and bad detections on the FRGC database and the BioID database using the proposed DHFs+eSVM method.

In order to assess the robustness of our proposed method and compare with the state-of-the-art eye detection methods, we finally implement experiments on the BioID face database [10]. The BioID database consists of 1521 frontal face images of 23 subjects. The methods we compare with include those used by Jesorsky et al. [10], Hamouz et al. [11,12], Cristinacce et al. [13], Asteriadis et al. [14], Bai et al. [15], Niu et al. [16], Campadelli et al. [8,17], and Valenti et al. [18]. All above methods reported the performance on the BioID database and applied the same normalized error as defined in Eq. (20) to evaluate the performance. Table 4
                         shows the performance comparison between our method and the methods mentioned above for the normalized error of 0.05, 0.10, and 0.25, respectively. Our results and the best results reported by other methods are highlighted in bold text. Note that for the performance which is in-explicitly reported by the authors, the results are estimated from the graphs in the literature. Table 4 reveals that for the normalized error of 0.05 and 0.25, our method outperforms all other state-of-the-art methods listed in the table. For the normalized error of 0.10, our method has comparable performance to the best results. Table 4 also shows the detection performance of our method on the FRGC database. The performance on the BioID database and that on the FRGC database are very close to each other, which indicates the robustness of our method.

Regarding the efficiency, not many papers reported the execution time of their methods. As a matter of fact, speed is an important factor in the real-word application of an eye detection system. Campadelli [8] presented an SVM based eye detection method and reported the execution time of 12s per image (Java code running on a Pentium 4 with 3.2GHz). In comparison, the average execution time of our method is only 0.12s per image due to the application of the eSVM (MATLAB code running on a Pentium 3 with 3.0GHz). In fact, the execution time can be further significantly reduced if some faster programming languages (like Java or C/C++) and multi-thread techniques are applied.

@&#CONCLUSION@&#

We present in this paper an accurate and efficient eye detection method using the discriminatory Haar features (DHFs) and a new efficient support vector machine (eSVM). The DHFs, which are extracted by applying a discriminating feature extraction (DFE) method to the 2D Haar wavelet transform, reside in a low dimensional space and possess discriminatory capability for eye detection problem. The eSVM significantly improves the computational efficiency upon the conventional SVM for eye detection without sacrificing the generalization performance. Experiments on the FRGC database and the BioID face database show that (i) the DHFs exhibit promising classification capability for eye detection problem; (ii) the eSVM runs much faster than the conventional SVM; and (iii) the proposed eye detection method achieves near real-time eye detection speed and better eye detection performance than some state-of-the-art eye detection methods.

@&#REFERENCES@&#

