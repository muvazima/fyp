@&#MAIN-TITLE@&#A maximum entropy method to assess the predictability of financial and commodity prices

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose a novel signal processing method for financial time series analysis.


                        
                        
                           
                           We predict the entropy by a least square minimization approach.


                        
                        
                           
                           We evaluate (by theory and simulation) the mean and variance of the predictions.


                        
                        
                           
                           We apply our technique to several sets of historical financial data.


                        
                        
                           
                           The efficiency of our technique is shown versus conventional econometrics approach.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Signal processing algorithms for financial engineering

Maximum entropy theory

Optimum linear prediction

Autocovariance function

Time series analysis

@&#ABSTRACT@&#


               
               
                  A novel signal processing method for the analysis of financial and commodity price time series is here introduced to assess the predictability of financial markets. Our technique, exploiting the maximum entropy method (MEM), predicts the entropy of the next future time interval of the time series under investigation by a least square minimization approach. Like in conventional ex-post analysis based on estimated entropy, high entropy values characterize unpredictable series, while more stable series exhibit lower entropy values. We first evaluate (by theory and simulation) the performance of our method in terms of mean and variance of the predictions. Then, we apply our technique to several sets of historical financial data, correlating the entropy trend to contemporary socio-political events. The efficiency of our technique for application to financial engineering analysis is shown in comparison with the conventional approximate entropy method (usually applied in econometrics).
               
            

@&#INTRODUCTION@&#

Notwithstanding their fundamental importance, signal processing and finance have been usually considered as two separate fields both in the general knowledge and in academic (higher) education [1]. Signal processing applications, which hold promising potential, are yet relatively unexplored within finance [2,3]. Conversely, in the last years there has been an explosive growth in the research area relating economics and mathematical modeling [4,5], especially in the fields of financial markets and trading researches and applications [6–11]. This work aims at exploiting signal processing methodologies applied to finance. In particular, we propose to apply prediction methods, usually employed in signal processing or communications problems, for assessing the predictability of market time series.

A typical example of signal processing application in finance is represented by financial stock trading, where most of the operators (i.e. the stock traders) ignore but fully exploit the potentialities of signal processing. They use analysis and prediction methodologies that are classical problems in research related with signal processing. In fact, stock traders usually try to profit from short-term price volatility with trades lasting anywhere, from several seconds to several weeks. Hence, the knowledge about the dynamic characteristics of the series under investigation becomes of fundamental importance in order to effectively perform effective forecasting procedures. The observation of historical data as well as the analysis of their standard deviation (e.g. volatility and price fluctuations) can be useful indicators of the dynamic characteristics of the series. Volatility is strictly related with the amplitude of the series fluctuations: high volatility results in large deviations from the mean, hence stating high unpredictability for that series. The observation of historical data as well as the analysis of their standard deviation (e.g. volatility and price fluctuations) can be useful indicators of the dynamic characteristics of the series. For example, the Chicago Board Options Exchange (CBOE) computes since 1993 the volatility index VIX® to measure market expectations of the near-term volatility implied by stock index option prices. As stated by [12], the VIX® index essentially offers a market-determined, forward-looking estimate of one-month stock market volatility. Most studies in the literature that tackle the information content of implied market volatility employ the VIX® index, see for example [13,14]. Options-implied volatility is typically more informative than time-series volatility models based on stock market index returns for forecasting purposes, though the latter may sometimes carry incremental information [15,16]. Since the VIX® index can be considered as a barometer of the overall market sentiment as to what concerns investors' risk appetite [17], many trading strategies rely on the VIX® index for hedging and speculative purposes. Then, volatility is strictly related with the amplitude of the series fluctuations: high volatility results in large deviations from the mean, hence stating high unpredictability for that series.

According to [18], financial market time series (FMTS) may deviate from constancy exhibiting two different behaviors: (i) the series is characterized by high standard deviation, and (ii) the series show a lot of irregularities. It is really important to discriminate between these two cases because they lead to different conclusions about the series predictability. In fact, the degree of variation from the mean is not usually related with unpredictability, while the amount of irregularities drastically affects the further forecasting process, resulting in unpredictable series. For example, if it would be possible to ensure to an investor that the series of future prices would be characterized by a precise sinusoidal pattern (although characterized by high deviation from the mean), then future prices can be planned according to a precise forecasting strategy. An example of a practical forecasting strategy exploited in the presence of sinusoidal patterns is shown in [19,20].

We can address, as usual, to standard deviation as a measure of deviation from the mean, while we will use entropy as the metric for evaluating the irregularities and, hence, the predictability of a series. In particular, it is well known since the publication of the pivotal work of the mathematician C. Shannon “A Mathematical Theory of Communications” in 1948, [21], that the concept of entropy is related to that of uncertainty. Hence, high values of the Shannon entropy results in an unpredictable series, while lower values mean less uncertainty and hence a more predictable behavior of that series. This concept has been also applied to non-stationary signals as well. In particular, in [22] it is found that, observing performance of entropy by time, the value of entropy is corresponding to “the predictivity of signals”. Namely, the entropy value becomes smaller, the predictivity becomes higher on the entropy curves by time. This concept has led to the publication of many works in the field of entropy-based analysis of financial markets. For example, the validity of the entropy approach in analyzing financial time series is demonstrated in [23]. Then, in [18], an empirical method for evaluating the entropy of a series is proposed, namely the approximate entropy (ApEn). ApEn is able to obtain the entropy estimation by modifying an exact regularity statistic, namely the maximum entropy method (or Kolmogorov–Sinai entropy). In particular, the authors use the approximate entropy technique as a marker of market stability, with rapid increases possibly foreshadowing significant changes in a financial variable. Entropy has been also used to quantify efficiency in foreign exchange markets, [24], in stock markets [25–28], and also to gain insight into the evolution of the aggregate market expectations [29,30]. Recently, studies focusing on energy commodity markets have been carried out under the entropy-based approach. As an instance, an entropy analysis of crude oil price dynamics is revealed in [31], while evidences from informational entropy analysis in evaluating the efficiency of crude oil markets were discussed in [32]. Then, in [33] a market efficiency index based on the ApEn metric is discussed for application to several energy commodities. However, all the aforementioned works evaluate the entropy of historical data and, applying ex-post considerations, try to declare the predictability of the series, i.e. they implicitly assume that the series under investigation are characterized by a stationary behavior. This means that they suppose that the past statistical features of the analyzed series remain unaltered also in the future.

In this paper, we move further by proposing an algorithm to assess the predictability of FMTS (and in particular of energy commodity market time series), by predicting the entropy regarding the future behavior of the series under investigation. We do not estimate the entropy of the analyzed series; rather, we predict the entropy of the next (i.e. future) time interval of the series. We remove the assumption of stationary series (i.e. we work in the presence of non-stationary signals), and then we exploit the maximum entropy method (MEM) to obtain the predicted entropy. In addition, our prediction is performed according to optimum prediction methods, such as the least squares minimization scheme. Finally, and according to the conventional entropy analysis (see for example [18,31,32]), if we predict high entropy values we are facing with unpredictable series, while more stable market time series exhibit lower predicted entropy values.

The remainder of this paper is organized as follows. Section 2 depicts the system model highlighting first the maximum entropy method, and then the conventional entropy estimation approach used in finance and economics. In Section 3 our entropy prediction method is described in details, while its performances, in terms of mean and variance of the estimates, are shown in Section 4. Applications of our technique to financial market time series are illustrated in Section 5, versus the conventional ApEn approach, and finally our conclusions are briefly summarized in Section 6.

Conventional entropy theories are usually related to infinite data series, corresponding to an infinitely accurate precision and resolution for entropy evaluation. However, practical data are finite time series data, sampled with a sampling rate 
                           
                              
                                 T
                              
                              
                                 s
                              
                           
                         and characterized by limited resolution. The problem is that accurate estimation of the series entropy requires a big amount of data to be processed, and the results will be greatly influenced by the system noise. In 1967, Burg proposed a new approach to spectral estimation by attempting to derive a procedure for high resolution when only a small number of data of the estimates of an autocorrelation sequence are available [34,35]. This is called the maximum entropy method (MEM). MEM gives a highest frequency resolution compared to auto-correlation and covariance methods [36]. In addition, [37] and [38] showed that MEM is equivalent to the least-squares method for fitting an autoregressive (AR) model (or all-pole model) to the given data. MEM relates the entropy rate of a time series with its power spectral density (PSD). Hence, knowing the PSD of a series, allows us to know its entropy rate. Previous characterizations of the maximum entropy spectral density assume that the process is stationary and Gaussian [36,39]. Nonetheless, in economics and financial time series analysis literature there is not a theoretical consensus among researchers that price sequences should exhibit stationary Gaussian process. For example, in the financial literature prices of high volume-traded markets, (such as US markets), are considered to follow a random walk process in order to avoid any trade-off and to eliminate predictable patterns [40]. In the following, we will consider a stationary Gaussian time-series only to show that in this particular case we expect to obtain the maximum entropy (i.e. the entropy upper bound). Then, in our approach (depicted in Section 3) we will completely remove the stationary Gaussian hypothesis.

More in details, let 
                           x
                           (
                           1
                           )
                           ,
                           x
                           (
                           2
                           )
                           ,
                           …
                           ,
                           x
                           (
                           N
                           )
                         be a stationary Gaussian time series (of length N samples and with a sampling rate 
                           
                              
                                 T
                              
                              
                                 s
                              
                           
                        ) with autocovariance function 
                           
                              Cov
                           
                           (
                           k
                           )
                        , where 
                           k
                           =
                           −
                           N
                           ,
                           …
                           ,
                           +
                           N
                        . Then, if we denote with 
                           S
                           (
                           ω
                           )
                         the PSD of the Gaussian time series, the entropy rate, h (in the following referred as entropy), is given by the following [37]:
                           
                              (1)
                              
                                 
                                    h
                                    =
                                    
                                       1
                                       2
                                    
                                    ln
                                    ⁡
                                    (
                                    2
                                    ⋅
                                    π
                                    ⋅
                                    e
                                    )
                                    +
                                    
                                       1
                                       
                                          4
                                          ⋅
                                          π
                                       
                                    
                                    ⋅
                                    
                                       ∫
                                       
                                          −
                                          π
                                       
                                       π
                                    
                                    ln
                                    ⁡
                                    
                                       (
                                       2
                                       ⋅
                                       π
                                       ⋅
                                       S
                                       (
                                       ω
                                       )
                                       )
                                    
                                    ⋅
                                    d
                                    ω
                                 
                              
                           
                         where 
                           ln
                           ⁡
                           (
                           ⋅
                           )
                         is the natural logarithm. It is now interesting to underline that the entropy of a finite segment of a stochastic process is upper-bounded by the entropy of a segment of a Gaussian random process, according to (1). This means that a white time series is characterized by the maximum entropy, i.e. it is obviously unpredictable as also noted in [36]. Then, lower entropy values result in more predictable time series, while the value of entropy is not found to decrease for noise [22]. Hence, the entropy can be used as an indicator of the time series predictability.

In [41], the ApEn method is introduced to numerically quantify the entropy content of a finite time series, as a measure of the regularity of the series itself (see Fig. 1
                        ). The regularity of the series clearly reflects in the predictability of the series. The ApEn computations are conceptually simple and are based on the likelihood that templates in the time series which are similar remain similar on the next incremental comparisons. In other words, the presence of repetitive patterns of fluctuation in a time series renders it more predictable than a time series in which such patterns are absent. ApEn needs two input parameters to be specified in order to evaluate the approximate entropy of a given time series: a block or run length m, and a tolerance window r. Then, the ApEn procedure first measures the logarithmic frequency that runs of patterns that are close (within the tolerance window r) for m contiguous observations remain close (within the same tolerance r) on the next incremental comparison.

More in details, the ApEn algorithm can be formalized as follows (see [18] and [41] for a detailed analysis). A time series 
                           x
                           (
                           n
                           )
                         of length N (i.e. with 
                           n
                           =
                           1
                           ,
                           2
                           ,
                           …
                           ,
                           N
                        ) sampled at time intervals 
                           
                              
                                 T
                              
                              
                                 s
                              
                           
                         is considered. The length N can be also related to a time scale 
                           τ
                           =
                           N
                           ⋅
                           
                              
                                 T
                              
                              
                                 s
                              
                           
                        . Let us now select two m-dimensional sequence vectors, 
                           u
                           (
                           i
                           )
                         and 
                           v
                           (
                           j
                           )
                        , defined as follows:
                           
                              (2)
                              
                                 
                                    u
                                    (
                                    i
                                    )
                                    =
                                    
                                       {
                                       x
                                       (
                                       i
                                       )
                                       ,
                                       x
                                       (
                                       i
                                       +
                                       1
                                       )
                                       ,
                                       …
                                       ,
                                       x
                                       (
                                       i
                                       +
                                       m
                                       −
                                       1
                                       )
                                       }
                                    
                                 
                              
                           
                        
                        
                           
                              (3)
                              
                                 
                                    v
                                    (
                                    j
                                    )
                                    =
                                    
                                       {
                                       x
                                       (
                                       j
                                       )
                                       ,
                                       x
                                       (
                                       j
                                       +
                                       1
                                       )
                                       ,
                                       …
                                       ,
                                       x
                                       (
                                       j
                                       +
                                       m
                                       −
                                       1
                                       )
                                       }
                                    
                                 
                              
                           
                         with 
                           i
                           ≠
                           j
                        , i≥ 1, and 
                           j
                           ≤
                           N
                           −
                           m
                           +
                           1
                        . The distance between these two sequences is defined as:
                           
                              (4)
                              
                                 
                                    
                                       
                                          d
                                       
                                       
                                          u
                                          ,
                                          v
                                       
                                    
                                    (
                                    i
                                    ,
                                    j
                                    )
                                    =
                                    max
                                    ⁡
                                    
                                       {
                                       u
                                       (
                                       i
                                       +
                                       q
                                       )
                                       −
                                       v
                                       (
                                       j
                                       +
                                       q
                                       )
                                       }
                                    
                                 
                              
                           
                         where 
                           0
                           ≤
                           q
                           ≤
                           m
                           −
                           1
                        . If the distance expressed by (4) is smaller than a specified tolerance r, the two vectors are called similar. Then, for each of the 
                           N
                           −
                           m
                           +
                           1
                         vectors 
                           u
                           (
                           i
                           )
                        , the number of similar vectors 
                           v
                           (
                           j
                           )
                         is given by measuring their respective distances. Now, if 
                           
                              
                                 NV
                              
                              
                                 i
                              
                           
                         is the number of vectors 
                           v
                           (
                           j
                           )
                         similar to 
                           u
                           (
                           i
                           )
                        , then the relative frequency 
                           
                              
                                 f
                              
                              
                                 i
                              
                           
                           (
                           m
                           ,
                           r
                           ,
                           τ
                           )
                         to find a vector 
                           v
                           (
                           j
                           )
                         which is similar to 
                           u
                           (
                           i
                           )
                         within a tolerance level r and in the time scale τ, is given by:
                           
                              (5)
                              
                                 
                                    
                                       
                                          f
                                       
                                       
                                          i
                                       
                                    
                                    (
                                    m
                                    ,
                                    r
                                    ,
                                    τ
                                    )
                                    =
                                    
                                       
                                          
                                             NV
                                          
                                          
                                             i
                                          
                                       
                                       
                                          (
                                          N
                                          −
                                          m
                                          )
                                       
                                    
                                 
                              
                           
                         where (
                           N
                           −
                           m
                        ) is the number of vectors 
                           v
                           (
                           j
                           )
                           ≠
                           u
                           (
                           i
                           )
                         that are potentially similar to 
                           u
                           (
                           j
                           )
                        . Now, we look at the relative frequency of the logarithm of (5), defined as:
                           
                              (6)
                              
                                 
                                    ∅
                                    (
                                    m
                                    ,
                                    r
                                    ,
                                    τ
                                    )
                                    =
                                    
                                       1
                                       
                                          N
                                          −
                                          m
                                          +
                                          1
                                       
                                    
                                    ⋅
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          N
                                          −
                                          m
                                          +
                                          1
                                       
                                    
                                    ln
                                    ⁡
                                    
                                       [
                                       
                                          
                                             f
                                          
                                          
                                             i
                                          
                                       
                                       (
                                       m
                                       ,
                                       r
                                       ,
                                       τ
                                       )
                                       ]
                                    
                                 
                              
                           
                        
                     

Finally, the approximate entropy is estimated by the following statistics [31]:
                           
                              (7)
                              
                                 
                                    
                                       AE
                                    
                                    (
                                    m
                                    ,
                                    r
                                    ,
                                    τ
                                    )
                                    =
                                    
                                       1
                                       
                                          
                                             T
                                          
                                          
                                             s
                                          
                                       
                                    
                                    ⋅
                                    
                                       [
                                       ∅
                                       (
                                       m
                                       ,
                                       r
                                       ,
                                       τ
                                       )
                                       −
                                       ∅
                                       (
                                       m
                                       +
                                       1
                                       ,
                                       r
                                       ,
                                       τ
                                       )
                                       ]
                                    
                                 
                              
                           
                        
                     

Widely used values for the first two input parameters are 
                           m
                           =
                           1
                        , 
                           m
                           =
                           2
                        , and 
                           r
                           =
                           20
                           %
                         of the standard deviation of the analyzed time series. Recently, in [31] a modified version of the ApEn procedure, namely the Multiscale approximate entropy (MApEn), is introduced in order to overcome the aforementioned limitations of the original ApEn, In particular, the authors in [31] apply the MApEn algorithm to energy commodity markets, to characterize and monitor the dynamics of crude oil prices. They consider the entropy of a price time series as an index of the market complexity: high entropy values are related to less predictable market evolution (high complexity market). They evaluate the approximate entropy for different time-scales, performing low-pass filtering of the price difference dynamics. One main drawback of this method is that the low-pass filtering introduces correlation in the analyzed time series (and also changes the original data) so that the estimation of the approximate entropy is biased by this filtering operation and depends on the considered time-scale. For instance, a simple uncorrelated series should be always characterized by high entropy values. However, applying the MApEN method for high time scales results in decreasing the complexity of the random signal, since the low-pass filtering removes the most complex dynamics of the input time series (that now paradoxically exhibits a lower entropy value).

The novelty of our approach, namely the maximum entropy estimator (MEE), is that we now predict (ex-ante) the entropy of the next (future) time interval, instead of estimating (ex-post) the entropy of the observed series. In other words, the ApEn as well as the MApEn methods estimate the entropy of the observed series, hence making ex-post considerations about the predictability of the series itself. Conversely, our approach allows us to make some ex-ante considerations (based on the historical observed data) by predicting the entropy of the series in the next time interval. Moreover, the conventional methods implicitly assume the stationarity of the series, i.e. they assume that future values of the series are characterized by the same behavior observed in the past. Here, we completely remove this statement (i.e. we work with non-stationary signals), assuming that future values of the series are different from the observed ones, but can be predicted as a linear combination of these past values. Then, in full accordance with the ApEn-based methods, if the predicted entropy is high, this means that the series under investigation would be characterized by high unpredictability. On the contrary, if we estimate lower entropy values, the series would be characterized by low irregularities, hence resulting in a more predictive behavior for the series itself.

The starting point of our proposed signal processing method is to first obtain the predicted autocovariance sequence of the series in the next (unknown) time interval. Then, we can easily obtain its power spectral density, thus finally obtaining the searched entropy according to the MEM theory outlined in (1). In the case of our interest, future values of the predicted autocovariance sequence are obtained as an optimum linear combination of past (observed) autocovariance values. These values are weighted and linearly combined by means of a number p of prediction coefficients. The Levinson–Durbin recursion is used to solve the equations of the AR prediction coefficients that arise from the least-squares formulation [42]. The inputs of the optimum linear predictor are hence p autocovariance sequences.

According to the block scheme in Fig. 2
                        , we first divide the input series into a number (
                           K
                           +
                           1
                        ) of consecutive blocks. Then, we remove from each block the mean estimated from the previous block. This step is required in order to remove the deterministic components and highlight the innovation process of the series itself. In fact, we are working with financial series that are positive series (i.e. they are series of prices), and characterized by a (positive or negative) trend. If we consider a financial time series as a sequence of random observations, this random sequence, or stochastic process, may exhibit some degree of correlation from one observation to the next [43]. Exploiting the correlation structure allows us to decompose the time series into a deterministic component (expressed as a function of any information known at the previous time, including past innovations) and a random component (i.e., the uncertainty or the innovation). In the context of financial series, the random components are interpreted as the market innovations, and usually assumed to be Gaussian processes [44]. Therefore, we assume original observation of financial sequence (given data) model free, but contamined by a stationary Gaussian process as an error term (i.e. innovation term) into it. Hence, we estimate the deterministic components as the mean of the previous block, and then we subtract this mean from the next block. The first block is used only to evaluate its mean and then is discarded. Then, for the remaining K blocks (with 
                           K
                           ≥
                           p
                        ) the autocovariance sequences are computed. Only p autocovariance sequences over K (the most recent ones) are used as the inputs of the optimum linear predictor.

Finally, these p autocovariance sequences are weighted and linearly combined by the p optimum prediction coefficients, in order to obtain the predicted autocovariance sequence, and then the searched entropy. In the following subsection, the MEE signal processing algorithm is described in details.

Given a time series 
                           x
                           (
                           n
                           )
                         of length N samples, i.e. 
                           n
                           =
                           1
                           ,
                           2
                           ,
                           …
                           N
                        , the proposed MEE algorithm works accordingly to the following steps (see Fig. 2):
                           
                              1.
                              The N samples of 
                                    x
                                    (
                                    n
                                    )
                                  are divided in (
                                    K
                                    +
                                    1
                                 ) blocks, each of length 
                                    M
                                    =
                                    N
                                    /
                                    (
                                    K
                                    +
                                    1
                                    )
                                  samples.

The mean of each i-th block (with 
                                    i
                                    =
                                    0
                                    ,
                                    …
                                    ,
                                    K
                                 ) is then estimated according to the following:
                                    
                                       (8)
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         μ
                                                      
                                                      
                                                         ˆ
                                                      
                                                   
                                                
                                                
                                                   i
                                                
                                             
                                             =
                                             
                                                1
                                                M
                                             
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                M
                                             
                                             
                                                
                                                   x
                                                
                                                
                                                   i
                                                
                                             
                                             (
                                             j
                                             )
                                          
                                       
                                    
                                  where 
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    (
                                    j
                                    )
                                  stands for the j-th sample of the i-th block, with 
                                    j
                                    =
                                    1
                                    ,
                                    …
                                    ,
                                    M
                                 .

Now, starting from 
                                    i
                                    =
                                    1
                                 , the mean of the previous (
                                    (
                                    i
                                    −
                                    1
                                    )
                                 -th) block is subtracted from the current (i-th) block, according to the following:
                                    
                                       (9)
                                       
                                          
                                             
                                                
                                                   y
                                                
                                                
                                                   l
                                                
                                             
                                             (
                                             j
                                             )
                                             =
                                             
                                                
                                                   x
                                                
                                                
                                                   l
                                                
                                             
                                             (
                                             j
                                             )
                                             −
                                             
                                                
                                                   
                                                      
                                                         μ
                                                      
                                                      
                                                         ˆ
                                                      
                                                   
                                                
                                                
                                                   l
                                                   −
                                                   1
                                                
                                             
                                          
                                       
                                    
                                  where 
                                    l
                                    =
                                    1
                                    ,
                                    …
                                    ,
                                    K
                                  and again 
                                    j
                                    =
                                    1
                                    ,
                                    …
                                    ,
                                    M
                                 . Note that we have now only K blocks (not 
                                    K
                                    +
                                    1
                                 ), since the first block (that is the one that contains the oldest samples) is used to evaluate the mean to be used in the next block and then is discarded (see again Fig. 2). As explained before, this step is realized so that each block now contains only the market innovations with respect to the past.

Now, the autocovariance sequence of each l-th block is estimated according to the following:
                                    
                                       (10)
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         Cov
                                                      
                                                      
                                                         ˆ
                                                      
                                                   
                                                
                                                
                                                   l
                                                
                                             
                                             (
                                             k
                                             )
                                             =
                                             
                                                1
                                                M
                                             
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                M
                                             
                                             
                                                
                                                   y
                                                
                                                
                                                   l
                                                
                                             
                                             (
                                             j
                                             )
                                             ⋅
                                             
                                                
                                                   y
                                                
                                                
                                                   l
                                                
                                                
                                                   ⁎
                                                
                                             
                                             (
                                             j
                                             −
                                             k
                                             )
                                             −
                                             
                                                
                                                   |
                                                   
                                                      
                                                         
                                                            
                                                               α
                                                            
                                                            
                                                               ˆ
                                                            
                                                         
                                                      
                                                      
                                                         l
                                                      
                                                   
                                                   |
                                                
                                                
                                                   2
                                                
                                             
                                          
                                       
                                    
                                  where 
                                    k
                                    =
                                    0
                                    ,
                                    ±
                                    1
                                    ,
                                    …
                                    ,
                                    ±
                                    M
                                 , 
                                    
                                       
                                          y
                                       
                                       
                                          ⁎
                                       
                                    
                                    (
                                    
                                    )
                                  means complex conjugate, and 
                                    
                                       
                                          
                                             
                                                α
                                             
                                             
                                                ˆ
                                             
                                          
                                       
                                       
                                          l
                                       
                                    
                                  is the mean of the l-th block, estimated according to (8), with 
                                    
                                       
                                          y
                                       
                                       
                                          l
                                       
                                    
                                    (
                                    j
                                    )
                                  instead of 
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    (
                                    j
                                    )
                                 .

Finally, p autocovariance sequences (over K) become the input of the optimum linear predictor of parametric order p. The outputs of the optimum linear predictor are p prediction coefficients that are used to predict the autocovariance 
                                    
                                       
                                          
                                             
                                                Cov
                                             
                                             
                                                ˜
                                             
                                          
                                       
                                       
                                          K
                                          +
                                          1
                                       
                                    
                                    (
                                    k
                                    )
                                  of the next (future) block. This sequence is evaluated according to the following:
                                    
                                       (11)
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         Cov
                                                      
                                                      
                                                         ˜
                                                      
                                                   
                                                
                                                
                                                   K
                                                   +
                                                   1
                                                
                                             
                                             (
                                             k
                                             )
                                             =
                                             
                                                ∑
                                                
                                                   b
                                                   =
                                                   0
                                                
                                                
                                                   p
                                                   −
                                                   1
                                                
                                             
                                             
                                                
                                                   a
                                                
                                                
                                                   b
                                                
                                             
                                             ⋅
                                             
                                                
                                                   
                                                      
                                                         Cov
                                                      
                                                      
                                                         ˆ
                                                      
                                                   
                                                
                                                
                                                   K
                                                   −
                                                   b
                                                
                                             
                                             (
                                             k
                                             )
                                          
                                       
                                    
                                  where 
                                    
                                       
                                          
                                             
                                                Cov
                                             
                                             
                                                ˆ
                                             
                                          
                                       
                                       
                                          K
                                          −
                                          b
                                       
                                    
                                    (
                                    k
                                    )
                                  are the (
                                    K
                                    −
                                    b
                                 ) previously estimated autocovariance sequences, now linearly combined by a number p of AR coefficients 
                                    
                                       
                                          a
                                       
                                       
                                          b
                                       
                                    
                                 . The predicted autocovariance sequence expressed by (11) is transformed in the frequency domain obtaining the PSD of the analyzed block defined as:
                                    
                                       (12)
                                       
                                          
                                             S
                                             (
                                             ω
                                             )
                                             =
                                             
                                                ∑
                                                k
                                             
                                             
                                                
                                                   
                                                      
                                                         Cov
                                                      
                                                      
                                                         ˜
                                                      
                                                   
                                                
                                                
                                                   K
                                                   +
                                                   1
                                                
                                             
                                             (
                                             k
                                             )
                                             ⋅
                                             
                                                
                                                   e
                                                
                                                
                                                   −
                                                   j
                                                   ⋅
                                                   ω
                                                   ⋅
                                                   k
                                                
                                             
                                          
                                       
                                    
                                 
                              

Then, in full accordance with the MEM theory described in Section 2.1, the entropy (of the innovation process) of the (future) (
                           K
                           +
                           1
                        )-th block is estimated according to (1), where the PSD is now expressed by (12). In conclusion, if the entropy tends to lower values, the series is characterized by high predictivity (small innovations), otherwise the series is unpredictable (high innovations). Finally, it has to be noted that the Levinson–Durbin recursion is used to solve the equations of the auto-regressive (AR) prediction coefficients used in eq. (11). In practice, the p AR coefficients are chosen as the best coefficients that minimize the error term 
                           e
                           (
                           t
                           )
                        . More in details, 
                           e
                           (
                           t
                           )
                         is the innovation or error term concerning omitted variables or socio-political events which can or cannot be determined before the time t regarding its structure, i.e., 
                           e
                           (
                           t
                           )
                         can be an 
                           
                              AR
                           
                           (
                           k
                           )
                         process or a white noise, with constant mean and variance. The error term should be white since the optimum linear predictor acts as a whitening filter. But, since a small number of prediction coefficients are usually employed, the innovation term could not be white. In such a situation, we can consider the Said–Dickey method or nonlinearity such as chaos or conditional heteroskedasticity [45] to whitening the error term, or we can also increase the prediction order (i.e. use more prediction coefficients) as well. The order of the predictor (i.e. how much of the past story should be taken into account to evaluate the future) drastically affects the performance of the method, in terms of both computational complexity and accuracy of the prediction, as shown in Section 4 for some case studies of interest.

In order to evaluate the performance of the proposed predictor, we will analyze in the following some case studies, first theoretically evaluating the entropy of the input series, and then comparing this value with the one obtained by our MEE approach. In particular, we evaluate the mean and standard deviation of the entropy estimated through our method, varying some parameters of interest such as: the prediction order p, the number of blocks K, and the number of samples M per block. In addition, since the innovation process for financial series is usually assumed as a Gaussian process [see [37], and references therein], we have considered three cases of interest for the input series: (i) white Gaussian series; (ii) Gaussian AR filtered series, and (iii) Gaussian moving average (MA) filtered series, respectively. In all the following analysis, a series of length 
                        N
                        =
                        5000
                      has been considered, and a high number of Monte-Carlo simulation trials (103 independent runs) have been implemented to numerically evaluate the performance of our method so to assure that the set of samples is not arbitrary. Note that these series are auto-regressive moving average (ARMA) type, but can be auto-regressive integrated moving average (ARIMA) structure as well [46,47]. However, these two models are characterized by the same predicted entropy since the drift term (that characterizes the ARIMA structure) is completely predictable, and hence it adds no contribution in the evaluation of the entropy. Thus, in the following we focus only on ARMA series, without loss of generality.


                     (i) Entropy of white Gaussian series
                  

Let us now consider that the input series 
                        x
                        (
                        n
                        )
                      of length N samples is a white Gaussian series of mean 
                        
                           
                              M
                           
                           
                              x
                           
                        
                      and variance 
                        
                           
                              σ
                           
                           
                              x
                           
                           
                              2
                           
                        
                     . For the sake of the simplicity, we can consider 
                        
                           
                              M
                           
                           
                              x
                           
                        
                        =
                        0
                     , since the mean does not add contribution in the evaluation of the entropy (i.e. translations of a random variable have the same entropy as the untranslated random variable). From the direct application of the MEM theory, see (1), we can theoretically compute the entropy h of this white Gaussian series as a function of its variance. In particular, it is well known that the entropy of the white Gaussian series (of variance 
                        
                           
                              σ
                           
                           
                              x
                           
                           
                              2
                           
                        
                     ) can be written as [48]:
                        
                           (13)
                           
                              
                                 h
                                 =
                                 
                                    1
                                    2
                                 
                                 ln
                                 ⁡
                                 (
                                 2
                                 ⋅
                                 π
                                 ⋅
                                 e
                                 )
                                 +
                                 
                                    1
                                    2
                                 
                                 ⋅
                                 ln
                                 ⁡
                                 
                                    (
                                    2
                                    ⋅
                                    π
                                    ⋅
                                    
                                       
                                          σ
                                       
                                       
                                          x
                                       
                                       
                                          2
                                       
                                    
                                    )
                                 
                              
                           
                        
                     
                  

The mean and standard deviation of the entropy estimated according to our MEE approach are presented versus the prediction order, for a white Gaussian series of length 
                        N
                        =
                        5000
                      samples, and for several numbers of blocks in Fig. 3
                      and in Fig. 4
                     , respectively. In particular, Fig. 3 reports here the curves referring to the estimations obtained exploiting a number of blocks K equal to 5, 10, 20, and 50. For the sake of completeness, the value of the theoretical (i.e. true) entropy of the series is reported. It can be seen from this graph that the best estimates can be obtained with lower values of the prediction order (for all the considered curves). This is obviously true, since the series is a white Gaussian series, and hence, increasing the correlation between consecutive blocks (i.e. increasing the prediction order p) means decreasing the efficacy of the estimation. Finally, in Fig. 4 the standard deviation is reported, again versus the prediction order and for several values of the number of blocks. It is interesting to note that the standard deviation decreases, while the number of blocks used to estimate the entropy decreases. In fact, decreasing the number of blocks implies increasing the number of samples per block (with a fixed length of the series of 
                        N
                        =
                        5000
                      samples), and hence is equivalent to increase the accuracy of the estimation.


                     (ii) Entropy of Gaussian AR series
                  

Let us now consider that the input series is a Gaussian AR series of parametric order. For the sake of simplicity, and without loss of generality, we can assume in the following that the series under investigation is a first-order Gaussian AR series. In particular, the first order Gaussian AR series is obtained, in the case of our interest, as the output of a one-pole filter when the input is the white Gaussian series of the previous case. The only vinculum is that we consider that the Gaussian AR series has the same variance 
                        
                           
                              σ
                           
                           
                              x
                           
                           
                              2
                           
                        
                      of the input white Gaussian series. This means that the one-pole filter is defined as follows:
                        
                           (14)
                           
                              
                                 
                                    
                                       H
                                    
                                    
                                       AR
                                    
                                 
                                 (
                                 ω
                                 )
                                 =
                                 
                                    
                                       1
                                       −
                                       
                                          
                                             a
                                          
                                          
                                             2
                                          
                                       
                                    
                                    
                                       1
                                       −
                                       a
                                       ⋅
                                       
                                          
                                             e
                                          
                                          
                                             −
                                             j
                                             ⋅
                                             ω
                                          
                                       
                                    
                                 
                              
                           
                        
                      where a is the pole of the filter, with 
                        |
                        a
                        |
                        <
                        1
                     . Again, as depicted in [48], it is well known that the entropy of a first-order Gaussian AR series (of variance 
                        
                           
                              σ
                           
                           
                              x
                           
                           
                              2
                           
                        
                     ) is the same as expressed by (13). It is also evident that the entropy does not depend on the order of the Gaussian AR series, neither on the values of the poles. Therefore, even if it is well-known that an AR series whose characteristic polynome has 1 or more roots is classified as a nonstationary process, our algorithm is still efficient in predicting its entropy. In fact, the entropy rate of the Gaussian AR series is equal to the one of the Gaussian white series. Hence, linear filtering generates no change in the per unit time entropy of the process (if the variance of the output series is equal to that of the input series).

In Fig. 5
                     , the entropy estimated according to our MEE approach is presented versus the prediction order, for a first order Gaussian AR series of length 
                        N
                        =
                        5000
                      samples, and for several numbers of blocks. Again, the value of the theoretical (i.e. true) entropy of the series is also reported on the graph. It can be seen that the best entropy estimation can be obtained exploiting 
                        K
                        =
                        50
                      blocks, and a prediction order of 
                        p
                        =
                        10
                     . Then, Fig. 6
                      reports the standard deviation of these estimates versus the prediction order and for several values of the number of blocks. As before, the standard deviation decreases, decreasing the number of blocks used to estimate the entropy. Hence, the curve with 
                        K
                        =
                        50
                      is characterized by the highest standard deviation. Finally, it has to be noted that the standard deviation of each curve goes to zero, increasing the order of the prediction.


                     (iii) Entropy of Gaussian MA series
                  

Finally, let us now consider that the input series is again the white Gaussian series of the first case, but now filtered with a moving average (MA) filter. With the constraint that the variance of the output Gaussian MA series is the same of the input series, the MA filter (composed by 
                        2
                        S
                        +
                        1
                      samples) is hence defined as follows:
                        
                           (15)
                           
                              
                                 
                                    
                                       H
                                    
                                    
                                       MA
                                    
                                 
                                 (
                                 ω
                                 )
                                 =
                                 
                                    1
                                    
                                       
                                          2
                                          S
                                          +
                                          1
                                       
                                    
                                 
                                 
                                    
                                       sin
                                       ⁡
                                       [
                                       
                                          ω
                                          2
                                       
                                       (
                                       2
                                       S
                                       +
                                       1
                                       )
                                       ]
                                    
                                    
                                       sin
                                       ⁡
                                       (
                                       
                                          ω
                                          2
                                       
                                       )
                                    
                                 
                              
                           
                        
                     
                  

Again, since linear filtering provides no change in the entropy of the output process, the entropy of the Gaussian MA series is the same as in (13), and the result does not depend on the number of samples of the MA filter.

As before, Fig. 7
                      and Fig. 8
                      report the entropy estimated by our approach and the standard deviation of these estimates, respectively, versus the prediction order, for a white Gaussian series of length 
                        N
                        =
                        5000
                      samples, and for several numbers of blocks. Referring to Fig. 7 (where, as before, the curve of the true theoretical entropy is shown), the best estimates can be obtained exploiting the highest number of blocks (since the filtered series is characterized by high correlation between samples and consecutive blocks). This is witnessed by the fact that, increasing the prediction order, the accuracy of the estimation for each curve increases. Then, Fig. 8 depicts the standard deviation of the previous estimates, showing an analogous behavior as in the previous cases. In fact, the curve with 
                        K
                        =
                        50
                      is characterized by the highest standard deviation. Moreover, the standard deviation of each curve goes to zero, increasing the order of the prediction.

@&#RESULTS AND DISCUSSION@&#

Several simulation trials were performed to validate the proposed entropy prediction method for application to energy commodity market time series analysis. We have analyzed daily prices of several energy commodities in the period between May 20, 1991 and August 14, 2012. The analyzed dataset contains the following commodities types: two metals series (gold, silver), two grains (corn, oats), two soft commodities (cocoa, coffee) and two other agricultural commodities (feeder cattle, lean hogs) futures from the Chicago Board of Trade (CBOT), Chicago Mercantile Exchange (CME), Inter Continental Exchange (ICE), New York Mercantile Exchange (NYMEX), and its division Commodity Exchange (COMEX), summarized in Table 1
                     . The time series were obtained from http://www.quandl.com.

Then, we have applied both the ApEn metric and the MEE method in order to verify if the entropy (that is ex-post estimated by ApEn and ex-ante predicted by MEE) and its variations can be related to the series predictability. We have considered one year as composed by 256 observed samples (i.e. 256 business days). Then, starting from 1991, we have predicted the entropy of the next year (i.e. the entropy of the next 256 samples) by the MEE method. We have compared this predicted value with the ApEn evaluated over 512 samples, i.e. after observing all the next 256 samples. Then, we have iterated this process, by increasing the length of the series year by year (i.e. increasing the series by 256 samples each time). The ApEn method estimates the entropy ex-post (i.e. we need all the samples of that current year to estimate the entropy of that year), while the MEE approach predicts the entropy ex-ante (i.e. we only need the past samples to predict the entropy of the next time interval). Finally, we have normalized to one the entropy values obtained by these two methods in order to compare them on the same scale. In the following, we are going to show: (i) first, that some entropy peaks are strictly connected to some socio-political events, strongly affecting the diversity of the energy commodities under investigation; then, (ii) that our MEE method is characterized by a super-resolution in terms of entropy peaks detection, versus the conventional approach.

Because most financial analyses and modeling center on log price differences or returns, rather than on raw data or prices, we have applied our analysis on both the two sequences composed by returns and raw prices. However, Pinkus and Kalman in [18], when demonstrating the application of the ApEn method to financial analysis, stated that they “expect that application of ApEn to price series directly will prove useful in clarifying additional changes”. This is a consequence of the fact that working on returns implies that the original time-series is filtered (by a differentiator filter), and a correlation is introduced in the data. Therefore, the estimation of the entropy is biased by this filtering operation, since many irregularities of the original series are smoothed. Figs. 9
                     
                     (a, b) show the annual entropy variations obtained with both the conventional ApEn technique and the new MEE method for the two kinds of metal commodities (gold and silver). In particular, Figs. 9 report here the entropy estimated via the ApEn technique and obtained with 
                        m
                        =
                        1
                     , and a value of 
                        r
                        =
                        20
                        %
                      of the SD of the series. Moreover, the MEE method is also depicted, with 
                        p
                        =
                        10
                     . More in details, Fig. 9a refers to the application of the two methods to log price differences (returns), while Fig. 9b refers to the cases of prices (raw data). It is evident that the entropy methods yield very similar results in both the analyzed cases, but (as expected and noted by Pincus and Kalman in [18]) the entropy peaks reduce in dynamics and resolution when applied to log prices differences, because of the smoothing performed on the original data by the differentiator filter. Hence, the following analysis is then performed only on raw prices, since the same behavior has been observed in the presence of series of log price differences. Figs. 9(a, b) clearly show that, for the gold series, some peaks of the entropy pattern coincide with the outbreak of some major events. At the same time, we see that some other peaks occur just before or after these critical events. For example, we have entropy peaks in correspondence to the 1991 Gulf War, while other peaks occur just before or after some critical events, such as 2001-9-11 twin towers crash, the Lehman Brothers bankruptcy, and the Asian Financial Crisis. This suggests that major financial and socio-political events strongly affect the diversity of the gold market. On the other hand, in the case of the silver prices, the correspondence between the entropy peaks and these socio-political events is less evident. This is because, as also stated in [33], the silver prices series is characterized by a more unpredictable behavior than the gold series, meaning that the silver process series is less predictable than before. In fact, in the work by Kristoufek and Vosvrda, [33], the ApEn evaluated for the silver commodity, for the same observation period, is greater than the ApEn of the gold commodity series. The same analysis has been conducted also for other different commodities.

In particular, Fig. 10
                      shows the annual entropy variations for two kinds of softs commodities (cocoa and coffee), then Fig. 11
                      reports the case of grains commodities (corn and oats). Finally, the two other agricultural commodities (feeder cattle and lean hogs) are depicted in Fig. 12
                     . It is now interesting to highlight some considerations. Again, applying the MEE method, we can confirm the same results obtained in [33] via the ApEn approach. In fact, it is evident from Fig. 10 that the two softs commodities (cocoa and coffee) show an entropy variation that is more correlated with the major historical events in the case of the coffee commodities than in the case of the cocoa commodities. In other words, the coffee commodity series should be more predictable than the cocoa commodity series. Again, this behavior is in accordance with the results in [33], where it is shown that the ApEn of the coffee is lower (i.e. higher predictability) than the one of the cocoa. The same behavior can be observed in the case of the other commodities shown in Figs. 11–12, where grains and other agricultural commodities are taken into account. Hence, the conventional ApEn and the new MEE approaches allows us to correlate the entropy variations to some socio-political events. In particular, this correlation can be performed only ex-post by the conventional method, while in the MEE procedure this correlation can be performed ex-ante, by predicting the entropy of the next time-interval. In addition, it is also evident from Figs. 9–12 that the proposed MEE method can extrapolate the dependencies of the entropy variations on the major financial and socio-political events better than the conventional ApEn approach. In terms of signal processing, these two aspects mean that the two signals (i.e. the entropy variations obtained with the ApEn and the MEE methods) are characterized by the same trend (i.e. the same low-pass components). Conversely, they differently follow the dynamicity of the entropy variations (i.e. they have different high-pass components). The following quantitative analysis will confirm these assumptions. Let us now filter the curves referring to the conventional and new methods by an ideal low-pass FIR filter, characterized by a band of 1/5 of the total bandwidth. Then, let us evaluate the correlation coefficient between the low-pass filtered versions of these curves. The correlation coefficient between two series x and y is defined as follows:
                        
                           (16)
                           
                              
                                 
                                    
                                       ρ
                                    
                                    
                                       x
                                       ,
                                       y
                                    
                                 
                                 =
                                 
                                    
                                       
                                          Cov
                                       
                                       (
                                       x
                                       ,
                                       y
                                       )
                                    
                                    
                                       
                                          
                                             σ
                                          
                                          
                                             x
                                          
                                       
                                       ⋅
                                       
                                          
                                             σ
                                          
                                          
                                             y
                                          
                                       
                                    
                                 
                              
                           
                        
                      where 
                        
                           Cov
                        
                        (
                        x
                        ,
                        y
                        )
                      is the covariance function between the two series x and y, while 
                        
                           
                              σ
                           
                           
                              x
                           
                        
                      and 
                        
                           
                              σ
                           
                           
                              y
                           
                        
                      are the standard deviations of the two series x and y, respectively. Table 2
                      reports the correlation coefficient between the ApEn and the MEE approaches for the commodities under investigation. The results show that, for each commodity and as expected, the two methods present similar low-pass components, meaning that they exhibit the same trend. The most correlated commodities are the ones belonging to the softs and grains areas. However, the two methods are characterized by an overall correlation that is (on average) equal to 75%, stating that the method we propose is able to follow the entropy variations as the conventional ApEn procedure.

Then, in order to determine how these two methods dynamically follow the entropy variations, we have defined the dynamics gain as the ratio (expressed in percentage) between the energy of the high-pass filtered ApEn and MEE curves, where the current high-pass filter is the complementary filter the previous low-pass filter. We have called it dynamics gain because it quantifies how much of the total energy is filtered by the high-pass filter, hence the filtered curve that exhibits the highest energy is the method that is characterized by a greater dynamicity in its high-pass components (i.e. the method that has the greater bandwidth). Accordingly, we have evaluated the dynamics gain between the two approaches, and Table 2 reports here the dynamics gain of all the analyzed commodities. In particular, the overall (on average) dynamics gain is equal to 10%, meaning that our method has a resolution (in detecting entropy peaks in time) that is greater by a 10% than the conventional method. This behavior is much more evident for the gold commodity, and for the case of the feeder cattle series (see also Fig. 9 and Fig. 12).

In order to complete our empirical analysis, we have also focused on the relation between volatility and predicted entropy. In particular, we have focused on the analysis of two new time-series: the Standard and Poor's 500 Index (S&P500 index), and the Chicago Board Options Exchange Volatility Index (CBOE VIX®) [49]. The S&P500 index is a capitalization-weighted index of 500 stocks and it is designed to measure the performance of the broad domestic economy through changes in the aggregate market value of 500 stocks representing all major industries. Conversely, the CBOE volatility index is a leading measure of market expectations of near-term volatility conveyed by the S&P500 index option prices. Fig. 13
                      reports the daily closing values of these indexes, from January, 1993 to December, 2013. Low and high volatility periods are evident from the graph, with a maximum volatility peak in correspondence to November, 2008. Conversely, the S&P500 index tends to a minimum in correspondence to the same period. Then, Fig. 14
                      shows the corresponding entropy values evaluated with the MEE and ApEn approaches for both the two aforementioned indexes. Regarding the relation between volatility and predicted entropy, it is interesting to note that, in full accordance with [29], the predicted entropy falls to minimum values in correspondence to high volatility regimes. In particular, the entropy predicted exploiting the CBOE index drops to its minimum value in the proximity of November, 2008 (i.e. when the volatility is at its maximum). Conversely, when the volatility index reaches its minimum levels (see for example the 2006 period in Fig. 13), it is interesting to underline that the predicted entropy reaches a maximum (see the maximum corresponding to the Iraq war- Asian growth of 2006 in Fig. 14). The same happens within the 1995 low volatility period and again around 2009 (see again Fig. 13), where we observe corresponding peaks in the predicted entropy (see again Fig. 14). It is in fact shown that the annual entropy variations predicted by the MEE approach is much more correlated with the major historical events than in the case of the entropy evaluated via the ApEn approach, meaning that our method can be usefully applied for a fruitful entropy-based analysis of the financial and commodity market time series.

@&#CONCLUSION@&#

This paper has devised a novel signal processing technique for application to financial engineering to assess the predictability of financial time series based on the maximum entropy method. The theoretical results, substantiated by simulation, have evidenced the performance of the proposed predictor in the presence of white, AR, and MA Gaussian random series. We have finally applied our method to real sets of financial data, matching the performance of the conventional econometric approach (i.e. the approximate entropy method). The obtained outcomes evidence the effectiveness of the proposed method to resolve entropy peaks that are strictly correlated to contemporary socio-political events.

@&#REFERENCES@&#

