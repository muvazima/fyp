@&#MAIN-TITLE@&#Visual Topic Network: Building better image representations for images in social media

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A network structured topic model, i.e., VTN is proposed for image representations.


                        
                        
                           
                           The loosely related tags can be efficiently explored in our method.


                        
                        
                           
                           A collection of images is organized as a network where image relations are modeled.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Image representation

Topic model

Visual Topic Network

Social media

@&#ABSTRACT@&#


               
               
                  Topic models have demonstrated to be effective on building image representations for general images. Recently, how to build better image representations for images in social media becomes an interesting problem, where one key issue is how to leverage images’ social contextual cues, e.g., user tags associated with images. Nevertheless, most previous methods either just exploited image content and neglect user tags, or assumed there are exact correspondences between image content and tags, i.e., tags are closely related to image content. Thus, they cannot be applied to the realistic scenarios where the images are only weakly annotated with tags, i.e., tags are only loosely related to image content as already manifested in real-world social media data. In this paper, we address the problem of building better image representations in social media, where the images are weakly annotated with user tags. In particular, we organize a collection of images as an image network where the relations between images are modeled by user tags. To model such image network and build image representations, we further propose a network structured topic model, namely Visual Topic Network (VTN), where the image content and their relations are simultaneously modeled. In this way, the weakly annotated tags can be effectively leveraged as building image representations. The proposed VTN model is inspired by the Relational Topic Model (RTM) recently introduced in the document analysis literature. Different from the binary article relations in RTM, the proposed VTN can model the multiple-level image relations. Our extensive experiments on two social media datasets demonstrated the advantage of the proposed VTN model.
               
            

@&#INTRODUCTION@&#

With the advancement of research on local image features such as SIFT [1], many works focus on building better image representations based on image local features. For example, the bag-of-words (BoW) representation [2–6] have been very popular. Typically, once local features are extracted and quantized, their overall distribution in an image is represented in terms of bag-of-words histograms.

Although the BoW representation is simple to build, the visual words are difficult to interpret, i.e., the ‘polysemy’ and ‘synonymy’ issue [7]: a visual word may represent different image visual content and several visual words may represent the same image visual content. To alleviate such an issue, many works try to build a hierarchical image representation, where the intermediate level representation is more interpretable than the low level visual words. For example, an scene image can be described by a three-level hierarchical structure: a scene (e.g., ‘street’) – scene elements (e.g., ‘sky’, ‘road’) – image patches. The scene elements are more interpretable than the image patches.

To exploit such multi-level hierarchy, topic models have been introduced for image representation and recognition [8–12]. In topic models, each image is represented as a mixture of ‘topics’, and each topic is described by a distribution over visual words. Through topic modeling, some topics could be learnt from a collection of images, and usually they are easily interpretable, i.e., the visual words representing the same image content are usually ‘clustered’ into one topic. Therefore, the mixture of topics can be regard as an intermediate level image representation, which is not only more interpretable but also has lower dimension compared to the BoW representation. The typical topic models introduced for visual recognition are the probabilistic Latent Semantic Analysis (pLSA) [13] and the Latent Dirichlet Allocation (LDA) [14]. For example, Fei-Fei and Bosch [9,8] exploited LDA and pLSA for scene recognition. Sudderth et al. [10] presented a hierarchical topic model for part-based object and scene category recognition.

Recently, with the popularity of social networks (e.g., Facebook) and content-sharing websites (e.g., Flickr and YouTube), images on social networks or content-sharing websites, called social images, become the dominating multimedia objects on the Internet. And how to build better image representations for such social images becomes more and more important for social media analysis. Different from general images, social images are often accompanied by various forms of contextual information like keywords, tags, comments, as well as surrounding texts. Such contextual cues are usually beneficial for understanding the image content. Thus, one key issue for social image understanding is how to leverage those contextual cues. However, the aforementioned works focused on modeling image content and neglecting such contextual cues.

One kind of the most leveraged contextual cues is the keyword associated with images. To leverage the associated keywords for social image understanding, some methods focused on modeling the joint distribution of visual features and keywords [15–19]. In [15,16], the process of building the relationship between the visual features and the keywords was analogous to a language translation. Further, Corr-LDA was proposed to extend this approach through a hierarchical probabilistic mixture model [17,18]. In [20], tr-mmLDA was proposed to capture correlations between images and annotation texts, where the correlations between the two data modalities were modeled with a linear Gaussian regression module. Recently, image recognition and annotation are simultaneously considered in [19] by modeling the joint distribution of image content, annotation keywords, and class labels.

However, these methods assume that there are explicit correspondences between the keywords and image content. Thus, they can only be applied to the case where all the keywords have a visual interpretation rather than the realistic scenarios where the images are weakly annotated with tags, i.e., tags are usually loosely related to the image content compared to keywords 
                     [21]. For example, a photo for the ‘Lincoln Memorial’ could have a tag ‘National Mall’ since the Lincoln Memorial is located at the National Mall street, but there does not exist a correspondence between the photo content and such a tag.

Recently, an image representation learning method [22,23] was proposed to leverage those loosely related tags in a new way. Specifically, a multimedia information network (MINets) was constructed with images and tags. And then the image similarity and the image-tag relationship were described by relational matrices. At last, the learning of image representation was formulated as a low-rank matrix approximation problem. Similarly, in [24] an image network was constructed with images and their social-network metadata, and a discriminative method was proposed to model such image network for image recognition. Although those social-network metadata (e.g., user tags, the comments, the groups to which those images belong, the uploaders of those images, etc.) are loosely related to image content, the experimental results reveal that they can still be effectively leveraged for understanding image content through such a model. So, these works suggest that the loosely related tags or loosely related metadata can be effectively leveraged by organizing them as a network.

Inspired by it, in this paper we make use of these loosely related tags by constructing an image network. Moreover, by proposing a topic model to model such image network, we build a hierarchical image representation. As shown in Fig. 1
                     , our method consists of the following steps: firstly, each image is represented as a visual document by using a bag-of-words representation, meanwhile user tags are leveraged to define the relations between each pair of images. As a result, a Visual document Network (VN) is constructed where the nodes represent images and links represent image relations. Secondly, a network structured topic model, namely Visual Topical Network (VTN), is proposed to model the VN and build an intermediate level image representation.

In this paper, instead of directly modeling these loosely related tags, we leverage them to describe image relations. For example, if two images share more common tags, we define that they have a strong relation which indicates they are more likely to have similar representations. Otherwise, we define that they have a weak relation which indicates they are not likely to have similar representations. In such a way, these tags can still be adequately explored although they are only loosely related to image content. For example, two photos about ‘Lincoln Memorial’ are both tagged with a common tag, e.g., ‘National Mall’. Although tag ‘National Mall’ is loosely related to the image content ‘Lincoln Memorial’, the positive image relation is still beneficial for understanding image content.

To model the constructed image network, a network structured topic model VTN is proposed in this paper. In particular, the proposed VTN is inspired by the idea of the Relational Topic Model (RTM) proposed by Chang and Blei [25]. The RTM is designed to model a text document network formed according to the citation relationships among scientific articles.

However, in [25] the document citation is described by a binary variable (i.e., with or without citation between two articles). But for our task, the relations among images are much more complicated. In particular, the image relations are defined as the correlation of the two tag sets, e.g., how many tags are shared between two images. Hence it is better to describe such relations by multiple-valued variables. And subsequently, a new distribution is needed to describe such multiple-valued variables. These are the major differences between the RTM and the proposed VTN model.

Obviously, our two-stage method has such an advantage - the stage of constructing network are separated from the stage of topic modeling. By doing so, we have more flexibility for exploiting image content and contextual information at the first stage. In particular, we can choose certain local features (e.g., SIFT, HOG, etc.) for a specific image type (e.g., optical image, infrared image, etc.). In addition, we can choose certain contextual cues and define certain kinds of image relations for a specific application. For example, we can choose the information of users uploading an image as contextual cues, and define the image relations by leveraging users’ social connections.

Moreover, the proposed VTN model in this paper is a general network structured topic model, which does not depend on specific contextual cues or local features, and can be used in other domains such as document analysis and social network analysis.

It is noticed that the proposed VTN is an unsupervised model for building image representations. To conduct the image recognition, we need to train another general classifier such as an SVM classifier on top of those built image representations. However, the processes of building image representations and training a classifier are optimized separately. To optimize them jointly, some orthogonal research works focused on developing supervised topic models [19,26]. The most related one is [26], where a semi-supervised topic model, called ss-RTM, was proposed to model image content, user tags and image labels simultaneously. Similar to RTM model, ss-RTM only coarsely describes image relations with binary variables. Therefore, at the Section 5.7 we describe how to incorporate the idea of the proposed VTN into the ss-RTM to improve its performance further.

In summary, our contributions are three folds:
                        
                           1.
                           A new method of learning hierarchical image representations for social images, where the loosely related tags can be effectively leveraged as building image representations.

A measurement for the correlation of the two tag sets is proposed. Based on it, the image relations are defined, and a collection of images is organized as an image network.

A network structured topic model, i.e., VTN, is proposed to model such image network, where the strength of image relations is properly modeled by a new link probability function.

@&#RELATED WORKS@&#

In the domain of document analysis, by considering the relationships among documents (e.g., the citation or co-authorship among documents), a collection of documents can be organized as a document network. There have been several topic models, dubbed topic model for document network, proposed to model such a document network [27,25,28]. As we have discussed, Chang and Blei [25] proposed the RTM to model a network of scientific articles, where the relation of two articles is defined as their citation relationship. Besides that, Cai et al. [27] tackled the problem of topic modeling within a network structure (TMN), where the co-authorship between documents were explored to construct the document network. A unified framework is proposed to combine statistical topic modeling with a discrete regularizer defined by the network structure. Motivated by the observation that a link between two articles is not only determined by its content similarity, but also affected by the community ties between the authors, Liu and Niculescu-Mizil [28] proposed Topic-Link LDA model for author community discovery in a unified framework. These works have largely inspired our proposed method. However, these existing topic models cannot be directly employed for our task since there are some special characteristics for images compared with text documents. And we need to design a proper topic model for our task.

In the domain of multimedia analysis, some topic models were also developed to jointly model image content and user tags. In [15], variations of topic models were proposed to learn the joint distribution of image regions and words, and were used for image annotation and region naming. Sizov [29] proposed the GeoFolk model for multi-modal characterization of social media by combining text features (e.g., tags as a prominent example of short, unstructured text labels) with spatial knowledge (e.g., geotags and coordinates of images and videos). GeoFolk can be seen as an extension of LDA, which explicitly describes geo-data jointly with tag co-occurrence patterns. Yang et al. [30] extracted visual words from the images and additional tags from the social web in order to build their annotation-based pLSA (aPLSA), where aPLSA was a combination of two separate pLSA models, one for image to visual feature co-occurrence matrix and another for text feature to visual feature co-occurrence matrix.

However, such methods did not model the image content and user tags in a structured way, and those proposed topic models are dependent on specific applications and visual/textual features. In contrast, the proposed method organizes such information with a network structure, and the proposed topic model is general and independent of specific visual/textual features.

Another line of work has focused on fusing the image content and user tags for image recognition in a discriminative way rather than building better image representations. For example, user tags were used as additional textual features and concatenated with visual features to train an SVM classifier for the recognition of touristic landmarks in [31]. In [32], two separate classifiers were built, one from the user tags, and the other one from the visual features. Then a third classifier was trained to combine the confidence values of these two different classifiers for the final prediction. To deal with the situation that only some of images were annotated with user tags, Guillaumin et al. [33] proposed a semi-supervised learning algorithm to explore both annotated and un-annotated images for image recognition, where the visual and textual features were combined under the Multiple Kernel Learning (MKL) framework.

However, these methods either take a pre-fusion at the feature level (i.e., the visual and textual features are concatenated together) or a post-fusion at the decision level (i.e., the classification scores from the two different modalities are combined) to fuse the image content and user tags. Moreover, these discriminative methods cannot build an image representation. In contrast, the proposed method can fuse the image content and user tags as building hierarchical image representations. So they are fused at a intermediate level, i.e., topic level, which could better exploit their latent relationship.

The Visual document Network (VN) is constructed as follows: the image content are encoded as nodes in the VN, and the user tags are encoded as links in the VN. In particular, we extract local features (e.g., Dense-SIFT) [1] for each image. By means of bag-of-words representation, each local feature is encoded as a visual word. As a result, each image is encoded as a visual document (e.g., a set of visual words).

In this paper, our task is to build better image representations for image category recognition, so two images from a same image category should have similar representations, and hence they should have a strong relation. On the other hand, it is obvious that two images from a same image category usually have correlated user tags – their tag sets are closely related to each other (e.g., the two images may have common tags). Therefore, the relation between two images in this paper is defined in terms of correlation between two corresponding tag sets. In particular, each relation between two images is described by a link between two nodes, and the strength of the relation is described by the weight of the link which is set as the correlation strength of two corresponding tag sets, as shown in Fig. 2
                     .

We propose two schemes to measure the correlation strength of the two tag sets: the first one calculates the correlation strength by simply counting the number of shared tags; the second one measures it by considering both the number of shared tags and the inter-tag relevance.

Suppose we have a total of M different tags for all images, the tag set of the image d is denoted as a binary vector 
                        
                           
                              
                                 y
                              
                              
                                 d
                              
                           
                           =
                           
                              
                                 [
                                 
                                    
                                       y
                                    
                                    
                                       d
                                       ,
                                       1
                                    
                                 
                                 ,
                                 
                                    
                                       y
                                    
                                    
                                       d
                                       ,
                                       2
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    
                                       y
                                    
                                    
                                       d
                                       ,
                                       M
                                    
                                 
                                 ]
                              
                              
                                 T
                              
                           
                        
                     , where each element 
                        
                           
                              
                                 y
                              
                              
                                 d
                                 ,
                                 m
                              
                           
                        
                      indicates whether the tag 
                        
                           
                              
                                 t
                              
                              
                                 m
                              
                           
                        
                      is present in its tag set or not. For the first scheme, the link weight between the image d and 
                        
                           
                              
                                 d
                              
                              
                                 ′
                              
                           
                        
                      is evaluated as
                        
                           (1)
                           
                              
                                 
                                    l
                                 
                                 
                                    d
                                    ,
                                    
                                       
                                          d
                                       
                                       
                                          ′
                                       
                                    
                                 
                              
                              =
                              
                                 
                                    y
                                 
                                 
                                    d
                                 
                                 
                                    T
                                 
                              
                              
                                 
                                    y
                                 
                                 
                                    
                                       
                                          d
                                       
                                       
                                          ′
                                       
                                    
                                 
                              
                              ,
                           
                        
                     which indicates the number of shared tags between the image d and 
                        
                           
                              
                                 d
                              
                              
                                 ′
                              
                           
                        
                     .

However, the number of shared tags may not properly describe the correlation strength between the two tag sets, since it treats all tags independently without considering their inter-tag relevance [34]. For example, it will give zero correlation between two images that do not share any common tag, but these images can still have a semantic correlation if their tags are different but with close semantic meanings, such as dog and cat, both of them are pets. To better exploit the inter-tag relevance, we introduce the tag relevance matrix 
                        
                           R
                        
                     , in which each element 
                        
                           
                              
                                 r
                              
                              
                                 kl
                              
                           
                        
                      indicates the relevance between tags 
                        
                           
                              
                                 t
                              
                              
                                 k
                              
                           
                        
                      and 
                        
                           
                              
                                 t
                              
                              
                                 l
                              
                           
                        
                     .

Similar to [35], the relevance 
                        
                           
                              
                                 r
                              
                              
                                 kl
                              
                           
                        
                      between tags 
                        
                           
                              
                                 t
                              
                              
                                 k
                              
                           
                        
                      and 
                        
                           
                              
                                 t
                              
                              
                                 l
                              
                           
                        
                      is measured based on their Normalized Google Distance 
                        
                           NGD
                           (
                           
                              
                                 t
                              
                              
                                 k
                              
                           
                           ,
                           
                              
                                 t
                              
                              
                                 l
                              
                           
                           )
                        
                      
                     [36],
                        
                           (2)
                           
                              
                                 
                                    r
                                 
                                 
                                    kl
                                 
                              
                              =
                              
                                 
                                    e
                                 
                                 
                                    -
                                    NGD
                                    (
                                    
                                       
                                          t
                                       
                                       
                                          k
                                       
                                    
                                    ,
                                    
                                       
                                          t
                                       
                                       
                                          l
                                       
                                    
                                    )
                                 
                              
                           
                        
                     
                  

In particular, the Normalized Google Distance between tags 
                        
                           
                              
                                 t
                              
                              
                                 k
                              
                           
                        
                      and 
                        
                           
                              
                                 t
                              
                              
                                 l
                              
                           
                        
                      is defined as
                        
                           (3)
                           
                              NGD
                              (
                              
                                 
                                    t
                                 
                                 
                                    k
                                 
                              
                              ,
                              
                                 
                                    t
                                 
                                 
                                    l
                                 
                              
                              )
                              =
                              
                                 
                                    
                                       max
                                    
                                    (
                                    log
                                    f
                                    (
                                    
                                       
                                          t
                                       
                                       
                                          k
                                       
                                    
                                    )
                                    ,
                                    log
                                    f
                                    (
                                    
                                       
                                          t
                                       
                                       
                                          l
                                       
                                    
                                    )
                                    )
                                    -
                                    log
                                    f
                                    (
                                    
                                       
                                          t
                                       
                                       
                                          k
                                       
                                    
                                    ,
                                    
                                       
                                          t
                                       
                                       
                                          l
                                       
                                    
                                    )
                                 
                                 
                                    log
                                    G
                                    -
                                    min
                                    (
                                    log
                                    f
                                    (
                                    
                                       
                                          t
                                       
                                       
                                          k
                                       
                                    
                                    )
                                    ,
                                    log
                                    f
                                    (
                                    
                                       
                                          t
                                       
                                       
                                          l
                                       
                                    
                                    )
                                    )
                                 
                              
                              ,
                           
                        
                     where 
                        
                           f
                           (
                           
                              
                                 t
                              
                              
                                 k
                              
                           
                           )
                        
                      and 
                        
                           f
                           (
                           
                              
                                 t
                              
                              
                                 l
                              
                           
                           )
                        
                      are the numbers of images containing tag 
                        
                           
                              
                                 t
                              
                              
                                 k
                              
                           
                        
                      and tag 
                        
                           
                              
                                 t
                              
                              
                                 l
                              
                           
                        
                      respectively and 
                        
                           f
                           (
                           
                              
                                 t
                              
                              
                                 k
                              
                           
                           ,
                           
                              
                                 t
                              
                              
                                 l
                              
                           
                           )
                        
                      is the number of images containing both 
                        
                           
                              
                                 t
                              
                              
                                 k
                              
                           
                        
                      and 
                        
                           
                              
                                 t
                              
                              
                                 l
                              
                           
                        
                     . And G is the total number of images. Generally, the inter-tag relevance 
                        
                           R
                        
                      is a kind of prior knowledge, and it should be estimated on a large social media dataset.

Therefore, for the second scheme we define the link weight between two images as a weighted dot product, i.e.,
                     
                        
                           (4)
                           
                              
                                 
                                    l
                                 
                                 
                                    d
                                    ,
                                    
                                       
                                          d
                                       
                                       
                                          ′
                                       
                                    
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          y
                                       
                                       
                                          ¯
                                       
                                    
                                 
                                 
                                    d
                                 
                              
                              R
                              
                                 
                                    
                                       
                                          y
                                       
                                       
                                          ¯
                                       
                                    
                                 
                                 
                                    
                                       
                                          d
                                       
                                       
                                          ′
                                       
                                    
                                 
                              
                              ,
                           
                        
                     where 
                        
                           
                              
                                 
                                    
                                       y
                                    
                                    
                                       ¯
                                    
                                 
                              
                              
                                 d
                              
                           
                        
                      is the normalization of 
                        
                           
                              
                                 y
                              
                              
                                 d
                              
                           
                        
                     , i.e., 
                     
                        
                           
                              
                                 
                                    
                                       y
                                    
                                    
                                       ¯
                                    
                                 
                              
                              
                                 d
                                 ,
                                 m
                              
                           
                           =
                           
                              
                                 y
                              
                              
                                 d
                                 ,
                                 m
                              
                           
                           /
                           
                              
                                 ∑
                              
                              
                                 i
                                 =
                                 1
                              
                              
                                 M
                              
                           
                           
                              
                                 y
                              
                              
                                 d
                                 ,
                                 i
                              
                           
                        
                     .

As aforementioned, the larger the social media dataset is used, the more accurate the inter-tag relevance 
                        
                           R
                        
                      can be estimated. In practice, 
                        
                           R
                        
                      is approximately estimated with the NUS-WIDE dataset in this paper. As a result, the correlation strength of tag sets, the 
                        
                           
                              
                                 l
                              
                              
                                 d
                                 ,
                                 
                                    
                                       d
                                    
                                    
                                       ′
                                    
                                 
                              
                           
                        
                      computed with Eq. (4), has a certain computational noise. With some empirical analysis, we have observed that the value of 
                        
                           
                              
                                 l
                              
                              
                                 d
                                 ,
                                 
                                    
                                       d
                                    
                                    
                                       ′
                                    
                                 
                              
                           
                        
                      is coarsely (rather than exactly linearly) related to the real correlation strength of images. To be tolerant to this kind of noises, the two links can be regarded to have different correlation strength only if the difference of their 
                        
                           
                              
                                 l
                              
                              
                                 d
                                 ,
                                 
                                    
                                       d
                                    
                                    
                                       ′
                                    
                                 
                              
                           
                        
                      is larger than a quantized threshold, i.e., we need to quantize the continuous variable 
                        
                           
                              
                                 l
                              
                              
                                 d
                                 ,
                                 
                                    
                                       d
                                    
                                    
                                       ′
                                    
                                 
                              
                           
                        
                      into multiple discrete levels. In particular, if we want to quantize the correlation strength into 
                        
                           W
                           +
                           1
                        
                      levels, we can equally quantize and linearly map the value of Eq. (4) from the interval 
                        
                           [
                           0
                           ,
                           1
                           ]
                        
                      into the discrete value 
                        
                           l
                           =
                           0
                           ,
                           1
                           ,
                           …
                           ,
                           W
                        
                     . Moreover, we need to choose a good quantization parameter W to make a trade-off between obtaining benefits from modeling the multi-level link strength and be tolerant to the computational noise.

After constructing a VN, in this section we present a network structured topic model, namely Visual Topic Network (VTN), to model the constructed VN. Considering the success of the RTM [25] in the domain of document analysis, we introduce it from document analysis domain to computer vision domain, and make some modifications and extensions so as to tailor it for our task.

The VTN is a generative probabilistic model that uses a set of ‘topics’ to describe an image network. Particularly, each image (i.e., a visual document) is described by a topic mixture (i.e., a distribution over topics), and each topic is described by a distribution over visual words. In its generative process, for each image, a topic mixture is generated from a Dirichlet distribution. Then, each visual word of the image is generated by first drawing a topic assignment according to the topic mixture, and then drawing the visual word from the corresponding topic distribution. Meanwhile, each image is generated according to their topic mixtures.

In particular, for an image network with D images, each image d is described by a visual document 
                           
                              
                                 
                                    w
                                 
                                 
                                    d
                                 
                              
                              =
                              {
                              
                                 
                                    w
                                 
                                 
                                    d
                                    ,
                                    1
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    w
                                 
                                 
                                    d
                                    ,
                                    
                                       
                                          N
                                       
                                       
                                          d
                                       
                                    
                                 
                              
                              }
                           
                        , where each visual word 
                           
                              
                                 
                                    w
                                 
                                 
                                    d
                                    ,
                                    n
                                 
                              
                           
                         corresponds to an image patch. And each 
                           
                              
                                 
                                    w
                                 
                                 
                                    d
                                    ,
                                    n
                                 
                              
                           
                         has a corresponding topic assignment 
                           
                              
                                 
                                    z
                                 
                                 
                                    d
                                    ,
                                    n
                                 
                              
                           
                         to indicate which topic it belongs to. Moveover, the link between two images d and 
                           
                              
                                 
                                    d
                                 
                                 
                                    ′
                                 
                              
                           
                         is described by a link variable 
                           
                              
                                 
                                    l
                                 
                                 
                                    d
                                    ,
                                    
                                       
                                          d
                                       
                                       
                                          ′
                                       
                                    
                                 
                              
                           
                         (i.e., the correlation between two tag sets which is measured according to Eq. (1) or Eq. (4)).

The VTN assumes that a set of observed visual documents 
                           
                              
                                 
                                    {
                                    
                                       
                                          w
                                       
                                       
                                          d
                                       
                                    
                                    }
                                 
                                 
                                    d
                                    =
                                    1
                                 
                                 
                                    D
                                 
                              
                           
                         and some observed links 
                           
                              
                                 
                                    {
                                    
                                       
                                          l
                                       
                                       
                                          d
                                          ,
                                          
                                             
                                                d
                                             
                                             
                                                ′
                                             
                                          
                                       
                                    
                                    }
                                 
                                 
                                    d
                                    ,
                                    
                                       
                                          d
                                       
                                       
                                          ′
                                       
                                    
                                    ∈
                                    D
                                 
                              
                           
                         are generated by the following process:
                           
                              1.
                              For each topic k:
                                    
                                       (a)
                                       Draw topic distribution over the visual vocabulary 
                                             
                                                
                                                   
                                                      ϕ
                                                   
                                                   
                                                      k
                                                   
                                                
                                                ∼
                                                Dir
                                                (
                                                β
                                                )
                                             
                                          .

For each visual document d:
                                    
                                       (a)
                                       Draw topic proportions 
                                             
                                                
                                                   
                                                      θ
                                                   
                                                   
                                                      d
                                                   
                                                
                                                ∼
                                                Dir
                                                (
                                                α
                                                )
                                             
                                          .

For each visual word 
                                             
                                                
                                                   
                                                      w
                                                   
                                                   
                                                      d
                                                      ,
                                                      n
                                                   
                                                
                                             
                                          :
                                             
                                                i.
                                                Select a topic 
                                                      
                                                         
                                                            
                                                               z
                                                            
                                                            
                                                               d
                                                               ,
                                                               n
                                                            
                                                         
                                                         ∼
                                                         Multi
                                                         (
                                                         
                                                            
                                                               θ
                                                            
                                                            
                                                               d
                                                            
                                                         
                                                         )
                                                      
                                                   .

Draw a visual word 
                                                      
                                                         
                                                            
                                                               w
                                                            
                                                            
                                                               d
                                                               ,
                                                               n
                                                            
                                                         
                                                         ∼
                                                         Multi
                                                         (
                                                         
                                                            
                                                               ϕ
                                                            
                                                            
                                                               
                                                                  
                                                                     z
                                                                  
                                                                  
                                                                     d
                                                                     ,
                                                                     n
                                                                  
                                                               
                                                            
                                                         
                                                         )
                                                      
                                                   .

For each pair of visual documents 
                                    
                                       d
                                       ,
                                       
                                          
                                             d
                                          
                                          
                                             ′
                                          
                                       
                                    
                                 :
                                    
                                       (a)
                                       Draw a link with weight 
                                             
                                                
                                                   
                                                      l
                                                   
                                                   
                                                      d
                                                      ,
                                                      
                                                         
                                                            d
                                                         
                                                         
                                                            ′
                                                         
                                                      
                                                   
                                                
                                             
                                          : 
                                             
                                                
                                                   
                                                      l
                                                   
                                                   
                                                      d
                                                      ,
                                                      
                                                         
                                                            d
                                                         
                                                         
                                                            ′
                                                         
                                                      
                                                   
                                                
                                                ∣
                                                
                                                   
                                                      z
                                                   
                                                   
                                                      d
                                                   
                                                
                                                ,
                                                
                                                   
                                                      z
                                                   
                                                   
                                                      
                                                         
                                                            d
                                                         
                                                         
                                                            ′
                                                         
                                                      
                                                   
                                                
                                                ∼
                                                ψ
                                                (
                                                
                                                   
                                                      l
                                                   
                                                   
                                                      d
                                                      ,
                                                      
                                                         
                                                            d
                                                         
                                                         
                                                            ′
                                                         
                                                      
                                                   
                                                
                                                ∣
                                                
                                                   
                                                      z
                                                   
                                                   
                                                      d
                                                   
                                                
                                                ,
                                                
                                                   
                                                      z
                                                   
                                                   
                                                      
                                                         
                                                            d
                                                         
                                                         
                                                            ′
                                                         
                                                      
                                                   
                                                
                                                )
                                             
                                          .


                        Fig. 3
                        (a) illustrates the graphical model for this process for a single pair of documents. The full model, which is difficult to fit in one figure, shall contain the observed words from all D documents, and 
                           
                              
                                 
                                    D
                                 
                                 
                                    2
                                 
                              
                           
                         link variables for each possible connection among them.

The main difference between the RTM and VTN are the definition of the link variable and the link probability function. In the RTM [25], the link variable 
                           
                              
                                 
                                    y
                                 
                                 
                                    d
                                    ,
                                    
                                       
                                          d
                                       
                                       
                                          ′
                                       
                                    
                                 
                              
                           
                         is a binary variable indicating whether there is a citation between two articles (i.e., 
                        
                           
                              
                                 
                                    y
                                 
                                 
                                    d
                                    ,
                                    
                                       
                                          d
                                       
                                       
                                          ′
                                       
                                    
                                 
                              
                              =
                              1
                           
                         indicates there is a citation and 
                           
                              
                                 
                                    y
                                 
                                 
                                    d
                                    ,
                                    
                                       
                                          d
                                       
                                       
                                          ′
                                       
                                    
                                 
                              
                              =
                              0
                           
                         indicates there is not). And two link probability function (i.e., the logistic regression function and the exponential mean function) were used to describe the probability that there is a citation between two articles.

However, the link variable 
                           
                              
                                 
                                    l
                                 
                                 
                                    d
                                    ,
                                    
                                       
                                          d
                                       
                                       
                                          ′
                                       
                                    
                                 
                              
                           
                         in VTN is defined as the relation between two images, thus it is a multi-valued variable 
                           
                              
                                 
                                    l
                                 
                                 
                                    d
                                    ,
                                    
                                       
                                          d
                                       
                                       
                                          ′
                                       
                                    
                                 
                              
                              =
                              0
                              ,
                              1
                              ,
                              …
                              ,
                              W
                           
                         (e.g., the correlation strength between two tag sets has been quantized into 
                           
                              W
                              +
                              1
                           
                         levels). So, those two functions in the RTM cannot be directly employed to model the image relations, and we should proposed a new link probability function in the VTN.

As aforementioned, if two images have a strong relation (i.e., the value of 
                           
                              
                                 
                                    l
                                 
                                 
                                    d
                                    ,
                                    
                                       
                                          d
                                       
                                       
                                          ′
                                       
                                    
                                 
                              
                           
                         is relatively large), they are more likely to be from the same image category, and they are encouraged to have similar representations.

On the other hand, given image topic assignments 
                           
                              
                                 
                                    z
                                 
                                 
                                    d
                                 
                              
                           
                         and 
                           
                              
                                 
                                    z
                                 
                                 
                                    
                                       
                                          d
                                       
                                       
                                          ′
                                       
                                    
                                 
                              
                           
                        , there are many ways to define the similarity of their representations 
                           
                              
                                 
                                    s
                                 
                                 
                                    d
                                    ,
                                    
                                       
                                          d
                                       
                                       
                                          ′
                                       
                                    
                                 
                              
                           
                        , such as histogram intersection or dot product between the 
                           
                              
                                 
                                    z
                                 
                                 
                                    d
                                 
                              
                           
                         and 
                           
                              
                                 
                                    z
                                 
                                 
                                    
                                       
                                          d
                                       
                                       
                                          ′
                                       
                                    
                                 
                              
                           
                        . In this paper we adopt the histogram intersection, i.e.,
                        
                           
                              (5)
                              
                                 
                                    
                                       s
                                    
                                    
                                       d
                                       ,
                                       
                                          
                                             d
                                          
                                          
                                             ′
                                          
                                       
                                    
                                 
                                 (
                                 
                                    
                                       z
                                    
                                    
                                       d
                                    
                                 
                                 ,
                                 
                                    
                                       z
                                    
                                    
                                       
                                          
                                             d
                                          
                                          
                                             ′
                                          
                                       
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          k
                                          =
                                          1
                                       
                                       
                                          K
                                       
                                    
                                 
                                 min
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      m
                                                   
                                                   
                                                      d
                                                      ,
                                                      k
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      m
                                                   
                                                   
                                                      d
                                                   
                                                
                                             
                                          
                                          ,
                                          
                                             
                                                
                                                   
                                                      m
                                                   
                                                   
                                                      
                                                         
                                                            d
                                                         
                                                         
                                                            ′
                                                         
                                                      
                                                      ,
                                                      k
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      m
                                                   
                                                   
                                                      d
                                                   
                                                   
                                                      ′
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        where 
                           
                              
                                 
                                    m
                                 
                                 
                                    d
                                    ,
                                    k
                                 
                              
                           
                         stands for the number of times that topic k is assigned to visual document d, and 
                           
                              
                                 
                                    m
                                 
                                 
                                    d
                                    ,
                                    k
                                 
                              
                           
                         is the number of visual words in visual document d.

Therefore, in order to encourage images with strong relations to have similar representations, a new link probability function, i.e., the binomial function, is proposed in the VTN:
                           
                              (6)
                              
                                 ψ
                                 (
                                 
                                    
                                       l
                                    
                                    
                                       d
                                       ,
                                       
                                          
                                             d
                                          
                                          
                                             ′
                                          
                                       
                                    
                                 
                                 |
                                 
                                    
                                       s
                                    
                                    
                                       d
                                       ,
                                       
                                          
                                             d
                                          
                                          
                                             ′
                                          
                                       
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   W
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         l
                                                      
                                                      
                                                         d
                                                         ,
                                                         
                                                            
                                                               d
                                                            
                                                            
                                                               ′
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       s
                                    
                                    
                                       d
                                       ,
                                       
                                          
                                             d
                                          
                                          
                                             ′
                                          
                                       
                                    
                                    
                                       
                                          
                                             l
                                          
                                          
                                             d
                                             ,
                                             
                                                
                                                   d
                                                
                                                
                                                   ′
                                                
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       (
                                       1
                                       -
                                       
                                          
                                             s
                                          
                                          
                                             d
                                             ,
                                             
                                                
                                                   d
                                                
                                                
                                                   ′
                                                
                                             
                                          
                                       
                                       )
                                    
                                    
                                       W
                                       -
                                       
                                          
                                             l
                                          
                                          
                                             d
                                             ,
                                             
                                                
                                                   d
                                                
                                                
                                                   ′
                                                
                                             
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        where the parameter W indicates the maximum value of the link variable 
                           
                              
                                 
                                    l
                                 
                                 
                                    d
                                    ,
                                    
                                       
                                          d
                                       
                                       
                                          ′
                                       
                                    
                                 
                              
                           
                        .

It is known that the binomial distribution has a unique property that the parameter 
                           
                              
                                 
                                    s
                                 
                                 
                                    d
                                    ,
                                    
                                       
                                          d
                                       
                                       
                                          ′
                                       
                                    
                                 
                              
                           
                         and variable 
                           
                              
                                 
                                    l
                                 
                                 
                                    d
                                    ,
                                    
                                       
                                          d
                                       
                                       
                                          ′
                                       
                                    
                                 
                              
                           
                         have a distinct relationship: the link probability will simply be higher if 
                           
                              
                                 
                                    s
                                 
                                 
                                    d
                                    ,
                                    
                                       
                                          d
                                       
                                       
                                          ′
                                       
                                    
                                 
                              
                           
                         and 
                           
                              
                                 
                                    l
                                 
                                 
                                    d
                                    ,
                                    
                                       
                                          d
                                       
                                       
                                          ′
                                       
                                    
                                 
                              
                           
                         are both larger or both smaller. So, according to Eq. (6), the images with strong relations can be encouraged to have similar representations.

Furthermore, there is a parameter for the link probability function in RTM to be estimated. In contrast, our link probability function (Eq. (6)) is a simple function without any parameter to be estimated, which will simplify the procedure of learning.

According to the generative process of the VTN, the joint distribution of visual words 
                           
                              w
                           
                        , image relations 
                           
                              l
                           
                        , topic mixture 
                           
                              θ
                           
                        , topic distribution 
                           
                              ϕ
                           
                        , and a set of topic assignments 
                           
                              z
                           
                         is given by
                           
                              (7)
                              
                                 
                                    
                                       
                                       
                                          
                                             p
                                             (
                                             w
                                             ,
                                             l
                                             ,
                                             θ
                                             ,
                                             ϕ
                                             ,
                                             z
                                             |
                                             α
                                             ,
                                             β
                                             ,
                                             η
                                             ,
                                             ω
                                             )
                                             =
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      ∏
                                                   
                                                   
                                                      d
                                                      ∈
                                                      D
                                                   
                                                
                                             
                                             p
                                             (
                                             
                                                
                                                   θ
                                                
                                                
                                                   d
                                                
                                             
                                             |
                                             α
                                             )
                                             
                                                
                                                   
                                                      ∏
                                                   
                                                   
                                                      n
                                                      ∈
                                                      
                                                         
                                                            N
                                                         
                                                         
                                                            d
                                                         
                                                      
                                                   
                                                
                                             
                                             p
                                             (
                                             
                                                
                                                   z
                                                
                                                
                                                   dn
                                                
                                             
                                             |
                                             
                                                
                                                   θ
                                                
                                                
                                                   d
                                                
                                             
                                             )
                                             p
                                             (
                                             
                                                
                                                   w
                                                
                                                
                                                   dn
                                                
                                             
                                             |
                                             
                                                
                                                   ϕ
                                                
                                                
                                                   
                                                      
                                                         z
                                                      
                                                      
                                                         dn
                                                      
                                                   
                                                
                                             
                                             )
                                             
                                                
                                                   
                                                      ∏
                                                   
                                                   
                                                      k
                                                      ∈
                                                      K
                                                   
                                                
                                             
                                             p
                                             (
                                             
                                                
                                                   ϕ
                                                
                                                
                                                   k
                                                
                                             
                                             |
                                             β
                                             )
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      ∏
                                                   
                                                   
                                                      l
                                                      ∈
                                                      L
                                                   
                                                
                                             
                                             ψ
                                             (
                                             
                                                
                                                   l
                                                
                                                
                                                   d
                                                   ,
                                                   
                                                      
                                                         d
                                                      
                                                      
                                                         ′
                                                      
                                                   
                                                
                                             
                                             |
                                             
                                                
                                                   z
                                                
                                                
                                                   d
                                                
                                             
                                             ,
                                             
                                                
                                                   z
                                                
                                                
                                                   
                                                      
                                                         d
                                                      
                                                      
                                                         ′
                                                      
                                                   
                                                
                                             
                                             )
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        where the first three and the fourth item indicate the generation of image visual content and image relations respectively.

Similar to many methods of image representation with topic modeling, the visual representation of an image is defined as the topic mixture 
                           
                              
                                 
                                    θ
                                 
                                 
                                    d
                                 
                              
                           
                        , which needs to be inferred from the VTN model. We proceed to present the details of the inference method which learns the visual representations of all the images in the collection.

To obtain the visual representation 
                           
                              
                                 
                                    θ
                                 
                                 
                                    d
                                 
                              
                           
                         of these images, we need to resolve a Bayesian inference problem. This refers to reversing the defined generative process and obtaining the posterior distributions of the latent variables in the model given the observed data. Gibbs sampling, as one member of a family of algorithms from the Markov Chain Monte Carlo (MCMC) framework [37], has been widely used to solve such a problem [38,39].

Specifically, we are interested in the latent document-topic proportions 
                           
                              
                                 
                                    θ
                                 
                                 
                                    d
                                 
                              
                           
                        , the topic-word distributions 
                           
                              
                                 
                                    ϕ
                                 
                                 
                                    k
                                 
                              
                           
                        , and the topic index assignments 
                           
                              
                                 
                                    z
                                 
                                 
                                    d
                                    ,
                                    n
                                 
                              
                           
                         for each word 
                           
                              
                                 
                                    w
                                 
                                 
                                    d
                                    ,
                                    n
                                 
                              
                           
                        . While conditional distributions can be derived for each of these latent variables, we note that both 
                           
                              
                                 
                                    θ
                                 
                                 
                                    d
                                 
                              
                           
                         and 
                           
                              
                                 
                                    ϕ
                                 
                                 
                                    k
                                 
                              
                           
                         can be calculated using just the topic index assignments 
                           
                              
                                 
                                    z
                                 
                                 
                                    d
                                    ,
                                    n
                                 
                              
                           
                         (i.e., 
                        
                           
                              z
                           
                         is a sufficient statistics for both distributions). Therefore, a simple algorithm can be used if we integrate out the parameters and simply sample 
                           
                              
                                 
                                    z
                                 
                                 
                                    d
                                    ,
                                    n
                                 
                              
                           
                        , which is a collapsed Gibbs sampler 
                        [39].

We are interested in computing the following Gibbs sampling equation
                           
                              
                                 p
                                 (
                                 
                                    
                                       z
                                    
                                    
                                       d
                                       ,
                                       n
                                    
                                 
                                 |
                                 
                                    
                                       z
                                    
                                    
                                       -
                                       dn
                                    
                                 
                                 ,
                                 w
                                 ,
                                 l
                                 ,
                                 α
                                 ,
                                 β
                                 )
                                 ,
                              
                           
                        where 
                           
                              
                                 
                                    z
                                 
                                 
                                    -
                                    dn
                                 
                              
                           
                         indicates all topic assignments except for 
                           
                              
                                 
                                    z
                                 
                                 
                                    d
                                    ,
                                    n
                                 
                              
                           
                        .

By rule of conditional probability, we have
                           
                              (8)
                              
                                 p
                                 (
                                 
                                    
                                       z
                                    
                                    
                                       d
                                       ,
                                       n
                                    
                                 
                                 |
                                 
                                    
                                       z
                                    
                                    
                                       -
                                       dn
                                    
                                 
                                 ,
                                 w
                                 ,
                                 l
                                 ,
                                 α
                                 ,
                                 β
                                 )
                                 =
                                 
                                    
                                       p
                                       (
                                       
                                          
                                             z
                                          
                                          
                                             d
                                             ,
                                             n
                                          
                                       
                                       ,
                                       
                                          
                                             z
                                          
                                          
                                             -
                                             dn
                                          
                                       
                                       ,
                                       w
                                       ,
                                       l
                                       |
                                       α
                                       ,
                                       β
                                       )
                                    
                                    
                                       p
                                       (
                                       
                                          
                                             z
                                          
                                          
                                             -
                                             dn
                                          
                                       
                                       ,
                                       w
                                       ,
                                       l
                                       |
                                       α
                                       ,
                                       β
                                       )
                                    
                                 
                              
                           
                        Combining Eqs. (8) and (7) together, the Gibbs sampling equation for VTN can be written as
                           
                              (9)
                              
                                 
                                    
                                       
                                       
                                          
                                             p
                                             (
                                             
                                                
                                                   z
                                                
                                                
                                                   d
                                                   ,
                                                   n
                                                
                                             
                                             =
                                             k
                                             |
                                             
                                                
                                                   z
                                                
                                                
                                                   -
                                                   dn
                                                
                                             
                                             ,
                                             w
                                             ,
                                             l
                                             ,
                                             α
                                             ,
                                             β
                                             )
                                             =
                                             (
                                             α
                                             +
                                             
                                                
                                                   m
                                                
                                                
                                                   d
                                                   ,
                                                   k
                                                
                                                
                                                   -
                                                   dn
                                                
                                             
                                             )
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             ·
                                             
                                                
                                                   
                                                      
                                                         n
                                                      
                                                      
                                                         k
                                                         ,
                                                         
                                                            
                                                               w
                                                            
                                                            
                                                               d
                                                               ,
                                                               n
                                                            
                                                         
                                                      
                                                      
                                                         -
                                                         dn
                                                      
                                                   
                                                   +
                                                   β
                                                
                                                
                                                   
                                                      
                                                         ∑
                                                      
                                                      
                                                         w
                                                      
                                                   
                                                   
                                                      
                                                         n
                                                      
                                                      
                                                         k
                                                         ,
                                                         w
                                                      
                                                      
                                                         -
                                                         dn
                                                      
                                                   
                                                   +
                                                   K
                                                   β
                                                
                                             
                                             
                                                
                                                   
                                                      ∏
                                                   
                                                   
                                                      d
                                                      ,
                                                      
                                                         
                                                            d
                                                         
                                                         
                                                            ′
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   ψ
                                                   (
                                                   
                                                      
                                                         l
                                                      
                                                      
                                                         d
                                                         ,
                                                         
                                                            
                                                               d
                                                            
                                                            
                                                               ′
                                                            
                                                         
                                                      
                                                   
                                                   |
                                                   
                                                      
                                                         z
                                                      
                                                      
                                                         d
                                                      
                                                   
                                                   ,
                                                   
                                                      
                                                         z
                                                      
                                                      
                                                         
                                                            
                                                               d
                                                            
                                                            
                                                               ′
                                                            
                                                         
                                                      
                                                   
                                                   )
                                                
                                                
                                                   ψ
                                                   (
                                                   
                                                      
                                                         l
                                                      
                                                      
                                                         d
                                                         ,
                                                         
                                                            
                                                               d
                                                            
                                                            
                                                               ′
                                                            
                                                         
                                                      
                                                   
                                                   |
                                                   
                                                      
                                                         z
                                                      
                                                      
                                                         d
                                                      
                                                      
                                                         -
                                                         dn
                                                      
                                                   
                                                   ,
                                                   
                                                      
                                                         z
                                                      
                                                      
                                                         
                                                            
                                                               d
                                                            
                                                            
                                                               ′
                                                            
                                                         
                                                      
                                                   
                                                   )
                                                
                                             
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    m
                                 
                                 
                                    d
                                    ,
                                    k
                                 
                                 
                                    -
                                    dn
                                 
                              
                           
                         stands for the number of times that topic k is assigned to visual document d except for 
                           
                              
                                 
                                    z
                                 
                                 
                                    d
                                    ,
                                    n
                                 
                              
                           
                        , and 
                           
                              
                                 
                                    n
                                 
                                 
                                    k
                                    ,
                                    w
                                 
                                 
                                    -
                                    dn
                                 
                              
                           
                         indicates the number of times that topic k is assigned to w except for 
                           
                              
                                 
                                    z
                                 
                                 
                                    d
                                    ,
                                    n
                                 
                              
                           
                        .

Finally, given a sample 
                           
                              z
                           
                        , we can then get an estimate for ϕ and 
                           
                              
                                 
                                    θ
                                 
                                 
                                    d
                                 
                              
                           
                         as
                           
                              (10)
                              
                                 
                                    
                                       ϕ
                                    
                                    
                                       k
                                    
                                 
                                 (
                                 w
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             n
                                          
                                          
                                             w
                                             ,
                                             k
                                          
                                       
                                       +
                                       β
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             k
                                          
                                       
                                       
                                          
                                             n
                                          
                                          
                                             w
                                             ,
                                             k
                                          
                                       
                                       +
                                       W
                                       β
                                    
                                 
                              
                           
                        
                        
                           
                              (11)
                              
                                 
                                    
                                       θ
                                    
                                    
                                       d
                                    
                                 
                                 (
                                 k
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             n
                                          
                                          
                                             d
                                             ,
                                             k
                                          
                                       
                                       +
                                       α
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             k
                                          
                                       
                                       
                                          
                                             n
                                          
                                          
                                             d
                                             ,
                                             k
                                          
                                       
                                       +
                                       K
                                       α
                                    
                                 
                                 ,
                              
                           
                        where 
                           
                              
                                 
                                    θ
                                 
                                 
                                    d
                                 
                              
                           
                         is regarded as the visual representation of image d in our VN.

To conduct the image recognition, we use a general classifier such as SVM on top of our image representations 
                           
                              
                                 
                                    θ
                                 
                                 
                                    d
                                 
                              
                           
                        . In particular, for each image category we use a binary linear SVM to distinguish it from other image categories.

In summary, the labeled training images are put in a VN. Through the Bayesian inference algorithm presented in Section 4.2, we learn a VTN model with parameters 
                           
                              
                                 
                                    ϕ
                                 
                                 
                                    k
                                 
                              
                           
                        , and obtain the representations 
                           
                              
                                 
                                    θ
                                 
                                 
                                    d
                                 
                                 
                                    tr
                                 
                              
                           
                         for training images. Then the unlabeled testing images are put in another VN, and use the learnt VTN (i.e., 
                        
                           
                              
                                 
                                    ϕ
                                 
                                 
                                    k
                                 
                              
                           
                         is learnt at training phase and fixed at testing phase) to obtain the representations 
                           
                              
                                 
                                    θ
                                 
                                 
                                    d
                                 
                                 
                                    ts
                                 
                              
                           
                         for testing images. Finally, a binary linear SVM classifier is trained based on 
                           
                              
                                 
                                    θ
                                 
                                 
                                    d
                                 
                                 
                                    tr
                                 
                              
                           
                        , which are subsequently applied to 
                           
                              
                                 
                                    θ
                                 
                                 
                                    d
                                 
                                 
                                    ts
                                 
                              
                           
                         on the testing images to produce the prediction.

@&#EVALUATION@&#

To evaluate the performance of the proposed method, we conduct some experiments on two social media datasets, which are described below:
                        
                           •
                           
                              NUS-WIDE: It contains 269,648 images which are crawled from Flickr website. The images are linked to 1000 different user tags, which are annotated by users registered in Flickr. Beyond these images and user tags, 81 concepts are defined in the dataset.


                              MIRFLICKR-25 k: It contains 25,000 images which are also crawled from Flickr website. In the collection there are 1,386 tags which occur in at least 20 images. And 23 labels are defined in the dataset.

This paper focuses on how to build better image representations. To evaluate the quality of the image representations, a simple way is by means of image recognition. In particular, through training an image category classifier with the built image representations and corresponding image category labels, the quality of the built image representations can be measured by the performance of the image category recognition. Usually, a method that could build better image representations could achieve better image recognition performance. Average Precision (AP) is used to measure the performance of recognition.

In this paper, the 81 and 23 concepts in the two datasets are respectively regarded as different image categories for the evaluation of image category recognition. For each image category, we generate its training and testing subsets in an One-vs-Other fashion, i.e., we select some images from its category as positive samples, and select some images from other categories as negative samples.

For the two datasets, the number of user tags for different images are significantly imbalanced. Take the NUS-WIDE dataset as an example, some images are annotated with more than 100 tags, yet some images do not have tags. However, the proposed method of image recognition is more suitable to the scenario that images are annotated with more tags. Therefore, we will filter out the images that are annotated with less than 3 tags. In another word, each image in the training and testing evaluation subsets will at least has 3 tags.

The number of image relations increases rapidly when more images are considered. Even though our algorithm can only consider a portion of image relations, it still takes a long time to conduct image recognition with all images in the datasets. So, we only randomly select some images to conduct image recognition, and repeat the random selection process many times to achieve a average performance.

In particular, if there are more than 1000 positive samples for a specific image category, only 1000 positive samples are randomly selected for evaluation. Otherwise, all the positive samples will be used for evaluation. Regarding negative samples, for each image category only 1000 negative samples are randomly selected for evaluation. Furthermore, we repeat the process 50 times independently. Thus, 50 independent training and testing subsets are generated for each image category. The algorithms are evaluated on each subset, and the average performance on the 50 subsets is regarded as the final performance. In addition, for each image, we densely extract SIFT features from 
                           
                              10
                              ×
                              10
                           
                         image patches. These SIFT features are quantized to form a visual codebook of size 500. For tags, the 1000 most frequently used tags are leveraged to form a tag codebook.

In this experiment, three methods for building image representations (i.e., TF-IDF [40], LDA [14], and C2MR [22]) are evaluated and compared with our proposed method.
                           
                              1.
                              
                                 TF-IDF: After describing each image as a bag of visual words, TF-IDF is computed as its representation. By considering the term weighting scheme, TF-IDF is usually better than naive bag-of-words representation and is regarded as the baseline in this paper.


                                 LDA: The training images are provided for the learning of LDA, and the inferred topic mixture vectors 
                                    
                                       
                                          
                                             θ
                                          
                                          
                                             d
                                          
                                       
                                       ,
                                       d
                                       ∈
                                       
                                          
                                             D
                                          
                                          
                                             tr
                                          
                                       
                                    
                                  are used as their representations; then the learnt LDA is used to infer the representations of testing images (i.e., 
                                 
                                    
                                       
                                          
                                             θ
                                          
                                          
                                             d
                                          
                                       
                                       ,
                                       d
                                       ∈
                                       
                                          
                                             D
                                          
                                          
                                             ts
                                          
                                       
                                    
                                 ). Finally a binary linear SVM classifier is used for image recognition based on the training and testing representation vectors.


                                 U-C2MR: C2MR in [22] is proposed to model a multimedia information network constructed with images and tags. There are two types of C2MR, i.e., Unsupervised C2MR (U-C2MR) and Supervised C2MR (S-C2MR). Since U-C2MR is proposed for building image representations, it is used for comparison in this experiment whereas S-C2MR is used for comparison at the Section 5.7. Moreover, a 137-D vector feature (concatenating a 64-D color histogram and a 73-D edge direction histogram) is used in [22]. But it is replaced with the SIFT features here for fair comparison.


                                 VTN: There are four different schemes according to the measurement and quantization of image relation 
                                    
                                       
                                          
                                             l
                                          
                                          
                                             d
                                             ,
                                             
                                                
                                                   d
                                                
                                                
                                                   ′
                                                
                                             
                                          
                                       
                                    
                                 :
                                    
                                       (a)
                                       the 
                                             
                                                
                                                   
                                                      l
                                                   
                                                   
                                                      d
                                                      ,
                                                      
                                                         
                                                            d
                                                         
                                                         
                                                            ′
                                                         
                                                      
                                                   
                                                
                                             
                                           is measured as Eq. (1). In particular, VTN-B is defined for quantizing 
                                             
                                                
                                                   
                                                      l
                                                   
                                                   
                                                      d
                                                      ,
                                                      
                                                         
                                                            d
                                                         
                                                         
                                                            ′
                                                         
                                                      
                                                   
                                                
                                             
                                           as a binary variable, i.e., 
                                          
                                             
                                                
                                                   
                                                      l
                                                   
                                                   
                                                      d
                                                      ,
                                                      
                                                         
                                                            d
                                                         
                                                         
                                                            ′
                                                         
                                                      
                                                   
                                                
                                             
                                           indicates whether there are shared tags between two images or not; VTN-M is defined for quantizing it as a multi-valued variable, i.e., 
                                          
                                             
                                                
                                                   
                                                      l
                                                   
                                                   
                                                      d
                                                      ,
                                                      
                                                         
                                                            d
                                                         
                                                         
                                                            ′
                                                         
                                                      
                                                   
                                                
                                                =
                                                0
                                                ,
                                                1
                                                ,
                                                …
                                                ,
                                                W
                                             
                                           indicates the exact number of shared tags between two images;

the 
                                             
                                                
                                                   
                                                      l
                                                   
                                                   
                                                      d
                                                      ,
                                                      
                                                         
                                                            d
                                                         
                                                         
                                                            ′
                                                         
                                                      
                                                   
                                                
                                             
                                           is measured as Eq. (4). Similarly, two schemes VTN-CB or VTN-CM are defined according to binary or multi-valued quantization of 
                                             
                                                
                                                   
                                                      l
                                                   
                                                   
                                                      d
                                                      ,
                                                      
                                                         
                                                            d
                                                         
                                                         
                                                            ′
                                                         
                                                      
                                                   
                                                
                                             
                                          .

For fair comparison, all the methods are based on the same extracted SIFT features and user tags. In addition, the number of topics for LDA, and VTN are set to be 100, and the dimension of the latent space for C2MR is also set to be 100.

It is noticed that the quantization parameter W has certain effect on the performance of our model. As aforementioned, we need to choose a good quantization parameter W to make a trade-off between obtaining benefits from modeling the multi-level link strength and be tolerant to the computational noise. With experimental evaluation, shown in Table 2, we found that the best recognition accuracy is achieved when the 
                           
                              
                                 
                                    l
                                 
                                 
                                    d
                                    ,
                                    
                                       
                                          d
                                       
                                       
                                          ′
                                       
                                    
                                 
                              
                           
                         is quantized into 4 levels, i.e., 
                        
                           
                              W
                              =
                              3
                           
                        .

As aforementioned, we randomly generate 50 independent subsets for each image category. Thus, for each image category, all methods are evaluated with those subsets, and the mean and variance of AP are calculated. The mean of AP for each image category are illustrated in Figs. 4 and 5
                        
                        . And the average over all the image categories are illustrated in Table 1
                        
                        .

Among these methods, TF-IDF, and LDA just leverage image visual content (i.e., local features). The recognition performance of TF-IDF and LDA are comparable. However, since the dimension of topic mixture for LDA (i.e., 100) is less than that for TF-IDF (i.e., 500), we can conclude that the LDA representation is more compact and discriminative than TF-IDF.

In contrast, U-C2MR and VTN both leverage image content and user tags. It is observed that U-C2MR and VTN present significant improvements in recognition accuracy against TF-IDF and LDA. Therefore, we can conclude that it is beneficial to leverage both image content and user tags for building image recognitions. Moreover, the performance of VTN-CM is much better than U-C2MR, thus we can conclude that the hierarchical image representations built by topic modeling is better than that built by other latent space analysis method.

In addition, among four different schemes of VTN, VTN-CM is better than other three schemes. It illustrates that it is necessary to consider the inter-tag similarity (i.e., Eq. (4)) if we model the image relations with a multi-value variable instead of a binary variable.

According to the different measurement and quantization of the correlation between two tag sets, there are four different schemes for the VTN model (i.e., VTN-B, VTN-M, VTN-CB, VTN-CM). However, Table 1 cannot clearly illustrate the difference of their performance in the details. For example, the performance of VTN-B, VTN-M and VTN-CB is almost similar in terms of average AP from Table 1.

In this section, the AP of the four schemes is compared every each image category, and the comparisons over all image categories are collected and sorted. The results are shown in Tables 3 and 4
                        
                        . For example, on NUS-WIDE dataset, the VTN-B achieves the best AP (i.e., 1st AP) on 16 over 81 image categories, and it achieves the second best AP (i.e., 2nd AP) on 20 over 81 image categories.

From Tables 3 and 4, the VTN-B achieves the best AP on more image categories against VTN-M. So, it can be found that the VTN-B performs better than the VTN-M, and we have a conclusion that the performance cannot be improved by directly increasing the number of quantization levels without considering the inter-tag similarity.

In this paper, the image relations are defined by leveraging user tags, i.e., if two images share common tags, we define that they have a strong relation which indicates they are more likely to be from the same image category. This definition is reasonable and can be justified by Fig. 6
                        . In particular, we randomly select 1000 images from the ‘airport’ category in NUS-WIDE dataset and generate a normalized histogram for the number of shared tags for all the image pairs (Fig. 6(a)); On the other hand, we randomly select 1000 images from the entire image collection (i.e., from all image categories) and generate another normalized histogram in the same way, as shown in Fig. 6(b). It is obvious that the images within a certain category usually share more tags, but the images from the entire image collection usually share fewer tags.

Generally, the more image relations are exploited, the more recognition accuracy can be improved. However, it takes more training and testing time. So we need a trade-off between the recognition accuracy and the efficiency. For some scenarios (Section 5.5.1), although we can model all the relations among images, we only need to select some of them to keep the algorithm to be efficient. For some other scenarios (Section 5.5.2), although some image relations are unavailable, we can still predict some of these unavailable relations and leverage them for improving the recognition accuracy. We will discuss them in the details as follows.

Obviously, the quantity of image relations increases rapidly with the increase of the quantity of images. For example, if we want to model all relations among n images, the quantity of relations is 
                              
                                 
                                    
                                       n
                                       (
                                       n
                                       -
                                       1
                                       )
                                    
                                    
                                       2
                                    
                                 
                              
                           . Since the computational cost of inference significantly depends on the quantity of image relations, we can only select and model some relations to keep the algorithm to be efficient. Thus, the question is how to select the reliable relations when the quantity of selectable relations is restricted.

Usually, the strength of image relations indicates its reliability. So, we propose a K-near-neighbors-like scheme to select reliable relations in this paper. Specifically, the relation strength between two images is described by the correlation between two tag sets (Eqs. (1) or (4)), and all the relations are sorted in an ascending order. Thus, if we want to select 
                              
                                 2
                                 K
                              
                            relations in the end, the top K relations in the sorted list are selected, and the other K relations are randomly selected from those relations with weak strength.

In Fig. 7
                           , take the image categories ‘animal’ in the NUS-WIDE dataset as an examples, we list the recognition accuracy and efficiency with respect to the percentage of the selected relations. Specifically, the x-axis indicates the ratio of selected relations to all relations, the recognition accuracy is measured in terms of AP, and the recognition efficiency is measured in terms of the relative runtime (i.e., the runtime for percentage 1% is defined as unit 1).

From Fig. 7, we can see that the accuracy increases rapidly against the runtime when the ratio is small (i.e., ratio 
                              
                                 <
                                 15
                                 %
                              
                           ), and the accuracy will increase very slightly after a saturation point (ratio 
                              
                                 =
                                 15
                                 %
                              
                           ). In contrast, the runtime increase linearly. We have similar experimental results on other image categories in both NUS-WIDE and MIRFLICKR-25k datasets. Therefore, without much loss of effectiveness we can achieve a good recognition accuracy by only selecting and modeling some image relations.

For some scenarios, some image relations cannot be obtained. For example, if an image is not annotated with tags, we cannot obtain its relations according to Eqs. (1) or (4). However, we can predict the unavailable image relations and make use of them to improve the recognition accuracy.

In this paper, we will predict the unavailable image relations according to the image visual similarity. In particular, the visual similarity of two images is evaluated by the histogram intersection of their bag-of-words histograms, and their relation strength is predicted by equally quantizing the similarity value.

For evaluating the advantage of the proposed method, another simple prediction method is evaluated for comparison, i.e., random prediction (assigning the unavailable relation strength with 
                              
                                 {
                                 0
                                 ,
                                 1
                                 ,
                                 2
                                 ,
                                 3
                                 }
                              
                            randomly).

In this experiment, we randomly drop some image relations to simulate the unavailable image relations, and we compare three different schemes: without prediction, visual similarity based prediction, and random prediction. Take the image category ‘beach’ in NUS-WIDE dataset as an example, Fig. 8
                            shows that the performance of the visual similarity based prediction is better than that of random prediction, and the method without prediction performs the worst.

There are similar experimental results for other image categories in both the NUS-WIDE and MIRFLICKR-25k datasets. Since the value scope of the recognition accuracy mAP varies for different image categories, we need to normalize them for further analysis. In particular, we firstly normalize them into interval 
                              
                                 [
                                 0
                                 ,
                                 1
                                 ]
                              
                            (i.e., let 
                              
                                 
                                    
                                       mAP
                                    
                                    
                                       ¯
                                    
                                 
                                 =
                                 0
                              
                            for the situation of 100% links are missing; and let 
                              
                                 
                                    
                                       mAP
                                    
                                    
                                       ¯
                                    
                                 
                                 =
                                 1
                              
                            for the situation of 0% links are missing), and secondly get the mean value of the normalized value over all image categories under each missing-links percentage. From Table 5
                           , we have a conclusion that the visual similarity based prediction is better than other methods under the same missing-links percentage.

For previous scenario, all the testing images are available before we conduct image recognition, and it is easy to exploit all the relations among them. However, when testing images are provided in batch mode, i.e., they are provided group by group, we cannot model all the relations among them. Whereas, only the image relations within each group can be exploited, and here we will evaluate the recognition performance for such scenarios.

In this experiment, all the testing images are first randomly divided into groups with equal size. Secondly the image relations are established group by group, i.e., construct a VN for each group. Thirdly the image recognition is conducted group by group, i.e., model each VN with a VTN. At last, the APs for all groups are averaged as the final recognition performance.

Particularly, we will evaluate how the group size impact the final recognition performance. Take the image category ‘bear’ in NUS-WIDE dataset as an example, from Fig. 9
                        , with the increasing of group size (the ratio between the number of images in each group and the number of all the testing images), more relations among images will be exploited, and the final recognition performance will be improved.

There are similar experimental results for other image categories in both the NUS-WIDE and MIRFLICKR-25k datasets. As in previous section, to normalize the recognition accuracy of different image categories, we normalize the recognition accuracy into 
                           
                              [
                              0
                              ,
                              1
                              ]
                           
                         and calculate their mean value over all image categories under each group size. From Table 6
                        , we have a conclusion that the recognition performance will be improved with the increase of the group size.

To build image representations and conduct the image recognition together, some supervised topic models are proposed. The one most related to VTN is ss-RTM proposed in [26], where the image content, user tags and image labels are modeled simultaneously.

However, ss-RTM is similar to RTM that it only coarsely describes image relations with binary variables, i.e., whether there are shared common tags between two images or not. And the inter-tag similarity is not considered too. Here, we describe how to incorporate the idea of the proposed VTN into ss-RTM to improve its performance further.

Since the framework of those two works are similar, the combination is simple: firstly the computation of link weights in ss-RTM is replaced with the Eqs. (1) or (4); secondly the probability function in ss-RTM is replaced with the binomial function Eq. (6). With this modification, our VTN models can be extended to a semi-supervised topic model, called ss-VTN.

We repeated the image recognition experiments with those two new models under the same experimental setting. Specifically, we compare ss-VTN with discriminative methods [33]. In particular, local features are extracted from those images and each image is represented as a BoWs vector. Meanwhile, each image can also be represented as a Tag vector by using its associated tags. Thus the image recognition can be conducted in four ways: using an SVM classifier with only the BoWs or Tag vector (denoted as ‘BoWs+SVM’ or ‘Tag+SVM’ respectively); using an SVM classifier with the vector generated by concatenating BoWs and Tag vectors together, i.e., in a pre-fusion way (denoted as ‘BoWs+Tag+SVM’); and using Multiple Kernel Learning (MKL) to fuse both BoWs and Tag vectors, i.e., in a post-fusion way (denoted as ‘BoWs+Tag+MKL’). Moreover, a supervised topic model, i.e., sLDA [19] and the Supervised C2MR (S-C2MR) [22] are also evaluated in this experiments.

From Table 7
                        , we found the performance of ss-VTN-CM is better than that of other methods. So, we have a conclusion that either for unsupervised topic model or for supervised topic model, it is beneficial to describe image relations with a multiple-valued variable, and it also beneficial to consider the inter-tag similarity as computing image relations.

In this paper, we proposed VTN to build better image representations for social images in social media. Particularly, we organize a collection of images as an image network, where the relations between images are modeled by user tags. By encoding tags as the pair-wise image relations, the proposed method could effectively leverage loosely related tags, and hence it could be applied to realistic scenario where the tags are usually loosely related to image content. We further propose a new topic model VTN to model such image network, where the image content and image relations are modeled simultaneously. By defining a proper link probability function, our proposed VTN can properly model the multiple-level image relations, which are in contrast to the binary document relations modeled with the logistic regression function in the RTM. Our extensive experiments on two social media datasets demonstrated the advantage of the proposed VTN model. Our future work will focus on improving the scalability of the current inference algorithm. We will also focus on exploring other social contextual information to model image relations.

@&#ACKNOWLEDGMENTS@&#

This research was supported partly by the NSFC (Grant Nos. 61125204, 61432014 and 61402348), the Fundamental Research Funds for the Central Universities (Grant No. BDZ021403), the Program for Changjiang Scholars and Innovative Research Team in University of China (No. IRT13088), the Shaanxi Innovative Research Team for Key Science and Technology (No. 2012KCT-02), the China Post-doctoral Science Foundation (No. 2014M562374), and the Natural Science Foundation of Shaanxi Province (No. 2014JQ8298). This research was partly supported by the National Institute of Nursing Research of the National Institutes of Health under Award Number R01NR015371. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. This work is also partly supported by US NSF Grant IIS 1350763, GH’s start-up funds from SIT, a Google Research Faculty Award, a gift grant from Microsoft Research, and a gift grant from NEC Labs American. Dr. Qi Tian is partly supported by ARO Grant W911NF-12-1-0057, Faculty Research Awards by NEC Labs American, and NSFC Grant 61429201 respectively.

@&#REFERENCES@&#

