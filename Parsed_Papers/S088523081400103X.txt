@&#MAIN-TITLE@&#Translating without in-domain corpus: Machine translation post-editing with online learning techniques

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           We present a method to customize machine translation systems when in-domain data is not available.


                        
                        
                           
                           For that we perform an online learning automatic post-editing from ready-to-use generic machine translation systems.


                        
                        
                           
                           The results show that the method is very effective on rule-based machine translation systems.


                        
                        
                           
                           On statistical machine translation systems the method performs well if no in-domain data was used in the training.


                        
                        
                           
                           Finally, if there is not enough repetition our method has limited use.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Machine translation

Statistical machine translation

Interactive machine translation

Automatic post-editing

Online learning

@&#ABSTRACT@&#


               
               
                  Globalization has dramatically increased the need of translating information from one language to another. Frequently, such translation needs should be satisfied under very tight time constraints. Machine translation (MT) techniques can constitute a solution to this overly complex problem. However, the documents to be translated in real scenarios are often limited to a specific domain, such as a particular type of medical or legal text. This situation seriously hinders the applicability of MT, since it is usually expensive to build a reliable translation system, no matter what technology is used, due to the linguistic resources that are required to build them, such as dictionaries, translation memories or parallel texts. In order to solve this problem, we propose the application of automatic post-editing in an online learning framework. Our proposed technique allows the human expert to translate in a specific domain by using a base translation system designed to work in a general domain whose output is corrected (or adapted to the specific domain) by means of an automatic post-editing module. This automatic post-editing module learns to make its corrections from user feedback in real time by means of online learning techniques. We have validated our system using different translation technologies to implement the base translation system, as well as several texts involving different domains and languages. In most cases, our results show significant improvements in terms of BLEU (up to 16 points) with respect to the baseline systems. The proposed technique works effectively when the n-grams of the document to be translated presents a certain rate of repetition, situation which is common according to the document-internal repetition property.
               
            

@&#INTRODUCTION@&#

Globalization has urged the need for high-quality translations with fast turn-around times. Examples of that are companies aiming to internationalize their businesses in order to discover new markets and gain competitive advantage, or transnational institutions that have legal requirements to produce documentation in multiple languages. Frequently, these documents need to be delivered with tight deadlines and, at the same time, clients are pushing to adjust prices. As a result, translation agencies and in-house translation departments have been compelled to adopt automated machine translation (MT) in an attempt to improve their translation pipelines (Dove et al., 2012). In that way, MT systems are used to produce drafts of the translations that later are post-edited by human translators in order to achieve the high-quality standards required by the industry.

Historically, rule based machine translation (RBMT) systems have been used by companies to automate their translation needs (Silva, 2012). Nevertheless, RBMT systems are expensive to personalize, as expert linguists are needed to create bilingual dictionaries or specific rules (Bennett and Slocum, 1985; Isabelle et al., 2007). As a result, these systems are only available for a handful of European languages. On the contrary, statistical machine translation (SMT) systems are created in a more unattended manner by harvesting parallel segments from a collection of Bi-texts or translation memories (TM). The quality that SMT systems achieve is often better than that of RBMT systems (Béchara et al., 2012; Silva, 2012), at least for some language pairs, and provided that there is enough data. However, it is only recently that SMT systems are being effectively used to improve the productivity of human translators by means of building engines customized from the client's data. Unfortunately, clients seldom have previous parallel corpora from the same domain that can be used to train these customized engines, or to adapt the domain of a pre-existent one (Irvine et al., 2013). Additionally, training such engines may take hours, days, if not weeks of computation. On the other hand, RBMT systems can be used right out-of-the-box, and they can be enhanced with an automatic post-editing (APE) by an SMT system in a way that translators appreciate it more than either of both systems alone, regardless their BLEU scores (Béchara et al., 2012). That paper shows that, although automatic evaluation metrics favor the pure SMT system, human evaluators prefer the output provided by the statistically post-edited RBMT system.

Thus, the premise of this work is based on a real case scenario: a human translator, probably a freelancer, is given a translation assignment with a tight turn-around time. Alas, our translator lacks the necessary linguistic resources such as TMs or parallel texts that would allow him or her to build an MT system (no matter which technology is used) adapted to the specific domain of the document. Under these circumstances, what are his or her alternatives?
                        W/O RESOURCES
                        
                           This is the traditional manual method, but it requires more time and effort. Note that, in this case, we are not considering the use of previously collected TMs neither TMs generated on the go. On the contrary, each sentence is supposed to be translated from an empty box, or filled up with the source text at most.

Translating with a web-based translation application, and then post-edit its output. Nowadays, there are many free web-based translation applications which can achieve a translation quality enough for gisting and, even in some cases, the quality can be satisfactory. However, it can be insufficient for many domains of interest. Also, these web-based translation applications can present some confidentiality issues that should be considered, because all content uploaded will be employed to enrich their models. Moreover, some of them are not free when translating more than a given quantity of words.

Translating with a RBMT system, and then post-editing its output. There are many RBMT translation systems, some of them free. Nevertheless, the output of RBMT systems is usually not tailored to the domain of the document being translated and fail to adapt to new domains (Isabelle et al., 2007), e.g., lexical choices may not be appropriate. Although APE may alleviate this problem, still parallel corpora is needed.

If he or she is familiar with SMT, he or she can train an SMT model with unrelated corpora (remember that there are no available in-domain TMs, which is a frequent case). As in the RBMT case, these SMT translations will contain several mistakes due to the fact that, in this case, the training corpus is out-of-domain.

Neither of these options is optimal since, as we have discussed above, MT customization is key to improve the translator's productivity. In this paper, we propose a technique to help the translator in this regard. We assume that the translator will adopt one of the different MT alternatives proposed previously as a draft for post-editing, none of which is customized to the document domain. APE can be specially useful under these circumstances since it can be used as a domain adaptation technique. Domain adaptation has received extensive attention from the SMT research community during the last years. However, this topic has typically been approached in scenarios where the set of training samples used to estimate the model parameters (both in and out-of-domain) are available beforehand, and the system does not get updated after the training stage has concluded.

In this work the domain adaptation problem is tackled in a different way, specifically, as translation hypotheses are amended by the user, the system will learn from these corrections, using them to train APE models on-the-fly. In this way, the following system translation hypotheses automatically will apply past user's amendments. To achieve this effect, an on-line learning (OL) SMT system will be used to customize the output of the original system after each translated segment. Hence, APE is considered here as an online technique, where the user interacts with the system in order to correct the initial hypotheses. In addition to this, these corrections could be taken as new corpora to retrain the APE models. In an ideal APE scenario, these models should be updated for each new hypothesis given by the system, in order to minimize the user post-editing effort. This is why we propose APE as a natural application field of OL techniques.

The rest of the paper is organized as follows. First, Section 2 introduces the techniques implemented in our system, which applies statistical machine translation (Section 2.1) to an automatic post-editing task (Section 2.2) in an online learning environment (Section 2.3). Then, Section 3 compares our proposal with some similar techniques appearing in the literature. Section 4 describes the proposed system, emphasizing the base translation systems (Section 4.1) and our OL approach (Section 4.2). Finally, we show some experiments in Section 5 that apply these techniques, employing corpora with different features (Section 5.1) and several base translation systems (Section 5.2). Finally, we discuss the results in Section 5.3, showing in Appendix A some plots with the dynamics of OL APE.

In this section we describe the theoretical foundations of our proposal, where we apply online learning techniques to an automatic post-editing task based on statistical machine translation. Thus, we introduce the statistical approach to machine translation (Section 2.1) as well as two language technologies that can be built upon it: automatic post-editing (Section 2.2), and online learning (Section 2.3).

Given a sentence f from a source language 
                           F
                         to be translated into a target sentence e of a target language 
                           E
                        , the fundamental equation of SMT (Brown et al., 1993) is the following:


                        
                           
                              (1)
                              
                                 
                                    
                                       
                                          
                                             e
                                          
                                       
                                       ˆ
                                    
                                 
                                 =
                                 
                                    argmax
                                    
                                       
                                          e
                                       
                                    
                                 
                                 
                                    
                                       
                                          Pr
                                          (
                                          
                                             
                                                e
                                             
                                          
                                          ∣
                                          
                                             
                                                f
                                             
                                          
                                          )
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (2)
                              
                                 =
                                 
                                    argmax
                                    
                                       
                                          e
                                       
                                    
                                 
                                 
                                    
                                       
                                          Pr
                                          (
                                          f
                                          ∣
                                          
                                             
                                                e
                                             
                                          
                                          )
                                          
                                          Pr
                                          (
                                          
                                             
                                                e
                                             
                                          
                                          )
                                       
                                    
                                 
                              
                           
                        where Pr(f
                        ∣
                        e) is approximated by a translation model that represents the correlation between the source and the target sentence and where Pr(e) is approximated by a language model representing the well-formedness of the candidate translation e.

State-of-the-art statistical machine translation systems follow a log-linear approach (Och and Ney, 2002), where direct modelling of the posterior probability Pr(e
                        ∣
                        f) of Eq. (1) is used. In this case, the decision rule is given by the expression:


                        
                           
                              (3)
                              
                                 
                                    
                                       
                                          
                                             e
                                          
                                       
                                       ˆ
                                    
                                 
                                 =
                                 
                                    argmax
                                    
                                       
                                          e
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             ∑
                                             
                                                m
                                                =
                                                1
                                             
                                             M
                                          
                                          
                                             λ
                                             m
                                          
                                          
                                             h
                                             m
                                          
                                          (
                                          
                                             
                                                e
                                             
                                          
                                          ,
                                          
                                             
                                                f
                                             
                                          
                                          )
                                       
                                    
                                 
                              
                           
                        
                     

where each h
                        
                           m
                        (e, f) is a feature function representing a statistical model and λ
                        
                           m
                         its weight.

Current most popular MT systems are based on the use of phrase-based models (Koehn et al., 2003) as translation models. The basic idea of phrase-based translation is to segment the source sentence into phrases, then to translate each source phrase into a target phrase, and finally to reorder the translated target phrases in order to compose the target sentence. If we summarize all the decisions made during the phrase-based translation process by means of the hidden variable 
                           
                              
                                 
                                    a
                                    ˜
                                 
                              
                              1
                              K
                           
                        , we obtain the expression:
                           
                              (4)
                              
                                 Pr
                                 (
                                 
                                    
                                       f
                                    
                                 
                                 ∣
                                 
                                    
                                       e
                                    
                                 
                                 )
                                 =
                                 
                                    ∑
                                    
                                       K
                                       ,
                                       
                                          
                                             
                                                a
                                                ˜
                                             
                                          
                                          1
                                          K
                                       
                                    
                                 
                                 
                                    Pr
                                    (
                                    
                                       
                                          
                                             f
                                             ˜
                                          
                                       
                                       1
                                       K
                                    
                                    ,
                                    
                                       
                                          
                                             a
                                             ˜
                                          
                                       
                                       1
                                       K
                                    
                                    ∣
                                    
                                       
                                          
                                             e
                                             ˜
                                          
                                       
                                       1
                                       K
                                    
                                    )
                                 
                              
                           
                        
                     

where each 
                           
                              
                                 
                                    a
                                    ˜
                                 
                              
                              k
                           
                           ∈
                           {
                           1
                           …
                           K
                           }
                         denotes the index of the target phrase 
                           
                              e
                              ˜
                           
                         that is aligned with the k-th source phrase 
                           
                              
                                 
                                    f
                                    ˜
                                 
                              
                              k
                           
                        , assuming a segmentation of length K.

According to Eq. (4), and following a maximum approximation, the problem stated in Eq. (2) can be re-framed as:


                        
                           
                              (5)
                              
                                 
                                    
                                       
                                          
                                             e
                                          
                                       
                                       ˆ
                                    
                                 
                                 ≈
                                 arg
                                 
                                    max
                                    
                                       
                                          
                                             e
                                          
                                       
                                       ,
                                       
                                          
                                             a
                                          
                                       
                                    
                                 
                                 {
                                 p
                                 (
                                 
                                    
                                       e
                                    
                                 
                                 )
                                 ·
                                 p
                                 (
                                 
                                    
                                       f
                                    
                                 
                                 ,
                                 
                                    
                                       a
                                    
                                 
                                 ∣
                                 
                                    
                                       e
                                    
                                 
                                 )
                                 }
                              
                           
                        
                     

Following the log-linear approach stated in Eq. (3), Eq. (5) can be rewritten as:


                        
                           
                              (6)
                              
                                 
                                    
                                       
                                          
                                             e
                                          
                                       
                                       ˆ
                                    
                                 
                                 =
                                 
                                    argmax
                                    
                                       
                                          
                                             e
                                          
                                       
                                       ,
                                       
                                          
                                             a
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             ∑
                                             
                                                m
                                                =
                                                1
                                             
                                             M
                                          
                                          
                                             λ
                                             m
                                          
                                          
                                             h
                                             m
                                          
                                          (
                                          
                                             
                                                e
                                             
                                          
                                          ,
                                          
                                             
                                                a
                                             
                                          
                                          ,
                                          
                                             
                                                f
                                             
                                          
                                          )
                                       
                                    
                                 
                              
                           
                        which is the approach that we follow in this work.

MT systems usually need a final revision step by a human post-editor, to assure a quality output. This can be a tedious task, where the post-editor will have to repeatedly correct the same mistakes, due to the systematic behavior of MT systems (Allen and Hogan, 2000; Carpuat and Simard, 2012).

This correction process can be understood as a transformation from an input (the translation provided by the previous MT system, usually with errors), to an output (a text in the same language where those errors have been amended). Thus, post-editing could be considered as a translation between two languages. APE systems were proposed by Knight and Chander (1994) to try to automate as far as possible that final human revision phase. Some authors consider APE as a domain adaptation or customization technique (Isabelle et al., 2007; Diaz et al., 2008; Rubino et al., 2012).

In a statistical APE system, SMT models are trained to correct the outputs of another MT system (Simard et al., 2007), which is often considered as a black box. In this way, the fundamental equation of SMT (Eq. (1)) would be applied from a sentence e′ of target language with errors 
                           
                              E
                              ′
                           
                        , which is the output of the previous MT system that need to be corrected, into a target sentence e of a target language 
                           E
                         without errors (hopefully, at least with fewer errors than 
                           
                              E
                              ′
                           
                        ).


                        
                           
                              (7)
                              
                                 
                                    
                                       
                                          
                                             e
                                          
                                       
                                       ˆ
                                    
                                 
                                 =
                                 
                                    argmax
                                    
                                       
                                          e
                                       
                                    
                                 
                                 
                                    
                                       
                                          Pr
                                          (
                                          
                                             
                                                e
                                             
                                          
                                          |
                                          
                                             
                                                
                                                   e
                                                
                                             
                                             ′
                                          
                                          )
                                       
                                    
                                 
                              
                           
                        
                     


                        Fig. 1
                         shows a diagram of a statistical APE system. In the diagram, the source sentences are processed as input by a generic MT system, producing a set of translations. After that, each system translation is used to feed the statistical APE system, whose models have been initialized from parallel corpus that have also been translated with the same MT system. For a given system translation, the APE system produces an automatic post-edition that is corrected by the user to generate the final output.

One key feature of the technique proposed in this paper is the application of online learning. Here we describe the concept of online learning, including a brief review of different works that apply this concept to SMT (Section 2.3.1), the specific formulation of the log-linear SMT model with online learning used in this paper (Section 2.3.2) as well as a brief description of how the models are extended from new training samples (Section 2.3.3).

Online learning is a machine learning task that is structured in a series of trials, where each trial has three steps: (1) the learning algorithm receives an instance, (2) a label for the instance is predicted and (3) the true label for the instance is presented.

Online learning fits nicely in MT post-editing tasks, since the output of MT systems is corrected and validated by expert translators. If an SMT system is used to generate the translations, then its statistical models can be modified from the newly generated training pairs. Fig. 2
                            shows a diagram of an SMT system with OL.

During the last years, there has been an increasing interest in developing techniques to adapt or train the features of a log-linear combination in online learning settings. As far as we know, the SMT system with OL proposed by Ortiz-Martínez et al. (2010) (the system used in this paper as it is explained in the following section) constitutes the first work that successfully applies OL to SMT, solving the technical limitations encountered in previous works without the need of introducing heuristic approximations. Such previous works on online SMT include the dynamic adaptation of an IMT system via cache-based model extensions proposed by Nepveu et al. (2004) and the statistical computer assisted translation scenario with online learning proposed by Cesa-Bianchi et al. (2008). In both cases, the proposed systems were heavily limited by their inability to extend the translation models due to technical limitations to efficiently incorporate new parameters in a principled way. The work presented by Hardt and Elming (2010) applies a cache-based strategy similar to that presented by Nepveu et al. (2004), where the translation model is extended by means of heuristic IBM4-based word alignment techniques. IBM-4 word alignment techniques are replaced by phrase alignment techniques to extend the translation model in Bertoldi et al. (2013), Wäeschle et al. (2013). An additional attempt to efficiently extend the translation model was proposed by Blain et al. (2012). Their proposal aligns the output of the decoder with the reference given by the user as a previous step to obtain the word alignments between the source and reference sentences that are necessary to extract new phrase pairs. The method used to align the system translation and the reference sentence is based on the edit distance algorithm since it is assumed that both sentences will be similar.

Here we adopt the online learning techniques described by Ortiz-Martínez et al. (2010). In that work, the authors define an incrementally updateable SMT model for its application in the interactive machine translation framework. Such an SMT model is able to process new training samples one by one, with constant computational complexity (i.e. the complexity does not depend on the training samples that have been previously seen). Moreover, their proposed system has already been implemented in a certain number of SMT prototypes (Ortiz-Martínez et al., 2011; Alabau et al., 2014).

The SMT system described by Ortiz-Martínez et al. (2010) uses a log-linear model to generate its translations. According to Eq. (6), we introduce a set of seven feature functions (from h
                           1 to h
                           7): a n-gram language model (h
                           1), an inverse sentence-length model (h
                           2), inverse and direct phrase-based models (h
                           3 and h
                           4 respectively), a target phrase-length model (h
                           5), a source phrase-length model (h
                           6), and a distortion model (h
                           7). The details for each feature function are listed below:
                              
                                 •
                                 
                                    n-gram language model (h
                                    1):


                                    
                                       
                                          h
                                          1
                                       
                                       (
                                       
                                          
                                             e
                                          
                                       
                                       )
                                       =
                                       log
                                       (
                                       
                                          ∏
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             |
                                             
                                                
                                                   e
                                                
                                             
                                             |
                                             +
                                             1
                                          
                                       
                                       
                                          p
                                          (
                                          
                                             e
                                             i
                                          
                                          ∣
                                          
                                             e
                                             
                                                i
                                                −
                                                n
                                                +
                                                1
                                             
                                             
                                                i
                                                −
                                                1
                                             
                                          
                                          )
                                       
                                       )
                                    ,
                                       2
                                    
                                    
                                       2
                                       |e| is the length of e, e
                                          0 denotes the begin-of-sentence symbol, e
                                          |e|+1 is the end-of-sentence symbol and 
                                             
                                                e
                                                i
                                                j
                                             
                                             ≡
                                             
                                                e
                                                i
                                             
                                             .
                                             .
                                             .
                                             
                                                e
                                                j
                                             
                                          .
                                     
                                    h
                                    1 can be implemented by means of smoothed n-gram language models. Here we adopt an interpolated n-gram model with Kneser-Ney smoothing.


                                    source sentence-length model (h
                                    2): h
                                    2(f, e)=log(p(|f|∣|e|)), h
                                    2 can be implemented by means of a set of Gaussian distributions whose parameters are estimated for each source sentence length.


                                    inverse and direct phrase-based models (h
                                    3, h
                                    4): 
                                       
                                          h
                                          3
                                       
                                       (
                                       
                                          
                                             e
                                          
                                       
                                       ,
                                       
                                          
                                             a
                                          
                                       
                                       ,
                                       
                                          
                                             f
                                          
                                       
                                       )
                                       =
                                       log
                                       (
                                       
                                          ∏
                                          
                                             k
                                             =
                                             1
                                          
                                          K
                                       
                                       p
                                       (
                                       
                                          
                                             
                                                f
                                                ˜
                                             
                                          
                                          k
                                       
                                       ∣
                                       
                                          
                                             
                                                e
                                                ˜
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      a
                                                      ˜
                                                   
                                                
                                                k
                                             
                                          
                                       
                                       )
                                       )
                                    , where h
                                    3 is implemented with an inverse phrase-based model. This phrase-based model is smoothed with an HMM-based alignment (Vogel et al., 1996) model by means of linear interpolation.

Analogously h
                                    4 is defined as: 
                                       
                                          h
                                          4
                                       
                                       (
                                       
                                          
                                             f
                                          
                                       
                                       ,
                                       
                                          
                                             e
                                          
                                       
                                       ,
                                       
                                          
                                             a
                                          
                                       
                                       )
                                       =
                                       log
                                       (
                                       
                                          ∏
                                          
                                             k
                                             =
                                             1
                                          
                                          K
                                       
                                       p
                                       (
                                       
                                          
                                             
                                                e
                                                ˜
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      a
                                                      ˜
                                                   
                                                
                                                k
                                             
                                          
                                       
                                       ∣
                                       
                                          
                                             
                                                f
                                                ˜
                                             
                                          
                                          k
                                       
                                       )
                                       )
                                    
                                 


                                    target phrase-length model (h
                                    5): 
                                       
                                          h
                                          5
                                       
                                       (
                                       
                                          
                                             f
                                          
                                       
                                       ,
                                       
                                          
                                             e
                                          
                                       
                                       ,
                                       
                                          
                                             a
                                          
                                       
                                       )
                                       =
                                       log
                                       (
                                       
                                          ∏
                                          
                                             k
                                             =
                                             1
                                          
                                          K
                                       
                                       p
                                       (
                                       |
                                       
                                          
                                             
                                                e
                                                ˜
                                             
                                          
                                          k
                                       
                                       |
                                       )
                                    , this feature is modelled by means of a geometric distribution. The geometric distribution penalizes the length of the target phrases.


                                    source phrase-length model (h
                                    6): 
                                       
                                          h
                                          6
                                       
                                       (
                                       
                                          
                                             f
                                          
                                       
                                       ,
                                       
                                          
                                             e
                                          
                                       
                                       ,
                                       
                                          
                                             a
                                          
                                       
                                       )
                                       =
                                       log
                                       (
                                       
                                          ∏
                                          
                                             k
                                             =
                                             1
                                          
                                          K
                                       
                                       p
                                       (
                                       |
                                       
                                          
                                             
                                                f
                                                ˜
                                             
                                          
                                          k
                                       
                                       |
                                       ∣
                                       |
                                       
                                          
                                             
                                                e
                                                ˜
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      a
                                                      ˜
                                                   
                                                
                                                k
                                             
                                          
                                       
                                       |
                                       )
                                       )
                                    , a geometric distribution can be used to model h
                                    6, such distribution penalizes the difference between the source and target phrase lengths.


                                    distortion model (h
                                    7): 
                                       
                                          h
                                          7
                                       
                                       (
                                       
                                          
                                             a
                                          
                                       
                                       )
                                       =
                                       log
                                       (
                                       
                                          ∏
                                          
                                             k
                                             =
                                             1
                                          
                                          K
                                       
                                       p
                                       (
                                       
                                          
                                             
                                                a
                                                ˜
                                             
                                          
                                          k
                                       
                                       ∣
                                       
                                          
                                             
                                                a
                                                ˜
                                             
                                          
                                          
                                             k
                                             −
                                             1
                                          
                                       
                                       )
                                       )
                                    , again, this feature function can be modelled by means of a geometric distribution. Such distribution penalizes the re-orderings.

In order to incrementally train the log-linear model, a set of sufficient statistics that can be incrementally updated should be maintained for each feature function. If the estimation of the statistical model does not require the use of the expectation–maximization (EM) algorithm (Dempster et al., 1977) (e.g. n-gram language models), then it is generally easy to incrementally update the model given a new training sample. By contrast, if the EM algorithm is required (e.g. word alignment models), the estimation procedure has to be modified, since the conventional EM algorithm is designed for its use in batch learning scenarios. For those models, the incremental version of the EM algorithm (Neal and Hinton, 1999) is applied. Incremental EM guarantees estimation convergence after each algorithm iteration in a similar way to conventional EM, but E and M steps are individually applied to each training sample.

In this section we identify the sufficient statistics for the main components used in the log-linear combination described above. These components are the language model (feature h
                           1) and the translation model (features h
                           3 and h
                           4). Source and target phrase-length models (features h
                           5 and h
                           6) and the distortion model (feature h
                           7) are implemented by means of geometric distributions with fixed parameters and thus they do not require a complex treatment. Finally, since the sentence length model (feature h
                           2) is implemented by means of gaussian distributions, well known incremental update rules using simple sufficient statistics can be found in the literature (see for instance Knuth (1981)).


                           Sufficient statistics for the language model (h
                           1
                           ). Since language models are implemented using interpolated Kneser-Ney smoothing, probabilities are generated according to the following equation:


                           
                              
                                 (8)
                                 
                                    p
                                    (
                                    
                                       e
                                       i
                                    
                                    ∣
                                    
                                       e
                                       
                                          i
                                          −
                                          n
                                          +
                                          1
                                       
                                       
                                          i
                                          −
                                          1
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          max
                                          {
                                          
                                             c
                                             X
                                          
                                          (
                                          
                                             e
                                             
                                                i
                                                −
                                                n
                                                +
                                                1
                                             
                                             i
                                          
                                          )
                                          −
                                          
                                             D
                                             n
                                          
                                          ,
                                          0
                                          }
                                       
                                       
                                          
                                             c
                                             X
                                          
                                          (
                                          
                                             e
                                             
                                                i
                                                −
                                                n
                                                +
                                                1
                                             
                                             
                                                i
                                                −
                                                1
                                             
                                          
                                          )
                                       
                                    
                                    +
                                    
                                       
                                          
                                             D
                                             n
                                          
                                       
                                       
                                          
                                             c
                                             X
                                          
                                          (
                                          
                                             e
                                             
                                                i
                                                −
                                                n
                                                +
                                                1
                                             
                                             
                                                i
                                                −
                                                1
                                             
                                          
                                          )
                                       
                                    
                                    
                                       N
                                       
                                          1
                                          +
                                       
                                    
                                    (
                                    
                                       e
                                       
                                          i
                                          −
                                          n
                                          +
                                          1
                                       
                                       
                                          i
                                          −
                                          1
                                       
                                    
                                    •
                                    )
                                    ·
                                    p
                                    (
                                    
                                       e
                                       i
                                    
                                    ∣
                                    
                                       e
                                       
                                          i
                                          −
                                          n
                                          +
                                          2
                                       
                                       
                                          i
                                          −
                                          1
                                       
                                    
                                    )
                                 
                              
                           where D
                           
                              n
                           
                           =(c
                           
                              n,1)/(c
                           
                              n,1
                           +2c
                           
                              n,2) is a fixed discount (c
                           
                              n,1 and c
                           
                              n,2 are the number of n-grams with one and two counts respectively), 
                              
                                 N
                                 
                                    1
                                    +
                                 
                              
                              (
                              
                                 e
                                 
                                    i
                                    −
                                    n
                                    +
                                    1
                                 
                                 
                                    i
                                    −
                                    1
                                 
                              
                              •
                              )
                            is the number of unique words that follows the history 
                              
                                 e
                                 
                                    i
                                    −
                                    n
                                    +
                                    1
                                 
                                 
                                    i
                                    −
                                    1
                                 
                              
                            and 
                              
                                 c
                                 X
                              
                              (
                              
                                 e
                                 
                                    i
                                    −
                                    n
                                    +
                                    1
                                 
                                 i
                              
                              )
                            is the count of the n-gram 
                              
                                 e
                                 
                                    i
                                    −
                                    n
                                    +
                                    1
                                 
                                 i
                              
                           , where c
                           
                              X
                           (·) can represent true counts c
                           
                              T
                           (·) or modified counts c
                           
                              M
                           (·) (see Chen and Goodman (1996) for more details).

Under these circumstances, the list of incrementally updateable sufficient statistics for the language model will include c
                           
                              k,1, c
                           
                              k,2, N
                           1+(·), c
                           
                              T
                           (·) and c
                           
                              M
                           (·). In this particular case, the EM algorithm is not required, greatly simplifying the update process for a new training sample (see Ortiz-Martínez et al. (2010) for more details).


                           Sufficient statistics for the translation model (h
                           3 
                           and h
                           4
                           ). Features h
                           3 and h
                           4 are implemented by means of inverse and direct phrase models. Since phrase-based models are symmetric models, only an inverse phrase-based model is maintained. Inverse phrase model probabilities are obtained from the relative frequencies of a set of phrase pairs:
                              
                                 (9)
                                 
                                    p
                                    (
                                    
                                       
                                          f
                                          ˜
                                       
                                    
                                    ∣
                                    
                                       
                                          e
                                          ˜
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          c
                                          (
                                          
                                             
                                                f
                                                ˜
                                             
                                          
                                          ,
                                          
                                             
                                                e
                                                ˜
                                             
                                          
                                          )
                                       
                                       
                                          
                                             ∑
                                             
                                                
                                                   
                                                      
                                                         f
                                                         ˜
                                                      
                                                   
                                                   ′
                                                
                                             
                                          
                                          
                                             c
                                             (
                                             
                                                
                                                   
                                                      f
                                                      ˜
                                                   
                                                
                                                ′
                                             
                                             ,
                                             
                                                
                                                   e
                                                   ˜
                                                
                                             
                                             )
                                          
                                       
                                    
                                 
                              
                           
                        

According to Eq. (9), the set of sufficient statistics for the phrase models is composed of a set of phrase counts 
                              c
                              (
                              
                                 
                                    f
                                    ˜
                                 
                              
                              ,
                              
                                 
                                    e
                                    ˜
                                 
                              
                              )
                           , which following standard estimation techniques, can be extracted from word alignment matrices. More specifically, given a sentence pair and its corresponding word alignment matrix, only those phrase pairs that are consistent with such word alignment matrix are extracted (see Koehn et al. (2003) for more details). Because of this, we also need to maintain direct and inverse HMM-based alignment models. These models are not only useful for smoothing purposes (as it was explained in Section 2.3.2), but also for generating the word alignment matrices that are required to obtain the phrase counts. Since the estimation of HMM-based alignment models requires the use of the EM algorithm, here we need to replace conventional EM by its incremental counterpart, allowing us to modify the parameters of the models for each individual training pair (a more detailed description of the incremental estimation of HMM-based alignment models can be found in Ortiz-Martínez et al. (2010)).

@&#RELATED WORK@&#

In the last few years, there have been different proposals involving APE systems. The majority of such proposals employ SMT to automatically post-edit the translations proposed by rule-based systems (Simard et al., 2007; Dugast et al., 2007; Terumasa, 2007; Lagarda et al., 2009; Béchara et al., 2012). However, statistical APE of an SMT system has also proved its effectiveness (Béchara et al., 2011; Oflazer and El-Kahlout., 2007).

On the other hand, some authors consider APE as a domain adaptation or customization technique (Isabelle et al., 2007; Diaz et al., 2008; Rubino et al., 2012). Domain adaptation constitutes an important topic within the SMT field, with many works tackling the problem under different points of view, such as model interpolation or parameter weighting (Koehn and Schroeder, 2007; Foster et al., 2010), data harvesting (Zhao et al., 2004), or data selection (Lü et al., 2007), just to name a few.

This work also embraces the domain adaptation perspective of APE mentioned above, but putting special emphasis on its applicability to online learning settings. In many real translation scenarios, the set of training samples used to estimate the model parameters is not known a priori, instead, there is a stream of training data that grows continually over time (for instance, the documents to be translated at a translation agency). Under these circumstances, the models used by the system can be continually updated so as to improve the quality of the output. This situation clearly differs from conventional domain adaptation scenarios, where there are closed training sets that are used to estimate the parameters of static models.

The most similar work to our own is presented by Simard and Foster (2013), where the authors propose an APE system in which the automatic post-editing module is implemented as an SMT system with OL. This technique learns post-editor corrections and applies them on-the-fly to further MT output. The authors of that paper prove that this method is effective when translating documents with high levels of internal repetition. This situation is not uncommon, since according to Church and Gale (1995), if a segment is to be repeated, this has the greatest chance of happening within the same document where the segment initially appeared. Under these circumstances, if an error occurs repeatedly, the system will be able to learn the correction provided by the user and apply it when necessary in the following sentences to post-edit. The automatic post-editing module is implemented as a phrase translation system trained from the system translations and the final translations validated by the user. To extract the phrase pairs, the proposed system generates word level alignments based on edit distance. The use of word alignments based on edit distance relies on the heuristic assumption that the system translation and the final translation given by the user are similar. However, this assumption may be difficult to justify with complex or real translation tasks, where the quality of the MT system translations may be low, decreasing the similarity between them and the final translations given by the user (one example of this situation occurs when the MT system is implemented using statistical methods and the sentences to be translated contain poorly represented or unseen events).

Our work differs from the work described by Simard and Foster (2013) in that it replaces the automatic post-editing module based on edit distance word alignments by a fully functional online SMT system that is able to learn from scratch or from previously estimated models in real time (Ortiz-Martínez et al., 2010). In this online SMT system, word level alignment generation is no longer based on edit distance but on an HMM-based alignment model estimated by means of the incremental version of the EM algorithm (see Section 2.3 for more details), removing the heuristic approximations that are required in Simard and Foster (2013) to update the phrase-based model.

In our system, first we machine translate the source document, and later we post-edit its translation in an OL framework. Let us remember that in the real case scenario proposed in the introduction, the user did not have translation memories, nor similar corpora to translate the text. To simulate this situation, we have chosen some RBMT systems, and some free web-based translation applications. In addition, we have contemplated the case where the user has some unrelated corpora to train an SMT system.

In this section, we introduce the base translation systems that we have chosen to post-edit in our experiments.
                           W/O RESOURCES
                           
                              Under the assumption that no translation memories are used, the user can only manually translate or, at most, copy the source text as a starting point. The latter may be useful when untranslatable proper nouns are present or when source and target languages are very similar (e.g., from the same family of languages).

We have employed two RBMT systems:
                                    
                                       •
                                       
                                          Apertium (Forcada et al., 2011), a free/open-source RBMT tool, initially aimed at related-language pairs but recently expanded to deal with more divergent language pairs.


                                          PAHOMTS (Vasconcellos and León., 1985), a proprietary RBMT system developed by the Pan-American Health Organization (PAHO), specialized in the translation of life science texts between English, Portuguese and Spanish.

In the last few years, many web-based translation applications have appeared. In spite of the confidential issues discussed in the introduction, they are broadly used by both professional translators and plain users. They combine many translation paradigms, including RBMT, SMT, TM, semantics, etc., covering a wide range of languages. We have selected the following web-based translation applications:
                                    
                                       •
                                       
                                          Google Translate, https://translate.google.com/
                                       


                                          Bing Translator, http://www.bing.com/translator
                                       


                                          Yandex Translate, http://translate.yandex.com/
                                       

SMT needs a previous training step where statistical models are learnt from the given corpora. Although in our premise there are not similar TM nor corpora to train those models, we have taken into account the option of downloading publicly available corpora to train statistical models, in spite of not knowing if they cover the same domains. We have performed these SMT experiments by means of:
                                    
                                       •
                                       
                                          Moses (Koehn et al., 2007), an open-source toolkit which implements state-of-the-art SMT techniques, in this case trained with out-of-domain data.

Finally, as an ORACLE, we have considered an in-domain SMT case, where Moses was trained with in-domain corpora. These experiments are only taken as a reference, as a best-case scenario where the user has found some completely related corpora to train the statistical models. As we have explained in previous sections, this case is not always realistic, because these corpora are frequently not available. However, we wanted to take them into account to know an empirical upper bound for the proposed techniques.

Once we have translated the source document with one of the previously introduced systems, we will need a post-editing step. In this task, the user will correct the hypotheses given by the MT system in order to achieve the desired final translation.

In our proposal, this post-editing step is performed in an OL framework. As a result, the system will learn from the corrections made by the user, and will use them to train statistical models on-the-fly. These models will be applied to automatically post-edit the next segments, so that once the user has amended a mistake, he or she will not need to amend it again if the mistake appears in the following segments to revise. Since MT post-editing is a repetitive task, OL models can ease this step by automatically amending those repetitive errors. Additionally, in the scenario without resources, OL will not be used as an APE system but rather as a regular SMT system incorporating OL initialized with empty models.

It should be noted that in the two scenarios considered above, namely, APE with OL and regular SMT with OL, the OL module will start with empty models. In spite of the fact that the performance of OL will be initially low due to the lack of training samples, empirical results shown in Section 5.3 clearly reflect the ability of OL to learn without previously existing training data. For instance, it is shown that an SMT system with online learning starting from empty models is able to obtain a better performance than that obtained using a regular SMT system trained from out-of-domain models when translating test documents belonging to different domains and language pairs.

Regarding the software used to carry out the experiments presented in this paper, both the APE module and the regular SMT system incorporating OL have been implemented by means of the Thot toolkit (Ortiz-Martínez and Casacuberta, 2014).

@&#EXPERIMENTS@&#

We have performed some experiments to assess the previously described techniques. In this section, we show the different corpora chosen, the experimental framework, and the results achieved by each of the systems.

We have worked with three different corpora to perform the experiments with our techniques. We have tried to cover different domains and languages, in order to assess the proposed system in different scenarios. Table 1
                         shows some statistics of these corpora.

First, we have chosen the English and Spanish versions of the EMEA corpus. This is a publicly available medical corpus, formed by documents from the European Medicines Agency (Tiedemann, 2009). This is an interesting corpus because it was also chosen by some previous related work (Simard and Foster, 2013), where its suitability for the considered techniques was proven due to the corpus document-like discourse units, and the technical and specialized nature of the texts. However, our results are not straightaway comparable to previous works, because they do not explain the employed partitions, and the involved languages differ.

Second, we wanted to choose a non-public corpus, due to the fact that some of the web-based translation applications against which we wanted to compare could have incorporated those public corpora into their training material. Thus, we chose the English and Spanish versions of the Xerox corpus. This corpus was compiled in the European project TransType2 (Esteban et al., 2004) from printer manuals provided by Xerox, one of the project partners.

Third, in order to cover different domains and languages, we chose the i3media corpus. It is a large corpus composed by newspaper articles in Catalan and Spanish (Lagarda et al., 2010). Its sentences are written in a richer language (with a more extensive vocabulary), covering several domains. In addition, Catalan and Spanish are similar languages, so we will be able to explore how our proposals behave in this situation.

Finally, as we have explained in previous sections, we have taken an out-of-domain corpus to assess the proposed techniques. For this purpose, we have chosen the seventh version of Europarl (Koehn, 2005).

We have divided each corpus in training, development and test sets. In order to preserve some of the context, we have grouped the sentences in blocks before randomly shuffling them and dividing them among the partitions. For clarity reasons, we only show figures for training and test partitions. Development numbers are similar to those from the test set, and it is used to tune the statistical model parameters of the offline SMT systems, where applicable.

Out-of-vocabulary (OOV) and perplexity figures give an idea of the complexity of each corpus. Perplexity is defined as the geometric average probability assigned by the model to each word in the test set (see for example Chen and Goodman (1996) for a formal definition). In Table 1, the perplexity has been computed for a 5-gram language model estimated from each (in-domain) training set using the IRSTLM toolkit (Federico et al., 2008). It is interesting to note how the Xerox task has radically different perplexities in each language. This can be related to the original language of the corpus (English) and its less complex translations into Spanish (Lembersky et al., 2012). Also, note that perplexities increase substantially with each corpus used, indicating that the latter are more complex than the former, and thus, the texts are richer.

Finally, Table 1 also shows the repetition rate (Bertoldi et al., 2013) for each test set. The repetition rate provides a quantitative measure of the degree of document-internal repetition for a given corpus. For this purpose, this measure looks at the rate of non-singleton n-grams contained in a given text. More specifically, the rates of non-singleton n-grams from n = 1–4 are calculated and geometrically averaged, using a sliding window of 1000 words to make the rates comparable across different sized corpora. Here we use a slightly modified version in which the sliding window calculation is removed, since in real translation scenarios, the text to be translated is available beforehand and should be completely translated. Our modified version of the repetition rate, RR’, is defined as follows:


                        
                           
                              (10)
                              
                                 
                                    RR
                                    ′
                                 
                                 (
                                 I
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                ∏
                                                
                                                   n
                                                   =
                                                   1
                                                
                                                4
                                             
                                             
                                                
                                                   |
                                                   
                                                      I
                                                      
                                                         n
                                                         ,
                                                         1
                                                         +
                                                      
                                                   
                                                   |
                                                   −
                                                   |
                                                   
                                                      I
                                                      
                                                         n
                                                         ,
                                                         1
                                                      
                                                   
                                                   |
                                                
                                                
                                                   |
                                                   
                                                      I
                                                      
                                                         n
                                                         ,
                                                         1
                                                         +
                                                      
                                                   
                                                   |
                                                
                                             
                                          
                                       
                                    
                                    
                                       1
                                       /
                                       4
                                    
                                 
                              
                           
                        where |·| represents the length of a given set, 
                           
                              I
                              
                                 n
                                 ,
                                 1
                                 +
                              
                           
                         represents the set of different n-grams contained in the in-domain corpus 
                           I
                        , and 
                           
                              I
                              
                                 n
                                 ,
                                 1
                              
                           
                         represents the set of different n-grams occurring only once in 
                           I
                        .

A more repetitive task will take advantage of the OL techniques (Simard and Foster, 2013). Repetition rates in Table 1 show how EMEA is a rather repetitive corpus, which can boost OL techniques. On the opposite side, i3media presents a lower repetition rate. It is interesting to explain the reason behind the differences in the observed repetition rates for each corpus. For this purpose, we can take a look to the way in which each of them was created. As it was mentioned above, the three used corpora consist of a collection of documents. Each one of the documents define some sort of a sub-domain that may change radically in the subsequent document. Analyzing the frequencies for the different document lengths, we observed a huge difference in those of i3media, the corpus with the lowest repetition rate, with respect to the document lengths for the other two corpora. More specifically, we found that the median length of an i3media document was 8 sentences, with 95% of the documents being shorter than 30 sentences. These figures are much higher for the other corpora, which presented documents composed of hundreds or even thousands of sentences. Therefore, the lower length of the i3media documents caused a much more frequent domain drift than that observed for the other two corpora, greatly decreasing the repetition rate.

We think it is important to stress out that the necessity of having a certain degree of repetition, so as to obtain translation quality improvements, it cannot be seen as a specific limitation of our proposal, but as a limitation of domain adaptation techniques in general. In batch learning scenarios, domain adaptation data should be representative of the document to be translated in order to observe gains in translation quality. In other words, to ensure the correct translation of a given n-gram contained in the test corpus, it is necessary that this n-gram or similar ones have been seen in the domain adaptation data. When we operate in online learning scenarios, we still have the same requirement but now the training and translation stages are no longer separated. Therefore, measuring the repetition rate of a given document could be seen as the equivalent of evaluating the representativity of domain adaptation data in a batch learning setting. In addition to this, sufficiently high repetition rates for test documents are common, according to the document-internal repetition property (Church and Gale, 1995).

For each one of the corpora (EMEA, Xerox, and i3media), we have carried the following experiments out:
                           W/O RESOURCES
                           
                              We consider the case where there is no base MT translation system, so the user manually translates the source language sentences.

Translation of the test set by means of the base RBMT systems. Depending on the languages pairs, we have employed Apertium and PAHOMTS.

Translation of the test set by means of the base web systems. We have employed the translation engines from Google, Bing, and Yandex.

In our premise, there are no in-domain corpora to train SMT models. To simulate this scenario, we have chosen an out-of-domain corpus (Europarl) to train the statistical models and translate each corpus test by means of Moses. All the Moses models have been trained employing the standard procedure, using a development set from the corpus to optimize their weights with MERT. These results are labelled in the plots as Moses-OOD.

In addition, as an optimistic scenario where we have a big enough in-domain corpus, we have trained SMT models with the corresponding training set of each corpus, and we have tuned them with their development sets. These results appear in the plots labelled as Moses-ID, and they are taken into account as an optimistic upper bound for these techniques, due to the fact that in the real scenario that we have considered, in-domain corpora are not frequent (Irvine et al., 2013). This is why we have tagged it as ORACLE in our plots. We have also tagged Google as an ORACLE, separating it from the rest of MT systems in those cases where we suspect that Google has probably been trained with in-domain corpora.

On the other hand, user corrections in the OL framework have been simulated with the reference translations of the test sets. After an empty initialization of the OL models, the process would be as follows for each test sentence:
                           
                              1.
                              The system provides the translation of the sentence (in the first sentence of each corpus, as the models are empty, it will propose the translation of the baseline system, or the source sentence in the w/o resources case).

The user corrects the proposed translation (in our simulation, we use the reference translation directly, as a simulation of the user corrections).

The system learns from the corrections in order to enrich its models, which will be used to translate the next sentence.

In APE, this process takes, as source sentences, the translations given by the previous MT system.

We have automatically evaluated each system by means of BLEU (BiLingual Evaluation Understudy) (Papineni et al., 2002) score, that has been extensively used in the SMT literature to measure the similarity between to texts. More specifically, BLEU score computes the geometric mean of the precision of n-grams of various lengths between a hypothesis and a set of reference translations multiplied by a factor BP(·) that penalizes short sentences:
                           
                              
                                 BLEU
                                 =
                                 BP
                                 (
                                 ·
                                 )
                                 exp
                                 
                                    
                                       
                                          
                                             ∑
                                             
                                                n
                                                =
                                                1
                                             
                                             N
                                          
                                          
                                             
                                                log
                                                
                                                   
                                                      p
                                                      n
                                                   
                                                
                                             
                                             N
                                          
                                       
                                    
                                 
                              
                           
                        where p
                        
                           n
                         denotes the precision of n-grams in the hypothesis translation. Typically, a value of N
                        =4 is used. In our experiments, BLEU score gives us an idea of the effort needed by the user to post-edit the proposed translation. In addition to this, we have tested the statistical significance of these results computing confidence intervals (95%) according to the method explained by Koehn (2004).

@&#RESULTS@&#

This section is devoted to the study and analysis of the experimental results for the different proposed corpora.


                           Fig. 3
                            compares the BLEU of the baseline systems with respect to their APE counterparts for the EMEA corpus from English to Spanish. In general, it can be observed how online APE outperforms significantly its baseline system translations. This means that OL models are able to learn from user corrections, and apply them successfully to post-edit the following sentences by adapting the baseline translation to the specific domain of the task.

Additionally, we can see some interesting results. OL improvements are more important when post-editing translations from systems with poorer translations (Apertium, PAHOMTS, and Moses-OOD), but are also very noticeable for the higher baseline WEB systems. With the ORACLE systems (Google and Moses-ID), post-editing is not able to improve translation quality. Although online APE systems perform a bit worse for them, differences are not statistically significant, i.e., the error bars overlap. Moreover, we should note that when SMT models are trained with out-of-domain corpora, their translations are much poorer than with in-domain data, as we expected. That is why in-domain training data is so critical for customizing SMT engines.

The extremely good Google baseline results could be explained by the fact that EMEA is a public corpus, and probably Google has incorporated the corpus to train its models. In fact, if we compare Google results with those achieved by Moses-ID (which is trained with the EMEA corpus), we see that they are quite similar. Moreover, the rest of web-based translation applications (Bing, Yandex) perform worse than Google, whereas in other corpora they perform more similarly. This confirms our suspicions to some extent. Also, both Bing and Yandex base results are improved when applying our technique pointing out that these WEB systems do not possess specific domain information.

In addition, a standard online MT system is represented in the W/O RESOURCES experiment. Online MT assumes an OL SMT system that is initialized with empty models. By default, words that are unknown are left as they are in the source sentence. Thus, for the first sentences, the user has to post-edit the source sentences. However, as the system learns from the correct translations, the post-editing task is more similar to a classical SMT post-editing task. Online APE systems are also initialized with empty models, but the output from the baseline system is copied into the target sentence instead of the source sentences. In this way, BLEU scores for the first sentences are expected to be higher for online APE systems than for online MT systems.

From the results, we can conclude that online APE systems outperform the online MT baseline. This implies that OL takes advantage of the translations given by the baseline MT systems more than when translating directly from the source language. Furthermore, we can see that online MT is better than the RBMT and SMT baselines because it is able to learn properly from the user translations. However, it cannot reach the quality of WEB systems, which are supposedly trained with huge amounts of data.

With respect to the Spanish–English experiments, we can observe that Fig. 4
                            is essentially the same as Fig. 3. Therefore, we can conclude that, in this case, the languages direction does not pose additional problems.

Moreover, it should be noted that these results are obtained with the same corpus as in Simard and Foster (2013). However, our results are not directly comparable to those from Simard and Foster (2013), because our partition and languages are not the same as theirs. Nevertheless, we can see a similar behavior. When analyzing the out-of-domain SMT experiments, our method achieves around +[8.6,10.8] BLEU points compared to +[6.6,7.4] BLEU points in Simard and Foster (2013).

As we have seen with the EMEA results, Google results were biased under the suspicion that Google probably included the EMEA corpus into their training material. Thus, we decided to repeat our experiments with a corpus that (hopefully) was not included in Google training.


                           Fig. 5
                            shows how Google performs much worse with respect to Moses-ID. In fact, it has not been considered as ORACLE. However, Google is still able to outperform their WEB counterparts. In addition, it is the only system, apart from Moses, where our technique, although achieving some improvements, is not able to achieve statistically significant results. In general, these results are quite consistent with those from EMEA. In almost all cases, our technique can improve the base translations with statistical significance.

When moving to the Spanish to English version of Xerox (Fig. 6
                           ), we can see that the improvements of our technique are coherent with those from English to Spanish. Nevertheless, BLEU scores are smaller here. This can be because the original corpus was generated translating from English to Spanish. As shown in Table 1, English perplexity is greater than the Spanish one, which points out that Spanish was translated in a, somehow, simplified version of Spanish. That is, since the original Xerox manuals were written in English and after translated to Spanish, part of the source language complexity was lost (Lembersky et al., 2012).

Finally, we wanted to apply our technique to a different pair of languages. We chose the i3media corpus, which translates newspapers articles between Spanish and Catalan. Figs. 7
                            and 8
                            show how, in contrast to the previous corpora, our technique is not able to improve the baseline systems translations. Due to the fact that both languages are very similar in lexic and syntax, Apertium, Google and Moses achieve very good baseline translations in both directions, suggesting that it is possible that OL APE does more harm than help.

However, even for Bing and Yandex, whose base BLEU are smaller, it cannot take advantage of our technique, which worsens their initial scores. In the latter cases, it is even preferable to directly translate from the source language in an OL framework.

In order to explain this behavior, we have to take into account that the n-gram repetition rate in the i3media corpus is much lower than in Xerox or EMEA (see Table 1). As it was already commented in Section 5.1, OL techniques are expected to perform better in repetitive tasks, because they will learn to correct those mistakes appearing several times in the test set. For instance, the first time that a mistake occurs, it will be corrected by the post-editor. If this mistake appears again in a posterior sentence, OL models will correct it automatically. This is probably why OL can improve base systems results in EMEA and Xerox, but not in i3media.

On slightly different note, we can see that the online MT baseline is very high in i3media, although the corpus repetition rate is low. This is due to the fact that the involved languages (Catalan and Spanish) are very similar (Lewis, 2009) in lexicon and word order. In fact, if we did not change a word from the source test set, we would have more than 14 points of BLEU when comparing to the reference translation (in the plots, labelled as W/O resources). This is exactly what happens when a translation system faces an unknown source word, it outputs that word to let the user translate it (or leave it unaltered in the case of being invariable in both languages). In first iterations of OL, that is, when post-editing the first sentences, statistical models are empty. Thus, every source word will be unknown and copied to the output for these first sentences. As a result, when languages are similar, OL baseline BLEU will be higher.


                           Figs. A1, A2, A3, A4, A5 and A6 in Appendix A show the evolution of BLEU and repetition rate for a window of the latest 100 sentences, as the user completes the translation. Each plot displays four values. First, filled with brown color, the repetition rate shows the degree of repetition of the latest 100 sentences with respect to the previous sentences, as computed by Eq. (10). Second, the green line represents the BLEU evolution of the Thot-OL system, i.e., plain online MT without previous resources. Its purpose is to provide a baseline where APE is not considered but only online learning. On the contrary, the blue line represents a baseline where a black box system is used, but also without APE nor online learning. Finally, the red line is the online APE system that we want to analyse. Note that we expect that the red line is always above the others. If the blue line is above the red line, that would mean that online learning is not being helpful to improve the black box system. On the other hand, if the green line is above the red line that would mean that online learning does not benefit from performing APE over the black box system. Also note that, as the online MT system and the repetition rate do not depend on the kind of black box system, they are always the same for the same corpus and language direction. In addition, the curves might seem very similar for the same corpus but for the reverse language direction since actually the sub-domains present in each of the 100-sentence blocks are the same regardless of the language direction, hence obtaining similar repetition rates.

With respect to the results, it must be pointed out that the performance of the online MT system is quite correlated to the repetition rate of the latest 100 sentences. When there is a peak in the repetition rate, we can deduce that the latest block of 100 sentences presents a more homogeneous input than for the previous sentences. Then, the online learning system is able to learn from this repetition to proportionally improve the BLEU scores. In fact, in EMEA the correlation is r(4826)=0.55 for (en-es) and r(4826)=0.5 for (es-en), whereas in Xerox it is r(1124)=0.62 for (en-es) and r(1124)=0.74 for (es-en), all with p
                           <.001. This indicates that repetition rate is a good predictor for the BLEU in the online MT system. However, i3media also behaves unexpectedly in this regard with r(5099)=0.12 for (ca-es) and r(5099)=0.10 for (es-ca) with p
                           <.001, implying that repetition rates and BLEU are not correlated for this task. This confirms again our suspicion that i3media is a case with special properties for which our technique does not perform as expected.

In general, the dynamics are very consistent between all the systems for a given corpus. However Xerox dynamics presents some steep drops and rises in BLEU windows. Their main reason is the fact that the corpus is not shuffled. This corpus is composed of several blocks of sentences with slightly different sub-domains. These sudden changes in BLEU windows indicate that a new subdomain block has begun. To confirm this, we have calculated the percentage of those n-grams present in the test that were not seen in the training. For instance, we obtain that only a 9.8% of the n-grams appearing in test sentences from 400 to 600 do not appear in the training set. This is why BLEU for these sentences is quite high. On the other hand, when moving to the 800–1000 test block, a 16.8% of the n-grams do not appear in the training set. This explains the sudden drop around these sentences for Moses in-domain experiments.

Second, in almost all cases, RBMT+APE systems outperform their respective RBMT systems, except probably for the first sentences where the APE system has not received enough training data. For instance, Fig. 9
                            shows the first APE changes that appear in some of the corpora. In EMEA (Fig. 9a) and i3media (Fig. 9c) we can observe a series of meaningless deletions and substitutions provoked by phrases extracted in the previous sentences from uniformly aligned models, which were obviously wrong. On the contrary, in Xerox (Fig. 9b) almost any APE system has learned Scanning and Services, since they have been repeated in the previous 6 sentences. This repetition has allowed to build better alignment models from the beginning. The effect of the first sentences being wrongly post-edited with APE is especially noticeable for the i3media corpus since Apertium obtains a very good BLEU baseline around 78 points of BLEU. However, after more than 1000 sentences, Apertium+APE achieves small but persistent improvements over Apertium alone. That is, although our technique is not better than Apertium overall in i3media, it can improve the baseline after some sentences have been learned by the online APE system. In fact, if we use the baseline Apertium system for the first 1000 sentences until the Apertium+APE system has learned and then switch to it, we can reach 79.4 BLEU points for (es-ca) and 76.8 for (ca-es), which is better that either system alone. Hopefully, online APE systems begin to compensate the problem with uniformly initialized models soon enough so that the final BLEU score is usually better for online APE systems than for baseline systems. In Fig. 10
                            we can see how APE is able to amend some of the errors produced by the baseline systems in a way that BLEU scores increase. In particular, in Fig. 10a APE is able to learn the proper casing of SINGULAIR in the English documents but also it is able to substitute chewable tablets in RBMT systems and Moses-OOD. APE is also useful to amend casing in Fig. 10b. In addition, APE has been able to fix the lexical choices and word ordering of Font Management Utility, which should be consistent throughout all the printer manual. Finally, Fig. 10c shows how the APE system is able to modify a couple of lexical choices that are originally correct but probably the publisher prefers them to be translated as Eso and ocurrió.

Third, we should note that Moses-OOD behaves in a similar way to RBMT systems, probably due to the fact that it was trained only with an out-of-domain corpus. Similarly, most web-based translation systems are trained with corpora from multiple domains, but not likely with a corpus with the same exact domain of our test sets. In these cases, the SMT+APE system is able to improve the baseline system in most of the cases, most notably where the peaks in repetition rate appear but the baseline system BLEU drops. On the other hand, in the valleys where we can perceive a drop in BLEU, the SMT+APE system tends to perform worse than the SMT system until the online learning algorithm adapts to the new sub-domain present in the 100-sentence window.

Additionally, Moses-ID and Google, which has been arguably trained with a bigger set of corpus and domains, are more difficult to beat by adapting the output with the APE system. This might be caused by these systems capturing already the nuances of the domain of the test set.

Finally, the SMT+APE models perform always worse than their SMT counterparts in the i3media corpus. As we have already explained, the language pairs are quite easy to learn for an SMT system, which is reflected by the high BLEU scores. In addition, this particular corpus presents a small repetition rate throughout all the test set. In the end, the APE system is not able to capture effectively any of the domain nuances more than the original SMT systems did.

@&#CONCLUSIONS@&#

In this work we have analyzed a real translation scenario, where a human expert needs to translate a document without having any similar translation memory, nor in-domain corpus to train SMT models. We have proposed the application of automatic post-editing in an online learning framework, similar to that shown in Simard and Foster (2013), but replacing the automatic post-editing module based on edit distance word alignments by a fully functional online SMT system (Ortiz-Martínez et al., 2010) that successfully removes any heuristic approximations introduced in previous works to update the model parameters.

We have assessed our technique with three corpora, namely, EMEA, Xerox and i3media, covering different domains and pairs of languages. Additionally, in the reported experiments we have combined our automatic post-editing module with three different kinds of base systems, including RBMT, Web and SMT systems. According to the obtained results, for the EMEA and Xerox corpora our system was able to significantly outperform the results obtained by all of the base systems. More specifically, improvements of up to 16, 11 and 7 BLEU points were obtained for the RBMT, Web and SMT systems respectively.

However, the proposed technique did not behave properly when facing our third corpus, i3media. As we have seen in our experiments, this is explained by the fact that the repetition rate of the n-grams appearing in the test set is lower than that observed for the other corpora used in the experimentation. From the results, we have proved the importance of n-gram repetitions to take advantage of OL techniques. This requirement has been previously suggested in other works (Simard and Foster, 2013). However, we think this is not an exclusive limitation of online learning but also of domain adaptation techniques in general (see Section 5.1 for a detailed explanation). On the other hand, in online learning scenarios, the document internal repetition property (Church and Gale, 1995) predicts a high probability of observing similar sentences composed of similar n-grams in a given document.

@&#ACKNOWLEDGEMENTS@&#

Work partially supported by the European Union 7th Framework Programme (FP7/2007-2013) under the CasMaCat Project (Grant Agreement No. 287576), by Spanish MICINN under Grant TIN2012-31723, and by the Generalitat Valenciana under Grant ALMPR ALMAMATER (PROMETEUII/2014/030) and under Grant IMASI (ISIC/2012/004).

See Figs. A1–A6
                     
                     
                     
                     
                     
                     .

@&#REFERENCES@&#

