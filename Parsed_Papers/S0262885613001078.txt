@&#MAIN-TITLE@&#Efficient and robust model fitting with unknown noise scale

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           robust scale estimation without a breakdown point


                        
                        
                           
                           insensitive to inlier noise distribution


                        
                        
                           
                           efficient and practical application to many computer vision problems


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Robust estimation

RANSAC

Scale estimation

Structure from motion

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

Data fitting (i.e., estimating the parameters of some hypothetical model that best explains a set of data measurements) is a critical task that arises in many disciplines. In general, the measurement set will contain some unknown fraction of outliers, and the good measurements will be subject to some unknown scale of noise, typically assumed to be Gaussian.

Least squares (LS) estimators, which minimize the sum of squared residual errors, are efficient and optimal for Gaussian noise [1], but are highly sensitive to outliers [2]. The breakdown point of the LS estimator is 0% because the estimate may be arbitrarily skewed when the percentage of outliers is greater than 0% ([3], p.9). A more robust approach is the least median of squares (LMS) estimator [4], which minimizes the median of squared residuals, and has a breakdown point of 50%.

Accumulator-based methods (e.g., the Hough transform [5,6] or variations such as the Randomized Hough Transform [7]) have no specific breakdown point, and are popular for simple line and curve detection. However, they are non-optimal, and are limited in their general applicability because they require discretization of a p-dimensional space for models with p parameters, which would result in prohibitively high time and space complexity for many problems.

Perhaps the most well-known and generally applicable robust estimator without a breakdown point is RANdom SAmple Consensus (RANSAC) [8]. RANSAC uses a hypothesize-and-test framework by randomly sampling subsets of the measurement set, and retaining the model that maximizes the number of inliers according to some threshold. Because it does not require discretization of the search space, estimation of high-dimensional models is computationally feasible, and there is no breakdown point beyond the minimal fraction necessary to define a model.

Due to its success, there have been many popular RANSAC variations. To summarize, robust M-estimators [9] were used for model evaluation with MSAC and MLESAC [10], the inner optimization from LO-RANSAC [11] attempts to compensate for the unrealistic assumption that all models estimated from uncontaminated (albeit noisy) data are good, and explicit testing for degenerate configurations has been incorporated in DEGENSAC [12] and QDEGSAC [13]. Other improvements have focused on performance optimizations by using heuristic bail-out tests [14–16] or guided sampling as in PROSAC [17], Preemptive RANSAC [18], and ARSSAC [19]. See [19] for a more thorough survey.

All of these aforementioned RANSAC variations implicitly require accurate a priori knowledge of the scale of inlier noise in order to choose the threshold. In many cases, it is possible to choose a reasonable threshold based on domain knowledge, but the sensitivity to this choice is undesirable and can sometimes result in instability. Indeed, it has been often noted that given a bad choice of threshold, RANSAC will completely break down [2,20].

One of the first approaches attempting to overcome this limitation was to first make a robust estimate of the model using LMS and then make a robust estimate of scale using the median squared residual [3], as proposed in [21]. However, because LMS and the median scale estimate both have 50% breakdown points, this method cannot be applied to data sets with more than 50% outliers, as is often the case.

More recent RANSAC variations have attempted to incorporate scale estimation with model estimation. For example, ASSC [2] modified the RANSAC objective by maximizing the inlier count divided by a robust estimate of scale. However, there is no statistical support for this modified objective, and it does not always detect the correct scale. ASSC also retains adaptive sampling from RANSAC, but not in a statistically valid way, and this can lead to premature convergence at grossly over-estimated scales when the outlier ratio is high. A similar approach was taken in [22], who suggest modifying the objective function to seek the model which minimizes their proposed weighted median absolute deviation (WMAD) estimate of scale.

Projection-based M-estimators (pbM-estimators) were used in the ‘projection pursuit’ approach of [23], and some performance enhancements were proposed in [24–26]. The most recent and best-performing technique along these lines is ASKC [27].

ASKC is an improvement upon the original ASSC algorithm, with a more statistically motivated objective function. The basic idea is to choose the random model hypothesis that maximizes a kernel density estimate (KDE) in residual space centered at the origin. ASKC also abandons the earlier attempt at adaptive sampling from ASSC, and instead uses a fixed number of samples. The recognition that adaptive sampling does not work in this context is a significant limitation in comparison to the original RANSAC algorithm, because using a fixed number of samples either prevents good performance in the presence of high inlier ratios (excessive sampling), or induces a breakdown point in the presence of low inlier ratios due to not enough sampling to find the structure within the data.

A subtle theoretical problem with ASKC is that the method was derived based on the assumption that the residual distribution should be normal and hence have a mode at the origin, but in their experiments is often applied to the distribution of squared fitting errors which has, in general, a scaled χ
                     
                        k
                     
                     2-distribution with a non-central mode that depends on σ for k
                     >2 (Section 3).

Ultimately, the greatest limitation of both ASSC and ASKC is that they attempt to estimate the scale directly from the fully contaminated set of residuals, which is an inherently difficult problem to solve under high outlier ratios. The proposed Two-Step Scale Estimator (TSSE) from [2], which is used by both methods, relies on a KDE of the residual error distribution and is capable of functioning under high outlier ratios, but only if the kernel bandwidth is chosen properly. Automatic methods for choosing the bandwidth rely on an accurate estimate of scale. Thus, it is somewhat of a ‘chicken-and-egg’ problem.

Wang et al. propose obtaining the initial estimate using the k-th order statistic, which we find sometimes works and sometimes does not. Outlier contamination can lead to over-estimated bandwidth, in turn leading to oversmoothing of the KDE, and finally poor scale estimation with TSSE. To counteract this oversmoothing they propose using a fraction c
                     
                        h
                     
                     ∈(0,1) of the automatically derived bandwidth, but we find that there is no ‘one size fits all’ value of this parameter, because it depends largely on the scale and distribution of outliers. In summary, the overall sensitivity of ASKC to the scale estimator leaves us unsatisfied.

Another recent approach to automatic scale estimation is based on the recognition that RANSAC tends to exhibit the greatest consistency in the discovered models when the threshold is set near the true scale level. This observation was first exploited in StaRSaC [28], which performs a brute force search across a wide range of logarithmically spaced scales, repeating RANSAC at least 30 times at each level, in order to identify the scale at which the Variance of the estimated model Parameters (VoP) is minimized.

A notable disadvantage of this approach is high computational cost: even with a modest granularity of 100 scales, this would require running RANSAC about 3000 times. One must also consider that running RANSAC with too small a scale imposes a near-zero inlier ratio, which requires an exponentially larger number of samples for the adaptive convergence criterion. This problem can be partially avoided by using an artificial limit on the number of iterations, although such a limit might prevent the true structure from being found if the outlier ratio is high.

Another more subtle problem is a dependence on model parameterization, because the algorithm assumes that the VoP is indicative of ‘structural variation’ of the model. Variance is completely meaningless for over-parameterized models (such as homogeneous entities), and even after projecting into a minimal parameterization (e.g., by performing a homogeneous division), variance is still not an accurate reflection of structural variation. For example, if one projects the homogeneous equation of a line into the familiar form of y
                     =
                     mx
                     +
                     b, one is likely to observe extremely high variance in the b parameter for near-vertical lines, which is much greater than the variance would be for a set of nearly horizontal lines of equal angular variation. Thus, in order to obtain good results for any particular problem, one may need to spend a great deal of effort into finding a parameterization in which the VoP corresponds well to structural model variation. This alone makes it unsuitable as a generic estimation routine.

A related problem is that the algorithm requires comparing models to find the largest-scale model that is ‘consistent’ with the model at the scale that minimizes the VoP. In their implementation, model consistency is assessed by using the Frobenius norm of model parameters with some unspecified threshold, but the Frobenius norm of model parameters is generally not an accurate measure of ‘structural difference’ between models. Furthermore, choosing this threshold automatically is implicitly related to the noise level, and is arguably no simpler than choosing the original RANSAC threshold.

Lastly, the scale cannot be estimated more finely than the search discretization, and while it is generally true that the lowest variance occurs around the true scale, this is not always the case. For example, if one is fitting lines to point data in 
                        
                           R
                           2
                        
                     , and there are two large outliers outside of a data set, then any threshold large enough to connect these two outliers will consistently result in the bad line connecting those outliers. Also, whenever there is a low outlier ratio, the scale encompassing all data points will be preferred over the true scale.

A more recent approach in the same spirit as StaRSaC is RECON [29], which also attempts to determine scale based on the recognition that model variance is low around the true scale, but with one major difference: rather than explicitly looking for low variance in the model parameters, RECON looks for models with low variance in the sort-order of residuals (or fitting errors).

RECON forms model hypothesis from randomly selected minimal subsets until K
                     ≥3 models with mutually α-consistent residual sets have been found. The α-consistency test searches for the smallest n-value such that the data points associated with the n smallest residuals have more than α
                     2 percent overlap, with α
                     =0.95. It is assumed that this n-value represents the separation between inliers and outliers. Although this test is statistically inspired, there are a number of practically occurring situations in which it fails.

First, it rests heavily on the implicit assumption that the fitting errors for inliers between any two good models will occur in random order. However, if one compares two identical models, then the fitting errors must have the exact same sort-order, and thus α-consistency would pass at any value, such as the minimal value of n
                     =1, meaning that none of the inliers are detected.

In the presence of noise, it is unlikely to find two identical models, but the similarity of sort-order can still be expected to be similar for similar models. This is mostly a problem in the fourth step of RECON, which calls for making M
                     =30 over-determined model estimates from outlier free data, and then taking the minimal n-value that passes the α-consistency test between all pairs. Because these estimates are over-determined and outlier free, it should be expected that some of these models are very accurate and hence very similar, and hence it would not be surprising if there were some pair of models with a very similar sort order, leading to a greatly under-estimated n-value. Because RECON then re-estimates the final model from this minimal number of points, it would destroy the model estimate.

Another problem is that, for very small values of n, the α-consistency test can easily pass for inconsistent models by pure chance. For example, consider a line fitting problem with two perpendicular intersecting line models that both have the smallest fitting error to the same point nearby their intersection. In this case, the normalized overlap θ
                     1
                     
                        i,j
                     
                     =1, and thus the two models will be deemed as α-consistent, despite that these models are not at all consistent in their support regions. Although the individual probability of this for a single trial is low, given a sufficiently large number of α-consistency trials, the probability of this problem occurring at least once becomes very large.

This problem can occur for arbitrarily large n values, and may be exacerbated by the distribution of data points, because the only condition is that two inconsistent models happen to share some of their lowest residuals in common order (e.g., the models ‘pivot’ around a similar point in the data). Thus, if the data distribution inherently supports a region of common points that are likely to be shared by many different models (such as a bow-tie distribution for line fitting), then the same problem may be common for larger values of n. In the case of F-matrix estimation, a large number of points on a common plane could create this problem, even if the data also contains a significant number of off-plane points as well.

The runtime performance of RECON can be prohibitive, because for each RECON hypothesis, one must test for α-consistency with all prior RECON hypothesis — and each test for consistency requires a brute force search through the residuals at all possible scales. Thus, despite that the overall number of samples is low, this high time complexity coupled with the large number of consistency checks required to find a set of mutually consistent models can quickly result in excessive runtime for low outlier ratios.

RECON also has difficulty with data sets that may contain multiple structures, because it returns the first significant structure that is found, which is not necessarily the most dominant structure in the data.

In the context of multiple model fitting, a number of methods have been proposed that also incorporate automatic scale selection [30–32]. However, [32] have already developed ASSC and ASKC which are optimized for single-model estimation, [31] effectively generalizes the principle used by RECON to the multiple model fitting problem, and the method of [30] does not work for single-model estimation problems. Thus, we will not consider these more complex and computationally intensive multiple-model fitting methods further.

To summarize, it is hard to justify the greatly increased computational cost or reduction in reliability that is associated with using any of the aforementioned RANSAC variations that incorporate automatic scale selection, especially for computer vision problems, where the errors are generally measured in image space, and one can often assume a sub-optimal threshold that works ‘acceptably well’ based on the assumption that image correspondence error is on the order of a pixel or two, as is done with current state of the art Structure from Motion systems like Bundler [33,34].

This is not always the case, as one might be using a subpixel matching algorithm for low-baseline pairs leading to subpixel errors, or one might be performing wide-baseline matching with multi-scale features that have significantly larger errors, or tracking points across multiple frames resulting in the accumulation of errors, or dealing with images of varying sizes and qualities leading to unpredictable error levels. Thus, automatic scale estimation is still preferable, if it could be done reliably and efficiently.

Our recognition is that it is usually not difficult to specify a conservative maximum scale, and doing so permits the development of a new approach to the scale estimation problem without the large sacrifices in efficiency or reliability that are associated with completely unbiased searching through scale space. This is the motivation behind the proposed Simultaneous Fitting and Scale Estimation (SIMFIT) algorithm.

Like the original RANSAC algorithm, SIMFIT is simple to implement, is applicable to arbitrarily-high dimensional data, has no specific breakdown point, is independent of model parameterization, and uses the same statistical convergence criterion to adapt the number of iterations. It does not require any additional parameters, and does not significantly increase computational cost or reduce reliability. Furthermore, SIMFIT is designed to be fully general, and not just limited to computer vision problems.

We begin by introducing the theoretical background for classifying inliers (Section 2) and estimating scale from the residuals or fitting errors (Section 3). We then introduce the SIMFIT algorithm (Section 4), clarify the parameters of algorithms compared (Section 5), and present our experimental results (Section 6), starting with a validation of the assumed noise distribution (Section 6.1), followed by an empirical comparison of accuracy and performance on line fitting and homography estimation (Section 6.2) and finally a real-data experiment with fundamental matrix estimation (Section 6.3). Our results show that SIMFIT produces a model estimate with greater likelihood, more accurately estimated scale, and lower computational cost.

For some implicit p-dimensional model defined by parameters 
                        
                           θ
                           ∈
                           
                              R
                              p
                           
                        
                     , let the function 
                        
                           f
                           
                              
                                 x
                                 |
                                 θ
                              
                           
                           :
                           
                              R
                              n
                           
                           →
                           
                              R
                              r
                           
                        
                      be a mapping from data measurements to residual errors, where n is the dimension of the measurement space and r is the number of residual errors per datum. Thus, for a set of N data measurements 
                        
                           
                              x
                              i
                           
                           ∈
                           
                              R
                              n
                           
                           ,
                           i
                           =
                           1
                           …
                           N
                        
                     , the function f(x
                     
                        i
                     |θ)=
                     d
                     
                        i
                      maps the ith datum to d
                     
                        i
                     , a vector of residual errors associated with the datum. We call ‖d
                     
                        i
                     ‖2 the squared fitting error of x
                     
                        i
                      with respect to the model θ, which is equal to zero only when x
                     
                        i
                      is perfectly consistent with θ.

When fitting an m-dimensional surface in an n-dimensional space there are k
                     =
                     n – m degrees of freedom in defining a surface normal, called the codimension 
                     [35]. If measurement noise is independent and normally distributed with standard deviation σ, and given a reasonable model estimate 
                        
                           
                              θ
                              ^
                           
                           ≈
                           θ
                        
                     , then residual errors in the codimension will be distributed approximately the same as measurement errors (normally). Thus, squared fitting errors will be distributed according to a scaled χ
                     2-distribution with k degrees of freedom [36].

Once the scale σ is known, a threshold may be calculated as τ
                     2
                     =
                     σ
                     2
                     F
                     
                        k
                     
                     −1(α) ([37], p.119), where α is the desired percentile (e.g., α
                     =0.95) and F
                     
                        k
                     
                     −1 is the standard inverse cumulative χ
                     
                        k
                     
                     2-distribution function. Given an estimated model 
                        
                           θ
                           ^
                        
                      and threshold τ, a datum may then be classified as an inlier when the squared fitting error is below the threshold; that is, ||d
                     
                        i
                     ||2
                     <
                     τ
                     2.

Given an existing model estimate 
                        
                           θ
                           ^
                        
                     , a robust scale estimator attempts to estimate the true scale of measurement noise from the distribution of residuals or fitting errors relative to the model estimate. The maximum likelihood (ML) estimate of σ is simply given by the sample standard deviation from the combined set of residuals. In the special case where the number of χ
                     2 degrees of freedom is equal to the number of residuals per datum (r
                     =
                     k), this can be written in terms of the fitting errors as
                        
                           (1)
                           
                              
                                 
                                    
                                       
                                          σ
                                          ^
                                       
                                    
                                    ML
                                 
                                 =
                                 
                                    
                                       
                                          1
                                          Nk
                                       
                                       
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             Nk
                                          
                                          
                                             
                                                r
                                                j
                                                2
                                             
                                          
                                       
                                    
                                 
                                 =
                                 
                                    
                                       
                                          1
                                          Nk
                                       
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             N
                                          
                                          
                                             
                                                
                                                   
                                                      d
                                                      i
                                                   
                                                
                                                2
                                             
                                          
                                       
                                       ,
                                    
                                 
                              
                           
                        
                     where rj
                      is the jth residual out of the combined set. However, it is well known that the ML estimate is not robust to outliers, and we expect that the data does contain outliers. A robust alternative comes from the median squared residual [21,3]. Assuming that residual errors are distributed as R
                     ~(0, σ
                     2), then from the definition of the median, we have
                        
                           (2)
                           
                              
                                 0.5
                                 =
                                 P
                                 
                                    
                                       
                                          R
                                          2
                                       
                                       <
                                       med
                                       
                                          R
                                          2
                                       
                                    
                                 
                                 =
                                 P
                                 
                                    
                                       
                                          R
                                       
                                       <
                                       med
                                       
                                          R
                                       
                                    
                                 
                              
                           
                        
                     
                     
                        
                           (3)
                           
                              
                                 =
                                 P
                                 
                                    
                                       
                                          Z
                                       
                                       <
                                       
                                          
                                             med
                                             
                                                R
                                             
                                          
                                       
                                       /
                                       σ
                                    
                                 
                                 ,
                              
                           
                        
                     where Z
                     =
                     R/σ is a standard normal random variable (RV). Because the distribution of Z is symmetric, the above is equivalent to
                        
                           (4)
                           
                              
                                 0.75
                                 =
                                 P
                                 
                                    
                                       Z
                                       <
                                       
                                          
                                             med
                                             
                                                R
                                             
                                          
                                       
                                       /
                                       σ
                                    
                                 
                                 ,
                              
                           
                        
                     which implies
                        
                           (5)
                           
                              
                                 
                                    Φ
                                    
                                       −
                                       1
                                    
                                 
                                 
                                    0.75
                                 
                                 =
                                 
                                    
                                       med
                                       
                                          R
                                       
                                    
                                 
                                 /
                                 σ
                              
                           
                        
                     
                     
                        
                           (6)
                           
                              
                                 σ
                                 =
                                 
                                    
                                       med
                                       
                                          R
                                       
                                    
                                 
                                 /
                                 
                                    Φ
                                    
                                       −
                                       1
                                    
                                 
                                 
                                    0.75
                                 
                                 ,
                              
                           
                        
                     where Φ is the cumulative distribution function of the standard normal distribution. Therefore, a robust and asymptotically consistent estimator for σ from the residuals is given by
                        
                           (7)
                           
                              
                                 
                                    
                                       
                                          σ
                                          ^
                                       
                                    
                                    MED
                                 
                                 =
                                 
                                    
                                       me
                                       
                                          d
                                          j
                                       
                                       
                                          
                                             r
                                             j
                                          
                                       
                                    
                                    
                                       
                                          Φ
                                          
                                             −
                                             1
                                          
                                       
                                       
                                          0.75
                                       
                                    
                                 
                                 =
                                 
                                    
                                       
                                          me
                                          
                                             d
                                             j
                                          
                                          
                                             r
                                             j
                                             2
                                          
                                       
                                    
                                    
                                       
                                          Φ
                                          
                                             −
                                             1
                                          
                                       
                                       
                                          0.75
                                       
                                    
                                 
                                 .
                              
                           
                        
                     
                  

In practice, when k
                     >1, one does not always have a residual vector, and it is tempting to use (7) to estimate σ from the fitting errors instead; however, this would be incorrect because the squared fitting errors have a non-central χ
                     2 distribution. The median absolute deviation (MAD) is often used to compensate for this non-centrality, but this is not an asymptotically consistent estimator.

The correct estimator can be derived in the same fashion as (7) (see also ([38], Appendix C, p.102)). Specifically, if D~ χ
                     2 (σ, k), then from the definition of the median we have
                        
                           (8)
                           
                              
                                 0.5
                                 =
                                 P
                                 
                                    
                                       D
                                       <
                                       medD
                                    
                                 
                                 ,
                              
                           
                        
                     which implies
                        
                           (9)
                           
                              
                                 0.5
                                 =
                                 
                                    F
                                    k
                                 
                                 
                                    
                                       medD
                                       |
                                       σ
                                    
                                 
                              
                           
                        
                     
                     
                        
                           (10)
                           
                              
                                 medD
                                 =
                                 
                                    σ
                                    2
                                 
                                 
                                    F
                                    k
                                    
                                       −
                                       1
                                    
                                 
                                 
                                    0.5
                                 
                              
                           
                        
                     
                     
                        
                           (11)
                           
                              
                                 σ
                                 =
                                 
                                    
                                       
                                          medD
                                       
                                       /
                                       
                                          F
                                          k
                                          
                                             −
                                             1
                                          
                                       
                                       
                                          0.5
                                       
                                    
                                 
                                 .
                              
                           
                        
                     
                  

Thus, a robust and asymptotically consistent estimator for σ, analogous to (7) but computed from the fitting errors and hence valid for all cases, is given by
                        
                           (12)
                           
                              
                                 
                                    
                                       
                                          
                                             σ
                                             ^
                                          
                                          ′
                                       
                                    
                                    MED
                                 
                                 =
                                 
                                    
                                       
                                          me
                                          
                                             d
                                             i
                                          
                                          
                                             
                                                
                                                   d
                                                   i
                                                
                                             
                                             2
                                          
                                       
                                       
                                          
                                             F
                                             k
                                             
                                                −
                                                1
                                             
                                          
                                          
                                             0.5
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     
                  

It should be noted that these median-based estimators have 50% breakdown points, which is ineffective for most previous scale estimation algorithms where the breakdown point of the scale estimator induces an equivalent breakdown point in the overall estimation routine [2,27,22,29].

This has led to the development of scale estimators with increased tolerance to outliers, such as the Compressed Histogram [39], the k-th order statistic [40,41], the WMAD [22], TSSE [2], IKOSE [32] and others.

However, the breakdown point of the scale estimator is not a significant concern for SIMFIT, because the scale estimate is only used to obtain an over-estimate anyway. Thus, we will prefer 
                        
                           
                              
                                 σ
                                 ^
                              
                              ′
                           
                           MED
                        
                      for its simplicity and reliability.

The sensitivity to threshold choice τ in RANSAC is revealed by the fact that, as τ
                     →∞, all constraints on the estimated model vanish, giving a purely random result. In contrast, we notice that MSAC [10], a modification of RANSAC that minimizes a robust M-estimator [9], becomes equivalent to the method of least absolute deviations (LAD) [42] as τ
                     →∞.

LAD is already a fairly robust method, and by using any τ
                     <∞, one may obtain far more robust results without a specific breakdown point. Thus, MSAC is quite robust to over-estimated scales, and this is the core concept we exploit in the algorithm outline below:
                        
                           1.
                           Starting from any initial overestimate of σ, the corresponding optimal threshold τ may be derived, and used to estimate a model with associated inliers using MSAC.

From the residuals of the inlier set, a robust estimate of σ may be computed using 
                                 
                                    
                                       
                                          σ
                                          ^
                                       
                                       ′
                                    
                                    MED
                                 
                              . Because it was estimated from a more restricted set of inliers, the newly estimated σ will usually be less than the previous.

If there is no significant change in the estimate of σ, then all the outliers must have been removed, and hence the model, inliers, and scale should all be accurate. Otherwise, one may repeat MSAC from step 1 using the newly reduced estimate of σ.

To clarify the algorithm details, we give pseudo-code in Algorithm 1, and proceed here with some analysis. First, the initial estimate of σ is used to calculate a corresponding over-estimate of τ (line 1), and all the data points are added to the potential inlier set (line 5).


                     
                        
                           
                        
                     
                  

On each iteration, MSAC is used to compute a robust estimate of the model θ (line 9) from within the potential inlier set (our modified version of MSAC that works with a shrinking inlier set is given in Algorithm 2). The set of inliers is reduced (line 10) and used to compute a new robust estimate of scale (line 16) and associated threshold (line 17).

In general, each new estimate of scale will be lower than the previous until convergence. However, this is not guaranteed, and in some very rare cases a cycle might be entered. Therefore, we perform explicit cycle prevention by computing a quick hash (e.g., the MurmurHash3 [49]) of the inlier indices and current scale estimate (rounded to nearest integer), and break out of the loop if a repeated state would be entered (line 24).

Normally, convergence is detected when the reduction in σ becomes insignificant, as detected by a difference less than some threshold ϵ (line 24). However, we note that one may ignore this parameter by setting ϵ
                     =0 here, which merely delays convergence until no further improvement is possible.

Additionally, one may add some optional convergence criteria to improve best and worst case performance: (a) If the found solution uses nearly all of the potential inliers (i.e., if the number of inliers reduced from the current iteration is an insignificant fraction); (b) If the found solution uses such a small number of inliers that further reduction of the inlier set would be pointless (i.e., the size of the current inlier set is less than 2 times the minimal number of points necessary to define a model).


                     
                        
                           
                        
                     
                  

In most cases, we do not expect to need more than 1–3 iterations of MSAC to converge to the correct scale. Moreover, because MSAC is run within a reduced inlier set (similar to LO-RANSAC [11]), subsequent runs of MSAC become computationally trivial, as the inlier ratio will be near to 1, requiring only a few random samples to meet the statistical convergence criterion of [8].

After convergence to the proper scale, we transition into a final (optional) model refinement stage (line 25), which we refer to as the model-shift procedure, because it is actually a generalization of the well-known mean-shift procedure [43], where the threshold is effectively the mean-shift bandwidth with a uniform kernel, and we generalize the sample mean from mean-shift with the over-determined estimate of the model. The only actual difference from mean-shift is that we only allow inliers to be added (and not removed) from the potential inlier set, which guarantees convergence by preventing cycles.

SIMFIT is usually quite robust to the choice of σMAX
                     . For example, if one chooses σ
                     
                        MAX
                     
                     =∞, then the first iteration would reduce σMAX
                      down to 
                        
                           
                              
                                 σ
                                 ^
                              
                              ′
                           
                           MED
                        
                      from an all-data fit. This is often sufficient to converge to the proper scale, although when the outlier points come from some distribution that also has finite variance, then the all-data fit may yield a stable model that is a false attractor. Thus, one should choose 
                        
                           
                              σ
                              MAX
                           
                           <
                           
                              
                                 
                                    σ
                                    ^
                                 
                                 ′
                              
                              MED
                           
                        
                      if possible.

In this section we identify previous algorithms that we compare to SIMFIT in our experimental results for their ability to do robust estimation with simultaneous scale detection. We also clarify the choice of free parameters and algorithm details when necessary. In general, we set τ so as to capture α
                     =0.99 percent of inliers, and we let the number of samples for RANSAC/MSAC be determined adaptively with pFail
                     =1×10−3, and a maximum of 10000.

Although RANSAC does not include scale estimation, we use RANSAC with an optimally derived threshold based on the true σ for the purposes of performance comparison.

Because we expect image noise on the order of a pixel or so, we use a conservative over-estimate of σMAX
                        
                        =15. We use the same value in synthetic tests. We use three conditions for early-termination of the main loop: (a) change in σ less than ϵ
                        =0.5, (b) reduction in the inlier set is less than 1% from the previous iteration, (c) size of the inlier set is reduced to less than 2 times the minimal number of points to define a model.

The first comparative algorithm is the procedure of [21], where LMS is used to obtain an initial estimate of the model, followed by robust scale estimation using 
                           
                              
                                 σ
                                 ^
                              
                              MED
                           
                        , and finally RANSAC with a threshold based on 
                           
                              
                                 σ
                                 ^
                              
                              MED
                           
                        . We have upgraded this method by replacing RANSAC with MSAC from [10] because it is strictly superior to RANSAC in this context. LMS is implemented (as per usual) by random sampling, and the number of iterations is determined based upon the assumption that the data may contain up to 50% outliers (because this is the breakdown point of LMS).

The second algorithm that merits comparison is ASSC [2], which modifies the RANSAC criterion to maximize the number of inliers divided by a robust scale estimate, as well as the newly developed ASKC [27]. ASSC uses adaptive sampling, but ASKC uses a fixed number of samples, and we chose M
                        =1000 samples.

Both ASSC and ASKC estimate scale using TSSE, which uses a KDE of the residual error, where the bandwidth for the KDE is chosen automatically using a rule of thumb multiplied based on an initial scale estimate, and then multiplied by an unspecified tuning parameter c
                        
                           h
                        
                        ∈(0,1) to compensate for oversmoothing.

For the initial scale estimate, we use the k-th order static with k
                        =0.2, as described in [27]. We use the default value of ch
                        
                        =1 because we are testing generic performance and do not permit tuning the algorithms for particular noise distributions.

Several comparable algorithms were proposed in [22]: RANSAC-MAD was essentially the same as ASSC but used the MAD to estimate scale instead of TSSC, RANSAC-EIS modified the objective by calculating a weight vector using Ensemble Inlier Sets (EIS) and then using WMAD instead of MAD, and finally RANSAC-EIS-Metropolis incorporated weighted sampling. The latter was found to be the superior version, so we only compare against this one. We note that their algorithm calls for some number of unspecified fixed iterations, so we use the same probabilistic argument from RANSAC and LMS to calculate the number of required iterations assuming there are 50% outliers.

STARSAC requires a minimum and maximum scale, we used σ
                        
                           MIN
                        
                        =1×10−5 and σMAX
                        
                        =1000, and tested at 20 scales logarithmically spaced within this range, performing the recommended 30 runs of RANSAC at each scale. Due to the large number of RANSAC iterations, and because the required number of RANSAC samples grows exponentially for under-estimated scales, we found it computationally necessary to impose a maximum of 100 samples for each run of RANSAC.

When dealing with homogeneous models, the set of model parameters was taken as the set of real parameters after performing homogeneous division. Two models were deemed ‘consistent’ if the maximum relative difference between model parameters was less than 0.2.

The version of RECON that we use has a few improvements over the algorithm described in [29]. First, we clarify that the termination condition of finding ‘K mutually α-consistent models’ requires clustering models into mutually α-consistent groups. These clusters are efficiently maintained using the union-find structure.

Because the initial α-consistency test will always pass for inconsistent models at some large scale encompassing all (or most of) data, RECON explicitly tests for 
                           
                              
                                 
                                    σ
                                    ^
                                 
                                 MED
                              
                              <
                              
                                 σ
                                 MAX
                              
                           
                        , or alternatively tests the KS-test for distribution equality. We opted to use the scale-based test in our experiments because it is faster, more comparable to SIMFIT, and because the KS-test will often accept the null-hypothesis of distribution equality for an all-data fit when the outlier distribution has finite variance, inducing a breakdown point as a set of α-consistent models are more likely to be found at maximum scale for low outlier ratios.

However, when the outlier ratio is less than 50%, the robustness of the 
                           
                              
                                 σ
                                 ^
                              
                              MED
                           
                         estimator may cause a low value of σ to be estimated even when n is large enough to be an all-data fit. This issue is corrected by instead testing against the sigma implied by the residual error of the nth residual. That is, 
                           
                              
                                 
                                    
                                       r
                                       n
                                       2
                                    
                                    /
                                    
                                       F
                                       k
                                       
                                          −
                                          1
                                       
                                    
                                    
                                       0.5
                                    
                                 
                              
                              <
                              
                                 σ
                                 MAX
                              
                           
                        .

Another issue is the assumption that residuals will always occur in random sort order. As discussed in the introduction, this assumption is invalid for over-determined models, and can lead to α-consistency check passing at an incorrect small n-value. This can be problematic in the fourth step of the original algorithm, which takes the minimum n value across all pairs between the M over-determined models. This issue is corrected by recomputing the scale from the final model using 
                           
                              
                                 σ
                                 ^
                              
                              MED
                           
                        , and then reclassifying inliers according to a threshold derived from this final scale estimate.

We use σMAX
                        
                        =15 in all experiments. Additionally, we use K
                        =3 and α
                        =0.95 as recommended by the authors. The authors recommend M
                        =30, but we found this to be computationally limiting, and reduced it to M
                        =5. To generate the final non-minimal models, we always use a sample size equal to twice the number of minimal points. Due to problems with the α-consistency test for small n-values, we only consider n
                        >10.

@&#EXPERIMENTAL RESULTS@&#

We begin with some experiments to validate that it is reasonable to assume squared fitting errors will be χ
                     
                        k
                     
                     2-distributed (Section 6.1). Then we compare the accuracy, reliability and performance of SIMFIT against previous methods using a synthetic line fitting problem as well as homography estimation (Section 6.2), and finally on real data for fundamental matrix estimation (Section 6.3).

A first assumption used by all algorithms is that the inlier noise distribution is normal, which implies squared fitting errors of an ideal model will be χ
                        
                           k
                        
                        2-distributed (Section 6.1). However, to our knowledge, this assumption has never been validated in practice. Moreover, it is unclear how sensitive an algorithm would be to departures in normality of the measurement noise. We explore both of these issues here.

First, we consider robust estimation of a homography from a set of sparse correspondences for image registration. Because a homography perfectly describes the mapping of a planar surface between any two arbitrary viewpoints, it is a good model for registering aerial images where the ground surface is well-approximated by a plane. Because each corresponding point provides 2 constraints, we expect that squared fitting errors will be χ
                        2-distributed with k
                        =2 degrees of freedom.

We obtained two aerial photographs (Fig. 1
                        , left and middle) taken from slightly different positions at different times of the day (as evidenced by the boats that have moved) and then automatically computed a set of typical correspondences by matching Harris feature points. These correspondences were filtered by SIMFIT to determine a set of inliers (indicated by the green points and lines in Fig. 1), along with the scale of inlier noise and an estimate of the homography, which was then used to register the first image into the frame of the second image for visual verification (Fig. 1, right).

There were a total of 244 potential correspondences found by the feature matcher. We used an initial over-estimate of σ
                        =15 pixels for SIMFIT. The first run of MSAC found 231 inliers and reduced the scale estimate to σ
                        =1.21 pixels. The second run of MSAC found 189 inliers and further reduced the scale estimate to 1.09 pixels. This resulted in convergence because our threshold is set at ϵ=0.5, causing SIMFIT to transition into the nonlinear model-shifting mode where it increased the potential inlier set to 199 correspondences and the final threshold was τ
                        2
                        =3.31.

We computed the KDE of the distribution of residual errors using the rule-of-thumb bandwidth ([46], p.48) and compared it to the PDF of the normal distribution using the found scale (Fig. 2
                        , left), observing good visual agreement (note that the KDE is a bit rough given that there are only 199 samples). We then computed the ECDF of the fitting errors and compared them to a χ
                        2
                        2 distribution at the appropriate scale factor, and again observed good visual agreement (Fig. 2, right). Thus, we conclude that the assumption of normality is reasonable.

Of course, we do not expect that measurement errors will always be normally distributed for all types of problems. Therefore, we are curious to investigate how different noise distributions will affect the distribution of fitting errors. To this end, we generated some synthetic data sets where the measurement noise was uniformly distributed as well as distributed according to an infinite variance stable distribution [47].

The stable distribution arises as a generalization of the central limit theorem: whereas the central limit theorem states that the sum of any independent and identically distributed (IID) RV with finite variance converges to a normal distribution, the generalized central limit theorem [47] states that the sum of any IID RV converges to a stable distribution. Thus, the normal distribution, Cauchy distribution, Levy (or Pearson) distribution, Landau distribution and Delta distribution can all be found as special cases of the stable distribution. Because of its theoretical generality, the stable distribution is often suggested as a model for non-normal data.

When measurement noise is uniformly distributed, we see that the distribution of squared fitting errors is still quite well-approximated by the χ
                        2 distribution (Fig. 3
                        , left). In fact, even when measurement errors are distributed according to a stable distribution with (α
                        =1.5, β
                        =0, γ
                        =5, δ
                        =0), which has infinite variance, we still observed fairly good visual agreement with the χ
                        2 distribution (Fig. 3, right). This can be explained by the fact that any tail effects of the stable distribution are automatically chopped off and considered as outliers by the algorithm.

The assumption that fitting errors will be χ
                        2-distributed is only relevant for picking an inlier threshold to capture the desired percentage of inliers, but this is an insensitive parameter that is just set with an arbitrary ballpark estimate anyway. Thus, although the estimated scale may be off, minor divergences from the χ
                        2-distribution are not otherwise relevant, and hence one does not generally need to worry about departures from normality in the residuals due to the correspondence detector or approximated error metrics.

Our experiments are designed to test all algorithms starting from outlier-free data up to the point of failure under extreme contamination from outliers and noise. The first test is 2D line fitting. For each trial, outliers were uniformly distributed in a 500×500 region, and the ‘true line’ was chosen by joining two random points from the outlier distribution. A random noise scale was chosen σ
                        ∈(1,10), and inliers were generated by choosing a random t
                        ∈(0,1) to interpolate between the two endpoints of the line (where it intersects the bounding box of the 500×500 region), and then adding normally distributed noise with standard deviation σ to each coordinate.

Because a line is one-dimensional, the codimension of fitting a line to n-dimensional points is k
                        =
                        n−1. Thus, the squared fitting error is distributed according to a χ
                        2-distributed with k
                        =1 degrees of freedom.

Our second test was estimation of a homography from 2D correspondences. The true homography was chosen as a random rotation matrix with θ
                        ∈(0, 2π), although we used a search space of all 3×3 homographies. Inlier correspondences were generated by choosing a random point in the 500×500 region for the first point, which was transformed by the true homography and then normally distributed noise was added to each coordinate of the second point.

For both tests, we vary the fraction of outliers from R
                        =0…0.9; for each R-level, we generate 100 random data sets consisting of 1000 points each, and plot the median of several performance statistics for each algorithm:
                           
                              Found inliers
                              This is the number of inliers found by the algorithm; ideally, it should be roughly 1000(1−R) because there are 1000 measurements.

This is the estimated scale divided by the true scale, so the ideal value is 1. Note that each data set has a random scale σ
                                 ∈(1,10).

This is an objective measure of the model error, calculated as the sum of squared fitting errors from all of the true inliers divided by the sum of squared errors from the true model. Thus, the ideal value is 1.

This records the total number of random models that were evaluated by the algorithm.

The total runtime of the algorithm in seconds (tested on a Lenovo X220 laptop with Core i7-2620M processor).

Some visual examples from the line fitting problem are shown in Fig. 4
                        , where the capacity of SIMFIT to routinely extract an accurate line from heavily contaminated data without any prior knowledge of the scale of inlier noise can be observed (e.g., upper left).

We also show a proof of concept of how SIMFIT can be used to extract multiple models with different noise scales from the same data set (Fig. 4, lower left). This is done by repeatedly fitting a model with SIMFIT to extract the most dominant structure, removing all the found inliers, and then running SIMFIT again to extract the next most dominant structure in the remaining data, until the inlier count falls below some threshold. It remains to be seen how this fit-and-remove method compares with other dedicated multiple-model fitting algorithms that perform scale selection [30–32].

The breakdown point can be identified as the highest outlier ratio in which the algorithm succeeded in finding a reasonable fit, rather than breaking down to an all-data fit. For line estimation (Fig. 5
                           , top left), LMS+MSAC has a breakdown point at 50% (as predicted), ASSC had a breakdown point at about 70%, RANSAC-EIS M had a breakdown point at about 60%. The other algorithms did not break down, but at R
                           =0.9, RECON and ASKC began to drastically over-estimate scale (Fig. 5, top middle) and STARSAC had very high model error (Fig. 5, top right). Thus, we may conclude that these algorithms are on the verge of breaking down.

For homography estimation (Fig. 5, top left), ASCC broke down earlier at around 40%, and ASKC also broke down at 70%. RECON also appears to breakdown at 70%, but this is an artificial breakdown point due to exceeding the 1h time limit that we set (after which we resort to an all-data fit). In theory, RECON should not have a breakdown point. At R
                           =0.9, SIMFIT was on the verge of breaking down, but this was usually corrected by the model shift procedure.

Overall, we conclude that SIMFIT is the most robust method to breakdown, being the only algorithm surveyed that did not break down for either problem (other than RANSAC with optimal threshold choice, which does not perform scale selection, and perhaps RECON, given infinite time).

The estimated scale for line fitting is shown in (Fig. 5, top middle) and (Fig. 6
                           , top middle) for homography estimation. In the presence of outliers, we see that LMS+MSAC tends to over-estimate scale, with a gradually increasing over-estimate up until it reaches the breakdown point. This mirrors the performance of the underlying 
                              
                                 
                                    σ
                                    ^
                                 
                                 MED
                              
                            estimator. RECON shows a similar trend because it uses the same estimator, but because the breakdown point is so much higher, this results in much more accurate estimates, generally between 1 and 2 times the correct scale.

RANSAC-EIS M was specifically designed to compensate for the over-estimated scale of LMS+MSAC [22], but we see that it instead under-estimates scale before reaching its breakdown point, after which it also over-estimates the scale.

On the other hand, STARSAC has a large degree of variance in the scale estimate. This is because the search granularity does not permit precise scale estimates, and because the check for model ‘consistency’ is based on a sub-optimal threshold choice. We observed greater variability in performance on the homography problem because the variance of model parameters becomes increasingly less representative of ‘structural’ model variation for higher dimensional models.

ASSC and ASKC tend to under-estimate scale (by about 50%) when they are operating well below the breakdown point for line estimation, and then transition into over-estimating scale as they near the breakdown point. ASKC is more sensitive to this under-estimated scale, detecting only about 50% of the inliers for low outlier ratios, despite finding a good model (Fig. 5, top right). This is because when the bandwidth is under-estimated, it changes the normalization factor of the KDE such that an under-estimated scale may have an equally high (or higher) kernel density at the origin of residual space. This problem did not occur for homography estimation (Fig. 6, top left), but only because the bandwidth was over-estimated, resulting in over-estimated scale estimates (Fig. 6, top middle).

It should be noted that these problems with ASSC/ASKC can be avoided by parameter tuning. We used the recommended values from [27], but different results are achieved by changing the k-value in the kth order statistic, or the ch
                            parameter, or using a different robust scale estimator. For example, using 
                              
                                 
                                    σ
                                    ^
                                 
                                 MED
                              
                            as suggested in [2], there was no difficulty in capturing all the inliers for low outlier ratios, but a breakdown point was introduced at around 30%. However, as evidenced by the tendency to over-estimate in one case (Fig. 6, top middle) and under-estimate in another (Fig. 5, top middle), there is no clear way to tune these parameters that works well for all cases.

Although SIMFIT began to slightly over-estimate scale for the line fitting problem at R
                           =0.9, we see that overall, it was the only algorithm that consistently and accurately found the true scale. The final model shift procedure performed negligible improvement for line fitting because the initial estimate was quite good; however, we see that for the higher dimensional problem of homography estimation where scale is slightly over-estimated initially, the model shift procedure consistently corrects this estimate to find the true value.

For the line estimation problem, the reconstructed model error (Fig. 5, top right) is fairly comparable for all methods up until they reach their breakdown points, being consistently in the range of 1–2 times the minimal error value. Nonetheless, SIMFIT performed the best, being the only algorithm that consistently found the minimum error at all outlier ratios (after model shifting).

For the higher dimensional homography estimation problem, the variation in model accuracy between algorithms is more pronounced (Fig. 6, top right). First, we note that even RANSAC with optimal threshold tends to find a model with about twice the minimal error. ASSC often has more than 4–5 times the minimal error, and LMS+MSAC often had 3–4 times the minimal error. ASKC, STARSAC, RECON, and RANSAC-EIS M were all capable of finding less than 2 times the minimal error, but this quickly increased as their breakdown points are neared.

Interestingly, the core SIMFIT estimation actually became more accurate as the outlier ratio increased, up until R
                           =0.9 where there was a significant increase in model error. Still, after model shifting, SIMFIT consistently found the minimum error at all outlier ratios.

RANSAC-EIS M, STARSAC, and ASKC do not attempt adaptive sampling, which means that runtime performance is independent of outlier ratio. Although MSAC does adaptive scaling, the overall performance of LMS+MSAC is dominated by the LMS phase which requires a fixed number of iterations, and because LMS breaks down for R
                           >0, MSAC is not able to scale beyond this point. As a result, the number of unique fits in LMS+MSAC is also effectively constant.

Lack of adaptive sampling is a notable disadvantage because choosing the optimal number of iterations in advance requires a priori knowledge the true outlier ratio. If too few iterations are chosen, a breakdown point may be induced. If too many iterations are chosen, performance for low outlier ratios will suffer.

It is interesting to note that, despite requiring nearly 100 times as many samples, STARSAC had almost the same performance as ASKC (about 4s) (Fig. 5, bottom right), due to the costly objective function of ASKC that requires using the KDE.

With ASSC, we see that adaptive sampling works up until R
                           =0.4 (the breakdown point), and then scales back down (Fig. 6, bottom middle), despite that the most samples are needed for high outlier ratios. This is because as soon as a model is found with an over-estimated scale that can over-classify an inlier set, it causes a reduction in the required number of iterations — despite that this over-estimated model may be incorrect. In other words, it introduces an artificial breakdown point, and this is likely why adaptive sampling was later abandoned in ASKC.

SIMFIT and RECON are the only methods that consistently succeed in adaptively scaling up the number of iterations. Using K
                           =3, RECON often requires fewer model hypothesis than SIMFIT or even RANSAC (at higher outlier ratios) — however, the large number of α-consistency checks leads to high computational complexity, and runtime that is often several orders of magnitude larger. For example, on the homography estimation problem, the performance of SIMFIT ranges from 0.01 to 1s, whereas RECON ranges from 1s to more than 1h (and sometimes needed to be aborted due to time constraints) (Fig. 6, bottom right).

In contrast, we see that the performance of the core SIMFIT routine (without model shift) was very similar to RANSAC, being the only algorithm that remains in the same order of magnitude for all outlier ratios, with median performance about 2 times slower than RANSAC. For example, at R
                           =0.5, RANSAC took 7.42×10−3
                           s, whereas SIMFIT took 1.37×10−2
                           s. The final model shift procedure added roughly constant time, never more than 0.4s.

Although we have so far demonstrated SIMFIT's capabilities on synthetic problem with higher outlier ratios and noise levels than typical vision problems, it should be stressed that SIMFIT is also competitive when applied to the lower outlier ratios and noise levels of typical correspondence data. We demonstrate this on another problem from computer vision: estimation of the fundamental matrix from image correspondences.

The fundamental matrix is a more general constraint on images than a homography because correspondences between any two images must obey the epipolar constraint, regardless of scene geometry. Given a set of corresponding image points 
                           
                              
                                 
                                    
                                       x
                                       ˜
                                    
                                 
                                 i
                              
                              ↔
                              
                                 
                                    
                                       
                                          x
                                          ˜
                                       
                                       ′
                                    
                                 
                                 i
                              
                           
                        , both homogeneous points in ℙ2, the epipolar constraint [37] dictates that the fundamental matrix F should satisfy
                           
                              (13)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   x
                                                   ˜
                                                
                                                ′
                                             
                                             i
                                          
                                       
                                       T
                                    
                                    F
                                    
                                       
                                          x
                                          ˜
                                       
                                       i
                                    
                                    =
                                    0
                                    ,
                                    
                                    ∀
                                    i
                                    .
                                 
                              
                           
                        
                     

Geometrically, this constraint represents the fact that each point 
                           
                              
                                 x
                                 ˜
                              
                              i
                           
                         in the first image defines an epipolar line 
                           
                              
                                 l
                                 i
                              
                              =
                              F
                              
                                 
                                    x
                                    ˜
                                 
                                 i
                              
                           
                         in the second image, and the second point 
                           
                              
                                 
                                    x
                                    ˜
                                 
                                 ′
                              
                              i
                           
                         should lie on this epipolar line, so 
                           
                              
                                 
                                    
                                       x
                                       ˜
                                    
                                    ′
                                 
                                 i
                              
                              ⋅
                              
                                 l
                                 i
                              
                              =
                              0
                           
                        . We performed minimal estimation of the fundamental matrix from 6 points as described in [37], and refined from over-determined point sets using bundle adjustment. Because the epipolar line constraint is a 1-dimensional constraint, we use k
                        =1 degrees of freedom for the χ
                        2-distribution.

It is tempting to measure error as the squared distance from the right correspondence point 
                           
                              
                                 
                                    x
                                    ˜
                                 
                                 ′
                              
                              i
                           
                         to the epipolar line l
                        
                           i
                        , as was done in [21]. However, this introduces a bias by assuming that all of the noise is distributed on the measurements of 
                           
                              
                                 
                                    x
                                    ˜
                                 
                                 ′
                              
                              i
                           
                         rather than 
                           
                              
                                 x
                                 ˜
                              
                              i
                           
                        . Therefore, we prefer the ML method, which is to simultaneously estimate the fundamental matrix and 3D structure points that minimize reprojection error.

We automatically generated correspondences by finding Harris corner points [48] and matched them by maximizing the normalized cross correlation with subpixel refinement. In order to assess the SSE from the true inliers, as well as false positive (FP) and false negative (FN) counts, we manually classified the true inliers with the assistance of a helper script (see Fig. 7
                        ).

In four out of the six image pairs (Table 1
                        ), SIMFIT had the lowest error. ASKC was lower for ANL and Kitchen pairs, but SIMFIT also did well here, and in a fraction of the time. For example, in the Kitchen pair, ASKC had squared error of 546.07 after 7.39s, whereas SIMFIT had squared error of 677.18 after just 0.09s.

In general, one expects to find more inliers than actually exist when fitting the fundamental matrix because it is impossible to detect outlier correspondences that randomly happen to lie close to the epipolar line. Thus, our best indicator of algorithm performance is the objective SSE measure, which corresponds to negative log-likelihood.

SIMFIT had the fastest performance in 5 out of the 6 trials, taking significantly less than 1s for all problems except Table, which had the highest outlier ratio, where it took 1.58s. ASSC was often a close second in terms of performance, although it took 10s on Table. The runtime performance of RECON was very reasonable due to the high inlier ratios, usually taking under 2s, except for Livingroom where it took 5.27s. STARSAC was by far the slowest algorithm, because it always performs a computationally intensive search through scale space to estimate model variances.

We do not have a ground truth reference for scale, but most algorithms estimate σ in the range of 0.4 – 0.7 pixels for each image pair, which is almost certainly under-estimated. This is to be expected, because each correspondence has three degrees of freedom and four constraints, and thus there will always be a significant degree of over-fitting which allows the triangulated points to have lower error than the true points would. Thus, the distribution of fitting errors will be more peaked than predicted by the χ
                        2-distribution, leading to partially under-estimated scale by all algorithms. Nonetheless, the model estimates are still good.

@&#CONCLUSIONS@&#

RANSAC has proven to be an effective technique for overcoming large outlier ratios when a good threshold can be chosen, but it is sensitive to this choice. Several methods for augmenting RANSAC with automatic scale estimation have been previously proposed, but these methods tend to break down, or are too slow for many practical applications.

To overcome these limitations we have proposed the novel SIMFIT algorithm, which efficiently and reliably performs simultaneous scale estimation and model estimation without a breakdown point. SIMFIT is simple to implement, requires no new parameters (other than optional parameters for early termination), is reliable, and allows for adaptive sampling, keeping the runtime on-par with RANSAC for low as well as high outlier ratios.

Because SIMFIT is designed as a drop-in replacement for RANSAC, it can also be incorporated into other algorithms that use RANSAC as a subroutine. For example, the QDEGSAC [13] algorithm was designed to cope with quasi-degenerate data sets, and works by first running RANSAC to find a model that explains the data, and then estimating the codimension of the found model from the found inliers, and finally searching the remaining data for additional inliers that may provide the constraints necessary to make the model non-degenerate, if necessary. Thus, the initial RANSAC step can simply be replaced by SIMFIT to remove the need for a priori knowledge of scale.

Although we have demonstrated SIMFIT on some specific vision-related problems, in addition to basic line fitting, we would like to stress that it is not specifically designed just for vision-related tasks. We feel that the simplicity and generality of the method make it applicable to robust estimation in many other fields of science.

Finally, we note that there is room for future improvement in deriving a more advanced version of the χ
                     2-distribution that accounts for increased peakedness due to over-fitting. Although the difference would be less than negligible for any normal model fitting problem (where errors are measured relative to a quantity that becomes increasingly over-determined from additional measurements), this would permit more accurate scale estimation for the special case of fundamental matrix estimation.

@&#REFERENCES@&#

