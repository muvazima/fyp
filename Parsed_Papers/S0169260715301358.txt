@&#MAIN-TITLE@&#“iSS-Hyb-mRMR”: Identification of splicing sites using hybrid space of pseudo trinucleotide and pseudo tetranucleotide composition

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           “iSS-Hyb-mRMR” model is proposed for identification of splicing sites.


                        
                        
                           
                           Trinucleotide and tetranucleotide composition are used as feature extraction schemes.


                        
                        
                           
                           Hybrid space is formed by using TNC and TetraNC spaces.


                        
                        
                           
                           Various classification algorithms are analyzed.


                        
                        
                           
                           mRMR is utilized to reduce feature space.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Splicing sites

PseTNC

PseTetraNC

KNN

mRMR

@&#ABSTRACT@&#


               
               
                  Background and objectives
                  Gene splicing is a vital source of protein diversity. Perfectly eradication of introns and joining exons is the prominent task in eukaryotic gene expression, as exons are usually interrupted by introns. Identification of splicing sites through experimental techniques is complicated and time-consuming task. With the avalanche of genome sequences generated in the post genomic age, it remains a complicated and challenging task to develop an automatic, robust and reliable computational method for fast and effective identification of splicing sites.
               
               
                  Methods
                  In this study, a hybrid model “iSS-Hyb-mRMR” is proposed for quickly and accurately identification of splicing sites. Two sample representation methods namely; pseudo trinucleotide composition (PseTNC) and pseudo tetranucleotide composition (PseTetraNC) were used to extract numerical descriptors from DNA sequences. Hybrid model was developed by concatenating PseTNC and PseTetraNC. In order to select high discriminative features, minimum redundancy maximum relevance algorithm was applied on the hybrid feature space. The performance of these feature representation methods was tested using various classification algorithms including K-nearest neighbor, probabilistic neural network, general regression neural network, and fitting network. Jackknife test was used for evaluation of its performance on two benchmark datasets S
                     1 and S
                     2, respectively.
               
               
                  Results
                  The predictor, proposed in the current study achieved an accuracy of 93.26%, sensitivity of 88.77%, and specificity of 97.78% for S
                     1, and the accuracy of 94.12%, sensitivity of 87.14%, and specificity of 98.64% for S
                     2, respectively.
               
               
                  Conclusion
                  It is observed, that the performance of proposed model is higher than the existing methods in the literature so for; and will be fruitful in the mechanism of RNA splicing, and other research academia.
               
            

@&#INTRODUCTION@&#

Gene splicing plays prominent role in protein diversity and thus enable a single gene to increase its coding capability. The precursor messenger RNA (pre-mRNA) transcribed from one gene can lead to different mature mRNA molecules during a typical gene splicing event, which causes to generate multiple functional proteins. In eukaryotes gene, splicing takes place prior to mRNA translation by the differential inclusion or exclusion of regions called exons and introns of pre-mRNA. Exons that code for proteins are interrupted by non-coding regions called introns in eukaryotic genomes. There is a line between introns and exons called splice site (Fig. 1
                     ). Sides of introns have splice sites, the former is called the 5′ splice site or donor site and the latter is called the 3′ splice site or acceptor site. The vast amounts of donor and accepter sites form a pattern which is recognized by the presence of GT and AG, respectively. Spliceosome, which is comprises of 300 proteins and five small nuclear RNAs (snRNAs U1, U2, U4, U5, and U6) that is responsible for identification of donor and acceptor sites in genome sequence [1]. When splice sites become identified, spliceosome bind to both 3′ and 5′ ends of the introns and cause the intron to form a loop. With the help of two sequential transesterification reactions the given intron is eradicated from the genome sequence as shown in Fig. 1, while the remaining two exons are linked together [2,3]. Eliminating non-coding regions (introns) from (pre-mRNA) and fusing the required consecutive coding regions (exons) to form a mature messenger RNA (mRNA) is a prominent and notable step in gene expression. Therefore, to better understand the splicing mechanism; it is essential to identify the splicing sites in genome accurately.

Biochemical experimental approaches provide little details about identifying splicing sites with certain limitations, thus to rely only on these techniques is not appropriate, because these are time-consuming and expensive operations. In addition, these are not mostly applicable. Hence with increasing the density of logic, it is a great challenge, and extremely desirable task to develop computational methods for precise, consistent, robust and automated system for timely identification of splicing sites. A series of methods have been proposed to identify splicing sites consequently, considerable results have been achieved, but still it contains large vacuum for further improvements in term of prediction performance. After the comprehensive review [4] and also a series of latest publications [5–11] revealed that, to develop a really effective statistical predicator for biological system, we need to pass from the following steps: (i) in order to train and test the predictor, we need to construct or select a valid benchmark dataset; (ii) for correct reflection of biological sample in their intrinsic correlation with the target to be predicted, we have to formulate the sample with an effective mathematical expression; (iii) to operate the predication, a powerful algorithm is needed; (iv) also to evaluate the anticipated accuracy of the predictor objectively, properly cross validation tests is needed to be performed.

In view of the importance of splicing sites for genome analysis, the present study was initiated to develop a computational method for predicting splice sites. In the present work, a hybrid model “iSS-Hyb-mRMR” is proposed, which used pseudo trinucleotide composition and pseudo tetranucleotide composition strategies to extract numerical descriptors. To eradicate the irrelevant and redundant features from feature space, minimum redundancy and maximum relevance (mRMR) was applied. Classification algorithms including K-nearest neighbors (KNN), probabilistic neural network (PNN), generalized regression neural network (GRNN) and fitting network (FitNet) were utilized in order to select the best one among these. Jackknife test was applied to assess the performance of the classification algorithms using two datasets S
                     1 and S
                     2 for donor sites and acceptor sites, respectively.

The rest of the paper is organized as; Section 2 describes materials and methods, Section 3 describes evaluation criteria for performance measurement, Section 4 describes result and discussions and finally conclusion has been drawn in Section 5.

In order to develop a statistical predictor, it is preliminary to establish a reliable and stringent benchmark dataset for training and testing the predictor. However, in case of erroneous and redundant benchmark dataset, consequently, the outcomes of predictor must be unreliable and inconsistent. In order to remove the redundancy and reduce the similarity from the dataset usually CDHIT is applied. In addition, as pointed out in a comprehensive review [12], for examining the performance of a prediction method there is no need to split a benchmark dataset into a training and testing dataset. Because, the performance of predictor is evaluated by leave one out cross validation or sub-sampling tests, actually, the predicted outcomes are the combination of different independent dataset tests. In this regards, human splice site-containing sequences are obtained from HS3D (http://www.sci.unisannio.it/docenti/rampone/), having the sequences of exons, introns, and splice regions [13]. GT-AG rule is obeyed by all the sequences in this database i.e. that is begin with the dinucleotide GT (GU in case if RNA) and end with the dinucleotide AG. Therefore, we obtained two datasets, one for the splice donor site-containing sequences and the other for the splice acceptor, which can be formulated as
                           
                              (1)
                              
                                 
                                    
                                       S
                                       1
                                    
                                    =
                                    
                                       S
                                       1
                                       +
                                    
                                    ∪
                                    
                                       S
                                       1
                                       −
                                    
                                     
                                    splice
                                     
                                    donor
                                     
                                    dataset
                                    ,
                                 
                              
                           
                        
                        
                           
                              (2)
                              
                                 
                                    
                                       S
                                       2
                                    
                                    =
                                    
                                       S
                                       2
                                       +
                                    
                                    ∪
                                    
                                       S
                                       2
                                       −
                                    
                                     
                                    splice
                                     
                                    acceptor
                                     
                                    dataset
                                    ,
                                 
                              
                           
                        where the positive dataset 
                           
                              
                                 S
                                 1
                                 +
                              
                           
                         contains 2796 true splice donor site-containing sequences while the negative dataset 
                           
                              
                                 S
                                 1
                                 −
                              
                           
                         composed of 2800 false splice donor site-containing sequences; 
                           
                              
                                 S
                                 2
                                 +
                              
                           
                         contains 2880 true splice acceptor site-containing sequences, while 
                           
                              
                                 S
                                 2
                                 −
                              
                           
                         having 2800 false splice acceptor site-containing sequences, the symbol ∪ means the union in the set theory.

Suppose a DNA sequence D with L nucleotides; i.e.
                           
                              (3)
                              
                                 
                                    D
                                    =
                                    
                                       R
                                       1
                                    
                                    
                                       R
                                       2
                                    
                                    
                                       R
                                       3
                                    
                                    
                                       R
                                       4
                                    
                                    
                                       R
                                       5
                                    
                                    
                                       R
                                       6
                                    
                                    
                                       R
                                       7
                                    
                                    ,
                                    …
                                    ,
                                    
                                       R
                                       L
                                    
                                 
                              
                           
                        where
                           
                              (4)
                              
                                 
                                    
                                       R
                                       i
                                    
                                    ∈
                                    
                                       
                                          A
                                          (
                                          adenine
                                          )
                                          ,
                                          C
                                          (
                                          cytosine
                                          )
                                          ,
                                          G
                                          (
                                          guanine
                                          )
                                          ,
                                          T
                                          (
                                          thymine
                                          )
                                       
                                    
                                     
                                    i
                                    =
                                    (
                                    1,2
                                    ,
                                    …
                                    ,
                                    L
                                    )
                                 
                              
                           
                        where R
                        1 represents the first nucleotide at position 1; R
                        2 represents the second nucleotide at position 2, and R
                        
                           L
                         represents the last nucleotide at position L, respectively. Although, the above sequential formulation of (3) is more informative regarding DNA sample, but it is difficult to preciously predict statistically such a huge number of nucleotides. DNA sequences consist of four unique nucleotides (A, C, G and T). Thus, let suppose if we consider of only 100 nucleotides sequences, the number of different order combinations would be 4100
                        =10100 log 4
                        >1.6065×1060. Actually the length of DNA sequences is much more longer than 100 nucleotides sequences, therefore, the number of different combinations will be >1.6065×1060 
                        [14]. Therefore, first of all for such an astronomical number it is not realizable to construct a reasonable training dataset to statistically cover all the possible different sequence-order information. Secondly, DNA sequences vary widely in length, which give arise to another harder for incorporating the sequence-order information in both the dataset construction and algorithm formulation. Thirdly, all the existing efficient operation engines, such as support vector machine (SVM) [15–17], random forest (RF) [18–20], covariance discriminant (CD) [20], neural network [21], conditional random field [10], nearest neighbor (NN) [22], SLLE algorithm [23], K-nearest neighbor (KNN) [23], OET-KNN [24], fuzzy K-nearest neighbor [25–27], and ML-KNN algorithm [5], can only handle vector form rather than sequential samples. In this regard, BLAST was proposed for sequential samples, but this approach remains insufficient due to the lack of significant similarity [28] among the samples. To avoid the complete loss of sequence-order or pattern information for proteins, the pseudo amino acid composition (PseAAC) was proposed [29]. Owing the wide and successful usage of PseAAC into the areas of computational proteomics to deal with protein/peptide sequences in computational proteomic, recently, the concept of “pseudo k-tuple nucleotide composition (PseKNC)” has been introduced to deal with DNA/RNA sequences in computational genetics and genomics [30,31]. More recently two web-servers repDNA and Pse-in-One were proposed which successfully generates various modes of pseudo k-tuple nucleotide composition [32,33]. Consequently, the concept of discrete model was proposed to incorporate the sequence-order information of DNA sample effectively [34]. The simplest discrete model used to represent a DNA sample in its nucleic acid composition (NAC) as given below:
                           
                              (5)
                              
                                 
                                    D
                                    =
                                    
                                       
                                          [
                                          f
                                          (
                                          A
                                          )
                                          f
                                          (
                                          C
                                          )
                                          f
                                          (
                                          G
                                          )
                                          f
                                          (
                                          T
                                          )
                                          ]
                                       
                                       T
                                    
                                 
                              
                           
                        where f(A), f(C), f(G), and f(T) are the normalized occurrence frequencies of adenine (A), cytosine (C), guanine (G), and thymine (T), respectively; the symbol T represents transpose operator. If we analyze (5) critically, we have zero information about all the sequence-order of nucleotides. Therefore, one way to solve this problem here is to represent the DNA sequence with the k-tuples nucleotides composition vector, i.e. with 4
                           k
                         components as given below:
                           
                              (6)
                              
                                 
                                    D
                                    =
                                    
                                       
                                          [
                                          
                                             f
                                             1
                                             
                                                k-tuples
                                             
                                          
                                          ,
                                          
                                             f
                                             2
                                             
                                                k-tuples
                                             
                                          
                                          ,
                                          
                                             f
                                             i
                                             
                                                k-tuples
                                             
                                          
                                          ,
                                          
                                             f
                                             
                                                
                                                   4
                                                   k
                                                
                                             
                                             
                                                k-tuples
                                             
                                          
                                          ]
                                       
                                       T
                                    
                                 
                              
                           
                        where 
                           
                              
                                 f
                                 i
                                 
                                    k-tuples
                                 
                              
                           
                         is the normalized occurrence frequency of the ith k-tuples nucleotides in the DNA segment. We can see from (6), the dimension of the vector is
                           
                              
                                 (7)
                              
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   4
                                                   1
                                                
                                                =
                                                4
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   4
                                                   2
                                                
                                                =
                                                16
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   4
                                                   3
                                                
                                                =
                                                64
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   4
                                                   4
                                                
                                                =
                                                256
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   4
                                                   5
                                                
                                                =
                                                1024
                                             
                                          
                                       
                                       
                                          
                                             ⋮
                                          
                                       
                                       
                                          
                                             ⋮
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The above equation indicating that by increasing the value of K, although the coverage scope of sequence order will be gradually increased, the dimension of the vector D will be rapidly increased as well. Extra ordinary increasing of vector D will cause high-dimension disaster [35] which reflects certain disadvantages such as overfitting problem, unnecessarily computational time, predication of serious bias and low capacity for generalization, noise and redundancy that leads to poor predication of accuracy. To avoid such a high-dimension disaster, we have chosen pseudo trinucleotide and pseudo tetranucleotide composition.

The basic and foundational step in machine learning processes is considered to be feature extraction technique. Discrete numerical attributes are extracted from biological sequences in this phase, because statistical models need numerical descriptors for training. Feature extraction starts from an initial set of measured data and builds derived values called features, intended to be informative, non-redundant, facilitating the subsequent learning and generalization steps. In this work, two powerful DNA sequences representation approaches are used to extract high discriminative features.

The main limitation of simple nucleotide composition is that; it does not preserve sequence order information. In order to amalgamate the occurrence frequency along with sequence order information, pseudo trinucleotide composition (PseTNC) was proposed [9]. In PseTNC, the relative frequency of nucleotides pair is computed. As a result, 4×4×4=64−
                           D corresponding features are extracted. It can be demonstrated as:
                              
                                 (8)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   D
                                                   =
                                                   
                                                      
                                                         [
                                                         
                                                            f
                                                            1
                                                            
                                                               3
                                                               -tuples
                                                            
                                                         
                                                         ,
                                                         
                                                            f
                                                            2
                                                            
                                                               3
                                                               -tuples
                                                            
                                                         
                                                         ,
                                                         
                                                            f
                                                            3
                                                            
                                                               3
                                                               -tuples
                                                            
                                                         
                                                         ,
                                                         
                                                            f
                                                            4
                                                            
                                                               3
                                                               -tuples
                                                            
                                                         
                                                         ,
                                                         …
                                                         ,
                                                         
                                                            f
                                                            
                                                               64
                                                            
                                                            
                                                               3
                                                               -tuples
                                                            
                                                         
                                                         ]
                                                      
                                                      T
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   D
                                                   =
                                                   
                                                      
                                                         (
                                                         f
                                                         (
                                                         AAA
                                                         )
                                                         ,
                                                         f
                                                         (
                                                         AAC
                                                         )
                                                         ,
                                                         f
                                                         (
                                                         AAG
                                                         )
                                                         ,
                                                         f
                                                         (
                                                         AAT
                                                         )
                                                         ,
                                                         …
                                                         ,
                                                         f
                                                         (
                                                         TTT
                                                         )
                                                         )
                                                      
                                                      T
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 
                                    f
                                    1
                                    
                                       3
                                       -tuples
                                    
                                 
                                 =
                                 f
                                 (
                                 AAA
                                 )
                              
                            is the normalized occurrence frequency of AAA in the DNA Sequence; 
                              
                                 
                                    f
                                    2
                                    
                                       3
                                       -tuples
                                    
                                 
                                 =
                                 f
                                 (
                                 AAC
                                 )
                              
                           , that of AAC; and so forth.

In trinucleotide composition, only three nucleotides are paired. This is still far behind the sequence order information, therefore, to give considerable information, we have to move one step more forward and to use pseudo tetranucleotide composition (PseTetraNC) [8,14]. In PseTetraNC, the occurrence frequency of four nucleotides pair is calculated. It can be formulated as:
                              
                                 (9)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   D
                                                   =
                                                   
                                                      
                                                         [
                                                         
                                                            f
                                                            1
                                                            
                                                               4
                                                               -tuples
                                                            
                                                         
                                                         ,
                                                         
                                                            f
                                                            2
                                                            
                                                               4
                                                               -tuples
                                                            
                                                         
                                                         ,
                                                         
                                                            f
                                                            3
                                                            
                                                               4
                                                               -tuples
                                                            
                                                         
                                                         ,
                                                         
                                                            f
                                                            4
                                                            
                                                               4
                                                               -tuples
                                                            
                                                         
                                                         ,
                                                         …
                                                         ,
                                                         
                                                            f
                                                            
                                                               256
                                                            
                                                            
                                                               4
                                                               -tuples
                                                            
                                                         
                                                         ]
                                                      
                                                      T
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   D
                                                   =
                                                   
                                                      
                                                         [
                                                         f
                                                         (
                                                         AAAA
                                                         )
                                                         ,
                                                         f
                                                         (
                                                         AAAC
                                                         )
                                                         ,
                                                         f
                                                         (
                                                         AAAG
                                                         )
                                                         ,
                                                         f
                                                         (
                                                         AAAT
                                                         )
                                                         ,
                                                         …
                                                         ,
                                                         f
                                                         (
                                                         TTTT
                                                         )
                                                         ]
                                                      
                                                      T
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 
                                    f
                                    1
                                    
                                       4
                                       -tuples
                                    
                                 
                                 =
                                 f
                                 (
                                 AAAA
                                 )
                              
                            is the normalized occurrence frequency of AAAA, 
                              
                                 
                                    f
                                    2
                                    
                                       4
                                       -tuples
                                    
                                 
                                 =
                                 f
                                 (
                                 AAAC
                                 )
                              
                            that of AAAC; in the DNA sequence and so forth, therefore, the corresponding feature space will contain 4×4×4×4=256−
                           D pairs of the nucleotides.

The above mentioned procedure revealed that, as the number of nucleotides in pair is increased consequently, the number of tuples increased. Hence the local or short range of sequence order information is gradually included into information.

Sometime, single feature extraction strategy does not achieve reasonable results due to lack of discrimination power. In such circumstances, the fusing of various feature extraction strategies are required to compensate the weakness of one's feature extraction strategy by another, and enhanced the discrimination properties [9]. In this regards, we have fused two feature extraction strategies including PseTNC and PseTetraNC to form hybrid space. This hybrid model has a feature vector of dimension 320-D (64+256). The main reason of using the hybrid feature extraction strategy is to exploit the benefits of the both PseTNC and PseTetraNC for the prediction of splicing sites. However, the dimensionality of the resultant feature space should not be so high that it affects the prediction performance of the classifier.

In machine learning and statistics, the feature reduction is the process of selecting a subset of useful and relevant features which are used as an input in model construction. The central assumption when using a feature reduction technique is that, if the data contains many redundant or irrelevant features [36]. Feature selection techniques are often used in domains where there are many features and comparatively few samples or data points. Feature selection technique provides three main advantages: it improves model interpretability; bring down training times, and enhances generalization by reducing overfitting.

Sometimes, the acquired attributes are highly correlated and not all of the attributes contribute in the comprehensive determination or definition of the target phenotypes. In addition, extra-ordinary large feature space significantly slows down the learning process as well as the efficiency of a classifier. Hence, we need to find mutually exclusive and low correlated feature subsets. These attributes are obtained by using minimum-redundancy-maximum-relevance (mRMR) [37]. It was first adopted by Peng et al. [38]. The mRMR method attempts to determine whether a feature vector has minimum redundancy with other features and maximum relevance with the target class. The set of selected features should be maximally different from each other. Let S denote the subset of features that we are looking for. The minimum redundancy condition is
                              
                                 (10)
                                 
                                    
                                       M
                                       i
                                       n
                                       =
                                       (
                                       
                                          P
                                          1
                                       
                                       ,
                                       
                                          P
                                          2
                                       
                                       )
                                       =
                                       
                                          1
                                          
                                             |
                                             S
                                             
                                                |
                                                2
                                             
                                          
                                       
                                       
                                          ∑
                                          
                                             
                                                X
                                                i
                                             
                                             
                                                X
                                                
                                                   j
                                                   ∈
                                                   S
                                                
                                             
                                          
                                       
                                       
                                          M
                                          (
                                          
                                             X
                                             i
                                          
                                          
                                             X
                                             j
                                          
                                          )
                                       
                                    
                                 
                              
                           where Min(i, j) represent similarity between features, and |S| is the number of features in S. In general, to achieve high performance minimizing just redundancy is not enough. So the minimum redundancy criteria should be supplemented by maximizing relevance between the target variable and others variables. To measure the level of discrimination power of features when they are differentially expressed for different target classes, again a similarity measure Min(y, x
                           
                              i
                           ) is used, between targeted classes y
                           ={0, 1} and the feature expression x
                           
                              i
                           . This measure quantifies the relevance of x
                           
                              i
                            for the classification task. Thus the maximum relevance condition is to maximize the total relevance of all features in S:
                              
                                 (11)
                                 
                                    
                                       M
                                       a
                                       x
                                       =
                                       (
                                       
                                          P
                                          1
                                       
                                       ,
                                       
                                          P
                                          2
                                       
                                       )
                                       =
                                       
                                          1
                                          
                                             |
                                             S
                                             
                                                |
                                                2
                                             
                                          
                                       
                                       
                                          ∑
                                          
                                             
                                                X
                                                i
                                             
                                             ∈
                                             S
                                          
                                       
                                       
                                          M
                                          (
                                          Y
                                          ,
                                          
                                             X
                                             j
                                          
                                          )
                                       
                                    
                                 
                              
                           
                        

Combining both the criteria such as: maximal relevance with the target variable and minimum redundancy between features is called the minimum redundancy-maximum relevance (mRMR) approach. The mRMR feature set is obtained by optimizing the problems P
                           1 and P
                           2, receptively in Eqs. (10) and (11) simultaneously. Optimization of both conditions requires combining them into a single criterion function
                              
                                 (12)
                                 
                                    
                                       M
                                       i
                                       n
                                       =
                                       {
                                       
                                          P
                                          1
                                       
                                       −
                                       
                                          P
                                          2
                                       
                                       }
                                    
                                 
                              
                           mRMR approach has advantageous over other feature selection techniques. In fact, we can get a more representative feature set of the target variable which increases the generalization capacity of the chosen feature set. Hence, mRMR approach gives a smaller feature set which effectively cover the same space as a larger conventional feature set does.

Classification is a supervised learning, in which the data are categorized into predetermined classes. In this study, several supervised classification algorithms are utilized in order to select the best one for identification of splicing sites.

KNN is widely used algorithm in the field of pattern recognition, machine learning and many other areas like that. KNN is a non-parametric method used for classification and regression purposes [39]. KNN algorithm is also known as instance based learning (lazy learning) algorithm. It does not build classifier or model on the spot but save all the training data samples and wait until new observation needs to be classified. Lazy learning nature of KNN makes it better than eager leaning which construct classifier before new observation needs to be classified. It is effective in case of dynamic data that changes and updates rapidly [40]. KNN algorithm has the following five steps:
                              
                                 Step 1: for training the model, the extracted DNA features are provided to KNN algorithm.

Step 2: Euclidean distance formula is used for measuring distance.
                                       
                                          (13)
                                          
                                             
                                                
                                                   E
                                                   
                                                      d
                                                      i
                                                      s
                                                   
                                                
                                                (
                                                
                                                   x
                                                   1
                                                
                                                ,
                                                
                                                   x
                                                   2
                                                
                                                )
                                                =
                                                
                                                   ∑
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   n
                                                
                                                
                                                   
                                                      
                                                         
                                                            
                                                               (
                                                               
                                                                  x
                                                                  
                                                                     i
                                                                     1
                                                                  
                                                               
                                                               −
                                                               
                                                                  x
                                                                  
                                                                     i
                                                                     2
                                                                  
                                                               
                                                               )
                                                            
                                                            2
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 

Step 3: Euclidean distance values are sorted as d
                                    
                                       i
                                    
                                    ≤
                                    d
                                    
                                       i
                                    
                                    +1, where i
                                    =1, 2, 3,…,
                                    k.

Step 4: apply voting or means according to the data nature.

Step 5: number of nearest neighbor (value of K) depends upon the nature and volume of data provided to KNN. The k value is taken large for huge data, and small for small data.

The probabilistic neural network (PNN) is based on Bayes theory, which was first introduced by Specht in 1990 [41]. It is often used for classification purposes [42]. On the basis of probability density function, PNN provides an interactive way to interpret the structure of the network. PNN has a similar structure as feed-forward networks but it works in four layers. The first layer is known as input layer, which contains the input vector that are connected to the input neurons and passed to the second layer also called pattern layer. The number of samples presented to the network and dimensions of pattern layer are equals in number. Pattern layer and input layer have one to one correspondence by exactly one neuron for each training data sample. The third layer is called summation layer, which has the same dimensions as the number of classes in the set of data samples. Lastly the fourth layer output/decision layer categories the number of samples into one the predefined classes.

General regression neural network (GRNN), which belongs to the category of probabilistic neural networks, was proposed by Donald F. Specht. It is non-parametrical kernel regression estimators used in statistics [43]. Structurally and functionally GRNN is very similar to PNN. It has four layers i.e., input layer, radial base layer, special linear layer and the output layer. The total number of neurons in the input and output of GRNN is equal to the dimension of the input and output vector. GRNN is most suitable network for small and medium size datasets. Its overall process is carried out in three steps. A set of training data and target data are created in the first step. The input data, target data and spread constant value is passed to new GRNN as arguments in the second step. And finally in the last step response of the network is noted by simulating it according to the data provided. GRNN is advantageous comparatively others neural networks, because it can accurately compute the approximation function from sparse data and also extract automatically the appropriate regression model (linear or nonlinear) from the data; it can train rapidly with very simple topology design [44].

Fitting network (FitNet) is an artificial neural network (ANN) that consists of N layers. It is a subtype of feed forward back propagation neural network (FFBPNN). Its first layer is connected to the input vector. The preceding layer has a connection with each subsequent layer. The resultant output is produced by final layer of the network. The training of FFBPNN is carried out using the following Eq. (14):
                              
                                 (14)
                                 
                                    
                                       
                                          U
                                          k
                                       
                                       (
                                       t
                                       )
                                       =
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          n
                                       
                                       
                                          
                                             w
                                             
                                                j
                                                k
                                             
                                          
                                          (
                                          t
                                          )
                                          ⋅
                                          
                                             x
                                             j
                                          
                                          (
                                          t
                                          )
                                          +
                                          
                                             b
                                             
                                                0
                                                k
                                             
                                          
                                          (
                                          t
                                          )
                                       
                                    
                                 
                              
                           
                           
                              
                                 (15)
                                 
                                    
                                       
                                          Y
                                          k
                                       
                                       (
                                       t
                                       )
                                       =
                                       φ
                                       (
                                       
                                          U
                                          k
                                       
                                       (
                                       t
                                       )
                                       )
                                    
                                 
                              
                           where x
                           
                              j
                           (t) shows the input value of j to the neuron at time t, w
                           
                              jk
                           (t) the weight that is assigned to input value by neuron k and b
                           0 is the bias of k neuron at time t. Whereas Y
                           
                              k
                           (t) is the output of neuron k and φ is the activation function [38]. Fitting network (FitNet) is used to fit an input–output relationship [45]. Levenberg–Marquardt algorithm is the default algorithm used for the training of the system. The algorithm divides the feature vector randomly into three sets: (i) the training data; (ii) the validation set data; and (iii) the test data [46]. A fitting network with one hidden layer and enough number of neurons can fit any finite numbers of input and output relationship [47].

Looking at the importance of splicing sites, “iSS-Hyb-mRMR” model is proposed for identification of receptor and donor of splicing sites as shown in Fig. 2
                        . In this model, two feature extraction methods including PseTNC and PseTetraNC were used to extract feature from the datasets S
                        1 and S
                        2, respectively. Then, the extracted features were fused to from a hybrid space in order to enhance their discrimination power. Further, feature selection technique mRMR was applied on the hybrid space to select those features which have minimum redundancy and maximum relevance for the best classification purpose. Four different classifiers, i.e. KNN, PNN, GRNN and Fit-Net were used for classification. The best results among individual classifiers were noted. Consequently “iSS-Hyb-mRMR” produced higher performance than existing methods is the literature so far.

Performance evaluation and metrics in classification is fundamental in assessing the quality of learning methods. However many different measures have been used in the literature with the aim of making better choices in general or for a specific application area. Choices made by one metric are claimed to be different from choices made by other metrics. In order to assess our model, we have used the following different performance measures, which are given below:

In the fields of science, engineering, and statistics, the accuracy of a measurement system is the degree of closeness of measurements of a quantity to that quantity's actual (true) value.
                           
                              (16)
                              
                                 
                                    Accuracy
                                    =
                                    
                                       
                                          TP
                                          +
                                          TN
                                       
                                       
                                          TP
                                          +
                                          FP
                                          +
                                          FN
                                          +
                                          TN
                                       
                                    
                                    ×
                                    100
                                 
                              
                           
                        where TP is true positive, TN is false negative, TN is true negative and FP is false positive.

Sensitivity and specificity are also known in statistics as classification function, which are statistical measures of the performance of a binary classification test. Sensitivity (true positive rate) measures the proportion of actual positives which are correctly identified.
                           
                              (17)
                              
                                 
                                    Sensitivity
                                    =
                                    
                                       
                                          TP
                                       
                                       
                                          TP
                                          +
                                          FN
                                       
                                    
                                    ×
                                    100
                                 
                              
                           
                        
                     

Specificity (true negative rate) measures the proportion of negatives which are correctly identified:
                           
                              (18)
                              
                                 
                                    Specificity
                                    =
                                    
                                       
                                          TN
                                       
                                       
                                          FP
                                          +
                                          TN
                                       
                                    
                                    ×
                                    100
                                 
                              
                           
                        
                     

The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary (two-class) classifications. It takes into account true and false positives and negatives. It can be used; even if the classes having varying sizes, due to this factor it is generally regarded as a balance measure. The MCC is in essence of a correlation coefficient between the observed and predicted binary classifications; it returns a value in the range of −1 and +1. A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and −1 indicates total disagreement between prediction and observation.
                           
                              (19)
                              
                                 
                                    
                                       MCC
                                       i
                                    
                                    =
                                    
                                       
                                          TP
                                          ×
                                          TN
                                          −
                                          FP
                                          ×
                                          FN
                                       
                                       
                                          
                                             
                                                [
                                                TP
                                                +
                                                FP
                                                ]
                                                [
                                                TP
                                                +
                                                FN
                                                ]
                                                [
                                                TN
                                                +
                                                FP
                                                ]
                                                [
                                                TN
                                                +
                                                FN
                                                ]
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The weighted average of Precision and recall is known as F-measure. It is used for the evaluation of statistical methods. F-measure can be calculated using:
                           
                              (20)
                              
                                 
                                    F-measure
                                    =
                                    2
                                    ×
                                    
                                       
                                          Precision
                                          ×
                                          Recall
                                       
                                       
                                          Precision
                                          +
                                          Recall
                                       
                                    
                                 
                              
                           
                        
                     

F-measure depends on two things; precision p and recall r.
                           
                              (21)
                              
                                 
                                    Precision
                                    =
                                    
                                       
                                          TP
                                       
                                       
                                          TP
                                          +
                                          FP
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (22)
                              
                                 
                                    Recall
                                    =
                                    
                                       
                                          TP
                                       
                                       
                                          TP
                                          +
                                          FN
                                       
                                    
                                 
                              
                           
                        where the resultant best value for F-measure is 1 and the worst value is 0.

Although in the literature so for, these four metrics (Eqs. (16)–(19)) are often used to measures the predication of the model, but for most biologist these are not easy-to-understand because of lacking intuitiveness. To void this difficulty, in the current study, we have adopted the following simple formulation that is proposed in the recent publications [48–50].
                           
                              (23)
                              
                                 
                                    Acc
                                    =
                                    1
                                    −
                                    
                                       
                                          
                                             N
                                             −
                                             +
                                          
                                          +
                                          
                                             N
                                             +
                                             −
                                          
                                       
                                       
                                          
                                             N
                                             +
                                          
                                          +
                                          
                                             N
                                             −
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (24)
                              
                                 
                                    Sp
                                    =
                                    1
                                    −
                                    
                                       
                                          
                                             N
                                             +
                                             −
                                          
                                       
                                       
                                          
                                             N
                                             −
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (25)
                              
                                 
                                    Sn
                                    =
                                    1
                                    −
                                    
                                       
                                          
                                             N
                                             −
                                             +
                                          
                                       
                                       
                                          
                                             N
                                             +
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (26)
                              
                                 
                                    Mcc
                                    =
                                    
                                       
                                          1
                                          −
                                          
                                             
                                                
                                                   
                                                      
                                                         N
                                                         −
                                                         +
                                                      
                                                      +
                                                      
                                                         N
                                                         +
                                                         −
                                                      
                                                   
                                                   
                                                      
                                                         N
                                                         +
                                                      
                                                      +
                                                      
                                                         N
                                                         −
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      1
                                                      +
                                                      
                                                         
                                                            
                                                               N
                                                               +
                                                               −
                                                            
                                                            −
                                                            
                                                               N
                                                               −
                                                               +
                                                            
                                                         
                                                         
                                                            
                                                               N
                                                               +
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      1
                                                      +
                                                      
                                                         
                                                            
                                                               N
                                                               −
                                                               +
                                                            
                                                            −
                                                            
                                                               N
                                                               +
                                                               −
                                                            
                                                         
                                                         
                                                            
                                                               N
                                                               −
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The above mentioned metrics given in Eqs. (23)–(26) are valid only for single-label system. In the above equations N
                        + represents the total number of true splicing sites samples and N
                        − represents the total number of false splicing sites investigated. Likewise, 
                           
                              
                                 N
                                 −
                                 +
                              
                           
                         represents the total number of true splicing sites samples that are incorrectly predicted as false splicing sites and 
                           
                              
                                 N
                                 +
                                 −
                              
                           
                         represents the total number of false splicing sites samples that are incorrectly predicted as true splicing sites. For multi-label systems whose existence has become more frequently in system biology [51,52], and system medicine [53], a completely different set of metrics are needed as defined in [4].

@&#RESULTS AND DISCUSSIONS@&#

In statistical prediction, the following three well known cross-validation methods are often used to examine a predictor for its effectiveness in practical application: independent dataset test, subsampling test, and Jackknife test [54,55]. However, as elucidated and demonstrated in [4,56], among the three cross-validation methods, the Jackknife test is deemed the least arbitrary (most objective) that can always yields a unique result for a given benchmark dataset, and hence has been increasingly used and widely recognized by investigators to examine the accuracy of various predictors [29,57–65]. Accordingly, the Jackknife test was also adopted here to examine the quality of the present predictor [9]. Performance comparisons of individual and hybrid feature spaces on two datasets have been discussed below:

The success rates of classification algorithms, using PseTNC feature space are listed in Table 1
                           . This features space contains 64 features. Among these classifiers, KNN achieved the highest predication performance with accuracy of 92.59%, sensitivity of 87.86%, specificity of 97.64%, MCC of 0.86, and F-measure of 0.92.

The experimental results of classifiers using PseTetraNC feature space are listed in Table 2
                           . This features space contains 256 features. Due to considerable information in this vector, it remained efficient in terms of accuracy as compared with PseTNC features space. Among these classification algorithms, KNN achieved the highest performance predication with accuracy of 93.40%, sensitivity of 87.53%, specificity of 96.71%, MCC of 0.85 and F-measure of 0.92.

The success rates of classification algorithms using hybrid feature space are listed in Table 3
                           . Due to hybrid space of PseTNC and PseTetraNC with 320 features, the performance of classification algorithms is enhanced compared to individual feature spaces. Among these classifiers, KNN achieved the highest results with accuracy of 93.51%, sensitivity of 86.89%, specificity of 97.67%, MCC of 0.85 and F-measure of 0.92.

After applying feature reduction technique mRMR on hybrid space the success rates of classification algorithms are listed in Table 4
                           . The reduced feature space contains 290 features, because highest results have been achieved on that feature space. Due to the eradication of irrelevant and redundant descriptors, the overall results remained excellent. Among these classifiers, KNN achieved the best prediction performance with an accuracy of 94.12%, sensitivity of 87.14%, specificity of 98.64%, MCC of 0.86 and F-measure of 0.93.

The experimental results of classifiers using PseTNC feature space are listed in Table 5
                           . This features space contains 64 features. Among these classification algorithms, KNN achieved the highest prediction performance with an accuracy of 92.58%, sensitivity of 87.85%, specificity of 97.64%, MCC of 0.85 and F-measures of 0.92.

The experimental results of classifiers using PseTetraNC features space are listed in Table 6
                           . This features space contains 256 features. Similar to the accepter's sites, for donor sites result become improved due to PseTetraNC features space. Among these classification algorithms, KNN achieved the highest success rates with an accuracy of 93.40%, sensitivity of 87.53%, specificity of 96.71%, MCC of 0.84 and F-measures of 0.92.

The experimental results of the classification algorithms using hybrid space of PseTNC and PseTetraNC for donor dataset are given in Table 7
                           . Due to hybrid space of PseTNC and PseTetraNC with 320 features, the overall performance of classification algorithms improved. KNN achieved the highest results with an accuracy of 93.51%, sensitivity of 86.90%, specificity of 97.67%, MCC of 0.85 and F-measure of 0.92, among the classifiers.

The success rates of the classification algorithms using reduced hybrid space of PseTNC and PseTetraNC for donor dataset are given in Table 8
                           . The feature space is reduced to 205 features, because excellent results have been reported on it. Due to reduction of irrelevant and redundant descriptors, the overall results remain excellent. Still, KNN obtained the highest results with an accuracy of 93.26%, sensitivity of 88.77%, specificity of 97.78%, MCC of 0.86 and F-measure of 0.92, among the classifiers.

Comparison has been drawn among the proposed model and already existing methods in the literature reported in Table 9
                        . The pioneer work has been carried out by BLAST [66]. The predicted outcomes of BLAST model for acceptor sites were accuracy of 39.62%, sensitivity of 39.09%, specificity of 40.20%, and MCC of −0.21 and predicted outcomes for donor sites were accuracy of 40.23%, sensitivity of 42.75%, specificity of 37.63%, and MCC of 0.20. Recently, iSS-PseDNC predictor was developed and produced considerable results [13]. The results of iSS-PseDNC for acceptor sites were 87.73%, 88.78%, 86.64%, and 0.75 accuracy, sensitivity, specificity, and MCC, respectively, and for donor sites iSS-PseDNC produced 85.45%, 86.66%, 84.25% and 0.71 accuracy, sensitivity, specificity, and MCC, respectively. In contrast, our proposed composite model “iSS-Hyb-mRMR” has achieved quite promising results compared to existing methods. “iSS-Hyb-mRMR” obtained with accuracy of 94.12%, sensitivity of 87.14%, specificity of 98.64% and MCC of 0.86 of acceptor sites and achieved accuracy of 93.26%, sensitivity of 88.77%, specificity of 97.78% and MCC of 0.87 for donor sites. As demonstrated in a series of recent publications [67–71], in developing new prediction methods, user-friendly and publicly accessible web-servers will significantly enhance their impacts [72–75]. Therefore, in future work we shall make efforts to provide a web-server for the prediction method presented in this paper.

@&#CONCLUSIONS@&#

RNA splicing is a complicated biological process, which involves interaction among DNA, RNA and proteins, thus to formulate it in a statistical model, and accurately analyze them is the central part of our work. In this study, a high throughput computational model called “iSS-Hyb-mRMR” has been developed for the identification of Splicing sites. Two feature extraction methods, pseudo trinucleotide composition and pseudo tetranucleotide composition were used to extract features from DNA sequences. The extracted features were then combined to form a hybrid space in order to enhance the discrimination power. Further mRMR was applied to select high discriminative feature from hybrid feature space. The performances of classification algorithms were evaluated on individual as well as hybrid feature spaces. After examined the performance of classifiers, the performance of KNN is remarkable. In addition, it performance is also higher than already existing methods in the literature so for. This remarkable achievement has been ascribed with high discriminative features of reduced hybrid PseTNC and PseTetraNC. It is anticipated that our proposed model might be helpful in drug related application along with academia.

@&#REFERENCES@&#

