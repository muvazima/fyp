@&#MAIN-TITLE@&#Latent semantics in language models

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           The unsupervised techniques of language modelling are investigated.


                        
                        
                           
                           We use global semantics, local semantics, and morphology information in our models.


                        
                        
                           
                           We experiment with modelling of six different languages.


                        
                        
                           
                           Final models dramatically reduce the perplexities and improve machine translation.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Language models

Latent Dirichlet allocation

Semantic spaces

Stemming

HAL

COALS

Random indexing

HPS

LDA

Machine translation

Moses

@&#ABSTRACT@&#


               
               
                  This paper investigates three different sources of information and their integration into language modelling. Global semantics is modelled by Latent Dirichlet allocation and brings long range dependencies into language models. Word clusters given by semantic spaces enrich these language models with short range semantics. Finally, our own stemming algorithm is used to further enhance the performance of language modelling for inflectional languages.
                  Our research shows that these three sources of information enrich each other and their combination dramatically improves language modelling. All investigated models are acquired in a fully unsupervised manner.
                  We show the efficiency of our methods for several languages such as Czech, Slovenian, Slovak, Polish, Hungarian, and English, proving their multilingualism. The perplexity tests are accompanied by machine translation tests that prove the ability of the proposed models to improve the performance of a real-world application.
               
            

@&#INTRODUCTION@&#

Language modelling is an essential part in many tasks of natural language processing (NLP). Speech recognition, machine translation, optical character recognition, and many other disciplines strongly depend on the language model and thus every improvement in language modelling can also improve the performance of the whole system.

In this paper we explore fully unsupervised methods for language modelling (which require no labelled data and no information about language itself). To prove their multilingualism we experiment with several languages including highly inflectional as well as low-inflection languages. We incorporate three different families of languages (Slavic, Uralic, and Germanic) into our experiments. As representatives of Slavic languages we experiment with Czech, Slovenian, Slovak, and Polish. Uralic languages are represented by Hungarian, and Germanic languages by English. All languages we investigate in this paper except English are characterized by a high level of inflection and relatively free word order (from the syntactic point of view, the words in a sentence can usually be reordered in several ways to carry a slightly different meaning). Properties of these languages complicate the language modelling task. The great number of word forms and large number of possible word sequences lead to a much higher number of n-grams. Data sparsity is a common problem of language models, but for highly inflected languages this problem is even more evident.

The highly inflected languages in this paper belong rather among non-mainstream languages, for which the language modelling task has not gained as much attention as it has for English, for example. We thus believe there is considerable potential for improvements. However we provide experiments also for English to compare our methods with the state of the art.

In this paper we extend our work on the application of semantic spaces in language modelling (Brychcín and Konopík, 2014), where we have achieved significant improvements in perplexity and in machine translation task especially with HAL, COALS and RI models. Thus, these models are investigated more deeply in this paper.

We attempt to improve language modelling by adding long-range semantic dependencies. We choose latent Dirichlet allocation (LDA) (Blei et al., 2003) for that task, because it has already been shown by many researchers that LDA improves language modelling (see, e.g., Tam and Schultz, 2005, 2006; Watanabe et al., 2011).

The performance of these language models is further enhanced by our unsupervised stemming algorithm called high precision stemmer (HPS)
                        1
                     
                     
                        1
                        A description of the algorithm and its implementation is available at http://liks.fav.zcu.cz/HPS.
                      introduced in Brychcín and Konopík (2015). We have already tested our stemmer in language modelling tasks and the results indicate that HPS performs best compared to other unsupervised stemmers.

To the best of our knowledge we are first to try to combine these three sources of information (i.e. local semantics, global semantics, and morphology).

In the context of this article, we work with various methods for modelling semantic relations between words, and use them to improve our language models. The backbone principle of methods for discovering hidden meaning from a plain text is the formulation of distributional hypothesis in Firth (1957) that says “a word is characterized by the company it keeps”. The direct implication of this hypothesis is that the word meaning is related to the context where it usually occurs and thus it is possible to compare the meanings of two words by statistical comparisons of their contexts. This implication was confirmed by empirical tests carried out on human groups in Charles (2000).

Several authors have made huge efforts to give an overview of the current state of the art in computational methods for extracting meaning from text (Turney and Pantel, 2010; Riordan and Jones, 2011; McNamara, 2011).

All the methods for extracting meaning can be approximately summarized in two categories. Authors Riordan and Jones (2011) and McNamara (2011) categorize these methods into context-word and context-region approaches. In this paper we use the notation local context and global context, respectively, because we think this notation describes the principle of meaning extraction better. These two categories are briefly described in the following Sections 2.1 and 2.2. Additionally, to give a better idea of how these two approaches differ, Fig. 1
                      shows an example of global context and local context semantics of words.

Models based upon the distributional hypothesis usually represent the meaning as a point in a multi-dimensional space. Thus, one meaning is represented as a single vector. These models are then referred to as the vector-space models (VSM). The vector representation enables an easy comparison of word meanings by computing distances between the vectors.

Global semantics models assume that words that are close in meaning will occur in similar pieces of text (documents). These methods are usually based on the bag-of-words hypothesis, which assumes the word order has no meaning.

Latent semantic analysis (LSA) (Deerwester et al., 1990) is a model for discovering global semantic relationships between words. A term-document matrix is constructed and then the singular value decomposition (SVD) is used to reduce the dimension of the matrix and to smooth the values of the matrix.

In Hofmann (1999) the probabilistic latent semantic analysis (PLSA) model was introduced, which is in fact the Bayesian version of LSA. PLSA assumes that the document is a mixture of topics and a topic is simultaneously a mixture of words. The probabilistic output makes this model more easily applicable in many tasks.

PLSA was later extended to LDA (Blei et al., 2003), which places the Dirichlet prior to the document-topic distribution as well as the topic-word distribution. LDA is thus the proper model even for unseen documents that has often been criticized in the case of PLSA.

The motivation for using such methods in language modelling is the fact that text can continuously change in domain, topics, writing style, and so on and it is not possible to recognize these changes only from a short history of words (let us say three words if we use four-gram language models). A much larger history is needed to observe these changes, where of course the data sparsity problem is much more evident. Inhibition of word order thus leads to far fewer possible combinations of histories.

LDA is used for boosting the probabilities of words that are likely to co-occur in the same document (as will be described in Section 4.3). This is independent of the position of words in the document (the document is a bag of words). These word probabilities only depend on the document itself (global context). This brings long-range dependencies, and language models are thus adapted to the current domain of text.

The second approach to modelling the semantics of words is to use only their local context. Local semantic models assume that the meaning of a word is related to the short context around the word. Methods based on this assumption usually use a small context window (let us say four words in both directions). These methods do not require text that is naturally divided into documents, which can be found advantageous compared to the methods mentioned in the previous section (LSA, PLSA, LDA).

Because of the short context, these methods can take word order into account, so the methods model semantic as well as syntactic relations between words. There are a lot of methods for deriving word meaning from the local context. We have already experimented with five of them in Brychcín and Konopík (2014). In this paper we continue our research and use only the three best performing methods in language modelling (HAL, COALS, and RI).

Hyperspace analogue to language (HAL) (Lund and Burgess, 1996) is a very simple method for building semantic space. HAL goes through the corpus and records the co-occurring words around the target word (in some small context window – typically four words in both directions). HAL distinguishes between left and right context of the target word and records them separately. Co-occurring words are weighted inversely to the distance from the target word. This results in the co-occurrence matrix 
                           
                              
                                 M
                              
                           
                           =
                           |
                           W
                           |
                           ×
                           2
                           |
                           W
                           |
                        , where |W| is the vocabulary size. The word vector dimension is 2|W| because of distinction between left and right context.

Correlated occurrence analogue to lexical semantics (COALS) (Rohde et al., 2004) is an extension of the HAL model. It starts almost identically to HAL, but it does not distinguish between left and right context. After recording the co-occurrence information, the raw counts of the matrix 
                           
                              M
                           
                         are converted into the Pearson's correlations. Negative values are zeroed and other values are replaced by their square roots. The optional final step, inspired by LSA (Deerwester et al., 1990), is to apply the SVD reduction to the matrix 
                           
                              M
                           
                        , resulting in the smoothing of values and also the discovery of latent semantic relationships between words.

Random indexing (RI) (Sahlgren, 2005) uses a completely different approach to recording co-occurrence statistics compared to HAL and COALS. For each word in the vocabulary, RI starts by creating high-dimensional index vectors randomly filled with few 1s and −1s (Sahlgren et al. (2008) recommend to fill index vectors with two 1s and two −1s). The dimension is typically of the order of thousands. Such vectors are very sparse and thus unlikely to overlap. The index vectors are assumed to be nearly orthogonal. The algorithm then iterates over the corpus and for each target word it sums all the co-occurring words’ index vectors. These sums are recorded in the matrix 
                           
                              M
                           
                        . Even though, RI performs a little worse than the other two methods in language modelling, we use it again because it is computationally very undemanding.

In Brychcín and Konopík (2014) we also tested BEAGLE (Jones and Mewhort, 2007) and P&P (Purandare and Pedersen, 2004) models, but these models did not perform as well in language modelling as the above mentioned methods.

There are several interesting methods which we have not investigated in language modelling yet, but which are certainly worth mentioning.

Similar to Purandare and Pedersen (2004), the authors of Reisinger and Mooney (2010) and Reisinger and Mooney (2010) address the common problem of VSMs, where each word is represented with a single vector, which clearly fails to capture homonymy and polysemy. In their approach, contexts for a single word are clustered to create several meanings of the word. Similar studies, where the word meaning is disambiguated according to the context, can be found in Dinu and Lapata (2010), Erk and Padó (2010) and Huang et al. (2012).

Recently, the neural-network models for learning word representations get attention from many researchers. Artificial neural networks hold an implicit ability to store all seen data (words) in their weights. In theory, it should be enough to infer word meanings. However, many researcher specifically design network architectures to support inference of semantic information. For example, recurrent neural networks can use a memory to see sequences and the context.

In Mikolov et al. (2013), authors introduce skip-gram and continuous bag-of-words (CBOW) models based on a simple single-layer architecture. They proved that even such a simple neural-network architecture can bring promising results. Huang et al. (2012) introduced a model based on neural network, which uses both local and global context via a joint training objective for modelling semantics of the words. They outperform models that use only local context on the word similarity tasks.

In other papers, words are usually regarded to as an independent entities without any relationship between morphologically related word forms. Luong et al. (2013) came with an idea to represent words as a composition of morphemes using the recursive neural network (RNN). The word semantics is than learned by neural language models (NLM).

Over the past few years, great attention has been concentrated on the exploration of semantic information in language modelling. Further, discovering latent semantic relations between words is more interesting because there are many methods that work in an unsupervised manner. These methods are usually based on assumptions introduced in the previous section (i.e. the distributional hypothesis and the bag-of-words hypothesis). In the context of this article we distinguish three directions of improvements in language modelling (using global context semantics: Section 3.1; using local context semantics: Section 3.2; and using morphological information: Section 3.3).

The use of global semantics in language modelling is motivated by the assumption that documents (long contexts) may differ in domains, topics, and writing styles. This also means that they have a different probability distribution of words. This assumption is used for adapting language models to the long context (domain, topic, style of particular documents). A method such as LSA, PLSA, or LDA is used to find out which documents (which global contexts) are similar and which are not. This long-context information is added to standard n-gram models to adapt them to the global context. The group of language models that benefits from this idea is sometimes called topic-based language models.

An important study on the application of LSA to language modelling was presented in Bellegarda (2000). Significant reductions in perplexity (down to 33%) and improvements in speech recognition of English [word error rate (WER) was decreased by 16%] were achieved in this paper. Several authors later obtained good results with PLSA (Gildea and Hofmann, 1999; Wang et al., 2003) and LDA (Tam and Schultz, 2005, 2006) approaches.

Topic tracing language models (TTLM) are investigated in Watanabe et al. (2011). These models are based on LDA and PLSA and integrate the ability to dynamically track changes in topics. The tracking is based upon focused text information and previously estimated topics. This research proves that TTLM significantly improves speech recognition of English.

The language models based on semantic composition are described in Mitchell and Lapata (2009). Word vectors are constructed from LDA (word distribution over topics) or SSM (simple semantic space based on word co-occurrence statistics). Different vector compositions are investigated to represent the history of upcoming words in the language model. Their composition models reduce the perplexity of English corpora when combined with a baseline.

The second direction is to use local context semantics for language modelling, where usually class-based language models or similar architectures are used. Individual words are clustered into much smaller number of classes, which reduces the data sparsity problem that the language models try to tackle.

One of the pilot studies in unsupervised language modelling methods was Brown et al. (1992), where class-based language models of English were introduced. The clustering algorithm builds clusters in a way that will minimize the entropy of the bigram class-based language model. This problem was reformulated so as to maximize the average mutual information between word clusters in whole training corpora [maximum mutual information (MMI) clustering]. To achieve this, classes keep the words that are most probable in the given context (one word window in both directions), which is essentially similar to the distributional hypothesis on which the methods investigated in this paper are based (words occurring in similar contexts are likely to have similar meanings). The MMI algorithm gives very satisfactory results but its computational complexity is very problematic.

This approach was later extended by Martin et al. (1998) in order to improve the complexity and to work with trigrams (not only bigrams as in Brown's MMI clustering). This clustering algorithm was also used to create class-based language models of Russian and English in Whittaker and Woodland (2003). Linear interpolation with a baseline model improved the perplexity of Russian by 23% and that of English by 7.9%. The authors also present a 2.2% relative reduction of WER in speech recognition of English.

In Deschacht et al. (2012), the Latent words language model (LWLM) was introduced. This model uses a very similar idea to the methods in Brown et al. (1992) and Martin et al. (1998), but the solutions differ. LWLM represents the word clusters as a latent variable in a graphical model and these clusters consist of the words that are most probable in the given context window. Gibbs sampling or the expectation maximization (EM) algorithm is used for inference. LWLM improved the perplexity of language modelling by 14–18% on three English corpora. LWLM groups words according to their local contexts and thus models the semantic relationships between words. LWLM provides an efficient framework that is able to work with a wider context than the MMI approach and with lower complexity.

In Brychcín and Konopík (2014) we were the first to apply semantic spaces (working with a small context window) to language models. Our clustering method based on semantic spaces build clusters of words that are similarly distributed in the corpus. We experimented with modelling of Czech, Slovak, and English and achieved perplexity reductions ranging between 10 and 18%. These language models were also able to significantly improve the BLEU score in machine translation tests.

Neural networks are becoming more attractive for language modelling in recent years (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010). They reach the state-of-the-art performance and successfully compete with n-grams. Neural networks can be designed to capture words contextual meaning which enables them to estimate similarity between words. The word sequences that have never been seen before receive high probability if they are made of words that are semantically similar to words forming an already seen sentence. Given virtually unlimited word combinations such an ability can dramatically increase model performance.

Some researchers also experiment with enrichment of neural networks with an external source of information. For example in Mikolov and Zweig (2012), an approximation of LDA topics is fed into the network input alongside with words to add global semantic information.

A third important direction for improving language modelling is to use morphological information about language. Many authors have already proved that this kind of information can be very useful for the modelling of inflectional languages, which, as stated above, are important for this paper. These approaches usually use supervised methods (lemmatization, part-of-speech tagging, etc.), but unsupervised methods of stemming also exist.

In Oikonomidis and Digalakis (2003), the authors used stems for language modelling of Greek. Class-based language models and maximum entropy language models were investigated. The authors present small but significant improvements in the WER of speech recognition.

Another approach to the modelling of Arabic was investigated in Kirchhoff et al. (2006). Several different approaches to incorporating morphological information (stems, roots, affixes, morphemes) into language models were tested, with a special focus on factored language models (FLMs) as an architecture. These models successfully improved the performance of speech recognition of Arabic.

Language modelling and speech recognition of Turkish using stems, endings, and morphemes was also investigated in Arsoy et al. (2006). The authors present significant improvements in WER by application of their combined model, which uses information about the morphology of Turkish.

In Oparin (2008), experiments with morphological random forests (using information about stems, lemmas, parts-of-speech, etc.) in the Czech and Russian languages were shown, with the conclusion that they can be used effectively for inflectional languages.

In Brychcín and Konopík (2011) we studied language modelling of Czech and Slovak. We used lemmatization and part-of-speech tagging to derive word clusters for class-based language models and achieved perplexity improvements ranging between 10 and 30% for both languages depending on the amount of training data.

We outlined three main directions (i.e. local semantics, global semantics, and morphology) in language modelling on which researchers often focus their attention. To put our research into the context of the state-of-the-art we can state that our method is based on all these sources of information. As will be shown later, these sources of information enrich each other; moreover, their combination dramatically improves language modelling.

The rest of the article is organized as follows. Section 4 describes how the latent semantics is discovered in unlabelled corpora and how it is incorporated into the language models. Our experiments are described in Section 5, which is followed by discussion of the results and the conclusion.

This section describes our language models and how these models are acquired from an unannotated corpus. The baseline is introduced in the following Section 4.1. The next sections present various sources of information, such as the morphology (Section 4.2), global semantics (Section 4.3), and local semantics (Section 4.4), which are used to improve language modelling. Finally, Section 4.5 describes how these sources of information are combined together with the baseline.

We are using the modified Kneser–Ney interpolation (introduced in Chen and Goodman, 1998), which is the state-of-the-art approach for smoothing methods. The formula for smoothing of word probabilities is


                        
                           
                              (1)
                              
                                 P
                                 (
                                 
                                    
                                       w
                                       i
                                    
                                    |
                                    
                                       w
                                       
                                          i
                                          −
                                          n
                                          +
                                          1
                                       
                                       
                                          i
                                          −
                                          1
                                       
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       cnt
                                       (
                                       
                                          
                                             w
                                             
                                                i
                                                −
                                                n
                                                +
                                                1
                                             
                                             i
                                          
                                       
                                       )
                                       −
                                       D
                                       (
                                       
                                          cnt
                                          (
                                          
                                             
                                                w
                                                
                                                   i
                                                   −
                                                   n
                                                   +
                                                   1
                                                
                                                i
                                             
                                          
                                          )
                                       
                                       )
                                    
                                    
                                       
                                          ∑
                                          
                                             
                                                w
                                                i
                                             
                                          
                                       
                                       
                                          cnt
                                          (
                                          
                                             
                                                w
                                                
                                                   i
                                                   −
                                                   n
                                                   +
                                                   1
                                                
                                                i
                                             
                                          
                                          )
                                       
                                    
                                 
                                 +
                                 γ
                                 (
                                 
                                    
                                       w
                                       
                                          i
                                          −
                                          n
                                          +
                                          1
                                       
                                       
                                          i
                                          −
                                          1
                                       
                                    
                                 
                                 )
                                 P
                                 (
                                 
                                    
                                       w
                                       i
                                    
                                    |
                                    
                                       w
                                       
                                          i
                                          −
                                          n
                                          +
                                          2
                                       
                                       
                                          i
                                          −
                                          1
                                       
                                    
                                 
                                 )
                                 ,
                              
                           
                        
                     

where P() is the probability given by the modified Kneser–Ney interpolation model and cnt() is the count of the n-gram. The goal of discounting function D(cnt) is to save some probability mass for lower-order models. The normalization function 
                           γ
                           (
                           
                              
                                 w
                                 
                                    i
                                    −
                                    n
                                    +
                                    1
                                 
                                 i
                              
                           
                           )
                           ∈
                           (
                           0
                           ,
                           1
                           )
                         makes the probability distribution sum up to 1. The definitions and derivations of these functions can be found in the original paper.

The main advantage of the modified Kneser–Ney smoothing is the clever way in which it calculates the unigram probability distribution


                        
                           
                              (2)
                              
                                 P
                                 (
                                 
                                    
                                       w
                                       i
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          N
                                          
                                             1
                                             +
                                          
                                       
                                       (
                                       
                                          •
                                          
                                             w
                                             i
                                          
                                       
                                       )
                                    
                                    
                                       
                                          N
                                          
                                             1
                                             +
                                          
                                       
                                       (
                                       
                                          •
                                          •
                                       
                                       )
                                    
                                 
                                 ,
                              
                           
                        
                     

where symbol • means an arbitrary word (class) and 
                           
                              N
                              r
                           
                           (
                           
                              w
                              
                                 i
                                 −
                                 n
                                 +
                                 1
                              
                              i
                           
                           )
                         is the number of n-grams with frequency r (i.e. the number of such n-grams, where 
                           cnt
                           (
                           
                              w
                              
                                 i
                                 −
                                 n
                                 +
                                 1
                              
                              i
                           
                           )
                           =
                           r
                        ). In other words, the unigram probability of 
                           
                              w
                              i
                           
                         is given by the number of different bigrams ending in 
                           
                              w
                              i
                           
                         divided by the total number of different bigrams.

We use HPS (see Brychcín and Konopík, 2015) as a stemming algorithm. HPS is a new unsupervised stemming algorithm that uses both lexical and semantic information about words to decide how to stem a particular word. The idea of HPS is to split the stemming into two stages. The first stage is based upon a clustering where stemming candidates are selected. The second stage uses a maximum entropy classifier with stemming-specific features that is trained on these candidates.

In our case, stemming is a mapping function 
                           
                              m
                              S
                           
                           :
                           w
                           →
                           s
                         that maps words 
                           w
                           ∈
                           W
                         to stems s
                        ∈
                        S. Note that stemming is contextually independent (in any context the same word always receives the same stem).

We suppose the stemming should be very helpful for languages with rich morphology. We use three ways of incorporating stemming information into the language modelling. The first way is to use class-based language models with stems representing classes, and the other two ways are used to improve global semantic (Section 4.3) and local semantic language models (Section 4.4).

Class-based language models are the state-of-the-art approaches to language modelling. The main task of the approach is to replace the statistical dependencies between words with dependencies among a much lower number of word classes, thus reducing the data sparsity problem. In our case the class consists of all words with the same stem.

The probability estimation of a word 
                           
                              w
                              i
                           
                         conditioned by its history 
                           
                              w
                              
                                 i
                                 −
                                 n
                                 +
                                 1
                              
                              
                                 i
                                 −
                                 1
                              
                           
                         (where n is the length of the n-gram) is given by the following formula


                        
                           
                              (3)
                              
                                 
                                    P
                                    S
                                 
                                 (
                                 
                                    
                                       w
                                       i
                                    
                                    |
                                    
                                       w
                                       
                                          i
                                          −
                                          n
                                          +
                                          1
                                       
                                       
                                          i
                                          −
                                          1
                                       
                                    
                                 
                                 )
                                 =
                                 P
                                 (
                                 
                                    
                                       w
                                       i
                                    
                                    |
                                    
                                       s
                                       i
                                    
                                 
                                 )
                                 P
                                 (
                                 
                                    
                                       s
                                       i
                                    
                                    |
                                    
                                       s
                                       
                                          i
                                          −
                                          n
                                          +
                                          1
                                       
                                       
                                          i
                                          −
                                          1
                                       
                                    
                                 
                                 )
                                 .
                              
                           
                        
                     

The probability 
                           P
                           (
                           
                              
                                 s
                                 i
                              
                              |
                              
                                 s
                                 
                                    i
                                    −
                                    n
                                    +
                                    1
                                 
                                 
                                    i
                                    −
                                    1
                                 
                              
                           
                           )
                         is calculated in the same way as in formula (1), but words are replaced with stems. To calculate the probability 
                           P
                           (
                           
                              
                                 w
                                 i
                              
                              |
                              
                                 s
                                 i
                              
                           
                           )
                        , we use Good-Touring smoothing.

For modelling global context properties we use the well-known topic model LDA (Blei et al., 2003) and our extension of LDA enriched with stem information: stem-based LDA (S-LDA).

LDA represents documents as a mixture of topics where each topic is simultaneously a mixture of words. Assume we have a set of documents 
                           D
                        
                        ={
                           D
                        
                        1, 
                           D
                        
                        2 …, 
                           D
                        
                        
                           M
                        } each containing a sequence of words. Let 
                           
                              w
                              i
                           
                         denote a word at position i in a corpus, d
                        
                           i
                         is a document index to which this word belongs and z
                        
                           i
                         is a hidden topic label for this word that we try to discover. The graphical model representation is depicted in Fig. 2
                        (a). The generative process of a word corpus in LDA is as follows:
                           
                              1
                              For each document 
                                    D
                                 
                                 
                                    m
                                 
                                 ∈
                                 
                                    D
                                 , sample a distribution 
                                    θ
                                 
                                 
                                    m
                                 
                                 ∼ Dirichlet(
                                    α
                                 ) over topics 1≤
                                 z
                                 
                                    i
                                 
                                 ≤
                                 K, where 
                                    α
                                  is a vector of hyper-parameters of Dirichlet distribution.

For each topic, sample the word distribution 
                                    ϕ
                                 
                                 
                                    k
                                 
                                 ∼ Dirichlet(
                                    β
                                 ) over words 
                                    1
                                    ≤
                                    
                                       w
                                       i
                                    
                                    ≤
                                    |
                                    W
                                    |
                                 , where 
                                    β
                                  is a vector of hyper-parameters of Dirichlet distribution.

For each position i in a corpus:
                                    
                                       (a)
                                       Sample a topic label z
                                          
                                             i
                                           from the distribution 
                                             
                                                
                                                   
                                                      θ
                                                   
                                                
                                                
                                                   
                                                      d
                                                      i
                                                   
                                                
                                             
                                          .

Sample the word 
                                             
                                                w
                                                i
                                             
                                           from the distribution 
                                             
                                                
                                                   
                                                      ϕ
                                                   
                                                
                                                
                                                   
                                                      z
                                                      i
                                                   
                                                
                                             
                                          .

For inference of topic assignments, we use Gibbs sampling, which needs to compute 
                           P
                           (
                           
                              
                                 z
                                 i
                              
                              =
                              k
                              |
                              
                                 
                                    
                                       z
                                    
                                 
                                 
                                    ¬
                                    i
                                 
                              
                              ,
                              
                                 
                                    w
                                 
                              
                              ,
                              
                                 
                                    α
                                 
                              
                              ,
                              
                                 
                                    β
                                 
                              
                           
                           )
                        , the probability of topic assignment at position i in the corpus given all other topic assignments for all words. According to Griffiths and Steyvers (2004) this leads to a simple formula


                        
                           
                              (4)
                              
                                 P
                                 (
                                 
                                    
                                       z
                                       i
                                    
                                    =
                                    k
                                    |
                                    
                                       
                                          
                                             z
                                          
                                       
                                       
                                          ¬
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          w
                                       
                                    
                                    ,
                                    
                                       
                                          α
                                       
                                    
                                    ,
                                    
                                       
                                          β
                                       
                                    
                                 
                                 )
                                 ∝
                                 
                                    
                                       
                                          cnt
                                          
                                             ¬
                                             i
                                             ,
                                             k
                                          
                                          
                                             (
                                             
                                                
                                                   w
                                                   i
                                                
                                             
                                             )
                                          
                                       
                                       +
                                       
                                          β
                                          
                                             
                                                w
                                                i
                                             
                                          
                                       
                                    
                                    
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          
                                             |
                                             W
                                             |
                                          
                                       
                                       
                                          [
                                          
                                             
                                                cnt
                                                
                                                   ¬
                                                   i
                                                   ,
                                                   k
                                                
                                                
                                                   (
                                                   j
                                                   )
                                                
                                             
                                             +
                                             
                                                β
                                                j
                                             
                                          
                                          ]
                                       
                                    
                                 
                                 ·
                                 
                                    
                                       
                                          cnt
                                          
                                             ¬
                                             i
                                             ,
                                             k
                                          
                                          
                                             (
                                             
                                                
                                                   d
                                                   i
                                                
                                             
                                             )
                                          
                                       
                                       +
                                       
                                          α
                                          k
                                       
                                    
                                    
                                       
                                          ∑
                                          
                                             l
                                             =
                                             1
                                          
                                          K
                                       
                                       
                                          [
                                          
                                             
                                                cnt
                                                
                                                   ¬
                                                   i
                                                   ,
                                                   l
                                                
                                                
                                                   (
                                                   
                                                      
                                                         d
                                                         i
                                                      
                                                   
                                                   )
                                                
                                             
                                             +
                                             
                                                α
                                                l
                                             
                                          
                                          ]
                                       
                                    
                                 
                                 ,
                              
                           
                        
                     

where 
                           
                              cnt
                              
                                 ¬
                                 i
                                 ,
                                 k
                              
                              
                                 (
                                 
                                    
                                       w
                                       i
                                    
                                 
                                 )
                              
                           
                         is the number of times the topic k has been assigned to a word 
                           
                              w
                              i
                           
                        , except the position i in the corpus. The 
                           
                              cnt
                              
                                 ¬
                                 i
                                 ,
                                 k
                              
                              
                                 (
                                 
                                    
                                       d
                                       i
                                    
                                 
                                 )
                              
                           
                         denotes the count of how many times the topic k occurs in the document d
                        
                           i
                         again except for the position i in the corpus.

From the topic assignments we can easily obtain estimates of 
                           θ
                        
                        
                           m
                         and 
                           ϕ
                        
                        
                           k
                        :


                        
                           
                              (5)
                              
                                 P
                                 (
                                 
                                    w
                                    i
                                 
                                 =
                                 j
                                 |
                                 
                                    z
                                    i
                                 
                                 =
                                 k
                                 ,
                                 
                                    
                                       β
                                    
                                 
                                 )
                                 =
                                 
                                    ϕ
                                    k
                                    
                                       (
                                       j
                                       )
                                    
                                 
                                 ≈
                                 
                                    
                                       
                                          cnt
                                          k
                                          
                                             (
                                             j
                                             )
                                          
                                       
                                       +
                                       
                                          β
                                          j
                                       
                                    
                                    
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          
                                             |
                                             W
                                             |
                                          
                                       
                                       
                                          [
                                          
                                             
                                                cnt
                                                k
                                                
                                                   (
                                                   j
                                                   )
                                                
                                             
                                             +
                                             
                                                β
                                                j
                                             
                                          
                                          ]
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (6)
                              
                                 P
                                 (
                                 
                                    z
                                    i
                                 
                                 =
                                 k
                                 |
                                 
                                    d
                                    i
                                 
                                 ,
                                 
                                    
                                       α
                                    
                                 
                                 )
                                 =
                                 
                                    θ
                                    k
                                    
                                       (
                                       
                                          d
                                          i
                                       
                                       )
                                    
                                 
                                 ≈
                                 
                                    
                                       
                                          cnt
                                          k
                                          
                                             (
                                             
                                                d
                                                i
                                             
                                             )
                                          
                                       
                                       +
                                       
                                          α
                                          k
                                       
                                    
                                    
                                       
                                          ∑
                                          
                                             l
                                             =
                                             1
                                          
                                          K
                                       
                                       
                                          [
                                          
                                             
                                                cnt
                                                l
                                                
                                                   (
                                                   
                                                      d
                                                      i
                                                   
                                                   )
                                                
                                             
                                             +
                                             
                                                α
                                                l
                                             
                                          
                                          ]
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

Finally, to derive the probability of a word 
                           
                              w
                              i
                           
                         in the context of the whole document (global context) we need to marginalize out the topic variable


                        
                           
                              (7)
                              
                                 
                                    P
                                    LDA
                                 
                                 (
                                 
                                    
                                       w
                                       i
                                    
                                    |
                                    
                                       d
                                       i
                                    
                                 
                                 )
                                 =
                                 
                                    ∑
                                    
                                       k
                                       =
                                       1
                                    
                                    K
                                 
                                 
                                    P
                                    (
                                    
                                       
                                          w
                                          i
                                       
                                       |
                                       
                                          z
                                          i
                                       
                                       =
                                       k
                                    
                                    )
                                    P
                                    (
                                    
                                       
                                          z
                                          i
                                       
                                       |
                                       
                                          d
                                          i
                                       
                                    
                                    )
                                 
                                 =
                                 
                                    ∑
                                    
                                       k
                                       =
                                       1
                                    
                                    K
                                 
                                 
                                    
                                       ϕ
                                       k
                                       
                                          (
                                          
                                             
                                                w
                                                i
                                             
                                          
                                          )
                                       
                                    
                                    
                                       θ
                                       k
                                       
                                          (
                                          
                                             d
                                             i
                                          
                                          )
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

In the second part of this section we describe our extension of LDA called S-LDA (shown in Fig. 2(b)). The generative process of S-LDA starts in the same way as the LDA does. Firstly, the topics z
                        
                           i
                         are generated. For each z
                        
                           i
                         the stem s
                        
                           i
                         is sampled from the Dirichlet distribution and finally the word form 
                           
                              w
                              i
                           
                         is selected.

In many languages, especially those with rich morphology, the suffixes contain morpho-syntactic information of the word. In topic models such as LDA based on the bag-of-words approach, the word order has no meaning and the syntactic information is inhibited. We assume the base forms of the words (approximated by stems) contain a satisfactory amount of information to infer topics. Moreover, taking these properties into account, we suppose that this model will deal with the data sparsity problem better.

Because the variables s
                        
                           i
                         and 
                           
                              w
                              i
                           
                         are both observed in a corpus and 
                           P
                           (
                           
                              w
                              i
                           
                           |
                           
                              s
                              i
                           
                           )
                         is constant during sampling z
                        
                           i
                        , the inference process is almost the same as for LDA (in formulas (4) and (5), the variables 
                           
                              w
                           
                        , 
                           
                              w
                              i
                           
                        , and W are only replaced with 
                           s
                        , s
                        
                           i
                        , and S, respectively, where 
                           s
                         denote stems on all positions and S is a set of different stems).

Finally, the unigram probability according to S-LDA is given by


                        
                           
                              (8)
                              
                                 
                                    P
                                    
                                       S
                                       −
                                       LDA
                                    
                                 
                                 (
                                 
                                    
                                       w
                                       i
                                    
                                    |
                                    
                                       d
                                       i
                                    
                                 
                                 )
                                 =
                                 P
                                 (
                                 
                                    w
                                    i
                                 
                                 |
                                 
                                    s
                                    i
                                 
                                 )
                                 
                                    ∑
                                    
                                       k
                                       =
                                       1
                                    
                                    K
                                 
                                 
                                    P
                                    (
                                    
                                       
                                          s
                                          i
                                       
                                       |
                                       
                                          z
                                          i
                                       
                                       =
                                       k
                                    
                                    )
                                    P
                                    (
                                    
                                       
                                          z
                                          i
                                       
                                       |
                                       
                                          d
                                          i
                                       
                                    
                                    )
                                 
                                 =
                                 P
                                 (
                                 
                                    w
                                    i
                                 
                                 |
                                 
                                    s
                                    i
                                 
                                 )
                                 
                                    ∑
                                    
                                       k
                                       =
                                       1
                                    
                                    K
                                 
                                 
                                    
                                       ϕ
                                       k
                                       
                                          (
                                          
                                             
                                                s
                                                i
                                             
                                          
                                          )
                                       
                                    
                                    
                                       θ
                                       k
                                       
                                          (
                                          
                                             d
                                             i
                                          
                                          )
                                       
                                    
                                 
                                 ,
                              
                           
                        
                     

where 
                           P
                           (
                           
                              w
                              i
                           
                           |
                           
                              s
                              i
                           
                           )
                         is calculated in the same way as in Section 4.2.

According to our previous research (Brychcín and Konopík, 2014), the well-performing semantic spaces in language modelling were discovered to be:
                           
                              •
                              Hyperspace analogue to language (HAL) (Lund and Burgess, 1996).

Correlated occurrence analogue to lexical semantic (COALS) (Rohde et al., 2004).

Random indexing (RI) (Sahlgren, 2005).

Since words in these semantic spaces are represented as real-valued vectors, we can apply clustering methods. The main assumption is that words within the same cluster should be semantically substitutable (i.e. they make sense at the same position in the appropriate context).

The selection of a suitable clustering algorithm is crucial for such a task. According to the study in Zhao and Karypis (2002), we selected the repeated bisection algorithm because of its efficiency and acceptable computational requirements. We use the implementation from the CLUTO software package (Karypis, 2003). As a measure of the similarity between two words, we use the cosine similarity of word vectors, calculated as the cosine of the angle between corresponding vectors.

Since we already have word clusters, we can easily define the mapping function 
                           m
                           :
                           w
                           →
                           c
                        , where 
                           w
                           ∈
                           W
                         denotes a word and c
                        ∈
                        C denotes a word cluster. Class-based language models seem to be a suitable architecture for applying this kind of information to the language models.


                        
                           
                              (9)
                              
                                 P
                                 (
                                 
                                    
                                       w
                                       i
                                    
                                    |
                                    
                                       c
                                       
                                          i
                                          −
                                          n
                                          +
                                          1
                                       
                                       
                                          i
                                          −
                                          1
                                       
                                    
                                 
                                 )
                                 =
                                 P
                                 (
                                 
                                    
                                       w
                                       i
                                    
                                    |
                                    
                                       c
                                       i
                                    
                                 
                                 )
                                 P
                                 (
                                 
                                    
                                       c
                                       i
                                    
                                    |
                                    
                                       c
                                       
                                          i
                                          −
                                          n
                                          +
                                          1
                                       
                                       
                                          i
                                          −
                                          1
                                       
                                    
                                 
                                 )
                                 .
                              
                           
                        
                     

The class (cluster) at position i is given by 
                           
                              c
                              i
                           
                           =
                           m
                           (
                           
                              w
                              i
                           
                           )
                        . The probability 
                           P
                           (
                           
                              
                                 c
                                 i
                              
                              |
                              
                                 c
                                 
                                    i
                                    −
                                    n
                                    +
                                    1
                                 
                                 
                                    i
                                    −
                                    1
                                 
                              
                           
                           )
                         is calculated in the same way as in formula (1), but words are replaced with word classes. To calculate the probability 
                           P
                           (
                           
                              
                                 w
                                 i
                              
                              |
                              
                                 c
                                 i
                              
                           
                           )
                         we use Good-Touring smoothing. The mapping function for particular semantic spaces will be denoted as m
                        HAL, m
                        COALS, and m
                        RI.

Similar to the previous section, we also extend these local semantic models with stemming information. During the building of semantic space we can simply use stems instead of word forms and the mapping function becomes 
                           m
                           :
                           w
                           →
                           s
                           →
                           c
                        , where 
                           w
                           ∈
                           W
                        , s
                        ∈
                        S, and c
                        ∈
                        C and everything else remain unchanged. The mapping functions using semantic spaces together with stemming will be denoted as m
                        
                           S−HAL
                        , m
                        
                           S−COALS
                        , and m
                        
                           S−RI
                        .

In this paper we work with two ways of combining language models: linear interpolation (see Section 4.5.1) and its extension, bucketed linear interpolation (see Section 4.5.2).

As in our previous works we use a simple but very effective linear interpolation to combine different language models


                           
                              
                                 (10)
                                 
                                    
                                       P
                                       LI
                                    
                                    (
                                    
                                       
                                          w
                                          i
                                       
                                       |
                                       
                                          w
                                          
                                             i
                                             −
                                             n
                                             +
                                             1
                                          
                                          
                                             i
                                             −
                                             1
                                          
                                       
                                    
                                    )
                                    =
                                    
                                       ∑
                                       
                                          k
                                          =
                                          1
                                       
                                       K
                                    
                                    
                                       
                                          λ
                                          k
                                       
                                       ·
                                       
                                          P
                                          k
                                       
                                       (
                                       
                                          
                                             w
                                             i
                                          
                                          |
                                          
                                             w
                                             
                                                i
                                                −
                                                n
                                                +
                                                1
                                             
                                             
                                                i
                                                −
                                                1
                                             
                                          
                                       
                                       )
                                    
                                    ,
                                 
                              
                           
                        

where λ
                           
                              k
                            is the weight of the kth language model P
                           
                              k
                           (). We use the expectation maximization (EM) algorithm described in Dempster et al. (1977) to calculate optimal weights λ
                           
                              k
                           , which maximizes the likelihood of the held-out data. Using linear interpolation it is quite straightforward to combine different sources of information such as our global and local context language models.

The linear interpolation can be extended to a method called bucketed linear interpolation, where weights become the function of the frequency of word history (Bahl et al., 1983). The main idea is that the weights λ
                           
                              k
                            should be different for words with histories of varying frequencies. For example we expect that the word n-gram language model would produce the best probability estimates (receive the highest weight) for very often frequented histories. The formula of linear interpolation is transformed to


                           
                              
                                 (11)
                                 
                                    
                                       P
                                       BLI
                                    
                                    (
                                    
                                       
                                          w
                                          i
                                       
                                       |
                                       
                                          w
                                          
                                             i
                                             −
                                             n
                                             +
                                             1
                                          
                                          
                                             i
                                             −
                                             1
                                          
                                       
                                    
                                    )
                                    =
                                    
                                       ∑
                                       
                                          k
                                          =
                                          1
                                       
                                       K
                                    
                                    
                                       
                                          λ
                                          k
                                       
                                       (
                                       
                                          
                                             w
                                             
                                                i
                                                −
                                                n
                                                +
                                                1
                                             
                                             
                                                i
                                                −
                                                1
                                             
                                          
                                       
                                       )
                                       ·
                                       
                                          P
                                          k
                                       
                                       (
                                       
                                          
                                             w
                                             i
                                          
                                          |
                                          
                                             w
                                             
                                                i
                                                −
                                                n
                                                +
                                                1
                                             
                                             
                                                i
                                                −
                                                1
                                             
                                          
                                       
                                       )
                                    
                                    .
                                 
                              
                           
                        

The weights λ
                           
                              k
                           () certainly cannot be different for each possible frequency of history because of data sparsity. Instead, the whole frequency spectrum is divided into buckets, where each bucket holds some range of frequencies. Histories with frequencies in the same bucket receive the same weights. The number of buckets can be tuned but it generally depends on the amount of training data available. The more training data are available, the more buckets can be used.

In our previous research (Brychcín and Konopík, 2014), it was shown that bucketed linear interpolation produces slightly better results compared to simple linear interpolation when combining several language models.

@&#EXPERIMENTAL RESULTS@&#

In this section we describe various results of our experiments in detail. Firstly, the corpora we use in our experiments are introduced in Section 5.1. Perplexity results, as the most commonly used metric for language models, are shown in the next section, 5.2. Finally, machine translation tests are shown in Section 5.3. We use six languages for our experiments: Czech (CZ), Slovenian (SL), Slovak (SK), Polish (PL), Hungarian (HU), and English (EN).

In all language models, the vocabulary consists of words occurring at least five times in the training data (and is kept fixed for all tests) and n-grams are smoothed by modified Kneser–Ney interpolation. These language models will be denoted as follows in our experiments:
                        
                           •
                           Word and stem-based language models:
                                 
                                    –
                                    
                                       BL: Four-gram word-based language model (baseline).


                                       HPS: Four-gram class-based language model, where classes are stems given by HPS.

Global semantics language models:
                                 
                                    –
                                    
                                       LDA: Global semantic language models using LDA.


                                       S-LDA: Global semantic language models using a stemmed version of LDA.

Local semantics language models:
                                 
                                    –
                                    
                                       HAL: Four-gram class-based language model, where classes are given by clustering HAL.


                                       S-HAL: Four-gram class-based language model, where classes are given by clustering HAL preprocessed by HPS.


                                       COALS: Four-gram class-based language model, where classes are given by clustering COALS.


                                       S-COALS: Four-gram class-based language model, where classes are given by clustering COALS preprocessed by HPS.


                                       RI: Four-gram class-based language model, where classes are given by clustering RI.


                                       S-RI: Four-gram class-based language model, where classes are given by clustering RI preprocessed by HPS.

LDA as well as the semantic spaces (HAL, COALS, and RI as well as their stemmed versions) works with the same vocabulary as the language models do (words occurring at least five times in the training part of the corpus). We use LDA implementation from the MALLET (McCallum, 2002) software package. For each experiment we always train the LDA with 1000 iterations of Gibbs sampling. The hyperparameters of Dirichlet distributions were initially set to α
                     =50/K, where K is the number of topics and β
                     =0.1. This setting is recommended by Griffiths and Steyvers (2004). The number of topics was ranging between 20 and 1000 to find an optimal configuration for each language (as shown later in experiments, 400 topics LDA performed best among tested languages).

Implementation of the HAL, COALS, and RI algorithms is available in an open source package S-Space (Jurgens and Stevens, 2010). The parameters of these semantic spaces are set as follows. For each semantic space we use a four-word context window (in both directions). HAL uses a matrix consisting of 50,000 columns, which keeps the largest amount of information. COALS uses a matrix with only 14,000 columns (as recommended by the authors of the algorithm). The SVD reduction was not used in our experiments (according to our previous research (Brychcín and Konopík, 2014), COALS with SVD reduction performed worse). RI uses vectors with a dimension of 1024.

For clustering of words, the repeated bisection algorithm with the cosine similarity metric is used. We take the implementation from the CLUTO software package (Karypis, 2003). For all semantic spaces the word vectors are clustered into four different numbers of clusters: 1000, 5000, 10,000, and 20,000. The use of more clusters is meaningless as they will be too sparse and too close to the baseline word model (as is clear from statistics on corpora in Section 5.1). From these clusters, appropriate class-based language models are constructed.

For stemming we use our own implementation of HPS available at http://liks.fav.zcu.cz/HPS. Already trained stemming models for languages used in this paper are also publicly available there.

We use 50 buckets in linear interpolation for combining our language models.

In our experiments we use the Europarl
                           2
                        
                        
                           2
                           Available at http://www.statmt.org/europarl.
                         corpora, version 7, provided by Koehn (2005). These corpora consist of parallel texts in many languages extracted from the proceedings of the European parliament. The texts of each language are aligned with the text in English on sentence level.

We chose to use these corpora for two reasons. Besides the perplexities, we test our language models in a machine translation task (where the parallel corpora are needed to train and evaluate the machine translation system). The second reason is that the Europarl corpora contain texts within the same domain (the same texts on the same topics) in several languages, enabling us to make comparisons of our models among different languages.

Corpora are tokenized with our simple language-independent tokenizer based upon regular expressions. The casing of all word tokens in corpora is normalized using the true casing method available within the Moses framework (see Section 5.3). The true casing method uses casing statistics collected from a raw text corpus. The statistics contain information about the most frequent casing of all words.

Parallel corpora are used for training machine translation systems (more information is given in Section 5.3). Some statistics on these parallel corpora are shown in Table 1
                        .

Texts available in the Europarl corpus are not divided into documents that are required for the training of our global context language models based on LDA. There are, however, publicly available source texts of Europarl corpora that are not already mutually aligned on sentence level (monolingual texts). Fortunately, these source texts are annotated with the tag SPEAKER, which indicates particular speakers of the texts (the text is about one topic just discussed in parliament). We assume that texts spoken by one speaker at one moment can be taken as documents. To find the boundaries of the documents we trace the points of change of the SPEAKER tag. Every change introduces a new document. We use these documents from these monolingual corpora for training all our language models. Statistics on these monolingual corpora are shown in Table 2
                        . We can see that there are more tokens for all languages, especially for English.

Both parallel and monolingual corpora are split into the training, development (held-out), and test sets in proportions of approximately 70, 10, and 20%, respectively. The held-out set and the test set for both types of corpora are chosen in such a way they contain the same sentences and these sentences are previously unseen for language models as well as for the machine translation model.

This section presents various experiments with our language models using perplexity as an evaluation measure. Perplexity is the most often used measure of the quality of a language model. The perplexities in the tables below are always calculated on the test part of appropriate corpora (see Section 5.1 about corpora), which is previously unseen for language models.

Baseline perplexities (BL) are shown in all tables in this section. Numbers in brackets shown on the right of perplexities of our models denote the relative improvements in perplexity compared to the baseline. We rely on these numbers to be more important than the perplexities themselves. Bold numbers represent the best results for the current language. If it is not written explicitly, then the combination of models is always done by linear interpolation (for more information see Section 4.5.1).

Our first way of incorporating morphological information (stemming) into language models is to use stem-based language models (class-based languages model with stems used as classes) and to interpolate them with baseline models. The perplexities are shown in Table 3
                        . We can see that there are significant improvements for all languages except English, even with this simple approach. The stem-based model performs similarly for all Slavic languages, that is, for Czech, Slovenian, Slovak, and Polish. For Hungarian, the representative of Uralic languages, the improvements given by stemming are more significant. The almost complete lack of improvement for English as a representative of Germanic languages is not surprising, as word normalization (stemming) is meaningless for languages with almost no inflection.


                        Local semantics language models are investigated in the next group of experiments, where we use semantic spaces (HAL, COALS, and RI) together with their stemmed versions (S-HAL, S-COALS, and S-RI) clustered into four different depths (1000, 5000, 10,000, and 20,000 clusters). Class-based language models given by appropriate clusters are always interpolated with a baseline. Results are shown in Table 4
                        . The last column (where a combination of all class-based language models is shown) in the table is especially interesting. We can see that the semantic spaces clustered into different depths enrich each other and are able to improve language models more than interpolations with only a stand-alone class-based language model.

For all languages, the best semantic space for 1000, 5000, and 10,000 clusters, respectively, is conclusively HAL without stemming. In the case of 20,000 clusters, S-HAL performed better for all languages except Hungarian (HAL performed best) and English (the RI model performed best, which we suppose is due to chance rather than its properties). The stemmed version of semantic spaces worked well only in the case of COALS, where S-COALS was almost always better, and also for sparse clusters (20,000 clusters) where the stemmed model was almost always better than the unstemmed one. From these results we can state that the HAL model is most suitable for language modelling of all languages. Perplexity improvements by HAL (baseline interpolated with all four class-based language models created from HAL) are 13% on average for inflectional languages. For English, the improvement is not as big (approximately 7%).


                        Global semantic language models are shown in Table 5
                        . The word unigram probability given by the LDA and S-LDA models with the number of topics ranging between 20 and 1000 is interpolated with the baseline model. We can see similar improvements compared to baseline for all six languages including English. In the case of LDA the best results are achieved with 300 topics on average. LDA can be trained well on English corpora even for higher a number of topics (because of lower significance of data sparsity). We can see that S-LDA models always performed better than unstemmed versions. Moreover, thanks to stemming, it is possible to infer the more distinct latent topics (as was expected mainly for inflectional languages). S-LDA performs almost 3% better than the LDA model in the modelling of inflectional languages. For English, both models are similar. By comparison with local semantics, the global semantics improves the language modelling slightly more (up to 16%).

The most important experiments are shown in Table 6
                        , where a combination of different sources of information is depicted. As the best-performing model for local semantics, the HAL model was chosen (a combination of HAL-based language models of 1000, 5000, 10,000, and 20,000 clusters). Global semantics in language models was best modelled by S-LDA with 400 topics. The stem-based model (HPS) is also added together with the baseline (BL) and the final model is thus a combination of all seven language models (the last two columns in the table, where we finally compare linear interpolation with a bucketed linear interpolation).

From the table we can clearly state that all three sources of information (morphology, local semantics, and global semantics) significantly enrich each other (which is especially evident for inflectional languages). Their bucketed linear combination leads to improvements of up to almost 26% for inflectional languages and up to 15% for English compared to the stand-alone baseline. Our language models perform similarly for all inflectional languages (improvements by each part are quite similar). We can also see that the bucketed linear interpolation (BLI-all) produces slightly better results than simple linear interpolation (LI-all).

In order to visualize what weights in the linear interpolation (LI-all model) are allocated for each sub-model by the EM algorithm, we render Fig. 3
                        , where our final model created from seven sub-models is shown. We can see that the baseline always has the highest weight, and then, in order of decreasing weight, S-LDA, HPS, HAL{20k}, HAL{10k}, HAL{5k} and HAL{1k} follow, where the numbers in brackets mean the numbers of clusters. A direct correlation between weights and perplexity improvements can be seen. The bigger the weights allocated by the EM algorithm, the greater the improvement in perplexity achieved.

This section describes the performance of the proposed language models in terms of a machine translation task. Success in this task should verify the ability of the models to improve the performance of a real-world application. The system used in this test is based upon the statistical machine translation toolkit called Moses
                           3
                        
                        
                           3
                           Available at http://www.statmt.org/moses/.
                        , briefly described in Koehn et al. (2007).

Europarl parallel corpora were used for training and evaluation of machine translation (see Section 5.1). Table 7
                         shows the settings for translation model training.

Our translation experiment consists in measuring the difference in translation performance when the standard four-gram language model is used and when our improved language model is employed. Language models are used during training as well as during decoding in Moses. The best approach would be to replace the standard model with our model. However, our model does not support left-to-right decoding because global semantic models require knowledge of word contexts. This prevents the use of our model for training as well as directly for decoding. Instead, we generate 5000 best hypotheses and re-score them with our model. Such a procedure is not as effective as direct incorporation of the model into Moses; however, it is the only possible approach.

The change of the language model in Moses is not possible without re-computing the model weights. Moses uses weights for translation, reordering, word penalty, and language models. The weights are set during the optimization phase by the minimum error rate training (MERT) algorithm (Och, 2003). The algorithm optimizes the weights for best translation scores on the held-out data. However, such weights are valid only for the original model. A different model returns different probability estimates and thus they play different roles during interpolation of the final translation probability.

We estimate the correct weight for our improved language model with the same algorithm, MERT. We use the n-best list generated during the optimization phase. We replace the probability estimates of the original language model with the estimates from our model. Then, we run the optimization procedure, which returns news weights including the weight for our improved language model. These new weights are used for translating test sentences.

The results of our translation experiment are shown in Table 8
                        . We measure the translation scores with the BLEU metric (Papineni et al., 2002). This metric is based on the ratio of n-gram overlaps with reference translations. We compare original translations from Moses with translations obtained by re-scoring them with our language model. We generate and re-score 5000 hypotheses for each translated sentence. The most probable hypothesis is taken as the translation result. Beside showing scores for the whole language model composed of all sub-models (local, global semantics, and stemming), we also study the performance of language models composed of various combinations of sub-models. Numbers in brackets denote the absolute improvements in BLEU score.

We can see that each of the three sources of information (morphology, local semantics, and global semantics) gives at least some improvement. Their combination again enriches each of them even in machine translation tests. The highest improvements are always achieved by HAL (combination of 1k, 5k, 10k, and 20k clusters), which is the best representative of local semantics models. HPS and S-LDA provide similar improvements. Stemming is again useless for English. The fact that S-LDA performs worse than HAL is probably caused by working on the sentence level (not document level), on which Moses is focused. If the S-LDA model had wider context (i.e. the whole document) it would probably perform better as was shown in Section 5.2. The combination of all sources of information (the last column in the table) studied in this paper leads to signification improvements in BLEU score, which is a solid proof that the proposed language models are usable and effective in a real-world application.

@&#DISCUSSION@&#

In this section we summarize our results and discuss the behaviour of our methods. In previous sections we presented various experiments on our language models. We experimented with a combination of three different sources of information (morphology, local semantics, and global semantics). Firstly, we tested language models from the theory of information point of view, where the perplexity was used as an evaluation measure. The results of these tests were followed by evaluation in a real-world application, where machine translation with the Moses system proved the quality of our methods.

First, let us look at the perplexity results. Stem-based language models (HPS) proved to be efficient for inflectional languages. The average relative improvement in perplexity for Slavic languages (Czech, Slovenian, Slovak, and Polish) was about 7%; for Hungarian it was as much as 12%. As we expected, stemming was found to be useless for English, with almost no improvement in perplexity.

Semantic spaces (HAL, COALS, and RI) and their stemmed versions (S-HAL, S-COALS, and S-RI) were explored as a next step, where we created class-based language models according to different numbers of clusters (i.e. 1000, 5000, 10,000, and 20,000). The HAL model performed best in the majority of experiments. It was found to be better not to use stemming as the results of stemmed versions of semantic spaces were almost always worse than those of unstemmed models. Stemming was often better only in cases where sparse clusters (20,000) were used. We suppose this is caused by the nature of local semantic models, which thanks to the short context, also incorporate morpho-syntactic information of words that is very useful especially in language models. In contrast, stemming inhibits this kind of information. The combination of class-based language models of different numbers of clusters was shown to improve language modelling by approximately 13% on average for inflectional languages and by 7% for English.

In our previous research (Brychcín and Konopík, 2014), HAL was also best for 1000, 5000, or 10,000 clusters, but the COALS model performed better than HAL in the case of 20,000 or more clusters. Our current results confirm that for inflectional languages the difference between HAL and COALS is lower with a growing number of clusters; moreover in the case of 20,000 clusters, the performance of COALS was slightly better. In the cases of Hungarian and English, HAL performed best all the time.

The third source of information was global semantics modelled by LDA and S-LDA (the stemmed version of LDA). LDA has already been proven by many researchers to be useful in language modelling (see, e.g., Tam and Schultz, 2005, 2006; Watanabe et al., 2011) and our results agree with that. LDA improved the perplexity of models of all languages by about 13% on average. Moreover, our stemmed extension of LDA was able to achieve even better results for inflectional languages (improvements of up to 16%). Here, the situation is opposite to that of local semantics. LDA is based on bag-of-word hypotheses, where the word order is inhibited (the morpho-syntactic information is useless for topic inference). Stemming thus helps to deal with the data sparsity problem better.

The most important finding, which is also the aim of this paper, is that the investigated sources of information mutually help each other in terms of improving language modelling. Their combination was able to dramatically reduce the perplexities of language models. By grouping HPS, HAL, and S-LDA with a baseline we achieved approximately 25% improvement on average for all inflectional languages, which is a very satisfactory result. The improvement for English was not as big (approximately 15%). Our explanation lies in the morphological complexity of languages. As expected, the word normalization was negligible for English. Also, the semantic spaces working with short context incorporate morpho-syntactic information of words. We believe that this is the main reason why they are more suitable for inflectional languages. Thus, the language modelling of English profits mainly from LDA model.

The same model combinations were also investigated in a machine translation task, where we measured BLEU scores. Only by changing the language model was significant growth of the BLEU score achieved (the average improvement among all languages was 0.8 BLEU points). If we compare our results with different works on the same corpora (e.g. works of Virpioja et al., 2007; Sanchis-trilles et al., 2010), we can claim that our models are very effective.

The results from machine translation correlate with the perplexity results but there are some deviations in improvements of inflectional languages even though the improvements in perplexities were almost identical. The performance of the machine translation system depends on several modules (not only on the language model) as well as on the optimization of parameters on held-out data (using MERT). The difficulty of the language must also be taken into account, as we observe proportionally different perplexities and different BLEU scores among all languages despite using the corpora within the same domain.

The fact that we significantly improved the modelling of several languages with different properties (different families of languages) testifies to the quality of our methods. Moreover, these methods are attractive due to their unsupervised nature and so they can be easily applied to other languages or tasks.

@&#SUMMARY@&#

@&#FUTURE WORK@&#

As shown in Section 2, there is a huge number of methods for latent semantics discovery that are worth investigating. These methods use various architectures, e.g. matrix factorization methods, graphical models, or neural networks. It is beyond the scope of this paper to compare them all. This is the main direction for the future work as we believe that other combinations may produce even better language models.

Another alternative for future work consists in studying machine translation more deeply. The experiments with the Moses machine translation framework revealed that it is an extraordinary framework with great extension capabilities. We expect that through the direct link between Moses and the methods investigated in this paper (local and global semantics or stemming), even higher improvements could be achieved. Moses supports several architectures for applying various sources of information. For example clusters given by semantic spaces, stems, or topics can be directly used in factored translation.

In addition we also want to test our methods in different NLP tasks (e.g. speech recognition, optical character recognition, spelling correction, etc.).

@&#CONCLUSION@&#

In this article we experimented with three kinds of various information sources whose application into language modelling yields significant improvements in model prediction ability. The beauty of our method is that all the information comes directly from the data. Nothing is added externally. The information we use is sometimes called latent or hidden because an algorithm must be employed to discover it. We use tree approaches for the discovery of hidden information. Topic models discover long semantic relations between words and the documents where they are used. Semantic spaces (HAL, COALS, RI) work with short semantic relations between words and their direct contexts. The stemming studies the information hidden directly in different forms of words.

All these sources were previously used in a research (semantic spaces in our preceding work). However, nobody had studied whether their combination carries some extra information. We conclusively proved that the combination provides much higher perplexity reduction than individual models do.

Our stemming algorithm (HPS) proved to be very useful in language modelling of inflectional languages. The use of stems as classes for class-based language models or extending LDA with stem information was shown to be very effective, as we achieved significant improvements in language modelling. Taking into account the results and our findings during testing, we can recommend HAL for modelling local semantics and S-LDA for modelling global semantics.

We believe that our article carries a potential beyond language models. All the methods investigated in this paper are based upon unsupervised training. We successfully used some of these methods in different NLP applications such as sentiment analysis (Habernal and Brychcín, 2013; Brychcín et al., 2014), document classification (Brychcín and Král, 2014), and named entity recognition (Konkol et al., 2014).

@&#ACKNOWLEDGEMENTS@&#

This work was supported by Grant No. SGS-2013-029 Advanced computing and information systems, by the European Regional Development Fund (ERDF) and by project “NTIS – New Technologies for Information Society”, European Centre of Excellence, CZ.1.05/1.1.00/02.0090. Access to the MetaCentrum computing facilities provided under the program “Projects of Large Infrastructure for Research, Development, and Innovations” LM2010015, funded by the Ministry of Education, Youth, and Sports of the Czech Republic, is highly appreciated. The access to the CERIT-SC computing and storage facilities provided under the programme Center CERIT Scientific Cloud, part of the Operational Program Research and Development for Innovations, reg. no. CZ. 1.05/3.2.00/08.0144 is acknowledged.

@&#REFERENCES@&#

