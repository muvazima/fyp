@&#MAIN-TITLE@&#ECG biometric authentication based on non-fiducial approach using kernel methods

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A new design of experimental evaluation setup for identity recognition is concerned.


                        
                        
                           
                           The use of mother wavelet functions is extended for denoising ECG signals.


                        
                        
                           
                           A novel non-fiducial verification framework is modelled by kernel-based methods.


                        
                        
                           
                           The effect of different feature extractor models is examined by recognition rates.


                        
                        
                           
                           Window and subject recognition rates are tested on random unknown data sets.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

ECG biometric recognition

Non-fiducial feature extraction

Kernel methods

Dimensionality reduction

Gaussian OAA SVM

@&#ABSTRACT@&#


               
               
                  Identity recognition faces several challenges especially in extracting an individual's unique features from biometric modalities and pattern classifications. Electrocardiogram (ECG) waveforms, for instance, have unique identity properties for human recognition, and their signals are not periodic. At present, in order to generate a significant ECG feature set, non-fiducial methodologies based on an autocorrelation (AC) in conjunction with linear dimension reduction methods are used. This paper proposes a new non-fiducial framework for ECG biometric verification using kernel methods to reduce both high autocorrelation vectors' dimensionality and recognition system after denoising signals of 52 subjects with Discrete Wavelet Transform (DWT). The effects of different dimensionality reduction techniques for use in feature extraction were investigated to evaluate verification performance rates of a multi-class Support Vector Machine (SVM) with the One-Against-All (OAA) approach. The experimental results demonstrated higher test recognition rates of Gaussian OAA SVMs on random unknown ECG data sets with the use of the Kernel Principal Component Analysis (KPCA) as compared to the use of the Linear Discriminant Analysis (LDA) and Principal Component Analysis (PCA).
               
            

@&#INTRODUCTION@&#

Biometric recognition technology provides a reliable security system through identity verification and recognition of individuals based on their inherent characteristics in both modes of identification and verification or authentication [1]. The human features can include physiological or behavioural traits. Physiological features include face, iris, and fingerprints whereas keystroke dynamics, gait and voice/speech are behavioural features. However, these modalities are not robust enough against falsification. Some instances of attacks in the biometrics security system involve the use of latex for recreation of a fingerprint, voice imitation and the application of contact lenses copied from original iris features [2].

A new generation of biometric identity recognition modalities has been introduced extensively during the last decades. It includes biosignals which are typically utilised for clinical diagnostic purposes such as Electrocardiogram (ECG) and many others [3,4]. ECG is a non-invasive diagnostic method which is effective, simple and involves a low-cost procedure to gather information on structural and functional heart muscle activity and other body tissues over time. Each individual has unique ECG characteristic signals such as universality, uniqueness and liveness detection and thus, offer robustness against falsification using fraudulent techniques [5–7]. However, despite these benefits, the technique encounters a number of difficulties due to time-varying nature of the ECG signals, irregular conditions of cardiac disorders, and the long waiting period to collect the ECG data [4,7,8]. Nevertheless, the ECG biometric technology can be widely applied in government and commercial environments, secure public and travel documents, health monitoring systems, forensic systems (including identifying criminals and law enforcement) [9] and distributed systems such as smart cards and many others [2,10,11].

Performance is the most important and significant criteria to evaluate a biometric recognition system [12]. As the ECG is not periodic and highly repetitive signal [2], the ECG biometric systems encounter a number of difficulties in building an accurate identity recognition system. The challenges occur mainly in feature extraction from ECG signals and so in biometric recognitions [4,11,12]. The statistical methods, machine learning and data mining techniques are suitable for dealing with feature extraction learning and pattern classification (recognition) problems and overcome overfitting and improve generalisation property in numerous real-world and biometric applications [13].

A typical normal heartbeat of ECG wave consists of three main components: P wave, QRS complex and T wave. Fiducial features are amplitudes of P, R and T waves, the temporal distance between wave boundaries (onset and offset of the P, Q, R, S and T waves), the area of the waves, and slope information [11]. The ECG biometric systems based on fiducial detection rely entirely on the localisation of the points in a heartbeat cycle. The fiducial feature extraction method proposed by Biel et al. [3] is among the earliest research in the use of ECG for human identification. The research has showed the impact of a number of features on misclassification and personal identification that used only one lead ECGs. The identification performance of the system was 100% over 20 subjects. Another study based on fiducial features by Israel et al. [14] has proven the effects of an individual's mental stress, and the placement of ECG leads to identification performance. The system achieved 100% subject recognition and 81% heartbeat recognition rate for a population of 29 individuals. Most of the proposed methods in fiducial-based ECG biometric have employed the signals that were acquired using standard leads on the chest and limbs using clinical grade equipment (clinical source data). [15–17] have conducted experiments on standard one-lead ECG signals over a different number of healthy subjects including 25, 76, and 51, respectively. The identification accuracies of these systems were respectively obtained 99, 98, and 99.85%, and equal error rates (EERs) of 1.88% [16] and 0.01% [17] in authentication mode. On the other hand, some studies have proposed fiducial based methodologies which extracted features from characteristic points on ECG recordings acquired at the hands and/or fingers in an off-the-person approach (non-clinical source data) [18–22]. The ECG signals captured from fingers and hands are noisier than the standard ECG signals acquired on the chest using clinical grade equipment, so it leads to make subsequence processing more difficult [18]. However, the fiducial-based algorithms mentioned in the above study and in many other studies have faced the difficulty of missing a number of efficient ECG morphological attributes [4]. In addition, there is no universally acknowledged rule that is able to detect the boundary of fiducial points, so complexity of biometric system is overall increased [7,23].

Many researches based on a non-fiducial approach have been carried out to overcome the fiducial based problems since 2006. The most important advantage of non-fiducial approach over the fiducial ones is in its simplicity of computation as it does not have to determine wave boundaries and fiducial points' detection. Indeed, non-fiducial based techniques extract discriminative information within the ECG waveform without having any particular reference points in heart beat cycles as features. Some methodologies have been presented different types of non-fiducial based features like a sequence of symbols [17], wavelet coefficients [24], and autocorrelation coefficients [23]. In [17], non-fiducial based features were extracted by simple analysis of quantisation to convert a set of ECG waveforms into a set of strings. The identification rate of the system was reported as 99.39% and equal error rate of 0.13% in authentication, tested over 51 healthy subjects. Chan et al. [24] has proposed a human identification system based on collecting ECG signals from fingers across multiple sessions. The Wavelet Distance (WDIST) was used to measure a distance between wavelet coefficients. The system achieved 89% identification rate on a dataset of 50 subjects. Plataniotis et al. [23] have suggested a new and robust approach for feature extraction that relies on a combination of Autocorrelation (AC) and Discrete Cosine Transform (DCT) of the overall morphology of ECG waveform. The DCT has been applied to the autocorrelation (AC) sequence of windowed ECG data segments for dimension reduction in the non-fiducial approach. The identification performance was 97.8% as tested over standard one-lead ECG signals from 13 subjects. However, the non-fiducial approach based on autocorrelation involves the challenge of high-dimensional data and so analysis or processing is a computationally demanding task, referred to in the literature as the curse of dimensionality [4]. The undesirable template size can have an impact on overall recognition, and it is not able to predict accurately [12]. Thus, it is important to handle data adequately through the construction of a meaningful low-dimensional space form a high-dimensional observation space [25]. Dimensionality reduction can be an effective solution to prevent the curse of dimensionality and enhances the efficiency of learning methods by extracting a number of relevant and significant features often using linear and nonlinear dimensionality reduction techniques [26]. Agrafioti and Hatzinakos [27] dealt with the problem of high-dimensional data in this non-fiducial approach with the use of a combination of AC with Linear Discriminant Analysis (LDA) for human identification system based on fusion of all of the 12 ECG leads' data. The AC/LDA algorithm has been also utilised in studies of [4] and [28] in order to extract discriminative features from one-lead ECG signals collected based on two-session and single-session data (non-clinical source data), respectively. LDA is a linear supervised learning method which has been extensively used in dimension reduction and classification areas for higher inter-class variability [29] in order to control misclassification errors. The linear dimension reduction methods cannot find adequate structures in real-world data that are nonlinear. In recent years, many different techniques have been broadly developed to solve the nonlinear dimensionality reduction problem by using the basic idea of kernel-based learning methods [25,30] such as the Kernel Principal Component Analysis (KPCA) [31]. The main concept of kernel functions is implicit nonlinear mapping of observations in the input space into higher dimensional feature space to be optimally separate input patterns [32]. To the best of our knowledge, there are no reports in the literature about the non-fiducial approach based on an autocorrelation in conjunction with nonlinear dimension reduction methods over non-clinical source data.

The biometric recognition systems in identification and verification modes are classifiers and they are considered respectively as one-to-many and one-to-one problems. The identification systems recognise a subject by comparing an individual's own biometric properties with all enrolled templates in a database to find the highest probability match. The verification systems will then indicate the individual's claimed identity as either accepted or rejected [1]. One verification approach is to compare the probabilities estimate during identification against a threshold to obtain verification decisions. Another approach is to perform a binary classification problem to achieve a probabilistic learning model when the number of individuals in the systems is large [1,33]. The biometric recognition performance is assessed by measuring two misclassification error rates of False Match Rate (FMR) and False Non-match Rate (FNMR) that are respectively termed as False Acceptance Rate (FAR) and False Rejection Rate (FRR) [1]. Training optimal recognition model can be effective on the highest match between the ECG test signals and a collection of training feature vectors. Therefore, the proper selection of robust classification methods can increase generalisation capability and enhance ECG biometric recognition accuracy. General recognition algorithms have been frequently applied in the use of the ECG biometrics including supervised classifiers such as Linear Discriminant Analysis (LDA) [14], K Nearest Neighbours (KNN) [27,6] and also in several research on Support Vector Machines (SVMs) [34,35] and other classification algorithms [11]. SVM is a kernel-based classifier based on a statistical learning theory in machine learning [36] that can attain predictive accuracy with reduced generalisation errors in object detection and classification problems [32]. Multi-class SVM classification is a supervised learning problem that has been proposed as an effective extension of binary classification. One-Against-All (OAA) is a multi-class approach which is proposed for biometric verification problem in the current paper by decomposing multi-class SVM into several binary classifiers.

This paper proposes a novel ECG verification framework based on non-fiducial approach using kernel methods in both feature extraction and classification after denoising signals with Discrete Wavelet Transform (DWT). The central consideration of the paper is to evaluate the effect of different linear and nonlinear dimension reduction algorithms in the recognition or authentication performance of a multi-class SVM, and to determine whether the learning classifier is robust in identity verification for individual recognition of unlabelled test data (unknown) by setting training samples.

The paper is organised as follows: Section 2 describes the methodology of the study, relevant technical backgrounds, and an investigation on the systematic analysis of some of the technical backgrounds. Section 3 explains how ECG data sets were collected and how design parameters for multi-class SVMs were chosen. The experimental recognition rates of the proposed feature extraction and recognition system over the ECG data sets are presented in Section 4. Finally, a conclusion and suggestions for future research are discussed in Section 5.

@&#METHODOLOGY@&#

The fundamental concept of underlying biometric technologies involves in different phases of data acquisition, preprocessing, feature extraction and classification. Fig. 1
                      depicts the general block diagram of the ECG authentication methodology proposed in this research paper. Preprocessing of ECG signals is conducted to remove noise of ECG signals through the filtering and threshold techniques. This is followed by the feature extraction phase to detect non-fiducial features to extract significant feature vectors. In classification phase, the biometric recognition system learns an optimal decision model to best match between training vectors enrolled in ECG dataset and test vectors.

Experiments on ECG signals collected were carried out at the BioSec.Lab,
                           1
                        
                        
                           1
                           
                              http://www.comm.utoronto.ca/~biometrics/.
                         at the University of Toronto to evaluate the performance of the proposed methodology. The database contains 68 biometric samples which were captured from 52 healthy volunteers in one session and the remaining ones in two sessions at rest position. ECG data acquisition was done by Vernier EKG-BTA sensor with a sampling frequency rate of 200 Hz which was attached on the subject's wrists to record one-lead ECG. Every database record contains a 3-minute conventional lead I ECG signal captured from each volunteer [4,10]. In this study, a record from each individual was applied to form the ECG datasets.

A single normal cycle of the ECG waveform that corresponds to a heartbeat is shown in Fig. 1. Any changes of cardiac function in each heartbeat makes a different deflection on the ECG pulse as series of positive and negative waves consisting of P, QRS complex and T. However, the collected ECG signals are usually corrupted by several sources of noise and therefore, it is not reasonable to perform feature extractions and classifications accurately [2,37]. Many filtering techniques are available for different signal types in order to reduce noise and improve signal-to-noise ratio (SNR) values including digital filters, adaptive denoising methods (involving Least Mean Squares and Recursive Least Squares) and time frequency domain representation (such as wavelet transform thresholding techniques). ECG is a non-stationary signal, so time-frequency representation methods are typically suited for its signal processing than the above mentioned adaptive denoising methods. Similarly, wavelet transform is a non-stationary signal processing technique that can be applied to sub-band coding data compression, feature extraction and denoising a signal without appreciable degradation [18,37–40]. Indeed, the wavelet transform can be thought of as an extension of Short Time Fourier Transform (STFT) that uses variable window sizes, depending on the frequency range of analysis. The wavelet transform can be classified into continuous and discrete. The Continuous Wavelet Transform (CWT) describes how a signal function 
                           f
                           (
                           t
                           )
                         is decomposed over entire time by fundamental mother wavelet ψ into a set of scale and position variables as shown in Eq. (1):
                           
                              (1)
                              
                                 
                                    
                                       
                                          
                                             CWT
                                          
                                          (
                                          a
                                          ,
                                          b
                                          )
                                          =
                                          
                                             1
                                             
                                                a
                                             
                                          
                                          
                                             ∫
                                             
                                                −
                                                ∞
                                             
                                             ∞
                                          
                                          f
                                          (
                                          t
                                          )
                                          ψ
                                          
                                             (
                                             
                                                
                                                   t
                                                   −
                                                   b
                                                
                                                a
                                             
                                             )
                                          
                                          d
                                          t
                                       
                                    
                                    
                                       
                                          
                                          a
                                          ∈
                                          
                                             
                                                R
                                             
                                             
                                                +
                                             
                                          
                                          −
                                          {
                                          0
                                          }
                                          ,
                                          b
                                          ∈
                                          R
                                       
                                    
                                 
                              
                           
                         where ‘a’ is a positive real number named as dilation or contraction parameter, and ‘b’ is a real number known as window translation or position parameter. The CWT method transforms a continual signal into highly redundant wavelet coefficients. Discrete Wavelet Transform (DWT) starts with a discrete set of data and considers a dyadic set of scales (power of two). The DWT is used to eliminate the difficulty of redundancy in CWT. DWT gives both time and frequency information through decomposition of a signal into sets of approximation (low frequency part of initial signal) and detail (high frequency part of initial signal) based on powers of two in different levels as shown in Eq. (2):
                           
                              (2)
                              
                                 
                                    
                                       
                                          
                                             DWT
                                          
                                          (
                                          a
                                          ,
                                          b
                                          )
                                          =
                                          
                                             1
                                             
                                                a
                                             
                                          
                                          
                                             ∫
                                             
                                                −
                                                ∞
                                             
                                             ∞
                                          
                                          f
                                          (
                                          t
                                          )
                                          ψ
                                          
                                             (
                                             
                                                
                                                   t
                                                   −
                                                   b
                                                
                                                a
                                             
                                             )
                                          
                                          d
                                          t
                                       
                                    
                                    
                                       
                                          
                                          a
                                          =
                                          
                                             
                                                2
                                             
                                             
                                                j
                                             
                                          
                                          ,
                                          
                                          b
                                          =
                                          k
                                          
                                             
                                                2
                                             
                                             
                                                j
                                             
                                          
                                          ,
                                          
                                          k
                                          ,
                                          j
                                          ∈
                                          Z
                                       
                                    
                                 
                              
                           
                         where 
                           Z
                         is a set of integers. Wavelet based denoising procedure consists of three steps: 1) choosing a mother wavelet and a decomposition level to perform the signal denoising through wavelet deconstruction, 2) performing appropriate soft or hard thresholding methods in wavelet domain, and 3) reconstructing the signal with the modified DWT coefficients via inverse wavelet transform. Fig. 2
                         shows a general scheme of the discrete wavelet decomposition and reconstruction process at three levels. The proper wavelet basis function leads to the maximisation of cross-correlation between coefficient values in the wavelet domain and the original signal, and it can be more effective in thresholding of wavelet coefficients for denoising of original signal [39]. The appropriate mother wavelet selection and the best decomposition level can be evaluated statistically based on measuring criteria including the higher Cross-Correlation (CC) coefficients, lower Root Mean Square Error (RMSE), higher Retained Energy (REnergy), highest Signal-to-Noise Ratio (SNR) and lowest Reconstruction Error (RError) between the original ECG signals and the wavelet coefficients at a particular level. The criteria are described below in Eqs. (3)–(5):
                           
                              (3)
                              
                                 
                                    
                                       
                                          
                                             
                                                CC
                                             
                                             
                                                
                                                   
                                                      s
                                                   
                                                   
                                                      0
                                                   
                                                
                                                
                                                   
                                                      s
                                                   
                                                   
                                                      r
                                                   
                                                
                                             
                                          
                                          (
                                          τ
                                          )
                                          =
                                          
                                             ∑
                                             
                                                n
                                                =
                                                0
                                             
                                             
                                                N
                                                −
                                                |
                                                τ
                                                |
                                                −
                                                1
                                             
                                          
                                          
                                             
                                                s
                                             
                                             
                                                0
                                             
                                          
                                          (
                                          n
                                          )
                                          
                                             
                                                s
                                             
                                             
                                                r
                                             
                                          
                                          (
                                          n
                                          +
                                          τ
                                          )
                                       
                                    
                                    
                                       
                                          
                                          where
                                          
                                          τ
                                          ≪
                                          N
                                          
                                          is time lag
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (4)
                              
                                 
                                    
                                       RMSE
                                    
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   n
                                                   =
                                                   0
                                                
                                                
                                                   N
                                                
                                             
                                             
                                                
                                                   (
                                                   
                                                      
                                                         s
                                                      
                                                      
                                                         0
                                                      
                                                   
                                                   (
                                                   n
                                                   )
                                                   −
                                                   
                                                      
                                                         s
                                                      
                                                      
                                                         r
                                                      
                                                   
                                                   (
                                                   n
                                                   )
                                                   )
                                                
                                                
                                                   2
                                                
                                             
                                          
                                          N
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (5)
                              
                                 
                                    
                                       SNR
                                    
                                    =
                                    log
                                    ⁡
                                    
                                       (
                                       
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   n
                                                   =
                                                   0
                                                
                                                
                                                   N
                                                
                                             
                                             
                                                
                                                   s
                                                
                                                
                                                   r
                                                
                                                
                                                   2
                                                
                                             
                                             (
                                             n
                                             )
                                          
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   n
                                                   =
                                                   0
                                                
                                                
                                                   N
                                                
                                             
                                             
                                                
                                                   (
                                                   
                                                      
                                                         s
                                                      
                                                      
                                                         0
                                                      
                                                   
                                                   (
                                                   n
                                                   )
                                                   −
                                                   
                                                      
                                                         s
                                                      
                                                      
                                                         r
                                                      
                                                   
                                                   (
                                                   n
                                                   )
                                                   )
                                                
                                                
                                                   2
                                                
                                             
                                          
                                       
                                       )
                                    
                                 
                              
                           
                         where 
                           
                              
                                 s
                              
                              
                                 0
                              
                           
                         is the original signal sample, 
                           
                              
                                 s
                              
                              
                                 r
                              
                           
                         is the reconstructed signal at each level, and N is the length of signal. Moreover, the shape of the scaling function of wavelet bases can also have an impact on the selection of mother wavelet. Daubechies, Symlet and Coiflet are orthogonal and compact support mother wavelets for denoising of ECG signals that have been used previously [37,39]. Therefore, this paper extends these mother wavelet functions.


                        Table 1
                         depicts the average denoising influence of different discrete wavelet transforms in the two optimal levels of decomposition among all tested levels on the 52 ECG signals. It can be seen from Table 1 that the performance criteria for wavelet denoising functions is very close in both decomposition levels. However, the performance of SNR and RError properties in db7 and coif3 at level 4 is better than the performance of other wavelet methods. Hence, these mother wavelets at level 4 can have an impressive performance for denoising the ECG signals.

Wavelet denoising entails soft or hard thresholding methods in which significant wavelet coefficients are selected by applying a specific threshold value (λ). Hard thresholding sets coefficients below the defined λ to zero and helps to remove noisy data without any change in the main characteristics of the original signals. Soft thresholding, on the contrary, sets coefficients below the value λ to zero and also shrinks other coefficients according to λ 
                        [41]. Therefore, the selection of the threshold values is able to overcome the challenge of having too much noise reduction and the change in signal shape as these are important criteria of a reliable signal threshold for denoising. Threshold selection methods estimate a different value of λ with or without rescaling, so that the techniques are able to have a direct influence over the quality of output denoised signal. Minimax rule utilises precomputed thresholds to minimise a constant term in the upper bound for the minimax risk of estimating a function by using a threshold estimator. The first technique uses Sqtwolog rule that has been proposed as an alternative to minimax threshold to calculate the threshold values λ with the use of universal threshold method 
                           
                              
                                 λ
                              
                              
                                 n
                              
                              
                                 u
                              
                           
                           =
                           
                              
                                 (
                                 2
                                 log
                                 ⁡
                                 n
                                 )
                              
                              
                                 1
                                 /
                                 2
                              
                           
                         where n is the length of signal [42]. The second is Rigrsure rule which is an adaptive thresholding method that uses Stein Unbiased Risk Estimator (SURE) principle to minimise error. The threshold value λ is estimated to 
                           λ
                           =
                           σ
                           
                              
                                 
                                    w
                                 
                                 
                                    b
                                 
                              
                           
                         where 
                           
                              
                                 w
                              
                              
                                 b
                              
                           
                         is bth coefficient at minimum risk from vector w and σ is the standard deviation of noisy signal. The third is Heursure rule which is a combination of the Sqtwolog and Rigrsure threshold techniques. If the signal-to-ratio is very small, the Sqtwolog threshold method gives better estimation than SURE technique. A small threshold yields noisy results, but large thresholds can remove significant segments of signals and miss the significant details of the signals. Penalisation rules are a type of Birgé–Massart strategy that uses level dependent thresholds in wavelet coefficients selection rule 
                           
                              
                                 n
                              
                              
                                 j
                              
                           
                           =
                           
                              m
                              
                                 
                                    (
                                    j
                                    +
                                    2
                                    −
                                    i
                                    )
                                 
                                 
                                    α
                                 
                              
                           
                         where j is decomposition level, m is the length of approximation coefficients over 2 and 
                           α
                           >
                           1
                         
                        [43].

Many different analysis criteria are applied in order to measure similarities and difference between the thresholding wavelet coefficients obtained at each decomposition level and the original ECG signals. The selection of proper soft or hard threshold methods is based on the criteria including the highest Signal-to-Noise Ratio (SNR), lowest Root Mean Square Error (RMSE), lowest percentage root mean square difference or PRD, highest L2 norm recovery value (PERFL2) and lowest percentage of zero coefficients used during reconstruction (PERF) that are defined with:
                           
                              (6)
                              
                                 
                                    
                                       PRD
                                    
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   n
                                                   =
                                                   0
                                                
                                                
                                                   N
                                                
                                             
                                             
                                                
                                                   (
                                                   s
                                                   
                                                      
                                                         (
                                                         n
                                                         )
                                                      
                                                      
                                                         original signal
                                                      
                                                   
                                                   −
                                                   s
                                                   
                                                      
                                                         (
                                                         n
                                                         )
                                                      
                                                      
                                                         reconstructed signal
                                                      
                                                   
                                                   )
                                                
                                                
                                                   2
                                                
                                             
                                          
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   n
                                                   =
                                                   0
                                                
                                                
                                                   N
                                                
                                             
                                             
                                                
                                                   (
                                                   s
                                                   
                                                      
                                                         (
                                                         n
                                                         )
                                                      
                                                      
                                                         original signal
                                                      
                                                   
                                                   )
                                                
                                                
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (7)
                              
                                 
                                    
                                       PERF
                                    
                                    =
                                    
                                       Number of zero coefficients at current level
                                       Number of coefficients
                                    
                                 
                              
                           
                        
                        
                           
                              (8)
                              
                                 
                                    
                                       
                                          PERFL
                                          2
                                       
                                    
                                    
                                       
                                          
                                          =
                                          
                                             
                                                (
                                                vector
                                                −
                                                norm
                                                
                                                   
                                                      (
                                                      coeffs of the current decomposition
                                                      ,
                                                      2
                                                      )
                                                   
                                                   
                                                      2
                                                   
                                                
                                             
                                             
                                                
                                                   (
                                                   vector
                                                   −
                                                   norm
                                                   (
                                                   original signal
                                                   ,
                                                   2
                                                   )
                                                   )
                                                
                                                
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     


                        Table 2
                         demonstrates the average denoising performance of the properties over the 52 ECG signals on six wavelet thresholding rules with hard and soft thresholding methods based on db7 and coif3 wavelet functions at the optimal decomposition level 4. Table 2 indicates that the Rigrsure rule for both soft and hard methods of coif3 wavelet gives the best results based on the performance of SNR and PRD. Based on the PERF value, Rigrsure of coif3 wavelet has performed well over other thresholding rules in db7 and coif3 wavelets. The major observation from the above results is that coif3 wavelet with Rigrsure rule of hard thresholding gives the best performance for the denoising of the ECG signals. Fig. 3
                         shows the comparison of a segment of original normal ECG signal (with cyan colour) and the signal denoised (with black colour) using coif3 wavelet at level 4 decomposition.

This paper focuses on ECG biometric feature extraction encompassing a combination of the non-fiducial based approach and different types of linear and nonlinear dimensional reduction techniques. Each approach is briefly described below.

The ECG is not a stationary signal and it consists of repetitive waveform patterns. The motivation behind non-fiducial approach is the use of normalised autocorrelation (AC) method on non-overlapping windows of the denoised individual ECG signal for detecting nonrandom and discriminative pattern without the use of fiducial point's boundaries [23]. Indeed, autocorrelation gives an automatic shift invariant feature set that represents repetitive characteristics over multiple heartbeat cycles [5]. Thus, the autocorrelation of ECG windows can be an effective method in identity verification process [6]. The autocorrelation coefficients 
                              
                                 
                                    
                                       
                                          R
                                       
                                       
                                          ˆ
                                       
                                    
                                 
                                 
                                    x
                                    x
                                 
                              
                              (
                              m
                              )
                            are computed by Eq. (9):
                              
                                 (9)
                                 
                                    
                                       
                                          
                                             
                                                R
                                             
                                             
                                                ˆ
                                             
                                          
                                       
                                       
                                          x
                                          x
                                       
                                    
                                    (
                                    m
                                    )
                                    =
                                    
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                i
                                                =
                                                0
                                             
                                             
                                                N
                                                −
                                                |
                                                m
                                                |
                                                −
                                                1
                                             
                                          
                                          x
                                          (
                                          i
                                          )
                                          x
                                          (
                                          i
                                          +
                                          m
                                          )
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      R
                                                   
                                                   
                                                      ˆ
                                                   
                                                
                                             
                                             
                                                x
                                                x
                                             
                                          
                                          (
                                          0
                                          )
                                       
                                    
                                 
                              
                            where 
                              
                                 
                                    
                                       
                                          R
                                       
                                       
                                          ˆ
                                       
                                    
                                 
                                 
                                    x
                                    x
                                 
                              
                              (
                              m
                              )
                            is the correlation of an ECG window 
                              x
                              (
                              i
                              )
                            and 
                              x
                              (
                              i
                              +
                              m
                              )
                            that is shifted by a time lag 
                              m
                              =
                              0
                              ,
                              1
                              ,
                              …
                              ,
                              M
                              −
                              1
                           ; 
                              M
                              ≪
                              N
                           , and N is the length of number of data points inside an ECG window 
                              x
                              (
                              i
                              )
                           . Large variations in amplitude are among the pulses of the same subject, so normalisation is essentially required. Normalisation and control of bias factor are carried out by dividing 
                              
                                 
                                    
                                       
                                          R
                                       
                                       
                                          ˆ
                                       
                                    
                                 
                                 
                                    x
                                    x
                                 
                              
                            with the maximum value 
                              
                                 
                                    
                                       
                                          R
                                       
                                       
                                          ˆ
                                       
                                    
                                 
                                 
                                    x
                                    x
                                 
                              
                              (
                              0
                              )
                           . Experiments conducted in this study for feature extraction based on the autocorrelation method show that the ECG signals have been windowed using a non-overlapping rectangular window of length 1028 coefficients which is equal to 5135 msec duration. The normalised AC is computed for each window by Eq. (9). A segment beginning at zero lag of the autocorrelation sequence is only preserved in order to do more analysis. The length of this segment is about equal to the duration of a QRS pulse with less variability in time [6]. Fig. 4
                            shows normalised autocorrelation sequence of an ECG window for two subjects, and also zooms in to the number of AC coefficients from different windows of both subjects.

The extracted autocorrelation coefficients of each ECG window can be directly utilised for classification, but the dimensionality of an AC vector is significantly high making it unsuitable for cost efficient systems and controlling false rejection rate [6]. Thus, dimension reduction is required to solve this problem as it converts a high-dimensional input space into a meaningful low-dimensional output space using the broadly linear and nonlinear techniques. The Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) are popular linear statistical learning techniques which have been widely used in computer vision, biometrics and other different areas [25]. The computation and memory requirement of PCA are smaller than LDA, so it is proper for a real time system [6]. However, the linear methods may fail to find essential data structure of nonlinear patterns in input space. Kernel-based techniques have been developed to handle nonlinear dimension reduction problem by mapping implicitly input observations into high dimensional feature space. Kernel-based methods are typically used in many real-world applications including Kernel Principal Component Analysis (KPCA) and Kernel Linear Discriminant Analysis (KLDA). The paper aims to evaluate the effects of the dimensionality reduction methods (LDA, PCA and KPCA) on ECG verification performance to determine whether the nonlinear method is more suitable for dimensionality reduction of the autocorrelation coefficients of ECG windows compared to the linear methods. Mathematical concepts for every one of the stated methods are given briefly in the following sections.

LDA is a statistical supervised learning method for classification and dimensionality reduction. A given data set consists of a number of n observation 
                                 X
                                 =
                                 
                                    
                                       {
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                       ,
                                       
                                          
                                             y
                                          
                                          
                                             i
                                          
                                       
                                       )
                                       }
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       n
                                    
                                 
                                 ∈
                                 
                                    
                                       R
                                    
                                    
                                       n
                                       ×
                                       d
                                    
                                 
                              , where 
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 ∈
                                 {
                                 1
                                 ,
                                 2
                                 ,
                                 …
                                 ,
                                 k
                                 }
                               denotes the corresponding class label of k classes, and 
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 ∈
                                 
                                    
                                       R
                                    
                                    
                                       d
                                    
                                 
                               indicates the d-dimensional input data. Let the data set is defined as 
                                 X
                                 =
                                 
                                    
                                       {
                                       
                                          
                                             X
                                          
                                          
                                             i
                                          
                                       
                                       }
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       k
                                    
                                 
                               and 
                                 
                                    
                                       X
                                    
                                    
                                       i
                                    
                                 
                                 ∈
                                 
                                    
                                       R
                                    
                                    
                                       
                                          
                                             n
                                          
                                          
                                             i
                                          
                                       
                                       ×
                                       d
                                    
                                 
                               is the data matrix of the ith class consisting of the number of samples 
                                 
                                    
                                       n
                                    
                                    
                                       i
                                    
                                 
                               and so 
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       k
                                    
                                 
                                 
                                    
                                       n
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 n
                               (total samples of all classes). LDA computes a linear transformation 
                                 w
                                 ∈
                                 
                                    
                                       R
                                    
                                    
                                       d
                                       ×
                                       l
                                    
                                 
                               that transforms 
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 ∈
                                 
                                    
                                       R
                                    
                                    
                                       d
                                    
                                 
                               in the d-dimensional space to a vector 
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                    
                                       l
                                    
                                 
                                 =
                                 
                                    
                                       w
                                    
                                    
                                       T
                                    
                                 
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 ∈
                                 
                                    
                                       R
                                    
                                    
                                       l
                                    
                                 
                               in the l-dimensional space (
                                 l
                                 <
                                 d
                              ) [44]. An optimal linear transformation is computed by simultaneously finding minimum intra-class distance and maximum inter-class distance so that it attains maximum class discrimination. The set of l feature basis vectors 
                                 
                                    
                                       {
                                       
                                          
                                             w
                                          
                                          
                                             m
                                          
                                       
                                       }
                                    
                                    
                                       m
                                       =
                                       1
                                    
                                    
                                       l
                                    
                                 
                               can be approximated by maximising the following Fisher's ratio function 
                                 J
                                 (
                                 w
                                 )
                               as shown in Eq. (10):
                                 
                                    (10)
                                    
                                       J
                                       (
                                       w
                                       )
                                       =
                                       arg
                                       ⁡
                                       
                                          max
                                          w
                                       
                                       ⁡
                                       
                                          
                                             |
                                             
                                                
                                                   w
                                                
                                                
                                                   T
                                                
                                             
                                             
                                                
                                                   S
                                                
                                                
                                                   B
                                                
                                             
                                             w
                                             |
                                          
                                          
                                             |
                                             
                                                
                                                   w
                                                
                                                
                                                   T
                                                
                                             
                                             
                                                
                                                   S
                                                
                                                
                                                   W
                                                
                                             
                                             w
                                             |
                                          
                                       
                                    
                                 
                               The vector 
                                 w
                               that maximises 
                                 J
                                 (
                                 w
                                 )
                               satisfies the following condition and is known as a generalised eigenvalue problem in Eq. (11) 
                              [6,29,44,45]:
                                 
                                    (11)
                                    
                                       
                                          
                                             (
                                             
                                                
                                                   S
                                                
                                                
                                                   W
                                                
                                             
                                             )
                                          
                                          
                                             −
                                             1
                                          
                                       
                                       
                                          
                                             S
                                          
                                          
                                             B
                                          
                                       
                                       
                                          
                                             w
                                          
                                          
                                             i
                                          
                                       
                                       =
                                       
                                          
                                             λ
                                          
                                          
                                             i
                                          
                                       
                                       
                                          
                                             w
                                          
                                          
                                             i
                                          
                                       
                                    
                                 
                               where 
                                 w
                                 =
                                 [
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    
                                       w
                                    
                                    
                                       l
                                    
                                 
                                 ]
                               and 
                                 
                                    
                                       S
                                    
                                    
                                       B
                                    
                                 
                               and 
                                 
                                    
                                       S
                                    
                                    
                                       W
                                    
                                 
                               are between-class and within-class variance matrices which are defined by Eq. (12) and Eq. (13) 
                              [44]:
                                 
                                    (12)
                                    
                                       
                                          
                                             
                                                S
                                             
                                             
                                                B
                                             
                                          
                                          =
                                          
                                             1
                                             n
                                          
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             k
                                          
                                          (
                                          
                                             
                                                
                                                   
                                                      x
                                                   
                                                   ‾
                                                
                                             
                                             
                                                i
                                             
                                          
                                          −
                                          
                                             
                                                x
                                             
                                             ‾
                                          
                                          )
                                          
                                             
                                                (
                                                
                                                   
                                                      
                                                         
                                                            x
                                                         
                                                         ‾
                                                      
                                                   
                                                   
                                                      i
                                                   
                                                
                                                −
                                                
                                                   
                                                      x
                                                   
                                                   ‾
                                                
                                                )
                                             
                                             
                                                T
                                             
                                          
                                       
                                    
                                 
                              
                              
                                 
                                    (13)
                                    
                                       
                                          
                                             
                                                S
                                             
                                             
                                                W
                                             
                                          
                                          =
                                          
                                             1
                                             n
                                          
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             k
                                          
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             
                                                
                                                   n
                                                
                                                
                                                   j
                                                
                                             
                                          
                                          (
                                          
                                             
                                                x
                                             
                                             
                                                i
                                                j
                                             
                                          
                                          −
                                          
                                             
                                                
                                                   
                                                      x
                                                   
                                                   ‾
                                                
                                             
                                             
                                                i
                                             
                                          
                                          )
                                          
                                             
                                                (
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      i
                                                      j
                                                   
                                                
                                                −
                                                
                                                   
                                                      
                                                         
                                                            x
                                                         
                                                         ‾
                                                      
                                                   
                                                   
                                                      i
                                                   
                                                
                                                )
                                             
                                             
                                                T
                                             
                                          
                                       
                                    
                                 
                               where 
                                 
                                    
                                       x
                                    
                                    
                                       i
                                       j
                                    
                                 
                               is the ith sample of jth class label, and 
                                 
                                    
                                       
                                          
                                             x
                                          
                                          ‾
                                       
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 
                                    1
                                    
                                       
                                          n
                                       
                                       
                                          i
                                       
                                    
                                 
                                 
                                    
                                       ∑
                                    
                                    
                                       j
                                       =
                                       1
                                    
                                    
                                       
                                          
                                             n
                                          
                                          
                                             i
                                          
                                       
                                    
                                 
                                 
                                    
                                       x
                                    
                                    
                                       i
                                       j
                                    
                                 
                               is the mean of set of samples of each class and 
                                 
                                    
                                       x
                                    
                                    ‾
                                 
                                 =
                                 
                                    1
                                    n
                                 
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       n
                                    
                                 
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                               is the global mean. However, classical LDA needs the so-called total scatter matrices (as: 
                                 
                                    
                                       S
                                    
                                    
                                       T
                                    
                                 
                                 =
                                 
                                    
                                       S
                                    
                                    
                                       B
                                    
                                 
                                 +
                                 
                                    
                                       S
                                    
                                    
                                       W
                                    
                                 
                              ) to be nonsingular if dimensionality of data is larger than the sample size which the case for many high-dimensional and low samples size data [44]. Maximisation of Fisher's ratio requires large separations between samples of different classes, and small variance between samples of each class. The optimal solution to the optimisation problem in Eq. (10) can be achieved by solving the generalised eigenvalue problem defined in Eq. (11). LDA finds the vector 
                                 w
                               as the l most significant eigenvectors of 
                                 
                                    
                                       (
                                       
                                          
                                             S
                                          
                                          
                                             W
                                          
                                       
                                       )
                                    
                                    
                                       −
                                       1
                                    
                                 
                                 
                                    
                                       S
                                    
                                    
                                       B
                                    
                                 
                               corresponding to the first l largest eigenvalues 
                                 λ
                                 ≥
                                 0
                               
                              [6]. Indeed, LDA reduces the dimensionality of feature space for a k class problem with the eigenvectors corresponding to the 
                                 k
                                 −
                                 1
                               largest eigenvalues [44]. The LDA linear projection for a data vector 
                                 x
                                 ∈
                                 
                                    
                                       R
                                    
                                    
                                       d
                                    
                                 
                               can be derived generally using Eq. (14) before the classification phase, where 
                                 y
                                 ∈
                                 
                                    
                                       R
                                    
                                    
                                       l
                                    
                                 
                               such that 
                                 1
                                 ≤
                                 l
                                 ≤
                                 k
                                 −
                                 1
                              :
                                 
                                    (14)
                                    
                                       y
                                       =
                                       
                                          
                                             w
                                          
                                          
                                             T
                                          
                                       
                                       x
                                    
                                 
                              
                           

PCA is a powerful unsupervised learning method to extract significant features based on principal components from a high-dimensional data set by solving an eigenvalue problem. PCA finds directions that have the least reconstruction error by describing as much variance of the data matrix as possible with a set of orthogonal directions [29,46]. Let average 
                                 
                                    
                                       x
                                    
                                    ‾
                                 
                               is subtracted from the unlabelled data set 
                                 X
                                 =
                                 
                                    
                                       {
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                       }
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       n
                                    
                                 
                                 ∈
                                 
                                    
                                       R
                                    
                                    
                                       d
                                    
                                 
                              , which gives the mean centred data (zero-mean data). These mean-centred data are used to compute a covariance matrix. PCA diagonalises the covariance matrix 
                                 
                                    
                                       S
                                    
                                    
                                       cov
                                    
                                 
                               of the multivariate data set X as in Eq. (15):
                                 
                                    (15)
                                    
                                       
                                          
                                             S
                                          
                                          
                                             cov
                                          
                                       
                                       =
                                       
                                          1
                                          n
                                       
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          n
                                       
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                       −
                                       
                                          
                                             x
                                          
                                          ‾
                                       
                                       )
                                       
                                          
                                             (
                                             
                                                
                                                   x
                                                
                                                
                                                   i
                                                
                                             
                                             −
                                             
                                                
                                                   x
                                                
                                                ‾
                                             
                                             )
                                          
                                          
                                             T
                                          
                                       
                                    
                                 
                               where n is the number of vectors in the data set. The covariance between two vectors reveals the extent in which the patterns are correlated. A transformation of the samples can be computed by using eigenvalue analysis of the covariance matrix 
                                 
                                    
                                       S
                                    
                                    
                                       cov
                                    
                                 
                               as in Eq. (16):
                                 
                                    (16)
                                    
                                       
                                          
                                             λ
                                          
                                          
                                             i
                                          
                                       
                                       
                                          
                                             w
                                          
                                          
                                             i
                                          
                                       
                                       =
                                       
                                          
                                             S
                                          
                                          
                                             cov
                                          
                                       
                                       
                                          
                                             w
                                          
                                          
                                             i
                                          
                                       
                                    
                                 
                               where eigenvalue 
                                 λ
                                 ≥
                                 0
                               and 
                                 w
                                 ∈
                                 
                                    
                                       R
                                    
                                    
                                       d
                                    
                                 
                                 ∖
                                 {
                                 0
                                 }
                               is the nonzero vector 
                                 w
                                 =
                                 [
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    
                                       w
                                    
                                    
                                       d
                                    
                                 
                                 ]
                               corresponding to eigenvalue λ 
                              [46]. 
                                 k
                                 ≤
                                 d
                               eigenvectors corresponding to the highest eigenvalues are actually needed to produce a complete basis for the linear transformation. Performance of classifier varies as the number of principal components (features) changes. The dimension of data vector 
                                 x
                               is generally reduced using PCA with the following Eq. (17), where y is k dimension, and 
                                 w
                                 =
                                 [
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    
                                       w
                                    
                                    
                                       k
                                    
                                 
                                 ]
                              :
                                 
                                    (17)
                                    
                                       y
                                       =
                                       
                                          
                                             w
                                          
                                          
                                             T
                                          
                                       
                                       (
                                       x
                                       −
                                       
                                          
                                             x
                                          
                                          ‾
                                       
                                       )
                                    
                                 
                              
                           

Kernel PCA is known as nonlinear technique since PCA is done on nonlinearly mapped data set of input space. Nonlinear techniques refer to the methodology used to attain a minimal generalisation error for data that are nonlinear in input space. KPCA overcomes the limitation of linear methods by nonlinearly mapping input space to a high dimension feature space. Indeed, KPCA is a nonlinear extension of PCA which computes principal components in higher dimension feature space. Kernel-based learning methods use an implicit mapping of the input data into a high dimensional feature space defined by a kernel function, i.e., a function returning the inner product 
                                 (
                                 Φ
                                 (
                                 x
                                 )
                                 .
                                 Φ
                                 (
                                 y
                                 )
                                 )
                               between the images of two data points x, y in the feature space. The kernel based methods compute inner products in the feature space directly from the inputs without explicitly transforming x and y into the feature space using function Φ. Kernel function is known as a kernel trick with the representation form 
                                 K
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       x
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 =
                                 (
                                 Φ
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 .
                                 Φ
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 )
                               which allows computing the dot product into Reproducing Kernel Hilbert Space (RKHS) without doing computation of the map Φ [32,45,46].

The common kernel functions are given below:
                                 
                                    •
                                    Polynomial: 
                                          K
                                          (
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          ,
                                          
                                             
                                                x
                                             
                                             
                                                j
                                             
                                          
                                          )
                                          =
                                          
                                             
                                                (
                                                (
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      i
                                                   
                                                
                                                ⋅
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      j
                                                   
                                                
                                                )
                                                +
                                                r
                                                )
                                             
                                             
                                                d
                                             
                                          
                                        where 
                                          d
                                          ∈
                                          N
                                       , 
                                          r
                                          ∈
                                          R
                                       
                                    

Gaussian RBF: 
                                          K
                                          (
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          ,
                                          
                                             
                                                x
                                             
                                             
                                                j
                                             
                                          
                                          )
                                          =
                                          exp
                                          ⁡
                                          (
                                          −
                                          γ
                                          
                                             
                                                ‖
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      i
                                                   
                                                
                                                −
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      j
                                                   
                                                
                                                ‖
                                             
                                             
                                                2
                                             
                                          
                                          )
                                        where 
                                          γ
                                          ∈
                                          R
                                       
                                    


                              Fig. 5
                               indicates the effect of PCA and RBF KPCA with 
                                 γ
                                 =
                                 1
                               into inter-subject variability for the two persons' normalised AC coefficients. We use a criterion by taking the m optimal number of components in PCA and KPCA that satisfies approximately 
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       m
                                    
                                 
                                 
                                    
                                       λ
                                    
                                    
                                       i
                                    
                                 
                                 /
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       d
                                    
                                 
                                 
                                    
                                       λ
                                    
                                    
                                       i
                                    
                                 
                                 ≥
                                 99
                               and 80% respectively, where 
                                 
                                    
                                       λ
                                    
                                    
                                       i
                                    
                                 
                               is the eigenvalue and d is the dimensionality of feature space in each method.

Biometric verification problem is applied to decide whether to accept or reject a user's claimed identity according to the different biometric sample vectors enrolled. Indeed, verification is a binary classification problem due to enrolment of samples of an individual for verification as one class and all other enrolment samples of the remaining persons as other classes [47]. Therefore, different subjects in the ECG data set need different binary classifiers to be recognised by the system. Support Vector Machine (SVM) is a very robust nonparametric classification technique that is based on statistical learning theory in machine learning to deal with problems of classification and regression [36]. The basic theories of SVM have been initially proposed for binary classification problems to increase generalisation ability in order to improve predictive accuracy. The binary SVM algorithms are based on learning a decision boundary with a maximum margin between specified classes either in data space or in feature space for linear and nonlinear SVMs. Nonlinear classification arises whenever the training data points (
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           ,
                           
                              
                                 y
                              
                              
                                 i
                              
                           
                        ), 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           ∈
                           
                              
                                 R
                              
                              
                                 d
                              
                           
                         and class label 
                           
                              
                                 y
                              
                              
                                 i
                              
                           
                           ∈
                           {
                           +
                           1
                           ,
                           −
                           1
                           }
                         cannot be separated with a linear classifier in the d-dimensional input space with lower susceptibility to overfitting. 
                           K
                           (
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           ,
                           
                              
                                 x
                              
                              
                                 j
                              
                           
                           )
                         defined by 
                           K
                           (
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           ,
                           
                              
                                 x
                              
                              
                                 j
                              
                           
                           )
                           =
                           (
                           Φ
                           (
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           )
                           .
                           Φ
                           (
                           
                              
                                 x
                              
                              
                                 j
                              
                           
                           )
                           )
                         and is known as the kernel function, which captures the similarity between points 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                         and 
                           
                              
                                 x
                              
                              
                                 j
                              
                           
                        . Kernel method is employed in SVM design to construct an optimal linear classifier in feature space which results into nonlinear decision boundary in input space. This margin maximisation is closely associated to the generalisation property of SVMs. SVM is a supervised classifier that has generalisation ability in the sense that it can classify an unseen pattern correctly. There is a trade-off between the size of margin and the number of training errors in SVM methods for the linear decision boundary [48]. Therefore, the learning of optimal decision boundary has an influence on minimising the generalisation errors and performs well on the test data [32].

Many real-world problems such as biometric recognition [47] encounter input data which consists of more than two classes and represented as a set of k classes 
                           Y
                           =
                           {
                           
                              
                                 y
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 y
                              
                              
                                 2
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 y
                              
                              
                                 k
                              
                           
                           }
                         
                        [32]. Multi-class classification is known as supervised learning problem that extends the binary classification methods. The computational effort and complexity of multi-class classification are more than binary classifiers because of the high connection among a number of different classes. Several approaches have been developed to handle multi-class problems by SVM through their decomposition into k binary problems. One-Against-All (OAA) approach [48] is the most common multi-class technique which is able to decompose multi-class problem into k binary classification problems. With regard to this approach, a training vector would be classified under a certain class if the class of SVM accepts data; otherwise, it would be rejected [32,49]. Test data is classified by combining the predictions learned with the binary methods according to OAA approach. A voting strategy is generally utilised to combine the predictions, where the class that gets the highest number of votes is assigned to the test sample. Another possible scheme is to map the binary classifiers output into probability approximates before assigning the test instances to the class with the highest one [32,50]. In this paper, we propose an ECG-based recognition model relying on multi-class SVM classification through OAA approach by using an appropriate kernel function. C-Support Vector Classification (C-SVC) as binary SVM classifier is proposed to build k binary models for multi-class recognition problems. The mathematical concept of C-SVC is briefly described below:


                        C-Support vector classification (C-SVC) is a type of binary classification method that has been formulated as the soft-margin SVM primal constrained optimisation problem in the following: the primal constrained optimisation problem for two classes with 
                           
                              
                                 {
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 }
                              
                              
                                 i
                                 =
                                 1
                              
                              
                                 n
                              
                           
                           ∈
                           
                              
                                 R
                              
                              
                                 d
                              
                           
                         and 
                           
                              
                                 y
                              
                              
                                 i
                              
                           
                           ∈
                           {
                           +
                           1
                           ,
                           −
                           1
                           }
                         is defined in Eq. (18):
                           
                              (18)
                              
                                 
                                    
                                       
                                          
                                             min
                                             
                                                w
                                                ,
                                                b
                                                ,
                                                ξ
                                             
                                          
                                          ⁡
                                          
                                             [
                                             
                                                1
                                                2
                                             
                                             
                                                
                                                   ‖
                                                   w
                                                   ‖
                                                
                                                
                                                   2
                                                
                                             
                                             +
                                             
                                                C
                                                n
                                             
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                
                                                   ξ
                                                
                                                
                                                   i
                                                
                                             
                                             ]
                                          
                                       
                                    
                                    
                                       
                                          subject to
                                          
                                          
                                             
                                                y
                                             
                                             
                                                i
                                             
                                          
                                          
                                             (
                                             w
                                             ⋅
                                             Φ
                                             (
                                             
                                                
                                                   x
                                                
                                                
                                                   i
                                                
                                             
                                             )
                                             +
                                             b
                                             )
                                          
                                          ≥
                                          1
                                          −
                                          
                                             
                                                ξ
                                             
                                             
                                                i
                                             
                                          
                                          ,
                                          
                                             
                                                ξ
                                             
                                             
                                                i
                                             
                                          
                                          ≥
                                          0
                                          ,
                                          
                                          ∀
                                          i
                                          ∈
                                          n
                                       
                                    
                                 
                              
                           
                         here, 
                           w
                         is the weight vector, b is bias term or offset of the hyperplane from the origin, 
                           
                              
                                 ξ
                              
                              
                                 i
                              
                           
                         is the margin violation of the ith training data and 
                           Φ
                           (
                           x
                           )
                           :
                           
                              
                                 R
                              
                              
                                 n
                              
                           
                           →
                           
                              
                                 R
                              
                              
                                 N
                              
                           
                         is a (possibly non-linear) feature mapping that maps a point in 
                           
                              
                                 R
                              
                              
                                 n
                              
                           
                         to a higher dimensional space 
                           
                              
                                 R
                              
                              
                                 N
                              
                           
                        . The first term in the above problem seeks to maximise the margin, while the second term corresponds to empirical risk minimisation. C is a cost parameter that controls trade-off between maximising the margin size and the minimising the training error. Slack variables (
                           
                              
                                 ξ
                              
                              
                                 i
                              
                           
                        ) demonstrate the estimation of the number of training errors. If 
                           
                              
                                 ξ
                              
                              
                                 i
                              
                           
                           =
                           0
                        , then there is no margin error; otherwise, all slack variables show margin errors. The above quadratic programming problem is computed by using the Lagrange multipliers method by redefining the objective function by attaching constraints 
                           f
                           (
                           x
                           )
                           =
                           w
                           ⋅
                           Φ
                           (
                           x
                           )
                           +
                           b
                         using Lagrange multipliers into the objective function (known as the primal problem) as an unconstrained optimisation problem. The primal problem can be simplified by reformulating the above primal problem as a dual problem through the Lagrange multipliers (e.g., 
                           
                              
                                 λ
                              
                              
                                 i
                              
                           
                           ≥
                           0
                        ) [32]. The Lagrange multiplier 
                           
                              
                                 λ
                              
                              
                                 i
                              
                           
                         must be zero unless the training data 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                         satisfies in the constraint 
                           
                              
                                 y
                              
                              
                                 i
                              
                           
                           (
                           w
                           ⋅
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           +
                           b
                           )
                           =
                           1
                        . Such training sample, with 
                           
                              
                                 λ
                              
                              
                                 i
                              
                           
                           ≥
                           0
                        , locates on the hyperplanes 
                           w
                           ⋅
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           +
                           b
                           =
                           ±
                           1
                         and is known as support vector. The corresponding dual problem for the above constrained optimisation problem of Eq. (18) is the following:
                           
                              (19)
                              
                                 
                                    
                                       
                                          
                                             min
                                             λ
                                          
                                          ⁡
                                          
                                             [
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                
                                                   λ
                                                
                                                
                                                   i
                                                
                                             
                                             −
                                             
                                                1
                                                2
                                             
                                             
                                                ∑
                                                
                                                   i
                                                   ,
                                                   j
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                
                                                   λ
                                                
                                                
                                                   i
                                                
                                             
                                             
                                                
                                                   λ
                                                
                                                
                                                   j
                                                
                                             
                                             
                                                
                                                   y
                                                
                                                
                                                   i
                                                
                                             
                                             
                                                
                                                   y
                                                
                                                
                                                   j
                                                
                                             
                                             Φ
                                             (
                                             
                                                
                                                   x
                                                
                                                
                                                   i
                                                
                                             
                                             )
                                             ⋅
                                             Φ
                                             (
                                             
                                                
                                                   x
                                                
                                                
                                                   j
                                                
                                             
                                             )
                                             ]
                                          
                                       
                                    
                                    
                                       
                                          subject to
                                          
                                          0
                                          ≤
                                          
                                             
                                                λ
                                             
                                             
                                                i
                                             
                                          
                                          ≤
                                          
                                             C
                                             n
                                          
                                          ,
                                          
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             n
                                          
                                          
                                             
                                                λ
                                             
                                             
                                                i
                                             
                                          
                                          
                                             
                                                y
                                             
                                             
                                                i
                                             
                                          
                                          =
                                          0
                                          ,
                                          
                                          C
                                          >
                                          0
                                          ,
                                          
                                          ∀
                                          i
                                          ∈
                                          n
                                       
                                    
                                 
                              
                           
                         here, 
                           
                              
                                 λ
                              
                              
                                 i
                              
                           
                         is the dual variable associated with the ith primal constraint, and 
                           Φ
                           (
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           )
                        . 
                           Φ
                           (
                           
                              
                                 x
                              
                              
                                 j
                              
                           
                           )
                         is elements of the kernel function, which captures the similarity between points 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                         and 
                           
                              
                                 x
                              
                              
                                 j
                              
                           
                        . The primal variable 
                           w
                         is related to the dual variable λ as Eq. (20):
                           
                              (20)
                              
                                 w
                                 =
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    n
                                 
                                 
                                    
                                       λ
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 Φ
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 )
                              
                           
                         The decision function can be derived from the dual optimisation problem can be represented by Eq. (21):
                           
                              (21)
                              
                                 f
                                 (
                                 x
                                 )
                                 =
                                 sign
                                 
                                 
                                    (
                                    
                                       ∑
                                       
                                          i
                                          ,
                                          j
                                          =
                                          1
                                       
                                       n
                                    
                                    
                                       
                                          y
                                       
                                       
                                          i
                                       
                                    
                                    
                                       
                                          λ
                                       
                                       
                                          i
                                       
                                    
                                    k
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    x
                                    )
                                    +
                                    b
                                    )
                                 
                              
                           
                         The bias term b can be calculated for data points by averaging as shown in Eq. (22):
                           
                              (22)
                              
                                 b
                                 =
                                 f
                                 (
                                 x
                                 )
                                 −
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    n
                                 
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       λ
                                    
                                    
                                       i
                                    
                                 
                                 k
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 x
                                 )
                              
                           
                         The goal of the classification problem is learning an optimal separable hyperplane known as a decision function as defined in Eqs. (21) and (22).

The interpretation of Eq. (21) is that each training point 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                         in the training set is assigned an importance weight of 
                           
                              
                                 λ
                              
                              
                                 i
                              
                           
                        . The label of a test point x is decided by a linear combination of a labels of important points similar to x. It may be noted that this interpretation is consistent with the fact non-support vectors are assigned 
                           
                              
                                 λ
                              
                              
                                 i
                              
                           
                           =
                           0
                        , and they are indeed unimportant for decision, since their removal does not affect the decision boundary.

@&#EXPERIMENTS@&#

This section evaluates recognition performance comparison of multi-class SVM models based on kernel methods with OAA approach over optimal feature sets which are extracted by using different dimension reduction algorithms described in Section 2.3.2. The objectives of the experiments are to find which linear and nonlinear feature extractors that can be more efficient for ECG biometric verification, and also determine the effect of kernel methods in recognition performance.

With regard to Section 2.3.1, an optimal number of ECG windows with an identical length examined for each subject's ECG recording is 35, so the total windows of the 52 ECG signals (single-session) are 1820. The dimension reduction experiments are carried out on the generic dataset including the 1820 AC windows with the vector of dimension 160 which is the number of normalised AC coefficients for every window. To improve performance of SVM classification algorithms, it is desirable to have data set with proper feature space dimension. For this purpose, in Section 2.3.2, LDA, PCA and KPCA have mathematically been discussed to reduce dimensionality of AC segments and to find significant features in order to enhance recognition performance. The kernel selection has a crucial effect on the SVM classification performance [45]. There are no definite rules for a specific kernel choice except satisfactory performance by simulation study. RBF kernel function with different parameter settings has been experimented in this study for both kernel-based learning methods of nonlinear feature extractor and classification (recognition).

When the supervised LDA method is used to find the significant features by solving the eigenvalue problem, the generic data set must be labelled by taking a class label for a set of AC windows of a subject. In this experiment, the generic data set is labelled by a set of 52 class labels which corresponds to the number of subjects considered in this study. As indicated earlier in Section 2.3.2, LDA can reduce the dimensionality feature space of a k class problem to 
                        k
                        −
                        1
                      because the rank of inter-class matrix cannot go beyond than 
                        k
                        −
                        1
                     . Therefore, the ECG data set is created by the 51 largest eigenvalues that can be a significant for LDA domain to reduce dimensionality of the generic data set. The dimensionality of the generic data set is reduced from 160 to 31 using PCA method which retains 99% of the data's variance. Using KPCA method with RBF kernel in different γ values of 0.01, 0.1, and 1 helps to reduce dimensionality of the generic data set from 160 coefficients to data sets including 54, 107 and 111 features respectively which preserve approximately 99, 98 and 80% of the data's variance. To build OAA SVM multi-class model with OAA approach, the same above set of 52 class labels have to be assigned to vectors of all the ECG data sets which are created by using PCA and KPCA feature extractors.

For each of the ECG data sets predefined, the generalised accuracies of multi-class SVM models are estimated using different values of the RBF kernel parameter γ in 
                        {
                        1
                        ,
                        5
                        ,
                        10
                        ,
                        15
                        }
                      and the regulation constant C of C-SVC binary classifier in 
                        {
                        10
                        ,
                        60
                        ,
                        100
                        }
                     . Then, 3 random data sets are generated from every ECG data sets predefined for each pair of (
                        C
                        ,
                        γ
                     ). Therefore, each of the ECG data sets is tested on 
                        4
                        ×
                        3
                        ×
                        3
                        =
                        36
                      combinations. Both training and testing sets are available for the ECG data sets, whereby each is randomly split into the training and test sets (mostly ≈60%: 40%)-called Percentage Split or Holdout method [32,45]. The number of training vectors is 1092 samples (AC windows) while the other 728 form the unlabelled (unknown) test data sets for classification. The result of the Gaussian OAA SVMs on the training and testing ECG data sets including the means and standard deviations are summarised.

Nevertheless, the proposed framework must be validated by performing it on a set of ECG signals collected from 16 subjects in two recording sessions with regarding Section 2.1, as well as running 10-fold cross-validation technique on each of the ECG data sets predefined from one and two recording sessions.

In this ECG based biometric system, the recognition performance of OAA multi-class SVM models is measured according to True Match Rate (TMR), and the two types of errors of False Nonmatch Rate (FNMR) and False Match Rate (FMR), also known as classification error rates. TMR measures how well the system is able to correctly match mate samples (classified), and it is known as Window Recognition Rate (WRR) in this verification system. The Subject Recognition Rate (SRR) can also be computed by measuring TMR. The subject recognition rate is a measurement for a number of subjects whose all test samples can be correctly classified into their exact class based on their learning patterns. FNMR refers to the number of mate samples falsely declared as nonmatch (unclassified) whereas FMR refers to the number of nonmate samples recognised as a correct match (misclassified). When the system becomes more secure, FMR should be less than FNMR.

@&#RESULTS AND DISCUSSIONS@&#

This section provides experimental results that consider the verification performance comparison of Gaussian (RBF) OAA SVM classifiers on the ECG data sets predefined by using several different extraction algorithms (combination of autocorrelation and dimension reduction methods), and also investigates the effect of different parameters settings (
                        C
                        ,
                        γ
                     ) in verification recognition system. The different ECG data sets are displayed in the following tables of results by Dimension Reduction (DR) method and the number of optimal features used (#F) for each of them.

The experimental results given in Tables 3, 4, 5 and 6
                     
                     
                     
                      indicate that the Gaussian OAA SVM models are very sensitive to the RBF kernel parameter γ than the cost parameter C for unlabelled test data sets. The parameter C controls the trade-off between training errors and norm of weight vectors. If the value of C selected is large enough, upper bound of the norm of weights is increased, and this leads to better classification. Indeed, regularisation of parameter C is able to control the influence of each individual support vector and prevent high overfitting in the recognition system. Overfitting occurs when the classifier model leads to low training errors and high generalisation errors in the data set. On the other hand, the width of the kernel surface is inversely proportional to parameter γ. When the value of γ is very small, a boundary for kernel RBF is larger than the maximum pairwise distance of data points, so SVM is not able to determine a certain decision boundary as overfitting is high. However, a large value of γ makes a simpler model with a low variance and high bias which does not tend to create an overfitting. The result tables show the effect of increasing the γ values on the test recognition rates causes substantial lower overfitting through decreasing False Nonmatch Rate in spite of a rise in False Match Rate. Fig. 6
                      shows the effect on (
                        C
                        ,
                        γ
                     ) in window predictive rates (for test data) for all the data sets.

It can be seen from Table 3 that the Gaussian OAA SVM model cannot be robust for biometric recognition because the highest generalisation error in the test data sets occurs when unsuitable value of the kernel parameter is chosen. In other words, the classifiers may well learn training data with the lowest biometric errors, but the worst recognition rates occur in all testing data. The different values of parameter C are not effective for recognition performance system and in attaining similar accuracy, so its values are disregarded in Table 3. The value highlighted in bold specifies the best result obtained in the different tables.

An effect that can be examined is whether the relationship between the optimal γ value of RBF kernel classification corresponds to the best recognition rates and the number of features in the data sets. Although the values of γ parameter and the number of features are not directly related, the best possible γ values mainly occur in data sets with a small number of features. The Gaussian kernel value computed with 
                        exp
                        ⁡
                        (
                        −
                        γ
                        
                           
                              ‖
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              −
                              
                                 
                                    x
                                 
                                 
                                    j
                                 
                              
                              ‖
                           
                           
                              2
                           
                        
                        )
                      is close to a zero for data sets with a large number of features (attributes) and large γ values. Hence, it cannot adequately generalise a large number of support vectors. Due to this fact, a direct correlation between increasing the number of features and the γ value up to 15 and the recognition performance of the KPCA data sets as in Tables 3, 4, 5 and 6 can be observed. However, the different variations captured (related to eigenvalues) by using KPCA for extraction of significant features can also be effective in determining the accuracy of the Gaussian OAA SVM models. The verification recognition rates (WRR and SRR) are illustrated by the 
                        KPCA
                        111
                        <
                        KPCA
                        107
                        <
                        KPCA
                        54
                      as the γ value and the number of attributes is increased. In addition, the result shows that an optimal γ value of KPCA data sets is 15. However, these tables do not report the recognition rates which decline.

Moreover, the relationship between the optimal γ value and the best recognition performance of both LDA and PCA features is similar to what have been explained earlier. The recognition rates increase when the γ values go up to 15 and 10 for LDA and PCA features respectively, after which they slowly decline. Nevertheless, a general correlation cannot be found between the optimal RBF γ and the number of features and the best recognitions on the experiment results of all the data sets. The highest window and subject predictive accuracies are achieved to approximately 88.41% and 41.66% by using the Gaussian OAA SVM classifier which is set at the optimal setting value 
                        (
                        C
                        ,
                        γ
                        )
                        =
                        (
                        60
                        ,
                        15
                        )
                      on the KPCA data set with (
                        #F
                        =
                        54
                     ) that has the lowest average biometric errors and less overfitting than others. The biometric errors of FNMR and FMR are decreased to about 7.59 and 3.97%, respectively on the KPCA54 data set.

The low subject recognition rate seems to have high unclassified samples which do not fit into any model. The highly skewed the data sets affect to perform classifiers due to poor bias approximation. Therefore, a brief investigation can also be carried out on the influence of FNMR and FMR errors on the biometric recognition rates of WRR and SRR in Tables 3, 4, 5 and 6. FNMR errors are reduced for each of the data sets with an increase in the γ values, so the window recognition rates (WRRs) as well as subject recognition rates (SRRs) are increased until the optimal γ value for each data is set. It can be seen from the tables that the window recognition rate may be reduced in some data sets, but subject recognition rate can be raised or vice-versa.


                     Fig. 7
                      clearly shows the effect of biometric errors for the best subject recognition rates on LDA and PCA and all KPCA test data sets.

Another effect that can be considered is whether the improvement in performance is due to the denoising approach or the application of KPCA. KPCA is more effective in increasing performance and good generalisation of the system compared with LDA and PCA methods according to above discussion. In order to evaluate the impact of denoising approach, the results of Gaussian OAA SVM classifier based on KPCA feature extraction on both the denoised individual ECG signals using coif3 and db7 wavelets are compared. According to Tables 1 and 2, the best performance of db7 wavelet is at level 4 with Rigrsure rule of hard thresholding. The experimental results given in Table 7
                      indicate that the effect of different mother wavelets on error and recognition rates of Gaussian OAA SVMs with 
                        (
                        C
                        ,
                        γ
                        )
                        =
                        (
                        60
                        ,
                        15
                        )
                      on test data including 54 features. The optimal selection of mother wavelet can be crucial to have signals with the lowest reconstruction errors and highest Signal-to-Noise Ratio for achieving higher recognition rates in the ECG based biometrics.

A comparison has been made between computational complexity and memory requirements which involve dimensionality reduction algorithms. The elapsed time needed to perform each reducing dimension processing of the techniques on the high dimensional space of AC windows (samples) has also been evaluated. In addition, the average elapsed training time as well as testing time needed to build the Gaussian OAA SVM classifiers for all subjects on the ECG data sets with significant dimensions and predict test samples on the SVM models have also been considered, and these are shown in Table 8
                     .

The computational complexity and memory required for the dimension reduction techniques LDA, PCA and KPCA are determined by two data properties – the number of data samples n and the original dimension space d. LDA involves the eigenvalue problem that can be especially expensive for both computational and memory. LDA has 
                        O
                        (
                        n
                        d
                        t
                        +
                        
                           
                              t
                           
                           
                              3
                           
                        
                        )
                      computational and requires 
                        O
                        (
                        n
                        d
                        +
                        n
                        t
                        +
                        d
                        t
                        )
                      memory, where 
                        t
                        =
                        min
                        ⁡
                        (
                        n
                        ,
                        d
                        )
                      
                     [51]. The computational complexity of PCA to solve eigenvalue problem of the 
                        d
                        ×
                        d
                      covariance matrix is 
                        O
                        (
                        
                           
                              d
                           
                           
                              3
                           
                        
                        )
                     . The memory required of PCA is 
                        O
                        (
                        
                           
                              d
                           
                           
                              2
                           
                        
                        )
                      to store the 
                        d
                        ×
                        d
                      covariance matrix. The nonlinear method of KPCA has computational and memory complexity disadvantages compared to PCA method. The KPCA method performs an eigenvalue problem with the 
                        n
                        ×
                        n
                      kernel matrix and demands for a memory including square with the number of data samples n. Therefore, the computational and memory complexity of KPCA are 
                        O
                        (
                        
                           
                              n
                           
                           
                              3
                           
                        
                        )
                      and 
                        O
                        (
                        
                           
                              n
                           
                           
                              2
                           
                        
                        )
                     , respectively [26]. In this paper, most of elapsed time, in seconds, for the linear and nonlinear dimension reduction techniques in which 
                        KPCA
                        111
                        >
                        KPCA
                        107
                        >
                        KPCA
                        54
                        >
                        LDA
                        >
                        PCA
                      with time values of 16.18, 15.16, 5.45, 0.2, 0.05 s, respectively. It can be seen from Table 8 that the optimal γ value for each of the data set is able to reduce complexity, and so the average elapsed tainting time to learn the Gaussian SVM classifiers for all subjects is decreased. There is no significant difference in the time of data sets in the optimal setting parameters (
                        C
                        ,
                        γ
                     ). In addition, recognition of test data is faster as shown in Table 8. The value of the C parameter does not affect the testing computational time of the SVM models, so the values have been disregarded in the table.

In order to evaluate the proposed framework validation, a performance comparison between random partition holdout (60–40%) and 10-fold cross-validation techniques is summarised in Table 9
                      for both one and two recording sessions. With regard to Section 2.3.1, the total windows for autocorrelation of the 32 ECG signals (two records for 16 subjects) are 1120 with the vector of dimension 160 for every window. In this experiment, the dimensionality of the generic data set is respectively reduced from 160 to 15, 26 and 54 using LDA, PCA and KPCA methods. PCA and Gaussian based KPCA in γ value of 0.01 which retain 99% of the data's variance. According to holdout method (60–40%), each data set is randomly split into the 672 training data and remaining 448 are test sets for Gaussian OAA SVM classification. However, 10-fold cross-validation is used to partition randomly each of the ECG data sets predefined into 10 equal-sized multi-class subsets. This process is repeated 10 times so that every partition is utilised for testing exactly once. Although this approach leads to estimate accurate performance, it is computationally expensive to repeat the procedure 10 times. As Table 9 shown, 10-fold cross-validation is an effective random partition method for predictive recognition rates, because the technique can reduce the variance [52]. The highest window and subject predictive accuracies are achieved on the KPCA data sets for each one and two recording sessions in both random partition methods that have the lowest average biometric errors and less overfitting than others. On the other hand, the lowest rates of subject recognition for both random partition methods on two recording sessions could be the result of variations within ECG waveforms over time-varying. Based on Table 9, when the SVM classifier deals with a large number of classes, window recognition rates are decreased due to poor bias approximation.

A comparison of the current framework with some methodologies found in literature is summarised in Table 10
                     . Although the two main factors of clinical source data and length of ECG segment were highly effective in increasing accuracy of identity recognition in study [6], it is impractical for real life applications in condition where recognition must be processed quickly.

With regard to Table 10, the number of subjects for the SVM based studies was small compared with other works [34,35]. Lourenço et al. has shown that SVM can be a powerful method for ECG-based recognition in small number of subjects [34]. The current methodology outperforms for a large number of single-session in resting status while the recognition performance is mostly degraded by increasing number of subjects. It seems that discarding of the non-significant features such as noise and redundant features can affect the SVM recognition performance which includes classification accuracy and computational efficiency.

As experimental results shown in Table 10, the performance of current framework is comparable with study [28] for 52 subjects. The study [28] was demonstrated that the periodicity transform can be considerably useful for ECG analysis in order to outlier detection. On the other hand, the proposed method and the work [4] were only tested over 16 subjects in two recording sessions, which can be more considered in multiple sessions with large number of recordings for different postures and physiological conditions in future.

@&#CONCLUSION@&#

We proposed a new formulation for biometric authentication system based on the one-lead electrocardiogram signal using the kernel methods. Denoising on the noisy ECG signals (non-clinical source data) was performed initially by Discrete Wavelet Transform with coif3 at level 4, followed by an evaluation of feature extraction and classification. This study is based on non-fiducial approach to extract the ECG features of a population where there is a large number of redundant information. The experiments of verification recognition system have been tested over random unknown ECG data sets designed in biometric feature extraction phase. Autocorrelation method was applied to extract discriminative patterns with high dimensionality from the ECG windows according to the previous methodologies. A systematic analysis shows that the autocorrelation of ECGs and a Gaussian KPCA method as feature extractor has a maximum effect on achieving high subject and window recognition rates of Gaussian OAA SVMs compared to linear dimension reduction techniques (PCA, LDA). However, the computation time of SVM recognition models is almost identical for all dimension reduction techniques. The proposed method is feasible and offers a secure solution to ECG verification system to control a trade-off between False Match and False Nonmatch rates when dealing with highly unknown subjects. Further experiments are needed to optimise the individual recognition system based on kernel SVM methods and make it more robust and further improves classifier generalisation ability in ECG based biometric systems.

@&#ACKNOWLEDGEMENTS@&#

This research has been supported by the Ministry of Education Malaysia (MOE) through Fundamental Research Grant Scheme (FRGS) under project code 5524616. The authors would like to thank the University of Toronto to make an agreement with Universiti Putra Malaysia (UPM) for applying ECG database collected at the BioSec.Lab.

@&#REFERENCES@&#

