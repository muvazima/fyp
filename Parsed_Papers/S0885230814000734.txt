@&#MAIN-TITLE@&#A swarm-inspired re-ranker system for statistical machine translation

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We design and implement a novel reranker system for statistical machine translation, which it is used a swarm algorithm.


                        
                        
                           
                           We introduce sort of new features, which they can be computed easily from n-best list generated by SMT.


                        
                        
                           
                           We examine our system in English–Persian dataset.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Re-ranking system

Quantum-behaved particle swarm optimization

Perceptron

BLEU

@&#ABSTRACT@&#


               
               
                  Recently, re-ranking algorithms have been successfully applied on statistical machine translation systems. Due to the errors in the hypothesis alignment and varying word order between the source and target sentences and also the lack of sufficient resources such as parallel corpora, decoding may result in ungrammatical or non-fluent outputs. This paper proposes a re-ranking system based on swarm algorithms, which makes the use of sophisticated non-syntactical features to re-rank the n-best translation candidates. We introduce plenty of easy-computed non-syntactical features to deal with SMT system errors plus the quantum-behaved particle swarm optimization (QPSO) algorithm to adjust the weights of features. We have evaluated the proposed approach on 2 translation tasks in different language pairs (Persian→English and German→English) and genres (news and novel books). In comparison with PSO-, GA-, Perceptron- and averaged Perceptron-style re-ranking systems, the experimental study demonstrates the superiority of the proposed system in terms of translation quality on both translation tasks. In addition, the impacts of the proposed features on the translation quality have been analyzed, and the most positive ones have been recognized. At the end, the impact of the n-best list size on the proposed system is investigated.
               
            

@&#INTRODUCTION@&#

Statistical Machine Translation (SMT) is a well-known approach to automatic translation based on the statistical models. These systems are the current paramount research projects for machine translation which have attracting remarkable commercial interest in recent years (Specia, 2013). Despite the remarkable advances in SMT system, its output often contains diverse errors. The errors could be lack of a main verb, wrong word order, wrong lexical choice, missing content words, incorrect punctuation and so on (Och et al., 2004a,b).

In SMT systems, performance improvements could be achieved by using a re-ranker system with two processing steps (Carter and Mon, 2011; Federico and Bertoldi, 2005). First, a decoder provides an n-best list of translation candidates. Next, the final translation is recognized by re-scoring and re-ranking the N-best list through additional scores. The scores are computed thereby using more sophisticated features. The additional features indeed reward better translations found among the n-best list entries of the decoder.

The present study aims at dealing with the aforementioned problems by exploring a variety of new sophisticated features and introducing a new learning algorithm based on QPSO to re-rank and re-score the translation candidates. The proposed features are estimated directly and easily on the language and translation models, n-best list of translation candidates and part-of-speech tags. They are classified into six specific groups; i.e., (1) Language model features set which consists of an n-gram and an adaptive n-best list language model features. (2) Translation model feature set. (3) N-best list feature set which contains an n-best source/target position word feature, and a Levenshtein distance feature. (4) Length feature set. (5) Anchor matcher and Part-of-Speech tag feature sets. (6) Morphological feature set.

The proposed re-ranking system makes use of a scoring function to assess the translation candidates. The scoring function returns the weighted feature scores optimized by a development data set. A popular method for adjusting the features weights is to apply swarm optimization algorithm because it can be utilized relatively easy and it is applicable to a very wide range of problems (Sun, 2004a,b). Therefore, our re-ranking system is based on quantum-behaved particle swarm optimization (QPSO), a form of particle swarm optimization algorithm. QPSO is introduced by integrating the classical PSO philosophy and quantum mechanics in order to enhance performance of PSO (Sun, 2004a,b). It has been shown that QPSO outperforms original Particle Swarm Optimization (PSO) considerably on several widely known benchmark functions (Sun, 2004a,b; Sun et al., 2005a,b). In QPSO, the only setting parameter is contraction-expansion coefficient, which is gradually decreased with the number of iterations (Sun et al., 2005a,b). Using a new re-ranking system with the sophisticated features and global-convergence algorithm, QPSO, to tune feature weights would have a positive impact on translation quality.

Two translation tasks with different language pairs (Persian→English and German→English) and genres (news and novel books) have been used, and several evaluations have been performed to imply the accuracy and efficiency of our re-ranking system. The evaluation scenarios are described as follows. (1) The performance of our system has been compared to the well-known algorithms such as PSO, GA, Perceptron- and averaged Perceptron. The results illustrate the superiority of our approach in terms of translation quality. The results show an increase about 1.03 BLEU score over the baseline on Persian→English translation task and 1.73 on German→English translation task. (2) The impacts of the proposed feature sets have been computed. The contribution of features in the re-ranking system has been reported and the effect of each feature is successively computed. (3) The impact of the n-best list size has been examined. The findings of experimental studies indicate that using more than 1000-best list does not improve the translation quality significantly.

Generally, we believe the actual new contributions of our work are the extension of previously introduced features to the sophisticated, easy-computed and non-syntactical feature sets, plus the use of the new optimization algorithm to adjust the features weights.

The paper is organized as follows. In Section 2, the parameter estimation algorithms are introduced. In Section 3 and Section 4, the features and experimental studies are presented. Related works are reviewed in Section 5. Finally, we draw some conclusions in Section 6.

Applying new models in a re-ranking system to achieve better results is a standard technique in SMT (Kumar and Byrne, 2004). In a re-ranking approach, SMT outputs a list of the top n translation candidates (an n-best list) and then the re-ranking algorithms take these n-best lists and re-rank them according to a weight vector and a function f(·), which maps a sentence into a feature space. At the end, the best hypothesis is then returned as a selected translation. Different weighting algorithms such as the Perceptron and averaged Perceptron to estimate the weight vector w are widely used in NLP applications (Carter and Mon, 2011; Liang et al., 2006b; Shen and Joshi, 2005; Tillmann and Zhang, 2006). However, because of the advantages of QPSO (Sun, 2004a,b) that lies on its simple concept, easy implementation and quick convergence, we decided to implement QPSO-based re-ranking system. In the next subsection, we intent to describe this algorithm in more details.


                        Sun (2004b) proposed a global convergence-guaranteed swarm based search technique, quantum-behaved particle swarm optimization algorithm (QPSO), whose performance is superior to the standard PSO.

The standard PSO optimizes a problem by moving its particles around in the solution space according to the particle's position and velocity. Because of the limited velocity, the solution space of the standard PSO is a restricted area and it cannot explore the entire feasible space. Therefore, the algorithm could not guarantee convergence to global optimal solution (Sun et al., 2012).

In QPSO with M individuals, a partial solution to an optimization problem is shown as a particle in D dimensional search space, with the position X
                        
                           i
                        
                        =(x
                        
                           i1,x
                        
                           i2,…,x
                        
                           iD
                        ). The best previous value of each particle is registered (the position giving the best fitness value) as pbesti
                        =(pbest
                        
                           i1,pbest
                        
                           i2,…,pbest
                        
                           iD
                        ) called personal best position. At each iteration, each particle competes with its neighbors or in the whole population for the best particle (with best fitness value entire search space) with best position gbest
                        
                           i
                        
                        =(gbest
                        
                           i1,gbest
                        
                           i2,…,gbest
                        
                           iD
                        ) called global best position.

In QPSO, the particles move according to the following iterative equation:
                           
                              (1)
                              
                                 
                                    
                                       x
                                       
                                          i
                                          d
                                       
                                    
                                    (
                                    t
                                    +
                                    1
                                    )
                                    =
                                    
                                       
                                          
                                             g
                                             
                                                i
                                                d
                                             
                                          
                                          ±
                                          β
                                          |
                                          m
                                          b
                                          e
                                          s
                                          
                                             t
                                             d
                                          
                                          −
                                          
                                             x
                                             
                                                i
                                                d
                                             
                                          
                                          (
                                          t
                                          )
                                          |
                                          ln
                                          (
                                          1
                                          /
                                          u
                                          )
                                       
                                    
                                 
                              
                           
                        where
                           
                              (2)
                              
                                 
                                    
                                       g
                                       
                                          i
                                          d
                                       
                                    
                                    =
                                    φ
                                    .
                                    p
                                    b
                                    e
                                    s
                                    
                                       t
                                       
                                          i
                                          d
                                       
                                    
                                    +
                                    (
                                    1
                                    −
                                    φ
                                    )
                                    g
                                    b
                                    e
                                    s
                                    
                                       t
                                       d
                                    
                                 
                              
                           
                        and
                           
                              (3)
                              
                                 
                                    m
                                    b
                                    e
                                    s
                                    
                                       t
                                       d
                                    
                                    =
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       m
                                    
                                    
                                       
                                          P
                                          
                                             i
                                             d
                                          
                                       
                                    
                                    /
                                    m
                                 
                              
                           
                        
                        mbest is defined as the mean value of all particles’ best position, 
                           φ
                         and u are random numbers distributed uniformly on [0,1], respectively, and m is the number of particles. The parameter, 
                           β
                        , called contraction-expansion coefficient, is the only parameter in QPSO algorithm.

In order to utilize QPSO to adjust features weights, we should primarily determine an encoding of particle's position and a fitness function, and then introduce an algorithm that updates particles position according to QPSO manner and fitness function.

In this work, the particle's position is made of features weights. Fig. 1
                            shows the particle's position encoding.

Another important factor is a fitness function, which checks the quality of each potential solution (particles). QPSO tries to minimize the fitness function. In this work, the fitness function is scored by Eq. (4) as follows:
                              
                                 (4)
                                 
                                    
                                       f
                                       i
                                       t
                                       n
                                       e
                                       s
                                       s
                                       (
                                       
                                          w
                                          →
                                       
                                       )
                                       =
                                       
                                          1
                                          
                                             B
                                             L
                                             E
                                             U
                                             (
                                             F
                                             i
                                             r
                                             s
                                             t
                                             B
                                             e
                                             s
                                             t
                                             (
                                             
                                                X
                                                ¯
                                             
                                             ,
                                             
                                                w
                                                →
                                             
                                             )
                                             )
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 X
                                 ¯
                              
                            is the set of the n-best lists, and BLEU(·) function calculates the BLEU metric of the first best sentences with respect to the references in the development set. 
                              
                                 F
                                 i
                                 r
                                 s
                                 t
                                 B
                                 e
                                 s
                                 t
                                 (
                                 
                                    X
                                    ¯
                                 
                                 ,
                                 
                                    w
                                    →
                                 
                                 )
                              
                            also determines the first best sentences of the n-best lists, 
                              
                                 X
                                 ¯
                              
                           , regarding to the weight vector 
                              
                                 w
                                 →
                              
                           . Eq. (5) shows FirstBest function:
                              
                                 (5)
                                 
                                    
                                       F
                                       i
                                       r
                                       s
                                       t
                                       B
                                       e
                                       s
                                       t
                                        
                                       (
                                       X
                                       ,
                                        
                                        
                                       w
                                       )
                                       :
                                        
                                       ∀
                                       x
                                        
                                       ∈
                                       X
                                        
                                        
                                        
                                        
                                       arg
                                        
                                        
                                       
                                          
                                             max
                                          
                                          
                                             (
                                             
                                                x
                                                i
                                             
                                             ∈
                                             x
                                             )
                                          
                                       
                                        
                                        
                                        
                                       f
                                       (
                                       
                                          x
                                          i
                                       
                                       )
                                       .
                                       w
                                    
                                 
                              
                           where x
                           
                              i
                            is a hypotheses in the n-best list x and the feature extractor f(·) is a vector of n functions f
                           =(f
                           1,…,f
                           
                              n
                           
                           ), and each function f
                           
                              j
                           (y) maps a feature y to a real value f
                           
                              j
                           (y). All utilized features will be explained more in Section 3.

In order to introduce the new scoring function (learning a new classifier), QPSO firstly initializes all features weights randomly and then updates them according to the evaluation of the best sentences found by FirstBest function with the help of BLEU metric. Generally, QPSO iterates over the n-best lists in a sequential manner and finds the best scoring function to select the best hypothesis in terms of quality and fluency. The following is a procedure of QPSO for the re-ranking system.
                              
                                 
                                    Step 1: Initialize the population by randomly generating the position vector W
                                    i of each particle and set pbesti
                                    =
                                    W
                                    i.


                                    Step 2: Run FirstBest function (Eq. (5)) to find the best sentence with respect to the position vector Wi for each n-best list in 
                                       
                                          X
                                          ¯
                                       
                                     and evaluate the fitness value of each particle by Eq. (4),


                                    Step 3: Update the personal best position (pbesti) and obtain the global best position (gbest) across the population.


                                    Step 4: If the stop criteria are met, go to step 6; or else go to step 5.


                                    Step 5: Update the position vector of each particle (W
                                    i) according to Eq. (1) and go to step 2.


                                    Step 6: Output the gbest as the best feature weights.

Two stop criteria based on maximum number of iterations and oracle best sentences have been employed. Eq. (6) shows the stop criterion based on oracle best sentences.
                              
                                 (6)
                                 
                                    
                                       s
                                       B
                                       L
                                       E
                                       U
                                       (
                                       O
                                       r
                                       a
                                       c
                                       l
                                       e
                                       (
                                       X
                                       )
                                       )
                                       -
                                       s
                                       B
                                       L
                                       E
                                       U
                                       (
                                       F
                                       i
                                       r
                                       s
                                       t
                                       B
                                       e
                                       s
                                       t
                                       (
                                       X
                                       ,
                                        
                                        
                                       W
                                       )
                                       )
                                       <
                                       ε
                                    
                                 
                              
                           
                        


                           Oracle(
                              
                                 X
                                 ¯
                              
                           ) determines the best translation (oracle sentence) for each n-best lists (x
                           
                              i
                           ). Note that BLEU does not work at the sentence level. Several sentence-level implementations of BLEU known as smoothed BLEU have been proposed (Liang et al., 2006b; Lin and Och, 2006). For example, in (Liang et al., 2006b; Lin and Och, 2006), a smoothed BLEU, illustrated in Eq. (7), has been proposed. In the proposed system, we utilize this measure as sentence level BLEU.
                              
                                 (7)
                                 
                                    
                                       s
                                       B
                                       L
                                       E
                                       U
                                       =
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                       
                                       
                                          
                                             
                                                B
                                                L
                                                E
                                                
                                                   U
                                                   i
                                                
                                                (
                                                c
                                                a
                                                n
                                                d
                                                ,
                                                r
                                                e
                                                f
                                                )
                                             
                                             
                                                
                                                   2
                                                   
                                                      4
                                                      −
                                                      i
                                                      +
                                                      1
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

The only setting parameter of QPSO is the contraction-expansion coefficient (β) which is gradually decreased for the interval (0, 1] with the number of iterations.

In order to introduce new features aimed at improving translation quality of SMT systems, first of all the essential problems of SMT system should be identified, and then several features are introduced to deal with SMT system errors. In the following subsection, we intend to explain typical SMT errors.

A Phrase-based SMT (Koehn et al., 2007) has been trained by 400K sentence pairs of an English-Persian parallel corpus
                           1
                        
                        
                           1
                           For other language pairs, see Vilar et al. (2006) and Llitj’os (2004).
                         called TPC
                           2
                        
                        
                           2
                           Tehran Parallel Corpus.
                         (Mansouri and Faili, 2012), and its output have been analyzed in more details. By exploring 400 translated sentences emerged randomly from TPC, 7 classes of main errors have been recognized according to the error typology presented in (Vilar et al., 2006). The complete error statistics are shown in Fig. 2
                        .

As shown in Fig. 2, the errors have been classified into 7 main error groups. The “others” contains the “Unknown words”, “Incorrect style” and “Incorrect idioms” errors (Vilar et al., 2006). The error groups are described as follow:
                           
                              •
                              
                                 Lack of main verb: 50% of sentences were detected problematic merely due to the lack of main verb in Persian sentences. Two sorts of errors were often committed. There was no verb in the translated sentence, or there was a defected verb in the translated sentence because of the absence of verbal part in the compound verbs. The verbal part of compound verbs in Persian are usually aligned wrongly to their corresponding English sides.


                                 Wrong word order: The percentage of errors made due to the wrong word order equaled 27.7%. This is the case due to the fact that there are many differences between English and Persian sentences in word order. Persian sentences use SOV (Subject (S), Object (O) and Verb (V)) word order whereas English sentences use SVO structure. Also, the modifier comes after the modified word in Persian whereas English is vice versa. The local word reordering as a short distance reordering could be handled by the phrase and language models. But the non-local word reordering is still an open problem. Neither the translation nor the language model is able to handle a long distance reordering problem.


                                 Wrong lexical choice: This class of error was caused by the wrong lexical choice (66.8%). It was found when the translation system did not find the correct translation of a given word (Duh et al., 2010). Usually, the lack of sufficient bilingual resources is the main reason. Lexical granularity gaps between two languages may intensify the problem, too.


                                 Incorrect form of words: This kind of error was caused when the translation system was not able to produce the correct form of a word, although the translation of the base form was correct. This category of errors considers verb tense, verb person and concordance problem between noun and verb in term of number. The percentage of errors due to the incorrect form of words amount to 31.9%. This sort of error becomes more common when the system deals with a highly inflectional language, like Persian.


                                 Missing function words and punctuations: The errors due to the missing function word and punctuation together amount to 33.3%. This sort of errors was caused by the absence of suitable function words and punctuations in the translated sentences. It also occurred when the translation system was unable to translate the function words like “
                                    
                                 /che/what”, “
                                    
                                 /ke/that” or to find correct punctuations like “:”, “;” and so on.

Almost all levels of probable errors have been explained in more details by Vilar et al. (2006).

The aim of introducing additional features is to deal with the aforementioned problems by exploring a variety of new sophisticated features to reward better translations found among the n-best entries of the decoder. Some features such as the language model, adaptive language model, translation model and so on are borrowed from the previous publications (Carter and Monz, 2010; Hildebrand and Vogel, 2008; Liang et al., 2006a; Och et al., 2004a,b) whereas the plenty of easy-computed and non-syntactical features such as the n-best list feature set, anchor and punctuation matcher feature set and so on have been introduced in this paper. According to the feature characteristics, the features can be classified into six sets as follows.

A language model feature set, which includes a language model and an adaptive language model feature, has been explained as follows:

A language model is one of two main models in the decoding phase of SMT system which is used to promote fluency in translation. Using high order n-grams is useful to control long distance dependencies in the target sentence. It is borrowed from (Och et al., 2004a,b).

An adaptive language model feature is another member of the language model feature set. This feature is calculated through the n-best list n-grams probabilities. The counts of n-grams are collected on the n-best list entries for one source sentence only. The aim of defining this feature is to dedicate the importance of n-grams repeated more than others in the n-best list, and to overcome the wrong lexical choice and wrong word order problems. This feature is calculated as follows:
                              
                                 (8)
                                 
                                    
                                       s
                                       c
                                       o
                                       r
                                       e
                                       (
                                       
                                          E
                                          s
                                       
                                       )
                                       =
                                       log
                                       (
                                       ∏
                                       
                                          
                                             p
                                             
                                                n
                                                −
                                                b
                                                e
                                                s
                                                t
                                             
                                          
                                          (
                                          
                                             e
                                             i
                                          
                                          |
                                          
                                             e
                                             
                                                i
                                                −
                                                n
                                                +
                                                1
                                             
                                             
                                                i
                                                −
                                                1
                                             
                                          
                                          )
                                       
                                       )
                                    
                                 
                              
                           
                           
                              
                                 (9)
                                 
                                    
                                       
                                          p
                                          
                                             n
                                             −
                                             b
                                             e
                                             s
                                             t
                                          
                                       
                                       (
                                       
                                          e
                                          i
                                       
                                       |
                                       
                                          e
                                          
                                             i
                                             −
                                             n
                                             +
                                             1
                                          
                                          
                                             i
                                             −
                                             1
                                          
                                       
                                       )
                                       =
                                       
                                          
                                             c
                                             o
                                             u
                                             n
                                             
                                                t
                                                
                                                   n
                                                   −
                                                   b
                                                   e
                                                   s
                                                   t
                                                
                                             
                                             (
                                             
                                                e
                                                i
                                             
                                             ,
                                             
                                                e
                                                
                                                   i
                                                   −
                                                   n
                                                   +
                                                   1
                                                
                                                
                                                   i
                                                   −
                                                   1
                                                
                                             
                                             )
                                          
                                          
                                             c
                                             o
                                             u
                                             n
                                             
                                                t
                                                
                                                   n
                                                   −
                                                   b
                                                   e
                                                   s
                                                   t
                                                
                                             
                                             (
                                             
                                                e
                                                
                                                   i
                                                   −
                                                   n
                                                   +
                                                   1
                                                
                                                
                                                   i
                                                   −
                                                   1
                                                
                                             
                                             )
                                          
                                       
                                    
                                 
                              
                           where E
                           
                              s
                            is sth hypothesis. Note that in our re-ranking system, n is set to 6. It is borrowed from (Hildebrand and Vogel, 2008).

By getting help from a statistical word-to-word translation model, a probabilistic dictionary, the word translation log probability of each entry is calculated as translation feature score. It is borrowed from (Och et al., 2004a,b). This feature is estimated as follows:
                           
                              (10)
                              
                                 
                                    s
                                    c
                                    o
                                    r
                                    e
                                    (
                                    
                                       E
                                       s
                                    
                                    ,
                                    F
                                    )
                                    =
                                    log
                                    (
                                    
                                       1
                                       
                                          (
                                          N
                                          (
                                          F
                                          )
                                          +
                                          1
                                          )
                                       
                                    
                                    
                                       ∏
                                       i
                                    
                                    
                                       
                                          ∑
                                          j
                                       
                                       
                                          t
                                          (
                                          
                                             e
                                             i
                                          
                                          |
                                          
                                             f
                                             j
                                          
                                       
                                       )
                                    
                                    )
                                 
                              
                           
                        where E
                        
                           s
                         and F are target and source sentences, respectively. e
                        
                           i
                         and f
                        
                           j
                         are ith target and jth source words, respectively. t(·) is a word to word translation probability and N(F) is number of words of the source sentence.

The translation and language model features provide a rough guide for translation, but they are far too coarse to fix specific mistakes. Therefore, the re-ranking system is equipped by several sophisticated features. The n-best list features set is one of them which utilizes information provided by the n-best list. It consists of three creative features. The wrong word order, especially the global word reordering, and the wrong lexical choice problems can be addressed by such a feature set.

This feature is estimated by the probability of the occurrence of a word in a special position of the target sentence. This feature is calculated as follows.
                              
                                 (11)
                                 
                                    
                                       s
                                       c
                                       o
                                       r
                                       e
                                       (
                                       
                                          E
                                          s
                                       
                                       )
                                       =
                                       log
                                       (
                                       
                                          ∏
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             N
                                             (
                                             
                                                E
                                                s
                                             
                                             )
                                          
                                       
                                       
                                          
                                             p
                                             
                                                n
                                                −
                                                b
                                                e
                                                s
                                                t
                                             
                                          
                                          (
                                          
                                             e
                                             i
                                          
                                          |
                                          i
                                          )
                                          )
                                       
                                    
                                 
                              
                           where N(E
                           
                              s
                           ) is the length of hypothesis E
                           
                              s
                           , e
                           
                              i
                            is ith word of E
                           
                              s
                            and p
                           
                              n-best
                           (e
                           
                              i
                           
                           |i) is the probability of the occurrence of e
                           
                              i
                            in ith position of E
                           
                              s
                            that is calculated as follows:
                              
                                 (12)
                                 
                                    
                                       p
                                       (
                                       e
                                       |
                                       i
                                       )
                                       =
                                       
                                          1
                                          
                                             
                                                N
                                                h
                                             
                                             (
                                             F
                                             )
                                          
                                       
                                       
                                          ∑
                                          
                                             h
                                             =
                                             1
                                          
                                          
                                             
                                                N
                                                h
                                             
                                             (
                                             F
                                             )
                                          
                                       
                                       
                                          γ
                                          (
                                          
                                             e
                                             
                                                i
                                                ,
                                                h
                                             
                                          
                                          ,
                                          e
                                          )
                                       
                                    
                                 
                              
                           where N
                           
                              h
                           (F) is the number of hypotheses generated for source sentence F and e
                           
                              i,h
                            is ith word of hth hypothesis in the n-best list. 
                              
                                 γ
                                 (
                                 
                                    e
                                    
                                       i
                                       ,
                                       h
                                    
                                 
                                 ,
                                 e
                                 )
                              
                            also is defined as follows.(13)
                              
                                 γ
                                 (
                                 
                                    e
                                    
                                       i
                                       ,
                                       h
                                    
                                 
                                 ,
                                 e
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             1
                                              
                                             
                                                e
                                                
                                                   i
                                                   ,
                                                   h
                                                
                                             
                                             =
                                             e
                                          
                                       
                                       
                                          
                                             0
                                              
                                             
                                                e
                                                
                                                   i
                                                   ,
                                                   h
                                                
                                             
                                             ≠
                                             e
                                          
                                       
                                    
                                 
                              
                           
                        

The aim of this feature is to fight with the wrong lexical choice problems.

The agreement score of a word f occurring in position i (the source sentence) and ith translations occurring in the position j (the target sentence) is calculated as the relative frequency of the N
                           
                              h
                            translation hypotheses in the n-best list. (j,f
                           
                              i
                           ) denotes the translation of f
                           
                              i
                            (ith source word) occurs in the position j (the target sentence). This feature tries to capture how many entries in the n-best list are on the same word order. Fig. 3
                            shows the different alignments of a source word in the n-best list. It is designed to overcome the wrong word order problem.

The feature is calculated as follows.
                              
                                 (14)
                                 
                                    
                                       s
                                       c
                                       o
                                       r
                                       e
                                        
                                       (
                                       
                                          E
                                          s
                                       
                                       ,
                                       F
                                       )
                                       =
                                       log
                                       (
                                       
                                          ∏
                                          
                                             j
                                             =
                                             0
                                          
                                          
                                             N
                                             (
                                             
                                                E
                                                s
                                             
                                             )
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             
                                                N
                                                (
                                                F
                                                )
                                             
                                          
                                          
                                             
                                                p
                                                
                                                   n
                                                   −
                                                   b
                                                   e
                                                   s
                                                   t
                                                
                                             
                                             (
                                             j
                                             |
                                             
                                                f
                                                i
                                             
                                             )
                                          
                                       
                                       )
                                    
                                 
                              
                           where N(F) and N(E
                           
                              s
                           ) are the length of the source and the length of sth hypothesis, respectively. p
                           
                              n-best
                           (j|f
                           
                              i
                           ) is the probability of the occurrence of the translation of f
                           
                              i
                            in j target position. Note that because of the existence of NULL alignments, j must be started by 0.
                              
                                 (15)
                                 
                                    
                                       
                                          p
                                          
                                             n
                                             −
                                             b
                                             e
                                             s
                                             t
                                          
                                       
                                       (
                                       j
                                       |
                                       
                                          f
                                          i
                                       
                                       )
                                       =
                                       
                                          1
                                          
                                             
                                                N
                                                h
                                             
                                             (
                                             F
                                             )
                                          
                                       
                                       
                                          ∑
                                          
                                             k
                                             =
                                             1
                                          
                                          
                                             
                                                N
                                                h
                                             
                                             (
                                             F
                                             )
                                          
                                       
                                       
                                          γ
                                          (
                                          a
                                          l
                                          i
                                          g
                                          
                                             n
                                             k
                                          
                                          (
                                          
                                             f
                                             i
                                          
                                          )
                                          ,
                                          j
                                          )
                                       
                                    
                                 
                              
                           where align
                           
                              k
                           (f
                           
                              i
                           ) returns the target position that f
                           
                              i
                            has been aligned to and 
                              
                                 γ
                                 (
                                 i
                                 ,
                                 j
                                 )
                              
                            denotes as follows.
                              
                                 (16)
                                 
                                    
                                       γ
                                       (
                                       i
                                       ,
                                       j
                                       )
                                       =
                                       
                                          
                                             
                                                
                                                   1
                                                    
                                                   i
                                                   =
                                                   j
                                                
                                             
                                             
                                                
                                                   0
                                                    
                                                   i
                                                   ≠
                                                   j
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

The Levenshtein distance is a measure of similarity between two strings (Snover et al., 2006). It calculates the similarity between one hypothesis and other hypotheses in the n-best list.
                              
                                 (17)
                                 
                                    
                                       s
                                       c
                                       o
                                       r
                                       e
                                        
                                       (
                                       
                                          E
                                          s
                                       
                                       ,
                                       F
                                       )
                                       =
                                       log
                                       (
                                       
                                          
                                             N
                                             (
                                             F
                                             )
                                          
                                          
                                             
                                                ∑
                                                
                                                   
                                                      E
                                                      j
                                                   
                                                   ∈
                                                   n
                                                   −
                                                   b
                                                   e
                                                   s
                                                   t
                                                   l
                                                   i
                                                   s
                                                   t
                                                   (
                                                   F
                                                   )
                                                
                                             
                                             
                                                L
                                                D
                                                (
                                                
                                                   E
                                                   s
                                                
                                                ,
                                                
                                                   E
                                                   j
                                                
                                                )
                                             
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        

where LD(E
                           
                              s
                           ,E
                           
                              j
                           ) calculates the Levenshtin distance between E
                           
                              s
                            and E
                           
                              j
                           .

It contains the hypothesis length and average hypothesis length features. It is borrowed from (Carter and Monz, 2010).

The ratio of the target sentence length to the source sentence length is determined by the Poisson distribution with specific 
                              λ
                            (Meng et al., 2009). Eq. (18) shows the Poisson distribution.
                              
                                 (18)
                                 
                                    
                                       
                                          p
                                          
                                             p
                                             o
                                             i
                                             s
                                             s
                                             o
                                             n
                                          
                                       
                                       (
                                       J
                                       |
                                       I
                                       )
                                       =
                                       
                                          
                                             
                                                λ
                                                J
                                             
                                             
                                                e
                                                
                                                   −
                                                   λ
                                                
                                             
                                          
                                          
                                             (
                                             J
                                             )
                                             !
                                          
                                       
                                    
                                 
                              
                           where J and I are the lengths of the target and source sentences, respectively. With the help of maximum likelihood estimation, 
                              λ
                            is calculated by the maximum likelihood estimation method (see (Meng et al., 2009) for more details).
                              
                                 (19)
                                 
                                    
                                       λ
                                       =
                                       
                                          
                                             ∑
                                             
                                                
                                                   I
                                                   s
                                                
                                             
                                          
                                          
                                             ∑
                                             
                                                
                                                   J
                                                   s
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

where J
                           
                              s
                            and I
                           
                              s
                            are sth target and source sentence of the parallel corpus, respectively. Investigating 400k sentence pairs in English-Persian parallel corpus showed that 
                              
                                 λ
                                 =
                                 1.1
                              
                           . Therefore, Eq. (20) calculates the target sentence score as follows:
                              
                                 (20)
                                 
                                    
                                       s
                                       c
                                       o
                                       r
                                       e
                                       (
                                       
                                          E
                                          s
                                       
                                       ,
                                       F
                                       )
                                       =
                                       log
                                       (
                                       
                                          p
                                          
                                             p
                                             o
                                             i
                                             s
                                             s
                                             o
                                             n
                                          
                                       
                                       (
                                       N
                                       (
                                       
                                          E
                                          s
                                       
                                       )
                                       |
                                       N
                                       (
                                       F
                                       )
                                       )
                                    
                                 
                              
                           
                        

The feature scores a translated sentence according to its length and average hypotheses length. Eq. (21) calculates the feature score.
                              
                                 (21)
                                 
                                    
                                       s
                                       c
                                       o
                                       r
                                       e
                                        
                                       (
                                       
                                          E
                                          s
                                       
                                       ,
                                       F
                                       )
                                       =
                                       log
                                       (
                                       
                                          1
                                          
                                             |
                                             N
                                             (
                                             
                                                E
                                                s
                                             
                                             )
                                             −
                                             
                                                N
                                                ¯
                                             
                                             (
                                             F
                                             )
                                             |
                                             +
                                             1
                                          
                                       
                                       )
                                    
                                 
                              
                           where 
                              
                                 
                                    N
                                    ¯
                                 
                                 (
                                 F
                                 )
                              
                            is the average length of translated hypotheses of the source sentence F.

This feature leads to sentences whose lengths are close to the average length of hypotheses.

One of the main problems of SMT systems is missing the function words and punctuations. Therefore, the anchor matcher feature set tries to overcome the problem. This set includes Punctuation, Day, Month and Number matcher features as follows.

This feature is used to handle the punctuation mismatching problem in the source and target sentences. It is calculated by Eq. (22) as follows:
                              
                                 (22)
                                 
                                    
                                       s
                                       c
                                       o
                                       r
                                       e
                                       (
                                       
                                          E
                                          s
                                       
                                       ,
                                       F
                                       )
                                       =
                                       log
                                       (
                                       
                                          1
                                          
                                             
                                                N
                                                
                                                   p
                                                   u
                                                   n
                                                   c
                                                   (
                                                   F
                                                   )
                                                
                                             
                                          
                                       
                                       
                                          ∑
                                          
                                             
                                                
                                                   
                                                      
                                                         p
                                                         f
                                                      
                                                      ∈
                                                      p
                                                      u
                                                      n
                                                      c
                                                      (
                                                      F
                                                      )
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         p
                                                         e
                                                      
                                                      ∈
                                                      p
                                                      u
                                                      n
                                                      c
                                                      (
                                                      
                                                         E
                                                         s
                                                      
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                       
                                          γ
                                          (
                                          
                                             p
                                             e
                                          
                                          ,
                                          
                                             p
                                             f
                                          
                                          )
                                       
                                       )
                                    
                                 
                              
                           where punc(F) is a list of the punctuation tokens of F, and N
                           
                              punc
                           
                           (F) is the number of the punctuation tokens of F.

‘Day’, ‘Month’, ‘Number’ and ‘Pronoun’ matcher features are other similar matcher features calculated by Eq. (23).
                              
                                 (23)
                                 
                                    
                                       s
                                       c
                                       o
                                       r
                                       e
                                       (
                                       
                                          E
                                          s
                                       
                                       ,
                                       F
                                       )
                                       =
                                       log
                                       (
                                       
                                          1
                                          
                                             
                                                N
                                                
                                                   a
                                                   n
                                                   c
                                                   h
                                                   o
                                                   r
                                                
                                             
                                             (
                                             F
                                             )
                                          
                                       
                                       
                                          ∑
                                          
                                             
                                                
                                                   
                                                      
                                                         p
                                                         f
                                                      
                                                      ∈
                                                      a
                                                      n
                                                      c
                                                      h
                                                      o
                                                      r
                                                      (
                                                      F
                                                      )
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         p
                                                         e
                                                      
                                                      ∈
                                                      a
                                                      n
                                                      c
                                                      h
                                                      o
                                                      r
                                                      (
                                                      
                                                         E
                                                         s
                                                      
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                       
                                          γ
                                          (
                                          
                                             p
                                             e
                                          
                                          ,
                                          
                                             p
                                             f
                                          
                                          )
                                       
                                       )
                                    
                                 
                              
                           where anchor(·) can be ‘Day’ or ‘Month’ or ‘Number’ or ‘Pronoun’. Note that for matching ‘Day’ or ‘Month’ or ‘Number’ or ‘Pronoun’, we use a bilingual dictionary. These features can potentially overcome missing function words and punctuation problems.

A POS-based feature set includes a POS-based language model and a POS tag matcher feature. The features aim at overcoming the lack of the content words and wrong word order problems.

A POS-based language model feature is a discriminative feature shown by Eq. (24).
                              
                                 (24)
                                 
                                    
                                       s
                                       c
                                       o
                                       r
                                       e
                                       (
                                       
                                          E
                                          s
                                       
                                       )
                                       =
                                       log
                                       (
                                       ∏
                                       
                                          p
                                           
                                          (
                                          P
                                          O
                                          S
                                          (
                                          
                                             e
                                             i
                                          
                                          )
                                          |
                                          P
                                          O
                                          S
                                          (
                                          
                                             e
                                             
                                                i
                                                −
                                                n
                                                −
                                                1
                                             
                                             
                                                i
                                                −
                                                1
                                             
                                          
                                          )
                                          )
                                       
                                       )
                                    
                                 
                              
                           where POS(e
                           
                              i
                           
                           ) is a POS tag of e
                           
                              i
                           
                           . Note that in this project n is set to 6. The feature aims at improving the quality of SMT output in terms of grammar. It is borrowed from (Liang et al., 2006a).

This feature controls the existence of a special POS tag in both sides (source and target sides). In other words, the number of nouns, verbs, adjectives and adverbs in both sides should be the same. The difference between the number of nouns, verbs, adjectives and adverbs illustrates that the hypothesis may suffers from the lack of the content words. Therefore, this feature is employed to prefer the hypotheses that the number of the nouns, verbs, adjectives and adverbs of both sides are the same or somewhat similar.

State of the art SMT systems tend to perform poorly when translating into rich morphological languages. When multiple morphemes inflect a single word stem, the total lexicon of surface forms can be enlarged, and many surface forms may occur rarely in a text, leading to a significant data sparseness problem (Popovi et al., 2006). Therefore, the morphological analysis of verbs and nouns can be useful to find the better hypothesis in the n-best list. The person (first, second, third person), number (singular vs. plural), tense (present, past, future), complex tenses (complex present, complex past, complex future), mood (indicative, imperative, subjunctive, etc.) and voice (active vs. passive) are verb and noun properties that have been extracted via morphological analysis and considered in this feature set. The score of morphological feature set is calculated by using Eq. (25).
                           
                              (25)
                              
                                 
                                    s
                                    c
                                    o
                                    r
                                    e
                                    (
                                    
                                       E
                                       s
                                    
                                    ,
                                    F
                                    )
                                    =
                                    log
                                    (
                                    
                                       1
                                       
                                          
                                             N
                                             
                                                p
                                                r
                                                o
                                                p
                                                e
                                                r
                                                t
                                                y
                                             
                                          
                                          (
                                          F
                                          )
                                       
                                    
                                    
                                       ∑
                                       
                                          
                                             
                                                
                                                   
                                                      p
                                                      f
                                                   
                                                   ∈
                                                   p
                                                   r
                                                   o
                                                   p
                                                   e
                                                   r
                                                   t
                                                   y
                                                    
                                                   (
                                                   F
                                                   )
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      p
                                                      e
                                                   
                                                   ∈
                                                   p
                                                   r
                                                   o
                                                   p
                                                   e
                                                   r
                                                   t
                                                   y
                                                   (
                                                   
                                                      E
                                                      s
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                    
                                    
                                       γ
                                       (
                                       
                                          p
                                          e
                                       
                                       ,
                                       
                                          p
                                          f
                                       
                                       )
                                    
                                    )
                                 
                              
                           
                        where property (F) returns a list of the property of verbs and nouns of F, and N
                        
                           property
                        
                        (F) returns the number of the property of F. The goal of this feature is to assign higher score to the sentences so that a higher percentage of their nouns and verbs are correctly translated.

In order to compare the performance of the proposed algorithm with GA, PSO, Perceptron- and averaged Perceptron-style algorithms, and also to demonstrate the effectiveness of different features on the translation quality, an English→Persian and a German→English SMT systems have been trained by an English-Persian parallel corpus called TPC
                        3
                     
                     
                        3
                        Tehran Parallel Corpus.
                      (Mansouri and Faili, 2012) and Europarl (Koehn et al., 2005), respectively. Three evaluation scenarios have been performed with the help of two translation tasks: (1) The performance of the proposed approach has been compared to GA-, PSO- Perceptron- and averaged Perceptron-style systems. (2) The impact of feature sets has been estimated. (3) The impact of the n-best list size has been examined.

First, two translation tasks and system configurations are clarified in more details. Then the evaluation scenarios are explained and the results are analyzed.

Two translation tasks with different language pairs, sizes and even domains have been employed to evaluate the proposed approach. TT-EnFa is an English-Persian translation task which is trained by TPC (Mansouri and Faili, 2012). TPC is constructed by high quality English-Persian texts extracted from novel books. Table 1
                         reports some statistics about TPC. Moreover, PCTS
                           4
                        
                        
                           4
                           Parallel Corpus Test Set.
                         (Mansouri and Faili, 2012) is employed as development and test sets. It is a set of 400 Persian sentences which are randomly emerged from TPC and translated manually by 4 human language experts. TMC
                           5
                        
                        
                           5
                           The largest freely available monolingual corpus for Persian language – more than 250M words in total www:http:\\ece.ut.ac.ir\nlp\resources.html (emitted for blind reviewing).
                         (Mansouri and Faili, 2012) which is also free Persian monolingual corpus is utilized to build the target 3-gram language model using the SRILM toolkit with modified Kneser–Ney smoothing (Stolcke, 2002). TT-GeEn is another translation task which is built on German→English SMT system. It utilizes dataset from workshop 2007 on statistical machine translation (WMT07). The training data contains about 50K sentences of parallel news commentary and about 1.1M sentences of Europarl (Koehn et al., 2005). We also use about 1.3M sentences out of monolingual data for language model. Newstest2007 and Newstest2006 are employed to tune and test, respectively. Basic statistics about the translation tasks are reported in Table 1. Due to the high computational cost, we have to remove sentences with more than 50 words in length.

We select Moses as a baseline Phrase-based SMT (Koehn et al., 2007) which uses multiple stacks to generate translation candidates. The setting parameters are: stack size of 100, distortion limit of 6, and phrase table limit of 20. Alignments have been extracted by using the GIZA++ toolkit in words level (Och and Ney, 2003). Additionally, BLEU is used for scoring the translations (Papineni et al., 2002). All SMT systems have been optimized on development sets via minimum-error rate training (MERT) (Och, 2003) with an unique 100-best list and optimizing toward BLEU. The data set is split into 8 folds in which 7 folds are used as a development set (dev set) to optimize the setting parameters. The resulting setting is employed by the re-ranking system to re-rank the remaining fold. The algorithms are coded in C# and runs have been done on an Intel core 2 Dou CPU 1.5GHz computer with 2GB memory. All algorithms run on equivalent conditions. Specific parameter settings of the algorithms are described in Table 2
                        . It should be noted that the initial populations of all algorithms consist of random individuals. Each experiment (for each algorithm) was repeated 10 times with different random seeds.

In order to compare the performance of the Perceptron-, averaged Perceptron-, GA-, PSO- and QPSO- style algorithms, 100-best lists have been generated by SMT systems, and runs have been done with different random seeds on the development and test sets under the same conditions. Table 3
                         reports mean, standard deviation, the worst and best solution found over 10 trials. Because of the superiority of the Perceptron-style algorithms over the minimum error rate algorithm (Shen and Joshi, 2005), the minimum error rate algorithm is not compared.

In comparison to QPSO, other algorithms do not gain good results. As shown in Table 3, for TT-EnFa translation task, the best BLEU score of QPSO, PSO, GA, averaged Perceptron and Perceptron are about 32.19, 32.19, 32.19, 31.91 and 31.91, respectively. QPSO on average achieves +0.27, +0.67, +1.04, +1.03 and +1.09 point improvements in BLEU comparing to PSO, GA, averaged Perceptron, Perceptron and baseline, respectively. The standard deviation of solution values obtained by QPSO is also the smallest among all approaches. The findings of experiments on TT-GeEn have been shown that the best BLEU score of QPSO, PSO, GA, averaged Perceptron and Perceptron are about 22.42, 22.42, 21.73, 21.73, 21.73 and 21.73, respectively. QPSO on average achieves +0.42, +0.86, +1.17, +1.22 and +1.73 point improvements in BLEU comparing to PSO, GA, averaged Perceptron, Perceptron and baseline, respectively. The standard deviation of solution values obtained by QPSO and GA are about 0.09 whereas the standard deviations of other algorithms are above 0.1.In order to determine if any of the differences in the mean of the algorithms are significant, a t-test is performed over QPSO and the other algorithms. Table 4
                         shows p-value of paired t-test with 95% confidence interval of the differences. The results illustrate that the mean of QPSO to be statistically significantly better than the mean of the other algorithms.


                        Fig. 4
                         compares maximum iterations required to reach the best solution for all algorithms.

The results demonstrate the superiority of QPSO over all the other approaches in terms of maximum number of iterations required to reach the best solution.

In general, the comparison of algorithms’ performance establishes the superiorities of QPSO-style algorithm in terms of the mean and best solution quality over the well-known algorithms. As far as the standard deviations are concerned, QPSO and PSO outperformed the others. It implies that the proposed approach is robust. A comparison of maximum iteration required indicates that QPSO and PSO are better than others in terms of convergence speed. In comparison with other mentioned algorithms, it can be argued that the only advantage of the Perceptron and averaged Perceptron is to implement the easiest ones. We run experiments by Eq. (26) as fitness function in order to compare convergence speed of Eqs. (4) and (26) as fitness functions.
                           
                              (26)
                              
                                 
                                    f
                                    i
                                    t
                                    n
                                    e
                                    s
                                    s
                                    (
                                    
                                       w
                                       →
                                    
                                    )
                                    =
                                    1
                                    −
                                    B
                                    L
                                    E
                                    U
                                    (
                                    F
                                    i
                                    r
                                    s
                                    t
                                    B
                                    e
                                    s
                                    t
                                    (
                                    
                                       X
                                       ¯
                                    
                                    ,
                                    
                                       w
                                       →
                                    
                                    )
                                    )
                                 
                              
                           
                        where 
                           
                              X
                              ¯
                           
                         is the set of the n-best lists, and BLEU(·) function calculates the BLEU metric of the first best sentences with respect to the references in the development set. 
                           
                              F
                              i
                              r
                              s
                              t
                              B
                              e
                              s
                              t
                              (
                              
                                 X
                                 ¯
                              
                              ,
                              
                                 w
                                 →
                              
                              )
                           
                         also determines the first best sentences of the n-best lists, 
                           
                              
                                 X
                                 ¯
                              
                           
                        , regarding to the weight vector 
                           
                              w
                              →
                           
                        . As shown by Fig. 5
                        , the result shows that QPSO with Eq. (4) as fitness function has bigger speed of convergence that QPSO with Eq. (26) as fitness function. Eq. (4) is needed fewer iterations to achieve good approximation.

In order to explore the question which feature set contributes the most to the improvement in translation quality, we run baseline (mentioned by b
                           6
                        
                        
                           6
                           All of the used abbreviations are mentioned in Section 3.
                        ) with each feature set on the dev and test sets. Figs. 6 and 7
                        
                         show the results for TT-EnFa and TT-GeEn, respectively.


                        Table 5
                         also summarizes the impact of each feature sets.

The n-best list and language model feature sets for TT-EnFa and POS and language model feature sets for TT-GeEn have the most positive impact on translation quality whereas the translation feature set has the least impact for both because of using the low quality probabilistic dictionary. The probabilistic dictionary is built by IBM model 1 aligning algorithm. Since the n-best list generated by SMT usually consists of limited set of words with less morphological diversity, the impact of the morphological feature set is not salient. The morphological feature set plays more important role when translated candidates come from different machine translation with different lexicons. One of the most important problems in English→Persian SMT is that the translated Persian sentences are much shorter than English ones, which causes to get more penalties in BLEU measuring. To overcome the problem, the length feature set has been used. Fig. 8
                         also shows the accumulative impact of all feature sets.

As shown in Fig. 8, the piecemeal addition of features to the re-ranking system (forward selection) improves its quality. The most positive impact belongs to the language model (lm) whereas translation model (tm) has no effect. The punctuation matcher feature (punc_m) is the only member of the anchor matcher feature set indicating positive effect whereas the number (num_m) and day (day_m) matcher features show no effect on BLEU score because their effects have been covered by other features. Investigating the impact of n-best feature set members shows that the n-best target position word feature (nb_tp) and n-best source position word feature (nb_sp) have positive impact whereas the Levenshtein distance feature (id) has no effect. Note that the n-best source position word feature (nb_sp) has more positive impact than the n-best target position word feature (nb_tp). Both members of length feature set proved to be positive especially the hypothesis length feature.

The n-best list size is one of the most effective parameters in re-ranking system quality. To find the optimal size for the n-best list, the results of using list sizes from 50-best up to 2000-best have been compared. Fig. 9
                         shows BLEU score on different n-best list sizes. Fig. 10
                         shows the training time of re-ranking system. Fig. 11
                         also presents the running time of re-ranking system. By increasing the size of the n-best list, both the quality and training time of re-ranking system do increase. Based on our results, it is figured out that using an n-best list longer than 1000 entries does not help to improve the translation quality and the difference between using 1000- and 2000-best list is neglectable.

@&#RELATED WORKS@&#


                     Och et al. (2004a,b) employed nearly 450 syntactic features to re-rank 1000-best translation candidates using MERT optimized on BLEU. All features were based on syntactic analyses of the source and target sentences to address the grammaticality of the translations. In contrast to (Och et al., 2004a,b), Our features can be computed easily without any syntactic process. On the other hand, Och et al. (2004a,b) made use of minimum error rate training to optimize the feature weights while our system uses a swarm based algorithm (QPSO) to optimize the feature weights. Shen and Joshi (2005) also utilized the similar syntactic features but they employed the Perceptron-style algorithm to adjust the features weights. The use of Perceptron-style algorithms with millions of features for SMT has been explored by Arun and Koehn (2007). They introduced the use of online algorithms for the discriminative training of a phrase-based SMT system. In contrast to (Shen and Joshi, 2005) and (Tillmann and Zhang, 2006), the lexical features extracted from a parallel corpus have not been used in the proposed system because the training of millions of features needs too many training data and it is a very time consuming task. The data sparseness is an important problem that the learning algorithm faces. Liang et al.’s algorithm (2006) was based on the Perceptron-style training, which used a large number of features. They also tried to utilize the translation model features, supplemented by additional language model, lexical and part-of-speech features. Their approach focused on improving the tuning algorithm, rather than re-ranking. Our approach has similarities to the one presented by Liang et al. (2006a,b). However, we try to implement a re-ranking system for scarce resource languages without using lexical and structural features. Furthermore, QPSO has been employed to tune the feature weights. Yamada and Muslea (2009) trained a Perceptron-based classifier on millions of features extracted from n-best lists of size 200 of the entire training set for re-ranking, and computed BLEU on a sentence level rather than a corpus level. In contract to the Yamada's approach (2009), we use not only a small tuning (development) set but also corpus-level BLEU as a fitness function to train our classifier. In addition, we do not make use of lexical features extracted from a training data. Carter and Monz (2011) introduced Perceptron style system that exploits syntactic features for the n-best translation candidates re-ranking. They motivated the utility of syntax by presenting the superior performance of parsers over n-gram language models in differentiating SMT output and human translations. (Suzuki et al., 2011) proposed a novel distributed MERT framework based on PSO method aimed at providing a method to improve the experiment turn-around time for SMT system development. MERT utilizes n-best approximation method to deal with high computational cost of decoding phase. This method splits decoding and the parameter optimization phase into an outer and inner loop. The inner loop is used to adjust the weights based on n-best list which is produced by outer loop. They substituted the modified PSO method for the inner loop. Their method can provide parameter optimization for SMT system whereas keeping the translation quality. Similar to (Suzuki et al., 2011), in this paper particles are made of the features weights. In contrast to (Suzuki et al., 2011), we utilize an oracle-based method in order to determine the best translate candidates from the n-best list. We define the stop criterion based on difference between the oracle sentence found by smoothed BLEU and the first best sentence found by the algorithm. Therefore, we can say that our system utilizes not only a different fitness function but also a different stop criterion.

@&#CONCLUSION@&#

We have introduced a QPSO-based re-ranking system, which makes use of sophisticated, non-syntactical and easy-computed features to re-rank the n-best translation candidates produced by Phrase-based SMT system. In this paper, two key factors of a re-ranking system, features and a learning algorithm, have been targeted in order to improve the machine translation outputs. Easy-computing and effectiveness are two essential properties of features of a re-ranking system. Therefore, by investigating the effect of features, we propose a set of the sophisticated features estimated directly on the n-best list, POS tags, and language and translation models. These features can be easily computed for any scarce resource languages such as Persian, which suffers from the lack of useful language resources. In contrast to previous works, which use the Perceptron-style algorithm, a swarm-based algorithm called QPSO is employed as a learning algorithm. The experiments imply that the comparison of algorithms’ performance establishes the superiorities of QPSO-style algorithms in terms of translation quality over the well-known algorithms. A comparison of maximum iteration required also indicates that QPSO is better than others in terms of convergence speed. In addition, penalty of experiments have been carried out to determine which feature set contributes the most to the improvement in translation quality. From the investigation of the impact of the n-best list size on re-ranking system outputs we can observed that by increasing the size of the n-beat list, both of the quality and training time of the re-ranking system are increased whereas using more than 1000-best list does not improve the translation quality significantly.

@&#ACKNOWLEDGMENTS@&#

Iran National Science Foundation (INSF) is gratefully acknowledged for financial support of this work. The authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve the quality of the paper.

@&#REFERENCES@&#

