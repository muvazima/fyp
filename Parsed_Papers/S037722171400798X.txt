@&#MAIN-TITLE@&#Bankruptcy prediction using terminal failure processes

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We design traditional failure models using different classification methods.


                        
                        
                           
                           We estimate a set of failure processes.


                        
                        
                           
                           We design financial failure models that fit each process.


                        
                        
                           
                           We examine model accuracy using these two methods of designing models.


                        
                        
                           
                           Model performance is assessed using different samples and over different time horizons.


                        
                        
                           
                           Failure process-based models achieved better mid-term forecasts than traditional models.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Forecasting

Finance

Bankruptcy prediction

Failure processes

@&#ABSTRACT@&#


               
               
                  Traditional bankruptcy prediction models, designed using classification or regression techniques, achieve short-term performances (1 year) that are fairly good, but that often worsen when the prediction horizon exceeds 1 year. We show how to improve the performance of such models beyond 1 year using models that take into account the evolution of firm’s financial health over a short period of time. For this purpose, we design models that fit the underlying failure process of different groups of firms. Our results demonstrate that such models lead to better prediction accuracy at a 3-year horizon than that achieved with common models.
               
            

@&#INTRODUCTION@&#

For many years, a large number of studies have been focusing on how to improve the accuracy of failure models. Such models are used by financial institutions to assess their credit risk, that is to say the maximal credit loss they may be exposed to if their counterparties do not reimburse their debts. One of the components used to assess this loss is made up of the probability of default of each borrower, usually given a 1-year horizon. The accuracy of the estimation of this probability is a key issue for prudential purposes, since it allows financial institutions to determine the amount of capital needed to cover credit losses. But the accuracy of this estimation at a mid-term horizon (between 2 and 5 years) is also important since risks incurred by financial institutions still exist up until the maturity of the debts owed by their clients. However, most of the time, traditional models used to make mid-term forecasts lead to predictions whose accuracy decreases as the horizon of the prediction increases. One possible explanation of this weakness relies on the fact that these models consider failure based on an erroneous assumption. As these models rely on explanatory variables that are measured over a unique period of time, they assume that the bankruptcy process is the same for all companies, and as a consequence that the warning signs of failure occur in the same way, at the same moment and with the same magnitude for all firms (Laitinen, 1991). Reality, however, is slightly different. One knows that companies follow different strategies of decline, which explains why some firms will go bankrupt extremely quick although they appear to be in rather good health, some others slowly decline before going bankrupt, and yet others will manage to survive even though everything suggests they will not (D’Aveni, 1989).

To improve model ability to correctly forecast the fate of companies not solely at a 1-year horizon, but also at a 2- or 3-year horizon, or more, some authors attempted to take into account these strategies of decline using measures of firm’s financial health over several years. Some of them used measures of variation of financial indicators over time (Altman et al., 1977; Dambolena and Khoury, 1980). Others designed multi-period models (Berg, 2007; Gepp and Kumar, 2008) that used indicators that were measured over several consecutive years. However, all these works have not led to conclusive results since they did not manage to achieve mid-term forecasts that are as accurate as short-term ones. Still others (du Jardin and Séverin, 2011; 2012) attempted to represent the different paths that firms follow during their lifetime, called trajectories – some leading to bankruptcy, some others not – to forecast their fate. This time, the results are fairly good as the forecasts are rather stable over time. However, this method has a major drawback because it requires a significant amount of historical data; indeed, the models are designed using financial ratios that are collected over 7 consecutive years and, in the real world, it is not uncommon that financial institutions are not able to collect so much data, either because some companies which are of interest are too young, or because data are simply not readily available. This is the reason why we study another way of designing bankruptcy models, using less data than the method mentioned above, but that still relies on this notion of “trajectory”. Models rely on a set of “terminal failure processes”. These processes represent prototype behaviors of companies that are measured over solely 3 years. Among companies that share the same process, some will finally go bankrupt, some others will not. Once these processes are estimated, companies are classified into groups that share the same process, and then a set of bankruptcy models are designed, one model for each group, using traditional modeling methods (discriminant analysis, logistic regression, survival analysis and a neural network). Thereby, for each combination of failure process and each modeling method, a forecasting model is designed. Models are then used with test data to assess their prediction ability over three time horizons: 1, 2 and 3 years. Results are then compared to those achieved with traditional models commonly used.

The remainder of this paper is organized as follows. In Section 2, we present a literature review that explains our research question. In Section 3, we describe the samples and methods used in our study. In Section 4, we present and discuss the results and, in Section 5, we conclude.

@&#LITERATURE REVIEW@&#

Traditional failure models ensure optimal predictive ability when the forecasting horizon is short, and their accuracy decreases severely beyond 1 year. All studies conducted on this topic since Altman (1968) to date (Lin et al., 2014) clearly show that this is the case. Indeed, the more the horizon increases, the more these models are not be able to capture the different underlying patterns that characterize firms which will go bankrupt. Table 1
                        presents a list of studies, published between 2000 and 2014, that assessed model accuracy at a 1-, 2- and 3-year horizons. This table indicates a general trend where accuracy decreases as the horizon of a prediction increases. As it happens, Berg (2007), while studying this issue, confirms this relationship between model accuracy and the horizon of a prediction.

Several factors seem to explain the phenomenon. The first one lies in the way models are estimated and optimized. Indeed, in general, they are designed using data that are measured over a period t to achieve a prediction over a period t + 1, with an average lag of 1 year between t and t + 1. This time period actually materializes at the point in time when the difference between the distributions of data that characterize the two groups of companies (failed and non-failed) is the largest—therefore the instant when the discrimination between the two groups is the easiest (Beaver, 1966). As a consequence, if a model is built to forecast an event such as bankruptcy, with data that are collected solely 1 year before this event occurs, its optimal forecasting horizon cannot exceed this timeframe in the future. Therefore, intrinsically, models are ill-suited to make mid-term forecasts that are as accurate as short-term forecasts they are used to make. This is probably the reason why El Hennawy and Morris (1983) had the intuition that a model designed using a large time lag between the moment when model parameters are estimated and that when the prediction is achieved (5 years) might have greater informational content than a model built using a shorter lag (1 year). This is precisely the result they obtained.

The second factor is due to the fact that the relationships between independent variables and the dependent variable of a model are supposed to be stable over time (Zavgren, 1983). However, this assumption is not consistent with reality (Charitou et al., 2004) because conditions that govern firms’ economic environment (market competitive structure, technological cycle, inflation rate, growth rate, etc.), and that may strongly influence the relationships between variables (Mensah, 1984; Platt et al., 1994), are hardly completely stable. Indeed, some firms that might survive in a favorable economic environment are sometimes unable to do so when economic conditions worsen. This is why, when large macro-economic changes occur, it becomes much more difficult to forecast the fate of unsound firms than that of sound companies (Pompe and Bilderbeek, 2005). Actually, the larger the time interval between the period during which a model is designed and that when it is used, the more models are likely to be influenced by macro-economic changes that affect companies, and as a consequence, the more they may lose a significant part of their accuracy. A few studies have demonstrated that, when economic fluctuations occur over different years, one may observe a phenomenon known as population drift (Balcaen and Ooghe, 2006). This phenomenon results in the fact that the boundary between failed and non-failed firms moves (Pompe and Bilderbeek, 2005) and the distributions of explanatory variables change (Pinches et al., 1973) from one period to another, thereby causing a decrease in model prediction ability. Experimental results clearly state that variations in the economic environment, that occur between the period during which a model is estimated and that when it is used for forecasts, is a key factor in explaining variations in model accuracy (Mensah, 1984; Platt et al., 1994; Grice and Dugan, 2003), especially in the mid-term.

The third factor arises from the fact that models are ahistorical, that is to say they do not take into account firm history (Laitinen, 1991), since they solely rely on instantaneous measures of their financial health: most of the time, independent variables (financial ratios) are measured once (Laitinen, 1991). Consequently, they give up the ideas developed by all disciplines that are rooted in organizational theory, where the firm’s past is a fundamental predictor of its ability to survive, and which explains that firms do not present the same ability to face threats. These models cannot account for the fact that two companies that share a similar financial profile, within a given time period, may very well have a real different probability of failure. One knows that some firms are able to survive, sometimes for a long time, while their financial profile makes them look like bankrupt ones. One also knows that others, which seem to be in perfect health, may fail very quickly, and still others which are in a very bad shape, may recover even though nothing suggests they are able to do so (Miller and Friesen, 1977; D’Aveni, 1989; Balcaen and Ooghe, 2006). This echoes the idea that bankruptcy is not the result of a sudden event (Altman, 1984), but that of a process which can be long (Dimitras et al., 1996), and which varies among firms (Sueyoshi and Goto, 2009). Therefore, this inability to take into account firm history leads to models that are unable to capture factors that may influence firm ability to survive, and hence, factors that might reinforce model accuracy.

The fourth factor lies in the lack of a reference model that is generally accepted (Lensberg et al., 2006), but also in the lack of a structured theoretical framework of bankruptcy (Laitinen, 1991). One therefore understands why most models are designed without any theoretical or conceptual basis and rely exclusively on data-mining techniques. These techniques aim to find, within data, patterns that may serve as a basis to design models. Independent variables are chosen among a set of variables that are commonly used in the financial literature, using automatic selection process (Scott, 1981). To do so, a sample of data is collected, and a selection method is applied to these data so as to extract the variables that best maximize (or minimize) a predefined criterion that characterized the classes to be discriminated (bankrupt vs. non-bankrupt firms). Since these variables are often sample specific (Zavgren, 1983), models may become unstable and their prediction ability can be severely affected (Scott, 1981), especially if the selected variables are likely to be influenced by macro-economic changes.

All these factors, even if they do not exhaust all explanations, lead to models that suffer from an inability to assess stable forecasts over time. This is why some authors attempted to improve model ability to correctly forecast the fate of companies beyond a 1-year horizon, using variables that were likely to provide a time dimension to a model. All started with the idea that a model, which relies on a static or instantaneous vision of firms’ financial health, may lead to other conclusions than those that can be drawn from a model that takes into account the evolution of firm health over time. The first authors sought to push the limits of single-period models using measures of variation of financial indicators over time (stability of earnings Altman et al. 1977; standard deviation of ratios and their standard error of estimate Dambolena and Khoury 1980; Betts and Belhoul 1987), in conjunction with discriminant analysis. The underlying assumption to these computations is that aggregate measures, synthesizing in a single value variations assessed over several years, are likely to highlight the different degrees of firm weakness, and hence their different ability to survive. However, if such measures are useful to improve forecasts, these forecasts do not remain stable over time, and their accuracy decreases as the prediction horizon increases: Altman et al. (1977) achieve a correct prediction rate of 91 percent at a 1-year horizon, which drops to 76.8 percent at a 5-year horizon, Dambolena and Khoury (1980) achieve a correct rate of 91.2 percent at a 1-year horizon and 89.1 percent at a 4-year horizon, and Betts and Belhoul (1987) achieve a rate of 90.1 percent at a 1-year horizon, and 37.5 percent at a 5-year horizon.

Others, a little later on, designed multi-period models, using survival analysis, with data that measured company financial health over several years (Berg, 2007; Gepp and Kumar, 2008; Dakovic et al., 2010). This time, variations of firm health are no longer estimated using synthetic measures, but are assessed with absolute values of financial indicators measured over several consecutive years. This way of modeling the firm’s financial health leads to better forecasting results than those obtained previously, probably because it takes into account more information. However the stability of these forecasts remains problematic since some results are rather stable, others are not. Berg (2007) achieves a correct prediction rate of 78 percent at a 1-year horizon and 75.9 percent at a 3-year horizon, Gepp and Kumar (2008) achieve a rate of 95.4 percent at a 1-year horizon and 86.7 percent at a 5-year horizon, Dakovic et al. (2010) achieve excellent results with a rate of 90.1 percent at a 1-year horizon and 89.3 percent at a 3-year horizon.

Finally, du Jardin and Séverin (2011); 2012) proposed another type of model, based on the way firms move over time in a space at risk, using “trajectories”, some of which lead to bankruptcy. Each trajectory symbolizes the variation of financial health of a subset of firms that share the same behavior, and is used to make forecasts. This method leads to rather stable results up to a 3-year horizon as opposed to those obtained with discriminant analysis and other traditional modeling methods. However, this method is not without drawbacks, particularly because it requires a large set of historical data. Indeed, a trajectory is measured using data that are collected over 7 consecutive years. As a consequence, this model cannot be used to assess a bankruptcy risk associated to very young firms or to firms for which some data are missing.

These latter works clearly indicate that the historical dimension of models and their ability to represent the evolution of firm’s financial health over time, are key factors to improving their accuracy when the horizon of a prediction exceeds 1 year. Nonetheless, the use of these factors all alone is not sufficient to always guarantee such an improvement.

Our study is in line with those of du Jardin and Séverin (2011); 2012) and aims to define a modeling method that can be used to overcome the main drawbacks of trajectories (by reducing the amount of data needed to design a model and allowing a model to be used with a large number of firms), and at the same time be used to improve model accuracy over time. This method conceptually relies on two streams of research. The first has demonstrated that the discrepancy between sound and unsound firms, in terms of financial health, tends to significantly widen generally 2 or 3 years before the bankruptcy of unsound firms (Beaver, 1966; Deakin, 1972; Altman et al., 1977; Dambolena and Khoury, 1980; D’Aveni, 1989; Laitinen, 1991; 1993). This suggests that data collected over such a period, even if this period is not that large, may capture the main differences, in terms of behavior, between these two groups. The second has shown that there is a limited number of failure processes (Argenti, 1976; Laitinen, 1991; 1993) and that the frequency of each process in a sample used to design a model may strongly influence its forecasting ability (Laitinen, 1991). This suggests that it is better to use as many models as there are failure processes, rather than one. Laitinen (1991) thought that these processes might be depicted using the same variables but with different coefficients. However, we may also think that each process can be depicted using specific financial indicators, and that these indicators may be used to design models. This is the reason why the method we present in this paper relies on data that represent the evolution of firm’s financial situation over a short period, using what we called “terminal failure processes”. Thus, as we based our technique on this notion of failure process, we build on the idea of Scott (1981), who thought that model predictive ability might be improved by using concepts derived from theoretical considerations about bankruptcy. With our method, a model is built using five steps. First, a sample of firms, half of which went bankrupt, is collected at time t. Data that characterized these firms are then gathered over 3 consecutive years at time t − 1, t − 2 and t − 3. Second, these data are then quantized using self-organizing maps, one per year. Each map then represents all possible financial situations of a sample of firms, 1, 2 and 3 years prior to the failure of parts of the initial sample. Third, once the maps are designed, the positions of each company, on each map, are then calculated. Each sequence of positions corresponds to a given process. Fourth, all individual processes are classified into a small number of prototype processes, called “failure processes” because they represent a typology of behaviors within the failure space. Fifth, firms are then classified, depending on the prototype process that is the closest to their own individual process, into a few numbers of groups, and each group is then used to build a specific failure model; one model per prototype process. Different classification methods are then used to design these models. Finally, the prediction ability of each set of models is assessed using different test samples and results are compared to those calculated using traditional models.

This method differs from that of du Jardin and Séverin (2011); 2012), even if the beginning shares some common points. Indeed, du Jardin and Séverin (2011); 2012) use Kohonen maps to quantize data that characterize bankrupt and non-bankrupt firms and that are collected over several years. Then they design six maps and they calculate the position of each company on each map: a set of six positions is then called a “trajectory”. These individual trajectories are then grouped into a small number of prototype trajectories and a label (bankrupt or non-bankrupt) is assigned to each of them based on the number of failed and non-failed firms whose individual trajectories are the closest to a given prototype trajectory. Actually, prototype trajectories are designed in such a way that each one represents the evolution of the financial health of a set of firms that are mostly either bankrupt or non-bankrupt firms.

In this paper, we also estimate a set of maps for each sample, but this set is reduced to three maps. We also calculate the position of each firm on each map and these positions are used to design individual failure processes. We then group these processes into a small number of prototype failure processes. Our failure processes somehow correspond to the prototype trajectories designed by du Jardin and Séverin (2011); 2012), but are not labeled. It is precisely here that the two methods differ. First, a failure process is not dedicated to a certain type of firm: failed or non-failed. A failure process is a process that is followed by a subset of firms that share the same evolution of their financial health, but whose fate may be quite different: some may go bankrupt, while some others may not. Second, du Jardin and Séverin (2011); 2012) use trajectories as a model to make forecasts. Since each prototype trajectory corresponds to the behavior of either failed or non-failed firms, to make a forecast, these authors seek the prototype trajectory that is closest to a given individual trajectory, using a distance calculation. Once the closest prototype is found, the label of this prototype is then used to achieve the forecast: if the prototype trajectory that is closest to that of a given company corresponds to the trajectory of bankrupt (non-bankrupt) firms, then this company is considered to be going bankrupt (non-bankrupt). The method presented in this paper suggests using trajectories, or failure processes, in another way: instead of using them to make a forecast, we suggest designing, by use of traditional modeling methods, as many models as there are different failure processes. Therefore, within a sample where one can find six different failure processes, and using discriminant analysis for example, our method will lead to the construction of six discrimination models. In using these models so as to make a forecast, one will have to calculate, for a given firm, its position on the three maps presented previously. Once the positions are calculated, and therefore once the firm failure process is estimated, one will just have to look for the prototype failure process that is closest to the firm process, and then use the model specifically designed for this process.

In the former situation, trajectories are used as a model; in the latter, failure processes are used to determine, among a set of models, the one that best fits a given firm or, more precisely, that best fits the evolution of its recent financial behavior. Our models are then models that are specialized by type of failure process.

Data come from a financial database, Diane,
                        1
                     
                     
                        1
                        
                           http://www.bvdinfo.com/en-gb/our-products/company-information/national/diane.
                      managed by Bureau Van Dijk, which currently provides financial data for over 1.3 million French firms that are required by law to file their annual reports with the French commercial courts. More specifically, this database contains balance sheets and income statements of these firms over a period of 10 years. It also includes data that characterize the board of directors, the ownership structure, the banks and the auditors of firms. Data are updated every day or every week.

We extracted from this database a set of samples of companies using criteria that are presented below, and we computed financial ratios that were used as explanatory variables. We solely used this type of variables, even if one knows that non-financial information can be useful for this kind of task (Psillaki et al., 2010).

We designed two sets of models. The first set is made up of models that are designed, each time, for a given sector of activity. We chose the sectors that concentrate the largest number of failed firms in France between 2003 and 2012 (retail, construction and services dedicated to companies) so as to check if models share similar results, whatever the origin of firms. The second set is made up of models designed for firms that belong to any sector of activity. These models make it possible to examine to what extent the different modeling methods may be useful to design classification rules with good prediction ability.

Changes that occur in firms’ macroeconomic environment, between the period during which a model is estimated and that during which it is used, often influence model prediction ability (Pompe and Bilderbeek, 2005). So as to control for this effect, we used different samples (presented below) we collected over the period from 2003 to 2011. Fig. 1
                        presents the relationship that exists between the changes in macro-economic conditions in France, measured using the changes in gross domestic product (GDP), and failure rates of each sector we took into account in this study. It shows in particular the influence of the period of growth, between 2003 and 2006, and that of the period of downturn, between 2007 and 2009, on failure rates.


For each sector, we collected 12 samples; 6 were used to estimate model parameters, and 6 to test model accuracy. For each period, we used different modeling methods to design our models that are presented below. Table 2
                        presents the way data were collected over time to design traditional and failure-based models. A first sample of firms was selected in 2006. With traditional models, firm’s financial data were collected in 2006 and firm status (failed vs. non-failed) was determined in 2007. Models were then used for prediction purposes, with a second sample of firms that was selected in 2005 (firm status being determined in 2006); financial data that characterized these firms were collected in 2005, 2004 and 2003 to assess model prediction ability at a 1-, 2- and 3-year horizon, respectively. With failure-based models, the first sample of firms was used to design models, and the financial data that characterized these firms were collected in 2006, 2005 and 2004. Failure processes were then estimated using these data and, for each process, a model was designed. These models were then tested using a second sample of firms selected in 2005, and whose financial data were collected in 2005, 2004 and 2003.

This procedure was performed six times so as to design six sets of models, each time with a lag of 1 year. Table 3
                        presents, for each year where a sample of firms was selected, the number of companies whose financial accounts were collected.

Firms from test samples were chosen at random from among those for which at least 3 years of financial data were available in the database. Training samples were selected so as to control for the influence of firm size and age on model accuracy. If these firms were selected at random, since young and small firms have a much higher probability of going bankrupt than other firms do, models would be more likely to discriminate between young and old or small and big firms rather than discriminating between sound and unsound firms (Taffler, 1983). This is why training samples were first chosen so as to balance firm age: we selected 1/3 of firms with no more than 5 years of age (they represent on average, over the period we studied, 54 percent of bankrupt firms), 1/3 between 6 and 15 years of age (31 percent of bankrupt firms) and 1/3 with more than 15 years of age (15 percent of bankrupt firms). We then used a similar procedure with firm size that was assessed using the number of employees. We chose a first half of companies with less than 10 employees (85 percent of bankrupt firms) and a second half with 10 or more employees (15 percent). All these numbers come from www.altares.fr
, which provides yearly detailed statistics about bankrupt firms in France.

These samples were selected using the same proportion of failed and non-failed firms. Indeed, if one uses a random sample, since the number of failed firms is far lower than that of non-failed firms (the ratio between these two types often ranges from 1/20 to 1/25), the design of a model would become problematic (Cielen et al., 2004). Data that characterized failed firms would be hidden by those that represent non-failed firms, and therefore would become rather useless. However, as shown by Zmijewski (1984), using a non-random sample may cause a “choice-based sample bias” leading to more or less biased probabilities in standard probit/logit models.

Is such a bias really problematic? Zmijewski (1984) shows that when one deals with a classification problem such as that involved in bankruptcy prediction, the overall error rate of a logit/probit model remains rather stable as the proportion of failed and non-failed companies ranges from 50/50 to a 1/20. This latter ratio corresponds, on average, to the true proportion of failed firms among non-failed ones. However, he empirically demonstrates that the percentage of correctly-classified failed companies is significantly affected and overestimated as compared to the same rate calculated with a random sample. Besides, his results clearly show that the lowest the number of failed firms in a learning sample, the less likely a model is able to correctly forecast their fate. These results are confirmed by Platt and Platt (2002). However, with other modeling methods such as neural networks, this bias is less pronounced (Neves and Vieira, 2000).

In practice, a model that tends to overclassify failed firms to the detriment of non-failed firms may be a rather useful model because of the asymmetry of misclassification costs. The cost of misclassifying a failed firm (Type-I error) is indeed far greater than the cost of misclassifying a non-failed firm (Type-II error). Ultimately, such a bias is not that problematic when one wants to design a classification model and when the groups that are to be discriminated are far from being evenly distributed within a given population. However, it is not the case when a probability of failure is used in equity or bond valuation problems.

Let us note that we control for sector, age and firm-size effects that represent the main bias that can affect bankruptcy models and that may be due to non-random sampling, and we also take into account the effect of the economic environment on data distribution.

We calculated, using company financial accounts, a set of 50 financial ratios among those that have been traditionally used in the literature since Altman (1968). These ratios were not chosen arbitrarily, but based on the main financial dimensions that govern bankruptcy. We first chose the two dimensions that are, from a legal and conceptual point of view, at the root of bankruptcy: liquidity (ability of a company to meet its current liabilities using its current assets) and solvency (ability of a company to pay its debts). Legally, a firm which is not able to reimburse its debts can file or can be forced to file for bankruptcy. Conceptually, a firm is not able to reimburse its debt because it is insolvent and/or not liquid enough. Let us note that the main models that are grounded on a theoretical framework that explains bankruptcy are primarily based on liquidity and solvency measures (Gentry et al., 1985; Aziz et al., 1988; Laitinen and Laitinen, 1998). Then, we added the following dimensions that are proven to explain or contribute to explain bankruptcy: profitability (efficiency with which a firm turns business activity into profits), financial structure (how resources are allocated within a company by their financial horizon), activity (ability of a firm to efficiently use resources) and turnover (ability of a business to convert various asset, liability and capital accounts into cash or sales). The 50 financial ratios we selected are presented in Appendix A (Tables A.1 and A.2). Ratios were computed, for companies belonging to learning samples and test samples, using data that were published over the 3 years prior to the determination of company status. Models were then designed, with each modeling method, using a particular variable selection technique. These techniques are presented below (Section 3.6).

Models were designed with the most common modeling method used in the financial literature; discriminant analysis, logistic regression, a neural network called Multilayer Perceptron (MLP) and survival analysis (Laitinen, 2007; Kumar and Ravi, 2007). These methods are presented below with the one used to design a self-organizing map.

Discriminant analysis is a well-known classification method that makes it possible, using a set of explanatory variables, to find the linear combination of these variables that best discriminate between observations one wants to classify into a few groups, such as sound and unsound firms. In such a situation, the dependent variable is binary and represents each of the two groups. To design a classification model, the method attempts to calculate a z score for each company that can be expressed as a linear combination of a set of explanatory variables xi
                            and then to compare this score to a threshold that represents the boundary between failed and non-failed firms. The score is computed as follows:

                              
                                 
                                    
                                       z
                                       =
                                       
                                          w
                                          0
                                       
                                       +
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             n
                                          
                                          
                                             
                                                w
                                                i
                                             
                                             
                                                x
                                                i
                                             
                                          
                                       
                                    
                                 
                              
                           where w represents the vector of coefficients wi
                            of the function to be estimated. A firm will be classified into one of the groups depending on the value of its z score. The vector w is estimated using a technique that looks for the combination of parameters that maximize the discrepancy between the average of the scores of each group.

This method, which was popularized by Altman (1968) in the field of bankruptcy prediction, quickly stumbled upon the limits of linear models, as highlighted by Laitinen and Laitinen (2000) – independence of explanatory variables is rarely fulfilled, combined effects of several variables are not taken into account, etc. – but also upon the binding nature of its optimal conditions of use, that were analyzed by Wald (1944) – multivariate normality of explanatory variables, equality of variance–covariance matrices of each group. Even if this method is particularly robust against departures from these conditions for optimality, especially when explanatory variables are financial ratios, other methods were often used to overcome some of the constraints it imposes on data.

Logistic regression is the first method that was used instead of discriminant analysis to design bankruptcy models (Ohlson, 1980). Logistic regression makes it possible to compute a z score for a given company, but this score is expressed as a probability of failure as follows:

                              
                                 
                                    
                                       z
                                       =
                                       
                                          1
                                          
                                             1
                                             +
                                             
                                                e
                                                
                                                   −
                                                   (
                                                   
                                                      w
                                                      0
                                                   
                                                   +
                                                   
                                                      ∑
                                                      
                                                         i
                                                         =
                                                         1
                                                      
                                                      n
                                                   
                                                   
                                                      
                                                         w
                                                         i
                                                      
                                                      
                                                         x
                                                         i
                                                      
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where xi
                            are explanatory variables and wi
                            the coefficients which are estimated using maximum likelihood estimation. Here as well, a company is classified into a given group by comparing its probability of failure to a threshold that is defined a priori. Even if this method does not impose any conditions on data distribution, it implicitly assumes that the distribution of the dependent variable follows a logistic distribution, which nothing guarantees a priori.

To forecast whether a company will go bankrupt or not, a neural network model calculates a z score that is compared to a threshold. The z score is estimated with the following function, using a network that is made up of a single-hidden layer, an output node and no bias, where w is the weight matrix of the network, f the activation function of neurons (most often the hyperbolic tangent or a logistic function), n the number of variables and p the number of neurons of the hidden layer:

                              
                                 
                                    
                                       z
                                       =
                                       f
                                       
                                          (
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             p
                                          
                                          
                                             w
                                             j
                                          
                                          ·
                                          f
                                          
                                             (
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                w
                                                
                                                   i
                                                   j
                                                
                                             
                                             
                                                x
                                                i
                                             
                                             )
                                          
                                          )
                                       
                                    
                                 
                              
                           Weights are estimated through a learning process. For this purpose, the network requires a set of variables xi
                            (financial ratios), that characterize a sample of observations (firms), and a set of zk
                            measures that correspond to the occurrences of a phenomenon the network will have to forecast (failure or non-failure). One supposes that a relationship may exist between xi
                            and zk
                            and one looks for the function that is more likely to represent this relationship. The search is performed using a learning process that leads the network to progressively assign to each observation, a value that corresponds to its status (failed vs. non-failed) that is as close as possible to a reference value. This learning process uses an iterative mechanism to adjust the weights of the network so as to gradually increase the consistency between its responses and the desired responses. The process uses a cost function to be minimized during the learning sequence, most often the quadratic error or the cross-entropy. There are different optimization techniques that can be used to look for an acceptable minimum to the cost function, called first- or second-order techniques. Finally, once the learning process is done, the network can be used for forecasting tasks.

The previous methods make it possible to design failure models using cross-sectional data. Each firm is then represented with variables that are measured only once, which means that time is not considered an explanatory variable. By contrast, a survival method allows to take into account this dimension and use, for this purpose, longitudinal data. Such a method relies on a survival function s(t) and a hazard function h(t). The survival function corresponds to the probability that a firm will survive at time t, or more generally, to the probability that this firm will not go bankrupt at time t. The hazard function corresponds to the instantaneous rate of failure at time t. The estimation of these two parameters depends on the nature of the relationship that may exist between them and the explanatory variables. With Cox’s model, also called proportional hazards model, the relationship can be expressed as follows:

                              
                                 
                                    
                                       h
                                       
                                          (
                                          t
                                          )
                                       
                                       =
                                       
                                          h
                                          0
                                       
                                       
                                          (
                                          t
                                          )
                                       
                                       ·
                                       
                                          e
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                w
                                                i
                                             
                                             
                                                x
                                                i
                                             
                                          
                                       
                                    
                                 
                              
                           where h
                           0 corresponds to the baseline hazard, which represents the probability of failure at time t when all variables are equal to zero, that is to say the instantaneous risk of failure of a firm that does not show any sign of weakness, xi
                            the explanatory variables and wi
                            the coefficients of the function that are estimated using a maximum likelihood technique. The survival function of a firm is given by:

                              
                                 
                                    
                                       s
                                       
                                          (
                                          t
                                          )
                                       
                                       =
                                       
                                          e
                                          
                                             −
                                             h
                                             (
                                             t
                                             )
                                          
                                       
                                    
                                 
                              
                           As with other methods, firms are classified depending on the fact that their survival function lies below or above a threshold.

Cox’s model relies on a strong assumption of proportional hazards where the ratio of hazard rates between two observations is constant and does not depend on time.

A self-organizing map is a special class of neural network (Kohonen, 2001) which is usually used for mapping multidimensional data onto a geometrical structure of lower dimensionality (most of the time 2 or 3 dimensions). Such a map has two main characteristics. First, it makes it possible to reduce the dimensionality of the data while limiting the loss of information. Second, it preserves the topology of data relatively well and therefore does not alter the proximities that exist between observations in the input space. A map is made up of a set of nodes, or neurons, that are organized on a square or rectangular grid, most of the time. Each neuron is represented by a set of weights which has the same dimension as the vector of data that characterized each observation. A map is designed using a learning process. The learning algorithm can be found in du Jardin and Séverin (2011).

If observations used during the learning process are companies, once this process is over, each neuron can be considered a prototype of a certain number of firms that are similar according to the metrics that were used. And if data that characterized firms are financial data, the map represents all financial situations of a sample of firms, ranging from those that characterized highly healthy, performing companies to those that correspond to firms that are in a very bad shape.

We used such a map as a basis to design some of our models, as described below.

A first set of models was built using discriminant analysis, logistic regression, a neural network (multilayer perceptron, also called feedforward neural network) and Cox’s model. Explanatory variables of each model were selected, among the initial set of variables, using a selection method that best fits each modeling technique. A variable selection method is made up of three components (Dash and Liu, 1997): a search procedure that explores the variable space; an evaluation criterion used to compare different subsets and choose the subset that presents the best characteristics; a stopping criterion so as to interrupt the search. We then selected, for each selection technique, the components presented below.

With discriminant analysis, variables were selected using a stepwise search procedure, a Wilks’ Lambda as an evaluation criterion and a Fisher’s F test as a stopping criterion. With logistic regression and Cox’s model, the selections were performed using a stepwise search procedure, a likelihood statistic as an evaluation criterion and a Chi2 as a stopping criterion.

With the neural network, the selections were performed using two steps, as suggested by Leray and Gallinari (1998). First, we defined network architecture. We used the Levenberg–Marquardt algorithm as an optimization technique, one hidden layer, one output node and the hyperbolic tangent as an activation function of neurons. To determine the size of the hidden layer, we ran a set of experiments. We randomly drew 100 subsets of variables from among the initial set. With each subset, we tested different sizes of the hidden layer (from 2 to 20 neurons) and different learning rates (from 0.05 to 0.5 with a 0.05 step). Half of each learning sample was used to estimate the weights of the network, and the other half was used to assess its prediction ability. Results were averaged and the architecture that led to the lowest error was finally chosen. Second, once network architecture was determined, variable selection was performed. We chose a backward search procedure to explore the variable space, a criterion inspired by weight-pruning methods and presented in Leray and Gallinari (1998), as an evaluation criterion. Variables were removed until no variable remained selected and the network was retrained after each removal. So as to select the final subset, we looked for the subset that led to the lowest error and we chose the subset whose error was statistically close to the lowest error, using a Fisher’s F test, and that was made up of the smallest number of variables. The cut-off value used to assess the significance levels (p-value) of the statistical tests presented above was set up to 0.05. Selecting variables using automated processes may lead to models that are strongly data-dependent (Balcaen and Ooghe, 2006), and thus to models whose predictive ability may be weak. So as to limit this effect, we chose the initial set of variables depending on the main financial dimensions that account for bankruptcy. And we also control for economic environment effect on variables by using samples that were collected over different time periods. We added, in Appendix A, some statistics that describe variables and their distribution among the different models.

A second set of models were then designed but this time, for a given sample, instead of designing solely one model, we designed as many models as firms might be quantized by different terminal failure processes. The procedure used to design these models is presented below.

A failure process represents the way some firms move toward bankruptcy over a period of 3 years, while some others, that share a similar financial profile as well as a similar evolution of their profile with the former, manage to survive. This process corresponds to the way a firm moves into the space at risk over time. The space at risk was quantized using self-organizing maps. Since a map represents all financial profiles that a sample of firms may embody at a given point in time, it also represents the different risks of failure of these firms.

We first designed a self-organizing map for each of the 3 years we study, and each sample. Then we assessed the positions of all firms on each map; a set a positions corresponds to an individual failure process. Finally, we grouped all individual failure processes into a few number of prototype processes, that is to say processes that best summarize the different evolutions of firms’ financial situation over time. The procedure used to design these prototype processes is as follows.

The first step was to select a particular subset of variables to design each self-organizing map so as to ensure that both the quantization performed with a map and the size of the same map fit at best the underlying structure of each sample. To do so, we defined n maps of different size and, for each map, we sought for the subset that maximized the evaluation criterion defined below. We then defined n pairs of “map-subset of variables” for which the subset of variables suited the size of the map. Then, we sought for the final pair within the n pairs of “map-subset of variables” and we chose that for which the subset best fitted the map. As each map must be able to properly quantize both non-failed and failed firms, 1, 2 and 3 years before some of them go bankrupt, we chose to design maps that minimize the degree of overlap between these two groups. As a consequence, we sought for maps where each neuron was as much representative of one of these two classes as possible. For this purpose, we designed a variable selection method whose evaluation criterion aimed to minimize such an overlap. For a given size of a map, subsets of variables were evaluated using a r relevance criterion that measures the contribution of a subset to class overlap on the map as follows:

                                 
                                    
                                       
                                          r
                                          =
                                          
                                             
                                                
                                                   ∑
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   
                                                      n
                                                      1
                                                   
                                                
                                                
                                                   
                                                      c
                                                      
                                                         1
                                                         i
                                                      
                                                   
                                                   
                                                      c
                                                      i
                                                   
                                                
                                                +
                                                
                                                   ∑
                                                   
                                                      j
                                                      =
                                                      1
                                                   
                                                   
                                                      n
                                                      2
                                                   
                                                
                                                
                                                   
                                                      c
                                                      
                                                         2
                                                         j
                                                      
                                                   
                                                   
                                                      c
                                                      j
                                                   
                                                
                                             
                                             n
                                          
                                       
                                    
                                 
                              where n
                              1 and n
                              2 are the respective number of neurons belonging to each class (non-failed and failed), n the total number of neurons and c
                              1 and c
                              2 the respective number of non-failed and failed firms that are the closest to a given neuron, with c = c
                              1 + c
                              2. To calculate n
                              1 and n
                              2, we estimated the percentage of firms of each class that were the closest to a given neuron, using a distance calculation. We then assigned to a neuron the label of the class whose percentage was the highest. Finally, we counted the number of neurons that were assigned to each of the two classes.


The r relevance criterion ranges from a value that is strictly greater than 0.5 and lower than or equal to 1, except when no neuron has been assigned to a given class, that is to say when c
                              1 = c
                              2 for each neuron, in which case r is equal to 0. The higher the coefficient, the greater the variables used for its calculation minimize class overlap. With this criterion, the variable selection method used a backward search procedure; variables were discarded one at a time and the map was retrained after every removal. Once all variables were discarded, we looked for the subset that led to the highest relevance. To do so, we selected the subsets whose relevance was statistically the closest to the highest relevance, using a test for differences between proportions, and we chose the subset that was made up of the smallest number of variables. The selection procedure is as follows, and begins with a map whose size is minimal:

                                 
                                    •
                                    
                                       Step 1: set up the minimum number of neurons of a map and initialize neuron weights using random values.


                                       Step 2: select n variables, n being the total number of variables that were initially chosen.


                                       Step 3: perform n learning of the map under consideration using n variable subsets – each subset is made up of n − 1 variables – by removing a different variable each time.


                                       Step 4: calculate the r relevance of each subset and remove the variable that leads to the lowest relevance.


                                       Step 5: repeat step 3 and step 4 using n = n − 1 until all variables have been discarded.


                                       Step 6: find the subset whose relevance is the highest.


                                       Step 7: find the subsets whose relevance is the closest to the subset with the highest relevance, using a test for differences between proportions, and select that whose number of variables is the smallest.


                                       Step 8: repeat step 2–step 7, each time increasing the number of neurons of the map by 1 until a stopping criterion (predefined maximum number of neurons) is reached.

At the end, the procedure has selected n variable subsets, and each subset has been defined for a given size of the map. We then sought for, from among all these subsets, the subset that best fitted the map that was used for its selection using a criterion defined by Bodt et al. (2002). For this purpose, we defined, for each map that was examined, a set of k maps whose number of neurons ranges from a number that is smaller than that of the map under examination to a number that is greater than that of the same map. Then, for each variable subset, and each set of k maps, we estimated the CV coefficient of variation of the quantization error qe. The error was assessed over 100 learning phases, and then averaged:

                                 
                                    
                                       
                                          q
                                          e
                                          =
                                          
                                             1
                                             n
                                          
                                          ·
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             n
                                          
                                          
                                             (
                                             ∥
                                          
                                          
                                             x
                                             i
                                          
                                          −
                                          
                                             w
                                             
                                                i
                                                c
                                             
                                          
                                          
                                             ∥
                                             )
                                          
                                       
                                    
                                 
                              where n is the number of observations (firms), xi
                               the data vector of each observation i, wic
                               the weight vector of the neuron that is the closest to observation i, according to the Euclidian distance that was used.

                                 
                                    
                                       
                                          C
                                          V
                                          
                                             (
                                             q
                                             e
                                             )
                                          
                                          =
                                          100
                                          ·
                                          
                                             
                                                σ
                                                
                                                   q
                                                   e
                                                
                                             
                                             
                                                μ
                                                
                                                   q
                                                   e
                                                
                                             
                                          
                                       
                                    
                                 
                              where σqe
                               and μqe
                               are respectively the standard deviation and the mean of the quantization error that were assessed after 100 learning phases. Then, we analyzed the variations of the error with respect to the number of neurons of a map, and we sought for a sudden increase in this error. When such an increase occurs, it means that neurons switch from one cluster of the input space to another, and as a consequence the estimation of these weights is not reliable (Bodt et al., 2002). Therefore, if the size of the map which is just below that leading to the largest increase in error corresponds to the size of the map that was used to select the variable subset under consideration, then this map has an optimal number of neurons given the variables that were used for its design. Indeed, this size corresponds to a situation where, on average, neuron weights are solely influenced by little variations due to their initialization and therefore do not strongly depend on their random initial values. Once all subsets were analyzed, we finally chose the subset with the smallest number of variables. Each map was designed using a particular subset—one per period and per sample.

Once the maps were estimated, the second step aimed at classifying neurons into a few numbers of groups so as to facilitate the estimation of the different failure processes. For this, we used a hierarchical ascending classification with Ward criterion and a square Euclidian distance, and we analyzed a few partitions. As we intended to obtain a small number of groups, we studied a few partitions that were made up of 2–8 groups of neurons. We then assessed their homogeneity using the three best indices that were studied by Milligan (1981): Point-Biserial Correlation, C-Index of Hubert and Levin and Gamma of Baker and Hubert.
                                 2
                              
                              
                                 2
                                 The Point-Biserial Correlation assesses the homogeneity of a set of groups using a calculation based on the difference between the mean value of distances between groups and that of distances within groups. The C-Index of Hubert and Levin uses a calculation based on the k distances that exist within groups so as to compute the difference between the sum of the k smallest distances and the sum of the k largest distances. The Gamma of Baker and Hubert uses a calculation based on the number of distances between groups that are greater than the largest distance within groups, and the number of distances within groups that are lower than the lowest distance within groups.
                               We chose, each time, the most homogeneous partition according to these three indices.

So as to illustrate the maps we estimated, we show, in Fig. 2
                              , the maps designed with the multi-sector sample, and firms that were selected in 2006 and for which we collected financial data in 2006, 2005 and 2004. The maps designed using data from 2006, 2005 and 2004 correspond respectively to the financial situations of firms 1, 2 and 3 years before some of them went bankrupt. On each map, the locations of the different groups are shown using a numerical value. The map from 2006 is divided into 7 groups, that from 2005, into 6 groups, and that from 2004, into 4 groups.


To represent these maps, the different groups were labeled by decreasing order of the financial health of the companies they represented. This labeling aimed to facilitate the reading and the interpretation of the maps presented in Fig. 2, so as to easily identify the different areas of the maps, i.e. those where firms are rather in good shape, those where firms are in a very bad situation and those where companies are in an intermediate state. It was also used to interpret the meaning of the different prototype processes that are presented in Fig. 3
                              , as discussed below. The labeling has absolutely no influence on the structure of models because they were built independently from the meaning we may assign to each group and failure process.

So as to label groups, we analyzed the financial health of companies using statistical figures that describe company profitability, solvency, liquidity, financial structure, short-, mid- and long-term debts, and supplier and trade credits. Based on this analysis, we then labeled, by decreasing order of financial health, the different groups of each map, and we assigned a numerical value to each group that reflects their position in this hierarchy. Each neuron was then assigned the number of the group it belonged to. These values represent a sort of scale of financial health, whose lowest value represents firms that are the best performing and non-failed companies, and whose largest value corresponds to firms that are the most fragile companies and that are in a very bad financial situation. These values are presented on the different maps.

During the third step, we calculated, with each learning sample, firm individual failure processes, that is to say the position of each company on each map. For this, we looked for the neuron, on a given map, that was the closest to each company, and we assigned each company the number of this neuron. We then designed as many individual failure processes as there are firms in each sample.

The fourth step aimed at grouping all individual processes into a few number of prototype processes that represent general evolutions over time of firm’s financial situations. We used the same procedure as that used to group neurons. Individual processes were then classified using a hierarchical ascending classification, and we also analyzed a few partitions so as to determine, here as well, the most homogeneous one. We analyzed partitions whose number of groups ranged from 2 to 8 so as to obtain a limited number of prototype processes given the three evaluation criteria cited above. Fig. 3 shows the partition estimated using the maps that are presented in Fig. 2. The graphs were designed using a quantization performed with a self-organizing map that was made up of as many neurons as there were groups within the partition, and using the values that corresponded to the position of each company on each map. The X-axis, on each graph, represents time (1 corresponds to 2006, 2 to 2055, and 3 to 2004) and the Y-axis represents the number of groups on each map (7 in 2006, 6 in 2005 and 5 in 2004). Each graph corresponds to a failure process. The first one, on the left, depicts firms that, within the failure space, were located over years within regions where the probability of going bankrupt was the smallest (from 2006 to 2004, regions 1, 1 and 2 respectively). The last one, on the right, represents firms that were located, over the same period, in regions where the probability of failure was the highest (from 2006 to 2004, regions 7, 5 and 4 respectively).

When one analyzes the individual processes that are the closest to a given prototype process, one better understands what the main financial dimensions that discriminate between failed and non-failed companies are within each group.

The first process represents a group of firms that were in good shape in 2004. Three years later, a part of this group, mostly made up of non-failed firms, landed in “zone 1” (Fig. 2) and the other part, mostly made up of bankrupt firms, landed in “zone 2”. This process corresponds, on average, to companies whose level of liquidity largely explains why some of them went bankrupt. Indeed, bankrupt firms exhibit a rather good financial profile except in terms of liquidity, which tends to worsen as failure approaches.

The second process begins in “zone 2”, in 2004, then move toward “zone 4” in 2005, and ends up again in “zone 2”. It corresponds to firms that experienced temporary difficulties, in 2005, and a part of it that managed to recover 1 year later. This process corresponds to firms that exhibit a rather good health, except in two dimensions, liquidity and solvency, that worsen over the last 2 years and that explain why some firms went bankrupt.

The third represents firms with an average financial situation, which also experienced difficulties in 2005, whose profitability is rather low and which faced debt issues. These two factors are the main causes that explain the bankruptcy of some firms.

The fourth corresponds to businesses that improved their situation in 2005, compared to that in 2004, but saw their health deteriorate in 2006. Bankruptcy is the result of a poor profitability and a fragile financial situation: failed firms did not manage to earn enough money to finance their activity and their capital and debt structures did not suit their financing needs.

The fifth is rather similar to the fourth, but firms are more fragile than the latter. Bankruptcy is mainly due liquidity and short-term debts issues.

The sixth corresponds to firms whose financial situations steadily deteriorated between 2004 and 2006, and bankruptcy is the result of a combination of different weaknesses: low profitability, low liquidity, bad financial structure and high short-term debts.

Finally, the seventh process is not very different from the previous one: in addition, firms face at the same time short-, mid- and long-term debt issues.

Prototype failure processes represent the way distinct groups of firms change from one possible financial situation to another, over time, using a common behavior. These groups were then determined by assigning each company the prototype process that was the closest to its own process, according to a distance measure. Then, for each group, we estimated a set of models using the modeling methods that were presented previously, and we ensured that the number of non-failed firms was similar to that of failed firms so as to avoid any imbalance between group sizes that may affect model coefficients. We then chose all firms belonging to the group that was made up of the smallest number of firms, and we selected at random a similar number of firms belonging to the other group.

For each modeling method, and each sample, we therefore designed a general model (called traditional model) and a set of sub-models, on per group of firms that shared the same failure process (called failure process-based models). Model prediction ability was assessed using different test samples. Before we estimated failure process-based models, we had to calculate all individual failure processes of firms that belonged to test samples. For this, we first computed the position of each firm on the three maps, then we estimated the prototype failure process that was the closest to their own process, and we finally assigned this prototype process to each firm. These positions were estimated using data that characterized each firm over 3 years and that represent different measures of their financial health, 1, 2 and 3 years prior to the period when their status was determined (failed vs. non-failed). This rule applies to data that belong to learning and test samples. As three maps are used to estimate a failure process, one must use data collected over 3 consecutive years to make a forecast. If data are not available over such a period, it is not possible to estimate any failure process. Moreover, the forecasting horizon begins after the last year of the 3-year period where data of a given company are collected. Thus, if data are gathered at times t − 2, t − 1 and t, the failure process that is estimated corresponds to the evolution of a firm’s financial health between t − 2 and t. As a consequence, failure-based models will use data from t, t − 1 or t − 2 to estimate model accuracy at a 1-, 2- and 3-year horizon respectively, and will provide a forecast that is likely to occur starting at t + 1.

When this assignment was done, firms were grouped according to their prototype process, and each group was used with the model that corresponded to its process to make forecasts. Then, for each modeling method, results obtained with each process-based model were summed up. Thereby, with discriminant analysis, and the sample of firms collected in 2006, 7 discriminant process-based models were estimated, and each model was used solely with firms whose failure process corresponded to the model. Then results achieved with these 7 discriminant models were summed up and the percentage of correct predictions was estimated. This was repeated as many times as there were modeling methods and samples. Model accuracy was assessed by comparing firm actual status (non-failed or failed) to the status assessed by a given model and calculating the number of firms whose status was correctly predicted.

So as to determine the predicted status of a firm, the score estimated using each method was compared to a threshold (cut-off value). We calculated this cut-off value with two different techniques. First, we estimated the value that maximized the overall rate of correct classifications, as it is the most common way to assess model accuracy in the literature. Second, we took into account the fact that misclassification costs are not symmetric; it is indeed much more problematic for a bank to forecast that a customer will not default, while in fact he will (Type-I error), than the contrary (Type-II error). In the first case, the cost may involve a loss in capital that will not be repaid, because the bank has wrongly accepted to lend money to a client that will not be able to reimburse its debt; and in the second, the cost may solely involve a reduced profit as the bank erroneously rejected a client that would have been able to reimburse its debt. This is the reason why we also calculated the cut-off value while taking into account the expected cost of misclassification. However, it is difficult to assess the ratio between the cost of Type-I error and that of Type-II error because it depends on many factors, such as the risk aversion of the user of the model. We then studied several scenarios: we used different costs of misclassification of failed firms (1, 10, 20,..., 90, 100) while the cost of misclassification of sound firms was kept to 1 (Frydman et al., 1985). The prior probabilities of bankruptcy were defined with the actual bankruptcy rates that characterized each sample of firms at the time when they were collected. The average failure rate within the retail sector, over the period studied, is 1.73 percent (with a minimum of 1.61 percent and a maximum of 1.82 percent), within the construction sector, the rate is 2.61 percent (minimum of 2.30 percent, maximum of 3.26 percent), within the services sector, it is 2.01 percent (minimum of 1.84 percent, maximum of 2.24 percent), and within all sectors, it is 2.03 percent (minimum of 1.95 percent, maximum of 2.11 percent).

The following cost function was used to assess cut-off values:

                           
                              
                                 
                                    Expected
                                    
                                    cost
                                    
                                    of
                                    
                                    misclassification
                                    =
                                    
                                       c
                                       1
                                    
                                    
                                       
                                          e
                                          1
                                       
                                       
                                          n
                                          1
                                       
                                    
                                    
                                       p
                                       1
                                    
                                    +
                                    
                                       c
                                       2
                                    
                                    
                                       
                                          e
                                          2
                                       
                                       
                                          n
                                          2
                                       
                                    
                                    
                                       p
                                       2
                                    
                                 
                              
                           
                        where c
                        1 and c
                        2 are the respective costs of Type-I and Type-II errors; e
                        1 and e
                        2 respectively Type-I and Type-II errors; n
                        1 and n
                        2 respectively the numbers of failed and non-failed firms in the sample; and p
                        1 and p
                        2 respectively the prior probabilities of bankruptcy and non-bankruptcy.

@&#RESULTS AND DISCUSSION@&#

Results achieved with test samples are presented in Tables 4
                     and 5. Table 4
                     shows the correct classification rates calculated with traditional models, that is to say models that do not take into account firms’ failure processes. Table 5 shows the correct classification rates estimated with failure process-based models. We highlighted in gray the results that correspond to situations where failure process-based models achieve lower correct classification rates than those achieved with traditional models.

These two tables show that, at a 1-year horizon, failure process-based models achieve similar performances to those achieved with traditional models. By contrast, at a 2-year horizon, they perform slightly better than traditional models, and significantly better at a 3-year horizon.

This result is probably due to the fact that when a forecast is made at a very short horizon (1 year), data used by models do not inherently contain sufficient information that might be exploited to discriminate between firms. In a sense, as failure approaches, additional information drained by failure-based models is not sufficient to improve their forecasts. On the contrary, when the horizon of a prediction increases, failure-based models are able to capture tiny differences that characterized firms over time and that better explain failure than differences that can be assessed solely over 1 year, and that are used by traditional models. For example, a traditional model would struggle to differentiate situations that explain the level of liquidity of firms. A firm may be very liquid because it is able to ensure that its customers pay faster than it settles its suppliers. But a firm may also be liquid because it is on the verge of collapse: its liquidity may just be the consequence of the fact that, trying to avoid bankruptcy, it no longer pays its suppliers and manage to reduce the amount of receivables. Solely failure-based models are able to account for such differences.


                     Table 4 also shows the influence of the variation of economic cycles on traditional models. If one observes the 3 years during which the largest changes occur in the economic environment of firms (2008, 2009 and 2010), one notices that models designed in 2009 (period of severe downturn) and in 2010 (period of recovery), and tested using data respectively from 2008 (period of low growth) and 2009, achieve performances at a 3-year horizon that are among the poorest. By contrast, Table 5 shows that, over the same two periods, failure-based models are more resistant to changes that affect economic conditions than the others.

This resistance is certainly due to their ability to better discern the results of the strategies, or of the lack of strategies, used by firms to face economic fluctuations, and which explain that firms sharing a similar financial profile, may in fact have a very different probability of bankruptcy. Indeed, some firms may benefit from a sort of carrying capacity offered by their clients and suppliers, during periods when economic conditions are tough, whereas others, which are almost identical in appearance, cannot. For example, when a firm stops paying suppliers, it does not always mean that this firm is not able to reimburse its debts, and hence that it runs into liquidity issues. It can also mean that the firm experiences difficulties and that its suppliers have decided to help it, so as to preserve their market share, and have accepted to delay the payment of their receivables. Other firms may experience a decrease in their sales, here as well when economic conditions worsen, and may decide to increase their stocks because they expect a recovery. This increase corresponds to their willingness to avoid a stock-out when an economic recovery will occur. But other firms, in the same situation, may decide to sell stocks to get cash. All in all, failure-based models seem to better account for this kind of ambivalence than traditional ones.

We then analyzed the differences between results that are presented in Tables 4 and5 using a statistical test for differences between proportions. The significance levels of this test are presented in Table 6
                     . In front each symbol mentioned in Table 6, we added a “ + ” sign so as to indicate that the correct classification rate achieved with a failure-based model is greater than that achieved with a traditional model. Table 6 shows that, as the forecast horizon increases, the differences between the two kinds of models are increasingly significant. Indeed, at a 3-year horizon, in almost all cases, the differences are particularly significant (p-value ≤ 0.001). There are solely a few situations where this is not the case (p-value > 0.05). These correspond to two models designed with logistic regression (retail, 2006 and 2010), four models designed with Cox’s method (construction, 2006; retail, 2008; services, 2006 and 2010) and one model designed with the neural network (retail, 2005).

All these results show that a set of models in which each model is suited to a given prototype failure process, makes it possible to improve prediction accuracy when forecasts exceed a 1-year horizon, and especially a 2-year horizon, whatever the modeling method, the period or the firm sector. They also show that when changes in the economic environment occur between the period when a model is designed and when it is used, such a set of models is more likely to capture the influence of these changes on a firm’s fate, than a traditional model does.

We then studied the ability of the two types of models to achieve stable performances over time. We calculated the differences between correct prediction rates achieved at a 1-year horizon and those achieved at a 3-year horizon, that are presented in Tables 4 and5. These tables show that traditional models lead to differences between these two time horizons that are sometimes fairly large whereas failure-based models lead to much smaller differences. To estimate the significance of these differences, we calculated, for each model, sector and sample, a test for differences between proportions. Table 7
                     shows the levels of significance of such a test.


                     Table 7 shows that with almost all traditional models, the differences between predictions performed at a 1-year horizon and those performed at a 3-year horizon, are significant. By contrast, the situation is different if we observe the results achieved using failure-based models. With test samples collected in 2005, 2006, 2007 and 2010, overall results do not show any significant differences, except in a few situations. However, results achieved with test samples from 2008 to 2009 are a bit different from the previous ones. This occurs precisely when test and learning samples come from periods experiencing very different economic conditions. Thus, with data from 2008, 7 results out of 16 show a level of significance larger than 5 percent, and with data from 2009, 9 results out of 16 show the same level of significance.

Therefore we can notice that failure-based models manage to stabilize forecasts at a 3-year horizon relatively well, but when economic conditions change markedly between the period during which they are estimated and when they are used, they are less likely to fully stabilize forecasts. To conclude this analysis, we studied the influence of different costs of misclassification on model accuracy. As already mentioned, the cost of misclassification of a sound firm was kept to 1 while the cost of misclassification of a failed firm was setup to different values: 1, 10, 20, 30, 40, 50, 60, 70, 80, 90 and 100. We calculated, for each model, and each sample, the difference between the expected cost of misclassification achieved using failure-based models and that achieved using traditional models. These are the differences that are presented in Fig. 4
                     : each curve corresponds to the evolution of the differences between these two expected costs of misclassification that were calculated using a sample of firms that belong to the same sector. Each graph represents, on the X-axis, the different costs of misclassification of a failed firm, and on the Y-axis, the values of the difference mentioned above.


Two trends emerge from these graphs. First, when the cost of misclassification of a failed firm is rather similar to that of a sound firm (between 1 and 10), traditional models appear to outperform failure-based models. The difference between these two costs is positive most of the time. However, when the cost of misclassifying a failed firm reaches and exceeds 20, then the difference becomes negative. Moreover, as this cost increases, the difference also increases on average, but its magnitude varies from one sample to another. Among all samples, the multi-sector sample appears to be the one for which this difference experiences the lowest increase as the cost of misclassifying a failed firm rises.

The second trend seems to depend on the phenomenon we noticed previously. When the economic environment of firms changes a lot between the period during which a model is designed and when it is used to make forecasts, we noticed that predictions at a 3-year horizon tend to become less stable than those achieved when economic conditions are fairly similar over these two periods. The consequence of this can be found in the distribution of the expected costs of misclassification. The differences in terms of expected cost of misclassification, between the two types of models, are less important when they are calculated with data from 2008 and 2009 than when they are assessed with data from the four other periods.

Failure-based models exhibit, on the whole, better performances than those achieved with traditional models, once the cost of misclassification of a failed firm reached 10 times that of a non-failed firm. However, this gap shrinks in a few situations during which economic conditions really affect model accuracy.

Actually, the difference between the error achieved with failure-based models and that achieved with traditional models is mainly the result of a better ability of the former to forecast the fate of failed firms. We can notice a slight difference, when it comes to forecast the fate of non-failed firms, in favor of failure-based models, but the most significant difference lies in predicting the fate of failed companies.

When the cost of misclassification of a failed firm increases, traditional models tend to classify in a similar way both groups of companies, and when this cost is high, these models tend to slightly better classify failed firms than others: Type-I error is often a bit lower than Type-II error, but the difference is not always very large. On the contrary, with failure-based models, such a difference is larger on average and, when the cost of misclassification of a failed firm is high, these models better classify failed firms than non-failed, and most of the time, they do so better than other models: Type-I error is on average much lower than Type-II error, while Type-II error is rather similar to that of traditional models.

Therefore, when the spread between the costs of the two types of error is large, which corresponds to a situation that is close to that commonly faced by financial institutions, failure-based models show a better ability to correctly forecast the fate of the most problematic firms than common models.

@&#CONCLUSION@&#

In this study, we have designed financial failure models that take into account the way firms move in the failure space over a short period of time. This period corresponds, for firms that finally went bankrupt, to the last 3 years of their operational activity.

This approach leads to models that, at a 1-year horizon, are as accurate as traditional models commonly used in the financial literature or used by financial institutions. By contrast, it leads to forecasts that exhibit a better stability over time, up to a 3-year horizon. Even if variations occurred among the results obtained with different modeling methods, or different periods during which samples were collected, on the whole this stability is indeed the result of the fact that failure-models attempt to account for the history of firms and also for the different behaviors they adopt to survive and to fight against decline. This approach also makes it possible to better identify vulnerabilities of some companies or the ability of some others to survive, than traditional models do, and may strongly influence their fate in the mid-term. Also, when sudden changes occur in the economic environment of firms, failure-based models achieve better forecasts than traditional models, but the stability over time of their results is not as pronounced as that achieved during other more quiet periods.

Therefore, failure-based models represent a valuable alternative to traditional models that can be used to forecast bankruptcy in the mid-term. But one might also consider using them to classify firms by their expected default risk, as rating agencies do. One should therefore estimate prototype processes which represent distinct classes of risk that would themselves serve to assign a rate to any company. This is an avenue for future research.

@&#ACKNOWLEDGMENTS@&#

We are very grateful to the two anonymous reviewers for their substantial contribution to the improvement of this article.


                     
                     
                  

@&#REFERENCES@&#

