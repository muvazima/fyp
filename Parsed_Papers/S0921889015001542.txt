@&#MAIN-TITLE@&#Service robot system with an informationally structured environment

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Introduction of architecture and components of the ROS–TMS.


                        
                        
                           
                           Integration of various data from distributed sensors for service robot system.


                        
                        
                           
                           Object detection system (ODS) using RGB-D camera.


                        
                        
                           
                           Motion planning for a fetch-and-give task using a wagon and a humanoid robot.


                        
                        
                           
                           Handing over an object to a human using manipulability of both a robot and a human.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Informationally structured environment

Intelligent space

Distributed sensor

Service robot

Robot operating system

Motion planning

@&#ABSTRACT@&#


               
               
                  Daily life assistance is one of the most important applications for service robots. For comfortable assistance, service robots must recognize the surrounding conditions correctly, including human motion, the position of objects, and obstacles. However, since the everyday environment is complex and unpredictable, it is almost impossible to sense all of the necessary information using only a robot and sensors attached to it. In order to realize a service robot for daily life assistance, we have been developing an informationally structured environment using distributed sensors embedded in the environment. The present paper introduces a service robot system with an informationally structured environment referred to the ROS–TMS. This system enables the integration of various data from distributed sensors, as well as storage of these data in an on-line database and the planning of the service motion of a robot using real-time information about the surroundings. In addition, we discuss experiments such as detection and fetch-and-give tasks using the developed real environment and robot.
               
            

@&#INTRODUCTION@&#

Aging of the population is a common problem in modern societies, and rapidly aging populations and declining birth rates have become more serious in recent years. For instance, the manpower shortage in hospitals and elderly care facilities has led to the deterioration of quality of life for elderly individuals. Robot technology is expected to play an important role in the development of a healthy and sustainable society. In particular, daily life assistance for elderly individuals in hospitals and care facilities is one of the most urgent and promising applications for service robots.

For a service robot, information about its surrounding, such as the positions of objects, furniture, humans, and other robots is indispensable for safely performing proper service tasks. For example, in order to deliver an object requested by a user, a robot must know the location of the target object and which trajectory should be followed to reach the object. However, current sensing technology, especially for cases of robots equipped with external sensors, is not good enough to complete these tasks satisfactorily. For example, a vision system is susceptible to changes in lighting conditions and the appearances of objects. Moreover, the field of vision is rather narrow. Although occlusions can be partly solved by sensors on a mobile robot, background changes and unfavorable vibrations of a robot body make processes more difficult. In addition, the payload of a robot is not so high and computer resources are also limited.

On the other hand, fixed sensors in an environment are more stable and can more easily gather information about the environment. If a sufficient number of sensors can be embedded in the environment in advance, occlusion is no longer a crucial problem. Information required to perform tasks is acquired by distributed sensors and transmitted to a robot on demand. The concept of making an environment smarter rather than the robot is referred to as an informationally structured environment [1–6].

An informationally structured environment is a feasible solution for introducing service robots into our daily lives using current technology, and several systems that observe human behavior using distributed sensor systems and provide proper service tasks according to requests from human or emergency detection, which is triggered automatically, have been proposed [1,7–12]. Several service robots that act as companions to elderly people or as assistants to humans who require special care have been developed [13–18].

We also have been developing an informationally structured environment for assisting in the daily life of elderly people in our research project, i.e., the Robot Town Project [2,19]. The goal of this project is to develop a distributed sensor network system covering a town-size environment consisting of several houses, buildings, and roads, and to manage robot services appropriately by monitoring events that occur in the environment. Events sensed by an embedded sensor system are recorded in the Town Management System (TMS), and appropriate information about the surroundings and instructions for proper services are provided to each robot [2].

We also have been developing an informationally structured platform (Fig. 1
                     ) in which distributed sensors (Fig. 2
                     a) and actuators are installed to support an indoor service robot (Fig. 2b). For example, in this platform, objects such as books, pens, pet bottles, chairs, and desks are detected by embedded sensors and RFID tags, and all of the data are stored in the TMS database. A service robot performs various service tasks according to the environmental data stored in the TMS database in collaboration with distributed sensors and actuators, for example, installed in a refrigerator to open a door.

We herein introduce a new Town Management System called the ROS–TMS. In this system, the Robot Operating System (ROS [20]) is adopted as a communication framework between various modules, including distributed sensors, actuators, robots, and databases. Thanks to the ROS, we were able to develop a highly flexible and scalable system. Adding or removing modules such as sensors, actuators, and robots, to or from the system is simple and straightforward. Parallelization is also easily achievable.

We herein report the followings:


                     
                        
                           •
                           Introduction of architecture and components of the ROS–TMS

Object detection using a sensing system of the ROS–TMS

Fetch-and-give task using the motion planning system of the ROS–TMS.

The detection and fetch-and-give tasks are among the most frequent tasks for elderly individuals in daily life and are a typical application task for the proposed system.

The remainder of the present paper is organized as follows. After presenting related research in Section  2, we introduce the architecture and components of the ROS–TMS in Section  3. In Section  4, we describe the sensing system of the ROS–TMS for processing the data acquired from various sensors. Section  5 describes the robot motion planning system of the ROS–TMS used to design the trajectories for moving, gasping, giving, and avoiding obstacles using the information on the environment acquired by the sensing system. We present the experimental results for service tasks performed by a humanoid robot and the ROS–TMS in Section  6. Section  7 concludes the paper.

A considerable number of studies have been performed in the area of informationally structured environments/spaces to provide human-centric intelligent services. Informationally structured environments are referred to variously as home automation systems, smart homes, ubiquitous robotics, kukanchi, and intelligent spaces, depending on the field of research and the professional experience of the researcher.

Home automation systems or smart homes are popular systems that centralize the control of lighting, heating, air conditioning, appliances, and doors, for example, to provide convenience, comfort, and energy savings [21–23]. The informationally structured environment can be categorized in this system, but the system is designed to support not only human life but also robot activity for service tasks.

Hashimoto and Lee proposed an intelligent space in 1996 [24]. Intelligent spaces (iSpace) are rooms or areas that are equipped with intelligent devices, which enable spaces to perceive and understand what is occurring within them [24–27]. These intelligent devices have sensing, processing, and networking functions and are referred to as distributed intelligent networked devices (DINDs). One DIND consists of a CCD camera to acquire spatial information and a processing computer, which performs data processing and network interfacing. These devices observe the position and behavior of both human beings and robots coexisting in the iSpace.

Moreover, the concept of a physically embedded intelligent system (PEIS) has been introduced in 2005 [28]. A PEIS involves the intersection and integration of three research areas: artificial intelligence, robotics, and ubiquitous computing. Anything that consists of software components with a physical embodiment and interacts with the environment through sensors or actuators/robots is considered to be a PEIS, and a set of interconnected physically embedded intelligent systems is defined as a PEIS ecology. Inside a PEIS ecology, the software components should interact, communicate, and achieve goals together. Tasks can be achieved using either centralized or distributed approaches using the PEIS ecology [29,30].

Ubiquitous robotics [31] involves the design and deployment of robots in smart network environments in which everything is interconnected. The authors define three types of Ubibots: software robots (Sobots), embedded robots (Embots), and mobile robots (Mobots), which can provide services using various devices through any network, at any place and at any time in a ubiquitous space (u-space). Embots can evaluate the current state of the environment using sensors, and convey that information to users. Mobots are designed to provide services and explicitly have the ability to manipulate u-space using robotic arms. A Sobot is a virtual robot that has the ability to move to any location through a network and to communicate with humans. The present authors have previously demonstrated the concept of a PIES using Ubibots in a simulated environment and u-space [32,33].

In Europe, RoboEarth [34–36] is essentially a World Wide Web for robots, namely, a giant network and database repository in which robots can share information and learn from each other about their behavior and their environment. The goal of RoboEarth is to allow robotic systems to benefit from the experience of other robots, paving the way for rapid advances in machine cognition and behavior, and ultimately, for more subtle and sophisticated human–machine interactions.

The informationally structured environment/space (also referred to as Kukanchi, a Japanese word meaning interactive human-space design and intelligence [37,38]) has received a great deal of attention in robotics research as an alternative approach to the realization of a system of intelligent robots operating in our daily environment. Human-centered systems require, in particular, sophisticated physical and information services, which are based on sensor networks, ubiquitous computing, and intelligent artifacts. Information resources and accessibility within an environment are essential for people and robots. The environment surrounding people and robots should have a structured platform for gathering, storing, transforming, and providing information. Such an environment is referred to as an informationally structured space by the IEEE Symposium on Robotic Intelligence in Informationally Structured Space (RiiSS [39]).

In Section  5, we present a coordinate motion planning technique for a fetch-and-give including handing over an object to a person. The problem of handing over an object between a human and a robot has been studied in Human–Robot Interaction (HRI) [40–43]. In particular, the work that is closest to ours is the one by Dehais et al. [42]. In their study, physiological and subjective evaluation for a handing over task was presented. The performance of hand-over tasks were evaluated according to three criteria: legibility, safety and physical comfort. These criteria are represented as fields of cost functions mapped around the human to generate ergonomic hand-over motions. Although their approach is similar to our approach, we consider the additional criteria, that is, the manipulability of both a robot and a human for a comfortable and safety fetch-and-give task.

The problem of pushing carts using robots has been reported in many studies so far [44–50]. The earlier studies in pushing a cart were reported using a single manipulator mounted on a mobile base [44,45]. In these systems, a single manipulator held a cart at a single point, and the planning of effector force to produce desired trajectories was discussed. The problem of towing a trailer has also been discussed as an application of a mobile manipulator and a cart [46]. This work is close to the approach in this paper, however, a pivot point using a cart is placed in front of the robot in our technique.

The work that is closest to ours is the one by Scholz et al. They provided a solution for real time navigation in a cluttered indoor environment using 3D sensing [49]. Though the first attempt with the cart by Tan et al. [44,45] was limited to simple paths using an open-loop controller, Scholz et al. proposed the solution to execute smooth and arbitrary trajectories in a closed loop controller with PR2. Many previous works focus on the navigation and control problems for movable objects. On the other hand, we consider the problem including handing over an object to a human using a wagon, and propose a total motion planning technique for a fetch-and-give task with a wagon.

We have been developing an informationally structured architecture referred to as the Town Management System (TMS) for assisting in the daily life of elderly people [2]. In the present paper, we extend the TMS and develop a new Town Management System called the ROS–TMS.

A conceptual diagram of the ROS–TMS is shown in Fig. 3
                     . This system has three primary components, i.e., real-world, database, and cyber-world components. Events occurring in the real world, such as user behavior or user requests, and the current situation of the real world, such as the positions of objects, humans, and robots are sensed by a distributed sensing system. The gathered information is then stored in the database. Appropriate service commands are planned using the environmental information in the database and are simulated carefully in the cyber world using simulators, such as choreonoid [51]. Finally, service tasks are assigned to service robots in the real world.

In order to realize the above-described concept, the following functions are implemented in the ROS–TMS.


                     
                        
                           1.
                           Communication with sensors, robots, and databases.

Storage, revision, backup, and retrieval of real-time information in an environment.

Maintenance and providing information according to individual IDs assigned to each object and robot.

Notification of the occurrence of particular predefined events, such as accidents.

Task schedule function for multiple robots and sensors.

Human–system interaction for user requests.

Real-time task planning for service robots.

The ROS–TMS has unique features such as high scalability and flexibility, as described below.


                     
                        
                           •
                           Modularity: The ROS–TMS consists of 73 packages categorized into 11 groups and 151 processing nodes. Re-configuration of structures, for instance adding or removing modules such as sensors, actuators, and robots, is simple and straightforward owing to the high flexibility of the ROS architecture.

Scalability: The ROS–TMS is designed to have high scalability so that it can handle not only a single room but also a building and a town.

Diversity: The ROS–TMS supports a variety of sensors and robots. For instance, Vicon MX (Vicon Motion Systems Ltd.), TopUrg (Hokuyo Automatic), Velodyne 32e (Velodyne Lidar), and Oculus Rift (Oculus VR) are installed in the developed informationally structured platform (Fig. 1) for positioning and human–system interaction. SmartPal-IV and V (Yaskawa Electric Corp.), AR Drone (Parrot), TurtleBot2 (Yujin Robot Co., Ltd.), a wheelchair robot, and a mobile robot with a 5-DOF manipulator are also provided in order to accomplish a variety of service tasks.

Safety: Data gathered from the real world is used to perform simulations in the cyber world in order to evaluate the safety and efficiency of designed tasks. According to the simulation results, proper service plans are designed and provided by service robots in the real world.

Privacy protection: One important restriction in our intelligent environment is to install a small number of sensors to avoid interfering with the daily activity of people and to reduce the invasion of their privacy as far as possible. For this reason, we do not install conventional cameras in the environment. Although the robots are equipped with several cameras, these cameras are only used in certain restricted scenarios.

Economy: Sensors installed in an environment can be shared with robots and tasks, and thus we do not need to equip individual robots with numerous sensors. In addition, most sensors are processed by low-cost single-board computers in the proposed system. This concept has an advantage especially for the system consisting of multiple robots since robots can share the resources in the environment. Currently, the cost of a distributed sensor network may be higher than the cost of a single robot. However, we believe that distributed sensor network (or Internet of Things, IoT) will become a standard facility in our society in the near future, and the cost of a sensor network will be greatly reduced since all the devices will be designed to be connected each other.

Some features mentioned above such as modularity, scalability, and diversity owe much to ROS’s outstanding features. On the other hand, economical or processing efficiency strongly depends on the unique features of ROS–TMS, since various information gathered by distributed sensor networks is structured and stored to the database and repeatedly utilized for planning various service tasks by robots or other systems.


                     Fig. 4
                      shows the overall architecture of ROM-TMS. This system is composed of five components: user, sensor, robot, task, and data. These components are also composed of sub-modules, such as the User Request sub-module for the user component, the Sensor Driver sub-module, the Sensing System and State Analyzer sub-modules for the sensor component, the Robot Controller, the Robot Motion Planning, and the Robot Service sub-modules for the robot component, the Task Scheduler sub-module for the task component, and the Database sub-module for the data component. Brief explanations of each node group are presented in Table 1
                     . The details of these components will be described later herein. 
                  

The sensing system (TMS_SS) is a component of the ROS–TMS that processes the data acquired from various environment sensors. In the current platform (Fig. 1), TMS_SS is composed of three sub-packages as described below and is described in Table 1.


                     
                        
                           1.
                           Floor sensing system (FSS)

Intelligent cabinet system (ICS)

Object detection system (ODS)

The current platform (Fig. 1) is equipped with a floor sensing system to detect objects on the floor and people walking around. This sensing systems is composed of a laser range finder located on one side of the room and a mirror installed along another side of the room. This configuration allows a reduction of dead angles of the LRF and is more robust against occlusions [52]. An example of object detection using this system is shown in Fig. 6
                        . People tracking is performed by first applying static background subtraction and then extracting clusters in the remainder of the measurements. Clusters are later tracked by matching profiles of cluster corresponding to legs and extending the motion using the accelerations of the legs. Moreover, this system can measure the poses of the robot and movable furniture such as a wagon using tags, which have encoded reflection patterns optically identified by the LRF [53].

The cabinets installed in the room are equipped with RFID readers and load cells to detect the types and positions of the objects in the cabinet. Every object in the environment has an RFID tag containing a unique ID that identifies it. This ID is used to retrieve the attributes of the object, such as its name and location in the database. Using the RFID readers, we can detect the presence of a new object inside the cabinet. In addition, the load cell information allows us to determine its exact position inside the cabinet. An example of object detection in the intelligent cabinet is shown in Fig. 7
                        . Additional details about the intelligent cabinets used herein can be found in [54].

If neither the FSS nor the ICS is available for detecting objects such as those placed on a desk, the object detection system using a RGB-D camera on a robot is provided in this platform as shown in Fig. 8
                        . In this system, a newly appeared object or movement of an object is detected as a change in the environment. The steps of the change detection process are as follows.


                        
                           
                              1.
                              Identification of furniture

Alignment of the furniture model

Object extraction by furniture removal

Segmentation of objects

Comparison with the stored information

It is possible to identify furniture based on the position and posture of robots and furniture in the database. Using this information, robot cameras determine the range of the surrounding environment that is actually being measured. Afterwards, the system superimposes these results and the position information for furniture to create an updated furniture location model.

The point cloud (Fig. 9
                           a) acquired from the robot is superimposed with the furniture’s point cloud model (Fig. 9b). The results are shown in Fig. 9c. After merging the point cloud data (Fig. 9a) and the point cloud model (Fig. 9b), as shown in Fig. 9c, the system deletes all other points except for the point cloud model for the furniture and limits the processing range from the upcoming steps.

We scan twice for gathering point cloud datasets of previous and current scene. In order to detect the change in the newly acquired information and stored information, it is necessary to align two point cloud datasets obtained at different times because these data are measured from different camera viewpoints. In this method, we do not try to directly align the point cloud data, but rather to align the data using the point cloud model for the furniture. The reason for this is that we could not determine a sufficient number of common areas by simply combining the camera viewpoints from the two point cloud datasets and can also reduce the amount of information that must be stored in memory. Using the aligned point cloud model, it is possible to use the point cloud data for objects located on the furniture, without having to use the point cloud data for furniture from the stored data. The alignment of the furniture model is performed using the ICP algorithm.

After alignment, all points corresponding to furniture are removed to extract an object. When removing the furniture according to the method described in a previous study using voxelized shape and color histograms [55], it is particularly difficult to cleanly remove items of furniture such as beds, blankets, and pillows, because they are easily deformable by hand or other objects. As such, the system removes furniture according to segmentation using color information and three-dimensional positions. More precisely, the point cloud is converted to a RGB color space and then segmented using a region-growing method. Each of the resulting segments is segmented based on the XYZ space. Using this method, each of these last segments becomes a continuous area of the same color. The system then selects only those segments that overlap with the model and then removes these segments. Fig. 10
                            shows the results of bed removal after blankets have been changed.

After performing the above-mentioned processing, only the points associated with objects placed on furniture remain. These points are further segmented based on XYZ space. The resulting segments are stored in the database.

Finally, the system associates each segment from the previously stored information with the newly acquired information. As a result, the system finds the unmatched segments and captures the movement of objects that has occurred since the latest data acquisition. In other words, the segments that did not match between the previous dataset and the newly acquired dataset, reflect objects that were moved, assuming that the objects were included in the previously stored dataset. On the other hand, the segments that appear in the most recent dataset, but not in the previously stored dataset, reflect objects that were recently placed on the furniture. The set of segments that are included in the association process are determined according to the center position of segments. For the segments sets from the previous dataset and the newly acquired dataset, the association is performed based on a threshold distance between their center positions, considering the shape and color of the segments as the arguments for the association. We use an elevation map (Fig. 11
                           b) that describes the height of furniture above the reference surface level to represent the shape of the object.

The reference surface level of furniture is, more concretely, the top surface of a table or shelf, the seat of a chair, or the sleeping surface of a bed. The elevation map is a grid version of the reference surface level and is a representation of the vertical height of each point with respect to the reference surface level on each grid.

When various vertical height measurements occur within a single grid, the chosen measurement value corresponds to the highest point from the reference surface. Then, comparison is performed on the elevation map for each segment, taking into consideration the variations in size, the different values obtained from each grid, and the average value for the entire map. The color information used to analyze the correlation between segments is the hue (H) and saturation (S). A two-dimensional histogram for H and S is created for each segment, as shown in Fig. 11c. The number of bins for H and S is 32. Using these H–S histograms, the previous data and the newly acquired data are compared, allowing the system to determine whether it is dealing with the same objects. The Bhattacharyya distance 
                              B
                              C
                              
                                 (
                                 p
                                 ,
                                 q
                                 )
                              
                            within H–S histograms 
                              
                                 (
                                 p
                                 ,
                                 q
                                 )
                              
                            is used for determining the similarity between histograms and is calculated according to Eq. (1). 
                              
                                 (1)
                                 
                                    B
                                    C
                                    
                                       (
                                       p
                                       ,
                                       q
                                       )
                                    
                                    =
                                    
                                       
                                          ∑
                                       
                                       
                                          h
                                          =
                                          0
                                       
                                       
                                          31
                                       
                                    
                                    
                                       
                                          ∑
                                       
                                       
                                          s
                                          =
                                          0
                                       
                                       
                                          31
                                       
                                    
                                    
                                       
                                          p
                                          
                                             (
                                             h
                                             ,
                                             s
                                             )
                                          
                                          q
                                          
                                             (
                                             h
                                             ,
                                             s
                                             )
                                          
                                       
                                    
                                 
                              
                           
                        

Once distance values are calculated, the object can be assumed to be the same as for the case in which the degree of similarity is equal to or greater than the threshold value. According to the scenarios for the two tables shown in Fig. 12
                           a and b, the objects placed on top of the tables becomes segmented, and the correlation between these segments is calculated using only H–S histograms. Fig. 12c shows the Bhattacharyya distance of the H–S histograms according to the overall combination of the previously stored dataset and the newly acquired dataset. If the distance is the highest for both sectors and is equal to or greater than the threshold, the system determines it to be the same object. The distances shown in bold represent the values for which it is regarded as the same object.

Robot motion planning (TMS_RP) is the component of the ROS–TMS that calculates the movement path of the robot and the trajectories of the robot arm for moving, gasping, giving, and avoiding obstacles based on information acquired from TMS_SS and is described in Table 1.

We consider the necessary planning to implement services such as fetch-and-give tasks because such tasks are among the most frequent tasks required by elderly individuals in daily life. Moreover, robot motion planning includes wagons for services that can carry and deliver a large amount of objects, for example, at tea time or handing out towels to residents in elderly care facilities as shown in Fig. 14a.

Robot motion planning consists of sub-planning, integration, and evaluation of the planning described below to implement the fetch-and-give task.


                     
                        
                           1.
                           Grasp planning to grip a wagon

Position planning for goods delivery

Movement path planning

Path planning for wagons

Integration of planning

Evaluation of efficiency and safety

Each planning, integration, and evaluation process uses environment data obtained from TMS_DB and TMS_SS, so that no further actions are required in order to acquire data. Moreover, actions that integrate overall planning are obtained from a combination of various individual planning threads. The integration method is considered to be not only efficient but also safe in places such as hospitals and elderly care houses. The output consists of a series of actions that can be executed efficiently and safely.

In order for a robot to push a wagon, the robot needs to grasp the wagon at first. There is an infinite number of base positions that the robot can have relative to a wagon that the robot must grip. However, a robot can push a wagon in a stable manner if the robot grasps the wagon from two poles positioned on its sides. Thus, the number of base position options for the robot with respect to the wagon is reduced to four (
                           i
                        ) as shown in Fig. 14. The position and orientation of the wagon, as well as its size, is managed using the ROS–TMS database. Using this information, it is possible to determine the correct relative position. This provides the distance, i.e., the control distance (CD), between the robot and the wagon when the robot is actually grasping the wagon. Based on the wagon direction when the robot is grasping its long side, valid candidate points can be determined using Eqs. Eq. (2) through (4) below 
                           
                              (
                              i
                              =
                              0
                              ,
                              1
                              ,
                              2
                              ,
                              3
                              )
                           
                        . Here, 
                           R
                         represents the robot, and 
                           W
                         represents the wagon. Subscripts 
                           x
                        , 
                           y
                        , and 
                           θ
                         represent the corresponding 
                           x
                        -coordinate, 
                           y
                        -coordinate, and posture (rotation of the 
                           z
                        -axis). Fig. 13
                         shows the positional relationship between the robot and the wagon, given 
                           i
                           =
                           2
                        . Moreover, Fig. 14
                         shows the wagon gripping position as a 3D model, given 
                           
                              
                                 W
                              
                              
                                 θ
                              
                           
                           =
                           0
                        . 
                           
                              
                                 (2)
                                 
                                    
                                       
                                          R
                                       
                                       
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                    =
                                    
                                       
                                          W
                                       
                                       
                                          x
                                       
                                    
                                    +
                                    
                                       (
                                       
                                          
                                             
                                                
                                                   W
                                                
                                                
                                                   
                                                      
                                                         
                                                            size
                                                         
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             2
                                          
                                       
                                       +
                                       C
                                       D
                                       )
                                    
                                    
                                       cos
                                    
                                    
                                       (
                                       
                                          
                                             W
                                          
                                          
                                             θ
                                          
                                       
                                       +
                                       
                                          
                                             i
                                          
                                          
                                             2
                                          
                                       
                                       π
                                       )
                                    
                                 
                              
                              
                                 (3)
                                 
                                    
                                       
                                          R
                                       
                                       
                                          
                                             
                                                y
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                    =
                                    
                                       
                                          W
                                       
                                       
                                          y
                                       
                                    
                                    +
                                    
                                       (
                                       
                                          
                                             
                                                
                                                   W
                                                
                                                
                                                   
                                                      
                                                         
                                                            size
                                                         
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             2
                                          
                                       
                                       +
                                       C
                                       D
                                       )
                                    
                                    
                                       sin
                                    
                                    
                                       (
                                       
                                          
                                             W
                                          
                                          
                                             θ
                                          
                                       
                                       +
                                       
                                          
                                             i
                                          
                                          
                                             2
                                          
                                       
                                       π
                                       )
                                    
                                 
                              
                              
                                 (4)
                                 
                                    
                                       
                                          R
                                       
                                       
                                          
                                             
                                                θ
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                    =
                                    
                                       
                                          W
                                       
                                       
                                          θ
                                       
                                    
                                    +
                                    π
                                    +
                                    
                                       
                                          i
                                       
                                       
                                          2
                                       
                                    
                                    π
                                 
                              
                              
                                 
                                    
                                       
                                          W
                                       
                                       
                                          
                                             
                                                
                                                   size
                                                
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                    =
                                    
                                       {
                                       
                                          
                                             
                                                length of the wagon’s long side
                                             
                                             
                                                
                                                   (
                                                   i
                                                   =
                                                   0
                                                   ,
                                                   2
                                                   )
                                                
                                             
                                          
                                          
                                             
                                                length of the wagon’s short side
                                             
                                             
                                                
                                                   (
                                                   i
                                                   =
                                                   1
                                                   ,
                                                   3
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

In order to hand over goods to a person, it is necessary to plan both the position of the goods to be delivered and the base position of the robot according to the person’s position. Using manipulability as an indicator for this planning, the system plans the position of the goods relative to the base position. Manipulability is represented by the degree to which hands/fingers can move when each joint angle is changed. When trying to deliver goods in postures with high manipulability, it is easier to modify the motion, even when small gaps exist between the robot and the person. Since it is difficult to know the exact position of the person, or to operate the robot without errors, a method that deals with these gaps is indispensable. The velocity vector 
                           
                              υ
                           
                         corresponds to the position of hands, and 
                           
                              q
                           
                         is the joint angle vector. In addition, we assume the high manipulability of the arm of the person makes him more comfortable for grasping goods. Their relation is represented in Eqs. (5) and (6). 
                           
                              
                                 (5)
                                 
                                    
                                       υ
                                    
                                    =
                                    
                                       J
                                    
                                    
                                       (
                                       
                                          q
                                       
                                       )
                                    
                                    
                                       
                                          
                                             q
                                          
                                       
                                       
                                          ̇
                                       
                                    
                                 
                              
                              
                                 (6)
                                 
                                    ω
                                    =
                                    
                                       
                                          
                                             det
                                          
                                          
                                          
                                             J
                                          
                                          
                                             (
                                             
                                                q
                                             
                                             )
                                          
                                          
                                             
                                                
                                                   J
                                                
                                             
                                             
                                                T
                                             
                                          
                                          
                                             (
                                             
                                                q
                                             
                                             )
                                          
                                       
                                    
                                 
                              
                           
                        
                     

If the arm has a redundant degree of freedom, an infinite number of joint angle vectors corresponds to just one hand position. Therefore, when solving the inverse kinematics of this issue, we calculate the posture that represents the highest manipulability within the range of possible joint angle movements. The planning procedure for the position of goods and the position of robots using manipulability is as follows:


                        
                           
                              1.
                              The system maps the manipulability that corresponds to the robots and each person on the local coordinate system.

Both manipulability maps are integrated, and the position of goods is determined.

Based on the position of goods, the base position of the robot is determined.

We set the robot as the origin of the robot coordinate system, assuming the frontal direction as the 
                           x
                        -axis and the lateral direction as the 
                           y
                        -axis. At each position on the XY plane, the manipulability is mapped for the situation in which objects are being carried by hand, as shown in Fig. 15
                        a. This mapping is superimposed along the 
                           z
                        -axis, which is the height direction, as shown in Fig. 15b. Thus, we create a three-dimensional manipulability map relative to the coordinate system of the robot. Similarly, we are also able to create a manipulability map for persons in Fig. 15b. Note that the current system uses a human model with a fixed size (1700 mm). However, we have proposed an estimation technique of a body shape using a statistical shape model of human and camera images [56], and thus we can apply the proposed technique for individual height adaptively with this technique.

The next step is to determine, using the manipulability map, the position of the goods that are about to be delivered. As shown in Fig. 16
                        a, we take the maximum manipulability value according to each height, and retain the XY coordinates of each local coordinate system. These coordinates represent the relationship between the base position and the positions of the hands. We apply the same process to the coordinates of persons, thus superimposing the manipulability maps for robots and people, as shown in Fig. 16b. In doing so, the 
                           z
                        -axis values on the manipulability map can be compensated for by using the face of the target person as a reference and synthesizing this data to suit the corresponding conditions of the person, for example, if the person is standing or sitting. As a result, the height value z in the absolute coordinate system used when delivering goods corresponds to the height of the sum of the maximum values of the manipulability.

According to the calculated height on the manipulability map for a person, the system requests the absolute coordinates of the goods to be delivered, using the previously retained relative coordinates of the hands. The position of the person that will receive the delivered goods is managed through TMS_SS and TMS_DB, and it is also possible to use this position as a reference point to request the position of the goods by fitting the relative coordinates. According to the aforementioned procedure, we can determine the unique position of the goods that are about to be delivered. As the final step, the base position of the robot is determined in order to hold out the goods to their previously calculated position. According to the manipulability map that corresponds to the height of a specific object, the system retrieves the relationship between the positions of hands and the base position. Using the position of the object as a reference point, the robot is able to hold the object out to any determined position if the base position meets the criteria of this relationship. Consequently, at the time of delivery, points on the circumference of the position of the object are determined to be candidate points on the absolute coordinate system of the base position. Considering all of the prospect points of the circumference, the following action planning, for which the system extracts multiple candidate points, is redundant. The best approach is to split the circumference 
                           n
                         time, fetch a representative point out of each sector after the split, and limit the number of candidate points. After that, the obtained representative points are evaluated as in Eq. (7), while placing special emphasis on safety. 
                           
                              (7)
                              
                                 
                                    
                                       E
                                    
                                    
                                       g
                                       i
                                       v
                                       e
                                       _
                                       o
                                       b
                                       j
                                       _
                                       p
                                       o
                                       s
                                    
                                 
                                 =
                                 V
                                 i
                                 e
                                 w
                                 +
                                 
                                    
                                       D
                                    
                                    
                                       h
                                       u
                                       m
                                       a
                                       n
                                    
                                 
                                 +
                                 
                                    
                                       D
                                    
                                    
                                       o
                                       b
                                       s
                                    
                                 
                              
                           
                        
                     

Here, 
                           V
                           i
                           e
                           w
                         is a Boolean value that represents whether the robot enters the field of vision of the target person. If it is inside the field of vision, then 
                           V
                           i
                           e
                           w
                         is 1, otherwise 
                           V
                           i
                           e
                           w
                         is 0. This calculation is necessary because if the robot can enter the field of vision of the target person, then the robot can be operated more easily and the risk of unexpected contact with the robot is also reduced. In the above equation, 
                           
                              
                                 D
                              
                              
                                 h
                                 u
                                 m
                                 a
                                 n
                              
                           
                         represents the distance to the target person, and 
                           
                              
                                 D
                              
                              
                                 o
                                 b
                                 s
                              
                           
                         represents the distance to the nearest obstacle. In order to reduce the risk of contact with the target person or an obstacle, the positions that represent the largest distance to the target person or obstacles are valued higher.

If all the candidate points on a given circumference sector result in contact with an obstacle, then the representative points of that sector are not selected. According to the aforementioned process, the base position of the robot is planned based on the position of the requested goods. The results for the case in which a person is standing still are shown in Fig. 17
                        a through 17c, and the results for the case in which a person is sitting by a table are shown for Fig. 17d and e. Although the robot stands in front of a person in Fig. 17a and b, Fig. 17b is slightly closer to the object (a table). The corresponding evaluation results are shown in Table 2
                        .

Path planning for robots that serve in a general living environment requires a high degree of safety, which can be achieved by lowering the probability of contact with persons. However, for robots that push wagons, the parameter space that uniquely defines this state has a maximum of six dimensions, that is, position (
                              x
                              ,
                              y
                           ) and posture (
                              θ
                           ) of a robot and a wagon, and planning a path that represents the highest safety values in such a space is time consuming. Thus, we require a method that produces a trajectory with a high degree of safety, but at the same time requires a short processing time. As such, we use a Voronoi map, as shown in Fig. 18
                           .

In order to be able to plan high-safety trajectories for wagons in real time, we need to reduce the dimensions of the path search space. The parameters that uniquely describe the state of a wagon pushing robot can have a maximum of six dimensions, but in reality the range in which the robot can operate the wagon is more limited. As such, for the case in which a robot is pushing a wagon, we set up a control point, as shown in Fig. 19
                           , which fixes the relative positional relationship of the robot with the control point. The operation of the robot is assumed to change in terms of the relative orientation (
                              
                                 
                                    W
                                 
                                 
                                    θ
                                 
                              
                           ) of the wagon with respect to the robot.

In addition, the range of relative positions is also limited. Accordingly, wagon-pushing robots are presented in just four dimensions, which shortens the search time for the wagon path planning. Path planning for wagon-pushing robots uses the above-mentioned basic path and is executed as follows:


                           
                              
                                 1.
                                 The start and end points are established.

The path for each robot along the basic path is planned.

According to each point on the path estimated in step 2, the position of the wagon control point is determined considering the manner in which the position of the wagon control point fits the relationship with the robot position.

If the wagon control point is not on the basic path (Fig. 20
                                    a), posture (
                                       
                                          
                                             R
                                          
                                          
                                             θ
                                          
                                       
                                    ) of the robot is changed so that the wagon control point passes along the basic path.

If the head of the wagon is not on the basic path (Fig. 20b), the relative posture (
                                       
                                          
                                             W
                                          
                                          
                                             θ
                                          
                                       
                                    ) of the wagon is modified so that it passes along the basic path.

Steps 3 through 5 are repeated until the end point is reached.


                           Fig. 21
                            shows the results of wagon path planning, using example start and end points. These start 
                              
                                 (
                                 
                                    
                                       R
                                    
                                    
                                       x
                                    
                                 
                                 ,
                                 
                                    
                                       R
                                    
                                    
                                       y
                                    
                                 
                                 ,
                                 
                                    
                                       R
                                    
                                    
                                       θ
                                    
                                 
                                 )
                              
                              =
                              
                                 (
                                 2380
                                 
                                 
                                    mm
                                 
                                 ,
                                 1000
                                 
                                 
                                    mm
                                 
                                 ,
                                 0
                                 °
                                 )
                              
                            and end points 
                              
                                 (
                                 
                                    
                                       R
                                    
                                    
                                       x
                                    
                                 
                                 ,
                                 
                                    
                                       R
                                    
                                    
                                       y
                                    
                                 
                                 ,
                                 
                                    
                                       R
                                    
                                    
                                       θ
                                    
                                 
                                 )
                              
                              =
                              
                                 (
                                 450
                                 
                                 
                                    mm
                                 
                                 ,
                                 2300
                                 
                                 
                                    mm
                                 
                                 ,
                                 −
                                 6
                                 °
                                 )
                              
                            use the outcomes of wagon grip position planning and position planning for goods delivery. This confirms that the movement traces of the wagon (indicated by green rectangles) are within the movement traces of the robot (indicated by the rounded gray shapes). Using this procedure we can simplify the space search without sacrificing the safety of the basic path diagram. The actual time required to calculate the path of a single robot was 1.10 (ms), and the time including the wagon path planning was 6.41 (ms).

We perform operation planning for overall item-carrying action, which integrates position, path and arm motion planning. First, we perform wagon grip position planning in order for the robot to grasp a wagon loaded with goods. Next, we perform position planning for goods delivery in order to hand-deliver goods to the target person. The results of these work position planning tasks becomes the candidate movement target positions for the path planning of the robot and the wagon. Finally, we perform an action planning that combines the above-mentioned planning tasks, from the initial position of the robot to the path the robot takes until grasping the wagon, and the path the wagon takes until the robot reaches the position at which the robot can deliver the goods. For example, if there are four candidate positions for wagon gripping and four candidate positions for goods delivery around the target person, then we can plan 16 different actions, as shown in Fig. 22
                        . The various action sequences obtained from this procedure are then evaluated to choose the optimum sequence.

We evaluate each candidate action sequence based on efficiency and safety, as shown in Eq. (8). 
                           
                              (8)
                              
                                 
                                    
                                       E
                                    
                                    
                                       v
                                       a
                                       l
                                       u
                                       e
                                    
                                 
                                 =
                                 α
                                 
                                    
                                       L
                                       e
                                       
                                          
                                             n
                                          
                                          
                                             m
                                             i
                                             n
                                          
                                       
                                    
                                    
                                       L
                                       e
                                       n
                                       g
                                       t
                                       h
                                    
                                 
                                 +
                                 β
                                 
                                    
                                       R
                                       o
                                       
                                          
                                             t
                                          
                                          
                                             m
                                             i
                                             n
                                          
                                       
                                    
                                    
                                       R
                                       o
                                       t
                                       a
                                       t
                                       i
                                       o
                                       n
                                    
                                 
                                 +
                                 γ
                                 V
                                 i
                                 e
                                 w
                                 R
                                 a
                                 t
                                 i
                                 o
                              
                           
                        
                     

The 
                           α
                           ,
                           β
                           ,
                           γ
                         are respectively the weight values of 
                           L
                           e
                           n
                           g
                           t
                           h
                        , 
                           R
                           o
                           t
                           a
                           t
                           i
                           o
                           n
                         and 
                           V
                           i
                           e
                           w
                           R
                           a
                           t
                           i
                           o
                        . The 
                           L
                           e
                           n
                           g
                           t
                           h
                         and 
                           R
                           o
                           t
                           a
                           t
                           i
                           o
                           n
                         represent the total distance traveled and total rotation angle. The 
                           L
                           e
                           
                              
                                 n
                              
                              
                                 m
                                 i
                                 n
                              
                           
                         and 
                           R
                           o
                           
                              
                                 t
                              
                              
                                 m
                                 i
                                 n
                              
                           
                         represent the minimum values of all the candidate action. First and second terms of Eq. (8) are the metrics for efficiency of action. 
                           V
                           i
                           e
                           w
                           R
                           a
                           t
                           i
                           o
                         is the number of motion planning points in the person’s visual field out of total number of motion planning point. It means the percentage for the number of motion planning points that exist in the visual field of person.

@&#EXPERIMENTS@&#

In this section, we present the results of fundamental experiments described below using an actual robot and the proposed ROS–TMS.


                     
                        
                           1.
                           Experiment to detect changes in the environment

Experiment to examine gripping and delivery of goods

Simulation of robot motion planning

Service experiments

Verification of modularity and scalability

We conducted experiments to detect changes using ODS (Section  4.3) with various pieces of furniture. We consider six pieces of target furniture, including two tables, two shelves, one chair, and one bed.

For each piece of furniture, we prepared 10 sets of previously stored data and newly acquired data of kinds of goods including books, snacks, cups, etc., and performed point change detection separately for each set.

As the evaluation method, we considered the ratio of change detection with respect to the number of objects that were changed (change detection ratio). Furthermore, we also considered over-detection, which occurs when the system detects a change that has actually not occurred. The results of this experiment are shown in Table 3
                        . The change detection ratios for each furniture type are as follows: 93.3% for tables, 93.4% for shelves, 84.6% for chairs, and 91.3% for beds. Examples of change detection for each piece of furniture are shown in Fig. 23
                        . The sections enclosed by circles in each image represent points that actually underwent changes.

We performed an operation experiment in which a robot grasps an object located on a wagon and delivers the object to a person. As a prerequisite for this service, the goods are assumed to have been placed on the wagon, and their positions are known in advance. After performing the experiment 10 times, the robot successfully grabbed and delivered the object in all cases. The operating state is shown in Fig. 24
                        .

We measured the displacement of the position of goods (
                           
                              
                                 O
                              
                              
                                 x
                              
                           
                         or 
                           
                              
                                 O
                              
                              
                                 y
                              
                           
                         in Fig. 25
                        ) and the linear distance (
                           d
                        ) between the measured value and the true value at the time of delivery, to verify the effect of rotation errors or arm posture errors. The results are listed in Table 4
                        . Note that the truth value is measured using a tape measure.

The distance error of the position of the goods at the time of delivery was 35.8 mm. According to the manipulability degree, it is possible to cope with these errors, because the system plans a delivery posture with some extra margin in which persons and robots can move their hands.

We set up one initial position for the robot 
                           
                              (
                              
                                 
                                    R
                                 
                                 
                                    x
                                 
                              
                              ,
                              
                                 
                                    R
                                 
                                 
                                    y
                                 
                              
                              ,
                              
                                 
                                    R
                                 
                                 
                                    θ
                                 
                              
                              )
                           
                           =
                           
                              (
                              1000
                              
                              
                                 mm
                              
                              ,
                              1000
                              
                              
                                 mm
                              
                              ,
                              0
                              °
                              )
                           
                        , the wagon 
                           
                              (
                              
                                 
                                    W
                                 
                                 
                                    x
                                 
                              
                              ,
                              
                                 
                                    W
                                 
                                 
                                    y
                                 
                              
                              ,
                              
                                 
                                    W
                                 
                                 
                                    θ
                                 
                              
                              )
                           
                           =
                           
                              (
                              3000
                              
                              
                                 mm
                              
                              ,
                              1000
                              
                              
                                 mm
                              
                              ,
                              0
                              °
                              )
                           
                        , and the target person 
                           
                              (
                              
                                 
                                    H
                                 
                                 
                                    x
                                 
                              
                              ,
                              
                                 
                                    H
                                 
                                 
                                    y
                                 
                              
                              ,
                              
                                 
                                    H
                                 
                                 
                                    θ
                                 
                              
                              )
                           
                           =
                           
                              (
                              1400
                              
                              
                                 mm
                              
                              ,
                              2500
                              
                              
                                 mm
                              
                              ,
                              −
                              90
                              °
                              )
                           
                         and assume the person is in a sitting state as shown in Fig. 26
                        a. Moreover, the range of vision of this person is shown in Fig. 26b by the red area. For each motion planning, two positions are candidate wagon grip positions (Fig. 27
                        b and 28
                        b), and three positions are candidate goods delivery positions (Fig. 27c–e and Fig. 28c–e), for a total of six possible action planning scenarios (Section  5.4).

The action planning result that passes over wagon grip candidate 1 is shown in Fig. 27, whereas the action planning result that passes over wagon grip candidate 2 is shown in Fig. 28. Furthermore, the evaluation values that changed the weight of each evaluation for each planning result are listed in Tables 5–7
                        
                        
                        . The actions of Plan 2–3 were the most highly evaluated (Table 5). Fig. 28a and d indicate that all of the actions occur within the field of vision of the person. Since the target person can monitor the robot’s actions at all times, the risk of the robot unexpectedly touching a person is lower, and if the robot misses an action, the situation can be dealt with immediately. The action plan chosen from the above results according to the proposed evaluation values exhibits both efficiency and high safety.

We performed a service experiment for the carriage of goods, in accordance with the combined results of these planning sequences. The state of the sequence of actions is shown in Fig. 29
                        .

The initial conditions were the same for the simulation (Fig. 26), and were set up as shown in Fig. 29a. The procedure for service execution is as follows:


                        
                           
                              1.
                              Path planning is executed from the initial position to the position at which the object is delivered.

The robot moves from its initial position to the position at which it grips the wagon.

The robot position is corrected according to its RGB-D camera.

The arm trajectory is planned and executed in order to grasp the wagon (Fig. 29b).

The robot moves to the delivery position while pushing the wagon (Fig. 29c).

The arm trajectory is planned and executed in order to separate the arm from the wagon.

The arm trajectory is planned and executed in order to grasp the target object from the top of the wagon (Fig. 29d).

The arm trajectory is planned and executed in order to hold the object in the position in which it is delivered (Fig. 29e and f).

This service was carried out successfully, avoiding any contact with the environment. The total time for the task execution is 312 sec in case the maximum velocity of SmartPal-V is limited to 10 mm/sec in terms of safety. Moreover, the robot position was confirmed to always be within the range of vision of the subject during execution. Accordingly, we can say that the planned actions had an appropriate level of safety. Moreover, there was a margin for the movement of hands, as shown in Fig. 29f, for which the delivery process could appropriately cope with the movement errors of the robot.

Note that pushing and pulling motions of the wagon by the robot cause a large tracking error. In reality, the maximum error from desired trajectory was about 0.092 m in the experiments. In worst-cases scenario, if the robot grasps the wagon rigidly and collides with obstacles, hands or wagon may be broken due to large impact force. In the experiments, we wrap a sponge around a pole of the wagon to absorb the impact force. In addition, the start and goal positions are precisely measured by the optical poisoning system (Vicon MX) or the floor sensing system.

We built the ROS–TMS for three types of rooms to verify its high modularity and scalability. The size and the structure of the rooms, sensor configurations, and variety of sensors and robots are all different between these rooms. The simulation models and actual photos of the rooms are shown in Figs. 30 and 31
                        
                        , respectively. In Room A (4 m×4 m, Figs. 30a and 31a), we used 18 packages and 32 processing nodes including a LRF, three RFID tag readers, a Xtion sensor and a humanoid robot. Room B (4.5 m×8 m, Figs. 30b and 31b) consists of 52 packages and 93 processing nodes including two LRFs, two RFID tag readers, four Xtion sensors, ten Vicon cameras, two humanoid robots, a floor cleaning robot, a robotic refrigerator, and a wheeled chair robot. On the other hand, Room C (8 m×15 m, Figs. 30c and 31c–f) utilizes 73 packages and 151 processing nodes including eight LRFs, two RFID tag readers, one Xtion sensor, five Kinect sensors, eighteen Vicon cameras, three humanoid robots, a floor cleaning robot, a mobile manipulator robot, and a wheeled chair robot. Thanks to high flexibility and scalability of the ROS–TMS, we could set up these various environments in a comparatively short time.

@&#CONCLUSIONS@&#

In the present paper, we have introduced a service robot system with an informationally structured environment named ROS–TMS that is designed to support daily activities of elderly individuals. The room considered herein contains several sensors to monitor the environment and a person. The person is assisted by a humanoid robot that uses information about the environment to support various activities. In the present study, we concentrated on detection and fetch-and-give tasks, which we believe will be among most commonly requested tasks by the elderly in their daily lives. We have presented the various subsystems that are necessary for completing this task and have conducted several independent short-term experiments to demonstrate the suitability of these subsystems, such as a detection task using a sensing system and a fetch-and-give task using a robot motion planning system of the ROS–TMS. Currently, we adopt a deterministic approach for choosing proper data from redundant sensory information based on the reliability pre-defined manually. Our future work will include the extension to the probabilistic approach for fusing redundant sensory information. Also, we intend to design and prepare a long-term experiment in which we can test the complete system for a longer period of time.

@&#ACKNOWLEDGMENTS@&#

This research is supported in part by a JSPS KAKENHI Grant Number 263244, a Grant-in-Aid for Scientific Research (A) (26249029) and the Japan Science and Technology Agency (JST) through its “Center of Innovation Science and Technology based Radical Innovation and Entrepreneurship Program (COI Program).”

We would like to thank all members of the ROS community and ROS development teams at Open Source Robotics Foundation.

@&#REFERENCES@&#

