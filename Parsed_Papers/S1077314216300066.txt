@&#MAIN-TITLE@&#Enhanced control of a wheelchair-mounted robotic manipulator using 3-D vision and multimodal interaction

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Wheelchair-mounted robotic arm integrates 3D computer vision with multimodal input.


                        
                        
                           
                           Object recognition was improved by combining RGB information and 3D point clouds.


                        
                        
                           
                           Hybrid input (using both gestures or speech) outperformed using a single modality.


                        
                        
                           
                           Input performance was validated for daily living tasks: feeding and dressing.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

3D vision

Multi-modal interface

Wheelchair mounted robotic manipulator

Assistive robotics

@&#ABSTRACT@&#


               
               
                  This paper presents a multiple-sensors, 3D vision-based, autonomous wheelchair-mounted robotic manipulator (WMRM). Two 3D sensors were employed: one for object recognition, and the other for recognizing body parts (face and hands). The goal is to recognize everyday items and automatically interact with them in an assistive fashion. For example, when a cereal box is recognized, it is grasped, poured in a bowl, and brought to the user. Daily objects (i.e. bowl and hat) were automatically detected and classified using a three-steps procedure: (1) remove background based on 3D information and find the point cloud of each object; (2) extract feature vectors for each segmented object from its 3D point cloud and its color image; and (3) classify feature vectors as objects after applying a nonlinear support vector machine (SVM). To retrieve specific objects, three user interface methods were adopted: voice-based, gesture-based, and hybrid commands. The presented system was tested using two common activities of daily living -- feeding and dressing. The results revealed that an accuracy of 98.96% is achieved for a dataset with twelve daily objects. The experimental results indicated that hybrid (gesture and speech) interaction outperforms any single modal interaction.
               
            

@&#INTRODUCTION@&#

Wheelchair-mounted robotic manipulators (WMRMs) have been developed to assist users with motor impairments to accomplish activities of daily living (ADL), such as feeding, dressing, and retrieval of daily objects (Amat, 1998). WMRMs can improve operational functions of users with upper extremity motor impairments (UEMIs) and reduce their needs for human assistance (Redwan Alqasemi, 2010).

Previous studies showed that a WMRM could benefit users, such as quadriplegics due to spinal cord injuries (SCIs) (Garber et al., 2003) and cerebral palsy (Kwee et al., 2002). Most UEMIs lack fine motor skills required to grasp and retrieve daily objects. To bridge this gap, we want to enhance current WMRMs to recognize and manipulate objects automatically. Machine vision based interfaces are key to accomplish these goals (Yanco, 2000). Vision-based systems can provide UEMIs with great autonomy and less reliance on human assistants. For example, Tanaka et al.,(2010) developed a WMRM for delivering, retrieving, and positioning objects with two web cameras utilizing 2D information (one for object recognition, the other for face detection). Besides recognizing objects with little movement, 2 D visual information has also been applied to detect motion and track moving objects in the development of an intelligent wheelchair in a smart home system (Ktistakis et al., 2015). For more reliable object recognition and human body parts detection, object recognition algorithm consolidating both 2D and 3D information was developed for this study. Two 3-D sensors (KinectTM and PrimeSenseTM) were used for daily object recognition, grasping, and positioning within the human body.

The aforementioned optic sensors were successfully integrated with commercially-available assistive robots (Kevin Edwards, 2006). Previously, a JACO™ (Kinova®) robotic arm with six degrees of freedom (DoF) was integrated with a vision-based system for automatic object retrieval due to its light weight and ease to be mounted to motorized wheelchair (Jiang et al., 2014, Jiang et al., 2013). Nevertheless, research related to human robot interaction (HRI) addressing the problem of WMRMs’ control is still lacking. While traditional modalities, such as joysticks, keypads, or teach pendants, have been designed to control commercial robotic arms (Jiang et al., 2013), most of them do not consider the operational limitations of UEMIs. Our paper explores intuitive control modalities (i.e. voice and gesture) as well as autonomous control to enhance WMRM operation.

The focus of the current paper is to improve on the main drawback of a previous autonomous vision-based system that was developed for users to control a WMRM using gesture commands (Jiang et al., 2014). In this system daily objects were automatically recognized using 2D information, while human body parts (face and hands) were recognized and localized using both color and depth information. In this previous system, users with UEMIs could independently perform daily tasks. However, with the previous system, we observed two shortcomings. First, objects were not reliably recognized when the orientation or positions of the objects were altered. Since the cameras were on fixed position, placing objects in different positions often resulted in a distorted view of the objects. This distortion occurs because only 2D information was used to represent objects, and visual information from the occluded views was lost. To solve this problem and to improve object recognition, we used 3D information as well as 2D color data to represent objects as 3D point clouds. The second limitation of our previous system was to limit users to control the WMRM using either voice or gesture commands. While voice commands were intuitive, they were vulnerable to noisy environment. Gesture commands responded quickly but required lengthy periods of learning. To tackle these limitations and leverage on each control modality, a hybrid control modality was developed in this study. Voice and gesture interface were enabled simultaneously, so that users could use either modality for each command. To summarize, the contribution of this paper is three-folds: (1) improving the versatility and reliability of object recognition algorithm by incorporating both 3D point cloud and RGB information; (2) enabling hand gesture, speech-, and hybrid (hand gestures and speech) control; and (3) developing a smart system by the integration of multimodal interface with autonomous object recognition and retrieval, and evaluating this system with more complex daily tasks based on real-world requirements.

The rest of the paper is organized as follows:
Section 2 provides a review of the related work. In Section 3, the architecture of the system is illustrated. In Section 4 the approaches for gesture recognition, object recognition, and robotic control are explained in details. Section 5 presents the experiments and results. Section 6 discusses and concludes the paper.

@&#RELATED WORK@&#

This section provides an overview of current research that pertains to this paper. First, vision-based assistive robotic systems are described and summarized. Then, various human-based interaction methods are presented and discussed.

A variety of vision-based assistive robotic systems have been developed to help users with various levels of motor impairments and improve their accessibility to their surrounding facilities and environment (Elisa Perez, 2012). Assistive robotic systems can be divided into three categories, according to different degrees of mobility that they provide (Song et al., 1999; Ktistakis and Bourbakis, 2015): workstation-based robotic systems, mobile robotic systems, and WMRM systems.

Workstation-based robotic systems integrated with desktop robots have been used for assistance to UEMIs with ADL tasks (Leifer et al., 1991). For example, the Desktop Vocational Assistant Robot (DeVar) is a workstation equipped with a robotic arm mounted over a desk which can be controlled by speech commands. My Spoon is a feeding robotic assistance developed by SECOM, which allowed UEMIs to eat independently (Ryoji Soyama, 1970). The food is detected and recognized using color image processing. The limitation of workstation-based robots is that they need to be set up within a restricted environment. To solve this hurdle, manipulators attached to mobile platforms were developed.


                        Care-O-robot 3 is an interactive mobile robot developed to assist UEMIs in ADL (Arbeiter et al., 2010). It is capable of detecting and grasping daily objects and autonomously delivering them to a pre-defined destination. Another popular mobile robot is the PR-2 developed by Willow Garage (no longer commercially available). It has two back-drivable arms and wrists for everyday object manipulation in a home or work environment (Chen et al., 2013). Most of mobile robotic systems are used as companion robots and are developed for navigating in a home environment.

A series of WMRM systems named FRIEND have been developed to assist users with severe disabilities in daily object manipulation tasks (Axel Graeser, 2012). The fourth generation of FRIEND is designed to assist quadriplegic librarians in books’ cataloging and collecting tasks (Graser et al., 2013). Schrock et al., (2009) developed a light-weighted WMRM named WMRM-II to assist wheelchair users with upper extremities motor impairments (Schrock et al., 2009). With the increased availability of commercial Assistive Robot Manipulator Devices, new applications are being enabled. For example, some of this research include and extend off-the-shelf products in creative ways. A system called PerMMA was developed using Manus to assist people with severe physical impairments in performing daily living tasks (Wang et al., 2013). Kim and his colleagues developed an 6 DOF intelligent WMRM system named UCF-MANUS for users with a wide range of disabilities (Kim et al., 2014). The performance of the UCF-MANUS II system was evaluated with ten post-SCI users (Kim et al., 2012) during a three-week user study. All the subjects reported an improvement of their functional abilities using UCF-MANUS II. Most of the aforementioned WMRM systems used vision information object detection, recognition, and grasping and for human body parts recognition for retrieval and interaction. There are two types of camera configurations used generally in WMRMs: fixed camera configuration (camera mounted on the wheelchair) and eye-in-hand (camera mounted on the end effector) (Palankar et al., 2009). The fixed configuration is commonly used to acquire the overview of the environment. For instance, this configuration was used to extract the shape of a book (Enjarini and Graser, 2014). In the same example, a stereo camera was fixed on the back of the wheelchair to acquire both color and depth information for the book's grasping. The eye-in-hand configuration facilitates capturing close view information and determining grasping points of objects while the robotic arm is moving (Muelling et al., 2015). Farelo et al., (2011) developed a 9 DOF WMRM using an eye-in-hand configuration for vision-based control (Farelo et al., 2011). The shortcoming of the eye-in-hand configuration is that as the tooltip gets close to the object, sensors struggle to capture accurate information due to the limited working range of depth camera. For instance KinectTM has a working range of 0.5m to 4m. Properly integrating these two approaches can lead to an effective WMRM system, such as FRIEND III and IV (Graser et al., 2013).

Our proposed system incorporates characteristics of workstation-based systems and those of WMRM systems. We used two 3D cameras (one fixed and the other attached to the arm) for objects recognition and human body parts localization. A comparison between our system and other similar state-of-the-art interfaces is shown in Table 1.
                        
                     

Naturalness and effectiveness have become two major considerations when designing an interface between human and robots. While joysticks and switches are classical ways to control a robot, speech commands, hand gestures, eye gazes, tongue movements and electroencephalographic (EEG) signals are emerging techniques that offer a higher level of naturalness for individuals with UEMIs. Joysticks are most common in commercially-available wheelchair mounted robotic manipulators (WMRMs). The problem with this type of interface is that UEMIs struggle to operate those due to the users’ limited fine motor control and stamina (Bailey et al., 2007; Jiang et al., 2013). Natural and intuitive interfaces that have been studied to control WMRMs, include the touchpad (Tsui et al., 2011), trackball, jelly switches (Park et al., 2007), speech and hand gesture recognition (Jiang et al., 2014), and body movement detection.

These interfaces have been integrated into WMRMs to help with object selection and manipulation during object retrieval activates (Tsui et al., 2008) (Table 1). With the development of the system KARES II (Bien et al., 2003), various control interfaces were integrated with a wheelchair-based rehabilitation robotic system, including eye gaze, EMG signals and a haptic device. People with different disabilities indicated individual preferences when using these interfaces. These interaction techniques are usually applied singularly; however human-to-human interaction is usually performed through multiple modalities either simultaneously or concurrently. For example, communication is often performed through speech and gestures. Therefore, multimodal interaction with robots should also be incorporated for more natural interaction.

Popular interaction approaches, such as speech commands and hand gestures, and brain-controlled interfaces (BCIs), have been applied to control WMRMs (Jiang et al., 2014; Palankar et al., 2009) with mixed outcomes. In spite of their naturalness and intuitive operation, they also have major drawbacks. Speech commands require noiseless environments, and hand gestures involve some physical effort. BCI usually requires a relatively long processing time. A good compromise is to integrate multiple interaction forms as means of improving control accuracy and interaction precision.

Multimodal interfaces are often referenced in the context of input control, output feedback, or both (Patrizia, 2009). When considered as input, it integrates multiple means of controlling a device. For example, in Koons et al., (1993) speech commands (e.g. “Move the blue circle there”) are used to move a blue circle to a specific place, where “there” was defined by a pointing gesture using a data glove. When multimodality refers to output feedback, information can be delivered to users in visual, auditory, haptic, and other sensory forms. In our work, a multimodal input method consisting of both speech commands and hand gestures was compared to using either one singularly.

The architecture of our system is illustrated in Fig. 1
                     . This 3D vision-based WMRM consists of five modules: (A) user interface, (B) automatic object recognition system, (C) human body part localization system, and (D) robotic arm control.

                        
                           A. User interface module

This module consists of three control methods including gesture recognition-based control, speech recognition based control, and hybrid (gesture and speech) control.

                        
                           B. Object recognition module

The goal of this module is to recognize objects in a complex scene. A series of models (spin images) were created for each object, and were then compared with each detected object in real-time to assert accuracy.

                        
                           C. Human body part localization module

This module provides persons with UEMIs an approach to position objects based on anthropometric relationships in order to deliver objects to specified parts of the body.

                        
                           D. WMRM control module

In this module typical feeding and dressing tasks were designed to test the validity of the system.

@&#METHODOLOGY@&#

In this section, the method proposed for each module in the system architecture is explained in detail.

Traditional used input modalities, such as keypad, joystick, and touchscreen, require fine motor skills for interaction. However, the target population of our system is people with UEMIs, who lack fine motor movements. Thus, traditional control modalities are not applicable in this case (Jiang et al., 2013). In our paper, three control modalities were adopted: (1) gesture recognition; (2) speech recognition; and 3) hybrid (gesture+speech).

The gesture recognition-based interface consists of four parts: (1) foreground segmentation; (2) face and hand detection; (3) face and hand tracking; and (4) gesture trajectory recognition. For a detailed description, refer to (Jiang et al., 2013, Jiang et al., 2014).

Two steps were adopted to segment the human body and its connected components (i.e. the wheelchair) as foreground. In the first step, the depth information acquired from a KinectTM sensor is used to threshold the depth image (ID
                           ) and the corresponding brightness pixels. Two depth thresholds (a low threshold TDL of 0.4
                           m and a high threshold TDH of 2.0
                           m) were applied to generate a binary mask image. If a pixel's value in the depth image ranged between TDL
                            and TDH
                           , the value of the corresponding pixel in the binary mask image is set to one, otherwise it is set to zero. In the second step, the largest blob was extracted from the mask image and set as foreground (If
                           ).

In the face and hand detection stage, the centroids of the face and hand regions were extracted to initialize the tracking stage. A face detector was used to determine the region (Rface
                           ) of the face (Viola and Jones, 2001). To detect hands’ regions, two (a skin and non-skin) histogram models (Hskin
                            and Hnonskin
                           ) were built using Compaq Database (Jones and Rehg, 2002). A Bayesian decision rule was applied to classify a pixel (denoted as x) in the color frame (IC
                           ) to a skin or non-skin category. Let p(x|skin) and p(x|nonskin) represent the conditional probability of x belonging to a skin or non-skin class, respectively and τ be a threshold (τ
                           =2). The pixels that satisfied Eq. 1 were used to generate a binary skin color mask image (Iskin
                           ). The hands’ regions (Rhands
                           ) were then determined by blob extraction. While there are additional segmentation approaches that haven shown satisfactory performance (Khan et al., 2012, Kakumanu et al., 2007, Tan et al., 2012), the one utilized in this work met the performance criteria required in our system.

                              
                                 (1)
                                 
                                    
                                       
                                          
                                             p
                                             
                                                (
                                                
                                                   x
                                                   
                                                   |
                                                   
                                                   s
                                                   k
                                                   i
                                                   n
                                                
                                                )
                                             
                                          
                                          
                                             p
                                             (
                                             x
                                             |
                                             n
                                             o
                                             n
                                             −
                                             s
                                             k
                                             i
                                             n
                                             )
                                          
                                       
                                       ≥
                                       τ
                                    
                                 
                              
                           
                        

In the tracking stage, a 3D particle filter framework incorporating color, depth, and spatial information was employed to track the centroids of face and hands (Cf, Ch1
                            and Ch2
                           ) through all video sequences. An interaction model was integrated to solve the “false labeling” (objects’ assigned labels exchange after occlusion or interaction) and “false merging” (more than one trackers focus on an objects with higher observation likelihood). To obtain optimal parameters for the particle filter framework, a neighborhood search method was adopted.

An eight-gesture lexicon previously created was used to control the WMRM (Jiang et al., 2012). A motion model (MG
                           ) was built for each gesture in the lexicon using training data collected from ten subjects. Dynamic time warping was applied to align all the trajectories temporally (Müller, 2007). Then, conditional density propagation (CONDENSATION) was used for gesture classification. Let St
                            represent the state at time t, then for both hands, we have:

                              
                                 (2)
                                 
                                    
                                       
                                          
                                             
                                                S
                                                t
                                             
                                          
                                          
                                             =
                                          
                                          
                                             
                                                (
                                                
                                                   μ
                                                   ,
                                                   
                                                      ϕ
                                                      i
                                                   
                                                   ,
                                                   
                                                   
                                                      α
                                                      i
                                                   
                                                   ,
                                                   
                                                   
                                                      ρ
                                                      i
                                                   
                                                
                                                )
                                             
                                          
                                       
                                       
                                          
                                          
                                             =
                                          
                                          
                                             
                                                (
                                                
                                                   μ
                                                   ,
                                                   
                                                      ϕ
                                                      
                                                         r
                                                         i
                                                         g
                                                         h
                                                         t
                                                      
                                                   
                                                   ,
                                                   
                                                   
                                                      ϕ
                                                      
                                                         l
                                                         e
                                                         f
                                                         t
                                                      
                                                   
                                                   ,
                                                   
                                                   
                                                      α
                                                      
                                                         r
                                                         i
                                                         g
                                                         h
                                                         t
                                                      
                                                   
                                                   ,
                                                   
                                                      α
                                                      
                                                         l
                                                         e
                                                         f
                                                         t
                                                      
                                                   
                                                   ,
                                                   
                                                      ρ
                                                      
                                                         r
                                                         i
                                                         g
                                                         h
                                                         t
                                                      
                                                   
                                                   ,
                                                   
                                                   
                                                      ρ
                                                      
                                                         l
                                                         e
                                                         f
                                                         t
                                                      
                                                   
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           where, μ is the index of the motion models, ϕ is the current phase in the model, α is an amplitude scaling factor, ρ is a time dimension scaling factor, i equals to right hand, or left hand.

The CMU sphinx API was used for speech recognition. CMU sphinx is a widely used open source toolkit for speech recognition (Lamere et al., 2003). The input voice signals were segmented into words and then compare with a pre-trained model. Whenever a key word is recognized, a corresponding command is sent to control the robotic arm.

Speech recognition-based control is selected because it does not restrict any upper limb movements. Although it may not be ideal under a noisy environment, it works well for users with UEMIs in a quiet indoor environment. Thus, speech recognition based control was selected to meet the physical limitations of users with profound motor impairments and was treated as a complementary input modality of gesture-based control.

During single modality control, gesture or speech recognition recognized instructions were sent in the form of commands to the WMRM. However, prolonged periods of using single modality may result in undesirable fatigue. This can be alleviated with multimodality. The hybrid control interface discussed in this section combines gesture and speech recognition-based control modalities into a single stream of imperatives. Users are able to preferentially choose a control modality and switch between these two interfaces in real-time to reduce physical strain. Both interfaces are always active when the system is running. Once a command is triggered via either interface, the robotic arm will initiate the task immediately. Subsequent commands given during the performance of a robotic task will be added in a queue and performed once the former task is completed.

The goal of human body parts localization is to provide customizable service for users with motor impairments based on their mobility restrictions and anthropometric differences, since the destination of object delivery could be different for each user or during different postures. Although Kinect provides skeletonization of human bodies without disabilities, it was unstable and had difficulties recognizing the arm movements of persons with UEMIs sitting in wheelchairs (Fig. 2
                        ). Therefore, in our study, the localization of upper limbs and the user's face were alternatively obtained using the method of hands and face detection and recognition explained in Section 4.1.1 (Algorithm 1
                        ).

An object recognition module was developed for users with UEMIs to accomplish automatic object retrieval for manipulating items (e.g. food) for use. A KinectTM sensor was used to acquire 3D point cloud data and color images for each frame in video sequences. The 3D shape and color information was used to describe the characteristics of each daily object. The object recognition pipeline is illustrated in Fig. 3
                        . Point clouds and color images, were the input to feature extraction algorithms. Support Vector Machine (SVM) was used for object classification. A detailed description of the pipeline is discussed in the following section.

The Point Cloud Library (PCL) (Rusu and Cousins, 2011) is used to pre-process the acquired point cloud and extract 3D features. A global descriptor, Clustered Viewpoint Feature Histogram (CVFH) (Aldoma et al., 2011), is applied to represent the point cloud into compact clusters, which in turn represent objects. CVFH is an extension of Viewpoint Feature Histogram (VFH) descriptor (Rusu et al., 2010), which uses a histogram representation of four angular distributions of surface normal to represent objects. CVFH has previously been used in similar contexts with satisfactory results in 3D recognition related challenges (Aldoma et al., 2011). Since CVFH is a global descriptor, it cannot be applied directly to the 3D clustered scenes. It is necessary to pre-process the clustered points first. This is accomplished through a pre-processing stage, which is applied before feature extraction to obtain the single point cloud for each object. The pre-processing stage consists of three steps: (1) thresholding the input point cloud (ƤI
                           ) using a pass through filter along z direction (keeping point clouds within the pre-defined range [τ1, τ2
                           ] along z direction and removing the clouds outside it) to obtain the scene (cloud) of the table and objects (ƤF
                           ), shown in Fig. 4
                           (b); (2) finding all the points (i.e. the point cloud of the table) within a cloud that support a plane model (ƤM
                           ) and removing the points belonging to the model from the cloud; (3) from the obtained cloud (ƤR
                           ), extracting each object's point cloud (p1,p2,…,pN
                           ), where N is the number of objects detected. Fig. 4(a) and (c) illustrates the point cloud scene before and after the pre-processing stage.

Eight steps were used to obtain the CVFH (Aldoma et al., 2011) descriptor (fCVFH
                           ): (1) describe a partial view of an objects using a set of points Ƥ; (2) determine the set of stable regions S found on Ƥ; (3) compute the centroid c and normal nc
                            for each point si
                            ∈ S; (4) fine a local Darboux coordinate system (Adams et al., 1993) D=(ui,vi,wi) for each point si
                            ∈ S (Eq. 3) (Rusu et al., 2010); (5) compute the normal angular deviations cos(αi), cos(βi), cos(Φi) and θi
                            for each point si
                            ∈ S and its normal ni
                            (Eq. 4); (6) use a 128-bin histogram to represent θi
                            and a 45-bin histogram to represent αi, βi
                           , and Φi
                           , respectively; (7) find the Shape Distribution Component (SDC) (Gao, 2002) using Eq. 5 represent it using a 45 bin histogram; (8) obtain a 308 bin (45+45+128+45+45) histogram by concatenating histograms (α, Φ, θ, SDC, β). A segmented object's point cloud and its CVFH are shown in Fig. 5
                           (a) and (b), respectively.

                              
                                 (3)
                                 
                                    
                                       
                                          
                                             
                                                u
                                                i
                                             
                                          
                                          
                                             =
                                          
                                          
                                             
                                                n
                                                c
                                             
                                          
                                       
                                       
                                          
                                             
                                                v
                                                i
                                             
                                          
                                          
                                             =
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         s
                                                         i
                                                      
                                                      −
                                                      c
                                                   
                                                   
                                                      |
                                                      
                                                         |
                                                         
                                                            
                                                               s
                                                               i
                                                            
                                                            −
                                                            c
                                                         
                                                         |
                                                      
                                                      |
                                                   
                                                
                                                ×
                                                
                                                   u
                                                   i
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                w
                                                i
                                             
                                          
                                          
                                             =
                                          
                                          
                                             
                                                
                                                   u
                                                   i
                                                
                                                ×
                                                
                                                   v
                                                   i
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                           
                              
                                 (4)
                                 
                                    
                                       
                                          
                                          
                                          
                                             
                                                cos
                                                
                                                   (
                                                   
                                                      α
                                                      i
                                                   
                                                   )
                                                
                                                =
                                                
                                                   v
                                                   i
                                                
                                                ·
                                                
                                                   n
                                                   i
                                                
                                             
                                          
                                       
                                       
                                          
                                          
                                          
                                             
                                                cos
                                                
                                                   (
                                                   
                                                      β
                                                      i
                                                   
                                                   )
                                                
                                                =
                                                
                                                   n
                                                   i
                                                
                                                ·
                                                
                                                   
                                                      n
                                                      c
                                                   
                                                   
                                                      |
                                                      
                                                         |
                                                         
                                                            n
                                                            c
                                                         
                                                         |
                                                      
                                                      |
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                          
                                          
                                             
                                                cos
                                                
                                                   (
                                                   
                                                      ϕ
                                                      i
                                                   
                                                   )
                                                
                                                =
                                                
                                                   u
                                                   i
                                                
                                                ·
                                                
                                                   
                                                      
                                                         s
                                                         i
                                                      
                                                      −
                                                      c
                                                   
                                                   
                                                      
                                                         ∥
                                                      
                                                      
                                                         s
                                                         i
                                                      
                                                      
                                                         −
                                                         c
                                                         ∥
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                          
                                          
                                             
                                                
                                                   θ
                                                   i
                                                
                                                =
                                                a
                                                t
                                                a
                                                n
                                                2
                                                
                                                   (
                                                   
                                                      
                                                         w
                                                         i
                                                      
                                                      ·
                                                      
                                                         n
                                                         i
                                                      
                                                      ,
                                                      
                                                         u
                                                         i
                                                      
                                                      ·
                                                      
                                                         n
                                                         i
                                                      
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                           
                              
                                 (5)
                                 
                                    
                                       S
                                       D
                                       C
                                       =
                                       
                                          
                                             
                                                
                                                   (
                                                   
                                                      c
                                                      −
                                                      
                                                         s
                                                         i
                                                      
                                                   
                                                   )
                                                
                                             
                                             2
                                          
                                          
                                             max
                                             
                                                {
                                                
                                                   
                                                      
                                                         (
                                                         
                                                            c
                                                            −
                                                            
                                                               s
                                                               i
                                                            
                                                         
                                                         )
                                                      
                                                   
                                                   2
                                                
                                                }
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

CVFH is a global descriptor that can account the 3D shape information of the objects. However, the CVFH descriptor is unable to discriminate objects with similar sizes and shapes. To improve the discriminative character of the descriptor, a local descriptor, histogram of oriented gradients (HOG) (fHOG
                           ), is applied. Let x and y be the pixel values of the color image (Dalal and Triggs, 2005). Then the magnitude (m) and orientation (θ) of an image's gradient can be computed using Eq. 6 and 7.

                              
                                 (6)
                                 
                                    
                                       m
                                       
                                          (
                                          
                                             x
                                             ,
                                             y
                                          
                                          )
                                       
                                       =
                                       
                                          
                                             d
                                             x
                                             
                                                
                                                   
                                                      (
                                                      
                                                         x
                                                         ,
                                                         y
                                                      
                                                      )
                                                   
                                                
                                                2
                                             
                                             +
                                             d
                                             y
                                             
                                                
                                                   
                                                      (
                                                      
                                                         x
                                                         ,
                                                         y
                                                      
                                                      )
                                                   
                                                
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           
                           
                              
                                 (7)
                                 
                                    
                                       
                                          
                                          
                                          
                                             
                                                θ
                                                
                                                   (
                                                   
                                                      x
                                                      ,
                                                      y
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                       
                                          
                                          
                                          
                                             
                                                
                                                =
                                                
                                                   {
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     tan
                                                                  
                                                                  
                                                                     −
                                                                     1
                                                                  
                                                               
                                                               
                                                                  (
                                                                  
                                                                     
                                                                        
                                                                           d
                                                                           y
                                                                           
                                                                              (
                                                                              
                                                                                 x
                                                                                 ,
                                                                                 y
                                                                              
                                                                              )
                                                                           
                                                                        
                                                                        
                                                                           d
                                                                           x
                                                                           
                                                                              (
                                                                              
                                                                                 x
                                                                                 ,
                                                                                 y
                                                                              
                                                                              )
                                                                           
                                                                        
                                                                     
                                                                  
                                                                  )
                                                               
                                                               −
                                                               π
                                                            
                                                         
                                                         
                                                            
                                                               i
                                                               f
                                                               
                                                               d
                                                               x
                                                               
                                                                  (
                                                                  
                                                                     x
                                                                     ,
                                                                     y
                                                                  
                                                                  )
                                                               
                                                               <
                                                               0
                                                               
                                                               
                                                                  and
                                                                  
                                                               
                                                               d
                                                               y
                                                               
                                                                  (
                                                                  
                                                                     x
                                                                     ,
                                                                     y
                                                                  
                                                                  )
                                                               
                                                               <
                                                               0
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     tan
                                                                  
                                                                  
                                                                     −
                                                                     1
                                                                  
                                                               
                                                               
                                                                  (
                                                                  
                                                                     
                                                                        
                                                                           d
                                                                           y
                                                                           
                                                                              (
                                                                              
                                                                                 x
                                                                                 ,
                                                                                 y
                                                                              
                                                                              )
                                                                           
                                                                        
                                                                        
                                                                           d
                                                                           x
                                                                           
                                                                              (
                                                                              
                                                                                 x
                                                                                 ,
                                                                                 y
                                                                              
                                                                              )
                                                                           
                                                                        
                                                                     
                                                                  
                                                                  )
                                                               
                                                               +
                                                               π
                                                            
                                                         
                                                         
                                                            
                                                               i
                                                               f
                                                               
                                                               d
                                                               x
                                                               
                                                                  (
                                                                  
                                                                     x
                                                                     ,
                                                                     y
                                                                  
                                                                  )
                                                               
                                                               0
                                                               <
                                                               0
                                                               
                                                               and
                                                               
                                                               d
                                                               y
                                                               
                                                                  (
                                                                  
                                                                     x
                                                                     ,
                                                                     y
                                                                  
                                                                  )
                                                               
                                                               >
                                                               0
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     tan
                                                                  
                                                                  
                                                                     −
                                                                     1
                                                                  
                                                               
                                                               
                                                                  (
                                                                  
                                                                     
                                                                        
                                                                           d
                                                                           y
                                                                           
                                                                              (
                                                                              
                                                                                 x
                                                                                 ,
                                                                                 y
                                                                              
                                                                              )
                                                                           
                                                                        
                                                                        
                                                                           d
                                                                           x
                                                                           
                                                                              (
                                                                              
                                                                                 x
                                                                                 ,
                                                                                 y
                                                                              
                                                                              )
                                                                           
                                                                        
                                                                     
                                                                  
                                                                  )
                                                               
                                                            
                                                         
                                                         
                                                            otherwise
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

From the pre-processing stage, each object was segmented and the corresponding indices were obtained (each object has an index assigned for future reference). Using these indices, a binary mask can be obtained for each object. The region of interest (ROI) consisting of the detected object is separated from the color image and resized to 96×48 (Dalal and Triggs, 2005). Fig. 6
                            is an illustration of ROI for four daily objects obtained from color images. Then the RGB color image is separate into three channels with the pixel values representing red, green, and blue, respectively. The goal of this operation is to account the RGB color information of the objects. Then the HOG features were extracted from each image and concatenated together. The HOG descriptors for objects in Fig. 6(a)–(d) are shown in Fig. 7
                           (a)–(d).

A vector (F) is created by concatenating the CVFH and HOG features. For each object, a set of models (M) were trained (using F) with each model representing a partial view of the object. The nonlinear Support Vector Machine (SVM) algorithm presented in (Hsu and Lin, 2002) was then applied to classify the objects (Fig. 8
                           ). The presented gesture recognition pipeline is shown in Algorithm 2
                           .

The JACO robot (Kinova®) is used in our WMRM system. It has been mounted on the left side of the wheelchair to enable interaction and manipulation. Fig. 9
                         is an illustration of the configuration of the WMRM system.

The robotic control module was implemented using the resident JACO API for C++. The control of the JACO robotic manipulator consists of three modes; 3-D Cartesian space translation of the end effector, wrist rotation, and three fingers' grasping mode. The WMRM was controlled through upper limb gestures and speech. Object recognition and body parts tracking algorithms were used once the input commands (e.g. hand gestures or voice) were evoked to control the robotic arm to perform different tasks. The object recognition algorithm was used for objects’ localization and grasping. Once the object was picked, it was moved towards the user, based on body parts detection. The architecture of the control sequence of actions is shown in Fig. 10.
                        
                     

@&#EXPERIMENTS AND RESULTS@&#

The object recognition performance of the proposed algorithm was evaluated in this experiment. A database with 12 daily living objects is built for WMRM manipulation. The point clouds of these objects are shown in Fig. 11
                        . An 8-fold cross validation is conducted resulting in a mean recognition accuracy of 98.43% for the CVFH descriptor and 98.96% for the CVFH+HOG descriptor, respectively. The confusion matrix when using the CVFH and CVFH+HOG descriptors is shown in Fig. 12
                        (a) and (b), respectively. It can be observed from Fig. 12 that Cup1 had consistently a relatively lower recognition accuracy in comparison to the other objects. For example, Cup1 was often confused with the coffee creamer (Fig. 11(d)) due to their appearance similarity. Cup1 is shown in Fig. 11(j), which is a transparent plastic cup. We noticed that cameras using infrared reflection for depth measurement perform poorly on transparent objects.

It was found that the current retrieval strategy improved object recognition when the objects were placed in different orientations and under different light conditions, compared with our previous work (Jiang et al., 2014). Specifically: (1) HOG+SVM vs. Speeded Up Robust Features (SURF) method (the later used in (Jiang et al., 2014)). Currently we used HOG, which has shown to boost object detection when combined with SVM (Dalal and Triggs, 2005); (2) Previously only 2D information was applied to characterize the objects and only a single view of the objects was saved as template. The object recognition is not robust when the detected views of the objects are drastically different from the template view. This problem has been solved in this paper by taking multiple views of the objects and building models for each single view. By integrating 3D spatial information (point cloud) with 2D color information, a more versatile and effective object recognition was achieved. The advantage of using 3D point cloud is that it can describe the characteristics of the objects without being affected by changes in illumination, object orientation, and field of view. 3D information was used for describing the objects’ shape, while 2D information was applied to discriminate between objects with similar size and shape, such as two cups with the same shape but different color. Thus, the integration of 2D color information with 3D point cloud allowed a rich object representation with low computing overhead.

The previously studied object recognition algorithm was integrated with the multimodal user interface, user body localization, and robotic arm control to assist UEMIs performing daily tasks. Two tasks were designed to validate the presented system including feeding and dressing. These two tasks are more complicated than the tasks in our previous study (Jiang et al., 2014), since they required more operations per task and more interaction with the user. For example, the dressing task requires the WMRM to accurately put on and take off the hat from the user's head. While the tasks designed in previous study (i.e. picking up and placing a mobile phone to the user's ear; positioning a mobile phone with a camera to take a picture of the user's face (“selfie”); putting a long straw in a cup and bringing it to the user's mouth) did not require as much accurate positioning. Three experiments were conducted for the dressing and feeding tasks. Each experiment adopted a different input control method (i.e. gesture-based, speech-based, or hybrid control). The assignment between different control modalities and task commands are summarized in Table 2.
                        
                     

The feeding task consists of five operations: (1) grabbing a cereal box; (2) pouring the cereal (simulated solid objects) into a bowl; (3) grabbing the bowl; (4) bringing the bowl to participants’ hand; and (5) placing the bowl back on the table. An illustration of the operational procedures is shown in Fig. 13.
                           
                        

Three subjects (two female and one male) aged 20–30 were recruited to test the validity of the system. Task completion time, the error rate, pouring accuracy, and the number of commands used to complete each operation were recorded. The total task completion time was computed by summing the times required for each operation. The error rate represents the ratio between the number of failed operations and the number of total operations, while the pouring accuracy is used to estimate the percentage of cereal that was poured into the bowl. For each operation, only one command is required to finish the task. However, due to possible failure of speech or hand gesture recognition, and mistakes done by the users, it may require more than one command to finish the task. Fig. 14
                            shows that the results of the average number of commands used over three trials of hybrid modality was significantly less than voice (P = 0.006 < 0.05) and gesture modalities (P = 0.003 < 0. 05). The average task completion times for gesture, voice and hybrid modalities are 117.11, 137.56 and 117.11, respectively. Although, there are no statistically significant difference between these three interfaces in completion time, voice control interface presented its shortcoming in fast responses. The average accuracy for gesture, voice, and hybrid interfaces is 97.78%, 95.56% and 97.78%. And the pouring accuracy for these three interfaces is 92.22%, 93.75% and 93.95%.

The dressing task assisted subjects putting on a baseball cap. This task enables operators to pick up a cap and put it on their head, or alternatively, take off the cap and place it back on the table.

Five subjects (three females and two males) aged 20–30 were recruited to test the validity of the system. Similar as in Task 1, task completion time, error rate, and the number of commands were recorded for each step. Fig. 15
                            that the average number of commands used over three trials for the hybrid modality was significantly lower than voice (P = 8e-5 < 0.05) and gesture modalities (P = 8.4e-5 < 0.05). The average task completion times for gesture, voice, and hybrid modalities are 118.8, 113.73, and 103.67, respectively. A two-way ANOVA shows that there was no statistically significant difference between these task completion times. The average accuracy for gesture, voice, and hybrid is 11.1%, 8.9%, and 0%, respectively, where the hybrid modality achieved the lowest error rate.

To compare with our previous study, which only utilized either gesture-based or speech-based interface, the hybrid interface developed in this paper was tested and compared with the two interfaces employed in previous work, in both feeding and dressing tasks. Observed from Figs. 14 and 15, the hybrid control interface required less number of commands for both tasks, and required relatively less time to complete the tasks, which indicated higher efficiency and effectiveness.

When using the hybrid interface, participants were able to command the robot using their preferred control modality when performing both tasks. To investigate gesture or speech-based command preference, the number of commands using gestures was compared with the number of commands using speech during the hybrid interface experiments, shown in Fig. 16
                           . Comparisons were made when performing each operation of the tasks, since control modality preferences varied during different users and operations. For instance, participants preferred gestures than speech for grabbing the cap, while they preferred speech for taking off the cap (Fig. 16). When participants were asked why they chose to use a certain modality when performing tasks, they indicated several criteria including which modality they thought at that time would result in the least errors. The ratio of accuracy between gesture and speech modalities was also plotted in Fig. 16 to indicate user preferences. When the ratio of accuracy was most extreme (ratio around and larger than 2.0), participants showed strong user preferences using certain modalities. While during the operations with small accuracy differences (ratio less than 1.5), user preference between control modalities were not pronounced. The only exception was when users were performing the operation to receive the cereal bowl from the robotic arm. Users overwhelmingly used speech commands during this operation, despite having high error rates, because their hands were busy receiving the bowl and could not perform gestures.

Post-experiment survey indicated several criteria that participants applied to select preferred interaction modality, including intuitiveness, easiness to perform, and recognition accuracy. Both gesture and speech commands were selected due to their ability to be performed by quadriplegics. The able-bodied participants in the study used both modalities and there was not a significant difference in performance accuracy during both tasks. However, it is evident participants preferred using some control modalities during operations over the other alternatives. There could be many reasons for this based on the movements required to perform an operation more correctly or efficiently. For example, speech commands may be more intuitive to use while gesture commands require memorizing the mapping between commands and gestures. The environment may also make a difference. Speech recognition accuracy is severely decreased in a noisy environment. When considering easiness to perform, it is dependent on different commands at different times. In these experiments, gestures “S” and “Z” were relatively difficult to perform correctly, while speech commands “Pour” and “Head” were hard for non-native speakers to pronounce correctly. Physical fatigue is also much higher for quadriplegics. The hybrid interface provides a flexible way for people to interact with the system by avoiding excessive memory load, overriding difficulties or fatigue when performing over time, and is more stable during different use scenarios and environments.

@&#CONCLUSIONS@&#

In this paper, a 3D vision-based system was implemented to more efficiently control a WMRM. The system consists of four modules that synergistically allow UEMIs to perform daily tasks (i.e. dressing and feeding) more effectively. Three input (gesture, speech, and hybrid) modalities were tested. A gesture recognition algorithm was presented using both 3D spatial and 2D color information. The algorithm could detect and recognize twelve different daily objects automatically. The users’ face and hands were tracked to automatically position those recognized objects to the respective body parts.

An experiment was conducted to test the performance of the presented object recognition algorithm. A dataset with twelve daily objects (i.e. cups, coke, and cap) were created. An eight-fold cross validation led to an average accuracy of 98.43% for the pipeline with CVFH descriptor and 98.96% with the CVFH+HOG descriptor. Two daily tasks (feeding and dressing) were conducted with eight subjects. Three interaction methods were tested, using gesture-based, speech-based, and hybrid interfaces. The results revealed that the hybrid interface outperformed using single control modalities.

Future work will consist of recruiting subjects with UEMIs to evaluate the feasibility and usability of the presented system in real-word scenarios.

@&#ACKNOWLEDGMENTS@&#

We thank the Center for Paralysis Research and the State of Indiana for supporting this project. This research was made possible through the Purdue University Discovery Park and Regenstrief Center for Healthcare Engineering.

@&#REFERENCES@&#

