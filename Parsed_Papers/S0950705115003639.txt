@&#MAIN-TITLE@&#Sensor-based human activity recognition system with a multilayered model using time series shapelets

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We exploit time series shapelets for complex human activity recognition.


                        
                        
                           
                           We present a multilayered activity model to represent four types of activities.


                        
                        
                           
                           We implement a prototype system based on smartphone for human activity recognition.


                        
                        
                           
                           Daily living and basketball play activity recognition are conducted for evaluation.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Human activity recognition

Complex activity

Multilayer

Time series

Sensor

@&#ABSTRACT@&#


               
               
                  Human activity recognition can be exploited to benefit ubiquitous applications using sensors. Current research on sensor-based activity recognition is mainly using data-driven or knowledge-driven approaches. In terms of complex activity recognition, most data-driven approaches suffer from portability, extensibility and interpretability problems, whilst knowledge-driven approaches are often weak in handling intricate temporal data. To address these issues, we exploit time series shapelets for complex human activity recognition. In this paper, we first describe the association between activity and time series transformed from sensor data. Then, we present a recursively defined multilayered activity model to represent four types of activities and employ a shapelet-based framework to recognize various activities represented in the model. A prototype system was implemented to evaluate our approach on two public datasets. We also conducted two real-world case studies for system evaluation: daily living activity recognition and basketball play activity recognition. The experimental results show that our approach is capable of handling complex activity effectively. The results are interpretable and accurate, and our approach is fast and energy-efficient in real-time.
               
            

@&#INTRODUCTION@&#

The increased accessibility of wearable and environmental sensors has raised interest in the development of human activity recognition techniques in ubiquitous computing [1]. The maturity of the miniaturized sensors and supporting technologies has pushed the research focus on context-aware triggered activity recognition and inference for a number of real-world applications, such as home monitoring and assisted living [2–4], smart hospitals [5,6], rehabilitation [7,8], physical and sport activities [9–11], terrorist detection [12], and so forth. Particularly, the prevalence of mobile devices, such as the smartphone, equipped with powerful sensors and high-speed processors, can offer advanced capabilities to recognize human activity for developing smartphone-based healthcare and wellbeing applications [13,14]. Consequently, a substantial number of projects and initiatives have been undertaken [15].

One goal of activity recognition is to uncover the knowledge of a user’s behaviour that allows computing systems to proactively assist users with their tasks [16]. Computer vision-based activity recognition has been at the forefront in this research field, where a large number of researchers investigated machine recognition of gestures and activities from still images and video in well-controlled environments or constrained settings [17]. Advances in mobile devices motivated steps towards more challenging and real-world applications in unconstrained daily life settings or dedicated scenarios, where cameras cannot be deployed everywhere nor can be employed anytime.

Successful research so far has focused mainly on simplified scenarios involving single-user simple-activity recognition using sensors. The nature of human activities is complex and pose challenges that the majority of approaches and algorithms designed for simplified scenarios cannot handle in more complex scenarios [18].

Recent works have tried to address the modeling and recognition of complex activities. They generally can be categorized into two kinds of approaches, knowledge-driven and data-driven [15]. The data-driven approaches first collect sensed data, and then exploit the unseen correlations between activities and sensor data, and eventually establish a model to classify the activities. Some commonly used data-driven approaches are Hidden Markov Models [19–25], Conditional Random Fields [26–28], Bayesian Networks [29–33] and pattern mining techniques [34–37]. Despite the fact that most of these probabilistic and stochastic methods can handle sequential and concurrent activities with high classification accuracy, they suffer from inflexibility in recognizing different levels of activities [38,39]. They often need to construct a separate network structure for each class of activities at different levels. Unfortunately, for some applications like daily living behavior monitoring, relevant activities even cannot be clearly defined upfront [17]. On the contrary, knowledge-driven approaches start with an abstract model of common knowledge and then implement and evaluate the model through sensed data. One commonly used technique for building such a model is ontology modeling. The ontology-based approaches are semantically clear, logically elegant, and easy to interpret [15]. However, they often lack the expressive power to capture and propagate the temporal dependencies [33,40]. In addition, it would be rather difficult to handcraft each formula whose temporal relations among activities are intricate [41].

To address the aforementioned issues, we believe that time series shapelets (or shapelets for short) is a promising technique basis for recognizing complex human activities. The shapelet has been shown efficiently in classification with extensive experiments [42]. It can inherently handle the temporal information for designing real-time recognition systems, and provide accurate and interpretable results. Most importantly, it is possible to combine shapelets with semantic-based techniques in order to recognize activities in different levels and granularity [43].

This paper extends the concept and usage of shapelets to the area of sensor-based complex human activity recognition. We present four contributions: a description of the transformation from sensor data to time series, a complex activity model, a shapelet-based recognition framework, and extensive experiments with public datasets and our dataset on daily living activities and sport activities. For the transformation approach, we represent a user’s activity as a set of time series, each one from a particular measured sensor attribute. A shapelet is a representative of a class of time series [44]; that is to say, an activity can be represented by a shapelet. For instance, to represent activities in a game of basketball, “walking” can be represented as a set of time series collected from accelerometer sensors on a user’s leg. For the activity representation and modeling approach, we present a recursive model of complex activities that are sequentially or concurrently composed of atomic activities. For instance, the activity “dribbling” is concurrently composed of overlapping atomic activities, like “running” and “bouncing ball”; the activity “slam dunk” is composed of a sequence of atomic activities, like “jumping”, “throwing ball” and “hanging on hoop”; the higher level activity “offensive” is sequentially composed of some sequences of concurrent activities “dribbling” and one sequential activity “slam dunk”. For shapelet-based recognition, we employ shapelets that represent atomic activities to recognize different types of activities whose time series are defined as a combination of shapelets through sequencing and overlapping.

Compared to other activity recognition approaches, time series and shapelets have a natural temporal ordering that make the recognition approach distinct from other data-driven or knowledge-driven approaches. A major limitation of ontology-based models concerns activities with temporal intra-relationships [39]. Although the time-domain or frequency-domain features are often extracted from sensor data in data-driven or knowledge-driven approaches, they are treated as independent points without taking into account these intra-relationship among data points over time [26]. In addition, the overlapping activities are hard to distinguish using such features [18]. In terms of temporal information, time series shapelet-based recognition can handle the intra-relationship. For instance,“set-shot” and “dribbling” are two concurrent activities, but the way that atomic activities concurrently compose them are different over time. For “set-shot”, “throwing ball” is performed during the “jump” and has to occur after “jumping” starts, whereas “running” and “bouncing” are performed simultaneously from the beginning for “dribbling”. Time series can record the occurrence time of each atomic activity, thereby recognizing the sequencing and overlapping activities.

To evaluate the time series shapelet-based approach for complex human activity recognition, we have implemented a prototype system comprising a server for training shapelets and a smartphone app for recognizing activities in real-time. We first evaluate our approach on two open datasets for atomic activity recognition, and then conduct two case studies for complex activity recognition, namely daily living activity recognition lasting for eight days, and basketball play activity recognition with 17 basketball play relevant activities of different levels. The experimental results are promising and show the capability of handling complex activity recognition effectively. We discuss the limitations and several directions to improve our approach in the conclusion.

@&#RELATED WORK@&#

Activity Recognition has been attracting growing attention in a number of application domains due to the prevalence of mobile devices, such as smartphones and tablets, which are often equipped with various sensors and powerful processors. As such, there are many works related to activity recognition. Aggarwal and Ryoo provided an extensive review on the vision-based methods in [45]. Chen et al. reviewed sensor-based activity recognition in [15], and wearable sensor-based activity recognition was further discussed by Lara and Labrador in [1]. In this section, we will highlight the works relevant to sensor-based activity representation and models and complex activity recognition.

In terms of recognition approaches, the activity models can be categorized as appearance-based representations and structure representations.

For computer vision-based methods, activities are determined using characteristic patterns of image appearance. Chowdhury and Chellappa et al. [46,47] incorporates the Kendall statistical shape theory to model the interactions of a group of people and the activities of individuals. In [48], activity is described as a sequence of flow vectors. 2D shapes are used to model activities in [49,50]. In [51], each human activity is represented by the 3D shapes. However, sensor-based activity recognition faces a number of unique characteristics.

In computer vision recognition, the problem definitions are often clear, such as “detect an object in image”, and the recognition systems are also well-defined and fixed, e.g. a defined number and type of cameras. In contrast, the sensor-based recognition system suffers heterogeneous data sources – sensors that differ in their capabilities and characteristics [52]. In addition, for some sensor-based applications, there is no common definition of human activities, such as in long-term daily living activity monitoring, where relevant activities are highly diverse and even cannot be clearly defined upfront [17]. A multilayered structure is one way to describe such unclear, complex activities. As mentioned by Blanke and Schiele [38], the hierarchical structures are necessary for the system’s recognition performance.

For knowledge-driven methods, activities are often modeled by structure representations. In [39,53], a multilevel structure was proposed to model activities that are represented in four different levels: atomic gestures that are in the lowest level and cannot be decomposed; manipulative gestures that are the execution of atomic gestures; atomic activities that a sequence of manipulative gestures; and the highest level, consisting of complex activities that are concurrent executions of atomic activities. The framework described the relationships between simple and complex activities. For the majority of data-driven methods, activities are modeled by logic or finite state representations. In [19], a finite state machine was used to represent complex activities where hidden states represent activities and state transitions represent the relationship between the activities.

There are three commonly used approaches to accomplish complex activity recognition from sensor data: data-driven, knowledge-driven and hybrid.

For data-driven methods, the Hidden Markov Model (HMM [7,19,21,22]), Conditional Random Field (CRF [2,26,27]), Skip-Chain Conditional Random Fields (SCCRF [28]), Emerging Patterns (EP [34]) and Dynamic Bayesian Network [32] utilize temporal information to implement complex activity recognition. In [54], Logan used two types of static classifiers – naive Bayes and C4.5 decision trees – to recognize complex behaviors using over 900 sensors. In HMM models, sensor-tagged objects are used as observations whilst activities are defined as hidden states. However, HMMs are more suitable for purely sequential activities [18]. Modayil et al. [24] presented an interleaved hidden Markov models (IHMM) to capture both inter-activity and intra-activity dynamics. CRFs have shown similar limitations in recognizing concurrent activities [55]. To solve such issue, a variant SCCRF uses correlation graphs to model concurrent activities, but SCCRF is computationally expensive in inference [56]. An interesting work presented by Kim et al. [18] compared four probability chain based methods, HMM, CRF, SCCRF and EP, and found that SCCRF and EP were able to recognize concurrent activities. Zhang et al. [33] proposed the interval temporal Bayesian network (ITBN) that combines the temporal relations of Allen’s interval with the probabilistic description of Bayesian network. However, since the BN model is a directed acyclic graph, some temporal relations from training dataset have to be removed in order to make the network temporally consistent. Moreover, their model addresses to learn a separate network structure for each class of activities. As such, it is difficult to build a unified network structure to represent the hierarchical structure of complex activity. In general, these graphical-based methods suffer from inflexibility in expressing hierarchical structures of activities and disadvantage of learning a large number of parameters in a complex graph [57], especially in real-time or mobile applications with limited resource or time constraints [58].

Several knowledge-driven works were presented that recognize complex activities with a different number, category, granularity, or application. In [4], Chen et al. introduced a knowledge-driven approach for inferencing both coarse-grained and fine-grained activities of daily living. In [59], Springer and Turhan proposed a method using Web Ontology Language Description Logics (OWL-DL) to recognize the current situation based on an ambient context. In [9], Ermes et al. used a hybrid classifier combining a tree structure containing a priori knowledge and artificial neural networks to detect daily activities and sports. In [60], a hybrid approach was presented that captures the temporal information to recognize physical activities. In [61], Riboni and Bettini proposed a hybrid approach of ontological reasoning and statistical inferencing for context-aware activity recognition. In [39], Helaoui et al. use the log-linear model to implement probabilistic ontological reasoning for high-level activity and combined machine learning approaches to classify low-level activity from sensor data, showing that the hybrid approach is able to recognize nearly 150 morning routine activities in different levels. Saguna et al. [62] proposed a context driven activity theory to discover complex activity signatures and generate complex activity definitions. However, a major limitation of these semantic-based models concerns activities with relevant and rich temporal relationships. They often lack the expressive power to capture and propagate temporal dependencies and need prior knowledge to handcraft axioms and their weights [33]. Our model consider both temporal information and the hierarchical structure for the complex activity recognition system.

Time series are a commonly used representation for temporal data. A time series 
                        
                           T
                           =
                           <
                           
                              t
                              1
                           
                           ,
                           
                              t
                              2
                           
                           ,
                           …
                           ,
                           
                              t
                              M
                           
                           >
                        
                      is a sequence of M data points measured in time and spaced at uniform time intervals. A particular measured sensor attribute can be represented as a time series. For example, the sequence of data collected from the x-axis of an accelerometer at a sampling rate of 20 Hz can be formalized as a time series of length one second containing 20 data points at a time interval of 50 ms. In the cases of multi-source sensed data, ti
                      is a vector of data points collected from multiple sensor attributes. We denote it as 
                        
                           
                              t
                              i
                           
                           =
                           
                              [
                              
                                 t
                                 i
                              
                              
                                 (
                                 1
                                 )
                              
                              ,
                              
                                 t
                                 i
                              
                              
                                 (
                                 2
                                 )
                              
                              ,
                              …
                              ,
                              
                                 t
                                 i
                              
                              
                                 (
                                 d
                                 )
                              
                              ]
                           
                           ,
                        
                      where ti
                     (j) is the sensed data from the j-th sensor attribute (1 ≤ j ≤ d), and d denotes the number of sensor attributes. The subsequence of length l of a time series 
                        T
                      is a contiguous sequence of l points starting from p in T, where l ≤ m and 
                        
                           1
                           ≤
                           p
                           ≤
                           m
                           −
                           l
                           +
                           1
                        
                     .

An atomic activity is a unit-level activity that cannot be broken down further under application semantics [62]. We introduce the concept of shapelet for atomic activity recognition. Shapelet was first defined in 2009 by Ye and Keogh [42] for classifying different classes of time series.

Given a training dataset 
                           D
                         consists of N time series 
                           
                              
                                 T
                                 1
                              
                              ,
                              
                                 T
                                 2
                              
                              ,
                              …
                              ,
                              
                                 T
                                 N
                              
                              ,
                           
                         each one labeled by one of K classes of atomic activities 
                           
                              
                                 c
                                 1
                              
                              ,
                              
                                 c
                                 2
                              
                              ,
                              …
                              ,
                              
                                 c
                                 K
                              
                              ,
                           
                         a shapelet is a subsequence that can maximally represent a class of time series. Let 
                           
                              D
                              k
                           
                         be the training dataset of class ck
                        , and Ωk
                         be the set of all possible subsequences of time series in 
                           
                              D
                              k
                           
                        . In order to find the shapelet that represents class ck
                         (1 ≤ k ≤ K), we evaluate each subsequence S in Ωk
                         using an evaluation function f. The subsequence with the best evaluation value is selected as the shapelet for class ck
                        .

We formally define shapelet for activity recognition as follows:

                           Definition 1
                           Shapelet


                           Given a time series training dataset 
                                 
                                    D
                                    ,
                                 
                               a shapelet 
                                 
                                    S
                                    k
                                 
                               of class ck
                               is a subsequence such that

                                 
                                    (1)
                                    
                                       
                                          
                                             
                                             
                                                
                                                   
                                                      S
                                                      k
                                                   
                                                   =
                                                   
                                                      
                                                         arg
                                                         max
                                                      
                                                      
                                                         S
                                                         ∈
                                                         
                                                            Ω
                                                            k
                                                         
                                                      
                                                   
                                                   f
                                                   
                                                      (
                                                      S
                                                      ,
                                                      
                                                         Ω
                                                         k
                                                      
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           

The evaluation function can be defined according to application requirements. Often, an evaluation function evaluates the similarity between shapelet and all subsequences in the training dataset. In our activity recognition system, we first extract several features from time series that are related to activities and then use the Euclidean distance to measure the similarity over these features. Different evaluation functions can be defined for different scenarios. For example, alternative measurements such as the cosine distance or the Chebyshev distance can also be used instead. More technical information about the definition of the evaluation function f can be found in Section 5.2 describing the implementation of our human activity recognition system for daily living and basketball play scenarios.

We employ shapelet to label a new time series 
                           
                              T
                              ′
                           
                         using the evaluation function. The time series is labeled as the class whose shapelet obtains the best evaluation value, as follows:

                           
                              (2)
                              
                                 
                                    
                                       
                                       
                                          
                                             
                                                c
                                                k
                                             
                                             =
                                             
                                                
                                                   arg
                                                   max
                                                
                                                
                                                   1
                                                   ≤
                                                   k
                                                   ≤
                                                   K
                                                
                                             
                                             f
                                             
                                                (
                                                
                                                   T
                                                   ′
                                                
                                                ,
                                                
                                                   S
                                                   k
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

In the following we illustrate an example of the shapelet training applied to activity recognition.

                           Example I
                           Suppose that we want to use an accelerometer to track a person’s daily routine by discriminating their walking, jogging, and standing activities. The classes are set to 
                                 
                                    
                                       c
                                       1
                                    
                                    =
                                 
                              walking, 
                                 
                                    
                                       c
                                       2
                                    
                                    =
                                 
                              running, and 
                                 
                                    
                                       c
                                       3
                                    
                                    =
                                 
                              standing. There are three sensor attributes from the accelerometer, x-axis, y-axis and z-axis. The sampling rate is 20 Hz.

We first collect a training dataset 
                           D
                         of 90 samples of time series of length two seconds, 
                           
                              
                                 T
                                 1
                              
                              ,
                              
                                 T
                                 2
                              
                              ,
                              …
                              ,
                              
                                 T
                                 90
                              
                           
                        . The sampling rate is 20 Hz, so each sample has 40 data points, i.e. 
                           
                              
                                 T
                                 n
                              
                              =
                              <
                              
                                 t
                                 
                                    n
                                    ,
                                    1
                                 
                              
                              ,
                              
                                 t
                                 
                                    n
                                    ,
                                    2
                                 
                              
                              ,
                              …
                              ,
                              
                                 t
                                 
                                    n
                                    ,
                                    40
                                 
                              
                              >
                              ,
                           
                         where 1 ≤ n ≤ 90. For each data point 
                           
                              
                                 t
                                 
                                    n
                                    ,
                                    i
                                 
                              
                              =
                              
                                 [
                                 
                                    t
                                    
                                       n
                                       ,
                                       i
                                    
                                 
                                 
                                    (
                                    x
                                    )
                                 
                                 ,
                                 
                                    t
                                    
                                       n
                                       ,
                                       i
                                    
                                 
                                 
                                    (
                                    y
                                    )
                                 
                                 ,
                                 
                                    t
                                    
                                       n
                                       ,
                                       i
                                    
                                 
                                 
                                    (
                                    z
                                    )
                                 
                                 ]
                              
                              ,
                           
                         1 ≤ i ≤ 40. t
                        
                           n, i
                        (x), t
                        
                           n, i
                        (y) and t
                        
                           n, i
                        (z) are sensed data from x-axis, y-axis and z-axis at time point i, respectively. Then we have the set 
                           
                              
                                 Ω
                                 k
                              
                              =
                              
                                 {
                                 <
                                 
                                    t
                                    
                                       n
                                       ,
                                       i
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    t
                                    
                                       n
                                       ,
                                       
                                          i
                                          ′
                                       
                                    
                                 
                                 >
                                 :
                                 1
                                 ≤
                                 i
                                 <
                                 
                                    i
                                    ′
                                 
                                 ≤
                                 40
                                 
                                 and
                                 
                                 
                                    T
                                    n
                                 
                                 ∈
                                 
                                    D
                                    k
                                 
                                 }
                              
                           
                         for each activity ck
                        . The evaluation function f can exploit Euclidean or other distance measurements to evaluate each subsequence in Ω(ck
                        ). The subsequence with the best evaluation value is chosen as the shapelet Sk
                         representing activity ck
                        .

When an unlabeled time series 
                           
                              T
                              ′
                           
                         arrives, the similarities are evaluated between 
                           
                              T
                              ′
                           
                         and each shapelet 
                           
                              S
                              k
                           
                        . Then 
                           
                              T
                              ′
                           
                         is labeled as the atomic activity ck
                         if its shapelet has the best similarity to 
                           
                              T
                              ′
                           
                        .

A complex activity is a collection of temporally and coherently related atomic activities [33]. It can be broken down into several atomic activities under application semantics [62]. Hence, hierarchies become necessary for complex activity recognition using shapelets. We illustrate an example of data acquisition on basketball play for complex activity recognition.

                           Example II
                           Suppose that we want to detect a basketball player’s activities in the court, including set-shot, jump-shot, lay-up, dribbling, blocking and positioning. These activities are more complex than walking or running.

Here, according to the common techniques and practices of basketball theory, these activities are composed of several atomic activities, like standing, jumping, running, walking, throwing the ball, bouncing the ball, raising hands and so forth. For instance, a player performing the set-shot first stands in a fixed position with neither foot leaving the floor, and then throws the ball; a player performing jump-shot jumps in mid-air, and releases the ball near the top of the jump; a player performing lay-up has to run toward the basket, and then jump and throw the ball into the basket.

An accelerometer and a gyroscope are placed on a player’s right arm as well as left leg. The shapelets for atomic activities, including standing, jumping, running, walking, throwing the ball, bouncing the ball and raising hands, can be calculated by way of the procedure similar to Example I, as shown in Fig. 1. More complex playing activities can be recognized on the basis of these atomic activity shapelets:

                           
                              •
                              
                                 set-shot: standing is recognized through the shapelet sensed from the left leg; throwing the ball, which is occurring during the standing, is recognized through the shapelet sensed from the right arm.


                                 jump-shot: jumping is recognized through the shapelet from the left leg; throwing the ball, which is occurring during the jumping, is recognized through the shapelet from the right arm.


                                 lay-up: running is recognized through the shapelet from the left leg; jumping is recognized through the shapelet from the left leg at the end of the running; throwing the ball, which is occurring during the jumping, is recognized through the shapelet from the right arm.


                                 slam dunk: jumping is recognized through the shapelet from the left leg; throwing the ball, which is occurring at the end of jumping, is recognized through the shapelet from the right arm; hanging on the ring hook after throwing, is recognized through the shapelets from the left leg and the right arm.


                                 dribbling: running or walking is recognized through the shapelet from the left leg whilst bouncing the ball is recognized through the shapelet from the right arm.


                                 positioning: standing is recognized through the shapelet from the left leg; raising arms is recognized at the same time through the shapelet from the right arm.

As we illustrate in the following sections, this shapelet-based mechanism is the core of our activity recognition method. Shapelet contains the local features over time, whereas time-domain or frequency-domain features are the global or mean representations of the entire time series. The benefits of shapelets for classification are (1) interpretability: shapelets are able to provide interpretable outcomes that help users better understand the knowledge underlying their data [63]; (2) accuracy: experimental results demonstrate that shapelets are more accurate than most other well known classifiers, because their local features are more robust to noise and distortion [64]; and (3) high-speed: shapelets are faster at classification than existing approaches [42]. In addition, temporal information concerning the time point or period when certain activity occurs is important for these complex activities.

In this section, we describe our shapelet-based classification technique for complex activity recognition.

In order to model activity composition, we investigated the activity taxonomies [18,53], and categorize human activity into four types in terms of temporal relationship: atomic activity, sequential activity, concurrent activity and complex activity. We name our model SC
                        2:

                           
                              •
                              An atomic activity is a unit-level action or activity that cannot be decomposed further. The decision whether an activity is an atomic activity or not is dependent on the application. The same activity might be assigned to different types in different applications. For instance, in basketball play, playing techniques are intended to be recognized, and therefore “walking” can be defined as an atomic activity. On the other hand,“walking” is regarded as a complex activity in gait recognition scenario where “leg lifting”, which is important in dementia rehabilitation monitoring application, is an atomic activity because “leg lifting” is a part of “walking”.

A sequential activity is a temporal sequence of atomic activities occurring in a certain order. Typically, any two atomic activities cannot be overlapped; that is to say, an activity would not occur until its anterior activity is completed. For instance, “slam dunk” is a sequential activity in which jumping, throwing, and hanging occur in a strict order. Hanging cannot be performed before throwing is done.

A concurrent activity is a set of atomic activities simultaneously occurring during a certain period. For instance, “set-shot”, “jump-shot”, “dribbling”, “blocking” are concurrent activities in which at least two atomic activities occur simultaneously. Although one activity may occur after another activity has been occurring for a while, the two activities are overlapped for a period of time. “Set-shot” and “jump-shot” are such concurrent activities. In other concurrent activities, atomic activities are occurring at the same time from the beginning, such as “dribbling”, where “running” and “bouncing” are performed simultaneously.

A complex activity is a combination of atomic activities, sequential activities, concurrent activities or other complex activities by means of sequencing or overlapping. The complex activity is a recursive definition that can represent many intricate situations. For instance, “lay-up” is a sequencing combination of the atomic activity, “running”, and a concurrent activity which comprises “jumping” and “throwing” (Fig. 2).

We believe that the SC
                        2 model is flexible for activity granularity and scalable for activity quantity. Activities can be modeled at either a coarse-grained or fine-grained level. Coarse-grained activity models are of high level and often used in application scenarios that are designed to recognize users’ behavior and routines like daily activities, or even users’ intent and planning like “team defense”. Fine-grained activity models are more specific in recognizing users’ low-level gestures or motions, and are designed for scenarios like gait tracking and training. Moreover, since every activity is either an atomic activity or a combination of atomic activities, the SC
                        2 model is scalable for new activity recognition. A new activity can be represented by several existing atomic activities that are sequentially, concurrently or complexly combined. For instance, a new basketball play action “blocking” can be modeled by a concurrent combination of the two existing atomic activities, “jumping” (or “standing”) and “raising hands”.

We use shapelets to recognize atomic activities that are represented on the basis of the SC
                        2 model. The rationale of the SC
                        2 model is that the temporal relationship between any two activities is either sequencing or overlapping. The essence of time series allows us to handle the sequential or concurrent activity recognition much more easily because time series inherently contain temporal information. An atomic activity can be represented as a shapelet. Sequential activity can be represented as an ordered set of shapelets, and concurrent activity can be represented as some suitable combinations of overlapped shapelets. In this way, only the shapelets associated with the atomic activities need to be trained rather than training shapelets for all activities.

Shapelets are used to recognize atomic activities and record their occurrence time. Sequential, concurrent and complex activity are inferred by the recognized atomic activities using predefined rules. The rules are defined based on common knowledge to describe the ways that how atomic activities form a sequential, concurrent, or complex activity over time. Many methods can implement the rules, such as temporal logic of actions (TLA) [65], linear temporal logic (LTL) [66], and etc. For simplicity, we use tree-like rules to describe the formations of sequential, concurrent and complex activity in this paper.

We define the following four methods to recognize atomic activity, sequential activity, concurrent activity and complex activity, respectively. We assume the shapelet for each atomic activity has already been determined. We describe the method for finding shapelets later in the Section 5.3.

Atomic activity is recognized with the following steps.

                              
                                 1.
                                 Collect sensor attribute data using a time window [τo, τp
                                    ]. These sensor data are acquired from different sensor types (accelerometer, gyroscope), sensor placement (arm, leg, torso), and users. They have the same sampling rate and synchronized time points.

Represent these sensor data in time series. To be specific, if there are d sensor attributes, the time series is 
                                       
                                          T
                                          =
                                          <
                                          
                                             t
                                             
                                                τ
                                                o
                                             
                                          
                                          ,
                                          …
                                          ,
                                          
                                             t
                                             
                                                τ
                                                p
                                             
                                          
                                          >
                                          ,
                                       
                                     where 
                                       
                                          
                                             t
                                             i
                                          
                                          =
                                          
                                             [
                                             
                                                t
                                                i
                                             
                                             
                                                (
                                                1
                                                )
                                             
                                             ,
                                             …
                                             ,
                                             
                                                t
                                                i
                                             
                                             
                                                (
                                                d
                                                )
                                             
                                             ]
                                          
                                          ,
                                       
                                     
                                    τo
                                     ≤ i ≤ τp
                                    , and ti
                                    (j) is the sensed data from the j-th sensor attribute.

Evaluate the similarity between 
                                       T
                                     and each shapelet using evaluation function f.

Label 
                                       T
                                     as the atomic activity c whose shapelet has the best evaluation value to 
                                       T
                                    .

In practice, the window size for atomic activity recognition is set to a small value that often equals the length of shapelets.

Sequential activity is often performed during a longer time window than the atomic activity. Suppose the unlabeled time series 
                              T
                            are collected during the period [τ
                           1, τ
                           2], 
                              
                                 T
                                 =
                                 <
                                 
                                    t
                                    
                                       τ
                                       1
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    t
                                    
                                       τ
                                       2
                                    
                                 
                                 >
                              
                           . Four steps are required to recognize sequential activity, as described below.

                              
                                 1.
                                 Define the rule in which atomic activity combinations form each sequential activity over time.

Set a time window [τo, τp
                                    ] for atomic activity recognition.

For each subsequence 
                                       
                                          
                                             T
                                             ′
                                          
                                          =
                                          <
                                          
                                             t
                                             
                                                τ
                                                a
                                             
                                          
                                          ,
                                          …
                                          ,
                                          
                                             t
                                             
                                                τ
                                                b
                                             
                                          
                                          >
                                       
                                     of 
                                       
                                          T
                                          ,
                                       
                                     where the length of 
                                       
                                          T
                                          ′
                                       
                                     equals to the time window and τ
                                    1 ≤ a < b ≤ τ
                                    2, use atomic activity recognition to find the activity c for 
                                       
                                          
                                             T
                                             ′
                                          
                                          ,
                                       
                                     and record its evaluation value.

Compare all evaluation values of subsequences, and then label the best evaluated subsequence 
                                       
                                          T
                                          *
                                       
                                     as its corresponding activity c
                                    *, and record its occurrence period as [τa, τb
                                    ].

Remove the labeled subsequence 
                                       
                                          T
                                          *
                                       
                                     from 
                                       T
                                    .

Execute step 3–5 iteratively until no atomic activity can be found.

Label 
                                       T
                                     as the sequential activity that is in line with the rule of atomic activity combination.

In order to be consistent with the definition of the sequential activity in SC
                           2 that any two atomic activities cannot be overlapped, sequential activity recognition removes the segment corresponding to the chosen atomic activity from the unlabeled data in each iteration.

In the SC
                           2 model, the concurrent activity is defined as the combination of several simultaneously occurring atomic activities. Similar to sequential activity recognition, concurrent activity recognition also makes use of the atomic activity recognition. The operations are the same as the operations in sequential activity recognition except for the step 5. Instead of removing the whole subsequence, the concurrent activity recognition method reduces the subsequence from the unlabeled data by a certain reduction function.

It is a challenge to define the reduction function because different concurrent activities may compose atomic activities in different ways. We present one reduction function that is reasonable in many scenarios where atomic activities are independent of each other to compose a concurrent activity.

                              Example III
                              A player performs dribbling, which consists of “walking” and “bouncing ball”. Two accelerometers are placed on the arm and the leg, respectively. x
                                 1, x
                                 2, x
                                 3 are sensor attributes from the arm, while x
                                 4, x
                                 5, x
                                 6 are sensor attributes from the leg. After “walking” is recognized by the atomic activity recognition, instead of removing the whole subsequence 
                                    
                                       
                                          T
                                          ′
                                       
                                       =
                                       <
                                       
                                          t
                                          
                                             τ
                                             a
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          t
                                          
                                             τ
                                             b
                                          
                                       
                                       >
                                    
                                  from 
                                    
                                       T
                                       ,
                                    
                                  only the subsequences from the leg sensor, i.e. 
                                    
                                       <
                                       
                                          t
                                          
                                             τ
                                             a
                                          
                                       
                                       
                                          (
                                          4
                                          )
                                       
                                       ,
                                       …
                                       ,
                                       
                                          t
                                          
                                             τ
                                             b
                                          
                                       
                                       
                                          (
                                          4
                                          )
                                       
                                       >
                                       ,
                                    
                                 
                                 
                                    
                                       <
                                       
                                          t
                                          
                                             τ
                                             a
                                          
                                       
                                       
                                          (
                                          5
                                          )
                                       
                                       ,
                                       …
                                       ,
                                       
                                          t
                                          
                                             τ
                                             b
                                          
                                       
                                       
                                          (
                                          5
                                          )
                                       
                                       >
                                       ,
                                    
                                  and 
                                    
                                       <
                                       
                                          t
                                          
                                             τ
                                             a
                                          
                                       
                                       
                                          (
                                          6
                                          )
                                       
                                       ,
                                       …
                                       ,
                                       
                                          t
                                          
                                             τ
                                             b
                                          
                                       
                                       
                                          (
                                          6
                                          )
                                       
                                       >
                                       ,
                                    
                                  will be removed from 
                                    T
                                  by the reduction function. The subsequence from the arm sensor will not be removed during this period in order to guarantee that the atomic activity “bouncing ball” can be recognized in the next iteration.

On the other hand, there are some scenarios where atomic activities are interrelated. For instance, looking at one’s wrist watch while opening a door both involve the same atomic activity, rotating the hand about the wrist, yet the two activities are not independent of a wrist sensor. To address such problem, more attributes are required, such as sensors on the head or neck, or sensors for detecting eye movement. Although more reduction functions are to be explored for other concurrent activities, the reduction function for independent activities works well in daily living and sports applications in this study, as shown in our experiments.

Complex activity recognition is similar to concurrent activity recognition. The rule is defined to describe how atomic activity combinations form each complex activity over time. Then, the atomic activities and their occurrence time can be recognized iteratively. Finally, the unlabeled time series can be inferred according to the rule.

Since complex activity is a recursive definition in SC
                           2, we use a hierarchical tree to label time series from leaves (atomic activities) to root (highest level complex activity). The following example describes a complex activity.

                              Example IV
                              An “offense” player “dribbles” forward to the basket, and then does a “lay-up”. The method determines a sequence of overlapping “walking” and “bouncing ball” for some period, followed by “running”, and then an overlapping of “jumping” and “throwing ball”.


                           Fig. 3
                            shows the inference of complex activity from atomic activity. Complex activity can be implemented according to predefined tree-like rules through occurrence time.

 “if walking and bouncing occur during time window 
                              
                                 [
                                 
                                    τ
                                    o
                                 
                                 ,
                                 
                                    τ
                                    
                                       o
                                       +
                                       1
                                    
                                 
                                 ]
                              
                            
                           then
                        

 infer activity as walk dribbling at 
                              
                                 [
                                 
                                    τ
                                    o
                                 
                                 ,
                                 
                                    τ
                                    
                                       o
                                       +
                                       1
                                    
                                 
                                 ]
                              
                           
                        


                           else
                        

 other rules”.

or

“if running occurs during 
                              
                                 [
                                 
                                    τ
                                    o
                                 
                                 ,
                                 
                                    τ
                                    
                                       o
                                       +
                                       1
                                    
                                 
                                 ]
                              
                            
                           then
                        

 
                           if jumping and throwing occur during 
                              
                                 [
                                 
                                    τ
                                    
                                       o
                                       +
                                       1
                                    
                                 
                                 ,
                                 
                                    τ
                                    
                                       o
                                       +
                                       2
                                    
                                 
                                 ]
                              
                            
                           then
                        

  infer activity as lay-up at 
                              
                                 [
                                 
                                    τ
                                    o
                                 
                                 ,
                                 
                                    τ
                                    
                                       o
                                       +
                                       2
                                    
                                 
                                 ]
                              
                           
                        

 
                           else
                        

  other rules


                           else
                        

 other rules”.

@&#IMPLEMENTATION@&#

We have implemented a prototype system in order to train atomic activity shapelet and evaluate SC
                        2 and the shapelet-based recognition approach. The system is composed of two parts: a remote sever for computing shapelets, and smartphones for recognizing activities online. Since shapelet discovery is a compute-intensive operation [67] that would quickly drain the battery if running on a smartphone, it is done on remote server. If new atomic activities or training data arrive, the server launches to compute the corresponding shapelet. When shapelet is worked out, it will be stored in a database implemented on MySQL installing on the server. The online activity recognition application on smartphones was implemented on the Android SDK. Users can download the atomic activity shapelets from the server. Data can be transmitted between server and smartphone, or between smartphones through Internet via wireless communication link like WiFi or cellular. The application can run without connecting to the server, but shapelets have to be downloaded beforehand. The application collects and uses the sensor data from the smartphone to recognize the activities according to shapelets in real-time. Fig. 4
                         depicts the architecture of the shapelet-based activity recognition prototype system.

The server comprises four parts, i.e. Candidate Shapelet Generator, Evaluation Tool, Shapelet Trainer and Shapelet Repository. The Candidate Shapelet Generator generates all possible subsequences that are extracted from training time series. The labeled training data are collected from the open datasets or from self-reported data collected from users. The final shapelets are selected from these candidate shapelets. The Evaluation Tool and Shapelet Trainer build the shapelets. When training data arrives, the Evaluation Tool starts up to evaluate the similarity between the candidate shapelets and the training time series. Evaluation comprises two parts: Feature Extractor processing and Similarity Measurement. The Feature Extractor first extracts the features from these time series, and then the Similarity Measurement measures similarity between them by features. The Shapelet Trainer uses a classifier to find out the best shapelet in terms of the similarities. Systems can incorporate different features, measurements or classifiers according to activity traits required by their applications. We will describe the Evaluation Tool and Shapelet Trainer used in our prototype system in the next section. The Shapelet Repository stores all shapelets found by the Shapelet Trainer as key-value pairs: a key, which is activity name; a value, which comprises a set of shapelets, each representing a sensor attribute.

After receiving shapelets from the server, a smartphone stores them in its Shapelets Storage, which is implemented in SQLite. The Sensor Data Collector collects the data generated from the sensors embedded on the smartphone, or perhaps the data from other devices connected to the smartphone (e.g., a heart rate sensor on the chest) or data from other smartphones. The sensor data and their corresponding time-stamps are stored in a buffer in the Sensor Data Collector. The Recognition Processor periodically wakes up to read the data from the buffer and recognize the user’s activity using the previously downloaded shapelets. The Recognition Processor contains an Evaluation Tool, which is the same as in the server. The buffer is cleared after the data have been read. The results of the predicted activities and their occurrence time can be either shown on the screen for users by way of UI, or provided for further processing via API, e.g., for the music recommendation according to the daily activity. In an application scenario where smartphones are required to collaborate to recognize activities, the Time Synchronizer is used to synchronize the time between smartphones.

We also packaged our mobile shapelet components as a plug-in, which enables mobile applications to dynamically integrate our activity recognition techniques with minimal coding effort. To illustrate how our shapelet plug-in can be useful in Web scenarios, we implemented a web-based prototype Website that is capable of automatically adapting its layout based on the activity of the user. For example, when the user is recognized as sitting or standing, the Website displays the complete media player user interface, including many small buttons for controlling all media options such as track position, repeat options, etc. When the user is recognized as walking or running, the Website automatically simplifies the media player’s interface by removing unnecessary buttons and enlarging the media controls, enabling the user to more easily control media playback while moving (e.g., while jogging).

In our prototype system, the buffer size is set to 1000 for sensor data storage. If new sensor data arrive and the buffer is full, the new data will replace the oldest one in the buffer. The Recognition Processor execution interval is set to 2 s in order to remain faithful to the real-time aspect of our prototype system. We used the world time server, Atomic Clock Sync, to synchronize the time. The prototype system was installed on Sony Xperia Z1 with a quad-core 2.2 GHz processor and 2 GB RAM, and a Samsung I9295 Galaxy S4 Active with a quad-core 1.9 GHz processor and 2 GB RAM. Fig. 5 depicts the UI for users. Users can start or stop activity tracking, and read their activities and the occurrence time predicted in real-time (as shown in Fig. 5(a)). Users can also retrieve their historical data stored in the History Storage on the phone (as shown in Fig. 5(b)). Fig. 5(c) shows the web-based UI (running in Chrome).

We aim to recognize the daily living and sports activities in our studies. During daily living ambulation, various users may perform the same activity with different strength or duration; hence, the corresponding patterns reflected in the time series collected from sensors may be distinct from each other. Using point-by-point distance measurement would not be robust enough for time shifting, amplitude scaling, time scaling, or even noise. Fig. 6
                            shows an example of time series of users’ walking and jogging data. We can see that the amplitudes are shifted or amplified between these time series. For instance, the higher amplitude pattern and shorter duration pattern during jogging imply joint movements with higher strength and speed. If the Euclidean distance is directly used to measure the similarity of these time series, the amplitude differences may give rise to misclassification issue. Since a specific activity may vary substantially from one user to another, common features need to be extracted for representing the activity in a generalized manner.

According to the gait cycle (or stride) theory [69], walking and jogging have different traits that are reflected in the time series. These traits are commonly divided into the stance phase and the non-support phase. Fig. 7
                            shows the unique traits in the gait cycle: the stance phase of walking is composed of two periods of double supports, and the non-support phase of jogging is composed of two float periods and one swing period of the ipsilateral leg between them. Although users performs the same activity in different manners, each specific activity (e.g., walking) has its common traits.

We considered four strategies for use in similarity measurement: edit-based, model-based, compression-based and feature-based methods [70]. Edit-based measurement characterizes the difference between two time series. Model-based and compression-based methods target a very specific dataset or prior knowledge about the data. Feature-based methods are robust for scaling, warping and noise. Notably, feature-based extractions often take comparatively shorter computational time, and are therefore suitable for energy constrained mobile devices. Hence, to address the problem of shifting and scaling, we choose feature-based method for the sake of scalability and flexibility.

The following features are adopted in our prototype system.

                              
                                 •
                                 
                                    Statical Features: min, max, mean, variance, STD and Median values. These features commonly refer to the amplitude information.


                                    Time and Frequency Features: auto-correlation, mean-crossing rate, spectral entropy, spectral energy, and wavelet entropy values. These features commonly refer to the time and frequency domain information.


                                    Structure Features: FFT, DFT, and DWT values. These features are often used to represent the structure of time series and address amplitude scaling and time warping issues.


                                    Peak and Segment Features: intensity, Kurtosis and peak frequency values of the time series. These features are often used to capture the interrelationships among data points in a time series.


                                    Coordinate Features: radial, polar angle and azimuthal (spherical coordinate); radius, azimuth, and elevation (cylindrical coordinates). We selected these features to unify the positioning information for various motion sensors.

We use distance measurement to evaluate two time series over the extracted features. There are several techniques for calculating distance measurements on these features, e.g., FDR, Divergence, Bhattacharyya Distance, and so forth. In the original shapelet paper [42], the authors adopted Information Gain as the distance measurement, and in a follow-up work [71], the authors used F-statistic.

In this work, we use the Euclidean Distance, denoted by dist() (i.e. the evaluation function 
                              
                                 f
                                 =
                                 −
                                 d
                                 i
                                 s
                                 t
                                 (
                                 )
                              
                           ), to assess the similarity between time series because it is simple to measure multiple activities, and because the utility of Information Gain degrades with multi-class problems. In addition, the Euclidean Distance is able to accommodate the incremental measurement of a new activity without retraining. Finally, the Euclidean Distance is well suited for resource-limited devices like smartphones because no data needs to be stored, and the computational complexity is less than that of Information Gain or F-statistic.

After the features of time series are extracted and distance measurement between them are determined, the best shapelet representing each activity is selected from the Candidate Shapelet set. In our prototype system, the Candidate Shapelet set consists of all subsequences of training time series.

The problem of training shapelets can be formalized as a constrained optimization problem of finding a shapelet 
                           
                              
                                 S
                                 k
                              
                              ∈
                              
                                 Ω
                                 k
                              
                           
                         for activity ck
                         such that the Euclidean Distance between 
                           S
                         and all training time series in 
                           
                              
                                 D
                                 k
                              
                              ,
                           
                         over feature space 
                           F
                         is minimized:

                           
                              (3)
                              
                                 
                                    
                                       
                                          
                                             S
                                             k
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                
                                                   arg
                                                   max
                                                
                                                
                                                   S
                                                   ∈
                                                   
                                                      Ω
                                                      k
                                                   
                                                
                                             
                                             
                                                (
                                                −
                                                d
                                                i
                                                s
                                                t
                                                
                                                   (
                                                   S
                                                   ,
                                                   
                                                      Ω
                                                      k
                                                   
                                                   )
                                                
                                                )
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                          ∝
                                       
                                       
                                          
                                             
                                                
                                                   arg
                                                   max
                                                
                                                
                                                   S
                                                   ∈
                                                   
                                                      Ω
                                                      k
                                                   
                                                
                                             
                                             
                                                (
                                                −
                                                
                                                   ∑
                                                   
                                                      T
                                                      ∈
                                                      
                                                         D
                                                         k
                                                      
                                                   
                                                
                                                
                                                   
                                                      (
                                                      F
                                                      
                                                         (
                                                         S
                                                         )
                                                      
                                                      −
                                                      F
                                                      
                                                         (
                                                         T
                                                         )
                                                      
                                                      )
                                                   
                                                   2
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Many optimization techniques [67,72,73] can provide a near-optimal solution in reasonable time, including meta-heuristic algorithms or nature inspired artificial techniques. Regardless of the method used for training shapelets, they are computed on the server.

@&#EVALUATION@&#

In order to evaluate the time series shapelet approach for activity recognition, we first used two open datasets that were collected on various settings (sensor types, sensor numbers, etc.) from the activity prediction dataset which is part of WISDM Lab [68] and the daily and sports activities datasets from UCI [74,75]. We conducted two case studies for complex activity recognition: daily living activity recognition and basketball play activity recognition.

We used six-fold cross-validation to evaluate the atomic activity datasets. The datasets were randomly partitioned into six equal size subsets. One subset that was retained for validation, and the remaining subsets were used as training data. We repeated the evaluation procedure six times, using each subset for validation. We report the average results using recall(rec), which measures the proportion of activities that are correctly predicted, precision(prec), which measures the proportion of a predicted activity that is relevant, and accuracy acc, which measures the proportion of the true positives of all activities in multi-class classification.

The WISDM dataset contains a set of time series from one accelerometer covering 6 activities from 29 users. The activities are walking, jogging, sitting, standing, upstairs and downstairs. Each time series sample lasts for 10 s at 20 Hz, and 38, 643 samples are used for evaluation. The UCI dataset contains 19 activities from 8 users. Five single activities were used for evaluation, including sitting, standing, walking on a treadmill, running on a treadmill and jumping. There are 60 samples of raw time series for each activity per person. Each sample lasts for 5 seconds at 25 Hz. The time series consists of multi-source data from 5 sensor units that were placed on a subject’s torso, right arm, left arm, right leg and left leg. Each sensor unit consists of an accelerometer, gyroscope, and magnetometer.

As shown in Table 1
                        , our approach achieves high recall and precision when recognizing activities like walking, jogging, running, standing, and sitting. Jumping recognition recall and precision are relatively low due to poor approach performance for one user (User8), whose results included 25% (running, recall), 5% (jumping, recall), 20.4% (running, precision) and 5% (jumping, precision). Running and jumping are difficult to discriminate for this user. Our approach is not very good at recognizing upstairs and downstairs for nearly all users; however, other approaches also have difficulties recognizing these activities, since upstairs and downstairs are not single activities, but complex activities [68]. Upstairs is actually a combination of at least two activities (walking and leg raising), while downstairs consists of walking and leg sustaining. As such, these activities are often falsely predicted as walking, lowering the recall and precision for walking when compared to other atomic activities. When upstairs and downstairs are removed from the dataset, the recall and precision for walking are 100% and 72.19%, respectively.

Each of ten subjects was asked to carry a Sony Xperia Z1 smartphone in his left leg pocket during eight work days. The mobile shapelet technique was executed on the smartphone in addition to compulsory apps. The routine of each subject contains five complex activities, i.e. relax, meeting, office working, having lunch/dinner, and physical exercise. Four atomic activities, walking, jogging, sitting and standing were recognized in this case study. Since it is difficult to ask a subject to continuously self-report every activity performed precisely, the percentage of time performing a certain activity is used for long-term activity recognition. The subject was asked to record what he had done every half hour. All training data for this case are from the activity prediction dataset in order to validate the user-independence of the approach.


                           Fig. 8
                            shows the routine of the subject. We found that the routine was extremely similar throughout the eight days between 9 am and 9 pm. We roughly estimated the average percentages of time that the user performed the four activities as follows: 18%(walk), 2%(jog), 40%(sit) and 40%(stand). The predicted percentages of the activities recognized by our shapelet approach were 8.23%(walk), 0.91%(jog), 38.69%(sit) and 52.17%(stand). Our predicted activities were generally very similar to the user’s self-reported activities; however, sitting was occasionally falsely predicted as walking because the subject shakes his leg while at his desk. Sitting, standing and jogging were more accurately recognized by our approach, which is in line with the results shown in the offline database evaluation.

We compared the shapelet approach with other seven popular classifiers, including C4.5 (J48), Logistic Regression (LR), Neural Network (NN), Naïve Bayes (NB), K-nearest Neighbour (KNN), Decision Table (DT), and Support Vector Machine (SVM). Table 2 shows a real-time online performance comparison of the different activity recognition approaches by the percentage distribution. We needed to set sliding window sizes for all approaches except for the shapelet approach. We found that the results are greatly affected by the selection of window sizes. For example, the recognized percentage of walking is 20.41% by LR with window size of 50. When window size changes, the range of the recognized percentage changes up to 16.49%. In our experiment, we compared the results with 10 window sizes (ranging from 50 to 500) for each approach.


                           Table 3
                           
                            shows the comparison results of average power usage, CPU usage and memory usage that were recorded during the eight days of evaluation (from 9 am to 9 pm). The subject’s routine was similar during these days. The average power usage of the shapelet approach is 0.44%, which is low compared with other approaches and acceptable for most of applications. NB has the highest power usage (3% per hour) whilst KNN has the lowest power usage (0.38%). In terms of CPU usage, the shapelet approach is the most efficient. Although NN and SVM are accurate on recognition, they consume as much as 10 more CPU time than the shapelet approach, and require twice as much memory.

This section reports our comparison study on the five complex activity recognition. In this experiment, our SC
                           2 model was compared with other three recognition models, i.e. IHMM, SCCRF and ITBN. We considered to generate a user-independent recognition system by inputting the entire sequences of atomic activities belonging to the same complex daily living activity. Table 4
                            depicts the comparison results of the average recall, precision and F-measure over eightfold cross-validations (data collected from the same day containing in a separate fold). SC
                           2, which uses the hierarchical structures organized by the four atomic activities and their temporal relations, achieves the best performance on F-measure. IHMM and SCCRF falsely recognizes most of meeting as having lunch/dinner. This is because both activities are associated with the sitting. It is hard to distinguish the two complex activities only using the frequencies at which the atomic activities occur. ITBN outperforms IHMM and SCCRF because it uses the temporal relations between atomic activities. All the three competing models do not consider any hierarchy about complex activity.

Four subjects, who are both basketball amateurs, were asked to perform basketball play actions with two smartphones attached on left leg and right arm, respectively (shown in Fig. 9
                           ). Three stages were undertaken for this study. The first stage is training data collection. In order to discriminate basic basketball play actions, eight atomic activities were performed and used to train their corresponding shapelets. These activities are categorized into two groups: lower limb (leg) activity and upper limb (arm) activity. Lower limb activities include walking, running, standing, and jumping. Upper limb activities include throwing ball, bouncing ball, passing ball and raising hands. Data for training lower limb activities were only collected from leg sensors while data for training upper limb activities were only collected from arm sensors. The subjects were asked to perform these activities according to their own habits. Walking, running and standing were performed for 3 minutes each, and the other activities were performed 30 times each. Accelerometer data were collected during the performances. Fig. 10
                            shows the shapelets of these atomic activities.

The second stage was to recognize basic complex basketball play activities, including set-shot, jump-shot, lay-up, walk dribbling, run dribbling, and blocking. Each activity was performed 30 times. Slam dunk was not contained in our recognition because no subject could perform it successfully. The second stage comprises four steps. The server first builds the shapelets for all atomic activities. The smartphones then download these shapelets into their local storage via campus WiFi. Before conducting the recognition, the two smartphones synchronize their clocks. Finally, during the activity recognition, the two smartphones collects sensor data in real time at 20 Hz, resulting in atomic activity recognition results every 2 s. The smartphone on the leg sends the predicted atomic activities along with the time-stamp to the smartphone on the arm via bluetooth every 2 seconds. The smart-phones share activity data rather than raw data in order to distribute the computation workload and save data transmission costs. The recognition process on the upper smartphone integrates the results from the lower smartphone in order to perform complex activity reasoning based on predefined rules.

The third stage was actual two-on-two basketball games. We want to use the six basic types of complex basket play activities to recognize higher complex activities. In the two-on-two games, we defined two offensive play, i.e. Play Type I (PT-I): player 1 receives the ball from throw-in and passes to player 2, and player 2 lays up the ball; Play Type II (PT-II): player 1 receives the ball from throw-in and jumps shot directly. The game was recorded by a video in synchronization with the sensor data collection system. The annotation of the two types of offensive play was completed by hand using the video annotation research tool ANVIL
                              1
                           
                           
                              1
                              
                                 http://www.anvil-software.org/
                              
                           .


                           Table 5
                            shows the recall and precision results for recognizing six complex basketball play activities. Set-shot is often falsely recognized as jump-shot because the subject bends his knees, which is a part of jumping. Accordingly, the recall performance of set-shot and the precision of jump-shot drop significantly. Dribbling has two independent, atomic activities occurring on the arm and leg, and thereby achieves a high recognition rate. Blocking has a relatively low precision result because set-shot, jump-shot and lay-up are often falsely recognized as blocking. The reason is that throwing a ball is very similar to raising hands, as shown in Fig. 10(e) and (h). Lay-up is the hardest activity to recognize. This is partially due to inaccuracy of the collected sensor data. Specifically, a lay-up is often completed within 1–3 s; however, our smartphone’s 20 Hz sampling rate was too low to adequately recognize the three atomic activities that comprise a lay-up. Hence, lay-up is often recognized as jump-shot. We think that our approach could perform better using more wearable sensors with a higher sampling rate.

The experiment of recognizing two offensive play types was performed with a tenfold cross validation setting. Fig. 11 shows the confusion matrices produced by the four models, i.e. IHMM, SCCRF, ITBN, SC
                           2. We can see that the performance of our SC
                           2 model is significantly better than the other three models. For clarity, we also provide the classification accuracies in captions under each subfigure. It is clear that SC
                           2 outperforms IHMM and SCCRF in recognizing both of the activities. ITBN recognizes PT-I slightly better than SC
                           2 but much worse for PT-II. This is because ITBN is built on the Bayesian network that is a directed acyclic graph. It has to remove some temporal relations from the training dataset in order to make the Bayesian network temporally consistent. For example, if a network contains “A before B” and “B before C”, ITBN will ignore the relation “A after C” in any training sequences, resulting in information loss during model training. Generally, SC
                           2 can perform accurate recognition mainly due to its ability to take advantage of the hierarchical and temporal information among activities from multiple layers. Besides, these models are also different in computational complexity. ITBN takes much time to check temporal consistency and evaluating all possible network structures (i.e. what relation should be ignored or not), and thus are more computationally expensive than IHMM, SCCRF and SC
                           2 models. In addition, in order to recognize activities at different levels, IHMM, SCCRF and ITBN have to construct a separate network structure for each class of activities, taking the activity recognition results as input for higher level classifiers. The SC
                           2 model can be implemented efficiently because it can recognize activities at arbitrary levels using shapelets.

@&#CONCLUSION@&#

In this paper, we presented a shapelet-based approach for recognizing complex human activity. A four-type activity model was defined to represent simple, sequential, concurrent and complex activities respectively. We formalized the shapelet-based recognition approaches for different types of human activity. The experimental results showed the viability and efficiency of our approach. The benefits of the proposed approach are manifold. It handles temporal information very well, and supports hierarchical activity recognition. The results generated by the shapelet-based approach can be interpretable and accurate, and the approach can operate efficiently in real-time, even on commodity smartphones.

Our experimental results show that there is still room to improve our SC
                     2 model for recognizing complex activities more accurately. The recognition errors made by our model are mainly due to the false measurement between two highly similar shapelets, each representing an individual activity. There are two ways to solve this issue. One strategy is to use shapelet transformation. In our model, tree-like rules are designed to classify complex activities. Decision trees are useful and robust classifiers, but are outperformed in many problem domains by other classifiers, such as support vector machines and Bayesian networks. Hills et al. [76] examined how best to use the shapelet primitive to construct classifiers, and proposed an algorithm that transforms the best k shapelets to a dataset of features that can be used in conjunction with any classifier. Their results show that the transformed data can be used with other classifiers more than decision trees, which achieve improved accuracy. The second solution is to generate an ensemble of shapelet-based decision trees for complex activity recognition. Cetin et al. [73] proposed a novel ensembling technique for finding the nearest neighbors of the candidate shapelets efficiently. They claimed that their technique can efficiently discover shapelets on datasets with multi-dimensional and long time series, which will be useful in our activity recognition scenarios involving multi-source sensed data (e.g. basketball play activity recognition) and long-term records (e.g. daily living activity recognition).

A major limitation of the proposed approach is the time-consuming process of finding shapelets. This limitation is mainly due to the estimation of a large amount of possible subsequences that are extracted from training time series. The majority of recent shapelet research has focused on techniques to speed up the search to make the shapelet-based methods more tractable [67,72]. One potential alternative for finding the shapelets is using optimization techniques, like the genetic algorithm or deep learning, to reduce the time consumption. We believe that this is a promising research direction to follow. Our future work involves the development of new techniques to alleviate the shapelet generation through optimization techniques. Another direction to alleviate the high time-consuming issue is finding a monolithic or group-specific model for certain applications would be more effective because the shapelets need to be calculated only once for all users.

To the best of our knowledge, no one-fits-all model has been defined for concurrent activity recognition so far because of the number and granularity of activities. Current approaches for multilayered activity recognition are typically semantic-based because of their ability to model complex relationships in different levels of activities (e.g. ontology-based approaches) [77]. However, logical rules or formulae in these semantic-based approaches need to be carefully handcrafted to a specific application [15]. The presumption of the shapelet-based approach is that every concurrent activity comprises several other atomic activities. A group of concurrent activities could be quantitatively measured if its reduction function exists. The reduction functions need to consider different concurrent activities that are combined in multiple ways. We consider the exploration and development of reduction functions an important next step in this work.

@&#ACKNOWLEDGMENT@&#

This work was supported by grants from Cuiying Grant of China Telecom Gansu Branch (grant no. lzudxcy-2013-3), Science and Technology Planning Project of Chengguan District, Lanzhou (grant no. 2013-3-1), National Natural Science Foundation of China (grant no. 61370219), Chongqing Social Science Planning Fund Program (grant no. 2014BS123), Chongqing Social Science Planning Fund Program (grant no. 2014BS123), Fundamental Research Funds for the Central Universities in China (grant no. XDJK2014A002 and XDJK2014C141 and SWU114005) and Scientific Research Foundation for the Returned Overseas Chinese Scholars, State Education Ministry (grant no. 44th and 48th).

@&#REFERENCES@&#

