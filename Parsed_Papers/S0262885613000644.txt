@&#MAIN-TITLE@&#Feature subset selection applied to model-free gait recognition

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A feature selection framework is proposed to achieve high performance model-free gait recognition.


                        
                        
                           
                           The feature selection mechanism relies on the Random Forest algorithm.


                        
                        
                           
                           Regions selected are more robust to covariates while reducing the computational cost.


                        
                        
                           
                           Panoramic gait recognition is achieved under covariate conditions.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Feature selection

Gait recognition

Model-free

Panoramic

Random forest

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

Many researchers believe that gait contains information regarding both the gender and identity of a person. The first psychological evidence in support of the above fact was provided by [21]. The main task involved in gait recognition is the selection of appropriate features, one that can extract as much information from the gait sequence. In the start temporal based features, specifically the ones that represent the entire gait trajectory were intuitively believed to represent gait and perform recognition effectively. But despite their success, after large data sets like CASIA Set B [38], Gait Human ID challenge [32], SOTON Database [35], and OU-ISIR Database [29] were introduced, the research community started shifting their interest towards robust static representations [28,17,8] as they are more compact and not computationally expensive.

Broadly, one can classify the different approaches for gait recognition in two categories: model-based and model-free. The model free approach considers the motion of human body holistically, and features are computed without considering the underlying structure. As compared to model-based approach, one requires significantly lesser amount of computational power but the feature space which one gets is more correlated and has more dimensions. On the other hand, the model based approach measures parameters concerning kinematic data or tries to fit the parameters to a human model. The advantage of a model based approach is that the feature space dimension is really low, but one of its down sides is high computational complexity. Johansson's experiment [21] pointed out to the fact that one does not require entire structural information for recognition of a person. This significant result motivated the research community to explore various possibilities in the context of model free features. This is the reason that motivates us to evaluate our feature selection framework on model-free features.

@&#RELATED WORKS@&#

There exists significant work in the field of model free approach [18,7,37]. The use of model free approach implies a high dimensional feature space and hence one has to apply data reduction techniques. On the one hand, the most common of the dimensionality reduction techniques applied are PCA and ANOVA, despite the fact that they suffer from some drawbacks [15]. On the other hand, Guo et al. in [15] performed feature selection. The work and results by Guo et al. are significant, as it is shown by using mutual information one can achieve a recognition rate higher than 90% while just using 0.61% of feature space for model free approach. The only issue with mutual information is with its computational complexity and to avoid that, one has to avoid the higher dependencies among variables as done in [15].

Obviously, we cannot assume the higher order independence among variables [2] and we wish to perform feature selection using a different framework.

We used CASIA database to evaluate our framework. We used CASIA Gait Energy Image (GEI) features provided on the database web page. These GEI features were used in [41] and made freely available.

We applied our feature subset selection on the GEIs. This feature has demonstrated the ability to improve the CCR and elevate the problem of processing complexity. A subset of features is selected from the feature space using the Random Forest algorithm [11]. The mask found is combined with commonly used classification methods.

Our results suggest that only two subparts of the GEI should be kept. They contribute to improve the correct classification rate while reducing the computational complexity of the recognition step. Our experiments demonstrate that these two parts are able to overcome the problem of covariates. The framework introduced indicates that panoramic gait recognition is feasible under unknown covariate conditions.

The rest of the paper is divided as follows. We give a brief description of the state of the art regarding features used in gait recognition in Section 2. The motivations that led us to shortlist the features used in this paper are also given in this part. In Section 3, we give an overview of our feature selection framework. Section 4 describes our experiments and results. Finally, in Sections 4.4.5 and 4.4.6, we discuss our findings and conclude.

Until now, there exists a plethora of features available for doing gait recognition. They can be classified according to various criterions, such as dynamic versus static, area based versus point based. To have a brief understanding of the strong and weak points of the existing state of the art features, we have categorized them in five families of features. Each family is believed to have their own advantage and disadvantage.

The following section gives a brief introduction about five families in which the existing features are categorized.

Methods falling within this family use points on contour as feature and have a common advantage of low computational cost. The disadvantage of using points is the effect of shape co-variants, requirement of a good amount of training data and ineffectiveness in the case of a poor resolution video. Within this family, the approach by Hayfron-Acquah et al. [18] seems to be one of the best in the field. The use of symmetry operator alleviates recognition from affect of segmentation errors and noise. One of its biggest advantages is that it can form a robust signature with a very small amount of training data.

One of the greatest advantages of the features lying in this family is the availability of a huge amount of data in a single template, and therefore minor segmentation errors can be avoided. On the down side, availability of such a huge amount of data increases the computation cost and more so ever shape co-variants affect the silhouette drastically. Many approaches have been done in this field and all require a preprocessing to be done to alleviate the affect of shape co-variants. One of the works worth mentioning in this family is by BenAbeldkar [8]. Author proposes the use of eigenspace projection of self-similarity plot of the person. Later [7] introduces SSU, which is basically a sub matrix of SSP and is more robust to shape co-variants, noise and image resolution.

Methods using optical flow/temporal difference have a common advantage that whether they are making a spatial template or a temporal template series, they inherently capture the dynamic aspect of human motion. The disadvantage is that the calculation of optical flow consumes some of the available computational power. Also, if one wants to take in account the direction of optical flow then the recognition results have been seen to deteriorate especially if the video is having a poor resolution. In starting, features introduced extracted local properties of an image and hence had a high computational overhead [33]. In the recent past, the scientific community has started looking for global features and some note worthy work can be seen on SVB frieze Patterns [26], CHILAC features [22], and Flow fields [6].

Due to the increased computational cost of temporal matching, the research community finally started to shift towards robust static signatures. In 2004, Liu and Sarkar [28] introduced the simplest signature, the average image of a silhouette in a gait cycle. The feature performs well against baseline algorithm proposed in the Gait identification problem [31]. Various approaches using Motion entropy or Motion History/Energy have been introduced to counter the affect of shape, shadow covariates and segmentation errors. One of the notable works in this field for improving the quality of silhouettes can be seen in [13]. Performance of their algorithm is evaluated on CASIA Data set B and CMU Mobo database, and a notable improvement is seen in the performance of wavelet and Frieze features [27].

As compared to other families, not much work has been done in this family up to the last decade. Although due to the complex mathematical moment's, one is able to extract useful information to represent the silhouette but most of the mathematical operators end up becoming computationally complex. The suitability of using simple moments for object recognition was demonstrated by Hu [19]. The problems associated with the widely used Cartesian moments are that the moments produced are correlated and are not orthogonal. To solve this problem, Shutler and Nixon [36] introduced Zernike velocity moments, in which the orthogonal moments are computed on the temporal flow. The features obtained are more robust to low resolution, occlusion and shape co-variants. In the recent past, many other approaches have been introduced like wavelet moment [39], Fractal scale [40] and Gabor Filters [37].

Among these five families, the fourth family offers the best trade-off between computational requirements and recognition performance. Gait Energy Image (GEI) [17] has proven to be a really good descriptor despite its simplicity. It is nowadays among the best descriptors [24] on challenging sets such as Gait Human ID challenge [32]. The GEI is computed as follows:
                           
                              (1)
                              
                                 
                                    GEI
                                    =
                                    G
                                    
                                       x
                                       y
                                    
                                    =
                                    
                                       1
                                       T
                                    
                                    
                                       
                                          ∑
                                          
                                             t
                                             =
                                             1
                                          
                                          T
                                       
                                       
                                          I
                                          
                                             x
                                             y
                                             t
                                          
                                       
                                    
                                 
                              
                           
                        where T is the number of frames in the gait cycle, and I is a silhouette image.

GEI energy image has two main regions: the static and dynamic areas [3]. These two areas contain different types of information. Dynamic areas are considered as being invariant to the individual's appearance. Bashir et al. defined it as the most informative part of the signature. He argued that, despite being useful for identification, the static area of the signature should be discarded as non invariant to the body shape. In order to identify the most significant body parts, he adopted a wrapper approach to sequentially remove rows from the signature and compared its results to an unsupervised feature selection approach [3]. The unsupervised method consists in keeping the pixels, which values variate a lot throughout the GEI signatures. In [4], Bashir et al. introduced the Gait Entropy Image (GEnI). GEnI actually emphasizes the dynamic part of signature. The GEnI is obtained by applying a pixel-wise Shannon Entropy operator to the GEI. The operator is computed as follows:
                           
                              (2)
                              
                                 
                                    GEnI
                                    =
                                    H
                                    
                                       x
                                       y
                                    
                                    =
                                    −
                                    
                                       
                                          ∑
                                          
                                             k
                                             =
                                             1
                                          
                                          K
                                       
                                       
                                          
                                             p
                                             k
                                          
                                          
                                             x
                                             y
                                          
                                          lo
                                          
                                             g
                                             2
                                          
                                          
                                             
                                                
                                                   p
                                                   k
                                                
                                                
                                                   x
                                                   y
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where pk
                        (x, y) is the probability that the pixel takes the kth value.

In the case of the GEI, pk
                        (x, y) represents the probability of a pixel of belonging to the foreground throughout the gait cycle. The GEnI actually maintains an excellent Correct Classification Rate (CCR) on normal gait sequence while improving the performance on covariates.

In 2010, Bashir et al. [5] proposed a feature selection method of the GEI signature, which demonstrated the ability to improve the performance of the signature. The GEI is masked with the gait entropy image (GEnI). While the resulting signature template is more robust to shape covariates, it also maxes out the performance for normal gait sequence. The extra steps required to compute the GEnI are low. A binary mask is formed by zeroing pixels having an entropy smaller than 0.75. The resulting signature, named Masked Gait Energy Image, is obtained by taking a logical AND between the GEnI mask and the GEI. An example of the GEI and its MGEI counterpart is given in Fig. 1a and b.

Despite, the static part of the GEI has been rejected since [3], we believe it actually contributes to the identification. As a consequence, we decided to work with the GEI. Considering the static area, as defined by Bashir et al., as one piece may not be the right option to handle covariates. Consequently, we propose a fine level definition of the features. For the rest of this paper, we will consider one pixel as a feature. Then, the aim of the feature selection algorithm presented in the next section is to find GEI's pixels that contribute to improve the CCR.

Feature subset selection is a technique used to reduce the variable dimensionality of a dataset. Contrary to linear algebra reduction-based techniques, the feature space is not modified. Feature subset selection techniques imply two components [1,16]: variable ranking and search strategy. Feature subset selection techniques can use a filter, a wrapper or an embedded approach. Decision tree classifiers are a good example of an embedded feature subset selection approach. Given a feature set, the algorithm itself decides which attributes to use or discard. Embedded approaches are really convenient as they combine the feature subset selection and learning in a unique framework. Contrary to filter approach, the selection criterion is not the relevance of the feature but its usefulness. Embedded methods are not prone to overfitting and are not computationally expensive. Still, once we choose the feature ranking strategy, we have to decide which overall search strategy we are about to choose. They can be classified in 3 categories.

Exponential algorithms evaluate a number of subsets that grows exponentially with the dimensionality of the space. Examples are exhaustive search, branch and bound or beam search. Sequential algorithms modify (add or remove) features sequentially from the search space. One drawback of these algorithms is that they can get trapped around local minima. Examples are forward selection, and backward elimination. Randomized algorithms overcome the problem of local minima by incorporating randomness into their search procedure. Examples of randomized algorithms are simulated annealing, and genetic algorithms.

As mentioned in the Introduction, Guo and Nixon already investigated feature subset selection for the purpose of gait recognition [15]. They achieve a CCR of 90% with only 0.61% of the original feature space. However, the complexity implied by the use of the mutual information approach forces them to neglect higher dependency among the variables. As demonstrated in [2], the high dependency between variables cannot be omitted. Moreover, Guo and Nixon used a forward sequential selection (FSS). Doak et al. [14] reported that FSS is outperformed by backward sequential selection (BSS). As a matter of fact, FSS is unable to remove a feature that has become obsolete after the addition of other features. Caruana and Freitag [12] reported the same argument in favor of BSS over FSS. However, their experiments did not clearly confirm their statement. As a resulting, we will consider BSS and FSS as candidate techniques for search strategies.

While one might think that removing feature corresponds to a loss of information, this is not true if redundant or irrelevant features are present. Let's consider the training set T
                     ={{X
                     1, Y
                     1},⋯, {Xn
                     , Yn
                     }} made of n observations. Let's use subscripts to represents the observation index while the superscript represents the variables. Xi
                     
                     ={Xi
                     
                     1,⋯, Xi
                        P
                     } is defined in the p-dimensional feature space. Yi
                      actually corresponds to the class of observation i. The aim of feature subset selection is to reduce the number of features used p
                     ≪
                     P while preserving the classification performance.

Ensemble methods have shown to improve classification accuracy by merging the prediction of multiple base classifiers. The final classifier decision is based on a voting merging scheme. Among embedded feature subset selection methods based on ensemble classification methods, Random Forest (RF) [11] is one of the most powerful and efficient algorithms. Random forest is based on the bootstrap-aggregating concept [10]. Random Forest has demonstrated to outperform other ensemble-based classification methods in several domains [25,9]. Random forest performs really well especially when there is a small number of informative variables hidden among a large set of variables.

A random forest actually combines fully-grown binary decision trees. Each tree is trained on a randomly chosen with replacement n in-bag ensemble of observations. Out-of-bag (OO) observations are used to test and as a result evaluate the performance of the tree. It was shown that one-third of the observations are left out and result as the OO data [11]. As a consequence, the OO classification error actually corresponds to combined error of all trees in the forest. Test set classification error is obtained for each observation for one-third of the tree pool. Each tree is tested on its OO data. For a given observation Xi
                      in T, the votes of each tree having Xi
                      in its OO data are combined to give the final class of an observation. A tree represents a vote for a particular class. The OO classification error corresponds to the rate of mistaken votes, i.e. when the vote is different from Yi
                     . For each node of the tree, randomly choose n
                     <
                     N features and base the best split and as a result the decision onto this subset. The best split is determined based on the Gini impurity criterion. The OO classification error actually gives a good representation of the generalization error.
                        
                           
                        
                     
                     
                        
                           
                        
                     
                  

Unfortunately, the main drawback of randomness is the difficulty to understand and visualized the decision boundary. In order to overcome this weakness, Breiman et al. proposed several variable importance measurements. As mentioned earlier, votes on OO data are used to evaluate the OO classification error. Several variants of variable importance measures exist. However, they are all based on the same concept. For a given tree t and its associated OO data, randomly shuffle all the value of the given variable. Then, test each Xi
                      on the modified OO data on this tree and measure the variation of votes for the associated class Yi
                     . Reproduce this operation for all variables among the trees. Variable importance measures can be proportional to the drop or increase of votes for the correct class. It corresponds to prediction error for the OO observations. We refrained from using this approach. Inspired by the large-margin classifier concepts, we scored the variable according to their importance in the classification margin. As a consequence, we aim to find the features that will generalize better.

@&#EXPERIMENTS@&#

The random forest (RF) used in the following experiments is a conventional random forest. The experiments were performed with Matlab. The RF is made of 200 trees. The impurity criterion for the trees is the Gini index.

In a first part, we carried out our experiments on the 90° view of CASIA Database B [38]. The GEI signatures were made available by [41]. We believe that this dataset will be extensively used in future works. In fact, GEI and its variants are extensively used by the community. However, contrary to Gait Human ID challenge, these signatures were not provided for CASIA. As a consequence, one had to compute them from silhouettes. The subsequent steps required to obtain the GEI, despite common methods, may slightly differ from one paper to another. It results slightly different signatures that may lead to different performances. Zheng et al. signatures provide a common framework where researchers will be able to compare their approach from common signatures. The signatures are 240×240pixel images stored using 256 shades of gray.

In a second part, we extend the results of our feature selection by proposing a simpler mask. We investigate its use on the entire CASIA Database B. We demonstrate that the mask contributes to achieve a fair panoramic gait recognition CCR.

Test sets are named and split differently among databases. As defined in [38], CASIA Database B was build to focus on effects other than human detection. Within this framework, three sets were defined. Set A is used to investigate how view angle affects the gait recognition performance and an algorithm's robustness to view variation. Set B is used to analyze how clothing affects the performance. Finally, Set C is used to consider how carrying condition alters the algorithm performance. Consequently, we wish to evaluate how our algorithm performance is altered by carrying and clothing conditions. Regarding the viewpoints, we want to measure the robustness of our algorithm when the probe and gallery have the same view angle for normal gait sequence. The algorithm may be specific to the 90° views and inefficient for other angles. The performance may also be reduced when the viewpoint is combined with changing clothing or carrying conditions. CASIA Database B includes 124 subjects. Ten sequences have been recorded for every subject. First, six out of the ten are normal sequences. This set is commonly referred to as CASIASetA [5]. It is usually split into two sets. The first four sequences are used for training and sometime named CASIASetA1. The last two sequences are used for the test and named CASIASetA2. Secondly, two of the sequences were recorded with bags carried by the subject. It is referred to as CASIASetB. Finally, the last sequences were recorded with the subject wearing a coat (CASIASetC). The average CCR on those three sets is referred to as Average CCR. Algorithms are usually trained on CASIASetA1 and tests are performed on the other three sets. Gait recognition performance should be independent of the training set. Correct Classification Rate (CCR) reported so far may not represent the generalized CCR. As a consequence, we overcome this issue by proposing a cross-validation approach to obtain a generalized CCR. Test sets have been composed of 2 sequences. As a consequence, we propose a 15-fold cross-validation approach. As a matter of fact, we propose to train the classification algorithm on any of the four combinations of sequences from CASIASetA. We define three extra tests. Firstly, CASIASetA2x will report the mean performance of our approach on any of the two remaining sequences from CASIASetA. Secondly, the index CASIASetBx will give the cross-validation performance on the test set CASIASetB. Finally, we will use CASIASetCx to notify the cross-validation mean CCR on CASIASetC. The average CCR on those three additional sets is referred to as Average CCRx.
                           
                              Table 1
                              
                                 Impact of signature resolution on matching performance.
                              
                              
                                 
                                 
                                 
                                 
                                 
                                    
                                       (a) CCR on CASIA's test sets — template matching on [41].
                                    
                                    
                                       Resolution
                                       CASIASetA2
                                       CASIASetB
                                       CASIASetC
                                    
                                 
                                 
                                    
                                       32×32
                                       97.18
                                       29.44
                                       13.71
                                    
                                    
                                       64×64
                                       97.18
                                       31.45
                                       14.11
                                    
                                    
                                       128×128
                                       97.58
                                       31.85
                                       14.52
                                    
                                    
                                       240×240
                                       97.58
                                       31.85
                                       14.52
                                    
                                    
                                       
                                          
                                       
                                    
                                 
                              
                              
                                 
                                 
                                 
                                    
                                       (b) CCR on CASIA's test sets — Yu et al. [38].
                                    
                                    
                                       Yu et al. [38]
                                       
                                       CCR
                                    
                                 
                                 
                                    
                                       CASIASetA2
                                       97.6
                                    
                                    
                                       CASIASetB
                                       52.0
                                    
                                    
                                       CASIASetC
                                       32.7
                                    
                                 
                              
                           
                        
                     

First and foremost, we investigate the impact of image resizing. In fact, each pixel is considered to as a feature. As a consequence, starting with GEIs of low resolution will reduce the time required to apply BSS. Resizing was performed to maintain 256 shades of gray images. To measure the impact of the image size, we applied the template matching suggested by Yu et al. [38]. Four distinct sizes were considered. Results are reported in Table 1a. Let's recall that we used the set provided by [41], which is available on the official CASIA database website. Amazingly, the GEI signatures have been computed and stored in a manner that makes them more challenging than the way used in [38] (c.f. Table 1b). The TM CCRs seem to be relatively invariant to the resolution. We chose to evaluate our framework on 64×64 pixel GEI as the CCR starts to slightly decline for 32×32 pixel resolution. Each GEI actually constitutes a 4096 feature vector.

Feature selection aims to find the features that best describe one's gait signature independently of the variants. The selection method should not be overspecialized for a particular training set. As a result, the feature should be obtained from a small subset of sequences. In [41], 22 subjects were selected to apply the feature selection. 100 subjects were left out to perform the test. We deviate from using that approach, as we do not want to evaluate the impact of viewpoints. We aim at keeping all the subjects. As a consequence, we used a Monte-Carlo based approach. We randomly selected with replacement 20 subjects out of the 124 subjects for CASIA database. From these 20 subjects, we randomly selected 3 sequences, representing each one variant, for each selected subjects. As a result, we fed 60 sequences to a 200 tree random forest (RF). Let us recall that we use the RF as a way to obtain feature ranking. We then applied both BSS and FSS search strategies as we mentioned in Section 3. All selected sequences were removed from the test sets.

Several classification algorithms have been used to perform identification. In this part, we will evaluate how feature selection, and in a way, the mask found by the RF perform on CASIA test sets. We also need to evaluate how sensitive the CCR is to the number of features.

The first 200 most important features for both search strategies are shown in Fig. 2
                           .

First, the BSS's most important features are located in the head region as well as the bottom part of the silhouette. They are mainly located from rows 1 to 7 and 44 to 64. The lower part features actually correspond to the region from the knee to the feet. Most of the top 40 features are located in the frontal region.

Secondly, the FSS's most important features are more spread over the silhouette. They are also located in the head region. However, features can be found in the frontal region where the arm swings. Features are also found from the hip region to the ground. Let us recall that the BSS features were mainly located from the knee region to the ground. Important features are not as clearly grouped as with the BSS. We may say that they tend to be slightly ahead of the coronal plane.

Both feature selection methods seem to confirm that features belonging to static areas, according to [3] definition, can contribute to gait recognition. In fact, Fig. 3
                            and Table 2 provide an analysis of the most important features in the Gait Entropy Image. Let us recall that this signature is used to obtain the MGEI. The MGEI is formed by zeroing pixels having an entropy smaller than 0.75. As one can notice in Table 2, these pixels represent 46% of the selected features for the BSS and 44% for the FSS when the top 200 features are considered. It means that about 90 out of the 200 most important features of both search strategies are discarded by Bashir et al. method. We ran a similar analysis on the 100 most important features. Features that would have had been rejected by Bashir et al. method also represents 45% of the most important features for the BSS and 40% for the FSS. Even if the percentage moderately decreased, it is still significant. 17% and 11%, respectively, of the selected features are located in region where the entropy is null. There are located in the static part of the gait signature.

We want to evaluate how our mask improves the different recognition methods. In this paragraph, we will focus on the template matching approach as presented in [38]. As the GEIs used in [41] are more challenging, we computed the CCRs for the different test sets mentioned in Section 4.1. For the RF-based masks, we report the median CCR on the first 300 features as it can be seen in Fig. 4
                           .

When the BSS search strategy is considered, the CCR variations differ on the different sets. CCRx flattens from 150 of the most important features on CASIASetA2x. For CASIASetBx, CCRx keeps increasing while more features are added. CASIASetCx curve is non-monotone. In fact, the first 120 features seem to offer a trade-off that accommodates the three sets. The clothing variate set CCRx peaks at 70% and then decreases and oscillates about the median value. The CCR decreases as features from the chest region are selected. At the same time, the bag CCRx starts increasing again after stabilizing at about 43%. As we can notice in Table 3a, the proposed mask increases the average CCR. The gap is really significant on the co-variant sets. However, the CCR reported for the normal sequences drops significantly. Picking variants for each subject force the random forest to find the features robust to the different variants. Consequently, the proposed mask does not overfit the normal gait sequence. The CCR is higher for the clothing variants than on the bag variants. When no feature selection is applied, the CCR is better for the CASIASetB than CASIASetC as the overall signature is less different with bags than a different coat. However, biomechanical studies [34] reported that load carriage truly modifies the gait pattern. As result, the selected features focus on parts of the gait pattern that are related to biomechanical aspects and not only aiming to maximize CCR. The reported CCRs on the cross-validated sets are higher than the regular test sets.

When the FSS search strategy is considered, the CCRx flattens from 60 of the most important features on CASIASetA2x. The FSS outperforms the BSS CCR. For CASIASetBx and CASIASetCx, the curves are non-monotone. They respectively peak for 25 and 45 features. Then, they tend to decrease and converge toward a common CCR. Clearly, the FSS overfits the normal gait sequences as the gap is really significant with the covariants. CASIASetBx CCRx is close for both search strategies. However, CASIASetCx gap is not as significant as with BSS when FSS is considered. As a result, it is difficult to draw the same conclusion as with the BSS search. As it can be seen in Table 3a, both RF-produced masks improve the CCRx as compared to the state-of-the-art. BSS surpasses FSS when template matching is applied.

Canonical Discriminant Analysis (CDA) was popularized in Gait recognition by [20]. It actually corresponds to a principal component analysis (PCA) followed by a discriminant analysis (DA) performed on the resulting eigenspace. The full derivation can be found in [20]. As suggested by Han and Bhanu [17], we retained 2c eigenvectors after PCA is applied, where c is the number of classes. In CASIA's framework, it actually corresponds to keep the first 248 eigenvectors. As PCA is not supervised, it aims to find a set of uncorrelated variables. A linear DA is then performed. DA is a supervised method. It is used to optimize the class separability. In our experiments, we kept the first 248 eigenvectors while adding up to the 300 most important features. As it can be seen in Fig. 5
                           , the CCR is stable about the median value. The CCRs do not oscillate as much as when template matching is used for identification. It results that the first 248 important features can be used to obtain the best performance. As expected with the use of CDA, the CCRx increases and reaches 78% for the BSS and 61.37% for the FSS (c.f. Table 3c). The gap between our proposed mask and the conventional CDA is constant. Interestingly, CASIASetBx CCR is larger than CASIASetCx CCR. As we can see in Table 3d, the MDA step seems to favor CASIASetBx. In the FSS case, CASIASetCx CCRx even drops significantly. The MDA subspace is obtained from the training set, i.e. normal sequences. As a result, the CCR will be high for the variants that are alike the CASIASetAx in this subspace. Combined with CDA, the effect of overfitting resulting from FSS is emphasized. Consequently, the gap between the BSS average CCRx and FSS is even larger.

Bashir et al. [5] proposed the use of Mj
                           
                           +
                           ACDA in order to improve the CCRx on CASIA database. They introduced the Gait Entropy Image (GEnI), which aims at selecting covariant condition invariant features. The GEnI corresponds to a filter-based feature selection approach. Consequently, the purpose of this section, while presenting the performance of MDA, is also to compare our feature selection approach with the state-of-the art feature selection method applied to CASIA dataset B. Bashir et al. method is computationally expensive. As a result, we refrain from using it as we aim to propose a new mask. However, for the shape of comparison the performance of this approach is given in Table 3c. The mask Mj
                            actually improves the CCRx as expected. It actually reaches more than 62%. Let us remember that the CCR was only 48% with the use of TM only. Still, our mask combined with the CDA and BSS outperforms it on the average CCRx. The FSS is not far with 61%. We decided to experience one more approach. In fact, the MDA suffers only from the small sample problem when applied directly on raw data. As a result, the number of important features kept has to be smaller than the number of samples in the training set. As presented in Table 3d, the CCRx is maintained while it did not involve the use of PCA to achieve a better representation. It actually means that the RF achieved a good representation and chose features that are slightly correlated. As it can be seen in Fig. 6
                           , the phenomenon seen in Fig. 4 is emphasized.

Feature selection is used to reduce the computational cost while maintaining or even improving the performance of classification algorithms. As demonstrated in this section, the proposed mask improved the CCRs. We have not yet discussed the computational cost (CC). The measurements were performed using a unique thread on an Intel I7 2630QM processor. We compare the different methods used in our experiments. The CCs are reported in Table 4
                           . To measure the CC of our proposed mask, we retain 248 features, i.e. about 6% of the original feature space. As one can notice, our mask combined with CDA is 220 faster than Bashir et al. method. At 248 features, we report a CCR equal to 76.7% while Bashir et al. approach reaches 61.8%. As a result, our mask achieves the appropriate trade-off between the CCR and the CC. When used with TM, our proposed mask, while being 1000 faster than Mj
                           
                           +
                           ACDA, achieves a better average CCRx than Bashir et al.

In this part, we demonstrated that our mask enables us to improve the performance of the different classification approaches. Despite the set provided by [41] is more challenging on co-variants, the use of our mask can improve the CCR while simplifying the classification method used. The RF is used to rank the features. Combined with a BSS approach, it actually found the important features from only 60 sequences. The state-of-the-art approach proposed by Bashir et al. is computationally expensive and the mask we propose outperforms their mask. As it can be seen in Fig. 3, 45% of the features involved in our mask are discarded by Bashir et al. Our results suggest that the static part of the silhouette is useful for the recognition even on co-variants (Table 3b & c). As the CCRx behaviors differ when our mask is combined with the different classification methods, we propose to combine our mask with CDA. In fact, PCA brings a better stability of the CCR while keeping the CC low.

As we can see in Fig. 2, important features are mainly located in two disjoint parts of the body. As mentioned in Section 4.4.1, important features are mainly located between rows 1 to 7 and 44 to 64 in the GEI signature. As a result, for the shape of simplification, we propose to keep only these body parts in a simplified mask. Let us now investigate how this mask impacts the performance of the CDA approach.

This part also demonstrated that BSS performs better than FSS in the framework of this study. The statements of Doak et al. and Caruana et al. are true within the context of this paper.

The features selected by the RF led us to think that a simpler mask could be proposed. In fact, the features are located in two parts of the signature. We propose to keep these rows and study the performance achieved when using them only (c.f. Fig. 7
                        ). In the previous section, our experiments demonstrated that the performance is in the same order of magnitude for the conventional and the cross-validation test sets. As a result, from now on, we will perform the experiments under similar covariate conditions on the conventional test sets only.

In the previous part, we ran our experiments with the RF-based mask. Bashir et al. defined the conventional test framework as experiments with the gallery under similar covariate conditions [5]. The training set is exclusively made of normal gait sequences. The CCR on test sets CASIASetA2, CASIASetB and CASIASetC is respectively 98.79%, 77.82% and 92.74%. The average CCR increases by 12% to reach 89.78%. Let us remind that the features selected by the RF were obtained from 20 subjects. We can believe that the features obtained were sub-optimal. By using the margin as the way of ranking the features, we obtained appropriate CCRs (c.f. Section 4.4). However, our intuition suggested that the RF did highlight ROIs, as the selected features were mainly located in two disjoint regions.

Bashir et al. defined the notion of gait recognition without subject cooperation. The gallery should be composed of gait sequences under different and unknown covariate conditions. In this section, we propose to follow the experiments proposed by Bashir et al. In their paper, they propose to use a gallery made out of one third of each covariate. Contrary to the conventional test sets, no indication was given on how to pick each covariate and how many sequences are in the gallery and the probe. As a result, we chose to use a gallery, for a given subject, made of three sequences; each sequence being a different covariate. It actually fits with the recommendations of Bashir et al. to perform gait recognition without subject cooperation. The probe corresponds to the five remaining normal sequences (SetA3), one carrying covariate sequence (SetB2) and one clothing covariate sequence (SetC2). Consequently, we tested all possible scenarios. It results in a 24-fold cross-validation. We evaluate three techniques as Bashir et al.: GEI-CDA, Mj
                           
                           −
                           ACDA and the Masked GEI-CDA we introduced. As we resized the GEI and the CCR decreased in Section 4.4 as compared to the performance reported in the original papers, we also run our experiments on GEI-CDA and Mj
                           
                           −
                           ACDA. The algorithms are the same as the one used to produce the result in the previous part (c.f Table 3). Only the training and test set change. For the Masked GEI-CDA, we also investigated how the number of principal components (PC) affects the performance of the CDA. The influence of the number of principal component can be seen in Fig. 8
                           . The three probe sets seem to respond in a similar manner to the number of principal components selected. The maximum is reached when 60 PC are selected. The average CCR is 61.06%. Then the curves start decreasing. The results of the three experiments are presented in Table 5
                           . For GEI-CDA and Mj
                           
                           −
                           ACDA, the numbers given corresponds to the mean and standard deviation. For the Masked GEI-CDA, the performance correspond to the median value when the number of selected principal components ranges from 1 to 124. The standard deviation corresponds to the mean standard deviation for a given number of principal components.

The Mj
                           
                           −
                           ACDA performance decreases as compared to the performance reported by Bashir et al. The standard deviation is not large enough to explain such difference. We tested their approach with 128×88 MGEI. The performance increased only by 2%. As the algorithm gave performance in the same order of magnitude for conventional sets, the only reason that could explain such gap is the sensitivity of Bashir et al. approach to our testing framework. GEI-CDA gives a decent mean performance compared to Mj
                           
                           −
                           ACDA. However, the standard deviation over the different folds is really large. Consequently, the performance is too dependent of the gallery set. Finally, our proposed mask outperforms both conventional approaches. Moreover, the performance is stable over the folds. Consequently, our mask is a really good candidate for gait recognition without subject cooperation. The median CCR as well as the maximum CCR surpass the performance reported by Bashir et al. in their paper [5].

Contrary to previous experiments, we evaluate how the region of interest (ROI) found by the RF performs on the entire CASIA dataset B. First, we investigate the CCR obtained for all gallery and probe angles. Detailed results are presented in Table 10. We summarize the average CCR when the gallery angle and the probe angle are equal in Table 6
                           . Amazingly, the CCRs can be regarded as high. The average CCR now reaches 87.6% when all angles are considered. Our intuition seems to be right as proven by the increase for the CCR of the 90° view as well as acceptable average CCRs (Table 6). As a consequence, the ROI suggested by the RF is adapted for all viewpoints in CASIA database. Then, as one can see in either Table 6 or 10, the CCR achieved on the coat variants is larger than for the bag variants. As mentioned in Section 4.4.2, biomechanical studies confirms that gait is affected by load carriage. Contrary to the state of the art approach as well as our results in Sections 4.4.3 and 4.4.4, the proposed mask combined with CDA lead to results that makes biomechanical sense.

As we achieved a fair CCR for all views, we propose to recognize individuals without prior knowledge of the viewpoint. The approach pursued recently is known as View Transform Model (VTM) [23,41]. The aim is to be able extract the transformation that exists between the different viewpoints. Consequently, one would be able to recognize people from many angles in the probe with few viewpoints in the gallery. We deviated from this approach as we believe that this path does not fit, for now, with applied biometrics concerns. First of all, despite impressive results, the VTM approach does not enable to obtain a CCR that would be constant for all viewpoints. Biometrics are mainly used to protect highly sensitive and/or personal data. One cannot afford to have a reduced CCR only to slightly reduced the number of viewpoints in the gallery while one could reach a high CCR with all the viewpoints and covariates. Secondly, no work has been carried out to evaluate the performance of the VTM with the covariates in [23]. Consequently, there is no clue on how their approach would behave when covariates are in the probe. In [41], the covariants were investigated. However, the usual testing framework was not respected, i.e. one covariants condition were tested at the time, the gallery and probe being made of the same covariants. Finally, there is no indication on how to apply the VTM framework in situation where the subject viewpoint is unknown. There is no clue on how to choose from contradictory identities that may be given by the classifier. All things considered, we propose to investigate a simple but yet efficient framework commonly found in object detection theory (Fig. 9
                           ). As in any object detection or recognition task, we propose to train a pose estimator. Then once the pose is estimated, we use the sequences of CASIASetA1 with the corresponding viewpoint as the gallery. We propose to name this experiment panoramic gait recognition as the viewpoint varies about one degree of freedom. CASIA dataset B does not include views from the top or the ceiling for instance. The viewpoint is moving about a circle not a sphere. As approach is different from the state of the art, we cannot name this experiment multi-view gait recognition. The gallery requires all the viewpoints. The multi-view gait signature is then learned. The dimensionality of the resulting model is reduced using dimensionality reduction techniques. We refrained from using a parametric approach. Consequently, our non-parametric approach consists in keeping all the views and gait signatures as obtained after applying the RF-inspired mask. In fact, the most challenging task is to collect the gallery under all views. Parametric and non-parametric approaches both require all the views to proceed. Parametric approaches are often used because they provide a more compact way to represent the data and/or lead to better results. In our case, we already reduced the dimension of the data in the gallery set. The only requirement left is to verify if the pose estimation does not affect the CCR too excessively. In order to estimate the viewpoint, we trained a decision tree on the region we use for gait recognition, i.e. lines 1 to 7 and 44 to 64. We used the Gini diversity index as the split criterion. We applied pruning with error as the pruning criterion. The confusion matrix for the pose estimation is given in Table 7
                           . The average CCR is 94.43%. Considering we directly used the GEI, the pose estimation can be regarded as appropriate. Moreover, the CCR can be regarded as homogeneous throughout the view-angle as the standard deviation is only 1.40. Such performance induces that the identification performance should slightly drop.

Experiments when panoramic gait recognition is considered are given in Tables 8 and 9
                           
                           . As we can notice, the angle-wise CCR is still appropriate despite the view estimation error. On the one hand, CCRs for view angles from 18° to 128° are not affected by the view estimation. On the other hand, the average CCR for the three remaining angles is altered by the decrease of CASIASetC's CCR. We notice that our pose estimator performance decreases when considering this particular set as well as view angle. Nevertheless, when one compares Table 6 (before view estimation) and Table 9 (after view estimation), the performance of our approach can be regarded acceptable. We obtain a CCR for CASIA Dataset B of 85.6% while it was 87.6% when a prior knowledge of the view angle is considered.

We have not focused yet on the computational cost of our approach when the pose estimation and the identification step are considered. It takes 1.5ms to classify one signature. Within this period, the pose estimation takes 2μs. A close look at Table 4 reveals that it is the time required to perform the TM on the entire GEI when only one view angle is considered. While we use a classification that is more computational expensive, we match the fastest state-of-the-art method. The region identified by the RF provides a really good trade-off between identification performance and computational cost.

@&#DISCUSSIONS@&#

Firstly, the use of RF to obtain the important features from a small subset of sequences and individuals led us to obtain a really good correct classification rate while reporting a low computational cost. These results highlight the fact that RF is a powerful tool to find the most representative features. It also demonstrates that a lot of features from the original signature are useless and should be discarded to perform individual identification. Only 6% of the original feature space is required to outperformed state-of-the art methods.

Secondly, the covariate's CCRs increase significantly with the RF-based mask. The use of the RF-based mask brings the performance to levels that can be considered as decent to perform gait recognition without subject cooperation. Our experiments suggest that the gallery can only be made of normal sequences to obtain a high average correct classification rate.

Thirdly, contrary to Bashir et al. approach, our RF-based mask does not burden the recognition process. When the CDA is considered for identification, the subset selected divides the CC by 220 compared to Bashir et al. The CC is divided by 6 folds when compared with the original work of Han et al. Table 3. Our experiments outperform both approaches.

Fourthly, our mask enables to perform gait recognition without subject cooperation as defined by Bashir et al. Our mask outperformed the standard techniques while reducing the computational cost.

Finally, the RF-based mask brought our attention to two parts of the gait signature. In order to propose a simpler mask, we investigated how the view angles impact the CCR. We wanted to find out if both regions were specific of the 90° view. Our experiments suggest that both regions can be considered for panoramic gait recognition as not being view specific. In fact, the CCRs are maintained throughout the different view angles. This performance combined with the pose estimator CCR leads us to achieve a fair CCR on CASIA Dataset B (Table 10
                     ). These results were obtained from the raw GEI information. Our results demonstrate that gait recognition does not necessarily requires complex and computationally expensive signatures to obtain proper correct classification rate. Non-parametric approaches can be used to perform panoramic gait recognition. Moreover, the CCR achieved on the variants demonstrates that the selected region has a biomechanical meaning contrary to the existing approaches that only focus on the signature visual closeness.

@&#CONCLUSION@&#

We presented a feature selection method based on Random Forest applied to model free gait recognition. Our approach and the feature selection reduce the computational cost required to recognize a subject. The results demonstrate that the feature selection method enables us to reach a fair recognition rate for the three variants in CASIA Dataset B. We also show that the features, as present in the GEI signature, do not necessarily require a computationally expensive technique to improve their discriminative power. Our first experimental results lead to introduce a simple but really efficient mask that enabled us to obtain a CCR of 85.6% on the overall CASIA Dataset B. We wish to extend these results to USF Human ID dataset. We are conscious that the segmentation quality is not as good as for CASIA Dataset. As any model free approach, our performance depends on the segmentation quality. As a result, the first step would be to refine the silhouette segmentation with more advanced techniques, such as motion boundaries [30] for instance, than the one initially proposed. Thus, testing our mask on the USF Human ID dataset would enable us to evaluate the robustness of our mask to other types of co-variates: surface type, shoe type, time and viewpoints. As a consequence, further researches will be focused on this part of the gait-based human identification.

@&#ACKNOWLEDGMENTS@&#

This work is part of the NOmad Biometric Authentication (NOBA) project sponsored by the European Regional Development Fund (ERDF) under the Interreg IVA program (Ref. No. 4051) in collaboration with the University of Kent (UK).

@&#REFERENCES@&#

