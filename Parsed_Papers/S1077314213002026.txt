@&#MAIN-TITLE@&#Macrofeature layout selection for pedestrian localization and its acceleration using GPU

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose a macrofeature selection to improve object detection and localization.


                        
                        
                           
                           Our algorithm prioritizes more discriminative local macrofeature layouts.


                        
                        
                           
                           Our technique is integrated into the pedestrian detection algorithm by boosting.


                        
                        
                           
                           We accelerate the pedestrian detection algorithm using GPU.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Macrofeature selection

Object localization

Pedestrian detection

@&#ABSTRACT@&#


               
               
                  Macrofeatures are mid-level features that jointly encode a set of low-level features in a neighborhood. We propose a macrofeature layout selection technique to improve localization performance in an object detection task. Our method employs line, triangle, and pyramid layouts, which are composed of several local blocks represented by the Histograms of Oriented Gradients (HOGs) features in a multi-scale feature pyramid. Such macrofeature layouts are integrated into a boosting framework for object detection, where the best layout is selected to build a weak classifier in a greedy manner at each iteration. The proposed algorithm is applied to pedestrian detection and implemented using GPU. Our pedestrian detection algorithm performs better in terms of detection and localization accuracy with great efficiency when compared to several state-of-the-art techniques in public datasets.
               
            

@&#INTRODUCTION@&#

Object detection involves the localization (where) as well as the identification (what) of predefined objects such as face, pedestrian, vehicle and so on. Although various object detection algorithms have been proposed so far, the research in object detection has been focused mainly on the identification with loose requirement of localization; the bounding boxes of detected objects are sometimes poorly aligned. However, the localization of an object is also a very important issue in object detection since it may affect the performance of subsequent procedures significantly. For example, multiperson tracking-by-detection approaches [1,2] rely on the outputs from the object detector and tracking problem is formulated as a data association task across adjacent frames [2], where the accuracy of object localization affects tracking performance directly. Since localization quality in object detection is critical for many computer vision applications, it is worthwhile to investigate the object localization problem for the development of a robust object detector. Fig. 1
                      illustrates some examples of true positives with variations of alignment accuracy in pedestrian detection. In this paper, we propose a macrofeature layout selection algorithm in a boosting framework to improve localization performance, and apply our technique to pedestrian detection for validation.

Object detection has been widely studied in computer vision to identify various objects such as faces [4,5], pedestrians [4,6–18], and others [3,6,11,19]. Recently, pedestrian detection has received much attention and many algorithms have achieved successful results. Viola et al. [13] proposed an efficient pedestrian detection framework using a boosted cascade with simple and efficient Haar-like features. As a classifier, AdaBoost [20] was employed to select a number of discriminative features and their corresponding weak classifiers among a huge number of candidates. Dalal and Triggs [7] proposed a good feature for pedestrian detection, which is called the Histogram of Oriented Gradients (HOG), and published the INRIA dataset for pedestrian detection. The HOG feature is combined with other types of features successfully in object detection [8,10,14,15,18]. Walk et al. [14] employed a new feature based on self-similarity of low-level features and combined the new feature with several different features. Dollár et al. [10] introduced the Caltech Pedestrians dataset and benchmarked existing detection algorithms in images—not in windows—with several performance metrics [10,21]. Felzenszwalb et al. [19] proposed an algorithm based on deformable part models, which presents the state-of-the-art detection performance in broad object categories.

Finding good features is crucial in various computer vision problems. The combination of low-level features, which is also called mid-level features, has been widely studied in object detection and recognition community. Boureau et al. [22] presented a supervised learning method for mid-level feature extraction by sparse coding and tested it on object recognition benchmark datasets. Laptev [11] introduced boosted histograms that combine local histogram features by boosting and demonstrated competitive performance on the PASCAL VOC 2005 dataset [23]. The feature mining strategy was discussed in [4], where a pool of informative and complementary features is obtained from the huge feature space and the optimal feature set is selected by AdaBoost. They introduced generalized Haar-like features that are similar to the original ones [5] but allow arbitrary configurations and numbers of rectangles. Random configurations of heterogeneous low-level features were integrated into integral channel features [9], which improve pedestrian detection performance. Multi-scale generalizations of low-level features was introduced in [6], where the multi-scale features outperformed the best single-scale features in object detection.

We propose a macrofeature layout selection to improve object localization. Our macrofeature layout selection employs the feature layouts representing lines, triangles, and pyramids. The selected layouts are composed of several low-level feature blocks closely located to each other in a multi-scale feature pyramid. According to our observation, features with local high-order information such as curves and surfaces are more discriminative than features with the zero-order information such as points. As a feature selection strategy by boosting, our macrofeature layout selection is closely related to the integral channel features [9] and the boosted histograms [11]. We combine low-level features by boosting similar to [9,11] and our weak learner is same with [11]. However, our method extracts low-level features from neighborhood blocks rather than random ones without spatial constraints [9]. Also, it selects discriminative block layouts corresponding to geometric primitives such as lines, triangles, and pyramids instead of spatial grid blocks [11]. The pedestrian detection algorithm based on the selected layouts improves detection and localization performance in our experiments, particularly on Caltech and Daimler datasets which contain low resolution pedestrians.

The preliminary version of our work appeared in [24], and this paper contains the following additions and updates. First, we change the evaluation protocol of localization performance. Miss rates with respect to PASCAL overlap criteria were reported previously, but we now present average overlap ratios between detections and ground-truths given false positives per image to focus on localization performance. Second, we test performance of several types of layouts to verify the effectiveness of the proposed feature layouts and report the advantage of our layouts in Section 5.2. Third, our detection algorithm based on the selected local layouts is implemented using GPU and its computational efficiency is presented.

The rest of this paper is organized as follows. We describe our boosting framework for object detection in Section 2. Section 3 defines macrofeature layouts and present a macrofeature layout selection method by boosting. We present pedestrian detection algorithm based on the macrofeature layouts and its parallelization in GPU in Section 4. Section 5 illustrates the performance of our macrofeature layout selection strategy and demonstrates the pedestrian detection results in several challenging datasets.

An object detector is a binary classifier that estimates a class label 
                        
                           y
                           ∈
                           {
                           1
                           ,
                           -
                           1
                           }
                        
                     —whether the target object exists or not—given an observed feature vector 
                        
                           x
                        
                     . The classifier is learned from training data, which comprise a set of feature vectors 
                        
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                        
                      and their corresponding labels 
                        
                           
                              
                                 y
                              
                              
                                 i
                              
                           
                        
                     , i.e., 
                        
                           
                              
                                 {
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 }
                              
                              
                                 i
                                 =
                                 1
                                 ,
                                 …
                                 ,
                                 N
                              
                           
                        
                     , where N is the number of training examples.

The discrete AdaBoost algorithm [20] constructs a strong classifier 
                        
                           f
                           (
                           ·
                           )
                        
                      as a weighted sum of weak classifiers 
                        
                           
                              
                                 h
                              
                              
                                 t
                              
                           
                           (
                           ·
                           )
                        
                     , which is given by
                        
                           (1)
                           
                              f
                              (
                              x
                              )
                              =
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       t
                                       =
                                       1
                                    
                                    
                                       T
                                    
                                 
                              
                              
                                 
                                    α
                                 
                                 
                                    t
                                 
                              
                              
                                 
                                    h
                                 
                                 
                                    t
                                 
                              
                              (
                              x
                              )
                              ,
                           
                        
                     where 
                        
                           
                              
                                 α
                              
                              
                                 t
                              
                           
                        
                      is the weight for the weak classifier 
                        
                           
                              
                                 h
                              
                              
                                 t
                              
                           
                           (
                           x
                           )
                        
                      and T denotes the total number of weak classifiers. The weak classifiers are added in a greedy manner and the procedure at each iteration focuses on the data samples that have still been misclassified.

To build weak classifiers and select discriminative features simultaneously, we extract feature subvectors using a binary feature selection matrix, 
                        
                           
                              
                                 Φ
                              
                              
                                 
                                    
                                       B
                                    
                                    
                                       (
                                       j
                                       )
                                    
                                 
                              
                           
                        
                     , whose non-zero elements are parametrized by a feature layout 
                        
                           
                              
                                 B
                              
                              
                                 (
                                 j
                                 )
                              
                           
                           ∈
                           
                              
                                 {
                                 
                                    
                                       B
                                    
                                    
                                       (
                                       j
                                       )
                                    
                                 
                                 }
                              
                              
                                 j
                                 =
                                 1
                                 ,
                                 ⋯
                                 ,
                                 M
                              
                           
                        
                     . In other words, each feature subvector is extracted by the product of feature selection matrix 
                        
                           
                              
                                 Φ
                              
                              
                                 
                                    
                                       B
                                    
                                    
                                       (
                                       j
                                       )
                                    
                                 
                              
                           
                        
                      and feature vector 
                        
                           x
                           ,
                           
                              
                                 Φ
                              
                              
                                 
                                    
                                       B
                                    
                                    
                                       (
                                       j
                                       )
                                    
                                 
                              
                           
                           x
                        
                     , and a weak classifier is learned to minimize the weighted training error defined by
                        
                           (2)
                           
                              err
                              [
                              
                                 
                                    h
                                 
                                 
                                    (
                                    j
                                    )
                                 
                              
                              ]
                              
                              ≜
                              
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                    
                                 
                              
                              
                                 
                                    w
                                 
                                 
                                    i
                                 
                              
                              δ
                              
                                 
                                    
                                       
                                          
                                             y
                                          
                                          
                                             i
                                          
                                       
                                       
                                       ≠
                                       
                                       
                                          
                                             h
                                          
                                          
                                             (
                                             j
                                             )
                                          
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      Φ
                                                   
                                                   
                                                      
                                                         
                                                            B
                                                         
                                                         
                                                            (
                                                            j
                                                            )
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      i
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                              ,
                           
                        
                     where 
                        
                           w
                           =
                           
                              
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    
                                       w
                                    
                                    
                                       N
                                    
                                 
                                 )
                              
                              
                                 ⊤
                              
                           
                        
                      is a normalized weight vector of samples, and 
                        
                           δ
                           (
                           ·
                           )
                        
                      is the Kronecker delta function. The weak classifier with the minimum error and its corresponding feature subvector are selected in each iteration and the weight of the selected weak classifier is given by
                        
                           (3)
                           
                              
                                 
                                    α
                                 
                                 
                                    t
                                 
                              
                              =
                              ln
                              
                                 
                                    1
                                    -
                                    err
                                    [
                                    
                                       
                                          h
                                       
                                       
                                          t
                                       
                                    
                                    ]
                                 
                                 
                                    err
                                    [
                                    
                                       
                                          h
                                       
                                       
                                          t
                                       
                                    
                                    ]
                                 
                              
                              .
                           
                        
                     After a weak classifier is selected, the weights of data samples are updated as
                        
                           (4)
                           
                              
                                 
                                    w
                                 
                                 
                                    i
                                 
                              
                              ←
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      w
                                                   
                                                   
                                                      i
                                                   
                                                
                                             
                                             
                                                if
                                                
                                                
                                                   
                                                      y
                                                   
                                                   
                                                      i
                                                   
                                                
                                                =
                                                
                                                   
                                                      h
                                                   
                                                   
                                                      t
                                                   
                                                
                                                (
                                                
                                                   
                                                      Φ
                                                   
                                                   
                                                      
                                                         
                                                            B
                                                         
                                                         
                                                            t
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      i
                                                   
                                                
                                                )
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      w
                                                   
                                                   
                                                      i
                                                   
                                                
                                                exp
                                                (
                                                
                                                   
                                                      α
                                                   
                                                   
                                                      t
                                                   
                                                
                                                )
                                             
                                             
                                                if
                                                
                                                
                                                   
                                                      y
                                                   
                                                   
                                                      i
                                                   
                                                
                                                
                                                ≠
                                                
                                                
                                                   
                                                      h
                                                   
                                                   
                                                      t
                                                   
                                                
                                                (
                                                
                                                   
                                                      Φ
                                                   
                                                   
                                                      
                                                         
                                                            B
                                                         
                                                         
                                                            t
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      i
                                                   
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                              ,
                           
                        
                     and the weights of the samples are renormalized so that 
                        
                           
                              
                                 ∑
                              
                              
                                 i
                              
                           
                           
                              
                                 w
                              
                              
                                 i
                              
                           
                           =
                           1
                        
                     .

As a weak learner, we employ the Weighted Fisher Linear Discriminant (WFLD) [11], which is a variant of the Fisher Linear Discriminant (FLD) [25]. The WFLD finds the optimal linear projection to maximize weighted between-class scatter and minimize weighted within-class scatter at the same time. The weak classifier predicts the label of the data by thresholding a linear projection of a feature vector as
                        
                           (5)
                           
                              
                                 
                                    h
                                 
                                 
                                    (
                                    j
                                    )
                                 
                              
                              (
                              
                                 
                                    Φ
                                 
                                 
                                    
                                       
                                          B
                                       
                                       
                                          (
                                          j
                                          )
                                       
                                    
                                 
                              
                              x
                              )
                              
                              ≜
                              
                              
                                 
                                    
                                       
                                          
                                             
                                                1
                                             
                                             
                                                if
                                                
                                                
                                                   
                                                      
                                                         
                                                            p
                                                         
                                                         
                                                            (
                                                            j
                                                            )
                                                         
                                                      
                                                   
                                                   
                                                      ⊤
                                                   
                                                
                                                
                                                   
                                                      Φ
                                                   
                                                   
                                                      
                                                         
                                                            B
                                                         
                                                         
                                                            (
                                                            j
                                                            )
                                                         
                                                      
                                                   
                                                
                                                x
                                                ⩾
                                                
                                                   
                                                      θ
                                                   
                                                   
                                                      (
                                                      j
                                                      )
                                                   
                                                
                                             
                                          
                                          
                                             
                                                -
                                                1
                                             
                                             
                                                otherwise
                                             
                                          
                                       
                                    
                                 
                              
                              ,
                           
                        
                     where the threshold value 
                        
                           
                              
                                 θ
                              
                              
                                 (
                                 j
                                 )
                              
                           
                        
                      is obtained by minimizing the training error in Eq. (2). The optimal projection vector 
                        
                           
                              
                                 p
                              
                              
                                 (
                                 j
                                 )
                              
                           
                        
                      for a given feature layout 
                        
                           
                              
                                 B
                              
                              
                                 (
                                 j
                                 )
                              
                           
                        
                      is obtained by the WFLD in the boosting procedure as
                        
                           (6)
                           
                              
                                 
                                    p
                                 
                                 
                                    (
                                    j
                                    )
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          Σ
                                       
                                       
                                          (
                                          j
                                          )
                                       
                                    
                                 
                                 
                                    -
                                    1
                                 
                              
                              
                                 
                                    
                                       
                                          
                                             μ
                                          
                                          
                                             y
                                             =
                                             1
                                          
                                          
                                             (
                                             j
                                             )
                                          
                                       
                                       -
                                       
                                          
                                             μ
                                          
                                          
                                             y
                                             =
                                             -
                                             1
                                          
                                          
                                             (
                                             j
                                             )
                                          
                                       
                                    
                                 
                              
                              ,
                           
                        
                     where the class-conditional weighted mean vectors 
                        
                           
                              
                                 
                                    
                                       
                                          μ
                                       
                                       
                                          y
                                          =
                                          1
                                       
                                       
                                          (
                                          j
                                          )
                                       
                                    
                                    ,
                                    
                                       
                                          μ
                                       
                                       
                                          y
                                          =
                                          -
                                          1
                                       
                                       
                                          (
                                          j
                                          )
                                       
                                    
                                 
                              
                           
                        
                      and covariance matrix 
                        
                           
                              
                                 Σ
                              
                              
                                 (
                                 j
                                 )
                              
                           
                        
                      are computed as described in [11].

The overall training procedure is summarized in Algorithm 1. The next section discusses how the feature selection matrix 
                        
                           
                              
                                 Φ
                              
                              
                                 {
                              
                           
                           
                              
                                 B
                              
                              
                                 (
                                 j
                                 )
                              
                           
                           }
                        
                      and the feature layouts 
                        
                           {
                           
                              
                                 B
                              
                              
                                 (
                                 j
                                 )
                              
                           
                           }
                        
                      are defined to construct a macrofeature.
                        Algorithm 1
                        Training algorithm by the discrete AdaBoost [20] 
                           
                              
                                 
                                    
                                    
                                       
                                          1: Input 
                                             
                                                
                                                   
                                                      
                                                         {
                                                         (
                                                         
                                                            
                                                               x
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                         ,
                                                         
                                                            
                                                               y
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                         )
                                                         }
                                                      
                                                      
                                                         i
                                                         =
                                                         1
                                                         …
                                                         N
                                                      
                                                   
                                                
                                              and 
                                                
                                                   
                                                      
                                                         {
                                                         
                                                            
                                                               B
                                                            
                                                            
                                                               (
                                                               j
                                                               )
                                                            
                                                         
                                                         }
                                                      
                                                      
                                                         j
                                                         =
                                                         1
                                                         …
                                                         M
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          2: Initialization 
                                             
                                                
                                                   
                                                      
                                                         w
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   ←
                                                   
                                                      
                                                         1
                                                      
                                                      
                                                         N
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          3: while 
                                             
                                                
                                                   t
                                                   <
                                                   T
                                                
                                              
                                             do
                                          
                                       
                                       
                                          4: 
                                             for 
                                             j 
                                             do
                                          
                                       
                                       
                                          5: 
                                              
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               h
                                                            
                                                            
                                                               ˆ
                                                            
                                                         
                                                      
                                                      
                                                         (
                                                         j
                                                         )
                                                      
                                                   
                                                   =
                                                   arg
                                                   
                                                      
                                                         min
                                                      
                                                      
                                                         
                                                            
                                                               h
                                                            
                                                            
                                                               (
                                                               j
                                                               )
                                                            
                                                         
                                                      
                                                   
                                                   err
                                                   [
                                                   
                                                      
                                                         h
                                                      
                                                      
                                                         (
                                                         j
                                                         )
                                                      
                                                   
                                                   ]
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                             
                                              where 
                                                
                                                   err
                                                   [
                                                   
                                                      
                                                         h
                                                      
                                                      
                                                         (
                                                         j
                                                         )
                                                      
                                                   
                                                   ]
                                                   
                                                   ≜
                                                   
                                                   
                                                      
                                                         ∑
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   
                                                      
                                                         w
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   δ
                                                   (
                                                   
                                                      
                                                         y
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   
                                                   ≠
                                                   
                                                   
                                                      
                                                         h
                                                      
                                                      
                                                         (
                                                         j
                                                         )
                                                      
                                                   
                                                   (
                                                   
                                                      
                                                         Φ
                                                      
                                                      
                                                         
                                                            
                                                               B
                                                            
                                                            
                                                               (
                                                               j
                                                               )
                                                            
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   )
                                                   )
                                                
                                             .
                                       
                                       
                                          6:  
                                             end for
                                          
                                       
                                       
                                          7:  
                                             
                                                
                                                   
                                                      
                                                         j
                                                      
                                                      
                                                         ˆ
                                                      
                                                   
                                                   =
                                                   arg
                                                   
                                                      
                                                         min
                                                      
                                                      
                                                         j
                                                      
                                                   
                                                   err
                                                   [
                                                   
                                                      
                                                         
                                                            
                                                               h
                                                            
                                                            
                                                               ˆ
                                                            
                                                         
                                                      
                                                      
                                                         (
                                                         j
                                                         )
                                                      
                                                   
                                                   ]
                                                
                                             .
                                       
                                       
                                          8:  
                                             
                                                
                                                   
                                                      
                                                         h
                                                      
                                                      
                                                         t
                                                      
                                                   
                                                   ←
                                                   
                                                      
                                                         
                                                            
                                                               h
                                                            
                                                            
                                                               ˆ
                                                            
                                                         
                                                      
                                                      
                                                         (
                                                         
                                                            
                                                               j
                                                            
                                                            
                                                               ˆ
                                                            
                                                         
                                                         )
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          9:  
                                             
                                                
                                                   
                                                      
                                                         B
                                                      
                                                      
                                                         t
                                                      
                                                   
                                                   ←
                                                   
                                                      
                                                         B
                                                      
                                                      
                                                         (
                                                         
                                                            
                                                               j
                                                            
                                                            
                                                               ˆ
                                                            
                                                         
                                                         )
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          10: 
                                             
                                                
                                                   
                                                      
                                                         α
                                                      
                                                      
                                                         t
                                                      
                                                   
                                                   =
                                                   ln
                                                   
                                                      
                                                         1
                                                         -
                                                         
                                                            
                                                               
                                                                  
                                                                     err
                                                                  
                                                                  
                                                                     ^
                                                                  
                                                               
                                                            
                                                            
                                                               t
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     err
                                                                  
                                                                  
                                                                     ^
                                                                  
                                                               
                                                            
                                                            
                                                               t
                                                            
                                                         
                                                      
                                                   
                                                
                                              where 
                                                
                                                   
                                                      
                                                         
                                                            
                                                               err
                                                            
                                                            
                                                               ^
                                                            
                                                         
                                                      
                                                      
                                                         t
                                                      
                                                   
                                                   
                                                   ≜
                                                   
                                                   err
                                                   [
                                                   
                                                      
                                                         h
                                                      
                                                      
                                                         t
                                                      
                                                   
                                                   ]
                                                
                                             .
                                       
                                       
                                          11: 
                                             
                                                
                                                   
                                                      
                                                         w
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   ←
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        
                                                                           w
                                                                        
                                                                        
                                                                           i
                                                                        
                                                                     
                                                                  
                                                                  
                                                                     if
                                                                     
                                                                     
                                                                        
                                                                           y
                                                                        
                                                                        
                                                                           i
                                                                        
                                                                     
                                                                     =
                                                                     
                                                                        
                                                                           h
                                                                        
                                                                        
                                                                           t
                                                                        
                                                                     
                                                                     (
                                                                     
                                                                        
                                                                           Φ
                                                                        
                                                                        
                                                                           
                                                                              
                                                                                 B
                                                                              
                                                                              
                                                                                 t
                                                                              
                                                                           
                                                                        
                                                                     
                                                                     
                                                                        
                                                                           x
                                                                        
                                                                        
                                                                           i
                                                                        
                                                                     
                                                                     )
                                                                  
                                                               
                                                               
                                                                  
                                                                     
                                                                        
                                                                           w
                                                                        
                                                                        
                                                                           i
                                                                        
                                                                     
                                                                     exp
                                                                     (
                                                                     
                                                                        
                                                                           α
                                                                        
                                                                        
                                                                           t
                                                                        
                                                                     
                                                                     )
                                                                  
                                                                  
                                                                     if
                                                                     
                                                                     
                                                                        
                                                                           y
                                                                        
                                                                        
                                                                           i
                                                                        
                                                                     
                                                                     
                                                                     ≠
                                                                     
                                                                     
                                                                        
                                                                           h
                                                                        
                                                                        
                                                                           t
                                                                        
                                                                     
                                                                     (
                                                                     
                                                                        
                                                                           Φ
                                                                        
                                                                        
                                                                           
                                                                              
                                                                                 B
                                                                              
                                                                              
                                                                                 t
                                                                              
                                                                           
                                                                        
                                                                     
                                                                     
                                                                        
                                                                           x
                                                                        
                                                                        
                                                                           i
                                                                        
                                                                     
                                                                     )
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                   .
                                                
                                             
                                          
                                       
                                       
                                          12:  Normalize 
                                                
                                                   
                                                      
                                                         w
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                
                                              so that 
                                                
                                                   
                                                      
                                                         ∑
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   
                                                      
                                                         w
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   =
                                                   1
                                                
                                             .
                                       
                                       
                                          13: end while
                                          
                                       
                                       
                                          14: Output 
                                             
                                                
                                                   f
                                                   (
                                                   x
                                                   )
                                                   =
                                                   
                                                      
                                                         ∑
                                                      
                                                      
                                                         t
                                                         =
                                                         1
                                                      
                                                      
                                                         T
                                                      
                                                   
                                                   
                                                      
                                                         α
                                                      
                                                      
                                                         t
                                                      
                                                   
                                                   
                                                      
                                                         h
                                                      
                                                      
                                                         t
                                                      
                                                   
                                                   (
                                                   
                                                      
                                                         Φ
                                                      
                                                      
                                                         
                                                            
                                                               B
                                                            
                                                            
                                                               t
                                                            
                                                         
                                                      
                                                   
                                                   x
                                                   )
                                                
                                             .
                                       
                                    
                                 
                              
                           
                        

Macrofeatures [22] are mid-level features that jointly encode a set of low-level features in a spatial neighborhood. We model the procedure of low-level feature extraction using a multivariate vector-valued function, and refer to it as feature vector field. Let 
                        
                           U
                           ⊆
                           
                              
                                 R
                              
                              
                                 d
                              
                           
                        
                      be a d-dimensional feature vector space and 
                        
                           Z
                        
                      be a state space in which a state represents the location and size of a block within a detection candidate window. A low-level feature vector 
                        
                           
                              
                                 u
                              
                              
                                 z
                              
                           
                        
                     , extracted from a block at 
                        
                           z
                        
                     , is given by a multivariate vector-valued function 
                        
                           g
                           (
                           ·
                           )
                        
                      as
                        
                           (7)
                           
                              
                                 
                                    u
                                 
                                 
                                    z
                                 
                              
                              =
                              g
                              (
                              z
                              ;
                              W
                              )
                              ,
                              
                              
                                 
                                    u
                                 
                                 
                                    z
                                 
                              
                              ∈
                              U
                              
                              for
                              
                              z
                              ∈
                              Z
                              ,
                           
                        
                     where W denotes a detection candidate window.

We model the complex shape of an object by a collection of simple geometric primitives observed in a feature vector field. In other words, a macrofeature is constructed by combining low-level feature vectors obtained from neighborhood blocks, which form a line, triangle, or pyramid. Fig. 2
                      illustrates how macrofeature vectors are obtained from a candidate window. Compared to a simple low-level feature vector, which ignores the spatial arrangement of features, a macrofeature is expected to be more discriminative since it can capture high-order information from multiple low-level features. We also expect that the macrofeatures are more reliable than the holistic feature vectors as in [7] when deformation or partial occlusion occurs.

Denote the location and size of the lth block inside a candidate window by 
                        
                           
                              
                                 z
                              
                              
                                 l
                              
                           
                        
                      for 
                        
                           l
                           ∈
                           I
                           =
                           {
                           1
                           ,
                           …
                           ,
                           K
                           }
                        
                     , where K is the total number of the blocks. A set of block indices 
                        
                           B
                           =
                           {
                           
                              
                                 l
                              
                              
                                 1
                              
                           
                           ,
                           ⋯
                           ,
                           
                              
                                 l
                              
                              
                                 k
                              
                           
                           }
                           ⊆
                           I
                        
                      defines a layout based on the k blocks, which means the geometric configuration of a macrofeature as shown in Fig. 2d. Let 
                        
                           x
                        
                      be the concatenation of all low-level feature vectors as 
                        
                           x
                           =
                           
                              
                                 [
                                 
                                    
                                       u
                                    
                                    
                                       
                                          
                                             z
                                          
                                          
                                             1
                                          
                                       
                                    
                                    
                                       ⊤
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    
                                       u
                                    
                                    
                                       
                                          
                                             z
                                          
                                          
                                             K
                                          
                                       
                                    
                                    
                                       ⊤
                                    
                                 
                                 ]
                              
                              
                                 ⊤
                              
                           
                        
                     , which comes from a given candidate window. The binary feature selection matrix 
                        
                           
                              
                                 Φ
                              
                              
                                 B
                              
                           
                        
                      extracts a subvector of 
                        
                           x
                        
                      corresponding to the layout B. Non-zero elements in 
                        
                           
                              
                                 Φ
                              
                              
                                 B
                              
                           
                        
                      are given by the layout 
                        
                           B
                           =
                           {
                           
                              
                                 l
                              
                              
                                 1
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 l
                              
                              
                                 k
                              
                           
                           }
                        
                      as
                        
                           (8)
                           
                              
                                 
                                    Φ
                                 
                                 
                                    B
                                 
                              
                              
                              ≜
                              
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      Φ
                                                   
                                                   
                                                      11
                                                   
                                                
                                             
                                             
                                                ⋯
                                             
                                             
                                                
                                                   
                                                      Φ
                                                   
                                                   
                                                      1
                                                      K
                                                   
                                                
                                             
                                          
                                          
                                             
                                                ⋮
                                             
                                             
                                                ⋱
                                             
                                             
                                                ⋮
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      Φ
                                                   
                                                   
                                                      k
                                                      1
                                                   
                                                
                                             
                                             
                                                ⋯
                                             
                                             
                                                
                                                   
                                                      Φ
                                                   
                                                   
                                                      kK
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                              
                              where
                              
                              
                                 
                                    Φ
                                 
                                 
                                    mn
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      I
                                                   
                                                   
                                                      d
                                                      ×
                                                      d
                                                   
                                                
                                             
                                             
                                                if
                                                
                                                
                                                   
                                                      l
                                                   
                                                   
                                                      m
                                                   
                                                
                                                =
                                                n
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      0
                                                   
                                                   
                                                      d
                                                      ×
                                                      d
                                                   
                                                
                                             
                                             
                                                otherwise
                                             
                                          
                                       
                                    
                                 
                              
                              ,
                           
                        
                     where 
                        
                           I
                        
                      and 
                        
                           0
                        
                      are 
                        
                           d
                           ×
                           d
                        
                      identity and zero matrix, respectively. Given a layout B, a macrofeature 
                        
                           
                              
                                 x
                              
                              
                                 B
                              
                           
                        
                      is obtained by the concatenation of low-level feature vectors computed from all the blocks in B as
                        
                           (9)
                           
                              
                                 
                                    Φ
                                 
                                 
                                    B
                                 
                              
                              x
                              
                              ≜
                              
                              
                                 
                                    x
                                 
                                 
                                    B
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      u
                                                   
                                                   
                                                      
                                                         
                                                            z
                                                         
                                                         
                                                            
                                                               
                                                                  l
                                                               
                                                               
                                                                  1
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                ⋮
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      u
                                                   
                                                   
                                                      
                                                         
                                                            z
                                                         
                                                         
                                                            
                                                               
                                                                  l
                                                               
                                                               
                                                                  k
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                              .
                           
                        
                     After the integration of the macrofeature extraction matrix 
                        
                           
                              
                                 Φ
                              
                              
                                 
                                    
                                       B
                                    
                                    
                                       t
                                    
                                 
                              
                           
                        
                      given the macrofeature layout 
                        
                           
                              
                                 B
                              
                              
                                 t
                              
                           
                        
                      selected for the weak classifier 
                        
                           
                              
                                 h
                              
                              
                                 t
                              
                           
                           (
                           ·
                           )
                        
                     , Eq. (1) can be rewritten as
                        
                           (10)
                           
                              h
                              (
                              X
                              )
                              =
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       t
                                       =
                                       1
                                    
                                    
                                       T
                                    
                                 
                              
                              
                                 
                                    α
                                 
                                 
                                    t
                                 
                              
                              
                                 
                                    h
                                 
                                 
                                    t
                                 
                              
                              (
                              
                                 
                                    Φ
                                 
                                 
                                    
                                       
                                          B
                                       
                                       
                                          t
                                       
                                    
                                 
                              
                              x
                              )
                              .
                           
                        
                     The macrofeature layout selection is employed in a boosting framework for pedestrian detection with HOG [7] as low-level feature. The macrofeature layouts are employed to build weak classifiers in the boosting procedure; the weak classifier with the lowest training error and its corresponding layout are selected at each iteration. Fig. 3
                      illustrates a sample pedestrian detector learned by the boosting procedure.

We tested the performance of several layout variations in terms of efficiency in training and accuracy in testing to avoid using all possible layouts and identify a subset of local layouts as selection candidates for boosting. The layouts for this evaluation are constructed by varying the number of blocks, and the spatial variations of block centers are modeled by Gaussian distribution with several different standard deviation values. For comparison, the per-window evaluation protocol [7] was employed
                        1
                        Ideally, per-image evaluation protocol is more desirable, but we employed per-window evaluation protocol to reduce search space, which is extremely large in per-image protocol, and remove selection bias from non-maximal suppression as discussed in [9].
                     
                     
                        1
                      and Area Under Curve (AUC) [26] values against the number of weak classifiers are presented.


                     Fig. 4
                      summarizes the performance of layout variations. Fig. 4a shows that increasing the number of blocks in spatial domain improves the detection performance but it is saturated quickly when more than three blocks are used. On the other hand, the detection performance is not improved any more after adding three blocks in multiple scales, which is presented in Fig. 4b. The level of spatial proximity in the multi-location layouts based on three blocks affects performance marginally, but the performance is best at 
                        
                           σ
                           =
                           6
                        
                      (corresponding to approximately 
                        
                           50
                           %
                        
                      overlap between blocks) as illustrated in Fig. 4c.

As illustrated in Table 1
                     , we choose three geometric primitives—line, triangle, and pyramid—as layout shapes and some of layout parameters such as the number of blocks and the space between blocks are determined based on the above results. The chosen layouts involve three blocks that are closely located to each other in the feature pyramid; we call such layouts as local layouts. Our local layouts represent lines in four directions, triangles in eight different orientations, and a pyramid. The layouts are designed to capture local structural information by varying either location or scale up to the second-order. The use of local layouts can reduce computational complexity in training and testing because we need to evaluate only local layouts and load relatively small amount of features due to spatial proximity of blocks inside the layout.

CUDA™ is a GPU-based parallel computing platform and programming model introduced by NVIDIA®. It has been successfully applied to various problems in computer vision including object detection [27–29] for accelerating algorithms through parallelization. The CUDA architecture is composed of several streaming multiprocessors sharing a large amount of memory called global memory. Each multiprocessor is a group of CUDA cores and has a limited amount of extremely fast memory, which is shared by its own cores and is called shared memory. For a GTX580 CUDA device used in our implementation, the global memory is 32K times larger, but the shared memory is 100 times faster.

A CUDA program comprises a series of code segments. Each segment loads input variables from the global memory, computes them on the shared memory, and stores the results back to the global memory. The code segments are operated in parallel by running multiple threads within multiple CUDA cores. To maximize the performance, an algorithm needs to properly manage the limited resources in CUDA, especially the shared memory.

We parallelize our pedestrian detection algorithm using CUDA, which is particularly slow in the computation of feature vectors and detection responses in CPU implementation. These two steps may be very inefficient for parallel processing without sophisticated implementation due to large amount of memory access. We present a technique that makes our pedestrian detection algorithm much faster by reducing memory requirement significantly.

Pedestrian detection based on the standard HOG, which is implemented using CPU in OpenCV library [30], is computationally expensive. Recently, several efficient HOG implementations using CUDA have been presented in [30,27–29] but most of them are based on the exact implementation of the standard HOG. We modify the standard HOG, and present our HOG implementation compared to the standard HOG in Fig. 5
                        .

A HOG-based low-level feature vector is represented by an orientation histogram of gradients, which are computed from each block. The standard HOG uses a spatial weighting within a block for anti-aliasing the histograms and employs the L2-Hys method [7,31] for normalizing local contrast in the block. More precisely, when calculating histograms, two-level spatial weighting is applied to each block and its 
                           
                              2
                              ×
                              2
                           
                         sub-blocks (cells): Gaussian weighting for blocks and bilinear weighting for cells. Then, the histogram for each block is normalized by the L2-Hys method, which consists of normalization, clipping, and renormalization steps. The clipping reduces the influence of large gradients, which often occur due to camera saturation or illumination changes [31]. The L2-Hys normalization is useful to achieve reliable performance of pedestrian detection [7].

Since the standard HOG uses the block-wise weighting and normalization, a cell is loaded to memory multiple times because it is associated with multiple partially overlapped blocks; a lot of redundant information may reside in memory. We discard some block-wise operations to save memory and improve speed significantly as long as they hardly deteriorate the detection performance. Specifically, the Gaussian weighting for each block is removed and we need to load a cell to memory only once now. Also, we propose an approximate version of the L2-Hys normalization, denoted by L2-Hys∗, which consists of cell-wise normalization and clipping followed by block-wise renormalization. For the efficiency in the block-wise renormalization, we store the squared L2 norm for each cell and normalize the response of a weak classifier instead of the feature vector. Hence, the dimension of a feature vector for a cell is reduced to the number of orientation bins plus 1. A similar dimensionality reduction strategy was discussed in [19] and it requires an additional feature projection step after block-wise normalization and clipping to obtain 13 dimensional features for each cell. Its detection time was approximately 30% slower than ours when both features were implemented with our detector using CUDA.

Our strategy is based on the observation, presented in Table 2
                        , that the block-wise Gaussian weighting has large memory requirement but affects detection accuracy only marginally and that we can achieve almost equivalent performance with significantly less memory by the slight modification of the L2-Hys normalization. Note that the bilinear weighting for each cell hardly affects the speed of the overall algorithm. Hence, we do not change the cell weighting method.

Our detection algorithm with sliding windows involves a sequence of convolutions
                           2
                           The response of each local layout throughout the image can be computed efficiently by convolution.
                        
                        
                           2
                         with projection coefficients corresponding to weak classifiers learned by boosting. Approximately, one thousand convolution kernels—projection coefficients—are applied to the HOG features at every location and scale in an image using millions of threads. The HOG features are stored in the shared memory and used repeatedly. The convolution kernels are stored in the global memory, but typically loaded from L1 and L2 caches. Each convolution is computed in parallel by multiply–add operations on the HOG features and coefficients, then the convolution results are accumulated concurrently by atomic add operations on global memory.

Our macrofeature layout selection algorithm was tested in pedestrian detection with several challenging datasets as summarized in Table 3
                     —INRIA [7], ETH [32], TUD-Brussels [33], Caltech [10], and Daimler [34] datasets—using the evaluation code provided by [21] with a few modifications to test localization accuracy additionally.

Our experiments are organized as follows. First, we tested the performance of various macrofeature layouts to support our layout selection (Section 5.2). Second, we analyzed the properties of the local layouts selected by our boosting classifier (Section 5.3). Third, the pedestrian detection by our local macrofeature layout selection was compared with other pedestrian detection algorithms (Section 5.4). Last, the performances of our pedestrian detection algorithms based on CPU and CUDA were compared (Section 5.5). For all of our experiments, INRIA training dataset was used for training. For the particular experiment presented in Fig. 11, Caltech training dataset (set00-05) was used for training.

We employ the per-image evaluation protocol [21] to evaluate detection and localization performance at the same time by analyzing the following two aspects.
                           
                              •
                              Detection performance is tested by the relationship between detection rate and the number of false positives per image (FPPI) [21]. Higher detection rate at the same FPPI means better performance. The detection rate is obtained using PASCAL VOC [3] overlap criterion 
                                    
                                       0.5
                                    
                                  unless otherwise noted.

Localization performance is measured by the mean of PASCAL VOC [3] overlap ratios given FPPI. Higher mean overlap ratio at the same FPPI indicates better alignment to ground truth annotation.

The mean value of overlap ratios needs to be computed with careful consideration of a trade-off between detection and localization. The localization performance of an algorithm with low detection rate is often very good since easy-to-detect examples tend to be easy-to-localize and a bad algorithm typically detects only easy ones. Therefore, we need to consider detection rate at the same time when comparing localization accuracies. We compute the average overlap ratio between ground-truths and detections for the examples that are detected successfully by at least one algorithm at a certain FPPI value and report the average overlap ratio and FPPI value together.

We analyze the detection and localization performance of various macrofeature layout selection (MLS) methods in pedestrian detection. The local layouts presented in Table 1 are compared with other options listed below. For all of multi-scale layouts, we used 
                           
                              8
                              ×
                              8
                              ,
                              16
                              ×
                              16
                           
                        , and 
                           
                              32
                              ×
                              32
                           
                         blocks.
                           
                              •
                              Single-location multi-scale (SLMS) layouts are based on individual blocks in various sizes within a detection candidate window.

Multi-location single-scale (MLSS) layouts are constructed by uniform sampling of three blocks with the same size of 
                                    
                                       16
                                       ×
                                       16
                                    
                                  in each layout.

Multi-location multi-scale (MLMS) layouts are similar to the MLSS layouts but blocks in each layout have various sizes.

Spatial grid (SG) layouts presented in [11] are based on one, two, and four blocks
                                    3
                                    In [11], arbitrary block sizes were used by means of integral images but we tested only 
                                          
                                             8
                                             ×
                                             8
                                             ,
                                             16
                                             ×
                                             16
                                          
                                       , and 
                                          
                                             32
                                             ×
                                             32
                                          
                                        blocks as in other layouts.
                                 
                                 
                                    3
                                  which have no overlap among intra-layout blocks. Contrary to the MLMS layouts, all the blocks within each layout have the same size but block sizes in different layouts may not be same.

Spatial grid layouts with overlapping blocks (SG-Overlap) are identical to SG, but have 50% overlap between intra-layout blocks.

Since these layout options have different model complexities, we continue adding weak classifiers to obtain a strong classifier until a margin constraint between positive and negative samples is satisfied, as presented in [12], instead of using the same number of weak classifiers. The training was started with random negatives and repeated by discarding easy negatives and bootstrapping hard ones until we could not obtain a sufficient number of hard negatives any more. We aggregated the weak classifiers from every bootstrapping round for each layout variation. The numbers of weak classifiers based on SLMS, MLSS, MLMS, SG, SG-Overlap, and our local layouts are 4762, 1930, 781, 948, 730, and 969, respectively, and the corresponding numbers of feature blocks are 4762, 5790, 2343, 3566, 2916, and 2907, respectively. The number of candidate layouts in each experiment was approximately 1000.


                        Fig. 6
                         illustrates that the proposed algorithm based on the local layouts is better than or comparable to other layouts. According to the observations, using either multi-scale or multi-location layouts (SLMS or MLSS) improves detection and localization performance compared to holistic modeling (HOG) [7]. The performance is even more improved by using our local layouts or MLMS. The SG does not improve the performance as much as our layouts, even though it utilizes 23% more feature blocks than ours. The SG-Overlap reduces the number of feature blocks with marginal performance improvement over SG. In summary, the integration of multiple scales in intra-layout blocks improves performance and overlaps among intra-layout blocks tend to reduce the number of weak classifiers with comparable performance. The performance of MLMS, a generalized version of our layout, is almost equivalent to ours, which decouples multi-location and multi-scale for layout selection. Considering the fact that the training time is directly related to the number of layout candidates, the layout selection based on our local layouts should be significantly more efficient than MLMS.

We investigate the local layouts that are selected in training our pedestrian detector by boosting. Fig. 7
                         illustrates the cumulative selection rate of each layout type, where the relative occurrences (Fig. 7a) and the weighted relative occurrences (Fig. 7b) at each iteration of boosting are presented.
                           4
                           The weight of a layout means the weight of the corresponding weak classifier.
                        
                        
                           4
                         At early iterations, the selection rates of line and triangle layouts are high, but after the first several iterations, the selection rate of the pyramid layouts becomes dominant. The line and triangle layouts are more discriminative in only a coarse level, but the pyramid layouts capture fine structural information that better differentiates pedestrians from hard non-pedestrian examples.

The spatial densities of the selected local feature layouts are presented in Fig. 8
                        . The line and triangle are complementary; they have strong responses near head and torso, respectively. The pyramid layouts capture various structures of human body such as head, shoulders, torso and feet.

Our algorithm based on the standard HOG implementation
                           5
                           Our detection algorithm in CPU employs the standard HOG while we modified the HOG as described in Section 4.1 for GPU implementation.
                        
                        
                           5
                         
                        [30] was evaluated and compared with six state-of-the-art algorithms—LatSvm-V2 [19], HOG [7], ChnFtrs [9], FPDW [17], Pls [18], and MultiFtr+CSS [14]. The results of the state-of-the-art algorithms were obtained from the Caltech Pedestrian Detection Benchmark website [35]. Table 4
                         summarizes the characteristics of the algorithms. All algorithms except HOG, LatSvm-V2, and ours are based on multiple heterogeneous features while our method is based only on HOG features as HOG and LatSvm-V2. Note that ChnFtrs, FPDW, and Pls are not straightforward to be applied to the dataset with gray-scale images, e.g., the Daimler dataset.


                        Figs. 9 and 10
                        
                         present evaluation results in pedestrian detection and localization on several public datasets. The results suggest that our algorithm is better than or comparable to other techniques in terms of detection rates. When low-resolution datasets were tested, our algorithm showed better localization performance in terms of mean overlap ratio, particularly in the Caltech and Daimler datasets. Table 5
                         summarizes the detection performance in terms of log-average miss rates [21] with overlap criteria 
                           
                              0.5
                           
                         and 
                           
                              0.7
                           
                         on the same datasets. Our algorithm has fairly good performance compared to other techniques, particularly with the tighter overlap criterion. The ranks of our algorithm with the overlap criterion 
                           
                              0.7
                           
                         are presented in Table 6
                        .

Note that LatSvm-V2 has good localization performance in high-resolution images as the ones in the INRIA dataset since it employs a part-based detection and manages various deformations effectively. However, the performance of LatSvm-V2 degraded when it was trained on high resolution images and tested on low resolution ones. For a fair comparison, we trained LatSvm-V5 [36] and our detector on Caltech training dataset (set00-05), which contains low resolution pedestrians. Fig. 11
                         illustrates evaluation results on Caltech testing dataset (set06-10) when the two different training datasets were used. When the Caltech training dataset was used instead of the INRIA training dataset, detection performance was increased by at least 7.9% points at 1 FPPI for both detectors, however localization performance remained almost same. The result shows again the advantage of our algorithm for detecting and localizing pedestrians in low-resolution images.

Overall, even though we use a single type of feature (HOG), the localization performance of our algorithm is improved by employing macrofeature layouts in local scale-space and it is more stable across multiple datasets than other methods; our layout selection technique improves the performance by finding relevant features efficiently and capturing structural information effectively.

We evaluated CPU and CUDA implementations in pedestrian detection and localization on the datasets—all datasets except INRIA—that have the fixed frame size of 
                           
                              640
                              ×
                              480
                           
                        . Table 7
                         presents average running time with four implementation variations—multiple-threaded CPU and CUDA implementations based on the standard and our modified HOG features. The running times were measured in a server with 
                           
                              2
                              ×
                              6
                           
                         CPU cores at 2.66GHz and in a PC with GTX580 (512 cores at 1.53GHz). Our CUDA implementation using 512 cores, based on the modified HOG features, ran approximately 40 times faster than the multi-threaded CPU implementation based on the same features and almost twice faster than the CUDA implementation based on the standard HOG features. The modified HOG features are more advantageous particularly in the CUDA implementation, compared to the standard features, because of the better utilization of the limited shared memory. Our CUDA implementation based on the modified HOG features had marginally lower detection rate by the average of 1.6% points in comparison to the CPU implementation based on the standard HOG features as illustrated in Fig. 12
                        . The degradation probably resulted from the absence of Gaussian weighting for HOG blocks, which coincides with the experiment in [7]. The modification also reduced mean overlap ratios slightly by the average of 0.4% points which seems negligible.

@&#CONCLUSIONS@&#

We proposed a local macrofeature layout selection to improve object localization in a general boosting framework for object detection. The macrofeature layouts model lines, triangles, and pyramids based on three blocks in a neighborhood. The structural information involved in the line, triangle, and pyramid features improves localization, and the “local” selection strategy reduces training and detection time by eliminating many irrelevant candidates and redundant loading to memory, respectively. Our macrofeature layout selection is successfully employed for pedestrian detection and presents better performance than the state-of-the-art techniques even with only a single type of feature, HOG.

@&#ACKNOWLEDGEMENT@&#

This work was supported by MEST Basic Science Research Program through the NRF of Korea (NRF-2012R1A1A1043658), the IT R&D program of MKE/KEIT [10040246, Development of Robot Vision SoC/Module for acquiring 3D depth information and recognizing objects/faces], and a grant from a Strategic Research Project funded by KICT [Development of real-time traffic tracking technology based on view synthesis-2013-0038-1-1].

@&#REFERENCES@&#

