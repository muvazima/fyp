@&#MAIN-TITLE@&#Lessons learnt from the DDIExtraction-2013 Shared Task

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           DDIExtraction aims to the extraction of drug–drug interactions from texts.


                        
                        
                           
                           Non-linear kernel-based methods overcome linear SVMs.


                        
                        
                           
                           Most results are statistically significant.


                        
                        
                           
                           Resolution of complex sentences could lead to better performance.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Information extraction

Relation extraction

Drug interaction

@&#ABSTRACT@&#


               
               
                  The DDIExtraction Shared Task 2013 is the second edition of the DDIExtraction Shared Task series, a community-wide effort to promote the implementation and comparative assessment of natural language processing (NLP) techniques in the field of the pharmacovigilance domain, in particular, to address the extraction of drug–drug interactions (DDI) from biomedical texts. This edition has been the first attempt to compare the performance of Information Extraction (IE) techniques specific for each of the basic steps of the DDI extraction pipeline. To attain this aim, two main tasks were proposed: the recognition and classification of pharmacological substances and the detection and classification of drug–drug interactions. DDIExtraction 2013 was held from January to June 2013 and attracted wide attention with a total of 14 teams (6 of the teams participated in the drug name recognition task, while 8 participated in the DDI extraction task) from 7 different countries. For the task of the recognition and classification of pharmacological names, the best system achieved an F1 of 71.5%, while, for the detection and classification of DDIs, the best result was an F1 of 65.1%. The results show advances in the state of the art and demonstrate that significant challenges remain to be resolved. This paper focuses on the second task (extraction of DDIs) and examines its main challenges, which have yet to be resolved.
               
            

@&#INTRODUCTION@&#

Pharmacovigilance is formally defined by the WHO as “the science and activities related to the detection, assessment, understanding and prevention of adverse effects or any other drug-related problems” [1]. One of the major aims of pharmacovigilance is the early detection of adverse drug reactions (ADRs), which are unintended and harmful reactions to drugs. Several studies point out that the number of ADRs has increased significantly in recent years [2] and are responsible for about 5% of all hospital admissions [3,4]. More seriously, ADRs cause more than 300,000 deaths per year in the USA and Europe [5,6]. As a result, ADRs are a direct cause of the increase in health care costs [2]. Thus, the pharmacovigilance process is considered vital by pharmaceutical companies and drug agencies due to the high and growing incidence of drug safety incidents as well as their high associated costs.

Healthcare professionals are responsible for recognizing and reporting side effects by spontaneous post-marketing reporting systems. However several published drug safety issues have shown that the adverse effects of drugs may be detected too late, when millions of patients have already been exposed to them [7]. This fact poses a serious problem for patient safety giving rise to a growing interest in improving the early detection of ADRs. Drug–Drug Interactions (DDIs), which can be defined as alterations in the effects of a drug due to the recent use or simultaneously one or more other drugs, are an important subset of ADRs. Although there are different databases supporting healthcare professionals in the detection of DDIs (such as DrugBank [8]), the quality of these databases is very uneven and the consistency of their content is limited, so it is very difficult to assign a real clinical significance to each interaction [9,10]. On the other hand, these databases do not scale well to the large and growing number of pharmacovigilance literature in recent years [10]. In addition, a large amount of the most current and valuable information is unstructured, written in natural language and hidden in published articles, scientific journals, books and technical reports [11]. Thus, the large number of databases with information on DDIs and the deluge of published research have overwhelmed most healthcare professionals because it is not possible to remain up to date on everything published about DDIs.

Therefore, there is an increasing interest in facilitating automated access to information relevant on DDIs described in biomedical texts. Information Extraction (IE) techniques applied to pharmacovigilance literature can be of great benefit in the pharmaceutical industry allowing the identification and extraction of relevant information and providing an interesting way of reducing the time spent by healthcare professionals and researchers on reviewing the literature.

With the support of collaborative events such as BioCreative [12–15], BioNLP [16–18], i2b2 [19,20], ShARe/CLEF eHealth [21] and SemEval-2014 Task 7 Analysis of Clinical Texts
                        1
                        
                           http://alt.qcri.org/semeval2014/task7/.
                     
                     
                        1
                      shared tasks, there has been significant progress in IE techniques in the biological domain. However IE technology applied to pharmacovigilance still remains quite unexplored compared to biology.

The extraction of DDIs from biomedical texts has gained popularity and has seen significant advances recently with the organization of the DDIExtraction Shared Tasks in 2011 [22] and 2013 [23]. The main goal of these community challenges is to provide a common framework for the evaluation of information extraction techniques applied to the extraction of DDIs from biomedical texts. While the first event in 2011 only focused on the identification of all possible pairs of interacting drugs, the 2013 edition also included, in addition to DDI detection, the classification of each DDI. Furthermore, a supporting task, the recognition and classification of pharmacological substances, was proposed in 2013.

In the latest edition of DDIExtraction, a total of 14 teams submitted runs for at least one of the proposed subtasks (6 of the teams participated in the drug name recognition task, while 8 participated in the DDI extraction task). In the drug name recognition subtask, the top scoring team reached an F-score of 71.5%. In the relation extraction task, the best system achieved an F1 of 65.1%. This paper focuses on the second task (extraction of DDIs). The aim of this paper is twofold: to provide a detailed description and discussion on the 8 participating systems in the second task, the extraction of DDIs, and to discuss the remaining challenges revealed by the error analysis on these systems.

This paper proceeds as follows: Section 2 describes the corpus used in the shared task; in Section 3 we give a detailed discussion of the participating systems; Section 4 presents the results obtained by the participating systems; Section 5 describes the major sources of errors in these systems; Section 6 presents a study as to whether the results are significant statistically; in Section 7 we propose an ensemble system of combining the top three methods using majority and union voting strategies; and finally, we close with a discussion in Section 8 of possible future steps of the DDIExtraction Shared Task.

The major contribution of DDIExtraction has been to provide a benchmark corpus, the DDI corpus. The DDI corpus is a valuable gold-standard for those research groups interested in the recognition of pharmacological substances or those specifically working in the field of DDI relation extraction. It consists of 792 texts selected from the DrugBank database (DDI-DrugBank dataset) and other 233 Medline abstracts (DDI-MedLine dataset) on the subject of DDIs. The corpus was manually annotated with a total of 18,502 pharmacological substances and 5028 DDIs, including both pharmacokinetic (PK) as well as pharmacodynamic (PD) interactions. Four entity types were proposed to annotate pharmacological substances: drug, brand, group and drug_n. The drug type is used to annotate those human medicines known by a generic name, whereas those drugs described by a trade or brand name are annotated as brand entities. The use of either generic or brand names depends on the drug information source. Thus, while generic names are used in medical and pharmacological textbooks as well as scientific medical journals, brand names are used in drug product labels. The group type was used to annotate groups of drugs. This type was included because the descriptions of DDIs involving groups of drugs are very common in texts. The last entity type, drug_n, refers to those active substances not approved for human use, such as toxins or pesticides. This type was included because interactions between drugs and substances not approved for human use are frequently reported in Medline documents.

DDIs were annotated at the sentence level and, thus, interactions spanning over several sentences were not annotated. Four different types of DDI relationships are proposed: mechanism (this type is used to annotate DDIs that are described by their pharmacokinetic mechanism), effect (this type is used to annotate DDIs describing an effect or a pharmacodynamic mechanism), advice (this type is used when a recommendation or advice regarding a drug interaction is given) and int (this type is used when a DDI appears in the text without providing any additional information). Tables 1 and 2
                     
                      show the numbers of the annotated entities and relationships in each corpus, respectively.


                     Fig. 1
                      shows some examples of annotated texts in the DDI corpus. This figure has been taken from the WBI corpora repository.
                        2
                        
                           http://corpora.informatik.hu-berlin.de/.
                     
                     
                        2
                      The DDI corpus was adapted to the Stav format by the WBI team in order to be visualized using Stav on-line visualization tool [24]. The first example (A), taken from the MedLineDDI dataset, describes a DDI of mechanism type between a drug (named using a synonym different from its most common generic name, fomepizole) that inhibits the metabolism of a substance not-approved to be used in humans (1,3-difluoro-2-propranol). The second example (B) is also a sentence taken from MedLine and describes the consequence of a DDI (effect type) between estradiol (a generic drug) and endotoxin (a drug-n) in an experiment performed in animals. The last example (C) is a paragraph from the DDI-DrugBank dataset. Its first sentence describes the consequence of the interaction (effect type) of a drug, denominated by its brand name (Inapsine), when is co-administered with five different groups of drugs. The third sentence in C shows a recommendation to avoid these DDIs (advice type).

Inter-annotator agreement (IAA) was measured in order to assess the consistency and quality of the corpus as well as the complexity of the annotation task. Tables 3 and 4
                     
                      present the results for the agreement per type of entity and per type of relationship, respectively. Results were calculated in terms of the standard Kappa statistic [25]. The overall IAA results suggest that the DDI corpus has enough quality to be used for training and testing NLP techniques applied to the field of pharmacovigilance. A detailed description of the DDI corpus can be found in [26].

This section reviews the 8 systems participating in the task of extracting DDIs and presents their results. For the evaluation of this task, the participants were given the test data with gold annotation only for pharmacological substances. The evaluation was then carried out by comparing the annotation predicted by each participant to the gold annotation. To simplify the task, the detection of DDIs was conducted at the sentence-level. The evaluation results are reported using the standard recall/precision/f-measure metrics, under different criteria: partial (only detection of DDIs) and exact (detection and classification of DDIs).

The system consisted of two separate steps: first the DDIs were detected and second, the extracted DDIs were classified according to the proposed types (mechanism, effect, advice and int) in the guidelines task. In the DDI detection phase, filtering techniques based on the scope of negation cues and the semantic roles of the entities involved were proposed to rule out possible negative instances from the test dataset. In particular, a binary SVM classifier was trained using contextual and shallow linguistic features to find less informative sentences. A sentence is considered less informative when all its entities as well as its relation clues fall under the scope of a negation cue (no, n’t, not). Less informative sentences were not considered in the relation extraction phase. Also, less informative negative instances were ruled out according to the following exclusion criteria: (1) if two mentions in a sentence refer to the same entity, this pair is not considered as a candidate DDI, (2) for any expression of the form “Drug1 (Drug2)”, the pair was ruled out because both entities refer to the same entity (Drug2 is usually the abbreviation of Drug1), and (3) a candidate pair was ruled out when its two mentions had anti-positive governors with respect to the type of the relation. Anti-positive governors are words that tend to prevent mentions, which are directly dependent on those words, from participating in a certain relation of interest with any other mention in the same sentence [27]. We refer the reader to [28] for detailed description of anti-positive governors.

Once these negative instances were discarded from the test dataset, a hybrid kernel (combining a feature-based kernel, the shallow linguistic kernel (SL) [29] and the Path-enclosed Tree (PET) kernel [30]) was used to train a RE classifier. For the classification of the extracted DDIs, four separate models were trained for each DDI type (using ONE-vs-ALL). If none of the separate models is able to assign a class label to a predicted DDI, a default class label was chosen (for example, effect). The trained models were applied only on the extracted DDIs (by the DDI detection module) from the test dataset. Experiments on the training dataset showed that the filtering techniques improve both precision and recall with respect to applying only the hybrid kernel. This team achieved the best three submitted runs. The only difference between the three runs was the default class label which was “int”, “effect” and “mechanism” for run 1, 2 and 3 respectively. The top run showed an F1 of 0.80 for DDI detection and 0.65 for DDI detection and classification.

The second best system was developed by the WBI team. The system relied on two step processes which first detected DDIs using ensembles of five different classifiers, and then the extracted DDIs were classified with one of the four proposed types. Several experiments were conducted combining the following eight machine learning methods: all-paths graph (APG) [31], the Shallow Linguistic Kernel (SL), SubTree (ST) kernel [32,33], Spectrum tree (Spt) [34], Turku Event Extraction System (TEES) [35], the case-based reasoning Moara system [36] and a self-developed feature based classifier (SLW), which is an extension of SL. Experiments were performed using 10-fold cross validation (CV) on the training set, and showed that the best results were achieved by the following majority voting ensembles: (1) Moara+SL+TEES, (2) APG+Moara+SL+SLw+TEES, and (3) SL+SLW+TEES. These ensembles were submitted as runs. This team was ranked second behind the FBK-irst team. Its best run was the third one, which yielded an F1 of 0.76 for DDI detection and 0.609 for DDI detection and classification.

The third best team was the Uturku team. The TEES system was used to participate in both tasks: drug named entity recognition and extraction of DDIs. TEES is a machine learning system based on SVM, which was originally developed to extract events (and relations) in the BioNLP shared task. The event extraction is tackled as a graph generation task where nodes are keywords and edges are the words that connect nodes. The node detection task is similar to named entity recognition, while the edge detection task can be thought of as a relation extraction task. Deep syntactic features and information from external domain resources such as DrugBank or MetaMap [37] were used to model the Turku system. In run 1, the Uturku system was trained using only a feature set from syntactic parses. In run 2, DrugBank features were added to the syntactic features. Run 3 further extended run 2 with MetaMap information. The results of each run seem to be very close to each other. The best performance was provided by run 2 (an F1 of 0.696 for DDI detection and 0.594 for DDI detection and classification). While drug name recognition benefits from the use of domain knowledge resources, these external resources do not achieve a significant improvement in the relation extraction task. This may indicate that the extraction of DDIs seems to depend more on the syntactic interpretation of parse trees. TEES (version 2.1) is available for research purposes from http://bionlp.utu.fi/eventextractionsoftware.html. The authors also provided their DDI predictions for all DDIExtraction-2013 participants.

The system was based on SVM using lexical, morphosyntactic and parse tree features. Information Gain ranker was used to eliminate the less informative features. The team submitted two different runs: (1) to train a SVM classifier with 5 categories (effect, mechanism, int, advice, null) and (2) to train a binary SVM classifer (DDI, non-DDI), and then, the extracted DDI were used to train a second SVM classifier with four categories (effect, mechanism, int, advice). The second run achieved better results (an F1 of 0.656 for DDI detection and 0.548 for DDI detection and classification) than the first one.

The system was based on the SL Kernel. First, the SL was trained to distinguish positive instances from negative instances, and then, a SL model was trained for each DDI type. The SL kernel uses the following features: tokens, lemmas, PoS tags and entity types. In addition to the features listed above, the Anatomical Therapeutic Chemical (ATC) code of each drug name was obtained from the ATC system,
                           3
                           
                              www.whocc.no/atc/,
                        
                        
                           3
                         the drug classification system adopted by the World Health Organization (WHO). The team submitted two runs. In the first run, the team used the default setting of SL, while in the second one, the lemma feature was replaced by the ATC code of the drug. The first run achieved an F1 of 0.676 for DDI detection and 0.537 for DDI detection and classification. However, the use of ATC codes seems to give rise to a significant detriment to the performance with an F1 of 0.537 for DDI detection and only 0.294 for DDI detection and classification.

The system relied on two step processes: the first one detected DDIs using a binary weighted SVM classifier to discriminate positive instances (that is, DDIs) from negative instances, and then, a multi-class weighted SVM classifier was applied on the extracted DDIs (by the binary SVM) in order to classify each DDI. The team’s hypothesis is that separating the detection and classification tasks into two different phases can help to handle the highly unbalanced class distribution. Texts were transformed into lower-case, drug names blinded and number were normalized. A feature set of lexical (such as bag of words and bigrams) and semantic features (synsets from WordNet [38]) was used to train a binary weighted classifier to discriminate positive instances from negative instances. Tokens were stemmed and lemmatized. The authors also used different stopwords lists of different size. The number of false positives was relatively high since the positive class was favored in the weighted SVM. The authors also defined a set of post-processing rules which were applied after the binary SVM classifier. For example, a rule consisted of discarding those pairs of interacting drugs referring to the same entity. Another example of a rule was when an interacting drug is a drug class of the other one; in this case, this pair should be ruled out since, in general, these pairs represent a hyponym/hyperonym relationship and not an interaction [39]. Other rules were aimed at detecting (without using any syntactic information) those pairs of drugs appearing in the same coordinative structure, since in general they are not interacting drugs.

Then, a multi-class SVM was trained on the set of extracted DDI classified by the previous binary classifier. In this case, the team proposed a rule that would assign the same type to all pairs obtained from drug mentions in a coordinative structure and other drug mention.

The team submitted three runs. The only difference between them was the size of the stopwords list used in each run. Experiments showed that the list of bigest size (263 stopwords) and the use of stems instead of lemmas achieve better results (F1=0.599 for DDI detection and F1=0.47 for DDI detection and classification) than the other settings.

This system was based on the combination of three machine learning techniques: LibLINEAR [40] (linear SVM), Naïve Bayes and Voting Perceptron. While the first run was generated using only LibLINEAR, the second and third ones were based on majority and union ensemble learning strategies, respectively. All ML techniques used a rich feature vector consisting of lexical, syntactic and semantic features. The classification of extracted DDIs was performed by a post-processing step. This post-processing step uses a list of trigger words related for each type DDI which were manually created based on the observation of the MedLine dataset. The authors also applied an undersampling technique to balance the corpora and study its influence on the performance (only on the training dataset).

According to the official scores, their best result was obtained by run 3 (union voting strategy) with an F1 of 0.704 for DDI detection and 0.458 for DDI detection and classification. As regards the results for DDI classification, the system achieved the top score on MedLine (micro F1=0.42), however the system ranked at 5th position for DrugBank. This may be due to the trigger words collected were based on the observation of MedLine abstracts. Therefore, it would be advisable to define trigger words for each DDI type depending on the corpus.

This team used LIBSVM [41] trained with morphosyntactic, lexical and semantic features. The team applied the one-vs-all multi-class classification technique to handle the different DDI types. Lexical and semantic features were used to train the classifiers. In run 2, the team also added features from TEES analysis provided by the UTurku team, and in run 3, features used in run 2 along with a list of interaction words were used as feature set. Their best run was the third one, achieving an F1 of 0.491 for DDI detection and 0.336 for DDI detection and classification.

@&#DISCUSSION@&#

A common characteristic of all participating systems was the use of SVMs. While most systems used feature-based methods, only three teams (FBK-irst, WBI-DDI, UC3M) applied kernel-based methods which in general achieved better performance than the feature-based ones. Unlike feature-based methods, kernel-based methods do not require the explicit definition of feature vectors. A kernel-based method contains a kernel function and a kernel learner. A kernel function is a function that computes the similarity between two instances (for example, drug pairs). A kernel learner (such as SVM) is a learning algorithm which performs a learning task in a feature space. Table 5
                         gives a detailed view of all the ML techniques and tools used by the participating systems.

Most participating systems separate the learning problem into two stages: first the DDIs are detected and then they are classified into one of the types proposed in the guidelines. The only exceptions were the UTurku and NIL_UCM team. The TEES system, developed by the UTurku team, uses a multiclass SVM on a rich graph-based feature set. The NIL_UC3M team trained a multi-class SVM classifier with 5 classes (mechanism, effect, advice, int and null for negative instances). The NIL_UC3M also developed an approach in which the DDI detection and classification stages were separated. The evaluation on test dataset showed that the two-stage approach yielded better results than those achieved by the multi-class classifier.

As regards the two-stage approaches, the first stage, the detection of DDIs, is always performed by a binary classifier responsible for distinguishing between negative and positive DDIs instances. Most teams treated each DDI type as a single classification subproblem (one-vs-all). The SCAI team was the only one that did not use any machine learning techniques in the classification task. DDI instances detected in the previous step were classified according to a set of trigger words related with each type of DDIs.

As regards the natural language processing (NLP) tools often integrated into the participating systems, stemming, POS tagging and syntactic parsing were the most common ones. Stanford parser tools [42] were widely used by most systems. Around half of the participating systems used the Charniak–Johnson parser [43] with David McClosky’s biomodel [44] trained on the GENIA corpus and unlabeled PubMed articles. From the results of the FBK-irst, WBI and UTurku teams, we can conclude that the parsers for the biomedical domain provided better performance than parsers trained for a general domain.

Some systems also used additional elements, such as lemmatization (WBI and UWM_TRIADS teams), semantic parsing provided by MetaMap (UTurku and NIL_UCM teams) or disease named entity recognition (team FBK-irst). Negation detection was only used by one team (FBK-irst). Surprisingly, only half of the participating systems used external lexical resources such as dictionaries or ontologies. Table 6
                         shows the NLP components and external resources used by the participating systems.

None of the participating systems made use of any additional training data collections to develop their systems, which implies that all systems relied only on the training dataset provided by the task organizers.

This section summarizes the evaluation results and provides detailed analysis and discussion.

For the evaluation, the test dataset with gold annotation only for pharmacological substances was released to participants. Then, the evaluation was conducted by comparing the annotation predicted by each system to the gold annotation. The evaluation results are reported using the standard recall/precision/f-score metrics.


                     Table 7
                      shows the results of the DDI detection task. These results are not directly comparable with those reported in DDIExtraction 2011 due to the use of different training and test datasets in each edition. However, it should be noted that there has been a significant improvement in the detection of DDIs: almost all participants (except for the two worst teams) achieved an F-measure above 65.4% (the best F1 in DDIExtraction 2011). The increase in the size of the corpus made for DDIExtraction 2013, the inclusion of different types of documents and the quality of their annotations may have contributed significantly to this improvement. The best system (run 1 submitted by the FBK-irst team) had precision of 83.8% and recall of 83.8% (F1 82.7%) on DrugBank dataset, compared to its precision of 55.8% and 55.5% recall (F1 53%) on the MedLine dataset. It should be noted that there is almost a 30 point F-measure difference between the DrugBank dataset and the MedLine dataset. Indeed a common characteristic observed in all systems was the strong decrease in their results on the MedLine dataset compared to the DrugBank dataset. This may be justified by the different styles of the two sources. In the one hand, the texts taken from DrugBank are manually curated to provide brief descriptions of DDIs. Therefore, DrugBank contains short and concise sentences. On the other hand, the main topic of the scientific texts from MedLine would not necessarily be on DDIs. Moreover, these texts are characterized by a very scientific language and it is common the use of long and subordinated sentences. The error analysis (see Section 5) showed that the systems fall drastically for long and complex sentences. Another possible reason may be the different size between the two corpora. In addition, while the best system obtained balanced results in both precision and recall, the rest of the participants showed biased scores towards one or other metric.

As stated earlier, the use of biomedical parsers seems to provide better performance than parsers trained for a general domain, and the kernel-based systems in general overcame the feature-based ones.

The DDI classification task does not only consist of the identification of all possible pairs of interacting drugs, but also their classification. The results did not exceed an F1 of 65.1% (FBK-irst team) on the DrugBank dataset and 42% (SCAI team) on the MedLine dataset (see Table 8
                     ). These results clearly demonstrate that the identification of what type of information (such as an advice, an effect or information about the way the interaction occurs) is being used to describe a DDI may be a very complex task. As in the DDI detection task, all systems (except the runs submitted by the FBK-irst team) showed a marked disparity between precision and recall.


                     Figs. 2 and 3
                     
                      show the results for each type of DDI on the DrugBank and MedLine test datasets, respectively. From each participant, we only select its best run. Fig. 2 suggests that some types of DDI are more difficult to classify than others on the DrugBank dataset, being the advice relationship being the easiest one. One possible explanation for this could be that recommendations or advice regarding a drug interaction are typically described by very similar text patterns such as ‘DRUG should not be used in combination with DRUG’ or ‘Caution should be observed when DRUG is administered with DRUG’. The participating systems achieve very similar performance for the mechanism and effect relationships, while the int relationships seem to be the most difficult to extract. This may be because the proportion of instances of int relationship (5.6%) in the DDI corpus is much smaller than those of the rest of the relations (41.1% for effect, 32.3% for mechanism and 20.9% for advice).

The aim of this section is to perform a detailed error analysis with the objective of providing a road map for future work in the extraction of DDIs from texts. To this end, we focus on the study of the main source of errors produced by the systems developed by the following teams: FBK-irst, WBI-DDI and Uturku. For each team, we only analyzed their best runs. The reason for this choice is that these systems were the top-performing in DDIExtraction 2013.


                        Tables 9 and 13
                         present the main causes for the false negatives in the DrugBank dataset and the MedLine dataset, respectively.

From Table 9, we can see that one of the most important factors contributing to false-negatives in DrugBank texts is the lack of cataphora resolution in the three systems. The resolution of the appositions in sentences, prior to the detection of DDIs, could allow to further improve the performance, particularly the FBK-irst and UTurku systems. Similarly, the resolution of anaphora and the detection of coordinate structures may also help to reduce false negatives, though fewer than the resolution of cataphoras and appositions. Another major cause of false negative is that many DDIs are described with very unusual text patterns. The high variability of natural language expression allows DDIs to be able to be composed using many different lexical and syntactic realizations. Classifiers have problems in detecting these cases since they are probably unrepresented in the training data. Tables 10–12
                        
                        
                         show some examples of false negatives in the DrugBank dataset.

Long, complex and compound sentences are other sources of false negatives. Many DDIs are described in long and complex sentences, which usually have a complex syntactic and lexical structure. Sentences with several embedded subordinated clauses are often encountered both in DrugBank and MedLine. Moreover, these sentences also pose a challenge to syntactic parsers due to their high levels of ambiguity. This may be one of the reasons why the methods using syntactic features from parsers (e.g. Standford parser) are not capable of dealing with these types of sentence. The FBK-irst system shows a lower rate of false negatives (only 2% are classified as long DDI descriptions and only 4% as complex and compound sentences) compared to the other two systems. In this case, the use of semantic roles, which were used to rule out possible negative instances, could be helping to overcome a wrong syntactic analysis.

Some sentences describe DDIs without giving an absolute certainty of their existence or using uncommon patterns. For example, in the sentence ‘Lapatinib may have the potential to convert Herceptin-refractory to Herceptin-sensitive tumors in HER2-positive breast cancer by up-regulation of the cell surface expression of HER2’, it is even difficult for a human being to determine whether they are DDIs or not. The detection of dosages, numeric and temporal expressions can also help to improve the performance of the systems, since many sentences describe DDIs including additional information such as dosages, dosage regimen or percents of change of parameters, among others.

As regards the MedLine dataset (see Table 13
                        ), false negatives have similar error sources to those in the DrugBank dataset. The major cause of false negatives for all three systems is their inability to detect those DDIs described by patterns that are very scarce, even unrepresented, in the training data. This may be due mainly to the small size of the training dataset from MedLine. The detection of doses, numerical and temporal expressions also seem to be another significant problem which these systems have to face in order to improve their performance in the detection of DDIs. Anaphoras, cataphoras, coordinated structures and appositions have a much less significant effect on the false negatives in the MedLine dataset than in the DrugBank dataset. A possible reason for this could be that many texts in DrugBank provide descriptions of DDIs involving a drug and a list of drugs. The use of these linguistic structures is very common and useful in providing these kinds of description. Tables 14 and 15
                        
                         show some examples of false negatives in the MedLine dataset.


                        Tables 16 and 19
                         show the main causes of false positives in DrugBank and MedLine, respectively.

The major cause of false positives in DrugBank refers to sentences in which interacting drugs have more than one mention. The systems were able to detect that there was an interaction between two drugs, but failed to identify their mentions that were actually involved in this DDI. The first example in Table 18 (see E28) shows a sentence describing a DDI between ibuprofen and ALIMTA. We can see that both drugs appear twice in the sentence, but only their last two mentions are involved in the description of a DDI. However, the three systems failed to detect this DDI, because they proposed the pair formed by the first two mentions of the drugs. Annotation errors (see E29 in Table 17
                        ) are the second source of false positives in DrugBank. The candidate pairs were correctly detected by the systems, but were not annotated in the DDI corpus. Another cause of false positives was the systems’ incapability to distinguish between drugs constituting a coordinate structure, and therefore, to recognize that they are not describing a DDI (see E30 in Table 17). Notably, one of the main sources of false positives would be fine with a simple rule that prevented mentions of drugs referring to the same drug which could be considered as a candidate DDI (see E31 in Table 18
                        ). The lack of evidence to confirm the existence of a DDI was another source of false positives (see E32 in Table 18). In fact, it was the main cause of false positives in MedLine (see Table 19 and Table 20
                        
                        ).

McNemar’s significance test [55] is a 
                        
                           
                              
                                 χ
                              
                              
                                 2
                              
                           
                        
                     -based significance test used to compare two groups, such as two classifiers or two population samples. We applied the McNemar’s significance test to compare the performance of the different runs and determine whether or not they differ significantly. Thus, for each pair of runs 
                        
                           
                              
                                 R
                              
                              
                                 a
                              
                           
                        
                      and 
                        
                           
                              
                                 R
                              
                              
                                 b
                              
                           
                        
                     , their corresponding models were performed on the test dataset, and the contingency matrix was then built for any pair of runs. The classification of each example in the test dataset by each model was recorded, counting the number of examples correctly classified by 
                        
                           
                              
                                 R
                              
                              
                                 a
                              
                           
                        
                      and 
                        
                           
                              
                                 R
                              
                              
                                 b
                              
                           
                        
                      (
                        
                           
                              
                                 n
                              
                              
                                 11
                              
                           
                        
                     ), the number of examples correctly classified by 
                        
                           
                              
                                 R
                              
                              
                                 a
                              
                           
                        
                      but not by 
                        
                           
                              
                                 R
                              
                              
                                 b
                              
                           
                        
                      (
                        
                           
                              
                                 n
                              
                              
                                 10
                              
                           
                        
                     ), the number of examples misclassified by 
                        
                           
                              
                                 R
                              
                              
                                 a
                              
                           
                        
                      but not by 
                        
                           
                              
                                 R
                              
                              
                                 a
                              
                           
                        
                      (
                        
                           
                              
                                 n
                              
                              
                                 01
                              
                           
                        
                     ), and the number of examples misclassified by both 
                        
                           
                              
                                 R
                              
                              
                                 a
                              
                           
                        
                      and 
                        
                           
                              
                                 R
                              
                              
                                 b
                              
                           
                        
                      (
                        
                           
                              
                                 n
                              
                              
                                 00
                              
                           
                        
                     ).

McNemar’s test is based on a 
                        
                           
                              
                                 χ
                              
                              
                                 2
                              
                           
                        
                      goodness-of-fit test comparing the distribution of counts expected under the null hypothesis to the counts observed. The null hypothesis 
                        
                           
                              
                                 H
                              
                              
                                 0
                              
                           
                        
                      states that the two classifiers (runs) should have the same error rate (i.e., 
                        
                           
                              
                                 n
                              
                              
                                 10
                              
                           
                           =
                           
                              
                                 n
                              
                              
                                 01
                              
                           
                        
                     ). According to Dietterich [56], under the null hypothesis the following statistic (see Eq. (1)) is distributed as an 
                        
                           
                              
                                 χ
                              
                              
                                 2
                              
                           
                        
                      distribution with one degree of freedom.
                        
                           (1)
                           
                              χ
                              =
                              
                                 
                                    
                                       
                                          (
                                          |
                                          
                                             
                                                n
                                             
                                             
                                                01
                                             
                                          
                                          -
                                          
                                             
                                                n
                                             
                                             
                                                10
                                             
                                          
                                          |
                                          -
                                          1
                                          )
                                       
                                       
                                          2
                                       
                                    
                                 
                                 
                                    
                                       
                                          n
                                       
                                       
                                          01
                                       
                                    
                                    +
                                    
                                       
                                          n
                                       
                                       
                                          10
                                       
                                    
                                 
                              
                           
                        
                     
                  

To test for significance, 
                        
                           
                              
                                 χ
                              
                              
                                 2
                              
                           
                        
                      was compared to the appropriate 
                        
                           
                              
                                 χ
                              
                              
                                 2
                              
                           
                        
                      table. Results with a probability greater than or equal to 0.05 are generally considered to be significant. Thus, the null hypothesis was correct if 
                        
                           
                              
                                 χ
                              
                              
                                 2
                              
                           
                        
                      was lower than 
                        
                           
                              
                                 χ
                              
                              
                                 1
                                 ,
                                 0.05
                              
                              
                                 2
                              
                           
                           =
                           3.841459
                        
                     . In other cases, the null hypothesis could be rejected in favor of the hypothesis that the two runs produce different levels of performance.

First, we analyzed whether the runs submitted by a same team were statistically significant from each other. The runs submitted by the top four teams (FBK-irst, WBI, UTurku and SCAI) did not show statistically significant differences between the other runs of the same team. However, when comparing the two runs submitted by the NIL_UCM, a statistically significant difference did exist between their results. The first run used a multiclassifier with 5 categories (effect, mechanism, int, advice, null), while its second run followed a two-stage approach in which DDIs were initially detected using a binary classification, and then these detected DDIs were used to train a second SVM classifier with four categories (effect, mechanism, int, advice). Similarly, we also observed a strong statistically significant difference between the two runs submitted by the UC3M team. The second run used the ATC code instead of the lemma feature in the shallow linguistic kernel. This is the only difference between the two runs and it seems to give rise to a strong decrease in the performance. The only differences between the runs submitted by the UWM_TRIADS team were the size of the stopwords lists used in each run and the use of stems instead of lemmas. These changes led to statistically significant differences between the results of the three runs. As regards the runs submitted by the UColorado_SOM team, the use of a list of interacting words as an additional feature in the third run yielded the best results and also brought about statistically significant differences with the other two runs.

To study whether the runs submitted by the teams obtained statistically significant results with respect to the other participating teams, we only chose the best run from each team. Table 21
                      summarizes the 
                        
                           χ
                        
                      statistic values for the pairwise comparison of the eight runs using the McNemar significance test (i.e., a total of 28 comparisons). Each cell of this pairwise comparison matrix represents the 
                        
                           χ
                        
                      statistic value for a given pair of runs. Table 21 shows that in general most systems significantly differ from all others. The UTurku system does not significantly differ from the SCAI system. Similarly, the differences in performance between the UC3M system and the NIL-UCM system are not significant.

In order to investigate whether it is possible to improve the best scores obtained in DDIExtraction 2013, we built different ensemble systems by combining the submitted runs.

We chose to use a majority voting-based ensemble strategy due to its simple implementation. In this strategy, each system votes for a particular prediction (DDI or non-DDI), and the class with the most votes is selected as the final decision. In case of a tie, the final prediction was set to DDI, improving the recall (though at the expense of reducing the precision). This decision is based on the fact that all the participating systems have achieved better precision than recall. Therefore, in order to improve the F-measure, it is necessary to improve the recall. The main shortcoming of majority voting is that this strategy does not take in account that sometimes the minority predictions are correct.

We also performed some experiments using a union voting strategy. In this strategy, if a candidate pair is classified as DDI by at least one system, then the final decision for this pair will be DDI. A pair will be classified as negative only when it is classified as negative by all the systems. This strategy achieves a significant improvement of the recall (above 90%), but at expense of a strong decrease in precision (under 55%), and thereby, the F-measure also suffers a significant decrease (no more than 69%). Therefore, we decided not to use this strategy in our ensemble system.

As mentioned above, we conducted numerous experiments using different combinations of the final submissions. Table 22
                      shows the experimental results of some of these ensembles. For example, the second row shows the results obtained by combining all the runs. Since the UWM-TRIAD team did not provide any prediction for a total of 202 pairs, we decided to consider their unseen pairs as non-DDIs. In general, the ensemble systems do not overcome the FBK-irst system (see row 1 in Table 22). Therefore, we can conclude that the FBK-irst system is considerably more robust than the other systems.

We decided to evaluate whether removing the predictions of some teams had a positive or negative effect on the final performance. For example, we removed the predictions provided by the UCOLORADO_SOM team since its performance is markedly below that of the rest of the teams. However, this did not achieve an improvement in the results (see row 9 in Table 22 of the ensemble system, particularly on the MedLine dataset).

The ensemble made by the best run of each team (see row 6 in Table 22) achieves very close results to those reported by the best system, but not better than them. We also conducted an experiment in which we selected, for each team, the run that maximized recall (see row 7 in Table 22). This experiment showed lower results than the previous experiment.

Only one ensemble system (see the “6bestruns” row in Table 22) manages to improve the results of the FBK system very slightly. This ensemble consists of the 6 best submitted runs, that is, the runs submitted by the FBk-irst and WBI-DDI teams. This may indicate that if the FBK-irst system is extended to integrate the kernels proposed by the WBI teams, its performance may be improved. However, the difference between this ensemble and the FBK-irst system does not seem to be statistically significant.

The goal of DDIExtraction is to promote the development of information extraction techniques applied to the detection of drug names and DDIs from biomedical texts. There were a total of 38 runs which were submitted by 14 different teams from 7 different countries (6 of the teams participated in the drug name recognition task, while 8 participated in the DDI extraction task). The highest F1 scores obtained was 71.5% for drug name recognition and classification and 65.1% for extraction and classification of DDIs. This paper focuses on the extraction of DDIs. We have presented the main approaches used and examined the main challenges, which have yet to be resolved.

As regards the task of detection of DDIs, the participating systems demonstrated substantial progress over the previous DDI Extraction 2011. The best team, FBK-irst, achieved a competitive F-measure of 82.7% on DrugBank texts. However, performance on MedLine was lower mainly due to the limited size of its training dataset. Another possible reason may be that MedLine texts have a greater complexity than DrugBank texts. All teams used machine learning methods, specifically SVM. In general, non-linear kernel-based methods overcome linear SVMs.

We conclude that research into DDI extraction must continue. The error analysis points out the main limitations of the participating systems. Current approaches have focused on syntactic aspects, drawing their attention to the sentence structure. The resolution of linguistic phenomena such as cataphora, anaphora, appositive and coordinate structures and complex sentences, among others, could lead to better performance.

On the other hand, few participating systems took into account the sentence meaning. Approaches using domain knowledge have been recently applied with success to the pharmacological domain [57,58]. The use of knowledge resources can reduce the number of false positives generated by the current DDI extraction systems because these resources can help to distinguish between those pairs of drugs that are DDIs from those that are not. The information required for a semantic-based IE system can be taken, for example, from pharmacological databases such as DrugBank, PharmGKB [59], SIDER [60] or KEGG [61], among others. Some of them describe specific pairs of interacting drugs. For example, in DrugBank 39 different drugs that interact with ciprofloxacin are described. On the other hand, a larger number of DDIs can be deduced indirectly by exploiting, for example, the drug-protein relationships. Thus, the relationships of two different drugs with the same protein can be used to infer the mechanism leading to a DDI [62]. For example, ciprofloxacin is described to inhibit the activity of the metabolic enzyme CYP1A2, and duloxetine is described to be metabolized by CYP1A2. Therefore, there could be an interaction between ciprofloxacin and duloxetine. Similarly, the relationships of two different drugs with the same adverse drug reaction (ADR) can be used to infer possible DDIs [63]. For example, morphine is related to the side effect central nervous system depression. Therefore, other drugs related to the same ADR, such as oxycodone, could interact with morphine.

Up to now, the main limitation for the development of semantic-based approaches has been the availability of appropriate knowledge bases in a machine-readable format. However, the creation of these knowledge bases is becoming more feasible and common in the pharmacological domain [64,65]. This is due to the increasing number of databases and web servers providing structured and semi-structured pharmacological information, such as DrugBank or KEGG. Moreover, there are different community projects such as BIO2RDF [66] or LODD [67], which work to link the various sources of biological and pharmacological data together, enabling the integration of several pharmacological aspects described in different databases [68]. Another important factor is the proliferation of biomedical ontologies to store and formally represent domain knowledge. Ontologies enable the integration of the information disperse through different and heterogeneous databases and provide artifacts that can be exploited by IE systems [69].

Therefore, future directions for DDI extraction might entail the combination of syntactic and semantic information. In addition, increasing the size of training dataset, in particular for MedLine, would also have a very positive impact on the results.

@&#ACKNOWLEDGMENTS@&#


                  Funding: This work was supported by the EU project TrendMiner [FP7-ICT287863], by the project MULTIMEDICA [TIN2010-20644-C03-01] and by the Research Network MA2VICMR [S2009/TIC-1542].

@&#REFERENCES@&#

