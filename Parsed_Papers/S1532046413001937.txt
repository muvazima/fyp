@&#MAIN-TITLE@&#A flexible approach to distributed data anonymization

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose a flexible approach for de-identifying distributed biomedical datal.


                        
                        
                           
                           Horizontal and vertical data distribution are handled in a consistent manner.


                        
                        
                           
                           Our method supports a broad spectrum of anonymization methods and privacy criteria.


                        
                        
                           
                           Supported algorithms include optimal methods, heuristics and clustering algorithms.


                        
                        
                           
                           Applicable criteria include k-anonymity, l-diversity, t-closeness and d-presence.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Personal data protection

Distribution

Privacy

Anonymization

Commutative encryption

Secure multi-party computing

SMC

@&#ABSTRACT@&#


               
               
                  Sensitive biomedical data is often collected from distributed sources, involving different information systems and different organizational units. Local autonomy and legal reasons lead to the need of privacy preserving integration concepts. In this article, we focus on anonymization, which plays an important role for the re-use of clinical data and for the sharing of research data. We present a flexible solution for anonymizing distributed data in the semi-honest model. Prior to the anonymization procedure, an encrypted global view of the dataset is constructed by means of a secure multi-party computing (SMC) protocol. This global representation can then be anonymized. Our approach is not limited to specific anonymization algorithms but provides pre- and postprocessing for a broad spectrum of algorithms and many privacy criteria. We present an extensive analytical and experimental evaluation and discuss which types of methods and criteria are supported. Our prototype demonstrates the approach by implementing k-anonymity, 
                        
                           ℓ
                        
                     -diversity, t-closeness and 
                        
                           δ
                        
                     -presence with a globally optimal de-identification method in horizontally and vertically distributed setups. The experiments show that our method provides highly competitive performance and offers a practical and flexible solution for anonymizing distributed biomedical datasets.
               
            

@&#INTRODUCTION@&#

Collaboration and data sharing have become core elements of biomedical research. Examples are international projects like the International Cancer Genome Consortium ICGC with its goal to “make the data available to the entire research community” [1], and BBMRI-LPC aiming “to help scientists to have better access to large European studies on health” [2]. Also, from the perspective of public funders, sharing of research data has become a request, and principles of sharing have been formulated [3,4]. Besides the international projects mentioned above, there are research projects on national, regional, and institutional levels, which collect, integrate, and share data.

The process of managing data from collection to analyses and also to sharing can be illustrated by different phases [5]. Research data is collected and managed, which may be accompanied by further processes, such as quality assurance. Sharing is initiated by allowing other researchers to get an overview over available data which fit their research objectives. Typically, access to core data is limited, and data access committees are involved before data use agreements (DUAs) are signed and data is released. Then, this data is integrated and used for new analyses.

There is a growing understanding of risks related to data sharing: disclosure of sensitive biomedical data may lead to harm for individuals, especially when different sources are available for linkage (for an overview see [6]). Basically, national laws and regulations, such as the HIPAA Privacy Rule [7], as well as international regulations, such as the European Directive on Data Protection [8], mandate stringent protection of personal data. In recent years, there has been extensive work on ethical, legal, social/societal issues (ELSI) of biomedical and genomic research and on data sharing, e.g., [9,10], which we will not further address in this article.

Anonymization is an important privacy measure when releasing and sharing sensitive datasets. As an important example, the HIPAA Privacy Rule has defined concrete measures to prevent re-identification. These include methods of statistical disclosure control. Basically, fuzziness is introduced to a degree which balances remaining semantics and usability against risk reduction. K-anonymity is a well known and understood privacy criterion, focusing on quasi-identifiers. These are attributes that are required for analyses but are associated with a high risk of reidentification. A dataset is k-anonymous if each data item cannot be distinguished from at least 
                        
                           k
                           -
                           1
                        
                      other data items regarding the quasi-identifiers [11]. Introducing k-anonymity is a measure against linkage attacks which may lead to identity disclosure when accessible data is combined with an attackers background knowledge [12].

Data is often collected from distributed sources, involving different types of data, different information systems, and different organizational units. Pseudonymity is another privacy measure of relevance, which leads to distribution. Here, directly identifying data is separated from medical data, and the links between identifiers and corresponding pseudonyms are secretly kept by a honest broker [13]. In general, data can be distributed vertically or horizontally. The former means that different sites hold different subsets of the attributes for a common set of individuals, so pseudonymity is a typical example. The latter means that different sites hold data with the same set of attributes for different individuals, for example, data for individuals in their region. Health services research is an example where integration of horizontally distributed data is needed, and disclosure has to be prevented.

In this article, we will focus on anonymization of datasets which are horizontally or vertically distributed. Existing approaches have focused on limited sets of privacy criteria, which in practice must often be combined with further criteria to prevent unintended disclosure of sensitive data. In most cases, specific algorithms were implemented which employ specific types of data transformations and search strategies. In contrast, we see are requirement for flexible solutions which allow the implementation of a broad spectrum of methods. Here, we agree with [14,15], that the suitability of methods depends on use cases. As efficient generic solutions do not exist, and as many approaches have unclear performance characteristics, we will also address performance questions. They are of relevance in situations which require near real-time updates, e.g. when the course of an infectious disease is analyzed over different areas.

We will present a flexible and efficient approach to distributed data anonymization in the semi-honest model. It is based upon a secure multi-party computing (SMC) protocol, which constructs an encrypted global view out of horizontally or vertically distributed datasets. To this global view a broad spectrum of anonymization algorithms and privacy criteria can be applied. Thus, centralized versions of a large number of data anonymization algorithms are supported, and we will provide a detailed overview in the discussion. We will show the flexibility of our solution by anonymizing data with a broad spectrum of privacy criteria, including k-anonymity, 
                           
                              ℓ
                           
                        -diversity, t-closeness and 
                           
                              δ
                           
                        -presence, using a globally optimal data anonymization algorithm. Most related approaches in the distributed setting implement heuristic methods, as their coding models result in large search spaces. While it has been shown that these heuristics combined with, e.g., local recoding, can outperform optimal algorithms using single-dimensional global recoding in terms of data quality, we chose such an algorithm as these have said to be very well suited for the biomedical domain [14].

We present an extensive analytical and experimental evaluation of our solution and show that it offers highly competitive execution times. The performance of our approach can be accurately estimated with a model that only depends on basic data characteristics. Our protocol relaxes the guarantees of traditional secure multiparty computations by exchanging non-anonymized – but encrypted – subsets of the data. We present effective means to lower privacy risks and discuss a trade-off between privacy, data quality and efficiency. Together with estimates derived from our model, this can be utilized to tailor our method to project specific requirements.

@&#BACKGROUND@&#

A typical approach for anonymization is to introduce fuzziness. In this work, we focus on the most common transformation methods: generalization and suppression. For an overview of further techniques, such as perturbation or permutation, the interested reader is referred to [16].


                        Generalization is often implemented with generalization hierarchies. These are transformation rules that allow to iteratively generalize the values of an attribute. Tabular representations of example hierarchies for the categorical attribute Gender with two generalization levels and the discrete numerical attribute ZIP with six generalization levels are shown in Fig. 1
                        . Generalization hierarchies are well suited for transforming categorical attributes and discrete numeric attributes. They can also be used for quasi-identifiers that are continuous numerical attributes. One solution is to formulate transformation rules as functions that dynamically create generalization hierarchies for the values of an attribute in a specific dataset. A more detailed discussion of how such quasi-identifiers can be handled with our method is given in Section 6.5. Suppression is a special kind of generalization, in which a data item is completely suppressed.

Many anonymization algorithms use the rules encoded in generalization hierarchies to transform a dataset. Depending on the type of transformations applied, this results in differently large search spaces. Some algorithms implement local recoding, while others implement global recoding 
                        [17]. The former means that different rules can be applied to equal data items, whereas the latter means that the same rule is applied. When single-dimensional recoding is implemented, the data items are values of an individual column, whereas multi-dimensional recoding means that data items are combinations of values from different columns, e.g., complete tuples [17]. Multidimensional global recoding means that the same rule is always applied to equal tuples. From the perspective of a single attribute, this results in local recoding of its values.

The method of generalization can be distinguished into full-domain generalization or subtree generalization 
                        [16]. The former means that the entire domain of a data item is transformed to a more general domain (i.e., level) of its generalization hierarchy [18]. The latter means that different generalization levels can be applied to different subsets of data items from the same domain.

Generalization-based techniques are sometimes distinguished by whether they are hierarchy-based or partition-based 
                        [17]. Partition-based algorithms are often used for continuous numerical attributes and require the existence of a total order on the data items. They generalize data items by partitioning them into ranges. Hierarchy-based approaches are often used for categorical and discrete numeric attributes and require the existence of generalization hierarchies.

We will now present a short overview of a broad spectrum of state-of-the-art anonymization algorithms. Apart from some restrictions, most of them are supported by our approach. A detailed discussion is presented in Section 6.2.

Optimal data anonymization algorithms often implement hierarchy-based global recoding with single-dimensional full-domain generalization to reduce the size of the search space. As this is a restrictive coding model that potentially results in much loss of information, suppression is added as a multi-dimensional global recoding technique. As a result, outliers are removed from the dataset as long as the total number of suppressed tuples remains under a given threshold. This significantly reduces the need for generalization to enforce the given privacy criteria. Examples include Incognito 
                        [19], OLA 
                        [14] or Flash 
                        [20].


                        Datafly is an early algorithm that solves the k-anonymity problem with a greedy search strategy [21]. It implements single-dimensional full-domain global recoding and uses the size of the domain of an attribute as a heuristic for selecting the next attribute to generalize.

Approaches that implement subtree-based generalization have to deal with much larger search spaces and most of them therefore implement heuristic strategies. Attribute Utility Motivated k-Anonymization (AUM) is a hierarchy-based approach proposed specifically for biomedical data [22]. It uses multi-dimensional global recoding and implements a heuristic search process. Top-Down Specialization is also a heuristic algorithm that uses hierarchy- and subtree-based generalization with single-dimensional global recoding [23]. Mondrian 
                        [24] is a partition-based heuristic search strategy that uses multi-dimensional global recoding.

Clustering algorithms implement local recoding and many are hierarchy-based. The approaches from [25,26] and the Sequential Clustering Algorithm from [15] use information derived from generalization hierarchies as distance measures, while the algorithm from [27] additionally uses generalization graphs that are built automatically.

Various extensions of the k-anonymity criterion exist. Many aim at preventing an attacker from learning sensitive information about data subjects from a k-anonymized dataset. The most well-known ones are 
                           
                              ℓ
                           
                        -diversity [28] and t-closeness [29]. LKC-privacy has been proposed in work on distributed data anonymization and is a relaxed abstraction of k-anonymity and 
                           
                              ℓ
                           
                        -diversity [30]. Moreover, the 
                           
                              δ
                           
                        -presence criterion aims at preventing attackers from inferring the presence of an individuals’ tuple in a dataset [31].

Some less wide-spread privacy criteria also exist. 
                           
                              (
                              α
                              ,
                              k
                              )
                           
                        -Anonymity [32] and p-sensitive k-anonymity [33] aim at preventing attackers from performing linkage attacks and from learning sensitive associations at the same time, while 
                           
                              (
                              k
                              ,
                              e
                              )
                           
                        -anonymity [34] aims at protecting sensitive numeric attributes. 
                           
                              (
                              ∊
                              ,
                              m
                              )
                           
                        -Anonymity [35] was proposed to protect sensitive numeric attributes from proximity breaches. Such breaches allow attackers to learn that a sensitive value falls in a certain interval with high probability. 
                           
                              
                                 
                                    (
                                    ∊
                                    ,
                                    δ
                                    )
                                 
                                 
                                    k
                                 
                              
                           
                        -Dissimilarity has been presented in [36] as a generalization of such criteria and is thus a generic criterion for protecting datasets against general proximity breaches. m-Invariance [37] aims at enabling the privacy-preserving re-release of data.

A criterion specific to distributed data anonymization is 
                           
                              ℓ
                           
                        -site-diversity. Here, it is required that at least 
                           
                              ℓ
                           
                         different parties contribute a tuple to each equivalence class. The aim is to protect the privacy of the sites, i.e., to prevent attackers from learning which site has contributed which data [38].

In our examples we focus on k-anonymity but, depending on which types of attributes from a dataset are incorporated into this view, a multitude of the aforementioned privacy criteria can be enforced. A detailed discussion of this matter is presented in Section 6.3. An example dataset and a 2-anonymous representation are shown in Fig. 2
                        . The transformation has been performed utilizing single-dimensional full-domain global recoding with the hierarchies from Fig. 1.

Without loss of generality, we assume that the data is stored in one table, where 
                           
                              A
                              =
                              {
                              
                                 
                                    a
                                 
                                 
                                    1
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    a
                                 
                                 
                                    c
                                 
                              
                              }
                           
                         is a set of c attributes and 
                           
                              T
                              =
                              {
                              
                                 
                                    t
                                 
                                 
                                    1
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    t
                                 
                                 
                                    r
                                 
                              
                              }
                           
                         is a set of r tuples over these attributes. The data is distributed amongst n parties, 
                           
                              P
                              =
                              {
                              
                                 
                                    p
                                 
                                 
                                    1
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    p
                                 
                                 
                                    n
                                 
                              
                              }
                           
                        . In general, data can be distributed horizontally or vertically. Horizontal distribution means that the parties involved collect data with the same database schema, but the data entries are from different data subjects, i.e., that 
                           
                              
                                 
                                    p
                                 
                                 
                                    i
                                 
                              
                           
                         manages the tuples 
                           
                              
                                 
                                    T
                                 
                                 
                                    
                                       
                                          p
                                       
                                       
                                          i
                                       
                                    
                                 
                              
                              =
                              {
                              
                                 
                                    t
                                 
                                 
                                    x
                                 
                              
                              |
                              1
                              ⩽
                              x
                              ⩽
                              r
                              }
                           
                         where 
                           
                              
                                 
                                    T
                                 
                                 
                                    
                                       
                                          p
                                       
                                       
                                          y
                                       
                                    
                                 
                              
                              ∩
                              
                                 
                                    T
                                 
                                 
                                    
                                       
                                          p
                                       
                                       
                                          x
                                       
                                    
                                 
                              
                              =
                              ∅
                           
                         for 
                           
                              1
                              ⩽
                              y
                              ,
                              x
                              ⩽
                              n
                           
                         and 
                           
                              y
                              
                              ≠
                              
                              x
                           
                        . Vertical distribution means that the parties collect data for the same subjects, but each party manages different attributes in different schemata, i.e., that 
                           
                              
                                 
                                    p
                                 
                                 
                                    i
                                 
                              
                           
                         manages the attributes 
                           
                              
                                 
                                    A
                                 
                                 
                                    
                                       
                                          p
                                       
                                       
                                          i
                                       
                                    
                                 
                              
                              =
                              {
                              
                                 
                                    a
                                 
                                 
                                    x
                                 
                              
                              |
                              1
                              ⩽
                              x
                              ⩽
                              c
                              }
                           
                         where 
                           
                              
                                 
                                    A
                                 
                                 
                                    
                                       
                                          p
                                       
                                       
                                          y
                                       
                                    
                                 
                              
                              ∩
                              
                                 
                                    A
                                 
                                 
                                    
                                       
                                          p
                                       
                                       
                                          x
                                       
                                    
                                 
                              
                              =
                              ∅
                           
                         for 
                           
                              1
                              ⩽
                              y
                              ,
                              x
                              ⩽
                              n
                           
                         and 
                           
                              y
                              
                              ≠
                              
                              x
                           
                        . In case of vertical distribution, the relationships between tuples in different subsets are represented by a common tuple identifier. A hybrid setup allows for arbitrary combinations of vertical and horizontal distribution.

Jurczyk and Xiong [38] have described three different basic methods of anonymizing distributed data. The integrate-and-anonymize method is the simplest approach [30]. Here, the data is first integrated at a trusted party and then anonymized. As the trusted party temporarily maintains a non-anonymized global dataset, this approach is not feasible for many scenarios. For example, the existence of a trusted party can often not be guaranteed. Moreover, the scientific sharing of non-anonymized individual-level health data requires data use agreements (DUAs), and it must be covered by informed consents, which is not always the case or feasible.

The basic idea of anonymize-and-integrate methods is to first anonymize all local data subsets and then integrate them into a global dataset [30]. In case of horizontal data distribution, this tends to have a negative impact on data quality. For example, enforcing local k-anonymity can lead to more information loss than enforcing global k-anonymity. If data is distributed vertically, the integration of the locally anonymous datasets can result in global non-anonymous datasets. If, e.g., new columns are added to a k-anonymized dataset, the result will in most cases not be k-anonymous anymore. A workaround is to compute an anonymized version of one local dataset and send information about the resulting equivalence classes (in terms of sets of tuple identifiers) to the other parties. These must then generalize their datasets to such a degree that the integrated version maintains the initial equivalence classes.


                        Virtual anonymization is implemented by constructing a Secure Multiparty Computing (SMC) protocol which exposes only anonymized data to all parties. The basic idea of SMC is that n parties, each of with defines an input 
                           
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                           
                         with 
                           
                              1
                              ⩽
                              i
                              ⩽
                              n
                           
                        , compute a function 
                           
                              f
                              (
                              
                                 
                                    x
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    x
                                 
                                 
                                    2
                                 
                              
                              ,
                              …
                              
                                 
                                    x
                                 
                                 
                                    n
                                 
                              
                              )
                           
                         in such a way that each party only gets to know its own input and the result. In general, this approach is computationally more complex than the others but guarantees to preserve the privacy of all subjects.

The SMC paradigm can be based upon three different scenarios [39]. We quickly covered the scenario involving a trusted third party in the previous section and found that it is not applicable in our context. The semi-honest model is a common scenario, which is sometimes also called honest-but-curious. Here it is assumed that all parties adhere to the protocol but try to extract additional information from temporary results and their own input. The model only covers attackers that participate in the computation (insiders). On the other hand of the spectrum, the malicious model defines no restrictions for participants.

SMC protocols are often implemented with commutative encryption. The basic idea is to encrypt data in an order-independent manner, i.e., the order of consecutive encryption and decryption operations with different cryptographic keys does not affect the result. Let 
                           
                              
                                 
                                    E
                                 
                                 
                                    1
                                 
                              
                              (
                              x
                              )
                              ,
                              …
                              ,
                              
                                 
                                    E
                                 
                                 
                                    n
                                 
                              
                              (
                              x
                              )
                           
                         be a set of encryption functions and 
                           
                              
                                 
                                    D
                                 
                                 
                                    1
                                 
                              
                              (
                              x
                              )
                              ,
                              …
                              ,
                              
                                 
                                    D
                                 
                                 
                                    n
                                 
                              
                              (
                              x
                              )
                           
                         be a set of associated decryption functions, i.e., 
                           
                              
                                 
                                    D
                                 
                                 
                                    i
                                 
                              
                              (
                              
                                 
                                    E
                                 
                                 
                                    i
                                 
                              
                              (
                              x
                              )
                              )
                              =
                              x
                           
                         for all 
                           
                              1
                              ⩽
                              i
                              ⩽
                              n
                           
                        . The functions are commutative if 
                           
                              
                                 
                                    E
                                 
                                 
                                    i
                                 
                              
                              (
                              
                                 
                                    E
                                 
                                 
                                    j
                                 
                              
                              (
                              x
                              )
                              )
                              =
                              
                                 
                                    E
                                 
                                 
                                    j
                                 
                              
                              (
                              
                                 
                                    E
                                 
                                 
                                    i
                                 
                              
                              (
                              x
                              )
                              )
                           
                         for all 
                           
                              1
                              ⩽
                              i
                              ,
                              j
                              ⩽
                              n
                           
                        . It follows that 
                           
                              
                                 
                                    D
                                 
                                 
                                    i
                                 
                              
                              (
                              
                                 
                                    E
                                 
                                 
                                    i
                                 
                              
                              (
                              
                                 
                                    E
                                 
                                 
                                    j
                                 
                              
                              (
                              x
                              )
                              )
                              )
                              =
                              
                                 
                                    D
                                 
                                 
                                    i
                                 
                              
                              (
                              
                                 
                                    E
                                 
                                 
                                    j
                                 
                              
                              (
                              
                                 
                                    E
                                 
                                 
                                    i
                                 
                              
                              (
                              x
                              )
                              )
                              )
                              =
                              
                                 
                                    E
                                 
                                 
                                    j
                                 
                              
                              (
                              x
                              )
                           
                        , i.e., arbitrary permutations of a set of cryptographic operations yield the same result. In this work, we use a deterministic encryption algorithm, which means that the same plaintext always results in the same ciphertext.

Several methods for anonymizing distributed data have been proposed in the literature. In this section we focus on approaches that implement the virtual anonymization methodology.

An algorithm-independent approach for k-anonymizing vertically distributed data with a secure set intersection method has been described in [40]. The protocol starts with all parties commutatively encrypting the common tuple identifiers. Each party iteratively transforms its local data subset and builds sets of common tuple identifiers that fall into one group. The sets of identifiers from all parties are then intersected. A transformation is globally anonymous when all intersections contain at least k common tuple identifiers. For secure set intersection, the authors use a protocol based on probabilistic homomorphic encryption [41].

Mohammed et al. propose a distributed implementation of Top-Down Specialization to k-anonymize vertically distributed datasets [42]. The algorithm starts with a maximally generalized dataset. In each step, one party is selected that specializes its attributes in the global dataset as far as the dataset remains anonymous. This specialized dataset is then passed to the next party, which again specializes its attributes. The process halts when no further specialization is possible without violating the privacy criterion. As a result, the protocol only exchanges anonymized versions of a dataset. The party which is to anonymize its subset in each step is selected greedily based on a heuristic score that measures information loss by means of a dataset’s entropy. The optimal party for each iteration is determined by distributing the dataset to all parties and selecting the party whose transformation results in the minimal score.

The work from [42] was extended for horizontally distributed health data in [30]. In this variant, the parties select a supervisor that collects a representative tuple and the sizes from all groups of quasi-identifiers, so called count statistics, from all parties. This allows to globally evaluate the anonymity criterion by matching the tuples and accumulating the counts with the secure summation protocol proposed in [43]. Analogously to the vertical scenario, the solution space is traversed with a greedy heuristic.

In [38] a distributed implementation of Mondrian for the horizontal setup is presented. It implements the same algorithm with secure primitives, i.e., privacy-preserving summation, minimum, maximum and median operators. The secure minimum and maximum protocol enables a master party to guide the anonymization process, while the secure median operator is used to distribute information about the next generalization. The privacy criterion is evaluated using the secure summation protocol.

In [44] a distributed implementation of the Sequential Clustering Algorithm is presented. The approach uses SMC primitives: secure summation and secure logical and. To calculate the number of elements in a given cluster and for measuring the induced information loss the approach uses the summation protocol. To generalize a distributed cluster, the secure protocol for the logical and operator is utilized.

Zhong et al. proposed a protocol for extracting a k-anonymous subset out of distributed datasets [45]. As the approach is not generalization-based and only extracts a subset of the data, we do not further consider it in this work.

@&#METHODS@&#

In this section we present our novel method for computing an anonymized and integrated dataset. We first describe our protocol, followed by examples for vertically and horizontally distributed data to clarify the approach. A description of implementation details, followed by a model of the protocol closes the section.

As our method is based on generalization hierarchies, we assume that such hierarchies have been constructed prior to its execution. In case of continuous variables, hierarchies can also be expressed as functions that are executed on the local data subsets. These functions must be designed in a way that the rules generated for equal values are consistent amongst the different subsets. An example would be to incrementally reduce the precision of numeric values in the range 
                           
                              [
                              0
                              ,
                              1
                              ]
                           
                         by one digit. For the sake of clarity, we use discrete variables in our examples and assume that the generalization hierarchies have been materialized prior to executing our protocol. We assume that all hierarchies are known by all parties prior to the anonymization process. In practice, the integration of distributed data will most likely lead to consistency problems. We assume that data integration and cleanup has happened before starting the protocol and that no consistency problems remain. We assume that the relationships between tuples in vertical subsets are represented by a common tuple identifier (TID) and that horizontal subsets do not overlap. Similar to most approaches, we assume a semi-honest security model and the existence of secure communication channels (e.g., SSL/TLS connections with certificates) to protect the protocol against external attackers. Moreover, we assume a multi-way negotiation of cryptographic algorithms and parameters prior to the execution of the protocol. This includes the deterministic and commutative encryption algorithm and its parameters, such as key and block sizes. Finally, we assume that each party has generated a set of random keys, one for each of its attributes. We will present a discussion of the prerequisites in Section 6.5 and outline extensions of our approach that relax these assumptions.

@&#OVERVIEW@&#

When anonymizing data, four different types of attributes are typically distinguished. Obviously identifying attributes (e.g., names) are removed from the dataset. This is done by the participants prior to the anonymization process. Quasi-identifying attributes are generalized, while sensitive attributes are preserved as-is and it is made sure that they fulfill privacy criteria such as 
                           
                              ℓ
                           
                        -diversity or t-closeness. Insensitive attributes pose no privacy threat and they are thus preserved as-is. For the sake of simplicity, we handle insensitive attributes analogously to quasi-identifying and sensitive attributes in our protocol.

The basic idea of our method is to encrypt quasi-identifying and sensitive attributes in a way that allows performing anonymization processes on the encrypted representation. To this end, firstly, an encrypted global view of the dataset and the generalization hierarchies is constructed. The cryptographic functions are deterministic, meaning that the same plaintext always results in the same ciphertext. Secondly, an anonymization algorithm is applied to the encrypted representation of the dataset using the encrypted generalization hierarchies. This is possible, because the same cryptographic operations are applied to the dataset and the generalization hierarchies. Finally, the anonymized encrypted dataset is decrypted, yielding an integrated and anonymized representation of the input datasets.

The integration phase is different for the horizontal and vertical scenario. In the horizontal setup, permutation is employed to prevent privacy threats and duplicate generalization rules must be removed. In the vertical scenario, the encrypted dataset is sorted by the tuple identifiers, which are then removed.

In our method, the parties are ordered in a closed circle, i.e., the right neighbor of 
                           
                              
                                 
                                    p
                                 
                                 
                                    i
                                 
                              
                           
                         is defined as 
                           
                              
                                 
                                    p
                                 
                                 
                                    1
                                    +
                                    (
                                    i
                                    
                                    mod
                                    
                                    n
                                    )
                                 
                              
                           
                        . The data subset maintained by 
                           
                              
                                 
                                    p
                                 
                                 
                                    i
                                 
                              
                           
                         is denoted by 
                           
                              
                                 
                                    d
                                 
                                 
                                    i
                                 
                              
                           
                        . An example setup with three parties is shown in Fig. 3
                        . Our protocol implements two different types of communication patterns, which we call sequential round robin (SRR) and parallel round robin (PRR). As can be seen in Fig. 3, one party, e.g., 
                           
                              
                                 
                                    p
                                 
                                 
                                    1
                                 
                              
                           
                        , initiates an SRR phase by sending a data packet, e.g., 
                           
                              
                                 
                                    d
                                 
                                 
                                    1
                                 
                              
                           
                        , to its right (R-SRR) or left (L-SRR) neighbor. This party processes the packet and passes it onto its right or left neighbor. The process halts after 
                           
                              n
                              -
                              1
                           
                         steps, when the neighbor of the initiating party receives the data packet. In contrast, a PRR is initiated simultaneously by all parties and each packet is passed along the circle until the neighbor of the originating party received and processed the packet. For example, 
                           
                              
                                 
                                    p
                                 
                                 
                                    3
                                 
                              
                           
                         first processes its own data (
                           
                              
                                 
                                    d
                                 
                                 
                                    3
                                 
                              
                           
                        ), then receives and processes the data from 
                           
                              
                                 
                                    p
                                 
                                 
                                    2
                                 
                              
                           
                        , and finally the data from 
                           
                              
                                 
                                    p
                                 
                                 
                                    1
                                 
                              
                           
                        .

When compared to each other, PRR phases are more efficient that SRR phases because they are parallelized. Synchronization points are only required at the end of each phase, where the parties have to wait until all messages have been processed by all parties. L-PRR and R-PRR as well as L-SRR and R-SRR respectively are only different in the direction in which the data is passed through the ring. This distinguishing is necessary, because encryption and integration have to be implemented with opposite directions. One of the parties acts as a master which receives and anonymizes the integrated view of the dataset.

Before the encryption phase, each party extracts the relevant subsets of the global generalization hierarchies according to the local domain of each of its quasi-identifiers. As a result, the local hierarchies only reflect generalization rules for the values contained in the local data subset. The preprocessing phase of our protocol, where an encrypted global dataset is constructed, can be divided into two sub-phases. During the first sub-phase (encryption) all local data subsets and corresponding generalization hierarchies are commutatively encrypted by all parties in a right-parallel round robin phase. As is shown in Fig. 4
                        , the preprocessing phase ends with an integration phase where in a left-sequential round robin manner the local data subsets and hierarchies of each party are integrated (integration). As the generalization hierarchies are encrypted analogously to the data, the resulting rules can be applied to the encrypted dataset which can thus be anonymized during the anonymization phase. The postprocessing phase consists of decrypting the anonymized dataset by all parties in a sequential round robin phase (decryption).

First, a R-PRR phase is utilized to commutatively and deterministically encrypt all data by all parties. Each party uses its keys to encrypt its data subset. The hierarchies are encrypted analogously, utilizing the key assigned to the attribute to which the hierarchy is to be applied. As the encryption function is deterministic, the same plaintext always results in the same ciphertext. As each column is encrypted with a different key, the same plaintexts from different columns result in different ciphertexts. Each party then sends all data to its right neighbor, which applies the same process.

The encryption phase halts, when each party holds a version of the data from its left neighbor that has been commutatively encrypted by all parties. In case of vertical data distribution, the parties now sort the data subsets according to the lexicographical order induced by the encrypted tuple identifiers. These identifiers can now be removed, as the order of the data items is sufficient to integrate the different subsets.

The integration phase is initiated by the left neighbor of the master party, e.g., 
                              
                                 
                                    
                                       p
                                    
                                    
                                       3
                                    
                                 
                              
                           . It implements a sequential round robin phase in the opposite direction of the encryption phase. 
                              
                                 
                                    
                                       P
                                    
                                    
                                       3
                                    
                                 
                              
                            sends the fully encrypted data from 
                              
                                 
                                    
                                       p
                                    
                                    
                                       1
                                    
                                 
                              
                            back to its left neighbor, 
                              
                                 
                                    
                                       p
                                    
                                    
                                       2
                                    
                                 
                              
                           , which integrates the data with the fully encrypted data from 
                              
                                 
                                    
                                       p
                                    
                                    
                                       3
                                    
                                 
                              
                           , and sends it to 
                              
                                 
                                    
                                       p
                                    
                                    
                                       1
                                    
                                 
                              
                           , which again integrates it with its fully encrypted data from 
                              
                                 
                                    
                                       p
                                    
                                    
                                       2
                                    
                                 
                              
                           . In case of vertical data distribution, the subsets are integrated by concatenating the columns and in case of horizontal data distribution, the subsets are integrated by concatenating the rows. In the latter scenario, generalization hierarchies for the same attributes from different parties must also be merged. In this context, duplicate rules must be eliminated, which result if two or more parties share the same data item and thus included the same generalization rule into their local hierarchies. Conflicts cannot occur, as hierarchies are defined globally (cf. Section 6.5). The process halts when the master party receives an encrypted global view of the dataset together with the encrypted generalization hierarchies.

In this phase the integrated and encrypted dataset is anonymized using a centralized anonymization algorithm and the encrypted generalization hierarchies. This is possible, because the generalization process is only a substitution of values with more generalized ones. As the semantic information is hidden in the encrypted generalization hierarchies, there is no need to know the plaintext values. Many of the previously presented algorithms and criteria are supported. More details will be given in Sections 6.2 and 6.3. The phase results in an encrypted anonymous dataset.

During the postprocessing phase the anonymous encrypted dataset is decrypted in a sequential round robin phase initiated by the master party. To this end, the party decrypts the dataset with its private set of keys and sends it to its right neighbor. This process is repeated until the left neighbor of the master receives an unencrypted anonymized version of the global dataset.

Similar to other approaches, e.g., [38,44], our protocol relaxes the privacy guarantees of traditional multi-party computations, where no information, except the result can be obtained by participants. Additional information is of course only available to the parties executing the protocol, and not to external entities. In our protocol, the parties process non-anonymized data subsets that are encrypted by some or all parties. As the data are encrypted deterministically on a cell level, the parties could extract statistical patterns from these representations. These patterns could be used to identify the subset of the integrated data that has been contributed by the party, which potentially allows to infer information about data from the other participants. To prevent this, each party shuffles the rows of the processed data subset randomly before sending it to the next party during the encryption phase. As a result, information about the order of the data items is removed and the construction of patterns is prevented. A collection of data distributions is still possible and these could be used for frequency attacks. See Section 6.1 for a more detailed discussion and countermeasures.

The protocol is designed in a way in which the master party is the only party that ever receives a fully encrypted version of its own data. At this point the master party’s data has been randomly merged with as many other subsets as possible (
                              
                                 n
                                 -
                                 2
                              
                           ), as can be seen from the example in Fig. 5
                           . 
                              
                                 
                                    
                                       P
                                    
                                    
                                       3
                                    
                                 
                              
                            sends the encrypted data from 
                              
                                 
                                    
                                       p
                                    
                                    
                                       1
                                    
                                 
                              
                            to 
                              
                                 
                                    
                                       p
                                    
                                    
                                       2
                                    
                                 
                              
                           , which randomly merges (i.e., merges and permutes) these data with the data from 
                              
                                 
                                    
                                       p
                                    
                                    
                                       3
                                    
                                 
                              
                            and sends it to 
                              
                                 
                                    
                                       p
                                    
                                    
                                       1
                                    
                                 
                              
                           . When 
                              
                                 
                                    
                                       p
                                    
                                    
                                       1
                                    
                                 
                              
                            receives its own subset it has thus been randomly merged with the data from 
                              
                                 
                                    
                                       p
                                    
                                    
                                       3
                                    
                                 
                              
                           . Analogously to the encryption process, data is randomly permuted during the decryption phase to prevent the mapping of local subsets to the anonymized global view.

Each party maintains one key for each attribute. We denote the TID with 1 and order the attributes according to their position in Fig. 1, i.e., Gender
                        
                        =
                        
                        2, Age
                        
                        =
                        
                        3 and ZIP
                        
                        =
                        
                        4. For example, 
                           
                              
                                 
                                    E
                                 
                                 
                                    23
                                 
                              
                              (
                              45
                              )
                           
                         denotes that 
                           
                              
                                 
                                    p
                                 
                                 
                                    2
                                 
                              
                           
                         encrypts age 45 with its respective key for attribute number 3.

In the following section, we present an example that anonymizes a vertically distributed version of the data from Fig. 2. As is shown in Fig. 6
                           , we assume three parties, from which 
                              
                                 
                                    
                                       p
                                    
                                    
                                       1
                                    
                                 
                              
                            stores the attribute ZIP, 
                              
                                 
                                    
                                       p
                                    
                                    
                                       2
                                    
                                 
                              
                            stores the attribute Age and 
                              
                                 
                                    
                                       p
                                    
                                    
                                       3
                                    
                                 
                              
                            stores the attribute Gender. Hierarchies for the example data are sketched in Fig. 1. The hierarchy for Gender is held by 
                              
                                 
                                    
                                       p
                                    
                                    
                                       3
                                    
                                 
                              
                            and the hierarchy for ZIP is held by 
                              
                                 
                                    
                                       p
                                    
                                    
                                       1
                                    
                                 
                              
                           . We focus on one party, 
                              
                                 
                                    
                                       p
                                    
                                    
                                       2
                                    
                                 
                              
                           , in the remainder of this section.

Firstly, 
                              
                                 
                                    
                                       p
                                    
                                    
                                       2
                                    
                                 
                              
                            generates random keys for each of the columns and commutatively encrypts the tuple identifiers and the payload. The result is shown in Fig. 7
                           . Now the data is permuted randomly and sent to the right neighbor, 
                              
                                 
                                    
                                       p
                                    
                                    
                                       3
                                    
                                 
                              
                            in our example. Analogously, 
                              
                                 
                                    
                                       p
                                    
                                    
                                       2
                                    
                                 
                              
                            receives the encrypted data from 
                              
                                 
                                    
                                       p
                                    
                                    
                                       1
                                    
                                 
                              
                           . It encrypts the data, randomly permutes it and sends it to 
                              
                                 
                                    
                                       p
                                    
                                    
                                       3
                                    
                                 
                              
                           .

Secondly, when the encryption phase has terminated, the left neighbor of the master, 
                              
                                 
                                    
                                       p
                                    
                                    
                                       3
                                    
                                 
                              
                            in our example, starts the integration process. It sorts the data from 
                              
                                 
                                    
                                       p
                                    
                                    
                                       1
                                    
                                 
                              
                            according to the encrypted TIDs, removes them and sends the data to 
                              
                                 
                                    
                                       p
                                    
                                    
                                       2
                                    
                                 
                              
                           , which applies the same process and merges the subsets. This process is sketched in Fig. 8
                           . As a result, 
                              
                                 
                                    
                                       p
                                    
                                    
                                       1
                                    
                                 
                              
                            receives a global view of the encrypted dataset. As is shown for the sake of clarity, all values in one column have been encrypted by the parties in the same order, but the orderings differ between columns.

Thirdly, 
                              
                                 
                                    
                                       p
                                    
                                    
                                       1
                                    
                                 
                              
                            enforces a privacy criterion. In our example we assume 2-anonymity, as is shown in Fig. 9
                           . During the anonymization process 
                              
                                 
                                    
                                       p
                                    
                                    
                                       1
                                    
                                 
                              
                            finds the optimal solution by replacing the encrypted values for Age with the values of the encrypted generalization hierarchy on level one and by replacing the values for ZIP with the corresponding values on level three. The attribute Gender can be left ungeneralized.

Finally, after the decryption phase, which is initiated by 
                              
                                 
                                    
                                       p
                                    
                                    
                                       1
                                    
                                 
                                 ,
                                 
                                    
                                       p
                                    
                                    
                                       3
                                    
                                 
                              
                            holds an unencrypted anonymized version of the global dataset. The result equals the data that would have resulted from anonymizing the dataset at a trusted third party, i.e., the data from Fig. 1 in Section 2.2.

In the following section, we present an example that anonymizes a horizontally distributed version of the data from Fig. 6 in Section 2.2. As is shown in Fig. 10
                           , we again assume three parties, from which 
                              
                                 
                                    
                                       p
                                    
                                    
                                       1
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       p
                                    
                                    
                                       2
                                    
                                 
                              
                            store three tuples while 
                              
                                 
                                    
                                       p
                                    
                                    
                                       3
                                    
                                 
                              
                            stores two tuples.

The set of relevant local generalization rules extracted from the global generalization hierarchies for the data subset of 
                              
                                 
                                    
                                       p
                                    
                                    
                                       1
                                    
                                 
                              
                            are shown in Fig. 11
                           .

Each party again generates a set of random keys and encrypts all columns and generalization hierarchies with the associated keys. The encrypted subset from party 
                              
                                 
                                    
                                       p
                                    
                                    
                                       1
                                    
                                 
                              
                            in our example is shown in Fig. 12
                           .

Next, the resulting data is sent to the right neighbor. 
                              
                                 
                                    
                                       p
                                    
                                    
                                       1
                                    
                                 
                              
                            sends its data to 
                              
                                 
                                    
                                       p
                                    
                                    
                                       2
                                    
                                 
                              
                           , which encrypts it with its own set of secret keys and sends it to 
                              
                                 
                                    
                                       p
                                    
                                    
                                       3
                                    
                                 
                              
                           . When 
                              
                                 
                                    
                                       p
                                    
                                    
                                       3
                                    
                                 
                              
                            receives the data originating from 
                              
                                 
                                    
                                       p
                                    
                                    
                                       1
                                    
                                 
                              
                            it encrypts and permutes it and sends it back to 
                              
                                 
                                    
                                       p
                                    
                                    
                                       2
                                    
                                 
                              
                           . 
                              
                                 
                                    
                                       P
                                    
                                    
                                       2
                                    
                                 
                              
                            randomly combines the data from 
                              
                                 
                                    
                                       p
                                    
                                    
                                       3
                                    
                                 
                              
                            with its own data and sends it to 
                              
                                 
                                    
                                       p
                                    
                                    
                                       1
                                    
                                 
                              
                            which again randomly combines it with its own data. 
                              
                                 
                                    
                                       p
                                    
                                    
                                       1
                                    
                                 
                              
                            now holds a global view of the encrypted dataset, as is shown in Fig. 13
                           . In contrast to the vertical scenario, the order in which values have been encrypted is now different within each column but consistent within each row. Note that this difference is only emphasized for the sake of clarity and is not relevant when executing the protocol due to the use of commutative encryption.

The encrypted global generalization hierarchies are an integration of local generalization hierarchies. As can be seen from the example in Fig. 14
                            the order of which values have been encrypted is different between rows but consistent within each row. This is due to the fact that the individual generalization rules originate from different local hierarchies. In our example, the first rule is from the local hierarchy of 
                              
                                 
                                    
                                       p
                                    
                                    
                                       1
                                    
                                 
                              
                            and the second rule from 
                              
                                 
                                    
                                       p
                                    
                                    
                                       2
                                    
                                 
                              
                           .

Anonymization and decryption are applied analogously to the vertical scenario and yield the same result as anonymizing the data at a trusted third party.

@&#IMPLEMENTATION DETAILS@&#

For evaluating our approach, we developed a prototype based on the Java platform. For data exchange, we employ dictionary compression and represent all data as two-dimensional arrays of structural information and associated dictionaries. As is shown in Fig. 15
                        , each dictionary contains a set of values that can be referenced by means of row indexes.

In our method the same set of keys will be used for encrypting several plaintexts. When choosing a commutative encryption algorithm, it is therefore important to make sure that the method is not vulnerable in such a scenario. For example, using a simple XOR cipher would be unsafe, as (part of) the key could be reconstructed from knowing the ciphertext for a known plaintext. Our requirements for an encryption algorithm are the same as in [46].

In our prototype we use a variant of the Pohlig–Hellman Cipher (see Section 16.2 in [47]). Instead of its classic implementation based on modular exponentiation, we use a variant based on multiplications on elliptic curves. The advantage of Elliptic Curve Cryptography (ECC) is that it requires smaller bit lengths to achieve security levels comparable to standard methods. We use the 192 bit elliptic curve prime192v1, which is standardized in ANSI X9.62 [48], resulting in a key and block size of 192 bits. The security level is comparable to 1536 bit RSA encryption but with increased performance and reduced data volumes [49]. Cryptographic operations are implemented with the GNU Multiple Precision Arithmetic Library 
                        [50]. The encryption process is multi-threaded and each thread handles a distinct set of data items.

We make sure that all values have a constant plaintext length, which allows to encrypt a data item with only one application of ECC-Pohlig–Hellmann. To this end, each party maintains a secret dictionary that maps its data items to 160 bit SHA-1 hash values. The encoding of plaintext data items as hash values is performed transparently during the encryption phase. For decoding the data, we implement an additional sequential phase in which each party replaces the hash values with the actual data items. The protocol implemented by our prototype is shown in Fig. 16
                        .

In this section, we present means to estimate the execution times and the exchanged data volumes for our algorithms. The overall execution times are dominated by the time needed for cryptographic operations and data exchange, which are thus the only operations considered in our model. We denote the number of parties with n, the number of rows in the global dataset with r and the number of columns with c. The total number of distinct values in the global dataset is denoted with d. We assume a homogeneous setup in which each machine can execute a commutative cryptographic operation in time 
                           
                              
                                 
                                    M
                                 
                                 
                                    c
                                 
                              
                           
                         and the machines can communicate with a bandwidth of 
                           
                              
                                 
                                    M
                                 
                                 
                                    b
                                 
                              
                           
                        . Commutatively encrypted values are assumed to have a bit length of 
                           
                              
                                 
                                    L
                                 
                                 
                                    c
                                 
                              
                           
                        . The global dataset can be represented by an array of size 
                           
                              c
                              ·
                              r
                           
                         for structural information and a dictionary of size d for data elements. Each element in the structural array is assumed to have a bit length of 
                           
                              
                                 
                                    L
                                 
                                 
                                    a
                                 
                              
                           
                        . For the scope of this article, we assume equal data distribution in our analysis. In the horizontal case each party holds the same number of rows 
                           
                              
                                 
                                    r
                                 
                                 
                                    n
                                 
                              
                           
                        , columns c and distinct values d, extracted from the global dataset. In the vertical setup, we assume that each party has the same number of columns 
                           
                              
                                 
                                    c
                                 
                                 
                                    n
                                 
                              
                           
                        , rows r and that the distinct values of the global dataset are equally distributed, i.e. each party has 
                           
                              
                                 
                                    d
                                 
                                 
                                    n
                                 
                              
                           
                         distinct values. The formulas can be extended to cover heterogeneous setups and unequal data distributions.

During the encryption phase, the workload is parallelized amongst all parties. The execution time is therefore determined by the workload for one party. In case of vertical data distribution, each party holds r distinct tuple identifiers and 
                              
                                 
                                    
                                       d
                                    
                                    
                                       n
                                    
                                 
                              
                            distinct data items. Each party must encrypt the data from all parties, resulting in a total of 
                              
                                 
                                    
                                       e
                                    
                                    
                                       v
                                    
                                 
                                 =
                                 n
                                 ·
                                 (
                                 r
                                 +
                                 
                                    
                                       d
                                    
                                    
                                       n
                                    
                                 
                                 )
                              
                            commutative encryptions. In case of horizontal data distribution, each party performs 
                              
                                 
                                    
                                       e
                                    
                                    
                                       h
                                    
                                 
                                 =
                                 n
                                 ·
                                 d
                              
                            commutative encryptions. As the data is already integrated in the decryption phase, there is no difference for horizontal or vertical distribution. Decryption is a sequential phase in which each party must decrypt all data items, i.e., 
                              
                                 
                                    
                                       d
                                    
                                    
                                       vh
                                    
                                 
                                 =
                                 n
                                 ·
                                 d
                              
                           . In summary, the execution time for cryptographic operations can be estimated with 
                              
                                 
                                    
                                       t
                                    
                                    
                                       v
                                    
                                    
                                       c
                                    
                                 
                                 =
                                 (
                                 
                                    
                                       e
                                    
                                    
                                       v
                                    
                                 
                                 +
                                 
                                    
                                       d
                                    
                                    
                                       vh
                                    
                                 
                                 )
                                 ·
                                 
                                    
                                       M
                                    
                                    
                                       c
                                    
                                 
                              
                            in the vertical setup and 
                              
                                 
                                    
                                       t
                                    
                                    
                                       h
                                    
                                    
                                       c
                                    
                                 
                                 =
                                 (
                                 
                                    
                                       e
                                    
                                    
                                       h
                                    
                                 
                                 +
                                 
                                    
                                       d
                                    
                                    
                                       vh
                                    
                                 
                                 )
                                 ·
                                 
                                    
                                       M
                                    
                                    
                                       c
                                    
                                 
                              
                            in the horizontal setup.

The number of messages exchanged in each of the four phases can be calculated based on the communication pattern, which is independent of data distribution. During phase one, each data subset is sent to 
                              
                                 n
                                 -
                                 1
                              
                            parties, resulting in 
                              
                                 
                                    
                                       f
                                    
                                    
                                       v
                                    
                                    
                                       1
                                    
                                 
                                 =
                                 
                                    
                                       f
                                    
                                    
                                       h
                                    
                                    
                                       1
                                    
                                 
                                 =
                                 n
                                 ·
                                 (
                                 n
                                 -
                                 1
                                 )
                              
                            message exchanges. In the second phase the data subsets are integrated iteratively, which we model as one message being sent 
                              
                                 n
                                 -
                                 1
                              
                            times, another message being sent 
                              
                                 n
                                 -
                                 2
                              
                            times and so on. This results in 
                              
                                 
                                    
                                       f
                                    
                                    
                                       v
                                    
                                    
                                       2
                                    
                                 
                                 =
                                 
                                    
                                       f
                                    
                                    
                                       h
                                    
                                    
                                       2
                                    
                                 
                                 =
                                 n
                                 
                                    
                                       (
                                       n
                                       -
                                       1
                                       )
                                    
                                    
                                       2
                                    
                                 
                              
                            message exchanges. During phases three and four, each message is sent 
                              
                                 
                                    
                                       f
                                    
                                    
                                       v
                                    
                                    
                                       34
                                    
                                 
                                 =
                                 
                                    
                                       f
                                    
                                    
                                       h
                                    
                                    
                                       34
                                    
                                 
                                 =
                                 n
                                 -
                                 1
                              
                            times, as each message is sent to all parties apart from the originator.

In the first phase of the vertical scenario, each party holds a data subset of size 
                              
                                 
                                    
                                       s
                                    
                                    
                                       v
                                    
                                    
                                       1
                                    
                                 
                                 =
                                 (
                                 
                                    
                                       c
                                    
                                    
                                       n
                                    
                                 
                                 +
                                 1
                                 )
                                 ·
                                 r
                                 ·
                                 
                                    
                                       L
                                    
                                    
                                       a
                                    
                                 
                                 +
                                 (
                                 
                                    
                                       d
                                    
                                    
                                       n
                                    
                                 
                                 +
                                 r
                                 )
                                 ·
                                 
                                    
                                       L
                                    
                                    
                                       c
                                    
                                 
                              
                           , as each party is assumed to hold 
                              
                                 
                                    
                                       c
                                    
                                    
                                       n
                                    
                                 
                              
                            columns and an additional column with tuple identifiers and 
                              
                                 
                                    
                                       d
                                    
                                    
                                       n
                                    
                                 
                              
                            distinct attribute values plus r tuple identifiers. The data held by each party in the second phase equals the data from the first phase without tuple identifiers, i.e., 
                              
                                 
                                    
                                       s
                                    
                                    
                                       v
                                    
                                    
                                       2
                                    
                                 
                                 =
                                 
                                    
                                       c
                                    
                                    
                                       n
                                    
                                 
                                 ·
                                 r
                                 ·
                                 
                                    
                                       L
                                    
                                    
                                       a
                                    
                                 
                                 +
                                 
                                    
                                       d
                                    
                                    
                                       n
                                    
                                 
                                 ·
                                 
                                    
                                       L
                                    
                                    
                                       c
                                    
                                 
                              
                           . In the first and second phase of the horizontal scenario each party holds a data subset of size 
                              
                                 
                                    
                                       s
                                    
                                    
                                       h
                                    
                                    
                                       12
                                    
                                 
                                 =
                                 
                                    
                                       r
                                    
                                    
                                       n
                                    
                                 
                                 ·
                                 c
                                 ·
                                 
                                    
                                       L
                                    
                                    
                                       a
                                    
                                 
                                 +
                                 d
                                 ·
                                 
                                    
                                       L
                                    
                                    
                                       c
                                    
                                 
                              
                           , as each party is assumed to hold d distinct values. In the third and fourth phase, each message contains a representation of the global dictionary compressed dataset, regardless of data distribution. This can be estimated with 
                              
                                 
                                    
                                       s
                                    
                                    
                                       v
                                    
                                    
                                       34
                                    
                                 
                                 =
                                 
                                    
                                       s
                                    
                                    
                                       h
                                    
                                    
                                       34
                                    
                                 
                                 =
                                 c
                                 ·
                                 r
                                 ·
                                 
                                    
                                       L
                                    
                                    
                                       a
                                    
                                 
                                 +
                                 d
                                 ·
                                 
                                    
                                       L
                                    
                                    
                                       c
                                    
                                 
                              
                           .

In summary, the data exchanged in the vertical scenario can be estimated with 
                              
                                 
                                    
                                       d
                                    
                                    
                                       v
                                    
                                 
                                 =
                                 
                                    
                                       f
                                    
                                    
                                       v
                                    
                                    
                                       1
                                    
                                 
                                 ·
                                 
                                    
                                       s
                                    
                                    
                                       v
                                    
                                    
                                       1
                                    
                                 
                                 +
                                 
                                    
                                       f
                                    
                                    
                                       v
                                    
                                    
                                       2
                                    
                                 
                                 ·
                                 
                                    
                                       s
                                    
                                    
                                       v
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 2
                                 ·
                                 
                                    
                                       f
                                    
                                    
                                       v
                                    
                                    
                                       34
                                    
                                 
                                 ·
                                 
                                    
                                       s
                                    
                                    
                                       v
                                    
                                    
                                       34
                                    
                                 
                              
                           , while the data volume in the horizontal scenario can be estimated with 
                              
                                 
                                    
                                       d
                                    
                                    
                                       h
                                    
                                 
                                 =
                                 
                                    
                                       f
                                    
                                    
                                       h
                                    
                                    
                                       1
                                    
                                 
                                 ·
                                 
                                    
                                       s
                                    
                                    
                                       h
                                    
                                    
                                       12
                                    
                                 
                                 +
                                 
                                    
                                       f
                                    
                                    
                                       h
                                    
                                    
                                       2
                                    
                                 
                                 ·
                                 
                                    
                                       s
                                    
                                    
                                       h
                                    
                                    
                                       12
                                    
                                 
                                 +
                                 2
                                 ·
                                 
                                    
                                       f
                                    
                                    
                                       h
                                    
                                    
                                       34
                                    
                                 
                                 ·
                                 
                                    
                                       s
                                    
                                    
                                       h
                                    
                                    
                                       34
                                    
                                 
                              
                           . The time needed for data transfer can thus be estimated by 
                              
                                 
                                    
                                       t
                                    
                                    
                                       h
                                    
                                    
                                       d
                                    
                                 
                                 =
                                 
                                    
                                       d
                                    
                                    
                                       h
                                    
                                 
                                 ·
                                 
                                    
                                       M
                                    
                                    
                                       b
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       t
                                    
                                    
                                       v
                                    
                                    
                                       d
                                    
                                 
                                 =
                                 
                                    
                                       d
                                    
                                    
                                       v
                                    
                                 
                                 ·
                                 
                                    
                                       M
                                    
                                    
                                       b
                                    
                                 
                              
                            respectively.

@&#RESULTS@&#

For our evaluation we used five real-world datasets, most of which have already been utilized for assessing previous work on data anonymization. The datasets include the an excerpt of the 1994 US census database (ADULT), KDD Cup 1998 data (CUP), NHTSA crash statistics (FARS), the American Time Use Survey (ATUS) and the Integrated Health Interview Series (IHIS). The ADULT dataset serves as a de facto standard for the evaluation of anonymization algorithms. An overview over the datasets is shown in Table 1
                     . They cover a wide spectrum, ranging from about 30k to 1.2M rows (2.52MB to 107.56MB) consisting of eight or nine quasi-identifiers, from which we also chose a sensitive attribute. The associated generalization hierarchies feature between two and six levels, resulting in search spaces between 12,960 (ADULT) and 45,000 (CUP) transformations.

In this section, we present estimates for our testbed, which can execute roughly 
                           
                              
                                 
                                    M
                                 
                                 
                                    c
                                 
                              
                              =
                              3000
                           
                         cryptographic operations per second. The network provides a net bandwidth of 
                           
                              
                                 
                                    M
                                 
                                 
                                    b
                                 
                              
                              =
                              88
                              ,
                              000
                              ,
                              000
                           
                         bit/s (Fast Ethernet). Each entry of the structural array requires 
                           
                              
                                 
                                    L
                                 
                                 
                                    a
                                 
                              
                              =
                              32
                           
                         bits. The encrypted values have a size of 
                           
                              
                                 
                                    L
                                 
                                 
                                    c
                                 
                              
                              =
                              192
                           
                         bits.

We compare these estimates with execution times for applying common privacy criteria, which were obtained with the ARX Anonymization Framework 
                        [51] that implements the optimal data anonymization algorithm presented in [20]. As privacy criteria, we chose 5-anonymity, recursive-(4,3)-diversity, 0.2-closeness with hierarchical Earth-Mover’s-Distance and (0.05, 0.15)-presence of 10% random samples as research subsets. t-Closeness and 
                           
                              δ
                           
                        -presence where combined with 5-anonymity. We decided to include numbers for an optimal anonymization algorithm, as this shows the ability of our approach to efficiently implement such algorithms in a distributed setting. We also chose a broad spectrum of privacy criteria to demonstrate the flexibility of our approach. We set 
                           
                              k
                              =
                              5
                           
                         as this represents a rough upper bound, i.e., the execution times will decrease with increasing values, and the parameter is typically used in the healthcare domain. All anonymizations were performed with a 3% suppression rate, which is again a typical parameter.


                        Table 2
                         shows estimated execution times for our protocol with two (2P) or three parties (3P). Note that the estimates only depend on the number of parties and basic statistics about the dataset. It can be seen that, in case of vertical distribution, the execution times of the pre- and postprocessing phase are about one magnitude higher than the execution times of the anonymization phase. It can also be seen that the execution times for two parties are lower than the execution times for three parties. This is due to the fact that in this case the costs are dominated by the number of tuple identifiers, which grows linearly with the number of parties. For the different datasets the execution times increase linearly with the number of rows. In case of horizontal data distribution, the execution times of the pre- and postprocessing phase are roughly equivalent to the anonymization phase. Here, the complexity of the former phase is dominated by the number of distinct values in the dataset. This is also represented in the execution times for the CUP dataset, where, due to the large number of distinct values, the pre- and postprocessing phase is about one magnitude slower than the anonymization phase. Generally, our method adds a notable overhead in case of vertical distribution and an almost negligible overhead in case of horizontal distribution.

Our testbed consisted of three desktop machines, each of which was equipped with a quad-core 3.1GHz Intel Core i5 CPU running a 64-bit Linux 3.2.0 kernel and a 64-bit Sun JVM (1.7.0). The machines were connected with a Fast Ethernet switch (100 Mbit/s).


                           Fig. 17
                            shows the summarized execution times for pre- and postprocessing all datasets with two (2P) and three parties (3P). It can be seen that the estimates from the previous section closely resemble actual execution times. In 13 out of 20 cases our estimates are off by only 1–2s. In four cases they are of by 5–10s and in three cases they are off by 20–50s. The latter three cases correspond to overall times of 10 to 20min.

In the vertical scenario, the execution times range from 21s (ADULT) to 13.2min (IHIS) with two parties. With three parties the execution times increase by roughly a factor of 1.5, i.e., 31s for the ADULT dataset and 19.8min for IHIS. When datasets have a constant number of distinct data values, the execution times scale linearly with the number of rows, as the tuple identifiers in each subset define the cryptographic overhead. As the CUP dataset contains significantly more distinct data items than the other datasets it stands out.

In the horizontal scenario, performance increases significantly. Here, execution times are much more strongly influenced by the total number of distinct data values per dataset. As a result the CUP dataset, which has the highest number of distinct data items, and the IHIS dataset, which has the highest number of rows, require almost the same processing time. When datasets have a constant number of distinct data items, the overall execution times are dominated by exchanging data over the network. The total execution time ranges from 1s for the ADULT dataset with two parties to 30s for the CUP dataset with three parties. The differences between executing the protocol with two or three parties range between a factor of 2.0 for ADULT and 0.73 for the CUP dataset.


                           Fig. 18
                            shows how these execution times are distributed amongst the four phases of the protocol. In the vertical scenario, times are dominated by the encryption phase, where n columns with unique tuple identifiers have to be encrypted in parallel n times (between 84% for CUP and roughly 98% for the other datasets). In the horizontal scenario, times are less influenced by cryptographic operations but by data exchange. As a result, integration, decryption and decoding contribute a significant share of the overall times. The CUP dataset is the only exception, because it contains the most distinct values which increases the cryptographic workload. When comparing the execution with two parties to the execution with three parties, there is a slight decrease in the contribution of encryption and decryption to the overall execution times. The reason is that the exchanged data volumes increase with the number of parties.


                           Fig. 19
                            depicts the total transferred data volumes for all configurations. It can be seen that data volumes range from 4MB for the ADULT dataset with two parties in the horizontal scenario to 690MB for the IHIS dataset with three parties in the vertical setup. Data volumes in the vertical scenario are roughly two times the volumes from the horizontal scenario. This is not directly related to the overall performance, as the execution times of the vertical scenario are dominated by cryptographic operations, whereas the execution times of the horizontal scenario are dominated by data exchange. In the vertical setup, most data is exchanged during the encryption phase, because packet volumes are mostly determined by the number of tuple identifiers. These are dropped before the integration phase. In the horizontal setup, the data volumes of the encryption, integration and decoding phases are roughly equivalent. This shows that they are dominated by the structural arrays, as dictionary compression is differently efficient in these three phases due to data distribution.

In this section, we compare the information loss of our approach with two baseline techniques: anonymize-and-integrate of horizontally and vertically distributed data. Again, we use an optimal anonymization algorithm with global recoding and tuple suppression as an example. In the horizontal case, the local data subsets are anonymized and then integrated into a global dataset [30]. In the vertical scenario, one local dataset is anonymized and the others are generalized to such a degree that an integrated version maintains the initial equivalence classes (see Section 3.1). Both techniques are an interesting baseline, as they are straightforward methods for privacy-preserving anonymization of distributed data. Regarding data distribution, we use the same configurations as in the previous sections. We restrict the comparison to k-anonymity and l-diversity, as t-closeness and 
                              
                                 δ
                              
                           -presence cannot easily be implemented with these approaches.

The results are given in Table 3
                           . It shows the information loss of our approach relative to the information loss of the baseline techniques. Information loss was measured with the non-uniform entropy metric [14]. As can be seen, our approach enables a significant and consistent increase in data quality, with loss of information being reduced by up to 87% (for k-anonymizing the vertically distributed CUP dataset with three parties). Two trends are obvious. First, the decrease in information loss enabled by our approach is stronger in 85% of cases when comparing the vertical scenario to the horizontal scenario. Second, the decrease in information loss enabled by our approach is stronger in 75% of cases when comparing the setups with three parties to the setups with two parties. Note that in the horizontal scenario, the baseline technique implements local recoding, as different generalization strategies can be applied to different horizontal subsets.

@&#DISCUSSION@&#

We will first discuss threats and countermeasures, describing how our approach implements a trade-off between privacy, data quality and efficiency. We will then present a comparison with previous work and justify the preconditions for and assumptions of our method.

Our approach relaxes the privacy guarantees of traditional SMC protocols. In this section we discuss to which extent the additional information leaked by our protocol could be used by participants to compromise the privacy of other parties. We then present countermeasures to mitigate these threats.

An obvious threat results from parties re-identifying their own encrypted contribution to the overall dataset. This allows learning the ciphertext for known plaintext values. In case of horizontal distribution, this knowledge could be used to decode values from other parties during the integration phase, if they share a common attribute value. In case of vertical distribution, the re-identification of own tuples could be used to infer additional information by knowing which other (encrypted) vertical subsets of tuples are associated.

Two countermeasures exist against this threat. First, the permutation of tuples as described in Section 4.3.5 prevents the construction of detailed patterns that contain information about the order of data items. Second, the associated scheme for merging the datasets makes sure that only the master party is able to see an encrypted version of its own data. If this is not deemed sufficient, an additional party that does not hold or encrypt any data itself can replace the master and integrate and anonymize the data. This completely prevents this threat.

A remaining threat comes from frequency attacks, a susceptibility to which lies in the nature of deterministic (not semantically secure) encryption schemes necessary for our approach. Here, an attacker could try to guess the values of an attribute and their distribution and match these with the distribution of the encrypted values. Again, this potentially allows the decryption of encrypted attribute values. The susceptibility of individual attributes to this attack depends on their characteristics, e.g., distribution in the dataset and availability of knowledge about their distribution in specific populations. An in-depth study of frequency attacks is out of the scope of this work, but they have extensively been studied in literature on order-preserving encryption schemes, e.g., [52,53].

Two countermeasures exist against frequency attacks. First, they can to some extent be mitigated by employing pregeneralization. Second, a very widespread countermeasure consists of Data Use Agreements (DUAs). They are a common measure before access to restricted data is granted, and their use is essential for HIPAAs limited dataset [54]. Often they are combined with partial anonymization. DUAs are contracts defining rights and duties, and they are typically used when complete anonymity would restrict data quality in an unacceptable way.

Pregeneralization can be applied as a technical measure for attributes which have a high risk of being susceptible to frequency attacks. This means that attributes in local datasets are pregeneralized before the anonymization process, which coarsens their distributions and thus makes frequency attacks more difficult. When implementing globally optimal algorithms, as in our example, it can be seen from the comparison with baseline techniques in the previous section that the margins of pregeneralization that can be applied while still yielding an increased data utility are often very high. For these algorithms, pregeneralization leads to a new search space, and it can simply be thought of as defining minimal generalization levels for the according attributes. It is important to note that such algorithms still guarantee to find an optimal solution within this new search space, but that this solution might result in higher information loss than without pregeneralization. Our experiments showed that reducing the local datasets’ entropy, i.e., the number of distinct data values, can result in a significant speedup. Therefore, pregeneralization offers a trade-off between privacy, data quality and efficiency.

The described threats become more severe when a malicious security model is assumed. In this case, the master party could provide manipulated data as input, which is specifically designed to be susceptible to frequency attacks. The provision of manipulated input data cannot be prevented in general and is also not taken into account by other work on anonymizing distributed data in a malicious setting, e.g., [55]. Moreover, the master party could skip the anonymization phase and return a non-anonymized version of the dataset, which would then be decrypted. This could be prevented by either distributing the encrypted dataset to all parties and anonymizing them independently, or by introducing multiple additional parties for this purpose. The results of these independent anonymizations can then be compared to ensure correctness. The malicious scenario is highly unlikely, as in biomedical research regulatory frameworks and contracts are employed to prevent such situations. Moreover, the biomedical domain is very aware of security problems and therefore state-of-the-art IT security measures are generally applied to prevent external attackers from compromising IT systems.

Although we used a globally optimal full-domain anonymization algorithm in our examples, our method can be used to implement many algorithms. Most of the methods presented in Section 2.1 are supported, but there are some restrictions that we will discuss in this section.

Compatible algorithms need to be hierarchy-based or able to build the required generalization structures out of the encrypted data items. This is true for all algorithms from our overview, except for Mondrian 
                        [24], as this is a partitioning-based algorithm that requires a total-order on the data items. Unfortunately, such an order cannot be provided for the encrypted data items resulting from our method. The automatic building of generalization graphs in [27] can be implemented with our method. Our approach also supports all of the presented clustering algorithms, as these implement distance metrics based on generalization hierarchies. When generalizing data, some algorithms implement generalization-based methods and partition-based methods for continuous attributes, e.g., [15,23], from which only the former can be used in our method. If continuous variables are assumed to be quasi-identifiers, generalization hierarchies must be provided (see Section 6.5).

The method presented in this article is the first to enable efficient implementations of many algorithms in a distributed setting, including heuristic algorithms, e.g., [23], clustering algorithms, e.g., [15], globally-optimal methods, e.g., [20] and domain-specific solutions, e.g., [22].

Our approach supports most of the privacy criteria presented in Section 2.2. The only limitation is that criteria must not require calculations on the non-encrypted data items, e.g., computations of differences between numerical values. A semantic comparison is still possible by using information from generalization hierarchies.

For this reason, our method does not support privacy criteria that are specifically targeted against numeric attributes. This includes t-closeness for numerical attributes, but all other variants are supported. Analogously, 
                           
                              (
                              k
                              ,
                              e
                              )
                           
                        -anonymity [34] and 
                           
                              (
                              ∊
                              ,
                              m
                              )
                           
                        -anonymity [35] are not supported.

In its current form, our method does not support 
                           
                              ℓ
                           
                        -site-diversity, as implementing it would require maintaining information about which site contributed which data item. This countervails our efforts to hide this information from the participants as discussed in Section 6.1.


                        
                           
                              
                                 
                                    (
                                    ∊
                                    ,
                                    δ
                                    )
                                 
                                 
                                    k
                                 
                              
                           
                        -Dissimilarity is a generic privacy principle that allows for arbitrary distance metrics. The Variational Distance, which is used as an example in the paper, can be implemented with our method.

To our knowledge, our approach is the first to provide support for such a broad spectrum of privacy criteria in a distributed setting. In our prototype we implemented k-anonymity, recursive-
                           
                              (
                              c
                              ,
                              l
                              )
                           
                        -diversity, t-closeness with hierarchical Earth-Mover’s-Distance as well as 
                           
                              δ
                           
                        -presence with explicit world knowledge. Moreover, we support the following criteria from Section 2.2: LKC-privacy, 
                           
                              (
                              α
                              ,
                              k
                              )
                           
                        -anonymity, p-sensitive k-anonymity, 
                           
                              
                                 
                                    (
                                    ∊
                                    ,
                                    δ
                                    )
                                 
                                 
                                    k
                                 
                              
                           
                        -dissimilarity and m-invariance.

In this section, we compare our approach with previous work on distributed data anonymization. We discuss the algorithms presented in Section 3.4, which are distributed variants of the algorithms presented in Section 2.1. In the following, we denote the implementation of Distributed k-Anonymity from [40] with 
                           
                              
                                 
                                    DkA
                                 
                                 
                                    v
                                 
                              
                           
                        , the distributed implementation of Mondrian from [38] with Mondrian
                        h, the implementation of Top Down Specialization for vertically distributed data from [42] with 
                           
                              
                                 
                                    TDS
                                 
                                 
                                    v
                                 
                              
                           
                         and the variant for horizontally distributed data from [30] with 
                           
                              
                                 
                                    TDS
                                 
                                 
                                    h
                                 
                              
                           
                         and the distributed version of the Sequential Clustering Algorithm from [44] with 
                           
                              
                                 
                                    SCA
                                 
                                 
                                    hv
                                 
                              
                           
                        .

Our work implements a mixture of all methods from the design space. It employs the integrate-and-anonymize methodology, but with encrypted data subsets. These are built by a SMC protocol resembling the virtual anonymization method. The trade-off between privacy and data quality implements the anonymize-and-integrate design alternative. In our evaluation, we have already shown that our approach outperforms baseline techniques implementing the anonymize-and-integrate method in terms of data utility and flexibility. Distributed data anonymization algorithms provide support for different de-identification methods, privacy criteria and setups:
                           
                              •
                              Our approach supports an arbitrary number of parties. This is true for all other approaches, apart from DkA
                                 v.

Similar to DkA
                                 v and SCA
                                 hv our approach supports global and local recoding, while Mondrian
                                 h, TDS
                                 h and TDS
                                 v only support global recoding.


                                 DkA
                                 v, TDS
                                 h and TDS
                                 v provide perfect privacy, while our approach and Mondrian
                                 h as well as SCA
                                 hv do not.

Similar to SCA
                                 hv our method supports vertically and horizontally distributed data. TDS
                                 h and Mondrian
                                 h only support the horizontal setup, while DkA
                                 v and TDS
                                 v only support the vertical setup.

The approaches also differ in the implemented privacy criteria. DkA
                                 v is the only algorithm-independent competitor. In the paper, the authors investigate only k-anonymity and as the approach is based on intersecting equivalence classes using tuple identifiers it is, e.g., not suitable for implementing clustering algorithms. The papers proposing 
                                    
                                       
                                          
                                             TDS
                                          
                                          
                                             v
                                          
                                       
                                    
                                  and 
                                    
                                       
                                          
                                             SCA
                                          
                                          
                                             hv
                                          
                                       
                                    
                                  provide information on k-anonymity and 
                                    
                                       ℓ
                                    
                                 -diversity, while 
                                    
                                       
                                          
                                             Mondrian
                                          
                                          
                                             h
                                          
                                       
                                    
                                  implements k-anonymity and 
                                    
                                       ℓ
                                    
                                 -site-diversity and 
                                    
                                       
                                          
                                             TDS
                                          
                                          
                                             h
                                          
                                       
                                    
                                  implements LKC-privacy, which implies support for k-anonymity and 
                                    
                                       ℓ
                                    
                                 -diversity. To which degree other privacy criteria are supported by these approaches needs further investigations. Our method supports a much broader spectrum of criteria as discussed in the previous section. In this paper, we have implemented k-anonymity, 
                                    
                                       ℓ
                                    
                                 -diversity, t-closeness and 
                                    
                                       δ
                                    
                                 -presence.

In the remainder of this section, we compare the performance of our approach to previous solutions. Note that our approach is not a distributed data anonymization algorithm but a pre- and postprocessing scheme. Although it enables the implementation of centralized versions of anonymization algorithms in distributed settings, this does not necessarily imply that such an implementation will be faster than a dedicated distributed variant, if such a variant exists.

As no implementations of DkA
                        v, Mondrian
                        h, TDS
                        v, TDS
                        h or SCA
                        hv are available, we base our comparison on hardware-independent measures or analytical models as for as possible. As a last resort, we rely on performance numbers from experiments with different hardware, but only use those in a very conservative manner, taking into account the increased performance of more modern hardware. As many works lack a systematic experimental or analytical evaluation, similar approaches have been taken in other articles, e.g., [42,30,44]. We exclude the approach from Mondrian
                        h, as it presents an implementation of the Mondrian algorithm [24], which is not supported by our approach.


                        DkA
                        v is algorithm-independent. As an example in their paper [40], the authors use the Datafly algorithm [21] for k-anonymizing a vertically distributed version of the ADULT dataset with two parties. The authors present a hardware-independent model for estimating execution times that depends on the number of homomorphic cryptographic operations per second (COps/s). Our testbed is able to perform 3000 commutative COps/s. We conservatively assume 3000 homomorphic COps/s as well, although these are generally slower that commutative operations. Under this assumptions the approach requires about 5days for k
                        =20, 6days for k
                        =50 and 8days for k
                        =100. In contrast, our pre- and postprocessing scheme requires 21s. We benchmarked Datafly on our hardware and measured execution times of not more than 1s for this dataset with k
                        =20, 50 and 100. This shows that executing a centralized version of Datafly with our approach would significantly outperform DkA
                        v.


                        TDS
                        v was evaluated by k-anonymizing a vertically distributed version of the ADULT dataset with two parties [42]. The authors provide performance numbers for a testbed consisting of machines with Intel Pentium IV 2.6GHz CPUs and a FastEthernet LAN. The authors state that their approach requires not more than 20s for 20⩽
                        k
                        ⩽50. Our pre- and postprocessing scheme requires 21s on more modern hardware. This indicates that executing the centralized version with our approach does not outperform TDS
                        v for 20⩽
                        k⩽50. How the approach scales for smaller parameters, such as the typically used value of k
                        =5 in the healthcare domain, needs further investigation.


                        TDS
                        h was evaluated by applying LKC-privacy to a horizontally distributed version of the ADULT dataset with three parties [30]. The testbed consisted of machines with Intel Core2 Quad Q6600 2.4GHz CPUs and a FastEthernet LAN. The authors indicate execution times of 30s for 
                           
                              L
                              =
                              4
                              ,
                              20
                              ⩽
                              K
                              ⩽
                              100
                           
                         and 
                           
                              C
                              =
                              0.2
                           
                        . Our pre- and postprocessing scheme requires 2s on more modern hardware. This indicates that using the centralized version of the algorithm with our approach does not yield any significant benefits over using the distributed variant.


                        SCA
                        hv supports vertically and horizontally distributed data. We base our evaluation on hardware-independent numbers about the communication overhead and ignore the additional computations performed by the approach. In [44], the authors report numbers for k-anonymizing a vertically distributed version of the ADULT dataset with two parties. For k
                        =100 their approach required 2880s for data transfer in a FastEthernet LAN. This number is almost independent of k, it only slightly decreases for increasing parameters. Our pre- and postprocessing scheme requires about 21s (including computations) in an equivalent network environment, independent of the parameter k. The authors also report numbers on a centralized implementation of their algorithm. For the same dataset this version requires 150s for k
                        =100 on older hardware (Intel Core Duo T2350 CPU 1.86GHz). As our scheme and the centralized variant of the algorithm significantly outperform the distributed implementation, this shows that implementing the centralized version with our approach outperforms SCA
                        hv.

The authors also report numbers for k-anonymizing a horizontally distributed version of the ADULT dataset with two parties. For k
                        =100 their approach required 800s for data transfer in a FastEthernet LAN. This number increases significantly with decreasing k, already requiring 4400s for k
                        =25. Whether the approach is able to handle smaller parameters, such as the typically used value of k
                        =5 in the healthcare domain, needs further investigation. Our pre- and postprocessing scheme requires about 1s (including computations) in an equivalent network environment, independent of the parameter k. Again, assuming not more than 150s are required for executing the centralized variant, this shows that implementing the centralized version with our approach outperforms SCA
                        hv.

In this section, we discuss the assumptions of and prerequisites for our approach, as outlined in Section 4.1.

Similar to other approaches, e.g., [40,38,42,30,44], we assume that data integration and cleanup has happened before starting the protocol. When collecting biomedical research data in a distributed environment, e.g., in research networks or when using a honest broker, the distribution of data is often predefined. In these cases, inconsistencies are rare. If data is inconsistent, this is not a problem for our protocol and it is also unlikely to result in privacy-problems. If inconsistencies must be resolved this can either be done offline or work on privacy-preserving data cleansing can be leveraged, e.g., [56,57].

Our approach is based on the semi-honest security model. This is also a typical assumption, cf. [40,38,42,30,44], and realistic in the biomedical domain (cf. Section 6.1). Secure communication channels, which are also required, are typically available and there is a multitude of protocols that support multi-way negotiation of cryptographic algorithms and parameters, e.g., [58].

We assume that shared global generalization hierarchies are available for all quasi-identifiers. This is also a common assumption for related approaches. As a result, there cannot be any consistency problems when merging generalization hierarchies in our protocol. As all rules result from the global generalization hierarchies, only duplicates can occur that can easily be removed. Even pregeneralization, proposed as a countermeasure against frequency attacks, is not a problem. The rules required for pregeneralized data can either be predefined in the global hierarchies, or inferred from the global hierarchies by each party for its own pregeneralized data.

In case of vertical distribution, global hierarchies are trivial to built. As each party holds a distinct set of attributes, they can define their own hierarchies. As a result, complex functions, e.g., clustering, can also be used to dynamically create hierarchies for continuous variables. In the horizontal setup, creating global hierarchies can be more difficult. For discrete variables this not a problem, even when these have very large domains. Firstly, only relevant rules that match at least one value in the dataset are extracted from the hierarchies and processed. Secondly, hierarchies can again be expressed as functions that are shared between the participants and used to dynamically create generalization rules only for the existing values. In the horizontal scenario it has to be made sure that the rules generated for equal values are consistent amongst all parties. Moreover, rules are generated locally without having a global view on the dataset. This means that complex functions for dynamically building generalization hierarchies for continuous variables, e.g., clustering, cannot be applied in this scenario. However, more simple functions, e.g., incrementally reducing the precision of floating-point values, can still be used.

Generalization hierarchies are only required for quasi-identifiers and (sometimes) for sensitive attributes. Typical quasi-identifiers are categorical [59]. They are assumed to have a high reidentification risk [60]. This requires attributes to be reproducible, i.e., have a high chance of repeatedly occurring for an individual. Moreover, there has to be a high risk of similar data being available to an attacker [61]. If continuous variables are assumed to have these properties in a horizontally distributed scenario and simple functional representations of generalization rules are not sufficient, it could be investigated how categorization methods, such as [62], can be combined with secure set union protocols [63].

@&#CONCLUSIONS@&#

We have presented a secure multi-party computing protocol that enables a novel and flexible anonymization method for distributed data. Prior to the data anonymization procedure an encrypted global view of the dataset is constructed, which is then anonymized. Our approach is the first to support a broad spectrum of privacy criteria and anonymization algorithms. This includes variants of common criteria for protecting datasets from membership, attribute and identity disclosure. The supported methods include heuristics, including clustering algorithms, and optimal methods. In our examples we used a globally optimal algorithm with k-anonymity, 
                        
                           ℓ
                        
                     -diversity, t-closeness and 
                        
                           δ
                        
                     -presence. We have motivated our approach by examples from the biomedical domain.

We have added an extensive experimental evaluation of our method and developed an analytical model that can be used to accurately estimate the overhead caused in terms of computational costs and transferred data volumes. Our experiments and comparisons have shown that it offers highly competitive performance and thus provides a practical solution for anonymizing distributed biomedical datasets. Our prototype of a globally optimal algorithm is the first efficient implementation of a such a method in a distributed environment. According to El Emam et al. this class of algorithms is very suitable for the biomedical domain [14]. Reasons include that it results in datasets, which are well suited for biomedical analyses and provides reproducible and understandable results that can be adjusted by non-experts (e.g., by changing generalization hierarchies or choosing another transformation from the search space).

In future work, there are multiple ways to extend our concept and our implementation. For example, first experiments have shown that employing additional data compression methods can reduce data volumes by a factor of up to 2.5. In case of vertical distribution, commutative encryption of the data items could be replaced with symmetric encryption (e.g., AES). This would enable additional speedups, especially for datasets with many distinct values. We measured a speedup of roughly 40% for the CUP dataset, but only 2% for the IHIS dataset.

Further performance gains can be achieved for datasets with many insensitive attributes. In our current implementation we handle these analogously to quasi-identifying and sensitive attributes, i.e., we apply commutative encryption. An alternative is to use a much more efficient symmetric cipher and only encrypt the keys for this cipher (one key per party) commutatively. This allows the last party to decrypt all insensitive attributes in the anonymized representation.

Our concept can easily be extended to cover hybridly distributed data. In this case the tuple identifiers need to be preserved during the integration phase, as they are needed to correlate the individual subsets. The remainder of the protocol can be executed analogously to the horizontal setup.

@&#REFERENCES@&#

