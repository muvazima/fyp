@&#MAIN-TITLE@&#A new approach for automatic sleep scoring: Combining Taguchi based complex-valued neural network and complex wavelet transform

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           In this study, a new complex classifier-based approach is presented for automatic sleep scoring using EEG signals.


                        
                        
                           
                           The effect of complex-valued classifiers shown to have a positive impact on classification accuracy of EEG signal data.


                        
                        
                           
                           One of the interesting parts of the study is the parameter optimization that significantly affects system performance.


                        
                        
                           
                           Proposed method can be used to design a computer support system for rapid and accurate sleep stage scoring.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

EEG signals

Dual-tree complex wavelet transform

Taguchi method

Sleep stage scoring

Complex-valued neural networks

@&#ABSTRACT@&#


               
               
                  Automatic classification of sleep stages is one of the most important methods used for diagnostic procedures in psychiatry and neurology. This method, which has been developed by sleep specialists, is a time-consuming and difficult process. Generally, electroencephalogram (EEG) signals are used in sleep scoring. In this study, a new complex classifier-based approach is presented for automatic sleep scoring using EEG signals. In this context, complex-valued methods were utilized in the feature selection and classification stages. In the feature selection stage, features of EEG data were extracted with the help of a dual tree complex wavelet transform (DTCWT). In the next phase, five statistical features were obtained. These features are classified using complex-valued neural network (CVANN) algorithm. The Taguchi method was used in order to determine the effective parameter values in this CVANN. The aim was to develop a stable model involving parameter optimization. Different statistical parameters were utilized in the evaluation phase. Also, results were obtained in terms of two different sleep standards. In the study in which a 2nd level DTCWT and CVANN hybrid model was used, 93.84% accuracy rate was obtained according to the Rechtschaffen & Kales (R&K) standard, while a 95.42% accuracy rate was obtained according to the American Academy of Sleep Medicine (AASM) standard. Complex-valued classifiers were found to be promising in terms of the automatic sleep scoring and EEG data.
               
            

@&#INTRODUCTION@&#

One of the important factors that affect our daily lives is healthy sleep. Examples of situations in which sleep affects our lives are with regard to work performance, mood, morale and relationships with other individuals [1]. Sleep research is of great importance for the detection and treatment of sleep-related problems. In addition, sleep analysis also provides significant benefits for various psycho-physiological analyses. For example, it has been reported that a healthy deep sleep stage accelerates physical healing. Furthermore, a healthy rapid eye movement (REM) stage increases learning ability and strengthens the memory.

The most important source referenced in sleep analysis is the sleep scoring process. Polysomnographic recordings (PSG) are used for sleep scoring. These PSG are comprised of data including the EEG, electromyogram (EMG) and electrooculogram (EOG) data of the patient [1]. These records are evaluated by experts using the R&K standards. According to these standards, each 30-second epoch is classified as awake, REM and non-rapid eye movement (NREM stages 1–4). Recently, NREM stages 3 and 4 have been combined and referred to as the slow wave sleep stage (SWS).

EEG signals are the most commonly used physiological signals in sleep studies. The reason for this is these signals give information about brain activity [2]. EEG signals are typically composed of four wave types. They are respectively delta (0–3.99Hz); theta (4–7.99Hz); alpha (8–12.99Hz) and beta (13–30Hz). In the different sleep stages, these waves have different properties. Low amplitude and combined EEG frequency can be seen in awake stage. In the NREM stage 1, a 2–7Hz frequency range and the presence of alpha waves in less than half of the cycle are observed in EEG signals. In the NREM Stage 2, there are sleep spindles (12–14Hz) and k-complexes. With low frequency waves of less than 2Hz, sleep spindles and k-complexes can be seen in NREM Stage 3 and NREM Stage 4 which are the deep sleep stages [3,4]. Low voltage and mixed EEG frequency can be seen in the REM stage. Also in this stage, there are low amplitude EMG, high level EOG signal from both eyes and sawtooth wave-like pattern [5].

EEG signals have a great deal of formal complexity. The amplitudes and the frequencies of these signals are constantly changing. Furthermore, these signals are not periodic. Therefore, measurements must be done with caution over a long period, in order to obtain meaningful information from these signals. Specialists analyze EEG data (recorded for hours) in 5–10s frames and it is shown on a computer screen [2]. This process is a rather tedious and difficult process, and there is also the possibility of making mistakes. As can be seen, visual analysis with regard to sleep staging and the interpretation of complex EEG signals is a difficult problem. For all these reasons, EEG signals must be analyzed using a suitable, highly accurate method with regard to an effective sleep staging system. Recently, many models for the automatic sleep scoring have been proposed. Sinha [6] proposed a method which classifies three sleep stages (sleep spindles, REM and awake). A 95.35% classification accuracy was achieved in the study in which artificial neural networks (ANNs) and wavelet transform were used. Susmakova and Krakovska [7] used 73 characteristic measures in the sleep scoring with discriminant analysis involving Fisher's quadratic classifier. Subasi et al. [8] used a wavelet-based ANN model to classify drowsy, alert and sleep states. The classification results of this study were 96.2% drowsy, 93.6% alert and 92.3% sleep. Zoubek et al. [9] used Fourier transform coefficients, Kurtosis values, entropy values and standard deviation values for sleep scoring. They achieved 80% accuracy rate in the study in which neural networks were preferred as the classifier. In a study in which they benefited from EEG signals, Doroshenkov et al. [10] proposed a hidden Markov model-based classification algorithm. The authors obtained the best results in terms of the classification of the REM stage. Ebrahimi et al. [11] proposed a method based on ANN and wavelet transform in their study. They achieved a 93% accuracy rate in the study in which five sleep stages were classified. Jo et al. [12] achieved an 84.6% accuracy rate using a genetic fuzzy classifier. Tagluk et al. [13] classified the sleep stages using a feed-forward neural network. A 74.7% success rate was obtained in the study and 5 sleep stages were classified. Fraiwan et al. [14] used time-frequency entropy values as a feature. They used these features, obtained in different frequency bands, as input data to a linear discriminant analysis algorithm. 84% success rate was obtained in the study in which 6 sleep stages were classified. Ozsen [15] obtained time-frequency based features in her study in which she used EEG, EMG and EOG signals. A class-dependent sequential feature selection algorithm was used for the detection of effective features among the obtained data. The features obtained after the feature selection step were presented as input data to the ANN and a 90.93% success rate was obtained in the study in which 5 sleep stages were classified. Hsu et al. [16] used energy-based features. Five sleep stages were classified in the study in which a recurrent neural network was used as the classification algorithm. The researchers obtained a success rate of 87.2%.

In this study, an effective system is proposed which can automatically perform the staging process with complex-valued classifiers. DTCWT is used in the feature extraction stage. In the next step, 5 statistical feature values are obtained from obtained features. The obtained complex valued features are classified using CVANN algorithm. Effective parameter values of the algorithm are obtained using the Taguchi method in order to improve the performance of CVANN algorithm.

The organization of the paper is as follows. In Section 2, information about the methods used in this study is presented. In Section 3, the details of the proposed method are given. In Section 4, the experimental design is described in detail. In Section 5, the experimental results are summarized and the comparative analyses of these results are presented. In Section 6, experimental results of this work are discussed. In Section 7, the results obtained in this study and information about future planned targets are provided.

@&#METHODS@&#

DWT has been successfully applied to the solution of many problems. There are successful applications of DWT in many areas such as feature extraction, noise reduction, pattern recognition and image processing [17–19]. Despite the successful applications of the DWT algorithm, alternative DWT-based methods have been developed due to the existence of some drawbacks. Some of the disadvantages of DWT are shift sensitivity, poor directionality and absence of phase information. Detailed information on this subject can be obtained from Ref. [20]. Because of these drawbacks with regard to the DWT algorithm, the DTCWT was developed by Selesnick et al. [20].

The DTCWT is composed of two parallel running DWTs. Fig. 1
                         shows the structure of the DTCWT. The algorithm consists of two Trees – both real and imaginary. A pair of filters is used in DTCWT algorithm for the wavelet function ψ(t) and the scaling function φ(t) [21]. These filters are respectively a low-pass and a high-pass filter pair for the real tree (h
                        0(n), h
                        1(n)) and, a low pass and a high pass filter pair for the imaginary tree (g
                        0(n), g
                        1(n)).


                        ψ(t) and φ(t) functions are calculated using Eqs. (1) and (2).
                           
                              (1)
                              
                                 
                                    
                                       ψ
                                       h
                                    
                                    (
                                    t
                                    )
                                    =
                                    
                                       2
                                    
                                    
                                       ∑
                                       n
                                    
                                    
                                       
                                          h
                                          1
                                       
                                       (
                                       n
                                       )
                                       
                                          φ
                                          h
                                       
                                       (
                                       2
                                       t
                                       −
                                       n
                                       )
                                    
                                 
                              
                           
                        
                        
                           
                              (2)
                              
                                 
                                    
                                       φ
                                       h
                                    
                                    (
                                    t
                                    )
                                    =
                                    
                                       2
                                    
                                    
                                       ∑
                                       n
                                    
                                    
                                       
                                          h
                                          0
                                       
                                       (
                                       n
                                       )
                                       
                                          φ
                                          h
                                       
                                       (
                                       2
                                       t
                                       −
                                       n
                                       )
                                    
                                 
                              
                           
                        
                     

Together with the real and imaginary components, a complex wavelet function can be expressed as ψ(t)≔
                        ψ
                        
                           h
                        (t)+
                        jψ
                        
                           g
                        (t). ψ
                        
                           h
                        (t) and ψ
                        
                           g
                        (t) are real-valued wavelets. In Fourier transform, complex base functions are analytic. This situation provides significant advantages such as shift invariance, directionality and lack of aliasing. If ψ(t) function is approximately analytic, then it may have these superior properties [20]. In order ψ(t) to meet that condition, a wavelet basis must be approximately equal to Hilbert transform of other wavelet basis [21]. This is given in Eq. (3).
                           
                              (3)
                              
                                 
                                    
                                       ψ
                                       g
                                    
                                    (
                                    t
                                    )
                                    ≈
                                    H
                                    {
                                    
                                       ψ
                                       h
                                    
                                    (
                                    t
                                    )
                                    }
                                 
                              
                           
                        
                     

For Eq. (3) to take place, there must be a certain distance between low-pass analysis filters on real and imaginary trees. This distance should be approximately in half-sample length [22]. This situation can be seen in Eq. (4).
                           
                              (4)
                              
                                 
                                    
                                       g
                                       0
                                    
                                    [
                                    n
                                    ]
                                    ≈
                                    
                                       h
                                       n
                                    
                                    [
                                    n
                                    −
                                    0.5
                                    ]
                                 
                              
                           
                        
                     

The CVANN algorithm can be used with multilayer ANNs where inputs, outputs, weights and bias values are all complex numbers. When real-valued neural networks are preferred with regard to solving problems with complex-valued numbers this leads to a processing load [23]. In this case, ANN must be applied separately to the real and imaginary parts. However, when CVANN is applied to the same problem, the computational load is reduced, and processing time is shortened due to the possibility of direct processing [23]. In addition, accuracy rate increases [23,24]. There are other advantages associated with CVANNs compared to real-valued ANNs. Detailed information on these benefits can be obtained from [25,26].

In this study, a complex-valued back-propagation algorithm (CBP) has been used for training of the CVANN. We will first describe the CBP model. A model neuron used in the CBP algorithm is shown in Fig. 2
                        .


                        Y
                        
                           n
                         activity value is calculated using Eq. (5).
                           
                              (5)
                              
                                 
                                    
                                       Y
                                       n
                                    
                                    =
                                    
                                       ∑
                                       m
                                    
                                    
                                       
                                          W
                                          
                                             n
                                             m
                                          
                                       
                                       
                                          I
                                          m
                                       
                                       +
                                       
                                          θ
                                          n
                                       
                                    
                                 
                              
                           
                        where W
                        
                           nm
                         is the weight value. I
                        
                           m
                         is input signal and θ
                        
                           n
                         is threshold value. These values are complex valued. For complex valued output, Y
                        
                           n
                         activity value is converted into a value, which has real and imaginary parts, using Eq. (6).
                           
                              (6)
                              
                                 
                                    
                                       Y
                                       n
                                    
                                    =
                                    x
                                    +
                                    i
                                    y
                                    =
                                    z
                                 
                              
                           
                        
                     

In the equation, i represents 
                           
                              
                                 
                                    −
                                    1
                                 
                              
                           
                        . Output function is calculated using Eq. (7).
                           
                              (7)
                              
                                 
                                    
                                       f
                                       c
                                    
                                    (
                                    z
                                    )
                                    =
                                    
                                       f
                                       R
                                    
                                    (
                                    x
                                    )
                                    +
                                    i
                                    ⋅
                                    
                                       f
                                       R
                                    
                                    (
                                    y
                                    )
                                 
                              
                           
                        where f
                        
                           R
                        (u) is the activation function. CVANN structure used in this study is shown in Fig. 3
                        . Information about the mathematical structure of CVANNs is presented below [23,27]. For the clarity of description, we will list some symbols below.

In Fig. 3, W
                        
                           kl
                         and V
                        
                           sk
                         are weight values between different layers. θ
                        
                           k
                         and λ
                        
                           s
                         represent threshold values. I
                        
                           l
                         is input data. H
                        
                           k
                         is the output value of hidden layer neuron k. O
                        
                           s
                         is the output value of output layer neuron s. U
                        
                           k
                         and S
                        
                           s
                         are activity values of their layer. U
                        
                           k
                        , H
                        
                           k
                        , S
                        
                           s
                         and O
                        
                           s
                         can be defined as Eqs. (8)–(11).
                           
                              (8)
                              
                                 
                                    
                                       U
                                       k
                                    
                                    =
                                    
                                       ∑
                                       l
                                    
                                    
                                       
                                          W
                                          
                                             k
                                             l
                                          
                                       
                                       
                                          I
                                          l
                                       
                                       +
                                       
                                          θ
                                          k
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (9)
                              
                                 
                                    
                                       H
                                       k
                                    
                                    =
                                    
                                       f
                                       c
                                    
                                    (
                                    
                                       U
                                       k
                                    
                                    )
                                 
                              
                           
                        
                        
                           
                              (10)
                              
                                 
                                    
                                       S
                                       s
                                    
                                    =
                                    
                                       ∑
                                       k
                                    
                                    
                                       
                                          V
                                          
                                             s
                                             k
                                          
                                       
                                       
                                          H
                                          k
                                       
                                       +
                                       
                                          λ
                                          s
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (11)
                              
                                 
                                    
                                       O
                                       s
                                    
                                    =
                                    
                                       f
                                       c
                                    
                                    (
                                    
                                       S
                                       s
                                    
                                    )
                                 
                              
                           
                        
                     

In this study, the square error function is preferred as the error function. This function is expressed as shown in Eq. (12).
                           
                              (12)
                              
                                 
                                    
                                       E
                                       p
                                    
                                    =
                                    
                                       
                                          
                                             1
                                             2
                                          
                                       
                                    
                                    
                                       ∑
                                       
                                          n
                                          =
                                          1
                                       
                                       K
                                    
                                    
                                       |
                                       
                                          T
                                          s
                                       
                                       −
                                       
                                          O
                                          s
                                       
                                       
                                          |
                                          2
                                       
                                    
                                    =
                                    
                                       
                                          
                                             1
                                             2
                                          
                                       
                                    
                                    
                                       ∑
                                       
                                          s
                                          =
                                          1
                                       
                                       K
                                    
                                    
                                       |
                                       
                                          δ
                                          s
                                       
                                       
                                          |
                                          2
                                       
                                    
                                 
                              
                           
                        
                     

(δ
                        
                           s
                        
                        =
                        T
                        
                           s
                        
                        −
                        O
                        
                           s
                        ), is the error between the actual pattern O
                        
                           s
                         and the target pattern T
                        
                           s
                         of output neuron s. Eq. (13) is used for complex valued operations as squared error function.
                           
                              (13)
                              
                                 
                                    
                                       E
                                       p
                                    
                                    =
                                    
                                       
                                          
                                             1
                                             2
                                          
                                       
                                    
                                    
                                       ∑
                                       
                                          s
                                          =
                                          1
                                       
                                       K
                                    
                                    
                                       {
                                       |
                                       Re
                                       (
                                       
                                          T
                                          s
                                       
                                       )
                                       −
                                       Re
                                       (
                                       
                                          O
                                          s
                                       
                                       )
                                       
                                          |
                                          2
                                       
                                       +
                                       |
                                       Im
                                       (
                                       
                                          T
                                          s
                                       
                                       )
                                       −
                                       Im
                                       (
                                       
                                          O
                                          s
                                       
                                       )
                                       
                                          |
                                          2
                                       
                                       }
                                    
                                 
                              
                           
                        
                     

CBP model was utilized as a learning rule to reduce the E
                        
                           p
                         error value [28]. Configuration of the weights and bias values is performed according to Eqs. (14)–(17). (where η
                        >0, and η is a small learning constant):
                           
                              (14)
                              
                                 
                                    Δ
                                    
                                       V
                                       
                                          s
                                          k
                                       
                                    
                                    =
                                    −
                                    η
                                    ⋅
                                    
                                       
                                          ∂
                                          
                                             E
                                             p
                                          
                                       
                                       
                                          ∂
                                          Re
                                          [
                                          
                                             V
                                             
                                                s
                                                k
                                             
                                          
                                          ]
                                       
                                    
                                    −
                                    i
                                    ⋅
                                    η
                                    
                                       
                                          ∂
                                          
                                             E
                                             p
                                          
                                       
                                       
                                          ∂
                                          Im
                                          [
                                          
                                             V
                                             
                                                s
                                                k
                                             
                                          
                                          ]
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (15)
                              
                                 
                                    Δ
                                    
                                       λ
                                       s
                                    
                                    =
                                    −
                                    η
                                    ⋅
                                    
                                       
                                          ∂
                                          
                                             E
                                             p
                                          
                                       
                                       
                                          ∂
                                          Re
                                          [
                                          
                                             λ
                                             s
                                          
                                          ]
                                       
                                    
                                    −
                                    i
                                    ⋅
                                    η
                                    
                                       
                                          ∂
                                          
                                             E
                                             p
                                          
                                       
                                       
                                          ∂
                                          Im
                                          [
                                          
                                             λ
                                             s
                                          
                                          ]
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (16)
                              
                                 
                                    Δ
                                    
                                       W
                                       
                                          k
                                          l
                                       
                                    
                                    =
                                    −
                                    η
                                    ⋅
                                    
                                       
                                          ∂
                                          
                                             E
                                             p
                                          
                                       
                                       
                                          ∂
                                          Re
                                          [
                                          
                                             W
                                             
                                                k
                                                l
                                             
                                          
                                          ]
                                       
                                    
                                    −
                                    i
                                    ⋅
                                    η
                                    
                                       
                                          ∂
                                          
                                             E
                                             p
                                          
                                       
                                       
                                          ∂
                                          Im
                                          [
                                          
                                             W
                                             
                                                k
                                                l
                                             
                                          
                                          ]
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (17)
                              
                                 
                                    Δ
                                    
                                       θ
                                       k
                                    
                                    =
                                    −
                                    η
                                    ⋅
                                    
                                       
                                          ∂
                                          
                                             E
                                             p
                                          
                                       
                                       
                                          ∂
                                          Re
                                          [
                                          
                                             θ
                                             k
                                          
                                          ]
                                       
                                    
                                    −
                                    i
                                    ⋅
                                    η
                                    
                                       
                                          ∂
                                          
                                             E
                                             p
                                          
                                       
                                       
                                          ∂
                                          Im
                                          [
                                          
                                             θ
                                             k
                                          
                                          ]
                                       
                                    
                                 
                              
                           
                        
                     

The expression given from Eqs. (14)–(17) can be rewritten as Eqs. (18)–(21):
                           
                              (18)
                              
                                 
                                    Δ
                                    
                                       V
                                       
                                          s
                                          k
                                       
                                    
                                    =
                                    
                                       
                                          
                                             H
                                             k
                                          
                                       
                                       ¯
                                    
                                    Δ
                                    
                                       λ
                                       s
                                    
                                 
                              
                           
                        
                        
                           
                              (19)
                              
                                 
                                    Δ
                                    
                                       λ
                                       s
                                    
                                    =
                                    η
                                    (
                                    Re
                                    [
                                    
                                       δ
                                       s
                                    
                                    ]
                                    (
                                    1
                                    −
                                    Re
                                    [
                                    
                                       O
                                       s
                                    
                                    ]
                                    )
                                    Re
                                    [
                                    
                                       O
                                       s
                                    
                                    ]
                                    +
                                    i
                                    ⋅
                                    Im
                                    [
                                    
                                       δ
                                       s
                                    
                                    ]
                                    (
                                    1
                                    −
                                    Im
                                    [
                                    
                                       O
                                       s
                                    
                                    ]
                                    )
                                    Im
                                    [
                                    
                                       O
                                       s
                                    
                                    ]
                                    )
                                 
                              
                           
                        
                        
                           
                              (20)
                              
                                 
                                    Δ
                                    
                                       W
                                       
                                          k
                                          l
                                       
                                    
                                    =
                                    
                                       
                                          
                                             I
                                             l
                                          
                                       
                                       ¯
                                    
                                    Δ
                                    
                                       θ
                                       k
                                    
                                 
                              
                           
                        
                        
                           
                              (21)
                              
                                 
                                    Δ
                                    
                                       θ
                                       k
                                    
                                    =
                                    η
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      (
                                                      1
                                                      −
                                                      Re
                                                      [
                                                      
                                                         H
                                                         k
                                                      
                                                      ]
                                                      )
                                                      Re
                                                      [
                                                      
                                                         H
                                                         k
                                                      
                                                      ]
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      x
                                                      
                                                         ∑
                                                         s
                                                      
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        
                                                                           Re
                                                                           [
                                                                           
                                                                              δ
                                                                              s
                                                                           
                                                                           ]
                                                                           (
                                                                           1
                                                                           −
                                                                           Re
                                                                           [
                                                                           
                                                                              O
                                                                              s
                                                                           
                                                                           ]
                                                                           )
                                                                        
                                                                     
                                                                  
                                                                  
                                                                     
                                                                        
                                                                           Re
                                                                           [
                                                                           
                                                                              O
                                                                              s
                                                                           
                                                                           ]
                                                                           Re
                                                                           [
                                                                           
                                                                              V
                                                                              
                                                                                 s
                                                                                 k
                                                                              
                                                                           
                                                                           ]
                                                                        
                                                                     
                                                                  
                                                                  
                                                                     
                                                                        
                                                                           +
                                                                           Im
                                                                           [
                                                                           
                                                                              δ
                                                                              s
                                                                           
                                                                           ]
                                                                           (
                                                                           1
                                                                           −
                                                                           Im
                                                                           [
                                                                           
                                                                              O
                                                                              s
                                                                           
                                                                           ]
                                                                           )
                                                                        
                                                                     
                                                                  
                                                                  
                                                                     
                                                                        
                                                                           Im
                                                                           [
                                                                           
                                                                              O
                                                                              s
                                                                           
                                                                           ]
                                                                           Im
                                                                           [
                                                                           
                                                                              V
                                                                              
                                                                                 s
                                                                                 k
                                                                              
                                                                           
                                                                           ]
                                                                           )
                                                                        
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    −
                                    i
                                    η
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      (
                                                      1
                                                      −
                                                      Im
                                                      [
                                                      
                                                         H
                                                         k
                                                      
                                                      ]
                                                      )
                                                      Im
                                                      [
                                                      
                                                         H
                                                         k
                                                      
                                                      ]
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      x
                                                      
                                                         ∑
                                                         s
                                                      
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        
                                                                           Re
                                                                           [
                                                                           
                                                                              δ
                                                                              s
                                                                           
                                                                           ]
                                                                           (
                                                                           1
                                                                           −
                                                                           Re
                                                                           
                                                                              
                                                                                 
                                                                                    O
                                                                                    s
                                                                                 
                                                                              
                                                                           
                                                                           )
                                                                        
                                                                     
                                                                  
                                                                  
                                                                     
                                                                        
                                                                           Re
                                                                           [
                                                                           
                                                                              O
                                                                              s
                                                                           
                                                                           ]
                                                                           Im
                                                                           [
                                                                           
                                                                              V
                                                                              
                                                                                 s
                                                                                 k
                                                                              
                                                                           
                                                                           ]
                                                                        
                                                                     
                                                                  
                                                                  
                                                                     
                                                                        
                                                                           −
                                                                           Im
                                                                           [
                                                                           
                                                                              δ
                                                                              s
                                                                           
                                                                           ]
                                                                           (
                                                                           1
                                                                           −
                                                                           Im
                                                                           [
                                                                           
                                                                              O
                                                                              s
                                                                           
                                                                           ]
                                                                           )
                                                                        
                                                                     
                                                                  
                                                                  
                                                                     
                                                                        
                                                                           Im
                                                                           [
                                                                           
                                                                              O
                                                                              s
                                                                           
                                                                           ]
                                                                           Re
                                                                           [
                                                                           
                                                                              V
                                                                              
                                                                                 s
                                                                                 k
                                                                              
                                                                           
                                                                           ]
                                                                        
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

It is well known that the CVANN generalization performance depends on the meta-parameters listed below being set up properly. These parameters are the very ones which are used in the current study for optimization.

(1) Learning rate (L) and momentum rate (M): The appropriate establishment of learning and momentum rate is an important factor in the performance of the network. The learning rate identifies the amount of change in the weights.

If large values are selected, network navigation and oscillation among local solutions are possible [29]. The selection of a lower learning rate may cause a deceleration in the learning rate. The momentum rate is an important factor that affects learning performance. The momentum coefficient is the addition of a specific ratio of change in the previous iteration to the new amount of change. This is mostly used for networks that tripped on local minima previously in order to arrive at better solutions. Small values in this regard may complicate the avoidance of local solutions, and high values may create complications in achieving single solutions. Hence, identifying the momentum coefficient is a big dilemma in itself. The current study prefers a 0–1 range for learning and momentum coefficients as frequently proposed in the literature [29]. The study identifies the learning and momentum coefficients whose best values are researched as L, M
                           ∈{0.1, 0.3, 0.5, 0.7and0.9}.

(2) Iteration value (S): This parameter is included in the optimization in order to identify the iteration value that provides the best results. Iteration values are identified as S
                           ∈{1000, 2500, 5000, 7500, 10,000}. Literature confirms that these values are commonly used [30,31].

(3) Activation function (HA): Selecting the appropriate activation function is a major problem in CVANNs [31]. It is seen that different activation functions were used in the literature. But, there is no study found for which activation function will provide better results. Moreover, the effect of the activation function can vary according to the type of problem [32]. In this study, it is aimed to determine the activation function which gives good results for the solution of problem. Information about activation functions used during the experiments is given below.

The sigmoid activation function was used by Leung and Haykin [31] for CVANNs. This function is calculated as shown in Eq. (22).
                              
                                 (22)
                                 
                                    
                                       f
                                       (
                                       z
                                       )
                                       =
                                       1
                                       /
                                       (
                                       1
                                       +
                                       
                                          e
                                          
                                             −
                                             z
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        

However, this activation function has z
                           =(2n
                           +1)iπ, 
                              
                                 n
                                 ∈
                                 ℤ
                              
                            singular points in each z. The researchers have scaled the input data to a specific section of the complex plane in order to solve this problem [32]. The sigmoid activation function has been adapted to CVANNs by using Eq. (23) 
                           [33,34].
                              
                                 (23)
                                 
                                    
                                       f
                                       (
                                       z
                                       )
                                       =
                                       1
                                       /
                                       (
                                       1
                                       +
                                       
                                          e
                                          
                                             −
                                             Re
                                              
                                             z
                                          
                                       
                                       )
                                       +
                                       i
                                       1
                                       /
                                       (
                                       1
                                       +
                                       
                                          e
                                          
                                             −
                                             Im
                                              
                                             z
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        

In addition, tanh function which is commonly used as the activation function is used in CVANNs with the conversion in Eq. (24).
                              
                                 (24)
                                 
                                    
                                       f
                                       (
                                       z
                                       )
                                       =
                                       tanh
                                        
                                       (
                                       Re
                                        
                                       z
                                       )
                                       +
                                       i
                                        
                                       tanh
                                        
                                       (
                                       Im
                                        
                                       z
                                       )
                                    
                                 
                              
                           sech is the form used in Eq. (25) as the selected hidden neuron's functional unit.
                              
                                 (25)
                                 
                                    
                                       sec
                                        
                                       h
                                       (
                                       z
                                       )
                                       =
                                       2
                                       /
                                       (
                                       
                                          e
                                          z
                                       
                                       +
                                       
                                          e
                                          
                                             −
                                             z
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        

In the current study, the sech function has been adapted to CVANNs as shown in Eq. (26):
                              
                                 (26)
                                 
                                    
                                       f
                                       (
                                       z
                                       )
                                       =
                                       sec
                                        
                                       h
                                       (
                                       Re
                                        
                                       z
                                       )
                                       +
                                       i
                                        
                                       sec
                                        
                                       h
                                       (
                                       Im
                                        
                                       z
                                       )
                                    
                                 
                              
                           
                        

As far as we know, there are no studies in which the sech function is used as an activation function in back propagation CVANNs. In this regard, the effects of the parameters on the results are important.

In this study, impacts of Mexican hat wavelet function and Haar wavelet activation function are also investigated. The Haar and the Mexican hat wavelet functions can be defined as Eqs. (27) and (28).
                              
                                 (27)
                                 
                                    
                                       
                                          ψ
                                          
                                             Haar
                                          
                                       
                                       =
                                       (
                                       1
                                       −
                                       k
                                       )
                                       
                                          e
                                          
                                             −
                                             (
                                             k
                                             /
                                             2
                                             )
                                          
                                       
                                    
                                 
                              
                           
                           
                              
                                 (28)
                                 
                                    
                                       
                                          ψ
                                          
                                             Mexhat
                                          
                                       
                                       =
                                       (
                                       1
                                       −
                                       x
                                       
                                          k
                                          2
                                       
                                       )
                                       
                                          e
                                          
                                             −
                                             y
                                             
                                                k
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

In this study, The Haar and the Mexican hat wavelet activation functions have been adapted to CVANNs as shown in Eqs. (29) and (30).
                              
                                 (29)
                                 
                                    
                                       f
                                       (
                                       z
                                       )
                                       =
                                       
                                          ψ
                                          
                                             Haar
                                          
                                       
                                       =
                                       (
                                       1
                                       −
                                       Re
                                        
                                       z
                                       )
                                       
                                          e
                                          
                                             −
                                             (
                                             Re
                                              
                                             z
                                             /
                                             2
                                             )
                                          
                                       
                                       +
                                       i
                                       (
                                       1
                                       −
                                       Im
                                        
                                       z
                                       )
                                       
                                          e
                                          
                                             −
                                             (
                                             Im
                                              
                                             z
                                             /
                                             2
                                             )
                                          
                                       
                                    
                                 
                              
                           
                           
                              
                                 (30)
                                 
                                    
                                       f
                                       (
                                       z
                                       )
                                       =
                                       
                                          ψ
                                          
                                             Mexhat
                                          
                                       
                                       =
                                       (
                                       1
                                       −
                                       x
                                        
                                       Re
                                        
                                       
                                          z
                                          2
                                       
                                       )
                                       
                                          e
                                          
                                             −
                                             y
                                              
                                             Re
                                              
                                             
                                                z
                                                2
                                             
                                          
                                       
                                       +
                                       i
                                       (
                                       1
                                       −
                                       x
                                        
                                       Re
                                        
                                       
                                          z
                                          2
                                       
                                       )
                                       
                                          e
                                          
                                             −
                                             y
                                              
                                             Re
                                              
                                             
                                                z
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

In this study, x and y in Eq. (30) are selected as 0.15 and 2, respectively.

(4) Hidden layer neuron number (HN): The number of nodes in the hidden layer affects the CVANN performance profoundly. A small number of neurons will be inadequate to understand the relationships in the data. The large number of neurons will increase the network complexity and computational time. Appropriate value for the number of neurons in the hidden layer is often found by trial and error method. This method, however, requires to make several trials to find the appropriate value and this can cause time loss.

By using orthogonal sequence, Taguchi has reduced the number of experiments necessary to obtain the target value. He aimed to design an experiment that minimizes the impact of the factors that cannot be controlled with a smaller number of experiments [35]. The characteristic of the Taguchi method is the ability to simultaneously change the factor and levels during experimental design studies with the help of developed orthogonal sequences and the ability to assess more than one factor and level. The reduction of the number of experiments in implementations provides time and cost benefits. The Taguchi method is a highly beneficial method for providing the best combination amongst different levels of different parameters.

(1) Signal-to-noise (S/N) ratio: The measure used in the Taguchi method to measure and evaluate the quality characteristics is the ratio of signal (S) to the factor of noise (N). Signal value represents the desirable value. The noise factor represents the share of unwanted factors in the measured value [35]. There are many different S/N ratios. Three of them are considered standard. These are given in below.

Smaller-is-better: In these types of problems, the target value of the quality variable y is zero. In this case, the S/N ratio can be defined as Eq. (31):
                           
                              (31)
                              
                                 
                                    η
                                    =
                                    −
                                    10
                                     
                                    log
                                     
                                    (
                                    ∑
                                    
                                       
                                          x
                                          2
                                       
                                       /
                                       k
                                    
                                    )
                                 
                              
                           
                        
                     

Here x is the value of the experimental observations and k is the number of experiments.

Larger-is-better: In these types of problems, the target value of the quality variable y is infinite and in this case, the S/N ratio can be defined as Eq. (32):
                           
                              (32)
                              
                                 
                                    η
                                    =
                                    −
                                    10
                                     
                                    log
                                     
                                    (
                                    ∑
                                    
                                       (
                                       1
                                       /
                                       
                                          x
                                          2
                                       
                                       )
                                       /
                                       k
                                    
                                    )
                                 
                              
                           
                        
                     

Here x is the value of the experimental observations and k is the number of experiments.

Nominal-is-best: In these types of problems, the target value of the quality variable x is specific. In this case, the S/N ratio can be defined as Eq. (33).
                           
                              (33)
                              
                                 
                                    η
                                    =
                                    10
                                     
                                    log
                                     
                                    (
                                    ∑
                                    
                                       
                                          
                                             x
                                             ¯
                                          
                                          2
                                       
                                       /
                                       σ
                                    
                                    )
                                 
                              
                           
                        
                     

Here, 
                           
                              x
                              ¯
                           
                         is the average value for the experimental observations and σ is the standard deviation of the experimental observations.

In all of these three problem types, the aim is to maximize the S/N ratio. According to Taguchi, maximization of the S/N ratio both increases the signal and reduces the variability.

(2) Main effect analysis: After the S/N ratio is calculated, the main impact of each factor is analyzed [36]. Main impact analyses are used to show the impact of each factor on the quality characteristics. These analyses also help us to see the optimum conditions of each factor with the help of response tables and graphics. Main impact analyses can be calculated by using Eqs. (34) and (35).
                           
                              (34)
                              
                                 
                                    
                                       
                                          
                                             e
                                             k
                                          
                                       
                                       ¯
                                    
                                    =
                                    
                                       1
                                       m
                                    
                                    
                                       ∑
                                       
                                          l
                                          =
                                          1
                                       
                                       m
                                    
                                    
                                       
                                          η
                                          
                                             k
                                             l
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (35)
                              
                                 
                                    Δ
                                    e
                                    =
                                    max
                                    (
                                    
                                       
                                          
                                             e
                                             i
                                          
                                       
                                       ¯
                                    
                                    ,
                                    
                                       
                                          
                                             e
                                             2
                                          
                                       
                                       ¯
                                    
                                    ⋯
                                    
                                       
                                          
                                             e
                                             s
                                          
                                       
                                       ¯
                                    
                                    )
                                    −
                                    min
                                    (
                                    
                                       
                                          
                                             e
                                             i
                                          
                                       
                                       ¯
                                    
                                    ,
                                    
                                       
                                          
                                             e
                                             2
                                          
                                       
                                       ¯
                                    
                                    ⋯
                                    
                                       
                                          
                                             e
                                             s
                                          
                                       
                                       ¯
                                    
                                    )
                                 
                              
                           
                        
                     

Here 
                           
                              
                                 
                                    
                                       e
                                       k
                                    
                                 
                                 ¯
                              
                           
                        , is the average S/N ratio of e factor in the kth level and m is the number of each factor in the kth level. η
                        
                           ab
                         is the lth S/N ratio for the kth level, Δe is the main effect value of e factor, while s is the level number for each factor.

This work proposes a novel method for automatic sleep stage scoring. The block diagram of the proposed model is provided in Fig. 4
                     . In data pre-processing stage, a non-causal moving average filter was used to smooth the EEG signals. A band-pass filter of 0.1–60Hz and a notch filter of 50Hz were used to remove artifacts from these signals and eliminate the power line interference in the signals. The signal was separated into 30s epochs after the filtering stage. Each epoch corresponded to a single sleep stage. In feature extraction stage, features are obtained using the DTCWT and statistical feature extraction. The CVANN was used in the classification stage. 2nd level DTCWT is preferred because it gives efficient results. Then, 5 statistical features were obtained from the reduced size of the new data set. These statistical features are presented in Eqs. (36)–(41).
                        
                           (36)
                           
                              
                                 Minimum
                                  
                                 value
                                 :
                                  
                                  
                                 min
                                 [
                                 
                                    x
                                    m
                                 
                                 ]
                              
                           
                        
                     
                     
                        
                           (37)
                           
                              
                                 Maximum
                                  
                                 value
                                 :
                                  
                                  
                                 max
                                 [
                                 
                                    x
                                    m
                                 
                                 ]
                              
                           
                        
                     
                     
                        
                           (38)
                           
                              
                                 Arithmetic
                                  
                                 mean
                                  
                                 (
                                 M
                                 )
                                 :
                                  
                                  
                                 
                                    1
                                    S
                                 
                                 
                                    ∑
                                    
                                       m
                                       =
                                       1
                                    
                                    S
                                 
                                 
                                    
                                       x
                                       m
                                    
                                 
                              
                           
                        
                     
                     
                        
                           (39)
                           
                              
                                 Standard
                                  
                                 deviation
                                 :
                                  
                                  
                                 
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   m
                                                   =
                                                   1
                                                
                                                S
                                             
                                             
                                                
                                                   
                                                      (
                                                      
                                                         x
                                                         m
                                                      
                                                      −
                                                      M
                                                      )
                                                   
                                                   2
                                                
                                             
                                          
                                          
                                             S
                                             −
                                             1
                                          
                                       
                                    
                                 
                              
                           
                        
                     
                     
                        
                           (40)
                           
                              
                                 Median
                                 :
                                  
                                  
                                 
                                    
                                       (
                                       (
                                       S
                                       +
                                       1
                                       )
                                       /
                                       2
                                       )
                                    
                                    
                                       t
                                       h
                                    
                                 
                              
                           
                        
                     
                     
                        
                           (41)
                           
                              
                                 Median
                                 :
                                  
                                  
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      S
                                                      2
                                                   
                                                
                                             
                                          
                                          
                                             th
                                          
                                       
                                       value
                                       +
                                       
                                          
                                             
                                                
                                                   
                                                      S
                                                      2
                                                   
                                                   +
                                                   1
                                                
                                             
                                          
                                          
                                             th
                                          
                                       
                                       value
                                    
                                 
                                 /
                                 2
                              
                           
                        
                     
                  

In Eqs. (36)–(39), x
                     
                        m
                      represents samples in the data set. S indicates how many samples in the data set. Two formulas are used for the median. If the number of samples in the data set is odd number then Eq. (40) is used. If the number of samples in the data set is even number then Eq. (41) is used. These features were classified by CVANN. For ease, the problem is called DTCWT-CVANN. Effective parameter values of CVANN algorithm have been obtained using Taguchi method.

@&#EXPERIMENTAL DESIGN@&#

The data obtained from St. Vincent's University Hospital and University College Dublin was used as the data set [37]. The data of 25 people from this database was used. The characteristics of the subjects were as follows: 21 male and 4 female, aged 50±10 years, with a range from 28 to 68 years; BMI: 31.6±4.0kg/m2, with a range from 25.1 to 42.5kg/m2; AHI: 24.1±20.3, with a range from 1.7 to 90.9.

The data set consisted of received data from two EEG channels (C4–A1 and C3–A2), two EOG channels and one EMG channel. In this study, the C3–A2 EEG channel – which is often preferred for good results in sleep research – was used [7,8,13,14]. The distribution of the sleep periods (30s epoch) of the twenty-five subjects are presented in Table 1
                        . Sleep scoring was carried out by sleep experts.

All the experiments were developed within a MATLAB environment using a of Intel Core i7-2670 QM (2.2GHz) with 8 GB RAM computer. In determining the effective parameter values, the selection of training and test data was performed by the 10-fold cross-validation method. Experiments were repeated 10 times for reliability. Because the 10-fold cross-validation method was used, a total of 100 experiments (10 repeat×10-fold) were performed, and averages of the obtained values were calculated.

In order to determine the effective values of the parameters in CVANN algorithm, Taguchi method was used. Operations applied in determining the effective parameter values are as follows. Experiments were performed on training data. The obtained parameter values were used in classifying of the test data. Table 2
                         classifies the parameters into 5 levels and identifies the lower and upper limits of the parameters. Table 3
                         presents the level values for factors.


                        Table 4
                         presents the results obtained from the experiments undertaken by using the Minitab [38] program. The L25 orthogonal order was used in the experiments. While 3×(55)=9375 experiments are needed for three observation values for experiments that use classic full factorials, only 3×(5×5)=75 experiments are sufficient when L25 orthogonal array (5 parameters with 5 levels in each) is selected. The S, HN, L, M and HA values generated based on this orthogonal order are given in the table. The table also provides the error values obtained by applying the parameters to the problem. All experiments were repeated three times and the observation values were averaged. In this way, the reliability of the results was provided statistically. Root mean square error (RMSE), a reliable assessment parameter in neural networks, is used as the observation value. For RMSE value getting closer to zero means that there is an increase in the prediction ability of model. RMSE is calculated as shown in Eq. (42).
                           
                              (42)
                              
                                 
                                    RMSE
                                    =
                                    
                                       
                                          
                                             1
                                             S
                                          
                                          
                                             ∑
                                             
                                                k
                                                =
                                                1
                                             
                                             S
                                          
                                          
                                             
                                                
                                                   (
                                                   
                                                      u
                                                      
                                                         t
                                                         ,
                                                         k
                                                      
                                                   
                                                   −
                                                   
                                                      y
                                                      
                                                         p
                                                         ,
                                                         k
                                                      
                                                   
                                                   )
                                                
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where S is the number of data patterns, y
                        
                           p,k
                         denotes the prediction term of u
                        
                           t,k
                        .

The results of the experiments obtained by using the Taguchi experimental design were evaluated by converting them into S/N ratios. Table 5
                         provides the S/N values of the level values of the parameters. Since smaller RMSE values are preferred for CVANN, the S/N values are calculated by using the smaller-is-better. The best level value for each parameter is the level value that has the highest S/N ratio. The bold values in the table represent the best level for each factor. For instance, examination of the analysis for DTCWT-CVANN shows that Level-3 is the best value for the iteration value (S) parameter. Table 3 shows that the real value that corresponds to the Level-3 value for the S parameter is 5000. Hence, it can be seen that the most effective value for the S parameter is 5000.

Rank signifies the order of importance for the parameters for the problem and it is calculated based on the main impact value. The main effects of the factor levels are calculated using Eqs. (34) and (35). Table 5 shows the rank order obtained for problem. For instance, investigation of the analysis for DTCWT-CVANN shows that the most effective parameter is iteration value (S) with rank value 1.

It is possible to see the effect values of factors graphically according to S/N values in Fig. 5
                        . It is seen from the rank values presented Fig. 5 and Table 5 that the most effective parameter for DTCWT-CVANN is S: changes in this parameter highly affect S/N values. This parameter is followed by HA, HN, M and L parameters respectively.


                        Fig. 5 displays that the best results for hidden layer activation function are obtained by complex logsig function. Also, Mexican hat function is seen as effective according to S/N values. Examining Table 5 and Fig. 5 shows that, the best parameter set for S/N values: S3-HN4-L3-M3-HA1 for DTCWT-CVANN. Table 6
                         presents the real values for these levels.

@&#RESULTS@&#

The values given in Table 6 were used for parameter values of CVANN algorithm. In the selection of the training and testing data, experiments were carried out using 10-fold cross validation and leave-one-out patient cross validation methods.

In the selection of the training and test data, 10-fold cross-validation method was used for the first stage. Experiments were carried out with the proposed method after training and test data were selected with this method. In the final stage, the success performances of each sleep stage were obtained. Fig. 6
                         shows the accuracy, sensitivity and specificity values obtained for each stage. These values were computed based on a one-versus-all classification method. The analyzed stage was positive, and all other combined stages were negative in this method. Maximum accuracy rate was obtained in the NREM stage 2. Following this stage, in terms of accuracy rate, awake, NREM stage 1, NREM stage 3, REM and NREM stage 4 were found. Good values were obtained in terms of other performance metrics. The melding of stages 3 and 4 which is seen as a significant problem in the literature, has been significantly resolved with the proposed method.

In each sleep stage, errors can be analyzed using confusion matrix. A confusion matrix is presented in Table 7
                        . The table shows the harmony between the experts scores and the proposed method. Experts score was obtained as the result of the consensus of three experts. As shown in Table 7, 93.84% average classification accuracy was obtained using this algorithm. These results show that proposed method can be useful in automatic sleep scoring.

Analyses described above were carried out for classification problem which was established by taking into account the criteria prepared by R&K as mentioned at the beginning of the study. The success of the proposed method in sleep scoring defined by the AASM standards was also measured [39]. In 2007, new rules were set for AASM sleep scoring, and these new standards were used in sleep scoring. The amendments can be summarized as: Respectively, instead of NREM stage 1, 2, 3 and 4; S1, S2, S3 and S4 are used [39]. S3 and S4 stages are combined because the features of NREM stages 3 and NREM stage 4 are very similar. This combined stage has been referred to as slow wave sleep (SWS). The SWS term is used to enhance the physical sense of this stage. This process was performed to classify sleep staging accurately and in a simple way. According to AASM standards, sleep scores were arranged in conjunction with experts. Table 8
                         shows the confusion matrix between the experts score and the proposed method.

As can be seen, in terms of the AASM standards, a 95.42% accuracy rate was obtained. This value is higher than the result obtained in terms of the R&K standards. The most important reason for this is the similarities between the NREM levels in the R&K standards. Because of this similarity, the classification success is low. In particular, combining the S3 and S4 stages in AASM has led to a better result. Fig. 7
                         shows the accuracy, sensitivity and specificity values obtained for each stage. Maximum classification accuracy was obtained in the S2. Following this stage, in terms of accuracy rate, awake, S1, SWS and REM were found. Good values were obtained in terms of other performance metrics.

In using the leave-one-out patient cross validation method, the data obtained from a patient is used as test data. The data obtained from the remaining patients is used to train the model. In the final stage, the test data is classified using the learned model. This process is repeated for every available patient. The expected performance of the evaluated methodology is the average performance across all the tests that have been conducted. In many studies it is reported that this performance measure is more robust and realistic compared to other methods [40]. Tables 9 and 10
                        
                         display the results obtained according to the performance evaluation criteria.

As shown in Tables 9 and 10, the obtained achievement was lower than that obtained using the 10-fold cross validation. The most important reason for this is the possibility that the data of the same person can be in both the test and the training data in the 10-fold cross-validation method. This may lead to an increase in the accuracy rate. There is no such possibility in the case of the leave-one-out patient cross validation method. This is because there is no possibility for the data of the same person to be both in the training and the test data. Thus, it can be said that the patient cross validation method is more reliable.

@&#DISCUSSION@&#

In this study, unlike the approaches presented in the literature, a new method for the classification of sleep stages is presented. Complex-valued classifiers (effective parameter values are determined) are used as a classification algorithm. A summary of information relating to the proposed method is presented below:
                        
                           1.
                           The study examines the influences of complex-valued classifiers on the classification of sleep stages. The advantages of complex-valued algorithms and why they are preferred are mentioned in Sections 2.1 and 2.2. In the light of this information, a system which works with complex numbers has been aimed for the automatic sleep staging system.

In this model, DTCWT is used to obtain the features. The resulting complex-valued feature values are presented as input to the CVANN. The cause for the preference of DTCWT-CVANN hybrid model is to solve the problem with an algorithm running entirely with complex numbers. Thus, effects of complex valued methods on automatic sleep scoring were examined. Furthermore, with this method, possibility of comparison between the methods working completely complex valued and the methods working real valued is obtained.

As with all types of neural networks, the efficiency of CVANNs depends on properly setting the network parameters. For researchers seeking solutions to the problem using CVANN, the most important problem is how to set the parameter values. Proper parameter values are important when it comes to providing a good generalization performance. A solution for this problem is proposed in this study. The Taguchi quality method was used in finding the optimal network structure and in setting the network parameters for CVANN.

In Section 5, our performance of the proposed method has been evaluated according to different aspects. In this context, different experiments have been carried out according to R&K and AASM rules. In addition, in the experiments carried out according to 2 different data selection methods, the success of the proposed method has been evaluated according to different performance measurement metrics. There are 6 sleep stages according to the R&K rule and 5 sleep stages according to the AASM rule. First of all, experiments have been carried out using 10-fold cross validation data selection method. A 93.84% accuracy rate was achieved with the proposed method, according to the R&K rule. In addition, a 95.42% accuracy rate was obtained according to AASM rule. In the next step, we carried out experiments using the leave-one-out patient cross validation data selection method. A 93.02% accuracy rate was achieved with the proposed method according to the R&K rule. A 94.51% accuracy rate was obtained according to AASM rule. The performance of the proposed method was then compared with the existing methods from the literature, as listed in Table 11
                     .

Analysing Table 11, it will be seen that the performance of automatic sleep staging applications involving 5 sleep stages, is generally in the range of 70–95%. Most of the studies which generalize the stages as 3–4 stages, obtained a 90% and above compliance rate. The results show that the proposed method gives better results compared to existing methods in the literature. For an important issue such as diagnostic systems and medical diagnostic, even a 0.1% increases in the accuracy rate is important. In this case, it is foreseen that the work described in this article will contribute significantly to this field. When Table 11 is examined, it can be seen that five sleep stages were classified with a real-valued DWT-ANN model in [11]. With the proposed method, a 93% accuracy rate was obtained. Also, from this point of view, it can be seen that the proposed method gives better results, compared to real-valued versions. In addition, experiments were performed on the same data with real-valued classifiers to compare the computation time between the real and complex valued classifiers. The feed-forward neural network with back-propagation algorithm was preferred as the neural network algorithm. Many models have been tested for the wavelet transform. Among them, the wavelet transform in which 4th degree of Daubechies wavelet is used, gave the best results. In the case of using the real-valued DWT-ANN structure, 92.85% classification accuracy was obtained according to the R&K standard, and 93.52% classification accuracy was obtained according to the AASM standard. In addition, the calculation time was determined as 210.2s for the R&K standard and 204.5s for the AASM standard. In the case of using the DTCWT+CVANN model for calculation time, it was determined as being 125.3s for the R&K standard and 117.6s for the AASM standard. As can be seen, compared to real-valued classifiers, the calculation time decreased significantly when the DTCWT+CVANN model was used.

Both in this study and as well as in several studies in the literature, it is seen that complex valued classifiers give good results compared to real valued classifiers [43]. There may be different reasons behind the successful results of the complex valued classifiers. These are listed below.
                        
                           1.
                           A complex valued neuron which has high functionality may be the reason for the success. High functionality is the ability of a single neuron to learn linearly inseparable input/output mappings in the real domain [26]. Hence, the neuron has the ability of learning these at the initial stage before producing an input at higher level and turning into higher dimensional space. There are studies available in which linearly inseparable problems, such as XOR, are solved by a single neuron with complex valued weights [43].

Input values consist of a single value in real valued neurons. In complex valued neurons, input values are complex numbers consisting of real and imaginary parts. Therefore, a 2-dimensional data input is possible. High-dimensional information and complex valued multiplication operations may bring increasing success.

As a result, the reason why CVANNs give good results may be due to mapping capabilities and high functionality of complex valued neurons.

Methods applied in the study are presented in detail. This does not mean that the applied method is complex and has a greater computational burden. However, there is no significant difference in terms of simplicity and computational load between the applied method and the methods presented in Table 11. The proposed system is a two-stage one, such as the studies given in Table 11. These stages are feature extraction and classification. The applied parameter optimization is not an additional process. The difference between the method described in this article and those of other studies, is the use of complex numbers instead of real values. The use of complex numbers does not create a computational burden. Therefore, the use of complex numbers for signal processing offers a more natural solution. Considering the calculation time, it can be seen that a complex-valued classifier classifies faster than a real-valued classifier. For this reason, the proposed method is a fast method with a low computational burden.

@&#CONCLUSION@&#

This study proposes a hybrid model for neurologists to help them analyze EEG signals for automatic sleep scoring. Important results of the study and future targeted studied are given below.
                        
                           1.
                           Complex valued classifiers gave good results in classification of EEG data and automatic sleep scoring. High accuracy rates can be obtained with the same methods in different areas of biomedical signal processing.

One of the interesting parts of the study is the parameter optimization that significantly affects system performance. Any system which uses the Taguchi method for parameter optimization in CVANNs requires fewer experiments and allows a saving of both time and effort on the part of the modeler.

This method lets the researchers arrive at the best parameter set with only 75 trials instead of the 9375 trials required for a full factorial design.

It is aimed to increase the number of parameters to be optimized in future studies. The parameters that are planned for inclusion are the number of hidden layers, an error function and an output layer activation function.

Consequently, these structures can be helpful as a learning-based decision support system to help doctors for diagnostic decisions. Analyzing the assessment results, the time required to obtain these results was given. Minimizing this period is the target for further studies by performing the system on graphics processors. In addition, for future studies, the creation of a visual interface for the system is also aimed. Creating a visual interface may increase usability.

There are no conflicts of interests.

@&#REFERENCES@&#

