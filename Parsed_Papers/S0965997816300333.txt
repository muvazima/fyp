@&#MAIN-TITLE@&#ARLS: A MapReduce-based output analysis tool for large-scale simulations


@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Hadoop and MapReduce solutions can speed up the analysis of large scale simulation datasets with unstructured format.


                        
                        
                           
                           A Hadoop and MapReduce-based tool has been proposed to semi-automatically analyze large-scale simulation datasets in any format.


                        
                        
                           
                           A Surface-To-Air Missile simulator has been developed to illustrate the viability of Hadoop and MapReduce platforms in processing large scale simulation datasets with unstructured format.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Large-scale simulation

Simulation-based analysis

Distributed computing

Cloud computing

Hadoop and MapReduce

Simulation output analysis

@&#ABSTRACT@&#


               
               
                  As simulations are becoming popular in the analysis of the complex behavior of large-scale systems with immense inputs and outputs, there is an increasing demand to efficiently store, manage, and analyze massive simulation outputs. Hadoop and MapReduce have been used in various applications to speed up the process of analyzing large amounts of datasets. In this paper, we present ARLS (After-action Reviewer for Large-scale Simulations), a MapReduce-based output analysis tool for simulation outputs. ARLS clusters distributed storages using Hadoop and automatically composes Map and Reduce functions to process the simulation outputs. ARLS has been applied to our SAM (Surface-to-Air Missile) simulator. The SAM simulator has been developed to analyze the dynamics of a missile in designing air-defense systems. ARLS takes a large amount of unstructured simulation outputs from SAM simulator, automatically generates Map and Reduce functions to analyze the missile and the aircraft component of SAM simulator, and executes Map and Reduce jobs in parallel. The results of our experiments show that ARLS can efficiently analyze a large amount of unstructured simulation datasets by distributing datasets and computations over the cluster of commodity machines.
               
            

@&#INTRODUCTION@&#

As simulations have come into a wide use in the analysis of large-scale systems with complex structures and dynamics, more engineers have experienced difficulties in interacting with and understanding massive amounts of simulation datasets. For example, simulations on the formation and evolution of large scale structures in the universe used 7.5 million CPU hours and generated 50 terabytes of raw data with additional 25 terabytes of post-processing information [1]. In the domain of catastrophic event risk, millions of simulations need to be quickly performed and large amounts of environmental datasets need to be rapidly analyzed [2]. With the increase of the number of Internet users and its applications, communication network simulators face many challenges related to a rapid analysis of growing teletraffic datasets. The needs of collecting, storing, managing, and analyzing big traffic datasets become imminent for teletraffic simulators to detect and analyze anomaly teletraffic datasets [3]. Defense and military simulations also confront difficulties in analyzing immense datasets coming from live, virtual, and constructive simulators interoperated over the network [4,5]. Therefore, an efficient mechanism is necessary to analyze large-scale simulation outputs and to evaluate the capabilities of the defense system under investigation.

In order to expedite the simulation-based analysis for large-scale systems, it is necessary to speed up not only the amount of time during simulations, but also the one after simulations. However, less attention has been paid to handling the performance required in the course of analyzing outputs after simulations [6]. On the other hand, parallel database management systems, such as Oracle [7], DB2 [8], Teradata [9], and Greenplum [10], as well as new types of large-scale data processing platforms, such as MapReduce and Hadoop [11], impact many scientific and engineering disciplines by providing high-performance computing capabilities in various applications, including bioinformatics, medical science, environmental engineering, e-commerce, manufacturing, and telecommunications.

While the parallel database management systems perform well to process large-scale datasets, they require that the data should conform to a well-defined schema. Unfortunately, many legacy simulators may not produce simulation outputs in a well-structured manner. For example, due to security reasons, legacy missile simulators tend to be available only in a binary format, dumping a large amount of unstructured simulation logs. Since we cannot access the implementation details of the black-box legacy missile simulators, we may need to go through pre-processing steps in order to transform the unstructured simulation logs into well-structured datasets before storing them in parallel database systems. On the other hand, NoSQL solutions, such as Hadoop and MapReduce, permit data to be in any format or even to have no structure at all. Therefore, we can directly handle large-scale and unstructured simulation datasets with Hadoop and MapReduce, without going through additional pre-processing steps. Hadoop is also sufficiently flexible to free us from defining storage requirements in advance. Rather than having a fixed number of data storages, as in the case of parallel database systems, Hadoop and MapReduce platforms allow for scaling the data storages with the increase of the size of outputs.

In this paper, we first illustrate how we can utilize Hadoop and MapReduce to analyze large-scale simulation outputs, with an example of the SAM simulator. The SAM simulator has been developed by us to analyze the dynamics of a missile in designing air-defense systems [12]. The preliminary results obtained from our experiments of SAM simulator suggest that Hadoop and MapReduce model can successfully process the unstructured simulation logs and expedite the analysis process by scaling the required computations over multiple nodes of a cluster. Motivated by these experimental results, we have implemented ARLS, a Hadoop and MapReduce-based output analysis tool, for large-scale simulation logs in any formats. ARLS comprised 8 networked storages clustered by Hadoop and analyzes the large-scale simulation outputs by using the MapReduce computation model. ARLS takes unstructured simulation outputs and stores them in a collection of partitions in HDFS (Hadoop Distributed File System). According to the output measurements a user wants to analyze, ARLS automatically generates Map and Reduce functions. These functions are then injected into the distributed processing framework and are executed to produce the desired analysis results. Results are delivered to users in texts, graphs, and data files.

This paper is organized as follows. Section 2 presents related research and discusses relevant issues in the analysis of large-scale simulation outputs. Section 3 outlines our preliminary research results to show performance improvements by conducting the Hadoop and MapReduce-based simulation output analysis. Section 4 presents ARLS and illustrates how we can utilize ARLS in analyzing large-scale simulation outputs of the SAM simulator. We conclude in Section 5 with a summary and a discussion of further research.

Parallel computing is a type of computation in which multiple computing resources, such as cores in CPU or network-connected computers, simultaneously and concurrently run operations to solve a computational problem [13]. Although parallelism has been used for many years for high-performance computing, the importance of parallel computing has been increasing as large computer clusters become increasingly available. However, due to their high complexity, parallel computing programs are usually more difficult to write than sequential ones. MapReduce was developed by Google [11] as a programming model and an associated implementation to facilitate processing and generating large datasets with a parallel, distributed algorithm on a cluster of commodity hardware. Apache Hadoop was created as an open-source project to make a way to use MapReduce outside Google and now it is becoming a primary platform for processing large amounts of data. Over a half of the Fortune 50 companies are using Hadoop [14]. MapReduce is useful in various applications, including web log analysis [15] and machine learning [16]. One of the most well-known use case of MapReduce is Google's web-search index-building system [17]. The finance industry has also actively adopted Hadoop and MapReduce. JPMorgan Chase, for example, uses Hadoop technology for fraud detection [18]. Morgan Stanley made a scalable portfolio analysis system based on Hadoop and MapReduce [19]. Moreover, MapReduce has already played an important role in many tasks in bioinformatics and life sciences [20], such as genome analysis [21], DNA sequencing [22], and drug development [23].

Unlike the massive enthusiasm around the MapReduce paradigm for large-scale data analysis in various applications, limited research has addressed applying the MapReduce paradigm to simulations and analysis on large-scale engineering systems. In [24], the authors pointed out that traditional molecular dynamics simulation running on parallel programming platforms, such as MPI, OpenMP, or Condor, have a limited capability to schedule the failed sub-tasks to another computing node to implement fault-tolerance. Instead, they used the distributed programming model MapReduce in a small Hadoop cluster and showed that this approach could achieve scalability and fault-tolerance to simulate the molecular dynamics of liquid argon. They also demonstrated that the simulation time could be speed up 28 times when the number of argon particles increased to 108,000. In [1], the authors applied the Hadoop system to cosmological simulations. In analyzing massive astrophysical datasets, ranging from 55GB to a few TB, it was found that the Hadoop system offers a consistent speedup to process selection and correlation queries. Also, this study confirmed that the Hadoop systems could perform well as the cluster becomes sufficiently large. In [25], the authors proposed a cloudification method based on the MapReduce paradigm to migrate scientific simulations into the cloud to provide a greater scalability. They analyzed its viability by applying it to a real-world railway power consumption simulator and showed that the MapReduce paradigm could be suitable for resource intensive simulations and multidimensional analysis.

Based on the previous research overviewed above, our claim is that MapReduce framework provided by Hadoop is well suited to address the following research issues relevant for the analysis of large-scale simulation outputs:

                        
                           •
                           To deal with unstructured outputs: Large-scale simulations frequently involve legacy simulators. Since legacy simulators usually do not allow analysts to access implementation details, analysts usually go through pre-processing steps in order to translate the unstructured outputs into a common format for analysis purposes. Unfortunately, the translation process is expensive and, sometimes, cannot be undertaken, if we cannot define a schema on the outputs from legacy simulators. Hadoop is flexible in terms of accepting data of any type, regardless of structure; therefore, it can be efficiently used in interoperating with legacy simulators.

To deal with scalability of simulation outputs: Simulation-based analysis is often conducted by many experts coming from different disciplines. The kinds and resolutions of outputs could be dynamically changed, as interesting events randomly occur during simulations. Therefore, it is difficult to predict the amount of simulation outputs in advance. Rather than having a fixed number of high-end data storages and servers, it is better to line up a large number of low-end storages and servers and gradually scale them up as the size of outputs becomes large, as in the case of Hadoop.

To support complex output analysis: Output measurements have a wide spectrum of complexity, ranging from simple statistics to complex values. Map and Reduce functions have the full generality and offer flexibility to express any output analysis algorithms. This feature is particularly beneficial in terms of computing complex and customized output measurements that typical database queries and their combinations are not sufficient to answer.

This paper presents the design and implementation of ARLS, a Hadoop and MapReduce-based simulation output analyser. Although, as discussed above, MapReduce is a preferable option to deal with large-scale simulation outputs, one of the problems is the difficulty to develop the Map and Reduce functions from scratch. We would like to automate the development of Map and Reduce functions in ARLS, so that users can easily utilize ARLS in various simulations and applications.

This section presents our experimental results on the MapReduce-based output analysis of SAM Simulator. We will start with a brief introduction on Hadoop and MapReduce and further illustrate how we can benefit from them by evaluating experimental results.

Hadoop is an open-source Apache project for reliable and scalable distributed processing of large data sets across clusters. It is widely used by leading technology companies, such as Yahoo, Amazon, and Facebook, and now it becomes a de facto standard for parallel and distributed processing on Big Data in industry. The core of Apache Hadoop comprises two components: HDFS and MapReduce.

HDFS is a distributed and scalable file-system written in Java for the Hadoop framework. HDFS is typically composed of a NameNode that manages the cluster metadata and DataNodes that stores actual data. HDFS splits files into fixed-sized blocks and stores them into multiple hosts in the cluster. Each block of the files is independently replicated at multiple DataNodes for the reliability. HDFS can handle tens of petabytes of storage.

MapReduce is a programming model for processing huge datasets with a parallel algorithm using a large number of computing machines. A MapReduce program is composed of two functions written by a user: Map and Reduce. The Map function takes input data, processes each, and generates output key/value pairs. Reduce functions merge all intermediate values associated with the same intermediate key and generate final results. To group all intermediate values with the same intermediate key, the MapReduce framework typically sorts the intermediate key/value pairs by keys and passes them to the Reduce function. Fig. 1
                         shows an execution workflow of a MapReduce.

One of the most important reasons for the popularity of MapReduce is that it is possible to use MapReduce without building any complex and expensive infrastructure. Cloud computing providers, such as Amazon, Microsoft, and Google, offer their platform as a service. AWS (Amazon Web Services), for instance, is one of the most popular public cloud services, which offers a collection of services that make up a cloud computing platform by Amazon.com. AWS consists of more than 30 services including Amazon EC2 (Elastic Compute Cloud), Amazon S3 (Simple Storage Service), and Amazon EMR (Elastic MapReduce). Amazon EC2 allows its users to rent virtual computing servers whenever they need. It provides a scalable computing capacity in the cloud without having any real computing hardware. Amazon S3 is an online file storage service where users can store any object (computer file) up to 5 terabytes in size. Amazon EMR provides a simple and easy way to analyze vast amounts of data using a Hadoop framework on top of a resizable cluster of Amazon EC2 instances. It executes MapReduce operations on the large amount of data stored in Amazon's data storage services, such as Amazon S3 and Amazon Dynamo DB.

@&#EXPERIMENTAL RESULTS@&#

By now, we have applied the MapReduce-based output analysis on SAM simulator by using AWS. This section introduces our SAM simulator and reports relevant experimental results.

The objective of our SAM simulator is to examine the capability of the SAM in designing air defense systems. Our SAM Simulator consists of Aircraft, AA (Anti-Air) Radar, Launcher, and Missile components (see Fig. 2
                           ).


                           Aircraft is an object that flies to the mission area. If Aircraft detects an approaching missile in the air defense system, it starts a counter maneuver by turning horizontally, based on the azimuth value, φ, from Eq. (1).

                              
                                 (1)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   φ
                                                   ˙
                                                
                                                =
                                                
                                                −
                                                
                                                
                                                   
                                                      a
                                                      n
                                                   
                                                   V
                                                
                                                ,
                                             
                                          
                                       
                                    
                                 
                              
                           where V is velocity, and an
                            is acceleration for the horizontal turn [26].


                           AA Radar component is responsible for searching the aircraft. If a target aircraft is closer than a lethal range, AA Radar detects the aircraft and sends position, orientation, and velocity of the target aircraft to Launcher. Launcher component is responsible for launching a surface-to-air missile. Missile is a flying object that flies to the target aircraft. Based on the target aircraft information, Launcher calculates the commanded coordinates, Xc, Yc, Zc, for homing guidance [27, 28]. Upon the launching signal from Launcher component, Missile starts the inertial navigation and flies to the commanded coordinates calculated by Eq. (2).

                              
                                 (2)
                                 
                                    
                                       
                                          
                                             
                                                X
                                                c
                                             
                                          
                                          
                                             =
                                          
                                          
                                             
                                                
                                                
                                                   
                                                      
                                                         X
                                                         DR
                                                      
                                                      +
                                                      
                                                      
                                                         (
                                                         
                                                            
                                                               V
                                                               DR
                                                            
                                                            *
                                                            
                                                               t
                                                               A
                                                            
                                                         
                                                         )
                                                      
                                                      sin
                                                      
                                                         φ
                                                         DR
                                                      
                                                   
                                                   2
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                Y
                                                c
                                             
                                          
                                          
                                             =
                                          
                                          
                                             
                                                
                                                
                                                   
                                                      
                                                         Y
                                                         DR
                                                      
                                                      +
                                                      
                                                      
                                                         (
                                                         
                                                            
                                                               V
                                                               DR
                                                            
                                                            *
                                                            
                                                               t
                                                               A
                                                            
                                                         
                                                         )
                                                      
                                                      cos
                                                      
                                                         φ
                                                         DR
                                                      
                                                   
                                                   2
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                Z
                                                c
                                             
                                          
                                          
                                             =
                                          
                                          
                                             
                                                
                                                
                                                   Z
                                                   DR
                                                
                                                +
                                                100
                                                ,
                                             
                                          
                                       
                                    
                                 
                              
                           where (XDR, YDR, ZDR
                           ) is the aircraft location detected by AA Radar and VDR
                            and φ
                              DR
                            are the velocity and azimuth of the aircraft, respectively. t
                           A is the expected time when Missile encounters the aircraft.

After completing the inertial navigation, Missile changes its trajectory by receiving signals from a seeker to search and track the target aircraft.

Our task is to analyze the hit ratio of the missile in AAW (Anti-Air Warfare). Unfortunately, we cannot access the implementation details on the homing guidance of the Missile, since, for security reasons, it has been handed to us in a black-box component (i.e. DLL) by a third party [29]. This necessitates taking the unstructured simulation logs directly from Missile (see Fig. 3
                           ) and applying Hadoop and MapReduce platforms to analyze the hit ratio of the missile in various AAW scenarios. Parallel database systems, such as Teradata and Greenplum, are inefficient for our analysis task, since it is difficult to define the database schema to embrace the different formats over the entire simulation time.

We experimented the SAM simulator by varying the initial positions of the aircraft. Simulation logs collected from the experiments were up to 1TB. In order to conduct the analysis procedure, we developed Map and Reduce functions. These functions filter out a series of distance values between the aircraft and the missile and establish the minimum distance value among them. If the minimum distance was within the hit threshold, we concluded that the missile successfully hit the aircraft. This procedure was repeated for the entire sets of simulation logs.

We clustered data nodes with Hadoop and loaded simulation logs from SAM simulator by using Amazon S3 (Simple Storage Service). Map and Reduce functions were developed to determine a set of hit ratios of the missile. Amazon EMR takes Map and Reduce functions in JAR format, distributes the datasets, and processes Map and Reduce jobs across a resizable cluster of Amazon EC2 instances.

We measured the amount of time to analyze a set of hit ratios of the missile by varying the number of clusters in Hadoop and compared it with the time on a single node. As summarized in Table 1
                           , Hadoop exhibits a performance improvement to process the large simulation outputs. For example, the amount of processing time to process 120
                           G simulation logs on a single node is 3,506s, which becomes 1,680s on 5 task nodes. The speedup becomes apparent as the size of simulation logs increases from 120G to 1T. Furthermore, the amount of speedup increases as more nodes are added to the cluster. The experimental results presented in Table 1 confirm that Hadoop offers a significant performance improvement in analyzing large-scale and unstructured simulation outputs. Our results also demonstrate that the amount of speedup is consistent as the size of the datasets increases.

Although we successfully demonstrated the viability of the Map/Reduce solution for analyzing a large amount of unstructured simulation outputs, our experiments further suggest the following issues to consider:

                              
                                 •
                                 The Map/Reduce solution requires a set-up time to start several new processes on remote nodes. If the set-up time overtakes the entire analysis time, the improvement might be dwarfed. If the size of datasets becomes larger on the same clustered nodes, the amount of overhead takes a small portion over the entire analysis time and, thus, the amount of speedup becomes more apparent. The results presented in Table 1 confirm this overhead – the speedup amount goes from 47.88 to 35.62 as the data size increases, although the Hadoop configuration remains the same.

Since distributed computing requires coordination between networked nodes, the Map/Reduce solution requires communication overheads. With the increase of the number of nodes, this communication overhead increases as well. With small clusters of fewer than 5 task nodes with 2 core nodes and 1 master node, the processing time to process 1TB is reduced by 35%. This improvement is dwarfed as the node size doubles. We expected that the processing time would be halved (i.e. 35/2=18%), but the actual speedup percentage was only 20%. This might be explained by the extra coordination that Hadoop must handle for the doubled data nodes. Fig. 4
                                     shows that the amount of speedup is not proportional to the number of clusters due to communication overheads.

These drawbacks are well-known in the Map/Reduce research group and several improvements have been suggested, such as BSP (Bulk Synchronous Processing) and RDD (Resilient Distributed Dataset) [30-31]. In order to improve the amount of speedup, these techniques can be combined into the Hadoop and Map/Reduce platforms.

We developed ARLS in order to help analysts to use Hadoop and MapReduce in the large-scale simulation output analysis and to improve the analysis performance (see Section 3). A brief introduction on ARLS can be found in our previous work [32].

ALRS clusters 8 nodes of Intel® Xeon 2.5GHz machines with 16GB of main memory. The total number of cores for distributed processes is 10 and the number of threads is 20. We used Hadoop version 2.4.0, Java 1.7.0_75 on Ubuntu 14.04 LTS to develop ARLS. This section presents the architecture of ARLS and illustrates how we can benefit from ARLS through an example.

There are several considerations in order to design a semi-automated output analysis tool for large-scale and unstructured simulation outputs. Firstly, we need a mechanism that (1) filters out the desired datasets from the unstructured simulation outputs, and (2) stores them in Hadoop. Another consideration is a mechanism that automatically generates Map and Reduce program codes to compute the desired output measurements. A meta-data file must be also considered in order to interact with users. Taking these considerations into account, the proposed ARLS architecture is shown in Fig. 5.
                        
                     


                        Journaler loads the simulation datasets into the HDFS. It also reads a meta-data file, described in XML, along with the datasets. The meta-data file contains the information on the undertaken simulation, for example, model name, simulation time, the name of attributes and their types. Analyzer extracts the information on the attributes (i.e. minimum requirements of XML) in order to help users select a set of attributes under investigation (i.e. x, y, z position), the measurements to analyze (i.e. min, max, average), and the formats of the analysis results (i.e. graphs, texts, data file). Based on these requirements, Analyzer produces Map functions that filter out the desired text strings, and generate key/value paired datasets from them. Analyzer also produces Reduce functions that merge or aggregate all values associated with the same key (i.e. the same simulation time). Map and Reduce functions are then combined into the MapReduce jobs and submitted to JobTracker, a master node in Hadoop. JobTracker splits the input file into several splits and distributes tasks to multiple worker nodes, called TaskTrackers. Each TaskTracker starts the Map task on the corresponding split and stores a number of key/value pair as an intermediate result. After finishing the Map tasks, the JobTracker selects several TaskTrackers for the reduce phase. Each selected TaskTracker reads the corresponding intermediate results remotely and executes the reduce function. It aggregates key/value pairs and generates the final output file. The JobTracker keeps monitoring the progress of each step and the health status of each TaskTracker. If a TaskTracker fails or crashes, the JobTracker reassigns the failed task to a different TaskTracker. Visualizer is working outside the Hadoop system. In ARLS, all the analysis results are produced in .dat or .csv, according to users’ preferences. These data files are stored in HDFS and can be sent to Visualizer upon request. Users interact with Visualizer to specify what kinds of graphs they want to draw. Visualizer takes the analysis results from HDFS and delivers them to users in the desired format. Further details on the ARLS components and an example are provided in Section 4.2.

We have applied ARLS to analyze SAM simulator. In order to test the effectiveness of a missile in conceptualizing air-defense systems, we have simulated SAM simulator with various AAW scenarios. Aircraft approaches to the mission area from the northeastern side of AA Radar. We have also considered the counter maneuver of the aircraft against the missile; if an approaching missile is detected, the aircraft performs a sharp right turn to escape from a possible hit. A total of 1,210 simulations have been conducted, which produced 67.7
                        GB simulation logs.

ARLS reads the following XML meta-file to show the list of attributes in the SAM simulator and interacts with users to conduct the analysis process.


                        
                           
                        
                     

Users specify attributes under investigation, alongside with the analysis measurements and the format of the results (see Fig. 6
                        ). In our experiment, we first selected Rmiss as the attribute to analyze. Rmiss is the distance between the aircraft and the SAM. The Rmiss trajectory shows a snapshot of the engagement between the missile and the target aircraft. If the missile is successfully guided to the target with the initial speed and operating range, Rmiss values consistently decrease. Conversely, if the target aircraft detects the missile and successfully changes its current trajectory by a sharp right turn, Rmiss values start to increase. If the minimum value of Rmiss after the entire simulation is within a hit threshold, it indicates that the missile has hit the aircraft. For this reason, we performed a minimum analysis on Rmiss datasets. Also, we selected the x
                        ,
                        y
                        ,
                        z coordinates of the missile and the aircraft, respectively, as the additional attributes to analyze. In order to trace the trajectory, we performed a time-series output analysis on the coordinates of the missile and the aircraft. Based on these information, ARLS generated Map functions, Reduce functions, and a main driver function (see Fig. 6).


                        Fig. 7
                         shows a part of Map and Reduce functions automatically generated by ARLS to find the minimum Rmiss value. The map function reads each line from a simulation log and filters out Rmiss values. The reduce function collects all (key, Rmiss) datasets and then determines the minimum Rmiss for the simulation. The Map and Reduce functions are applied to all 1,210 simulation logs over the data clusters, delivering the final results to the output database. The final results can be reported in Excel data sheets and various graphs on ARLS. Fig. 8
                         shows the Rmiss graph on ARLS when the missile hits the target aircraft. We can also use ARLS data files to draw various graphs with commercial software products. Fig. 8 shows a Matlab graph that depicts the trajectory of the target aircraft and the missile when the missile hits the target aircraft. Currently, Visualizer can only draw 2D graphs (see Fig. 8(a)). The 3D Matlab graphs (see Fig. 8(b)) were drawn manually by using the data files in ARLS.


                        Fig. 9
                         shows that the performance of the missile is related to the initial position of the target aircraft. If the aircraft approaches the mission area far from the east side of the anti-air radar, it has a low level of survivability. This is because the anti-air radar is able to detect the target aircraft as early as possible, so that the missile has enough time to guide itself to the target. Conversely, if the aircraft starts its navigation close to the lethal range of the air-defense system, the anti-air radar loses the golden time to detect the target aircraft and wake the launcher to fire and guide the missile. Fig. 9 shows that the performance of the missile is also related to the avoidance tactic of the target aircraft. Upon detection by a radar system, an aircraft makes a sharp right turn and changes its current trajectory. If the aircraft approaches the mission area far from the north side of the anti-air radar, this sharp right turn only makes the aircraft closer to the Launcher so that the missile has a high chance to hit it down. As shown in Fig. 9, the minimum Rmiss values are high, where the initial position of the target aircraft is within the detection range of the anti-air radar and biased to the south.

In order to evaluate the performance improvement that ARLS could achieve in analyzing SAM effectiveness, we have measured the processing time to determine 1,210 minimum Rmiss values from 67.7
                        G unstructured simulation logs. According to the experiments, it took 1,870
                        s to process the simulation outputs on a single node. Meanwhile, ARLS was able to produce the same output results within 960
                        s. This suggests that, by distributing computations over 8 nodes, ARLS can perform the output analysis almost 2 times faster. We can also expect that the amount of speedup becomes appealing as the size of the simulation logs increases (see Section 3).

@&#CONCLUSION@&#

As simulations become an indispensable technique to analyze scientific, social, and engineering problems, there is an increasing need to efficiently store, manage, and process large-scale datasets. Recent advances in high-performance computing technologies, such as Hadoop and MapReduce, are believed to be viable solutions to deal with large-scale simulation datasets, especially if they are unstructured. In this paper, we have implemented SAM simulator and conducted extensive experimentations to confirm the benefits of Hadoop and MapReduce platforms. According to our results, Hadoop offers a significant improvement in analyzing large-scale simulation outputs. The performance improvement was consistent as we added more data nodes to process larger datasets. Motivated by the preliminary experimental results, we developed ARLS as a tool to process large-scale simulation datasets in any format. ARLS stores large datasets in the clustered storages over the distributed computing environment, automatically generates Map and Reduce functions to analyze the datasets, and executes Map and Reduce jobs in parallel, dealing with data partitioning, scheduling, and inter-machine communication. We illustrated the viability of ARLS with the example of SAM simulator in designing air-defense systems and showed how ARLS could efficiently process simulation outputs within a practical time bound.

We are now improving the Visualizer component of ARLS with HTML 5. The new Visualizer can draw various graphs including 3D plots. It supports easy reading and navigation with minimum of resizing, panning, and scrolling across a wide range of devices, from desktop computer monitors to mobile phones, without reprogramming web pages to fit a specific device [33].

There are numerous challenging yet interesting research topics to improve ARLS. We would like to apply ARLS to various application domains and improve it as we learn from future lessons. We believe that recent solutions in BSP and RDD might be the possible ways to improve the processing time of ARLS. In order to improve the applicability and generality of ARLS, we would like to extend the set of output measurements, including the relation analysis based on data mining techniques, automatic sensitivity analysis, and other complex measures that traditional RDBMS queries can hardly answer. We would also like to have an intelligent front-end that consults with users to figure out what they want for the analysis and represents their preferences with patterns. By having this powerful pattern-matching capability, ARLS will be useful for various engineering applications and problem solutions. Real-time output analysis is another prospective research area to improve ARLS. Rather than delivering analysis results in a batch mode, we would like to conduct the output analysis in real-time. To this end, we plan to adopt scalable real-time data analysis platforms, such as Apache Spark [34] and Apache Storm [35], on top of the current Hadoop-based ARLS. Once established, we can utilize ARLS in various real-time simulations and their analysis, such as real-time large scale environment simulations, real-time dashboards in manufacturing simulators, and real-time COP (Common Operational Picture) in live, virtual, and constructive simulators.

@&#ACKNOWLEDGMENTS@&#

This work was supported by Defense Acquisition Program Administration and Agency for Defense Development under the contract UD140022PD, Republic of Korea, and MyongJi University.

@&#REFERENCES@&#

