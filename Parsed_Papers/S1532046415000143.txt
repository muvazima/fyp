@&#MAIN-TITLE@&#Learning vector representation of medical objects via EMR-driven nonnegative restricted Boltzmann machines (eNRBM)

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We introduce a novel framework called eNRBM to model electronic medical records.


                        
                        
                           
                           Medical objects such as disease and intervention are embedded into a vector-space.


                        
                        
                           
                           The embedding facilitates manipulations and visualization using existing tools.


                        
                        
                           
                           
                              eNRBM learning is guided by clinical structures extracted from EMRs.


                        
                        
                           
                           
                              eNRBM displays factors grouping, and predicts suicide risk better than clinicians.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Electronic medical records

Vector representation

Medical objects embedding

Feature grouping

Suicide risk stratification

@&#ABSTRACT@&#


               
               
                  Electronic medical record (EMR) offers promises for novel analytics. However, manual feature engineering from EMR is labor intensive because EMR is complex – it contains temporal, mixed-type and multimodal data packed in irregular episodes. We present a computational framework to harness EMR with minimal human supervision via restricted Boltzmann machine (RBM). The framework derives a new representation of medical objects by embedding them in a low-dimensional vector space. This new representation facilitates algebraic and statistical manipulations such as projection onto 2D plane (thereby offering intuitive visualization), object grouping (hence enabling automated phenotyping), and risk stratification. To enhance model interpretability, we introduced two constraints into model parameters: (a) nonnegative coefficients, and (b) structural smoothness. These result in a novel model called eNRBM (EMR-driven nonnegative RBM). We demonstrate the capability of the eNRBM on a cohort of 7578 mental health patients under suicide risk assessment. The derived representation not only shows clinically meaningful feature grouping but also facilitates short-term risk stratification. The F-scores, 0.21 for moderate-risk and 0.36 for high-risk, are significantly higher than those obtained by clinicians and competitive with the results obtained by support vector machines.
               
            

@&#INTRODUCTION@&#

Modern electronic medical records (EMRs) have changed the landscape of clinical data collecting and sharing, facilitating efficient care delivery [1]. The data in EMR offers insights into key questions: What are the comorbidity patterns? [2] What are the relationships between diseases and interventions under multimorbidity? What is the risk of adverse events for this patient? [3] However, it remains an open problem in formulating efficient mining techniques to discover these answers [4]. This is partly due to the complexity of the EMR data. The EMR contains a mixture of static, temporal, type-specific data packed in irregular episodes. Huge effort is required for extracting meaningful features [4] and developing prognostic models from EMR [5].

We hypothesize that the answers lie in unsupervised learning of EMR representations [4,6]. Unsupervised learning lets clinical patterns emerge through the learning process. We approach the problem by utilizing a recent advancement in deep learning [7,8]. In particular, we adopt restricted Boltzmann machines (RBM) [9] as a generative model of EMR. RBM has a bipartite structure, in which an input layer is connected to a representation layer. The input layer consists of observed clinical variables over multiple periods of time. The representation layer is composed of unobserved binary factors, which act as the underlying aspects of illness and healthcare processes. These aspects jointly generate clinical observables. The RBM transforms raw, high-dimensional and mixed-type EMR data into a homogeneous representation. Clinical objects such as disease, procedure and health trajectory are embedded in the same vector space. The embedding facilitates visualization, manipulation and risk prognosis. See Fig. 1
                      for a graphical illustration of the RBM-based framework.

The standard RBM, however, suffers from two key limitations that hinder its usability in the clinical context. First, the embedding coefficients can be either positive or negative, making interpretation of group membership difficult. Second, the RBM assumes unstructured inputs but ignores explicit structures inherent in the EMR, leading to incoherent grouping.

We modify the RBM to overcome these limitations. First, the embedding coefficients are constrained to be nonnegative. This leads to model sparsity where only a few embedding coefficients are non-zeros. Each latent factor corresponds to a small group of features which potentially play the role of a derived phenotype. Second, model learning is guided by clinical structures derived from the disease taxonomy, the procedure hierarchy and the temporal progression of illness and care. These two modifications result in a novel model called EMR-driven nonnegative RBM (
                        
                           e
                           NRBM
                        
                     ).

We validate the proposed 
                        
                           e
                           NRBM
                        
                      on a large cohort of 7578 mental health patients in several tasks, including disease/procedure embedding and visualization, comorbidity grouping, and short-term suicidal risk stratification. We demonstrate that 
                        
                           e
                           NRBM
                        
                     -based embedding leads to meaningful grouping of diseases and interventions. The merit of the proposed method is highlighted by comparing the predictive performance on risk stratification against support vector machines.

The rest of the paper is organized as follows. Section 2 introduces restricted Boltzmann machines. Section 3 presents the main contributions of the paper: (a) an introduction of the RBM as a generative model of the EMR; (b) introducing medical object embedding; (c) introducing nonnegative coefficients into the RBM leading to coherent feature grouping and more compact representations; and (d) adding structural constraints into the RBM by exploiting inherent structures in the EMR. This is followed by an experimental section which demonstrates the capacity of the proposed methods on a large cohort of mental health patients. Finally, Section 5 discusses findings, limitations and future work.

Restricted Boltzmann machine (RBM) is a type of neural networks. As illustrated in Fig. 2
                     , a RBM is a bipartite graph consisting of: (i) an input layer of visible units that encode the observables (e.g., disease occurrences), (ii) a latent layer of hidden units, and (iii) weighted connections between every visible unit to hidden units [8,9]. The RBM differs from standard neural networks in important ways. First, it is stochastic rather than deterministic: Variables are randomly distributed according to a joint distribution specified by the model. Second, the network is undirected allowing information to propagate in both directions (feedforward and feedback modes). And finally, learning is unsupervised without labels.

Let 
                        
                           v
                        
                      denote the set of visible variables: 
                        
                           v
                           =
                           
                              
                                 
                                    
                                       
                                          v
                                       
                                       
                                          1
                                       
                                    
                                    ,
                                    
                                       
                                          v
                                       
                                       
                                          2
                                       
                                    
                                    ,
                                    …
                                    ,
                                    
                                       
                                          v
                                       
                                       
                                          N
                                       
                                    
                                 
                              
                           
                           ∈
                           
                              
                                 
                                    
                                       0
                                       ,
                                       1
                                    
                                 
                              
                              
                                 N
                              
                           
                        
                      and 
                        
                           h
                        
                      the set of hidden factors: 
                        
                           h
                           =
                           
                              
                                 
                                    
                                       
                                          h
                                       
                                       
                                          1
                                       
                                    
                                    ,
                                    
                                       
                                          h
                                       
                                       
                                          2
                                       
                                    
                                    ,
                                    …
                                    ,
                                    
                                       
                                          h
                                       
                                       
                                          K
                                       
                                    
                                 
                              
                           
                           ∈
                           
                              
                                 
                                    
                                       0
                                       ,
                                       1
                                    
                                 
                              
                              
                                 K
                              
                           
                        
                     . Let 
                        
                           W
                           ∈
                           
                              
                                 R
                              
                              
                                 N
                                 ×
                                 K
                              
                           
                        
                      be the weight matrix connecting the hidden and visible units. The connection weight 
                        
                           
                              
                                 W
                              
                              
                                 nk
                              
                           
                        
                      measures the association strength between the visible unit i and the hidden unit k, that is the tendency of these two units being co-active. The interaction between variables defines an energy function:
                        
                           (1)
                           
                              E
                              
                                 
                                    
                                       v
                                       ,
                                       h
                                    
                                 
                              
                              =
                              -
                              
                                 
                                    
                                       
                                          
                                             a
                                          
                                          
                                             ⊤
                                          
                                       
                                       v
                                       +
                                       
                                          
                                             b
                                          
                                          
                                             ⊤
                                          
                                       
                                       h
                                       +
                                       
                                          
                                             v
                                          
                                          
                                             ⊤
                                          
                                       
                                       Wh
                                    
                                 
                              
                           
                        
                     where 
                        
                           a
                           ,
                           
                           b
                        
                      are the bias coefficients of hidden and visible units, respectively. The model admits the Boltzmann distribution:
                        
                           (2)
                           
                              P
                              
                                 
                                    
                                       v
                                       ,
                                       h
                                    
                                 
                              
                              
                              ∝
                              
                              
                                 
                                    e
                                 
                                 
                                    -
                                    E
                                    
                                       
                                          
                                             v
                                             ,
                                             h
                                          
                                       
                                    
                                 
                              
                           
                        
                     The RBM is a generative model of data whose density is 
                        
                           P
                           (
                           v
                           )
                           =
                           
                              
                                 ∑
                              
                              
                                 h
                              
                           
                           P
                           (
                           v
                           ,
                           h
                           )
                        
                     .

The parameters are often estimated by maximizing the data likelihood 
                        
                           P
                           (
                           v
                           )
                        
                     . For example, an update rule for mapping weights is
                        
                           (3)
                           
                              
                                 
                                    W
                                 
                                 
                                    ik
                                 
                              
                              ←
                              
                                 
                                    W
                                 
                                 
                                    ik
                                 
                              
                              +
                              η
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         v
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   
                                                      
                                                         ρ
                                                      
                                                      
                                                         k
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   P
                                                
                                                
                                                   ∼
                                                
                                             
                                          
                                       
                                       -
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         v
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   
                                                      
                                                         h
                                                      
                                                      
                                                         k
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             P
                                          
                                       
                                    
                                 
                              
                           
                        
                     where 
                        
                           
                              
                                 ρ
                              
                              
                                 k
                              
                           
                        
                      represents 
                        
                           P
                           (
                           
                              
                                 h
                              
                              
                                 k
                              
                           
                           =
                           1
                           |
                           v
                           )
                           ,
                           
                           
                              
                                 P
                              
                              
                                 ∼
                              
                           
                        
                      denotes empirical distribution of the visible data, 
                        
                           
                              
                                 
                                    
                                       ·
                                    
                                 
                              
                              
                                 P
                              
                           
                        
                      denotes expectation with respect to distribution P, and η is learning rate. The data expectation 
                        
                           
                              
                                 
                                    
                                       
                                          
                                             v
                                          
                                          
                                             i
                                          
                                       
                                       
                                          
                                             ρ
                                          
                                          
                                             k
                                          
                                       
                                    
                                 
                              
                              
                                 
                                    
                                       P
                                    
                                    
                                       ∼
                                    
                                 
                              
                           
                        
                      is easy to evaluate. The model expectation 
                        
                           
                              
                                 
                                    
                                       
                                          
                                             v
                                          
                                          
                                             i
                                          
                                       
                                       
                                          
                                             h
                                          
                                          
                                             k
                                          
                                       
                                    
                                 
                              
                              
                                 P
                              
                           
                        
                      is computationally difficult but can be efficiently approximated by short Markov chains starting from the observations 
                        
                           v
                        
                      in a procedure known as “contrastive divergence” [10].

The EMR data broadly consist of two types: static information (such as gender, ethnic background) and healthcare trajectory. The trajectory is recorded as a series of time-stamped events (such as admission, diagnosis or intervention).
                           1
                           Demographic factors such as age, location and income do change over time, but they might be considered as static at the present time if their interaction with clinical variables are not obvious.
                        
                        
                           1
                         We are mainly interested in discrete events and assume that continuous and real-valued data such as EEG signals and blood sugar readings have been discretized through existing methods such as temporal abstraction [11]. Static elements naturally form a vector. The entire trajectory is divided into disjoint intervals of predefined lengths. Events occurring within each interval are aggregated and arranged as a sparse vector. All intervals form a temporal matrix, as illustrated in the data layer of Fig. 1.

In RBM-based modeling of EMRs, as illustrated in Fig. 1, all data elements share the same hidden representation layer. The hidden layer is utilized in the tasks of interest (e.g., visualization of patients, diagnosis of a present disease, or prognosis of future risk). Thus, the hidden layer is a mediator between history (recorded illness), present (diagnosis) and future (prognosis). It “explains” the data through:
                              
                                 (4)
                                 
                                    P
                                    (
                                    
                                       
                                          v
                                       
                                       
                                          i
                                       
                                       
                                          1
                                       
                                    
                                    |
                                    h
                                    )
                                    =
                                    σ
                                    
                                       
                                          
                                             
                                                
                                                   a
                                                
                                                
                                                   i
                                                
                                             
                                             +
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      k
                                                   
                                                
                                             
                                             
                                                
                                                   W
                                                
                                                
                                                   ik
                                                
                                             
                                             
                                                
                                                   h
                                                
                                                
                                                   k
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 
                                    
                                       v
                                    
                                    
                                       i
                                    
                                    
                                       1
                                    
                                 
                              
                            represents 
                              
                                 
                                    
                                       v
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 1
                              
                           , and 
                              
                                 σ
                                 (
                                 x
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             1
                                             +
                                             
                                                
                                                   e
                                                
                                                
                                                   -
                                                   x
                                                
                                             
                                          
                                       
                                    
                                    
                                       -
                                       1
                                    
                                 
                              
                           . As all hidden units jointly represent the data, the representation is said to be fully distributed. This makes the representation highly compact: The model can be considered as a giant mixture of 
                              
                                 
                                    
                                       2
                                    
                                    
                                       K
                                    
                                 
                              
                            components with only 
                              
                                 KN
                                 +
                                 K
                                 +
                                 N
                              
                            parameters.

This mixture view is attractive because healthcare is a complex process, and the recorded events are the result of interaction between multiple processes (e.g., the underlying illness, comorbidity, diagnostic decision and intervention), each of which can be captured by one or more hidden units.

The RBM embeds medical objects (e.g., diagnosis codes) and health trajectories into a vector space. Each object i is represented by a row vector 
                              
                                 W
                                 
                                    
                                    
                                       i
                                       •
                                    
                                 
                              
                            in 
                              
                                 
                                    
                                       R
                                    
                                    
                                       K
                                    
                                 
                              
                           . The vector embedding facilitates algebraic manipulations such as similarity calculation and retrieval, translation and rotation, and 2D projection for visualization. See Fig. 4 for an example of diseases embedded in 2D. An entire health trajectory can also be represented in the same space through probabilistic projection:
                              
                                 (5)
                                 
                                    
                                       
                                          ρ
                                       
                                       
                                          k
                                       
                                    
                                    =
                                    P
                                    
                                       
                                          
                                             
                                                
                                                   h
                                                
                                                
                                                   k
                                                
                                             
                                             =
                                             1
                                             |
                                             v
                                          
                                       
                                    
                                    =
                                    σ
                                    
                                       
                                          
                                             
                                                
                                                   b
                                                
                                                
                                                   k
                                                
                                             
                                             +
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      i
                                                   
                                                
                                             
                                             
                                                
                                                   W
                                                
                                                
                                                   ik
                                                
                                             
                                             
                                                
                                                   v
                                                
                                                
                                                   i
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 σ
                                 
                                    
                                       
                                          x
                                       
                                    
                                 
                              
                            is the sigmoid function defined in Eq. (4). The posterior vector 
                              
                                 ρ
                                 =
                                 
                                    
                                       
                                          
                                             
                                                ρ
                                             
                                             
                                                1
                                             
                                          
                                          ,
                                          
                                             
                                                ρ
                                             
                                             
                                                2
                                             
                                          
                                          ,
                                          …
                                          ,
                                          
                                             
                                                ρ
                                             
                                             
                                                K
                                             
                                          
                                       
                                    
                                 
                              
                            represents the entire patient trajectory. This can then be used for classification and prognosis (see Section 4.5 for a demonstration).

For a typical EMR, a practical issue arises since the input features are not binary but counts. We employ a simple solution: features are normalized into the range 
                              
                                 [
                                 0
                                 ,
                                 1
                                 ]
                              
                            and treated as empirical probability. A more theoretical drawback is that the RBM is not effective in organizing features, and does not takes the inherent structures of the EMR into account. In what follows, we show how to modify RBM to tackle these problems.

This subsection presents modifications to RBMs for promoting the grouping of features and enhancing interpretability. We introduce two constraints into the parameter structure: nonnegative weights and EMR-driven smoothness, resulting in a novel model called EMR-driven nonnegative RBM (
                           
                              e
                              NRBM
                           
                        ).

The first modification is to constrain the connection weights 
                              
                                 
                                    
                                       
                                          
                                             
                                                W
                                             
                                             
                                                ik
                                             
                                          
                                       
                                    
                                 
                              
                            to be nonnegative. To enforce nonnegativity, we augmented the data log-likelihood 
                              
                                 log
                                 P
                                 (
                                 v
                                 )
                              
                            with a barrier function 
                              
                                 B
                                 
                                    
                                       
                                          
                                             
                                                W
                                             
                                             
                                                ik
                                             
                                          
                                       
                                    
                                 
                                 =
                                 
                                    
                                       W
                                    
                                    
                                       ik
                                    
                                    
                                       2
                                    
                                 
                              
                            if 
                              
                                 
                                    
                                       W
                                    
                                    
                                       ik
                                    
                                 
                                 <
                                 0
                              
                            and 0 otherwise. Minimizing the augmented log-likelihood would drive negative weights toward zeros.

This leads to several interesting properties. First, the mapping matrix 
                              
                                 W
                              
                            is sparse, that is, only few elements are non-zeros. Second, hidden factors must “compete” to generate data, and thus creating an “explaining away” effect (where only a few latent factors are plausible explanation of the data). The result is a parts-based representation where each hidden unit is responsible to explain a part of the EMR [12].

The “explaining away” effect also leaves some hidden units unused (with near-zero mapping weight vectors 
                              
                                 W
                                 
                                    
                                    
                                       •
                                       k
                                    
                                 
                              
                           ). Thus it offers a natural way to estimate the intrinsic dimensionality of the data. A hidden unit k is declared “dead” if 
                              
                                 
                                    
                                       
                                          
                                             W
                                             
                                                
                                                
                                                   •
                                                   k
                                                
                                             
                                          
                                       
                                    
                                    
                                       1
                                    
                                    
                                       1
                                    
                                 
                                 
                                    
                                       N
                                    
                                    
                                       -
                                       1
                                    
                                 
                                 ⩽
                                 τ
                              
                            for small τ. This capacity is not seen in standard RBMs.

The other modification is based on the inherent structures in the EMR. Due to the progressive nature of health, events often repeat over time. Thus, a disease occurring in consecutive time-intervals results in related features. Other structures are in the hierarchical organization of diseases and interventions, including the disease taxonomy ICD-10
                              2
                              
                                 http://apps.who.int/classifications/icd10.
                           
                           
                              2
                            and the procedure cube ACHI.
                              3
                              
                                 https://www.aihw.gov.au/procedures-data-cubes/.
                           
                           
                              3
                            For example, two diseases that share the same parent in the taxonomy, by definition, possess similar characteristics.

Here we introduce a novel regularization scheme to realize these structures. Assume that the structures can be encoded into a feature graph G whose edges indicate the relatedness between features. Let 
                              
                                 
                                    
                                       γ
                                    
                                    
                                       ij
                                    
                                 
                                 >
                                 0
                              
                            be the relation strength between feature i and j, the relatedness can be realized by minimizing the following smoothness objective:
                              
                                 (6)
                                 
                                    Ω
                                    (
                                    W
                                    )
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             ij
                                          
                                       
                                    
                                    
                                       
                                          γ
                                       
                                       
                                          ij
                                       
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             k
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      W
                                                   
                                                   
                                                      ik
                                                   
                                                
                                                -
                                                
                                                   
                                                      W
                                                   
                                                   
                                                      jk
                                                   
                                                
                                             
                                          
                                       
                                       
                                          2
                                       
                                    
                                 
                              
                           In model estimation, this objective is added to the data log-likelihood, in addition to the nonnegativity constraint mentioned above. The details are presented in Appendix A.

In our implementation, we construct the feature graph as follows. An edge is created if any of the following requirements are met:
                              
                                 •
                                 Two codes share the same two-character prefix. In particular, we use the first two numbers or letters (using ICD-10 for diseases, and ACHI for procedures). For example, F10 (mental disorder due to alcohol) and F17 (mental disorder due to tobacco) are linked since they are children of F1 (mental disorders due to psychoactive substance use). However, F10 and F20 (schizophrenia) do not share a direct relation. We feel that this balances well between the relatedness and specificity of the disease classification.

A code is recorded in consecutive intervals. For example, if F10 is recorded in [0–3] months and [3–6] months prior to a specified date, this constitutes an edge. This is because two close events of the same type would behave similarly.

Our focus is on mental health patients who were under assessment for suicidal risk. Mental health is a global burden that accounts for 14% of the world health expenditure [13]. Among mental health problems, suicidal risk is devastating: suicidal thoughts occur in 10% of the population in their lifetime [14], and suicide attempts happen in 0.3% of the population each year [15]. The risk of suicide has led to mandatory assessments. However, suicide risk assessments are often inaccurate leading to concern over practicality [16,17].

We used a mental health cohort previously extracted from Barwon Health, a large regional hospital in Australia [18,19]. Data was collected between January 2009 and March 2012. The dataset contains 7578 patients (49.3% male, 48.7% under 35) who underwent collectively 17,566 assessments. Any patient who had at least one encounter with the hospital services and one risk assessment was included. Most patients had one assessment (62%), but 3% of patients had more than 10 assessments. Diagnoses are coded using ICD-10. More details are described in [19].

Each assessment was considered as a data point from which a prediction would be made. The future outcomes within 3months following an assessment were categorized into three ordinal levels of risk according to [18]: no-risk, moderate-risk (non-fatal consequence), and high-risk (fatal consequence). The risk classes were decided using a look-up table from the ICD-10 codes. If there were more than one outcome classes, the highest risk class would be chosen. There were 15,272 (86.9%) no-risk outcomes, 1436 (8.2%) moderate-risk and 858 (4.9%) high-risk.

@&#IMPLEMENTATION DETAILS@&#

Following [18,19], we split the 48-month history prior to each risk assessment into non-overlapping intervals: (0–3), (3–6), (6–12), (12–24) and (24–48). The increasing interval widths toward the far past are based on the assumption that events in the far past have less influence on current outcomes. Each interval has the same set of time-stamped variables: 201 diagnoses, 657 procedures, 31 Elixhauser comorbidities, diagnosis related groups (DRG), emergency attendances and admissions. Infrequent diagnoses and procedures were grouped into rare categories. Together with demographic variables (ages in 10year intervals and gender), there were totally 5267 input variables.

The posterior vector 
                              
                                 ρ
                              
                            (Eq. 5) was used as input for logistic regression classifiers (LR) for predicting outcomes. For robustness, the LR was equipped with elastic net regularization [20]. Besides the standard RBM, we employed support vector machines (SVM) which ran on normalized features and PCA-derived features. We used the implementation of SVM in LIBSVM package [21]. As the LR and the SVM are binary classifiers, the one-versus-all strategy was used for this 3-class problem.

For risk stratification, we used 10-fold validation. For each fold, parameters were learnt on the training set and hyperparameters were turned for the best performance on the validation set. Results were reported as an average across folds. For the SVM, we used the linear kernel. For both the RBM and the eNRBM, the numbers of hidden units were set to 
                              
                                 K
                                 =
                                 200
                              
                           . The learning rate was scheduled as 
                              
                                 0.1
                                 /
                                 
                                    
                                       t
                                    
                                 
                              
                            at epoch t. This weight decay helped stabilize the parameter updates towards the end of the learning process. The weights were initialized randomly from 
                              
                                 N
                                 
                                    
                                       
                                          0
                                          ;
                                          0.1
                                       
                                    
                                 
                              
                           , and the biases were from zeros. Parameters were then updated after every “mini-batch” of 100 data points. Learning was terminated after 100 epochs. Hyperparameters of the 
                              
                                 e
                                 NRBM
                              
                            were empirically tuned to obtain accurate data reconstruction and high group coherence, while keeping the F-measure competitive.

To estimate the number of hidden units, we examined the intrinsic dimensionality of data, as described in Section 3.2.1. Fig. 3
                         plots the number of used hidden units against the total number for an 
                           
                              e
                              NRBM
                           
                         estimated on 1005 diagnosis codes. The curves were averaged over a set of thresholds (
                           
                              τ
                              ∈
                              
                                 
                                    
                                       0.01
                                       ;
                                       0.02
                                       ;
                                       …
                                       ;
                                       0.06
                                    
                                 
                              
                           
                        ). The dimensionality stays around 250. To obtain a compact representation, we used 
                           
                              K
                              =
                              200
                           
                         hidden units in subsequent experiments.

To quantify the coherence of feature group, we borrowed the concept from topic modeling [22]. For each group, we kept T member features with largest mapping weights. Let 
                           
                              D
                              
                                 
                                    
                                       
                                          
                                             v
                                          
                                          
                                             i
                                          
                                          
                                             (
                                             k
                                             )
                                          
                                       
                                    
                                 
                              
                           
                         and 
                           
                              D
                              
                                 
                                    
                                       
                                          
                                             v
                                          
                                          
                                             i
                                          
                                          
                                             (
                                             k
                                             )
                                          
                                       
                                       ,
                                       
                                          
                                             v
                                          
                                          
                                             j
                                          
                                          
                                             (
                                             k
                                             )
                                          
                                       
                                    
                                 
                              
                           
                         be occurrences of feature i and feature pair 
                           
                              (
                              i
                              ,
                              j
                              )
                           
                         under factor k, respectively. The group coherence was defined as:
                           
                              (7)
                              
                                 C
                                 (
                                 k
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          T
                                          -
                                          1
                                       
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          j
                                          =
                                          i
                                          +
                                          1
                                       
                                       
                                          T
                                       
                                    
                                 
                                 log
                                 
                                    
                                       1
                                       +
                                       D
                                       
                                          
                                             
                                                
                                                   
                                                      v
                                                   
                                                   
                                                      i
                                                   
                                                   
                                                      (
                                                      k
                                                      )
                                                   
                                                
                                                ,
                                                
                                                   
                                                      v
                                                   
                                                   
                                                      j
                                                   
                                                   
                                                      (
                                                      k
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       1
                                       +
                                       D
                                       
                                          
                                             
                                                
                                                   
                                                      v
                                                   
                                                   
                                                      i
                                                   
                                                   
                                                      (
                                                      k
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        Intuitively, the coherence of a group is large if its members co-occur frequently, relative to the popularity of each member. With 
                           
                              T
                              =
                              10
                           
                        , the 
                           
                              e
                              NRBM
                           
                         had a coherence of 
                           
                              -
                              130.88
                           
                        , higher than that of the standard RBM (
                           
                              -
                              173.3
                           
                        ).

Here we validate the effectiveness of object embedding (Section 3.1.2). Two 
                           
                              e
                              NRBMs
                           
                         were created, one using only diagnoses (called model DIAG), the other using both diagnoses and procedures (called model DIAG
                        
                        +
                        
                        PROC). A RBM was learned using diagnosis codes for comparison.

For each model, the mapping weight matrix 
                           
                              W
                           
                         was examined. Elements of row vector 
                           
                              W
                              
                                 
                                 
                                    i
                                    •
                                 
                              
                           
                         are coordinates of the object i in the embedding space of K dimensions. Objects were projected onto 2D using t-SNE [23]. As shown in Fig. 4
                        , diseases naturally form coherent groups (colored by k-means). Note that t-SNE is a visualization method and it was not involved in computing the embedding of codes.

Similarly, Fig. 5
                         presents the embedding/clustering of both diseases and procedures. Since diseases and procedures are jointly embedded in the same space, their relations can be directly assessed. For several groups, we plotted the top 5 procedures and 5 diagnoses, where the font size was proportional to inverse distances to the group centers. The grouping is meaningful, for example:
                           
                              •
                              
                                 Group 1: Diagnosis C34 (Malignant neoplasm of bronchus and lung) is associated with procedures 543 (Examination procedures on bronchus) and 536 (Tracheostomy).


                                 Group 2: Diagnosis C78 (Secondary malignant neoplasm of respiratory and digestive organs) and C77 (Secondary and unspecified malignant neoplasm of lymph nodes) are associated with procedures 392 (Excision procedures on tongue) and 524 (Laryngectomy).


                                 Group 3: Diagnosis K35 (Acute appendicitis) is associated with procedure 926 (Appendicectomy).

In contrast, the groups produced by RBM in Fig. 6
                         are less coherent and their diagnosis codes do not clearly explain suicide risks.

We compared the discovered groups with the risk factors found in previous work [18]. The relevance of a group is the number of matches in the top 10 risk factors under the group. On average, 4.4 out of 10 risk factors per group found by the eNRBM matched those in [18]. This is higher than the matching rate by the RBM, which was 1.6.

To identify which feature group was predictive of future risk, we used the posterior embedding of patients (see Eq. (5)) as inputs for two logistic regression classifiers, one for the moderate-risk class, the other for the high-risk class. Groups were ranked by their regression coefficients.


                        Table 1
                         presents top five feature groups corresponding to moderate-risk and high-risk classes (model DIAG). Moderate-risk groups consist of abnormality in function findings (ICD-10: R94), non-fatal hand injuries (ICD-10: S6x), mental disorders such as dementia (ICD-10: F03) and (ICD-10: F05), obesity (ICD-10: F66), and potential hazards related to communicable diseases (ICD-10: Z2s). High-risk groups involve self-harms (ICD-10: X6s) as the top risk, followed by poisoning (ICD-10: T39, T5s), hazards related to communicable diseases (ICD-10: Z2s), and finally hand injuries (ICD-10: S5s).

We now report results on suicide risk stratification for a 3-month horizon. Fig. 7
                         shows the relative performance of the 
                           
                              e
                              NRBM
                           
                         (for representation learning) coupled with logistic regression classifiers (for classification), in comparison with support vector machines (SVM) that ran on raw EMR data and on PCA-derived features. Using the full EMR-derived data leads to better results than those using the diagnoses alone, suggesting the capability in data fusion by the 
                           
                              e
                              NRBM
                           
                        .


                        
                        Table 3
                         presents more detailed results. The F-scores achieved by 
                           
                              e
                              NRBM
                           
                         are 0.212 and 0.359 for moderate-risk and high-risk, respectively. The high-risk F-score is already three times better than the performance achieved by clinicians who admitted the risk assessment [18,19]. The F-scores are also competitive with the results obtained by rival methods: SVM on raw features obtained F-score of 0.156 and 0.340; and SVM on PCA-derived features yielded 0.135 and 0.325 for moderate and high-risk, respectively. We ran a bootstrap simulation and found that (i) for moderate-risk, 
                           
                              e
                              NRBM
                           
                         is significantly better than SVM or RBM at 
                           
                              p
                              =
                              0.05
                           
                        ; (ii) for high-risk, there is no statistical difference, largely due to the smaller number of high-risk cases.

@&#DISCUSSION@&#

The 
                           
                              e
                              NRBM
                           
                         belongs to, but differs radically from the rest of the latent variable family used in biomedical fields [24]. The family includes traditional methods such as factor analysis [25] and modern models such as latent Dirichlet allocation [26] and Indian buffet processes [27]. All of these existing models can be represented as directed graphical models whose inference is usually expensive. Importantly, while these methods are effective in analyzing latent factors or thematic structures, they are not typically designed for data representation on which further manipulations can be performed. The 
                           
                              e
                              NRBM
                           
                        , on the other hand, is undirected and permits fast inference and learning on massive high-dimensional data. The 
                           
                              e
                              NRBM
                           
                         offers multiple benefits: nonlinear; compact distributed representation; embedding medical objects into Euclidean space; and feature grouping. Importantly, the 
                           
                              e
                              NRBM
                           
                         can compute predictive representations.

The feature grouping capability facilitates better understanding of feature interactions. This is critical in modern medicine where multimorbidity is the rule, not exception, especially among the elderly [28]. The illness trajectories and healthcare processes become increasingly interwoven [29], and it is crucial to automatically disentangle these dependencies.

The direct modeling of dependencies between clinical variables has been studied in Bayesian networks [30,31]. The main difficulties are: designing acyclic structures, and slow inference in large networks. The 
                           
                              e
                              NRBM
                           
                        , on the other hand, requires no structure design, and is fast with only a single matrix operation.

Finally, we wish to emphasize that the RBM is a fully generative model of EMRs with distribution 
                           
                              P
                              (
                              v
                              )
                           
                        . The RBM can simulate EMRs whose distribution follows 
                           
                              P
                              (
                              v
                              )
                           
                        . This offers a new solution for data sharing without compromising privacy. Details of the simulation are beyond the scope of this paper, but in general they are based on Monte Carlo simulation (see for example, [32]). For this paper, code and simulated data are available for download.
                           4
                           
                              http://prada-research.net/∼truyen/code/eNRBM-jbi.zip.
                        
                        
                           4
                         The data was sampled from a RBM which was learnt from the real data. Thus the simulated data reflects the true statistical properties of the real source.

Medical objects and events are discrete in nature. This creates significant computational challenges for symbolic representation. First, the number of unique objects (e.g., diagnosis codes) is often very large, and the number of events grows in time. Second, rare objects (e.g., rare diseases) are not robust to quantify statistically. And third, relations such as nearness with continuously varying degrees are hard to specified to fine details.

This calls for an embedding of objects into low-dimensional spaces (e.g., see also [33] for similar arguments in linguistics). In other words, the representation of an object is distributed. Embedding promotes algebraic manipulations such as similarity computation and retrieval. It is also easy to assess the relatedness between objects of different kinds (e.g., a disease and a procedure), as we have seen in Fig. 5. Once objects have been embedded, an event can be considered as a set of objects observed in a period of time. The discussion can be extended to relations, for example, the parent–child relationship in the disease taxonomy: A parent is close to its children in the embedding space. This offers a novel way of exploiting existing medical knowledge bases.

The 
                              
                                 e
                                 NRBM
                              
                            applied to mental health, as shown in Table 1, discovered risk factors that resemble those well-documented in the literature [19,34]. For instance, psychiatric problems and prior attempts are well-recognized risk factors [35,36]. Our method differs in that it is hypothesis-free and time-specific.

Comorbidities that appear remotely related to psychiatric issues were also discovered, for example infectious diseases [37,38] and obesity [39–41]. While these findings are interesting to warrant a deeper analysis, a full clinical investigation is beyond the scope of this paper. Finally, the automatic grouping suggests a potential in automated phenotyping [4,6].

@&#LIMITATIONS@&#

We recognize several limitations. First, a relation was defined if two ICD-10 codes shared the first character and the first digit, and the relation strength was always 1. This could be extended to be more flexible. For example, F20 and F31 share the parent F (Mental and behavioural disorders), so the relation strength can be thought as a half of that between F20 and F21. Determining the precise strength is a difficult problem itself. First, the 
                           
                              e
                              NRBM
                           
                         primarily ran on binary (or probability-like) observations. However, model can be easily extended to other data types such as counts (e.g., number of previous admissions) and continuous variables (e.g., lab test measurements) or a mixture of these [42,43]. This suggests an interesting integration of multiple modalities, such as administrative data (this work), text (e.g., carer notes), and medical images [44]. Extension to unstructured clinical notes is not difficult: time-stamped notes can be aggregated into intervals just like other composite events (such as admissions), and known relations between concepts (e.g., using the UMLS or SNOMED-CT) can be naturally encoded into the 
                           
                              e
                              NRBM
                           
                        .

Second, some discovered groups may not be clinically relevant but a data artifact. However, the structural relations can be modified without difficulty to encode known phenotypes and to prevent meaningless grouping.

Finally, the empirical study has been limited to EMRs from a single institution. The EMR is known for its quality issues [45]. However, EMRs are comprehensive and readily available, making them an attractive alternative to standard clinical data collection. In fact, the quality of the Charlson comorbidity index computed from EMR is comparable to that computed from the standard chart [46,47]. The 
                           
                              e
                              NRBM
                           
                         is cohort-independent, and thus it is possible to run on multiple databases. Alternatively, 
                           
                              e
                              NRBM
                           
                         could be evaluated intensively using simulated data with controlled variations so that its behaviors and performance can be assessed. However, faithfully generating EMR data is a challenging research topic by itself (see, for example, a recent work by [48]).

@&#CONCLUSION@&#

We have proposed a novel model called EMR-driven nonnegative restricted Boltzmann machine (
                           
                              e
                              NRBM
                           
                        ) for EMR modeling. The 
                           
                              e
                              NRBM
                           
                         supports a variety of healthcare analytics tasks with minimal manual feature engineering. The model learns EMR representation by embedding features and trajectories into a low-dimensional space. Through nonnegativity and domain-specific structural constraints, intrinsic dimensionality can be estimated, meaningful grouping of medical objects can be discovered. The homogeneous representation leads to simple algebraic manipulations and easy use with existing classifiers. Experimental results on suicide risk stratification demonstrate that the proposed method is competitive in predictive performance. The model paves a pathway toward EMR-driven phenotyping.

To see how the nonnegativity constraints in the 
                           
                              e
                              NRBM
                           
                         let the grouping emerge, consider the activation probability of the hidden unit in Eq. (5):
                           
                              (A.1)
                              
                                 
                                    
                                       ρ
                                    
                                    
                                       k
                                    
                                 
                                 =
                                 P
                                 
                                    
                                       
                                          
                                             
                                                h
                                             
                                             
                                                k
                                             
                                          
                                          =
                                          1
                                          |
                                          v
                                       
                                    
                                 
                                 =
                                 σ
                                 
                                    
                                       
                                          
                                             
                                                b
                                             
                                             
                                                k
                                             
                                          
                                          +
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   i
                                                
                                             
                                          
                                          
                                             
                                                W
                                             
                                             
                                                ik
                                             
                                          
                                          
                                             
                                                v
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                 
                              
                           
                        Suppose for the moment that 
                           
                              
                                 
                                    
                                       
                                          
                                             b
                                          
                                          
                                             k
                                          
                                       
                                    
                                 
                              
                           
                         is bounded from above. Then, the visible units must “compete” against each other to turn on the k-th hidden unit by making 
                           
                              
                                 
                                    
                                       
                                          
                                             b
                                          
                                          
                                             k
                                          
                                       
                                       +
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                          
                                       
                                       
                                          
                                             W
                                          
                                          
                                             ik
                                          
                                       
                                       
                                          
                                             v
                                          
                                          
                                             i
                                          
                                       
                                    
                                 
                              
                              ≥
                              0
                           
                        , since 
                           
                              
                                 
                                    
                                       
                                          
                                             v
                                          
                                          
                                             i
                                          
                                       
                                    
                                 
                              
                           
                         are nonnegative. The result is that some elements of the k-th column vector 
                           
                              
                                 
                                    W
                                 
                                 
                                    •
                                    k
                                 
                              
                           
                         are driven to zeros. The remaining elements will self-organized into the k-th group.

Since the bipartite structure of the 
                           
                              e
                              NRBM
                           
                         has no within-layer connections, the conditional distributions over visible and hidden units can be factorized as:
                           
                              (A.2a)
                              
                                 p
                                 
                                    
                                       
                                          v
                                          |
                                          h
                                       
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∏
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          N
                                       
                                    
                                 
                                 p
                                 
                                    
                                       
                                          
                                             
                                                v
                                             
                                             
                                                i
                                             
                                          
                                          |
                                          h
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (A.2b)
                              
                                 p
                                 
                                    
                                       
                                          h
                                          |
                                          v
                                       
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∏
                                       
                                       
                                          k
                                          =
                                          1
                                       
                                       
                                          K
                                       
                                    
                                 
                                 p
                                 
                                    
                                       
                                          
                                             
                                                h
                                             
                                             
                                                k
                                             
                                          
                                          |
                                          v
                                       
                                    
                                 
                              
                           
                        
                     

Thus inference can be efficiently performed by layer-wise sampling. Model density can be estimated as
                           
                              (A.3)
                              
                                 P
                                 (
                                 v
                                 )
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          s
                                          =
                                          1
                                       
                                       
                                          S
                                       
                                    
                                 
                                 P
                                 
                                    
                                       
                                          v
                                          |
                                          
                                             
                                                h
                                             
                                             
                                                (
                                                s
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        using S random samples 
                           
                              
                                 
                                    
                                       
                                          
                                             h
                                          
                                          
                                             (
                                             s
                                             )
                                          
                                       
                                    
                                 
                              
                           
                         for 
                           
                              s
                              =
                              1
                              ,
                              2
                              ,
                              …
                              ,
                              S
                           
                        .

Learning in the 
                           
                              e
                              NRBM
                           
                         was carried out by maximizing the data log-likelihood 
                           
                              log
                              P
                              (
                              v
                              )
                           
                         subject to several constraints:
                           
                              •
                              
                                 Nonnegativity: 
                                    
                                       
                                          
                                             W
                                          
                                          
                                             ik
                                          
                                       
                                       ≥
                                       0
                                    
                                  for all 
                                    
                                       i
                                       ,
                                       
                                       k
                                    
                                 . For simplicity, we used the barrier function 
                                    
                                       B
                                       
                                          
                                             
                                                
                                                   
                                                      W
                                                   
                                                   
                                                      ik
                                                   
                                                
                                             
                                          
                                       
                                       =
                                       
                                          
                                             W
                                          
                                          
                                             ik
                                          
                                          
                                             2
                                          
                                       
                                    
                                  if 
                                    
                                       
                                          
                                             W
                                          
                                          
                                             ik
                                          
                                       
                                       <
                                       0
                                    
                                  and 0 otherwise.


                                 Bounding: 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      a
                                                   
                                                   
                                                      i
                                                   
                                                
                                             
                                          
                                       
                                       ,
                                       
                                          
                                             
                                                
                                                   
                                                      b
                                                   
                                                   
                                                      k
                                                   
                                                
                                             
                                          
                                       
                                       ⩽
                                       c
                                    
                                 . This could be realized by adding a penalty term to the data likelihood 
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                          
                                       
                                       
                                          
                                             a
                                          
                                          
                                             i
                                          
                                          
                                             2
                                          
                                       
                                       +
                                       
                                          
                                             ∑
                                          
                                          
                                             k
                                          
                                       
                                       
                                          
                                             b
                                          
                                          
                                             k
                                          
                                          
                                             2
                                          
                                       
                                    
                                 .


                                 Structural smoothness: similar features should share similar weights, as encoded in the regularizer 
                                    
                                       Ω
                                       (
                                       W
                                       )
                                    
                                  in Eq. (6).

Finally, the augmented log-likelihood is
                           
                              (A.4)
                              
                                 L
                                 (
                                 W
                                 )
                                 =
                                 log
                                 P
                                 (
                                 v
                                 )
                                 -
                                 
                                    
                                       α
                                    
                                    
                                       2
                                    
                                 
                                 B
                                 
                                    
                                       
                                          
                                             
                                                W
                                             
                                             
                                                ik
                                             
                                          
                                       
                                    
                                 
                                 -
                                 
                                    
                                       β
                                    
                                    
                                       2
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   i
                                                
                                             
                                          
                                          
                                             
                                                a
                                             
                                             
                                                i
                                             
                                             
                                                2
                                             
                                          
                                          +
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   k
                                                
                                             
                                          
                                          
                                             
                                                b
                                             
                                             
                                                k
                                             
                                             
                                                2
                                             
                                          
                                       
                                    
                                 
                                 -
                                 
                                    
                                       λ
                                    
                                    
                                       2
                                    
                                 
                                 Ω
                                 (
                                 W
                                 )
                              
                           
                        where 
                           
                              α
                              ,
                              
                              β
                              ,
                              
                              γ
                              >
                              0
                           
                         are tunable hyperparameters.

The structural smoothness can be rewritten as
                           
                              
                                 Ω
                                 (
                                 W
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          k
                                       
                                    
                                 
                                 W
                                 
                                    
                                    
                                       •
                                       k
                                    
                                    
                                       ⊤
                                    
                                 
                                 LW
                                 
                                    
                                    
                                       •
                                       k
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    L
                                 
                                 
                                    ii
                                 
                              
                              =
                              
                                 
                                    ∑
                                 
                                 
                                    j
                                    ≠
                                    i
                                 
                              
                              
                                 
                                    γ
                                 
                                 
                                    ij
                                 
                              
                              ;
                              
                              
                                 
                                    L
                                 
                                 
                                    ij
                                 
                              
                              =
                              -
                              
                                 
                                    γ
                                 
                                 
                                    ij
                                 
                              
                           
                        . The matrix 
                           
                              L
                           
                         is known as the Laplacian of the graph whose edge weight is 
                           
                              
                                 
                                    γ
                                 
                                 
                                    ij
                                 
                              
                           
                        .

Finally, the parameter update rule becomes:
                           
                              
                                 
                                    
                                       
                                       
                                          
                                             
                                                
                                                   a
                                                
                                                
                                                   i
                                                
                                             
                                             ←
                                             
                                                
                                                   a
                                                
                                                
                                                   i
                                                
                                             
                                             +
                                             η
                                             
                                                
                                                   
                                                      
                                                         
                                                            v
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                      -
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        v
                                                                     
                                                                     
                                                                        i
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                         
                                                            P
                                                         
                                                      
                                                      -
                                                      β
                                                      
                                                         
                                                            a
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             
                                                
                                                   b
                                                
                                                
                                                   k
                                                
                                             
                                             ←
                                             
                                                
                                                   b
                                                
                                                
                                                   k
                                                
                                             
                                             +
                                             η
                                             
                                                
                                                   
                                                      
                                                         
                                                            ρ
                                                         
                                                         
                                                            k
                                                         
                                                      
                                                      -
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        h
                                                                     
                                                                     
                                                                        k
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                         
                                                            P
                                                         
                                                      
                                                      -
                                                      β
                                                      
                                                         
                                                            b
                                                         
                                                         
                                                            k
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             
                                                
                                                   W
                                                
                                                
                                                   ik
                                                
                                             
                                             ←
                                             
                                                
                                                   W
                                                
                                                
                                                   ik
                                                
                                             
                                             +
                                             η
                                             
                                                
                                                   
                                                      
                                                         
                                                            v
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                      
                                                         
                                                            ρ
                                                         
                                                         
                                                            k
                                                         
                                                      
                                                      -
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        v
                                                                     
                                                                     
                                                                        i
                                                                     
                                                                  
                                                                  
                                                                     
                                                                        h
                                                                     
                                                                     
                                                                        k
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                         
                                                            P
                                                         
                                                      
                                                      -
                                                      α
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        W
                                                                     
                                                                     
                                                                        ik
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                         
                                                            -
                                                         
                                                      
                                                      -
                                                      λ
                                                      
                                                         
                                                            LW
                                                         
                                                         
                                                            ik
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    
                                       
                                          
                                             
                                                W
                                             
                                             
                                                nk
                                             
                                          
                                       
                                    
                                 
                                 
                                    -
                                 
                              
                           
                         denotes the negative part of the weight. The “contrastive divergence” procedure [10] was used to approximate expectations with respect to the model distribution 
                           
                              P
                              (
                              v
                              ,
                              h
                              )
                           
                        . The Markov chain started from the observation 
                           
                              v
                           
                        , runs for one step, then the pair 
                           
                              (
                              v
                              ,
                              h
                              )
                           
                         was collected to approximate P.

@&#REFERENCES@&#

