@&#MAIN-TITLE@&#An informatics framework for the standardized collection and analysis of medication data in networked research

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Medication coding and classification tasks have different requirements.


                        
                        
                           
                           Various medication coding and classification systems exist.


                        
                        
                           
                           Heterogeneous studies complicate a standard approach to handling medication data.


                        
                        
                           
                           Classification systems can support data retrieval and analysis.


                        
                        
                           
                           A reference classification system can support standardized analysis approaches.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Medication terminological systems

Drug classification

Standards

Data coordinating centers

Data coding

Data analysis

@&#ABSTRACT@&#


               
               
                  Medication exposure is an important variable in virtually all clinical research, yet there is great variation in how the data are collected, coded, and analyzed. Coding and classification systems for medication data are heterogeneous in structure, and there is little guidance for implementing them, especially in large research networks and multi-site trials. Current practices for handling medication data in clinical trials have emerged from the requirements and limitations of paper-based data collection, but there are now many electronic tools to enable the collection and analysis of medication data. This paper reviews approaches to coding medication data in multi-site research contexts, and proposes a framework for the classification, reporting, and analysis of medication data. The framework can be used to develop tools for classifying medications in coded data sets to support context appropriate, explicit, and reproducible data analyses by researchers and secondary users in virtually all clinical research domains.
               
            

@&#INTRODUCTION@&#

Standardized coding and classification of medication data enable comparability within and across research studies and can bridge clinical and research networks and applications. However, there is currently no explicit standards-based process for organizing granular medication codes (e.g., “acetaminophen”, “Tylenol”, or “acetaminophen 325 mg oral tablet”) into classes defined by various drug properties (e.g., “antipyretic” or “prostaglandin receptor antagonists”). Such a process is badly needed to ensure efficient, reproducible classification and analysis.

While property-based drug classification seems straightforward, there is ambiguity in the definitions of classes in coded data sets (e.g., “Does the class ‘antibiotics’ in this analysis include topical antibiotics?”), new indications (“Is Donnatol™ included as a seizure medication in this analysis?”), and new products (“Is the recently approved drug Ravicti™ included in the group of urea cycle disorder medications in this data set?”). Comprehensive reference terminology valid across the spectrum of clinical domains can ensure that the class memberships used in categorical analyses are explicit and easily reproducible, enabling comparability of different data sets. In this paper, we examine current approaches, propose a framework for standard medication coding and classification systems, and identify needed research and tools.

@&#BACKGROUND@&#

Current systems for coding medication data include granular medication entities (e.g., the Unique Ingredient Identifiers [UNII] for active ingredients and chemical substances, and the National Drug Codes [NDC] for packaged products maintained by the U.S. FDA) with varying levels of adherence to sound coding management principles [1–3]. Other U.S. coding systems include the NDC Directory [4] and RxNorm [5,6], both of which include route of administration, in addition to the information on active ingredients, trade and generic names, drug strength, dosage form, and package size information found in the NDC codes. RxNorm has been shown to have near perfect coverage of ambulatory e-prescriptions [7] and it is being increasingly used in a variety of applications [8]. RxNorm does not include codes for dietary supplements such as mineral and herbal preparations, although there is no designated standard in this area for all research studies [9]. (It should be noted that the WHO Herbal Dictionary is used in many industry-sponsored trials [10].) In general, less specificity is required for coding of medications for research than is for clinical purposes (which require identifiers for branded products to support prescribing and pharmacy management activities).

RxNorm is linked to source vocabularies in the National Library of Medicine’s (NLM) Unified Medical Language System (UMLS), including the National Drug File Reference Terminology (NDF-RT). NDF-RT was created by the Department of Veterans Affairs (VA) to organize drugs in the VA formulary, and it uses a logic-based model to group drug products into classes based on chemical structure, mechanism of action, physiological effect, drug–disease relationship describing therapeutic intent, pharmacokinetics describing the mechanisms of absorption and distribution of an administered drug in a body (e.g., hepatic metabolism), and legacy VA-NDF classes for pharmaceutical preparations (e.g., non-opioid analgesic). NDF-RT is the standard for the U.S. FDA’s Structured Product Label (SPL) initiative [11].

Other reference terminologies and classifications that have formal systems for organizing and classifying medications include the Anatomical Therapeutic Chemical (ATC) Classification System maintained by the WHO Collaborating Centre for Drug Statistics Methodology and used widely in Europe [12]. Proprietary medication systems obviously use a formal class organization, presumably based on the American Hospital Formulary Service (AHFS Drug Information) [13,14], but they are not available for research or comparison. The fairly new Chebi ontology of chemical compounds of biological interest also contains medication compounds and information related to their molecular and biological function and interactions [15–17]. Existing reference terminologies and classifications, however, support different uses and user groups, and the identification of the most appropriate classification system is driven by the research question and the business case.

To date, paper workflows have dominated clinical research [18]. Medication data for clinical trials are typically collected as free text and coded later, usually by a medical coder employed by the trial sponsor; errors and uncertainties are coded after data collection as they are discovered [19]. The completeness of data collection depends in part on the method of elicitation used by the investigator (e.g., open ended questions, symptom checklists) [19]. In industry-sponsored trials, quality control checks are performed by a clinician or pharmacist; reported medications are often compared with reported events/diseases to verify the accuracy of the information.

There has been little empirical research on the quality of coding in clinical trials. A 2012 systematic review [19] found only one published study on the validity and inter-rater reliability of adverse event coding using Medical Dictionary for Regulatory Activities (MedDRA) [20]. That study found that 12% of sampled adverse events were coded differently by two coders, and 8% of the coding was declared medically inaccurate by experts. Similarly, White et al. looked at 204 post marketing surveillance events to examine the impact of different classification systems on the identification of adverse events [21]. When the same verbatim text was coded with terms from MedDRA and the WHO Adverse Reaction Terminology (WHO-ART), 32 coded pairs (16%) of events were rated as medically different by expert reviewers.

Richesson et al. found high completion rates using RxNorm for coding in two different multi-site studies, but they did not evaluate coding accuracy [22]. The designation of the WHO Drug Dictionary as a medication coding standard for regulated clinical trials may eliminate the need for comparative research on coding schemes in industry-sponsored trials, but observational studies do not have the same restrictions. As electronic health records (EHRs) become more widely adopted, there will be greater potential to capture medication data from clinical repositories; the Clinical Data Standards Interchange Consortium (CDISC) is driving these efforts as part of its Electronic Source Data Interchange (eSDI) Group [18]. The emergence of “big data” and increasing interest in observational research are creating a sizable constituency of prospective data users who will demand free and open data standards. Thus comparative studies of the fitness of various medication classifications, and ways to integrate them into various workflows, are urgently needed.

“Big data” necessitate specialized data centers with massive capacities for data collection, storage, exchange, aggregation, visualization, and analysis [23]. Data centers for research networks and multi-site trials can benefit greatly from the use of standards to support data collection and analysis [24], yet there is little published guidance on how to implement these in large research networks and multi-site trials. The framework we provide can guide the development of tools to support improvements and standardization of medication data and enable sharing of research data sets and meaningful interpretation, comparison, communication, and application of results.

The major issues related to the standardization of medication data are (1) selection of a standardized medication coding scheme; (2) development of a process for collecting and coding data, including systems and interfaces to support data collection, entry, or coding that are customized to study needs and workflows; (3) choice of a classification system to support reporting and analysis; and (4) development of methods and tools for inserting the classification into the data management and analysis workflow. Fig. 1
                      represents this framework, including where potential sources of error lie and points of opportunity for informatics tools and theories to reduce error and increase efficiency.

Medication data must be coded for analysis, and data centers can benefit from the re-use of tools, personnel, and training materials by using the same medication system for all studies. Cimino’s desiderata [2] has obvious relevance for selecting a structurally sound coding system, but other considerations (e.g., regulatory reporting requirements, sponsor requirements, costs, licensing issues, and assurance of ongoing maintenance) are often more critical to research networks.

In the U.S., RxNorm is the designated standard for representing medication brand names, clinical drug names, and allergies/adverse events for medication products [25]. RxNorm uses a relational model to name drugs at different levels of specificity, ranging from active ingredients to packaged products with trade name, dosage, formulary and packing information. This is an appealing feature for data centers supporting multiple studies with multiple sources of medication data. RxNorm includes content from the FDA (UNII and NDC codes) as well as codes from commercial medication knowledge vendors. Because a core objective of the U.S. Meaningful Use regulations is to increase the use of computer provider order entry [26], it is likely that future regulations will require providers to use RxNorm for primary medication order entry [8].

Several organizations have reported mapping local medication code lists and free text data to RxNorm [7,27–30]. Mapping approaches include syntactic, semantic, and hybrid methods [31]. Others have demonstrated the feasibility of using RxNorm as a primary coding source (for medical history) [31] and research [9,32–34]. Bennett has described a live clinical system implementing RxNorm for primary data capture [31].

As shown in Fig. 1, medication coding systems can be applied before, at, or after (electronic or paper-based) data collection. Once a coding system is selected for a research application, coding tools and procedures must be developed. Generally, coding tools can be integrated into user interfaces by allowing users to either select a code from a pre-configured drop list or from a search or browsing interface [31]. Desired features of clinical terminology servers for distributed coding are well summarized in Dr. Chute’s desiderata [35]; they include word normalization, word completion, target terminology specification, spelling correction, lexical matching, term completion, semantic locality, term composition and decomposition. The Rare Diseases Clinical Research Network (RDCRN) uses embedded RxNorm search features in the online data collection forms for all studies [36]. Selection of RxNorm codes are done by distributed research staff, on the assumption that researchers are best suited to check ambiguous or uncertain entries [37].

Resources and logistical issues can limit ability for distributed coding in some cases, e.g., studies collecting data using postal mail or whose research staff vary enough between sites that coding consistency is a concern. In multi-national studies, multiple languages might necessitate a centralized approach to coding. For example, The Environmental Determinants of Diabetes in the Young (TEDDY) study, which collects reported pediatric medication use from four European countries, uses a pre-configured approach, and has developed an online code-book containing country-specific trade names mapped to corresponding generic drug names and RxNorm codes [38].

The requirements of a data center and study sponsors, as well as empirical and comparative informatics research, should drive the selection of a medication classification system to support data retrieval and analysis. There are few published comparisons between reference terminologies (a term used loosely here to include classifications and ontologies), but there do appear to be differences. Mortensen and Bodenreider, for example, identified discrepancies between NDF-RT and SNOMED CT medication classes, noting that there were 390 and 527 classes, respectively, and within the two, the majority (75.6%) of the VA classes had less than 50% overlap in medications with the corresponding SNOMED CT classes [39]. These authors suggest that medication entities are more easily standardized than semantic properties, and advocate that data should be coded at the medication (rather than class) level and later classified at the time of analysis [39].

Recently, Walcer, Gilder, and Jainhey compared the structure and usability of several pharmacologic classifications and ontologies by examining intentional definitions for five medication products [40]. They identified inconsistencies in the organization and structure of SNOMED CT, NCI Thesaurus, and ATC, although they found the ATC structure to be more “user-friendly”. Using the metrics of structure, depth, medication coverage, and specificity, the authors concluded that the NDF-RT, which classifies medications by physiologic effects, mechanism of action, established pharmacologic class, and chemical/ingredients, offers the best overall classification scheme. Others have reported expert (physician) agreement with the structure and properties of the physiological effects axis for classifying organ specific and generalized systemic effects of a random selection of commonly prescribed medications [41]. Richesson and Pathak found NDF-RT class names and properties to be comprehensive and adequate for pediatric medications in the TEDDY study data [42]. They also noted that instances of underspecified medications (e.g., “unknown steroid”, “non-specified antibiotic”), while not codable in RxNorm, can be coded in medication classification systems.

Data centers provide leadership and technical support in the collection, storage, management and distribution of data for analysis [24]. Consequently, they are well suited to embed standard classification systems into data cleaning and analysis tools and workflows. Pre-analysis tasks include transforming (“re-coding”) raw data into variables and categories for computational analysis. We are not aware of any published workflow analyses or studies, but our experience suggests that researchers classify medication data in an ad hoc manner, focusing on the classes most relevant to their analysis, using their “expert knowledge” and information resources to assign reported medications to appropriate classes, and later identifying apposite classes for any unassigned medications.

This lack of a uniform approach to classifying medications in research data sets is likely to cause duplication of efforts within research networks or data centers, and raise questions about the reproducibility of analyses and the comparability of medication classifications of data sets generated by different research networks [44]. Because medication classification is done ad hoc, the association of medications with analysis-appropriate classes is time-consuming, susceptible to error or variation, and not easily reportable or reproducible without access to the analysis-specific programming code.

As shown in Fig. 1, a medication classification system can support tasks related to data retrieval or analysis and reporting of data by providing property-based classes (e.g., anti-pyretic, beta-blockers) relevant to a particular research question. Our experience in data centers leads us to believe that the research questions requiring medication classification represent two different use cases and cognitive approaches: deductive reasoning (as typically applied to hypothesis-driven investigations) and inductive reasoning (which is relevant to discovery activities, fueled by the growing availability of clinical and research data for secondary analyses). Deductive reasoning begins with a general statement, hypothesis, or theory, and examines the possibilities for reaching a specific conclusion. Typical drug efficacy research questions (e.g., “Does x influence the outcome of y?”) fall under this model, and medication data are often included in analyses as independent variables or as potential confounders. Inductive reasoning, in contrast, strives to identify broad generalizations from specific observations, and is used to generate new hypotheses and theories. Table 1
                      contrasts the different features, classification needs, and system design requirements for deductive and inductive approaches for exploring medication data.

These different reasoning approaches leverage classification systems differently for processing data. In deductive reasoning, the classes of interest are pre-specified and represent properties suspected or known by the investigator to be relevant to the intervention being tested, e.g., ace inhibitors, antifungals, or stimulants. The classification task, therefore, is to determine class membership (e.g., “Is <coded drug A> a <beta blocker>?” – yes or no) as a dichotomous variable for one or more pre-specified classes for each reported medication.

In contrast, exploratory analysis or data mining activities use an inductive approach to analysis. The relevant drug properties and classes are not known at the time of data collection or coding, but rather are selected based upon the values and frequencies in the data. In qualitative research, large data sets also are condensed into smaller analyzable units using categories and concepts that are derived from the data [43]. Thus, the process of inductive research is by definition data-driven, and the medication categories are generated after examining the collected data. Often the goal is to “characterize a data set” by classifying medication records into a handful of classes to identify distributions, patterns, and areas for further exploration. In exploratory analyses related to the TEDDY study, for example, investigators wanted to visualize the thousands of instances of 284 generic medications in the data set; they identified approximately 20 clinical classes they thought important and comprehensive, but the data set still included many medications outside of the 20 classes identified [44].

In contrast, the classification tasks for inductive reasoning (exploratory research) are related to organizing and visualizing a data set or large corpus of reported medications. For example, given a set of 150 different purported medications, the question is “what kinds of medications are in this data set? The distribution of medications in the data set will drive the number of classes to avoid “heaping” the data (putting too much data into the same category) and to ensure that the classified medication variables show variance. Using the medications from a given data set, iterative queries of the reference terminology will identify relevant parent classes and properties; the distribution of values in the data will impact group size for each class, which will ultimately to inform the optimal number and level of specificity of medication classes needed to characterize a particular data set. This is similar to the data-driven extraction of ontologies described by Brewseter et al. [45], who propose several methods to evaluate the congruence of an ontology with a given corpus (or data set) for the representation of knowledge in that given domain. By extension, data-driven queries and evaluations of class-membership medication reference terminology in a given domain (e.g., pediatrics, oncology) can produce measures of its ‘fitness’ and identify areas where the ontology should grow [44].

In both deductive and inductive reasoning, classes relevant to the analysis task must be selected and then medications classified (and re-coded) so that the data set represents reported medications by class groupings rather than as medication entities. Groupings might be dichotomous or categorical, and the group size requirements of the analytic method determine the number of classes. This number can be increased by identifying increasingly specific subclasses, or by choosing new classes representing different properties. The selection of a finite and meaningful set of medication classes for analysis, however, is complicated by the fact that drugs have many properties (chemical, mechanistic, and clinical). A standard robust and multi-axial classification can support both types of reasoning by allowing medications to be classified by various properties, at varying levels of specificity.

Best practices for data management dictate that the categories of a coded variable in an analysis data set should be both mutually exclusive (i.e., each medication instance should be assigned to only one drug class) and exhaustive (i.e., a unique code number should have been created for each category). The latter requirement often leads to a residual “Other” category to classify data instances that are not typical or anticipated. (While “other” and “not elsewhere classified” are discouraged in coding systems, they are very useful and common in data reporting and analysis.).

@&#DISCUSSION@&#

In an era of transparent research and pressure for data sharing, there is a great need for standardized methods that can be easily replicated. Major questions include how to ensure consistent and accurate medication coding across sites, and how to develop models and tools for applying classification systems (e.g., ontologies, reference terminologies) to medication data collected in different studies. Informatics methods that enable researchers to use existing data sets for new analyses or to replicate previously reported findings are badly needed. While many tools exist for collecting, classifying, and analyzing medication data, the reliability and validity of these approaches have not been evaluated. There is no consensus on research data center requirements and approaches, although the AMIA CRI summit has become an early forum for such discussions [46].

While the integration of standard medication classifications into query and data analysis systems (e.g., i2b2, JMP, SAS) would be ideal, current publicly available tools from the NLM offer an immediate approach to implementing standards that is scalable yet customizable and adaptive to change. Researchers and data analysts can use application programming interfaces (API’s), including the RxMix tool, which leverages UMLS mappings between RxNorm and NDF-RT medications (at level of ingredients and clinical drugs), to support a variety of classification tasks. Current tools allow batch queries for the retrieval of all NDF-RT classes relevant to a set of RxNorm medication codes, as well as all medication entities in one or more specified classes. These membership assignment files can be distributed to analysts in two-dimensional spreadsheets as part of a data dictionary, and incorporated into data cleaning and analysis programs, as SAS macros, for example. The reference terminology provides an externally maintained standard, and the data center can develop generic tools and simple instructions for querying the NDF-RT for specific research needs, as well as standard language for reporting methods. These methods and analyses, therefore, can then be easily replicated or validated by other researchers. This will result in a standardized approach to medications and comparable data across studies regardless of the content of the classification.

Although the framework we describe addresses research application, it can also support clinical questions – for example, “Which patients have been pharmacologically treated for depression?” posed by Saitwal and colleagues [1]. Those authors demonstrated that existing ontologies (SNOMED CT and UMLS Metathesauraus) can be used to classify granular medication codes into groups that are useful for queries on chronic disease management, quality measurement, population health, or research cohort identification. The separation of coding and classification tasks affords flexibility in selecting the most appropriate knowledge structure to support multiple analyses of data for different questions and audiences.

Although still immature, our proposed framework provides multiple targets for future research. In research contexts, evaluation of coding systems revolves around validity (Do the codes accurately represent the “true” medication entity or property?) and reliability (Can the codes be consistently reproduced across coders, systems, and time?) at the data collection stage. Using this framework, measures of validity and reliability should be applied both to candidate coding systems for data collection and storage, and to candidate classification systems that support querying, analysis, and reporting. Previous approaches to the evaluation of coding systems have included coverage, structural features, management processes for new terms, ease of use, and formal relationships (i.e., “maps”) to other relevant knowledge systems. Methods for using scientific experts to validate coding and coding systems vary, and the ideal number and types of experts are not clear. Because coding systems themselves often cannot be separated from the tools that support them, future coding system validation studies might include usability heuristics [47] as well as the ways in which the tools enable valid and reliable coding. One systematic review of the performance of automated coding applications in biomedical informatics attempted to identify uniform evaluation criteria, or ways to compare coding systems and tools, but found that widely varying study methods made it almost impossible to compare system performance [48]. The methodological features that varied most were the statistical methods used to evaluate system performance and the mechanisms used to create a reference standard against which the automated systems were evaluated [48]. The authors of the review identified three general methods of determining reference standards: (a) a “gold standard” using two or more independent reviewers with adjudication of disagreements to establish consensus in some manner—for example, by majority vote or review/discussion to obtain agreement; (b) a “trained standard” using one expert reviewer to classify the majority of the training set, verifying the validity of the reviewer’s assignment and providing training to improve the reviewer’s performance/consistency; and (c) “regular practice” using one human reviewer reflecting the normal or usual coding practice.

To date, the metric for coding efficiency has been inter-rater reliability, which can be facilitated using multiple coders for real or training cases. However, this metric has not been reported for assessing medication coding in research. Implementation of a coding system in different clinical or research sites presents the same challenges as the use of multiple data collectors in a single site, although there might also be organizational or regional variations in addition to individual variations in coding practice. Future research should compare agreement of coding across individuals in relation to interviewer/coder features (e.g., education, age, amount of training). For multi-site studies, it is possible that additional site-level data (e.g., clinical population features, such as distribution of indicator diseases and medication use) should be compared [49].

Workflows for data coding in clinical research have not been published, to our knowledge, and identifying optimal workflows for collecting and analyzing medication data is an important informatics research activity. Proposed workflows should include steps for determining which classification should be used for an analysis, how to identify a limited but complete set of relevant classes, and how to represent the encoded and classified data in analysis data sets that can be used by future analysts. This knowledge would support automated and reproducible medication classification in research settings, and perhaps extend to clinical decision support and documentation.

@&#CONCLUSION@&#

Data coordinating centers must be prepared to support a wide variety of study designs, research questions, disease domains, patient populations, and data sources in clinical research. Clinical research informatics frameworks and tools can support the use of standards across these spectra. Various study designs and implementation models present challenges for identifying the optimal standards for medication data. Our framework supports the separation of coding and classification tasks. Further, the framework identifies different points in the research process where standard coding and classifications systems can be implemented. Varying requirements for medication classification derive from different research questions related to hypothesis testing and hypothesis generation. A common reference terminology, such as NDF-RT, can support a standardized and reproducible approach to classifying medications across research settings. Publicly available tools allow the use of NDF-RT for this purpose, but future work is needed to better integrate medication classification into data management and analysis workflows, and to assess the utility, validity, reproducibility, and scalability of different subsets of the classification.

The author has no competing interests or conflicts of interest related to the content or publication of this manuscript.

@&#ACKNOWLEDGMENTS@&#

The author would like to thank the investigators and research staff at the Pediatrics Epidemiology Center of the University of South Florida for the development of tools for collection and coding of research data: Jeff Krischer, Kendra Vehik, Heather Guillette, Ken Young, Veena Gowda, Jamie Malloy, David Cuthbertson, Susan Smith, Cristina McCarthy, and Wendy McLeod. I also thank Anita Walden (Duke University) for background on medication coding in industry trials and Joe Bonner (Michigan State University) for promoting communication and collaboration across data centers, and am grateful to Elizabeth Flint (Duke) for providing graphic support.

TEDDY is funded by several NIH institutes, Juvenile Diabetes Research Foundation (JDRF), and Centers for Disease Control and Prevention (CDC). This publication was supported by TEDDY (Grant Number 1UC4DK095300) and by the RDCRN Data Coordinating Center award from the National Institute for Neurologic Disease and Stroke (NINDS), (Grant Number 7U54-RR019259-02), a component of the NIH, and supported by the Office of Rare Diseases Research. The contents are solely the responsibility of the authors and do not necessarily represent the official views of NINDS or ORDR or NIH or RDCRN investigators. The views expressed in written materials or publications do not necessarily reflect the official policies of the Department of Health and Human Services; nor does mention by trade names, commercial practices, or organizations imply endorsement by the U.S. Government.

@&#REFERENCES@&#

