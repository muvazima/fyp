@&#MAIN-TITLE@&#Selection of a best metric and evaluation of bottom-up visual saliency models

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Introduced a set of experiments to judge the biological plausibility of visual saliency models.


                        
                        
                           
                           Introduced a novel method to evaluate saliency map comparison metrics using a database of human fixation maps.


                        
                        
                           
                           Employed the introduced method to identify the best saliency map comparison metric.


                        
                        
                           
                           Examined nine well-known models of visual saliency using the best metric to identify the best visual saliency models.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Bottom-up saliency mechanism

Best comparison metric

Computer vision

Human visual system

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

It has been shown that analyzing or storing all information entering the human eye at every moment is beyond the capabilities of the human visual system (HVS) [3]. Controlling fixations and saccades of the eye, the visual saliency mechanism enables the HVS to focus its limited perceptual and cognitive resources on the most “prominent” locations of the scene [4]. These locations are often referred to as “salient points” or “interesting points” in the literature [5]. The HVS gathers information mostly at the fixation points and little information is collected in saccades [6]. In addition to increasing the functionality of the HVS, the saliency mechanism helps the visual perceptual system organize visual information faster [1,5,7].

The saliency activity of the HVS is an interaction between two mechanisms, bottom-up and top-down saliency [1,3,5]. Bottom-up saliency is a fast and purely stimulus driven mechanism (independent of any high-level visual task), which biases the observer towards selecting locations in the scene based on the saliency of the locations only [5]. In this case, the saliency of a stimulus can be defined as the state or quality of standing out relative to other stimuli in the scene. Top-down saliency is a slower, memory driven process, directing visual attention based on activities in which the human neural system is engaged [3]. Herein, the goal is to find visual saliency models that can mimic human observers' behavior; however, models of the saliency mechanism of the HVS can be employed in different applications of computer vision. Most of the computer vision algorithms depend upon scanning a scene from left to right or top to bottom to locate objects of interest. Saliency mechanisms offer a reasonably fast method to find regions in the scene that contain key information and may include the object of interest. Applications of the saliency mechanisms comprise, but are not limited to, automatic target detection, robotics, image and video compression and advertising.

There are several “Machine Vision” models for visual saliency, and it is important to know which models perform the best in mimicking the saliency mechanism of the HVS. In an extensive survey of visual saliency models, Borji and Itti [8] point out the importance of creating a standard set of experiments to evaluate visual saliency models and judge their biological plausibility. Moreover, there are many evaluation metrics in the literature for comparing different saliency mechanisms, or saliency mappings. Results of different evaluation metrics often disagree [8,9]. Contributions of this paper are as follows:
                        
                           1.
                           A method is given to evaluate saliency map comparison metrics using a database of human fixation maps.

Different comparison metrics are investigated and ranked using 1.

Ten well-known models of visual saliency are examined using the best metric from 2 to identify the best saliency models.

It is worth mentioning that finding the best comparison metric is also necessary for training various machine vision algorithms. Comparison metrics are usually used as the objective function in algorithms in which a visual saliency model is trained based on a human fixation/saliency map database. It is critically important to choose a metric that can discriminate between a good and a poor set of saliency data.

Visual saliency models have often been validated against human eye movement data. In a study at the MIT Computer Science Artificial Intelligence Laboratory, “Learning to Predict Where Humans Look” (LPWHL) [10], eye tracking data of 15 human observers on 1003 images of different scenes was collected. Observers free viewed the images. Gaze tracking paths and the first 5 fixation locations were recorded for each viewer. For every image in the database, a continuous saliency map was found by convolving a 2-D Gaussian filter over the fixation locations of all observers. As an example, an image, human observers' fixations, and its saliency map are shown in Fig. 1(a), (b) and (c). A binary map showing the 20% most salient pixels in the image is demonstrated in Fig. 1(d). The database created in the LPWHL study will be used in this paper to find the best comparison metric and also to examine visual saliency models.

In what follows, Section 2 summarizes the main steps of bottom-up visual saliency models as well as ten visual saliency models that will be evaluated in this paper. In Section 3, three new saliency map comparison metrics are introduced and six published saliency map comparison metrics are explained. An evaluation procedure to evaluate comparison metrics is introduced in Section 4, and all metrics are ranked accordingly. Employing the best comparison metric, all visual saliency models are examined on their performances on a database of human observers' fixation data, and the best models are identified in Section 5. Finally, Sections 6 and 7 provide discussion and conclusions.

There has been increasing effort to present computational principles of the HVS saliency mechanism in the last decades. One of the first models was proposed by Koch and Ullman in 1985 [11], which is based on the “Feature Integration Theory” of Treisman and Gelade [12]. According to this theory, features are extracted early, automatically, and in parallel across the visual field, while objects are identified separately and only at a later stage, which requires focused attention. Treisman and Gelade assumed that at the first step of visual processing in the HVS, several feature spaces such as color, orientation, spatial frequency, brightness, and direction of movement are initially extracted from the scene. Then, feature spaces are processed in parallel to generate feature saliency maps that are later integrated in a saliency map. They claimed that without focused attention, features cannot be related to each other.

According to Harel et al. [13], models of the bottom-up visual saliency can be organized into the following three stages, illustrated by Itti et al. [14] in Fig. 2
                     :
                        
                           1.
                           
                              Extraction: Given an image of the scene, several feature spaces such as image intensity, orientation and color are extracted by linearly filtering the input image.


                              Activation: Computing feature saliency maps (activation maps) from feature spaces.


                              Normalization/Combination: Normalizing feature saliency maps and combining them together to form the saliency map.

Ten well-known bottom-up saliency mechanisms are briefly summarized herein.

Itti et al. [14] developed the Koch and Ullman model and proposed one of the first complete implementations of human visual saliency models. They considered one intensity, two colors, and four orientation feature spaces, which are among the most important feature spaces based on work by Wolfe et al. [15]. Then, feature spaces are analyzed with six sets of radii for center and surround circles, which results in forty two feature saliency maps. Itti et al. [14] proposed to normalize feature saliency maps before combining them together and presented a new normalization method, which is the main difference from the Koch and Ullman model. Normalizing and summing feature saliency maps corresponding to each elementary feature space results in three “conspicuity maps”. Finally, the saliency map is calculated as an average of the three conspicuity maps. The Itti et al. model has been the basis of many newer models of the saliency mechanism of the HVS. Itti and Khoch [3] further developed [14] and introduced a more efficient feature saliency map normalization method. The Itti and Khoch [3] model of visual saliency mechanism is examined in this paper.

Meur et al. [16] proposed a model based on the architecture of the Koch and Ullman model [11] to overcome the following drawbacks of the classical models of visual saliency such as the Itti et al. [14] model:
                           
                              1.
                              Applying several normalization steps during the process

Ineffective normalization methods

Ignoring or overlooking some aspects of HVS saliency mechanism

Hou and Zhang [17] introduced the spectral residual approach. Their method is based on the general property of natural images, described by the 1/f law. This law states that the amplitude of the averaged Fourier spectrum, 
                           A
                        (f), of the ensemble of natural images is proportional to 1/f, in which f is the frequency. They use this law to find the statistical similarities between input images and calculate the residual spectrum, which they called the bottom-up saliency map (
                           M
                        ) of the input image (
                           I
                        ). This method can be summarized in the following steps:
                           
                              (1)
                              
                                 
                                    A
                                    
                                       f
                                    
                                    =
                                    Amplitude
                                    
                                       
                                          F
                                          
                                             I
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (2)
                              
                                 
                                    P
                                    
                                       f
                                    
                                    =
                                    Phase
                                    
                                       
                                          F
                                          
                                             I
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (3)
                              
                                 
                                    L
                                    
                                       f
                                    
                                    =
                                    log
                                    
                                       
                                          A
                                          
                                             f
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (4)
                              
                                 
                                    R
                                    
                                       f
                                    
                                    =
                                    L
                                    
                                       f
                                    
                                    −
                                    
                                       H
                                       n
                                    
                                    ∗
                                    L
                                    
                                       f
                                    
                                 
                              
                           
                        
                        
                           
                              (5)
                              
                                 
                                    M
                                    
                                       x
                                       y
                                    
                                    =
                                    G
                                    ∗
                                    
                                       F
                                       
                                          −
                                          1
                                       
                                    
                                    
                                       
                                          
                                             
                                                e
                                                
                                                   R
                                                   
                                                      f
                                                   
                                                   +
                                                   P
                                                   
                                                      f
                                                   
                                                
                                             
                                          
                                          2
                                       
                                    
                                 
                              
                           
                        where 
                           
                              F
                              
                                 .
                              
                           
                         is the 2D Fourier transform, 
                           G
                         is a 2D Gaussian filter to smooth the saliency map, 
                           H
                           n
                         is a n
                        ×
                        n matrix defined by:
                           
                              
                                 
                                    
                                       H
                                       n
                                    
                                    =
                                    
                                       1
                                       
                                          n
                                          2
                                       
                                    
                                    
                                       
                                          
                                             
                                                1
                                             
                                             
                                                1
                                             
                                             
                                                .
                                                .
                                                .
                                             
                                             
                                                1
                                             
                                          
                                          
                                             
                                                1
                                             
                                             
                                             
                                             
                                          
                                          
                                             
                                                ⋮
                                             
                                             
                                             
                                                ⋱
                                             
                                             
                                          
                                          
                                             
                                                1
                                             
                                             
                                             
                                             
                                                1
                                             
                                          
                                       
                                    
                                 
                              
                           
                        and * indicates the convolution of two terms.

Harel et al. [13] introduced a new approach for modeling bottom-up visual saliency. To calculate the feature saliency map (
                           M
                        ) corresponding to a feature space (
                           F
                        ), the authors first generated a fully connected graph 
                           G
                        , obtained by connecting every two pixels in 
                           F
                        . Then, a weight w was assigned to the edge from pixel (k,l) to (p,q), defined by
                           
                              (6)
                              
                                 
                                    w
                                    
                                       
                                          k
                                          l
                                       
                                       
                                          p
                                          q
                                       
                                    
                                    =
                                    
                                       
                                          log
                                          
                                             
                                                F
                                                
                                                   k
                                                   l
                                                
                                             
                                             
                                                F
                                                
                                                   p
                                                   q
                                                
                                             
                                          
                                       
                                    
                                    exp
                                    
                                       
                                          −
                                          
                                             
                                                
                                                   
                                                      
                                                         k
                                                         −
                                                         p
                                                      
                                                   
                                                   2
                                                
                                                +
                                                
                                                   
                                                      
                                                         l
                                                         −
                                                         q
                                                      
                                                   
                                                   2
                                                
                                             
                                             
                                                2
                                                
                                                   σ
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where the first term on the right is called the dissimilarity between 
                           F
                        (k,l) and 
                           F
                        (p,q); and the second term is a Gaussian function to increase the weight of two close pixels and decrease the weight of pixels which are far from each other. Subsequently, a Markov chain is defined on 
                           G
                         by normalizing the weights of the outbound edges of each node/pixel to 1. The equilibrium distribution of the Markov chain reflects the fraction of time a random walker would spend at each node if he were to walk forever [13]. It would naturally accumulate mass at nodes that have high dissimilarity with their surrounding nodes. This equilibrium distribution is considered the feature saliency map in the Harel, et al. approach. A linear combination of all feature saliency maps results in the final saliency map.

Zhang et al. [18] proposed a visual saliency model using a Bayesian framework from which bottom-up saliency is defined as the self-information of visual features, and prior information emerges as the pointwise mutual information between the features and the target when searching for a target. Differing from most of the visual saliency models, Zhang et al. defined saliency based on the natural image statistics instead of considering the image of interest only. Let the binary random variable C represent whether or not a pixel location L belongs to the target. To calculate the feature saliency map value at pixel location (k,l) (
                           M
                        (k,l)), using the corresponding feature space 
                           F
                        (k,l), they offered the following equation:
                           
                              (7)
                              
                                 
                                    log
                                    
                                       
                                          M
                                          
                                             l
                                             k
                                          
                                       
                                    
                                    =
                                    −
                                    log
                                    
                                       
                                          p
                                          
                                             
                                                F
                                                =
                                                F
                                                
                                                   k
                                                   l
                                                
                                             
                                          
                                       
                                    
                                    +
                                    log
                                    
                                       
                                          p
                                          
                                             
                                                F
                                                =
                                                F
                                                
                                                   k
                                                   l
                                                
                                                
                                                   
                                                      C
                                                      =
                                                      1
                                                   
                                                
                                             
                                          
                                       
                                    
                                    +
                                    log
                                    
                                       
                                          p
                                          
                                             
                                                C
                                                =
                                                1
                                                
                                                   
                                                      L
                                                      =
                                                      
                                                         k
                                                         l
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        The first part on the right of Eq. (7) (−log[p(F
                        =
                        F(k,l))]) is known as self-information of the random variable 
                           F
                         when it takes the value 
                           F
                        (k,l). It increases when the probability of a feature decreases (the rarer a feature, the more informative it is). The second part is a log-likelihood term which supports feature values that are consistent with the prior knowledge about the target. The third part on the right of Eq. (7) is independent of visual features and reflects the prior knowledge about where the target is more likely to appear.

Achanta et al. [19] introduced a simple definition for computing the feature saliency map 
                           M
                         using the feature space 
                           F
                         as follows
                           
                              (8)
                              
                                 
                                    M
                                    =
                                    
                                       
                                          
                                             μ
                                             F
                                          
                                          −
                                          
                                             F
                                             whc
                                          
                                       
                                    
                                 
                              
                           
                        where μF
                         is the mean value of 
                           F
                         and 
                           F
                           whc
                         is a smoothed version of 
                           F
                         with a 2-D Gaussian kernel. They used the Euclidian length of the vector {M
                        1(l,k), M
                        2(l,k),..., M
                        
                           d
                        (l,k)} to combine d feature saliency maps together and compute the saliency map value at pixel location (k,l).

Bruce and Tsotsos [20,21] proposed a saliency mechanism model based on the principle of maximizing information sampled from a scene. Information in their model is computed using Shannon self-information of each local image patch.

Goferman et al. [22] proposed to employ the following four basic principles of the HVS saliency mechanism in the visual saliency model:
                           
                              1.
                              Local low-level considerations, to promote regions which differ from their immediate surroundings.

Global considerations, to suppress frequently occurring features.

Visual organization rules, which state that visual forms may possess one or several centers about which the form is organized.

High-level factors, for example including human face match detection in the visual saliency model. They used the Euclidian distance between feature values and positions to define dissimilarities between two pixels.

Xiaodi et al. [23] introduced a new image descriptor named image signature to create saliency maps. They used Discrete Cosine Transform (DCT) to define the image signature (
                           IS
                        ) of gray scale image 
                           I
                         as
                           
                              (9)
                              
                                 
                                    IS
                                    
                                       I
                                    
                                    =
                                    sign
                                    
                                       
                                          DCT
                                          
                                             I
                                          
                                       
                                    
                                 
                              
                           
                        and defined the saliency (
                           M
                        )
                           
                              (10)
                              
                                 
                                    M
                                    =
                                    G
                                    ∗
                                    
                                       
                                          
                                             X
                                             ¯
                                          
                                          ∘
                                          
                                             X
                                             ¯
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              X
                              ¯
                           
                         is the inverse discrete cosine transform of 
                           IS
                         and the operator ∘ is the Hadamard (entrywise) product operator and 
                           G
                         is a 2-D Gaussian kernel to smooth the saliency map.

Emami and Hoberock [9,24] introduced a model based on the Itti and Khoch visual saliency model [3] with a new normalization method. They used the weighted volume trapped between the feature saliency map and a surface parallel to the ij plane passing through the point [l, k, M[l,k]] as the normalized value M
                        
                           Normalized
                        [l,k] of the feature saliency map at [l,k], given by:
                           
                              (11)
                              
                                 
                                    
                                       M
                                       Normalized
                                    
                                    
                                       l
                                       k
                                    
                                    =
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          I
                                       
                                       
                                          
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                J
                                             
                                             
                                                
                                                   f
                                                   lk
                                                
                                             
                                          
                                       
                                    
                                    
                                       i
                                       j
                                    
                                    ×
                                    
                                       
                                          M
                                          
                                             l
                                             k
                                          
                                          −
                                          M
                                          
                                             i
                                             j
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        where 
                           f
                           lk
                         is a Gaussian weight function centered at pixel [l,k], and both 
                           M
                         and 
                           f
                         are matrices of size I
                        ×
                        J.

Some researchers define the goal of attention modeling to be finding a model which minimizes the error in locating human observer's fixations [8]. However, the most common approach is finding a model which predicts a human eye saliency map [2,10,13,16,25]. The method to compute human saliency maps is explained in the next section. Accordingly, all comparison metrics are designed to find similarities between a saliency map calculated by a visual saliency model and a same size reference map. In this paper, the saliency map computed with a visual saliency model is referred to as the Predicted Saliency Map (PSM), denoted by saliency values 
                        M
                     (k,l). The reference maps are extracted from the LPWHL database [10], which are called Reference Human Saliency Maps (RHSMs), and are denoted by saliency values 
                        N
                     (k,l). 1
                     ≤
                     k
                     ≤
                     K and 1
                     ≤
                     l
                     ≤
                     L, where K and L are the height and width of the maps in pixels, respectively. In this section, three new metrics along with six already published metrics to compare saliency maps are explained.

Rearranging a PSM and an RHSM in ‖.‖ to KL length vectors m
                        1 and m
                        2, we define cosine of the angle between two maps as:
                           
                              (12)
                              
                                 
                                    Cosθ
                                    =
                                    
                                       
                                          
                                             m
                                             1
                                          
                                          
                                             m
                                             2
                                          
                                       
                                       
                                          
                                             
                                                m
                                                1
                                             
                                          
                                          .
                                          
                                             
                                                m
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 〈.,.〉 is the inner product of the vectors and is the vector 2-norm. Cosθ
                        =1 indicates two maps are identical. We introduce Cosθ as a measure of similarities between two maps.

We define Score2 as the average of an RHSM values at the first (highest) N peaks of a PSM, given by Eq. (13):
                           
                              (13)
                              
                                 
                                    Score
                                    2
                                    =
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          N
                                       
                                       
                                          M
                                          
                                             
                                                k
                                                l
                                             
                                             i
                                          
                                       
                                    
                                    /
                                    N
                                 
                              
                           
                        where (k, l)
                           i
                         is the pixel location of the ith peak of the PSM (ith fixation of the visual saliency model). A large Score2 value shows that the N highest salient points found by the algorithm are the prominent locations of the image found by human observers. Therefore, the larger is Score2, the better is the performance of the visual saliency mechanism. Herein, Score2 is introduced to find resemblances between a PSM and the RHSM, when N
                        =
                        5.

The difference map is defined as the difference between the PSM computed for an image and the image RHSM. We suggest treating the difference map as a vector with length K
                        ×
                        L and using its vector 1-norm as a comparison metric to find dissimilarities between two maps, as follows:
                           
                              (14)
                              
                                 
                                    DM
                                    =
                                    
                                       
                                          ∑
                                          
                                             k
                                             =
                                             1
                                          
                                          K
                                       
                                       
                                          
                                             
                                                ∑
                                                
                                                   l
                                                   =
                                                   1
                                                
                                                L
                                             
                                             
                                                
                                                   
                                                      Μ
                                                      
                                                         k
                                                         l
                                                      
                                                      −
                                                      N
                                                      
                                                         k
                                                         l
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    /
                                    
                                       
                                          K
                                          ×
                                          L
                                       
                                    
                                 
                              
                           
                        
                        NDM defined as:
                           
                              (15)
                              
                                 
                                    NDM
                                    =
                                    1
                                    −
                                    DM
                                    ,
                                 
                              
                           
                        is introduced in this paper as a measure of similarities between two maps.

In [26] the authors used the number of correct objects detected in the first 5 fixations of a saliency model as a measure of its performance, where, local maxima of the PSM are considered as fixation points. Hit rate is defined as follows:
                           
                              (16)
                              
                                 
                                    Hit
                                    
                                    Rate
                                    
                                    =
                                    
                                    
                                       
                                          Number
                                          
                                          of
                                          
                                          correct
                                          
                                          Hits
                                       
                                       N
                                    
                                    
                                 
                              
                           
                        In [10] images are not divided into objects of interest and background. However, the 20% most salient pixels of the image based on its RHSM is selected as the foreground, see Fig. 1(d), and the rest is called the background of the image. Accordingly, if a fixation point happens to be in the foreground, it is considered as correct object detection (0
                        ≤
                        Hit Rate
                        ≤
                        1). Herein N
                        =
                        min(5,Number of peaks in the PSM).

In addition to “Hit Rate”, the ability to find the most salient object in the image in the first four fixations is employed in [26] to evaluate visual saliency models. In this paper, the global maximum of the RHSM is considered as the most salient location of the image, and the minimum distance (d) of the first four fixations of a visual saliency model is defined as a metric to evaluate its performance. Since in this paper comparison metrics are defined to measure similarities between two maps, DS is defined as follows:
                           
                              (17)
                              
                                 
                                    DS
                                    =
                                    1
                                    −
                                    
                                       d
                                       
                                          
                                             
                                                L
                                                2
                                             
                                             +
                                             
                                                K
                                                2
                                             
                                          
                                       
                                    
                                    
                                 
                              
                           
                        as a comparison metric (0
                        ≤
                        DS
                        ≤
                        1and the larger is DS the closer are two maps).

The correlation coefficient of two saliency maps defined in Eq. (18) is used in [16,27,28] to find the linear relationship between two maps.
                           
                              (18)
                              
                                 
                                    CC
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                k
                                                ,
                                                l
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            M
                                                            1
                                                         
                                                         
                                                            k
                                                            l
                                                         
                                                         −
                                                         
                                                            μ
                                                            1
                                                         
                                                      
                                                   
                                                   ×
                                                   
                                                      
                                                         
                                                            M
                                                            2
                                                         
                                                         
                                                            k
                                                            l
                                                         
                                                         −
                                                         
                                                            μ
                                                            2
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      ∑
                                                      
                                                         k
                                                         ,
                                                         l
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               M
                                                               1
                                                            
                                                            
                                                               k
                                                               l
                                                            
                                                            −
                                                            
                                                               μ
                                                               1
                                                            
                                                         
                                                      
                                                   
                                                
                                                2
                                             
                                             ×
                                             
                                                
                                                   
                                                      ∑
                                                      
                                                         k
                                                         ,
                                                         l
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               M
                                                               2
                                                            
                                                            
                                                               k
                                                               l
                                                            
                                                            −
                                                            
                                                               μ
                                                               2
                                                            
                                                         
                                                      
                                                   
                                                
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where μi
                         is the mean value of the map Mi
                        . Normalizing CC in the [0 1] interval produces the Normalized Correlation Coefficient, NCC, defined by:
                           
                              (19)
                              
                                 
                                    NCC
                                    =
                                    
                                       
                                          1
                                          +
                                          CC
                                       
                                    
                                    /
                                    2
                                 
                              
                           
                        
                        NCC
                        =
                        1 implies that two maps are either exactly equal or are different by a constant value.

One of the commonly used saliency map comparison metrics in the literature is the ROC area [2,13,17,20,23,25,29,30]. This metric determines how well salient and non-salient regions of the image can be discriminated by their saliency value in a PSM using a simple threshold [25]. Similar to “Hit Rate”, the 20% most salient pixels of the image based on its RHSM are selected as salient regions of the image, and the rest are called non-salient. Also, a binary map is created by thresholding the PSM. The threshold is increased gradually from the minimum of the map to its maximum, which changes both the hit rate (labeling salient locations as salient by the PSM) and false alarm rate (labeling a non-salient location as salient). The ROC is a curve that plots the false alarm rate as a function of the hit rate (0
                        ≤
                        Hit Rate & False alarm rate
                        ≤
                        1). The area under the ROC, ROC Area, is a well-known measure of similarity between two saliency maps.

The Kullback–Leibler divergence value introduced in Eq. (20) is used as a measure of dissimilarities between two maps in [16,31].
                           
                              (20)
                              
                                 
                                    KL
                                    
                                       
                                          p
                                          
                                             h
                                          
                                       
                                    
                                    =
                                    
                                       
                                          ∑
                                          x
                                       
                                       
                                          p
                                          
                                             x
                                          
                                       
                                    
                                    log
                                    
                                       
                                          p
                                          
                                             x
                                          
                                       
                                       
                                          h
                                          
                                             x
                                          
                                       
                                    
                                 
                              
                           
                        where p(x) is the predicted probability density function calculated from the PSM. h(x) is the probability density drawn from the RHSM of the image. KL is generally used to measure distance between two probability distributions. NKL, defined as
                           
                              (21)
                              
                                 
                                    NKL
                                    =
                                    1
                                    −
                                    KL
                                 
                              
                           
                        is used in this paper to find similarities between two maps.

Define average fixation saliency (
                           
                              
                                 M
                                 ¯
                              
                              fix
                           
                        ) obtained when sampling the PSM at the fixations of human observers, and average saliency (μM
                        ) as the mean of the PSM. Then Score 
                        [32] is defined as:
                           
                              (22)
                              
                                 
                                    Score
                                    =
                                    
                                       
                                          M
                                          ¯
                                       
                                       fix
                                    
                                    −
                                    
                                       μ
                                       M
                                    
                                    /
                                    
                                       μ
                                       M
                                    
                                 
                              
                           
                        
                        Score is also used in [33,34] and a similar metric is used in [35]. In this paper, Score values are normalized to lie in the interval [0 1].

There are several saliency map comparison metrics in the literature, but it can be easily shown that results from these in ranking different saliency models do not agree. The best visual saliency model identified by one comparison metric might show poor results when evaluated by another metric. Therefore, before ranking visual saliency models, it is important to identify which metric could be considered the best. The database created in [10] is used herein to design an evaluation procedure for comparison metrics. The longest dimension of each image in the data base is 1024 and the other dimension varies from 405 to 1024, with the majority having 768 pixels.

It is commonly accepted that the best saliency map for an image is the one created using human observers' fixation data. Using the LPWHL database [10], two fixation maps are defined for each image herein. Randomly selecting a simple majority of fixations for a given image, a reference fixation map is created. The remaining fixations for that image are used to generate another map called the human fixation map. Saliency maps are generated from these fixation maps by convolving them with a 2-D Gaussian function, namely the Gaussian function used in [10]. The saliency map computed from the human fixation map is called the Human Saliency Map (HSM), and the saliency map computed from the reference fixation map is designated the RHSM. Accordingly, we propose for our first criterion a good comparison metric would be expected to find HSMs analogous to the corresponding RHSMs.

Observers in the LPWHL study might have had different priorities during the experiment. It has been proved [6] that given the same image, fixations and patterns of saccade do change for different questions that were asked of an observer prior to viewing the image, which is believed to be a property of top-down saliency [5,6]. Accordingly, we believe it is important to select fixations randomly for the reference fixation map, instead of choosing all fixations of a fixed sample of observers.

In this paper, it has been assumed that the worst fixation map for a specific image is a random selection of image points that are designated as “random fixation points” when in fact they typically would not be true fixation points. Such a map is generated for each image by randomly selecting some locations across the image as “fixations”. Similar to HSMs and RHSMs, a map is created convolving this random fixation map with the 2-D Gaussian function used in [10], and it is designated a Random Saliency Map (RSM). Our second criterion for a good comparison metric is that it should clearly distinguish an RSM for a given image as dissimilar from an RHSM for that image. Fig. 3
                         shows an image (a) from [10] along with its original saliency Map with 75 fixation points (b). Samples of RHSM (c), HSM (d) and RSM (e) for N
                        
                           Ref.fix.
                        
                        =50 are also shown with the average of RHSMs (f), HSMs (g) and RSMs (h) over 100 samples. The higher the number of fixations used in creating RHSM and HSM, the closer are these maps to the original saliency map. Although HSMs created using a small number of fixations do not seem similar to the original saliency map, the average RHSMs and HSMs (over 100 samples) are very similar to the original saliency map. As expected, the average of the RSMs is still a random map, and different regions are highlighted randomly Fig. 3(h).

In this Section, for each image with fixation points from [10], reference fixation maps are created using N
                        Ref.fix.
                        =40,45, …, 70 fixations chosen randomly from the fixation database. The remaining fixations in the database (35, 30,…, 5 fixations) are used to create human fixation maps. Then for each image, a number of random pixels are selected equal to the number of fixations chosen for the human fixation map. For each fixation number, all comparison metrics are employed to compare RSMs and HSMs with the corresponding RHSMs. This process was repeated 100 times, with seven N
                        
                           Ref.fix.
                         values for all images in the database (702,100 repetitions) and the results are illustrated in Table 1
                        . 100 repetitions are chosen so that the results do not change as the number of repetitions increases from 70 to 100.

Since the number of salient pixels usually varies in different saliency maps, Judd et al. [36] suggest matching the histogram of the saliency maps created for an image with the histogram of the reference saliency map of the image before comparing them together. This way the number of salient pixels in different saliency maps would be very close together. It creates a fairer basis for comparing saliency maps. Fig. 4
                         shows an RHSM with 50 fixations (a) for the image shown in Fig. 3(a), HSM and RSM (b) and (d) with 25 fixations, and the HSM and RSM after histogram matching (c) and (e). Cumulative frequency distributions with 256 bins for these maps are plotted in Fig. 4(f). For simplicity in analyzing graphs, in the cumulative frequency distribution curves, the numbers of zeros in the maps are not counted.

As demonstrated in Fig. 4(f), although the number of fixations in the sample HSM and RSM are equal, and fixation maps are convolved with the same Gaussian filter, the number of salient points in the RSM is almost twice the HSM. This is caused by the fact that in the random fixation maps, fixations are distributed widely and usually far from each other. On the other hand, in the human fixation maps, fixations are distributed mostly around salient locations in the central parts of the map. The number of salient pixels in HSM and RSM will be close to the number of salient pixels in the RHSM after histogram matching, as shown in Fig. 4(f). RHSMs, HSMs and RSMs are normalized to the [0 1] interval before histogram matching. Two approaches were taken in our investigations: RSMs and HSMs were compared with the corresponding RHSMs before and after histogram matching.

All comparison metrics compute scalar values in the [0 1] interval. Good metrics are expected to produce high values close to 1 when comparing HSMs with RHSMs, and low values close to 0 when comparing RSMs with RHSMs. Accordingly, we designate the best comparison metric as the one which discriminates the best between HSMs and RSMs. We use thresholding to determine how HSMs are discriminated from RSMs by comparison metrics. The threshold for each metric is given by:
                           
                              (23)
                              
                                 
                                    Threshold
                                    =
                                    
                                       
                                          
                                             Arg
                                             min
                                          
                                       
                                       Thr
                                    
                                    
                                       
                                          
                                             
                                                
                                                   ∫
                                                   0
                                                   Thr
                                                
                                                
                                                   
                                                      y
                                                      1
                                                   
                                                   dx
                                                
                                             
                                          
                                          +
                                          
                                             
                                                
                                                   ∫
                                                   Thr
                                                   1
                                                
                                                
                                                   
                                                      y
                                                      2
                                                   
                                                   dx
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where y1
                         is the HSM histogram and y2
                         is the RSM Histogram, respectively; and Thr stands for threshold (metric result histograms are created using 1000 bins). For example, the NCC threshold value is 0.621 for which NCC produces results larger than this threshold for 688,915 of HSMs (out of 702,100) and produces results smaller than this threshold for 693,443 of RSMs. Accordingly, we consider NCC as correctly classifying 688,915 HSMs and 693,443 RSMs, and the rest (13,185 HSMs and 8,657 RSMs) are misclassified. The percentage of misclassifications and related threshold values are shown in Table 1 for each of the metrics. We designate the best metric as that which produces the lowest misclassification percentage.

Abbreviations for each metric are given in Section 3. Table 1 ranks the metrics according to their misclassification percentage, with evaluation rank 1 being the best. Table 1 shows that without histogram matching, the best metrics are NCC and Cosθ, which result in misclassification percentages of 1.56% and 1.89%, respectively. However, we believe matching the histograms of the saliency maps with RHSMs is essential in creating an impartial evaluation. Accordingly, since Score creates minimum misclassification error after histogram matching (0.634%), we designate Score as the best metric for comparing saliency maps. On the other hand, NKL (Kullback–Leibler divergence) with 21.45% and 43.56% misclassification, without and with histogram matching respectively, is the worst metric. NKL compares the probability density functions of the saliency maps. After histogram matching probability density functions of RSMs and HSMs would be very similar to each other, such that NKL cannot distinguish between them. As shown in Eqs. (12) and (18), the Cosθ and NCC formulations are very similar. Therefore, we expected them to produce similar results. Table 1 demonstrates that their results are very similar for both cases.

All visual saliency models summarized in Section 2 have been applied to all the 1003 images in the LPWHL database [10]. Their codes were downloaded from their websites, except for the CC model [16] for which the authors provided us with their saliency maps on the LPWHL database. Fig. 5
                         gives pictorial saliency results for one image before and after histogram matching. Abbreviations for each of these methods are given in Section 2.

As discussed before and demonstrated in Figs. 4 and 5, the number of highlighted (salient) pixels returned by each visual saliency model for an image varies greatly. As shown in Fig. 5, histogram matching helps us find locations that each model finds most salient. Accordingly herein, histograms of the PSMs (predicted saliency maps) are matched with the RHSM histograms before comparison. In this section, the RHSM for each image in the database is computed using all of the fixation points for that image. Saliency model comparison results without histogram matching can be found in [9].

PSMs for each image are compared with the RHSM for that image using all comparison metrics. Box-plots of the Score (the highest ranked saliency map comparison metric for the case with histogram matching) results are shown in Fig. 6
                        . To check if there are statistically significant differences between models, first the normality of their Score values are checked using the Shapiro–Wilk method [37]. Score values of none of the results are normally distributed with a significance level of 0.05. Accordingly, one of the best non-parametric methods, the Wilcoxon test [38], was used, and results are given in Table 2
                         with 0 for no statistically significant difference and 1 for significant difference.

In each box in Fig. 6, the central horizontal red line is the median. The higher the median, the better the performance of the visual saliency model. The lower and upper edges of the box mark the 25th and 75th percentiles, q1
                         and q3
                         respectively, and the horizontal whiskers show ±2.7σ (or 99.3% coverage if the data are normally distributed). In these boxplots, data-points larger than q3
                        
                        +
                        1.5
                        ×(q3
                        
                        −
                        q1
                        ) or smaller than q1
                        
                        −
                        1.5
                        ×(q3
                        
                        −
                        q1
                        ) are drawn as outliers (magenta pluses). The horizontal dotted line (Score
                        =
                        0.0553) shows the threshold for Score in Table 1 that discriminates RSMs from HSMs. Accordingly, saliency maps for which Score is larger than the threshold are considered more similar to the corresponding RHSM, the larger indicating the more similar. Maps for which Score is less than the threshold line are closer to an RSM than the corresponding RHSM. For example, using the threshold value of 0.0553, 661 saliency maps (out of 1003) computed by the GBVS algorithm were found more similar to an RSM than the corresponding RHSM.


                        Fig. 6 shows GBVS outperforms other models and EH is ranked 2nd based on the Score comparison metric. All medians are under the threshold line, which indicates the majority of the PSMs created by each model are classified as RSMs. The Wilcoxon test found no significant difference between GBVS and EH, which are statistically different from the rest of the models as demonstrated in Table 2. CC, IS and CASD differences are not significant, as well as the difference between IK, SUN and SR.

Saliency comparison metric averages and the rankings based on them in parentheses (the lower the rank number, the better) for all visual saliency models are shown in Table 3
                        . The number of maps classified as RSM based on each metric and the ranking based on the number of RSM maps in parentheses are also shown in Table 4
                        . The right column of this table shows the average number of maps classified as RSM based on all saliency comparison metrics.


                        Tables 3 and 4 show that the GBVS model outperforms other visual saliency models based on Score and also based on the average number of maps classified as RSM over all metrics. EH is ranked 2nd and FTSRD is ranked lowest in these tables. As a further note, the rankings based on Score are very similar to the overall ranking based on the average RSMs for all metrics. Also, after histogram matching, saliency map comparison metrics tend to agree more on ranking visual saliency models compared to the case without histogram matching in [9]. Accordingly, we select GBVS (Graph Based Visual Saliency) as the best bottom-up visual saliency model.

Judd et al. [36] and Borji et al. [39] stated that visual saliency models that create blurrier saliency maps usually are ranked higher than models that create saliency maps with sharp edges. Also, maps that are biased towards the image center tend to gain better results than others at predicting HRSMs. In this section, we optimize the level of blurriness and center-bias of each visual saliency model by varying appropriate parameters characterizing these effects and choosing the parameter values that maximize the performance of the visual saliency model. This creates the opportunity to compare visual saliency models at the best levels of blurriness and degree of center-bias for each model. For blurriness, PSMs are convolved with 2D Gaussian filters with σ
                           =10, 20, 30, …, 150pixels to produce a map designated 
                              M
                              Blurred
                           . Then a weighted Center-Map (
                              CM
                           ) is added to a weighted 
                              M
                              Blurred
                            to produce a new saliency map 
                              M
                              New
                           , using the following equation as suggested in [36]:
                              
                                 (24)
                                 
                                    
                                       
                                          M
                                          new
                                       
                                       =
                                       w
                                       ×
                                       CM
                                       +
                                       
                                          
                                             1
                                             −
                                             w
                                          
                                       
                                       ×
                                       
                                          M
                                          Blurred
                                       
                                    
                                 
                              
                           where w
                           =0,0.1,0.2,…,1, and the closest 2D Gaussian blob to the average of all RHSMs shown in Fig. 7
                            is selected as the 
                              CM
                           ; however, the histogram of the center-map is matched to the histograms of the RHSMs before using (24). w
                           =
                           0 indicates that the new map is identical to the blurred map, and w
                           =
                           1 indicates the new map is equal to the center map.

Exploring histogram matching, the optimized level of blurriness and center-bias of each visual saliency model by varying the variance σ of the Gaussian filter and the weight w in Eq. (24) is studied in this section. As an example of this study, Fig. 8
                            shows a sample image, its RHSM, PSMs produced by the SR visual saliency model, blurred and center-biased saliency maps, and their Score results. The histograms of the resulting saliency maps are also matched to the histograms of the corresponding RHSM before comparison.

Note in Fig. 8 that for the shown image, SR benefits from a certain degree of blurriness, with the highest value of Score
                           =0.0981 for σ
                           =10 and w
                           =0. After modifying PSMs using Eq. (24), they are compared to the corresponding RHSM using Score (as the best metric for the histogram matching process). The optimum Gaussian blurring value for σ of each visual saliency map, the best weight values for adding the center-map, the visual saliency models new average Score, the standard deviations and their percentages of improvement are shown in Table 5
                           .

As demonstrated in Table 5, all visual saliency models can improve their performance with blurring and adding a center biased map. GBVS performed better than other models; however, all visual saliency models produce 614 to 667 minimum number of maps classified as RSM which are 61.2% to 66.5% of the images in the database. This indicates that visual saliency models under histogram matching produce relatively poor results when evaluated by Score. This also indicates that there is clearly an opportunity for an improved visual saliency model. GBVS used the minimum of the center map (50%), and FTSRD and SR used the maximum (100%). This means that the center map outperforms FTSRD and SR in mimicking human observers.

In this section, we test visual saliency mechanisms on a benchmark of 54 synthetic images used in [39], 6 of which are shown in Fig. 9
                         first row. CC [16] is not included here because we did not possess the computer codes for CC. Since there are no reference saliency maps available for the synthetic images dataset, reference fixation maps were generated by manually selecting 1 to 5 pixels at the central parts of the salient regions, depending on the number of salient regions in the image. Then, reference saliency maps were created by convolving the fixation maps by a 2D Gaussian filter with proper σs that highlight the salient regions, with results depicted in Fig. 9 second row. In Fig. 9, images are sorted based on their average Score values from all saliency models from high at the left to low at the right. All visual saliency models could find the salient regions in Fig. 9(a), and none of the models could detect the salient parts of (f).

To evaluate and rank saliency model performance on the synthetic images, first the histograms of the PSMs for each image were matched to the corresponding reference saliency map. Then PSMs were compared to reference maps using Score, and results are depicted in Fig. 10
                         and Table 7. The Shapiro–Wilk test shows that Score results for none of the models are normally distributed, with a significance level of 0.05. Accordingly, the Wilcoxon method is used to analyze the difference between models, and the results are shown in Table 6
                        .


                        Table 6 demonstrates that GBVS, CASD and IS are not found statistically different by the Wilcoxon test; however, we believe that the differences between IS and the first two ranked models are significant. This is a drawback of non-parametric statistical models like Wilcoxon, that when the number of data points is small, their results are not as reliable as when there are many data points. Also, no significant differences between EH results and AIM and IS were found, as well as between SUN, SR, FTSRD and IK.

As shown in Fig. 10 and Table 7
                        , GBVS with 6 maps classified as RSM (11.1%) produces the best PSMs for the synthetic image database. CASD and EH are 2nd and 3rd, respectively, with RSM percentages of 18.5 and 25.9, respectively. Models ranked 4th to 6th are very close in RSM percentages (around 42%). The saliency models FTSRD, SR and SUN, with Score means of 0.268, 0.217 and 0.232, have 61.1%, 62.9%, and 66.7% of maps classified as RSM, which rank them lowest. Accordingly, we select GBVS as the best bottom-up visual saliency model on synthetic images. As demonstrated in Table 7, the standard deviations for EH and IS are so large that their 25th and 75th percentiles in Fig. 10 meet the lower and the upper bounds of the data (0 and 1), respectively.

Comparison of Table 7 with Table 5 shows bottom-up visual saliency models performed much better on synthetic images than natural images. This is because, synthetic images contain solely bottom-up features and information, e.g. change in color or orientation. Also, reference saliency maps created for synthetic images highlight only salient regions of the image and include all such regions. In the RHSMs created in the LPWHL database [10], sometimes salient regions close to the boundary of the image are missed, and non-important parts of the images close to the center are highlighted in the saliency maps.

As suggested in [36,39] for fair model comparison, PSMs were convolved with 2D Gaussian filters with σ
                           =5, 10, …, 25pixels and then were compared with the corresponding reference saliency maps using Score after histogram matching. Since, most of the saliency maps highlight parts other than the center, analyzing center bias would not create useful results. Accordingly, only the level of blurriness is studied here. The optimum Gaussian blurring level for each visual saliency model, their new average Score values, the number of maps classified as RSM, and their percentages of improvement are shown in Fig. 11
                            and Table 8
                           .


                           Fig. 11 and Table 8 show that blurring improves the performance of most of the visual saliency models. After blurring, with 9.25% improvement compared to Table 7, CASD reaches a mean of Score of 0.846 with only 9.25% of maps classified as RSM. However, we see that the performances of GVBS and IS visual saliency mechanisms deteriorates with blurring. Improvement percentages are calculated in comparison with means of Score in Table 7.

@&#DISCUSSION@&#

Our studies show that GBVS, the highest ranked visual saliency model for all but the synthetic images, mimics human observer behavior better than other bottom-up saliency mechanisms discussed herein for two main reasons. First, in an RHSM, the central parts of salient objects are highlighted, not their boundaries. Visual saliency algorithms based on the center-surround mechanism and self-information have difficulty activating salient regions far from the salient object boundaries in the feature saliency map. On the other hand, the GBVS algorithm highlights salient regions that are distant from the object boundaries. Accordingly, saliency maps computed by GBVS algorithm are more similar to human saliency maps than saliency maps from other algorithms. Second, saliency maps generated in the LPWHL study [10] highlight the central areas of the images as salient regions more often than non-central areas. Fig. 7 shows that on average, saliency maps in this dataset are center-biased. It is worth mentioning that the saliency map datasets from Bruce and Tsotsos [20] and ORIG [40] also are highly center-biased [8]. Due to the Gaussian weight function in Eq. (6), the GBVS model tends to highlight salient objects in the central regions of an image more than objects close to the image boundaries. This means that GVBS saliency maps tend to be center-biased as well. However, results in Section 5 showed that GBVS outperforms other bottom-up visual saliency models in spite of this feature.

Since the IK [3] and EH [9,24] saliency models differ only in the normalization methods employed to normalize feature saliency maps, this study shows that the EH normalization method performs much better than the IK normalization method. This also shows the importance of normalizing feature saliency maps before combining them to produce a calculated saliency map Table 4 shows that for saliency maps generated by GBVS (the best model) 661 (65.9%) of the PSMs (calculated using Score) are more similar to RSMs than RHSMs. This fact demonstrates that there is significant room for improvement in bottom-up visual saliency models. Fig. 12(a) presents an image from [10] for which all PSMs produced by all visual saliency models have Score values smaller than the thresholds. Therefore, they are closer to an RSM than the RHSM of the image shown in Fig. 12(b). This occurs because the bottom of the image shows a written phrase that attracts a human observer's attention. Bottom-up visual saliency models find no difference between texts and textures. This shows the importance of incorporating high level top-down visual saliency attributes in an over-all visual saliency model. However, results on the synthetic images showed that the high-rated bottom-up visual saliency models perform satisfactorily on synthetic images that contain bottom-up salient features.

One of the main problems of visual saliency models is using predetermined parameters in the algorithm. For example center-surround mechanisms are applied with predetermined sets of radii, while the radii should be determined based on the properties of the display, such as: size of the objects in the display, distances between objects in the display, and textures of the objects. Another problem is that only one set of parameters is employed to analyze the entire image. We believe that to effectively imitate the HVS saliency mechanism, different parts of the visual scene should be analyzed with different sets of parameters (analyzing the image locally). Of course, this remains a research challenge.

@&#CONCLUSION@&#

Ten well-known bottom-up visual saliency models and different saliency map comparison metrics are examined in this paper. Employing a novel method to evaluate comparison metrics, the best metrics were found to be NCC (the normalized value of the correlation coefficient between two maps) and Score for comparison without and with histogram matching, respectively. NCC and Score produce minimum misclassification errors on discriminating human saliency maps and random saliency maps. Interestingly, two commonly used comparison metrics, ROC Area and Kullback–Leibler divergence (NKL), were ranked 5th and 9th among 9 metrics. All visual saliency models were applied to databases of 1003 natural and 54 synthetic images. The resulting saliency maps were compared with reference human saliency maps using all comparison metric. The Graph Based Visual Saliency (GBVS) model outperformed other visual saliency models on mimicking human observers' behavior on natural images. Optimum level of blurring and center-bias was studied for all visual saliency models. Context-Aware Saliency Detection (CASD) was ranked first on synthetic images after applying the optimum level of blurriness.

@&#REFERENCES@&#

