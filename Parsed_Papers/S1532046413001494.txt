@&#MAIN-TITLE@&#Supervised methods for symptom name recognition in free-text clinical records of traditional Chinese medicine: An empirical study

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Symptom name recognition from free-text clinical records of TCM is well studied.


                        
                        
                           
                           Supervised methods are investigated and helpful strategies are proposed.


                        
                        
                           
                           Elaborate experiments are implemented and results are explained carefully.


                        
                        
                           
                           Effectiveness of the investigated methods is validated.


                        
                        
                           
                           This work is a fundamental task to other clinical researches of TCM.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Symptom name recognition

Free-text clinical records

Traditional Chinese medicine

Supervised sequence classification

Natural language processing

@&#ABSTRACT@&#


               
               
                  Clinical records of traditional Chinese medicine (TCM) are documented by TCM doctors during their routine diagnostic work. These records contain abundant knowledge and reflect the clinical experience of TCM doctors. In recent years, with the modernization of TCM clinical practice, these clinical records have begun to be digitized. Data mining (DM) and machine learning (ML) methods provide an opportunity for researchers to discover TCM regularities buried in the large volume of clinical records. There has been some work on this problem. Existing methods have been validated on a limited amount of manually well-structured data. However, the contents of most fields in the clinical records are unstructured. As a result, the previous methods verified on the well-structured data will not work effectively on the free-text clinical records (FCRs), and the FCRs are, consequently, required to be structured in advance. Manually structuring the large volume of TCM FCRs is time-consuming and labor-intensive, but the development of automatic methods for the structuring task is at an early stage. Therefore, in this paper, symptom name recognition (SNR) in the chief complaints, which is one of the important tasks to structure the FCRs of TCM, is carefully studied. The SNR task is reasonably treated as a sequence labeling problem, and several fundamental and practical problems in the SNR task are studied, such as how to adapt a general sequence labeling strategy for the SNR task according to the domain-specific characteristics of the chief complaints and which sequence classifier is more appropriate to solve the SNR task. To answer these questions, a series of elaborate experiments were performed, and the results are explained in detail.
               
            

@&#INTRODUCTION@&#

Traditional Chinese medicine (TCM) provides a distinctive way to perceive the human body and is becoming a medical theory complementary to Western medicine [1–4]. TCM knowledge is held largely in the minds of clinically experienced TCM doctors. Consequently, the clinical records of TCM, which are documented by TCM doctors during their routine diagnostic work, naturally constitute an abundant clinical knowledge source of TCM that can be inherited by the next generation of practitioners. However, with the increase in the accumulation of clinical records, comprehensive summarization of complicated TCM regularities is difficult. Fortunately, with the modernization of TCM clinical practice, clinical records have begun to be digitized. This provides an opportunity for researchers to discover, with the help of data mining (DM) and machine learning (ML) methods, TCM regularities buried in the large volume of digitized clinical records.

There has been some work on applying DMs and MLs to TCM knowledge discovery, such as discovering TCM knowledge from well-structured literature by using Bayesian networks [5,6] and establishing TCM expert systems for decision support by the naïve Bayes classifier based on a limited amount of manually structured data [7]. However, the contents of most fields in the clinical records, e.g. the chief complaints, are unstructured (or free-text). The result is that the methods that are verified on the well-structured data, cannot be directly applied to knowledge discovery in the free-text clinical records (FCRs) of TCM. These methods, consequently, require FCRs to be structured in advance.

Manually structuring the large volume of TCM FCRs is tedious, time-consuming, and labor intensive. Hence there is an urgent need for the development of an effective method to automatically structure the FCRs, i.e. recognizing medical named entities in FCRs [8]. Named entity recognition (NER) in general text has been widely studied in the natural language processing (NLP) community [9]. For example, a hybrid Chinese NER model based on multiple features was proposed in [10], and the model was evaluated on the general text dataset called “People’s Daily”. In [11], a lexicalized hidden Markov model (HMM) approach to NER was designed and validated on “newswire” data, which is also a general text dataset. In addition, a pragmatic approach to Chinese word segmentation was proposed in [12]. This approach was implemented in an adaptive Chinese word segmenter (MSRSeg), which can simultaneously segment general Chinese text and perform NER. However, FCRs of TCM are different from general text. They have domain-specific characteristics [13] and therefore the methods designed for NER in general text might need a domain-specific adaptation for NER in the FCRs.

Medical information extraction in English FCRs of Western medicine has become a topic of great interest in recent years, and has been extensively studied due to the efforts of the Informatics for Integrating Biology and the Bedside (i2b2) project, which has released clinical record datasets that can be used as gold standards by the medical NLP research community. In 2009, i2b2 organized a medical information extraction challenge on extracting medications, dosages, modes, durations, etc. from the English discharge summaries [14]. In the following year, a medical problem, test, and treatment concept extraction challenge was organized by i2b2 [15]. Subsequently, based on the public clinical record datasets, a series of excellent work on NER in English discharge summaries of Western medicine has been published. These NER tasks are usually treated as a sequence labeling problem, and then the open-domain sequence classifiers, e.g. Conditional Random Field model (CRF), are adapted to the medical domain [16–26] with the help of domain knowledge and domain-specific sources. For example, the domain vocabulary used in [16], the domain-specific rules in [17], and the knowledge-rich sources utilized in [18–20] have been used for these purposes. Because of the differences between English and Chinese [27] and owing to the distinctive characteristics of TCM FCRs [13], methods, that could be borrowed from English NER in the discharge summaries of Western medicine need adaptation for NER in FCRs of TCM.

The development of NER in FCRs of TCM has fallen behind the progress of English NER in FCRs of Western medicine. NER in the TCM community was first attempted in 2012 [13]. Symptom name recognition (SNR) in the chief complaints, which is one of the most important tasks of NER in FCRs of TCM, was accomplished by the bigram-based dictionary-matching method. However, various literal forms of symptoms were generated during the routine diagnostic work of TCM doctors [28]. Consequently, the application of the dictionary-matching method in clinical practice requires maintenance of a symptom name dictionary. However, the dictionary-matching method is laborious, making it less appropriate for use in practice. SNR in the chief complaints was also studied in [29]. Based on method for English NER in discharge summaries of Western medicine, SNR in chief complaints was treated directly as a sequence labeling problem and solved by CRF with two types of useful features. This preliminary work still leaves many questions waiting to be answered, such as:
                        
                           (1)
                           Is it suitable to treat SNR in the chief complaints as a sequence labeling problem? In other words, are there any domain-specific characteristics that would facilitate SNR completion by the sequence classifiers?

The chief complaints in TCM FCRs have domain-specific characteristics. Some domain-specific adaptation to the general sequence labeling strategy for the SNR task might be needed. How can an appropriate adaptation be made?

Several sequence classifiers, e.g. HMM, maximum entropy Markov model (MEMM), and CRF, can be used to solve the sequence labeling problem. Each sequence classifier has its own specializations. Which is most suitable to SNR and which can achieve the best performance?

To answer these questions, we focus our attention in this paper on studying SNR in chief complaints. First, the SNR task is treated as a sequence labeling problem reasonably. Second, a new sequence labeling strategy is designed for SNR in the chief complaints based domain-characteristics. These approaches are introduced in Section 2. In Section 3, three typically supervised sequence classifiers (HMM, MEMM, and CRF) are applied to the SNR task with an empirical analysis. Elaborate experiments are performed and described in Section 4 aiming to answer the previously raised questions. Finally, Sections 5 and 6, respectively, provide further discussion and conclusions.

Symptom names in TCM are usually composed of three aspects of descriptions: body location, sensation, and intensity. For example, the symptom name “头痛剧烈” (a severe headache) consists of three aspects of descriptions including a body location “头” (head), a sensation “痛” (ache), and a intensity “剧烈” (severe).

These three aspects preferably appear in sequence and should not be split by other descriptions (e.g. possessives or temporal descriptions). For instance, the sensation “晕” (dizziness) should come after the body location “头”, but the temporal description “昨天” (yesterday), which is used to indicate the occurring time of the symptom, should be written before the symptom name “头晕” (dizziness) rather than in between the body location and the sensation.

Furthermore, technically, words used in the symptom names of TCM should have different distributions from other words that are used outside of symptom names (see Fig. 1
                         as an example). Therefore, it is appropriate to treat SNR in the chief complaints as a sequence labeling problem; the characteristics described above should facilitate the completion of SNR by the sequence classifiers.

The commonly-used sequence labeling strategy for NER in general text or in English discharge summaries of Western medicine is to label each word (defined as a “labeling unit”) in each sentence (defined as a “labeled sequence”) with a predefined tag that is used to indicate the role of the labeling unit, e.g. that it is a beginning, an intermediate, or an outside part of the named entity, usually symbolized by “B”, “I”, and “O” respectively [30]. However, chief complaints in TCM FCRs have domain characteristics [13]. The general sequence labeling strategy might need an adaptation for SNR in chief complaints. Therefore, in this section, we describe the adaptation of general sequence labeling strategy, based on several empirical reasons.

Generally, labeling units are words in the text [30]. However, there are no natural whitespaces between Chinese words in general text, including chief complaints. First, a Chinese word segmentation task should be completed before recognizing symptom names in the chief complaints based on sequence labeling [12]. Unfortunately, the Chinese word segmentation methods designed for general text will not work effectively for segmenting the chief complaints into words due to the distinctive characteristics of the chief complaints [13]. Thankfully, Chinese is different from Western languages in that its characters can bear specific meanings. Therefore, in this paper, we change the labeling units from Chinese words to Chinese characters in the chief complaints.

The chief complaints in FCRs of TCM are documented by TCM doctors during their routine diagnostic work through Physician Visit procedures. In order to improve work efficiency, TCM doctors directly note the physical conditions described by patients without organizing the content and rewriting the text, which loses contextual coherence. For example, a more coherent description of the physical conditions “昨日肠鸣, 失气多, 心中不适” (Yesterday, the patient had borborygmus and more flatulence, and his/her heart was uncomfortable) may described by different patients in different incoherent forms, such as “昨日肠鸣, 心中不适, 失气多” (Yesterday, the patient had borborygmus, his/her heart was uncomfortable, and the patient had more flatulence) or “肠鸣, 心中不适, 失气多 (昨天)” (The patient had borborygmus, his/her heart was uncomfortable, and the patient had more flatulence (all these symptoms appeared yesterday)). They would be recorded by TCM doctors directly in the chief complaints section of the record.

Moreover, various kinds of punctuations can appear in chief complaints, and they are used by TCM doctors without any standard criteria. For instance, TCM doctors may like to use a comma instead of all other punctuations (e.g. the period, which is used to divide natural sentences) for convenience. The result is that several natural sentences in chief complaints are combined into one. Taking the chief complaint “昨日肠鸣, 失气多, 心中不适, 早晨大便提早, 头昏.” (Yesterday, the patient had borborygmus, his/her heart was uncomfortable, and had more flatulence. This morning, the patient had a bowel movement earlier than before and felt dizziness.) as an example, it should be two natural sentences “昨日肠鸣, 失气多, 心中不适.” (Yesterday, the patient had borborygmus, his/her heart was uncomfortable, and had more flatulence.) and “早晨大便提早, 头昏.” (This morning, the patient had a bowel movement earlier than before and felt dizziness.), but the period, which is used to represent the end of a sentence, is replaced by a comma.

For these reasons, general labeled sequences (sentences in the text) need a domain-specific adaption. We define clauses in the chief complaints as the labeled sequences instead of the sentences. The clauses in this paper are groups of sequential Chinese characters separated by commas in the chief complaints. For example, a chief complaint “昨日肠鸣, 心中不适, 失气多” (Yesterday, the patient had borborygmus and more flatulence, and his/her heart was uncomfortable) will be divided into three clauses “昨日肠鸣” (the patient had borborygmus), “心中不适” (his/her heart was uncomfortable), and “失气多” (the patient had more flatulence). This adaptation would bring about two advantages:
                              
                                 (1)
                                 Clauses in chief complaints as labeled sequences keep the internal coherence of every clause and, at the same time, reduce the possibility of incoherent sentences occurring.

Defining the clauses as labeled sequences is similar to empirical feature selection. It reduces the number of noisy features extracted from the disorganized context in the chief complaints.

The predefined tag set is usually {“B”, “I”, “O”}, denoted as BIO. However, according to the results reported in [29], the end of symptom names in the chief complaints is more difficult to identify. Therefore, we consider defining a specific tag, e.g. “E”, to individually identify the end of the symptom names in order to improve sequence labeling performance. Thus the predefined tag set is adapted to {“B”, “I”, “E”, “O”}, denoted as BIEO.

Referring to the introduced domain-specific adaptation to general sequence labeling strategy, SNR in chief complaints can be naturally defined as follows:

Given a clause x
                        =
                        x
                        1,
                        x
                        2,…,
                        xn
                         in a chief complaint, the goal is to construct a sequence classifier to accurately label each xi
                        , which is the ith Chinese character in x, with the most reliable predefined tag yi
                        , where yi
                         is one of the elements in BIEO. Accordingly, the most reliable tag sequence 
                           
                              
                                 
                                    y
                                 
                                 
                                    ˆ
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          y
                                       
                                       
                                          ˆ
                                       
                                    
                                 
                                 
                                    1
                                    ,
                                 
                              
                              
                                 
                                    
                                       
                                          y
                                       
                                       
                                          ˆ
                                       
                                    
                                 
                                 
                                    2
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    
                                       
                                          y
                                       
                                       
                                          ˆ
                                       
                                    
                                 
                                 
                                    n
                                 
                              
                           
                         given x would be:
                           
                              (1)
                              
                                 
                                    
                                       y
                                    
                                    
                                       ˆ
                                    
                                 
                                 =
                                 
                                    
                                       arg max
                                    
                                    
                                       y
                                    
                                 
                                 P
                                 (
                                 y
                                 |
                                 x
                                 )
                              
                           
                        
                     

This problem can be effectively solved by the supervised sequence classifiers.

In Section 2, SNR in the chief complaints is reasonably treated as a sequence labeling problem and, at the same time, a new sequence labeling strategy that is adapted from the general sequence labeling strategy based on several empirical reasons is proposed for the SNR task. To complete the SNR task, supervised sequence classifiers are employed. Different sequence classifiers have their own specialties. In this section, we aim to compare three typically supervised sequence classifiers (HMM, MEMM, and CRF) and introduce them to the SNR task.

HMM is a statistical structure with stochastic state transitions (e.g. a transition from a hidden state to another hidden state, where the hidden states refer to the corresponding predefined tags of the labeling units in the SNR task) and observation generation processes (e.g. an observed Chinese character “气” (pneuma) in the clause “失气多” could be generated from a hidden state “I”). HMM can be flexibly used to solve sequence labeling task [31]. According to the definition of HMM, Eq. (1) can be modified to:
                           
                              (2)
                              
                                 
                                    
                                       y
                                    
                                    
                                       ˆ
                                    
                                 
                                 =
                                 arg
                                 
                                    
                                       
                                          max
                                       
                                       
                                          y
                                       
                                    
                                 
                                 P
                                 (
                                 
                                    
                                       y
                                    
                                    
                                       0
                                    
                                 
                                 )
                                 
                                    
                                       
                                          ∏
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          n
                                          +
                                          1
                                       
                                    
                                 
                                 P
                                 (
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 
                                    
                                       y
                                    
                                    
                                       i
                                       -
                                       1
                                    
                                 
                                 )
                                 
                                    
                                       
                                          ∏
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          n
                                       
                                    
                                 
                                 P
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 )
                              
                           
                        where y
                        0 and yn
                        
                        +1 refer to a default start tag “START” and a default end tag “STOP” respectively, P(y
                        0) equals to 1, P(yi
                        |yi
                        
                        −1) represents the state transition probability, and P(xi
                        |yi
                        ) is the observation generation probability. The state transition probabilities and the observation generation probabilities can be estimated through the Maximum Likelihood method based on a training dataset. And then 
                           
                              
                                 
                                    y
                                 
                                 
                                    ˆ
                                 
                              
                           
                         can be efficiently predicted by using the Viterbi algorithm based on the estimated state transition probabilities and observation generation probabilities [31].

HMM models SNR in the chief complaints as a probabilistic generative process in which the Chinese characters in the chief complaints are sequentially generated by their corresponding hidden tags. However, considering the SNR task from the perspective of simulating the human labeling process, the corresponding tag of an observed Chinese character is determined by a human labeler through by observing the chief complaint, finding and analyzing the evidence that can help the labeler to judge which predefined tag is the best choice of a current Chinese character, and then labeling it with the best tag. In other words, there many useful features (e.g. the evidence mentioned previously) can be utilized to help the sequence classifiers to complete the SNR task. Unfortunately, generative models (e.g. HMM) are known to be unable to conveniently incorporate additional features, whereas discriminative models are known to have an advantage in this respect. Therefore, the typical discriminative sequence classifier MEMM [32] is investigated. It is an extension of HMM in which the state transition probability and the observation generation probability in HMM are replaced by a single discriminative probability P(yi
                        |x,
                        yi
                        
                        −1). This discriminative probability allows MEMM to conveniently incorporate additional features into the sequence labeling procedure.

Mathematically, P(yi
                        |x,
                        yi
                        
                        −1) is a conditional probability of current tag yi
                         given its previous tag yi
                        
                        −1 and the observed Chinese characters in x. Then, Eq. (1) can be redefined as:
                           
                              (3)
                              
                                 
                                    
                                       y
                                    
                                    
                                       ˆ
                                    
                                 
                                 =
                                 arg
                                 
                                    
                                       
                                          max
                                       
                                       
                                          y
                                       
                                    
                                 
                                 
                                    
                                       
                                          ∏
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          n
                                       
                                    
                                 
                                 
                                    
                                       exp
                                       
                                          
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      j
                                                      =
                                                      1
                                                   
                                                   
                                                      N
                                                   
                                                
                                                
                                                   
                                                      ω
                                                   
                                                   
                                                      j
                                                   
                                                
                                                
                                                   
                                                      f
                                                   
                                                   
                                                      j
                                                   
                                                
                                                (
                                                x
                                                ,
                                                
                                                   
                                                      y
                                                   
                                                   
                                                      i
                                                      -
                                                      1
                                                   
                                                
                                                )
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             
                                                
                                                   y
                                                
                                                
                                                   i
                                                
                                             
                                             =
                                             t
                                          
                                          
                                             BIEO
                                          
                                       
                                       exp
                                       
                                          
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      j
                                                      =
                                                      1
                                                   
                                                   
                                                      N
                                                   
                                                
                                                
                                                   
                                                      ω
                                                   
                                                   
                                                      j
                                                   
                                                
                                                
                                                   
                                                      f
                                                   
                                                   
                                                      j
                                                   
                                                
                                                (
                                                x
                                                ,
                                                
                                                   
                                                      y
                                                   
                                                   
                                                      i
                                                      -
                                                      1
                                                   
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where y
                        0 refers to a default start tag “START” of every labeled sequence, n is the length of x, ωj
                         is the weighting parameter of the feature fj
                        (x,
                        yi
                        
                        −1), j
                        ∊[1,…,
                        N], and N represents the number of features incorporated. The features are fetched from x and the previous tag yi
                        
                        −1 of yi
                        , and yi
                         is the potential corresponding tag of xi
                        . In Eq. (3), t is one of the predefined tags chosen from BIEO.

The key to MEMM is to learn the weighting parameter ωj
                         for each feature. This can be efficiently solved using quasi-Newtonian methods, such as L-BFGS, gradient descent, conjugate gradient descent, and various iterative scaling algorithms [33]. In this paper, L-BFGS [34] is employed. Finally, 
                           
                              
                                 
                                    y
                                 
                                 
                                    ˆ
                                 
                              
                           
                         can also be predicted by using the Viterbi algorithm efficiently based on the learned weighting parameters [31].

Compared to HMM, MEMM has an advantage in incorporating useful features into the sequence labeling procedure. However, MEMM suffers from the label bias problem [35]. This problem is serious if the hidden states have a small number of possible outgoing transitions. In other words, the hidden states would like to make light of their corresponding observations when they have low-entropy next-state distributions [35]. As shown in Fig. 2
                        , SNR in the chief complaints by MEMM is, unfortunately, a victim of the label bias problem. In order to cope with the label bias problem in MEMM, we employ CRF [35] which is an undirected graphical model. To avoid the label bias problem, CRF represents the conditional distribution over y 
                        globally conditioned on x. Therefore, Eq. (1) can be modified to:
                           
                              (4)
                              
                                 
                                    
                                       y
                                    
                                    
                                       ˆ
                                    
                                 
                                 =
                                 
                                    
                                       arg max
                                    
                                    
                                       y
                                    
                                 
                                 
                                    
                                       exp
                                       
                                          
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   
                                                      n
                                                   
                                                
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      k
                                                      =
                                                      1
                                                   
                                                   
                                                      K
                                                   
                                                
                                                
                                                   
                                                      λ
                                                   
                                                   
                                                      k
                                                   
                                                
                                                
                                                   
                                                      f
                                                   
                                                   
                                                      k
                                                   
                                                
                                                (
                                                
                                                   
                                                      y
                                                   
                                                   
                                                      i
                                                      -
                                                      1
                                                   
                                                
                                                ,
                                                
                                                   
                                                      y
                                                   
                                                   
                                                      i
                                                   
                                                
                                                ,
                                                
                                                   
                                                      y
                                                   
                                                   
                                                      i
                                                      +
                                                      1
                                                   
                                                
                                                ,
                                                x
                                                ,
                                                i
                                                )
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             TS
                                          
                                       
                                       exp
                                       
                                          
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   
                                                      n
                                                   
                                                
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      k
                                                      =
                                                      1
                                                   
                                                   
                                                      K
                                                   
                                                
                                                
                                                   
                                                      λ
                                                   
                                                   
                                                      k
                                                   
                                                
                                                
                                                   
                                                      f
                                                   
                                                   
                                                      k
                                                   
                                                
                                                (
                                                
                                                   
                                                      y
                                                   
                                                   
                                                      i
                                                      -
                                                      1
                                                   
                                                
                                                ,
                                                
                                                   
                                                      y
                                                   
                                                   
                                                      i
                                                   
                                                
                                                ,
                                                
                                                   
                                                      y
                                                   
                                                   
                                                      i
                                                      +
                                                      1
                                                   
                                                
                                                ,
                                                x
                                                ,
                                                i
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where fk
                         is an indicator feature function, k
                        ∊[1,
                        K], and K is the number of global features. Moreover, 
                           
                              
                                 
                                    λ
                                 
                                 
                                    k
                                 
                              
                           
                         is a weighting parameter of fk
                         that would be learned based on a training dataset. TS is the number of all possible tag sequences. In this paper, CRF is implemented by the CRF++ tool [36] for estimating the weights, and the default settings are used.

Commonly used features are words and other higher level syntactic features. However, in Section 2 we show that splitting the chief complaints into Chinese words, let alone extracting the higher level syntactic features form the chief complaints, is not a trivial task due to the distinctive characteristics of the chief complaints [13]. Therefore, referring to [29], two types of empirical features are utilized in this paper and explained below.

Contextual words are the obvious and informative features that can be utilized by MEMM and CRF for SNR in the chief complaints. However, as previously noted, splitting the chief complaints into Chinese words is not trivial. Fortunately, Chinese characters bear specific meanings and are viable alternatives to Chinese words as the features. Furthermore, because the content of the chief complaints recorded by TCM doctors is concise [13] and the average length of Chinese words approximates 2 [37], Chinese character unigrams, bigrams, and trigrams cover most of the meaningful words and some other useful features in the chief complaints, e.g. the prefixes and the suffixes of symptom names, etc. Therefore, we choose Chinese character unigrams, bigrams, and trigrams as the features used by MEMM and CRF in this paper, and they are denoted by the symbols “U”, “B”, and “T” respectively.

Empirically, TCM doctors prefer initially to record background information about the patients’ symptoms, e.g. the temporal information, and then note the patients’ symptom information concisely. This means that symptom names are not likely to appear at the beginning of the sentences in the chief complaints. Instead, they appear more frequently at the beginning of the subsequent clauses in the sentences. For example, the chief complaint “昨日肠鸣, 失气多, 心中不适.” (Yesterday, the patient had borborygmus and more flatulence, and his/her heart was uncomfortable) starts with a temporal background description (i.e. “昨日” (yesterday)), and then the following clauses in this chief complaint all start with the symptom names, i.e. “失气多” (more flatulence) and “心中不适” (his/her heart was uncomfortable). Therefore, the positions of the Chinese characters in the chief complaints are useful features for the SNR task. In this paper, the position features are represented as “[SubSID-PosID]”, where “SubSID” is the index of current clause in a chief complaint and “PosID” indicates the position of current Chinese character in the clause SubSID.

To evaluate the feasibility of the adapted sequence labeling strategy and the performance of the introduced sequence classifiers for SNR in the chief complaints, two groups of evaluation metrics are employed. The first group is symptom name recognition precision (P
                        rec), recall (R
                        rec), and F-Measure (FM
                        rec). The second group is labeling precision (P
                        lab), recall (R
                        lab), and F-Measure (FM
                        lab) of each predefined tag.


                           P
                           rec,
                           R
                           rec, and FM
                           rec are used to comprehensively assess the feasibility of the sequence labeling strategy with a domain-specific adaptation and appraise the performance of HMM, MEMM and CRF for the SNR task. They are formulated as follows.
                              
                                 (5)
                                 
                                    
                                       
                                          P
                                       
                                       
                                          rec
                                       
                                    
                                    =
                                    
                                       
                                          |
                                          NSRC
                                          |
                                       
                                       
                                          |
                                          NSR
                                          |
                                       
                                    
                                 
                              
                           
                           
                              
                                 (6)
                                 
                                    
                                       
                                          R
                                       
                                       
                                          rec
                                       
                                    
                                    =
                                    
                                       
                                          |
                                          NSRC
                                          |
                                       
                                       
                                          |
                                          NS
                                          |
                                       
                                    
                                 
                              
                           
                           
                              
                                 (7)
                                 
                                    
                                       
                                          FM
                                       
                                       
                                          rec
                                       
                                    
                                    =
                                    
                                       
                                          2
                                          ·
                                          
                                             
                                                P
                                             
                                             
                                                rec
                                             
                                          
                                          ·
                                          
                                             
                                                R
                                             
                                             
                                                rec
                                             
                                          
                                       
                                       
                                          
                                             
                                                P
                                             
                                             
                                                rec
                                             
                                          
                                          +
                                          
                                             
                                                R
                                             
                                             
                                                rec
                                             
                                          
                                       
                                    
                                 
                              
                           where |NSRC| is the number of symptom names recognized correctly, |NSR| is the number of symptom names recognized, and |NS| is the number of symptom names in the test dataset. A symptom name is correctly recognized, if and only if its beginning, intermediate and end positions are all accurately labeled.


                           P
                           lab,
                           R
                           lab, and FM
                           lab are designed for validating the feasibility of the sequence labeling strategy with a domain-specific adaptation and evaluating the performance of HMM, MEMM and CRF for the SNR task in detail. They are defined below.
                              
                                 (8)
                                 
                                    
                                       
                                          P
                                       
                                       
                                          lab
                                       
                                    
                                    =
                                    
                                       
                                          |
                                          NCLC
                                          |
                                       
                                       
                                          |
                                          NCL
                                          |
                                       
                                    
                                 
                              
                           
                           
                              
                                 (9)
                                 
                                    
                                       
                                          R
                                       
                                       
                                          lab
                                       
                                    
                                    =
                                    
                                       
                                          |
                                          NCL
                                          |
                                       
                                       
                                          |
                                          NC
                                          |
                                       
                                    
                                 
                              
                           
                           
                              
                                 (10)
                                 
                                    
                                       
                                          FM
                                       
                                       
                                          lab
                                       
                                    
                                    =
                                    
                                       
                                          2
                                          ·
                                          
                                             
                                                P
                                             
                                             
                                                lab
                                             
                                          
                                          ·
                                          
                                             
                                                R
                                             
                                             
                                                lab
                                             
                                          
                                       
                                       
                                          
                                             
                                                P
                                             
                                             
                                                lab
                                             
                                          
                                          +
                                          
                                             
                                                R
                                             
                                             
                                                lab
                                             
                                          
                                       
                                    
                                 
                              
                           where |NCLC| represents the number of Chinese characters in the test dataset that are correctly labeled with their corresponding tags, |NCL| is the number of Chinese characters labeled, and |NC| is the number of Chinese characters that should be labeled.

The gold standard experimental dataset of the chief complaints used in our experiments is from a TCM clinical record dataset, which was collected by TCM doctors during their routine diagnostic work from April 2006 to June 2008. The dataset contains 11,613 clinical records; an excerpt is shown in Fig. 3
                        . The gold standard experimental dataset is constructed through following steps:
                           
                              (1)
                              Two TCM experts are invited to independently annotate symptom names mentioned in the chief complaints according to an annotation guideline, which was developed jointly by the two annotators and the authors in advance (the detailed information of the annotation guideline is described in Appendix 1). The Inter-Annotator Agreement [38] on the annotating results reaches 0.84. This value lies in “the almost perfect agreement interval” [39] or in “the excellent agreement interval” [40].

In our experiments, only the symptom names agreed on by both annotators are treated as the gold standard symptom names. The Chinese characters in these symptom names are labeled with the predefined tags “B”, “I” and “E” when validating our method or “B” and “I” when performing comparative experiments. Incompatible symptom names in the annotating results are viewed in the same way as other general descriptions, and their corresponding Chinese characters are labeled with the predefined tag “O”.

For convenience, a further process is performed on the gold standard experimental dataset. Every number, e.g. integers, decimals, and fractions, etc., in the dataset is uniformly replaced by an English character “N”. Moreover, the punctuations are all replaced by the English character “P”. These “N”s and “P”s will be all labeled with the predefined tag “O”.

Finally, the gold standard experimental dataset is randomly divided into two parts. One contains 3483 chief complaints (about 30% of the original dataset), which is to be treated as the training dataset. The remaining 8130 chief complaints form the test dataset. Detailed information of the training and test datasets is listed in Table 1
                        .

@&#EXPERIMENTAL RESULTS@&#

A comprehensive evaluation of the feasibility of the adapted sequence labeling strategy and the performance of the introduced classifiers for the SNR task are described below based on the symptom name recognition results.

The results in Figs. 7–9, compared with those in Figs. 4–6
                           
                           
                           , show that after making a domain-specific adaptation to the labeled sequence based on the characteristics of the chief complaints, the SNR results obtained by different sequence classifiers are all improved. The highest improvement of FM
                           rec reaches 3.39% and the average improvement of FM
                           rec rises by 1.03%. Specifically, comparing the results displayed in Figs. 8 and 9
                           
                           
                            to the results in Figs. 5 and 6, after defining the clauses as the labeled sequences, P
                           rec, R
                           rec, and FM
                           rec are all improved. At the same time, with increasing CWS P
                           rec, R
                           rec, and FM
                           rec rise and then remain stable (see Figs. 8 and 9) rather than improving and then worsening as shown in Figs. 5 and 6. This demonstrates the effectiveness of the domain-specific adaption to the labeled sequence. As introduced in Section 2.2.2, this adaptation keeps the internal coherence of every clause and, at the same time, reduces the possibility of encountering incoherent sentences. This is similar to empirical feature selection, i.e. empirically reducing noisy features extracted from the disorganized context in the chief complaints.

Comparing the results depicted in Figs. 10–12
                           
                           
                            with the results shown in Figs. 4–6 reveals that after individually identifying the end of the symptom names, the SNR results (P
                           rec, R
                           rec, and FM
                           rec) obtained by different sequence classifiers are all improved. The highest improvement of FM
                           rec reaches 4.34% and the average improvement of FM
                           rec rises by 1.65%. These results demonstrate that the domain-specific adaptation to the predefined tag set is effective and that the strategy of individually identifying the end of the symptom names is appropriate. More detailed analysis of the contribution of the domain-specific adaptation to the predefined tag set is described in Section 5.2.2.

Comparing the results revealed in Figs. 13–15
                           
                           
                            to the results depicted in Figs. 4–6, vividly shows that after adapting the sequence labeling strategy based on the domain-specific characteristics of the chief complaints, the SNR results obtained by different sequence classifiers are all improved. The highest and the average improvements of FM
                           rec reach 6.18% and 3.64% respectively; the best FM
                           rec reaches 95.12% (P
                           rec 94.77% and R
                           rec 95.48%). This is achieved by CRF with the features U, B, T, and P which are fetched in CWS=2 or 3 (see Fig. 9). This result is also obtained when the general sequence labeling strategy is made a domain-specific adaptation. Furthermore, compared to the results in Figs. 7–12
                           
                           
                           , the results shown in Figs. 13–15 are better. This demonstrates that every adaptation to the general sequence labeling strategy is helpful and they all make contributions to SNR in the chief complaints. In addition, after adaptation to the general sequence labeling strategy, not only P
                           rec but also R
                           rec of every sequence classifier is improved; the results do not suffer from adverse effects when CWS is increased. All these results demonstrate that the domain-specific adaptation of the general sequence labeling strategy based on the characteristics of the chief complaints is appropriate and effective.

Comparing the SNR results achieved by MEMM and CRF with the results obtained by HMM (see Figs. 4–15) shows that MEMM and CRF are better than HMM. This demonstrates that the discriminative models (i.e. MEMM and CRF) outperform the generative model (i.e. HMM), because they effectively incorporate useful features into the sequence labeling procedure, thus improving the SNR performance. At the same time, a comparison of SNR results obtained by CRF to the results achieved by MEMM clearly shows that CRF outperforms MEMM, proving that CRF effectively avoids the label bias problem from which MEMM suffers.

In the previous section, the effectiveness of the domain-specific adapted sequence labeling strategy for the SNR task has been demonstrated through a comprehensive evaluation of SNR results, and the performance of different sequence classifiers for the SNR task is also described. In this section, we will give a detailed analysis of the adaption to the general sequence labeling strategy through evaluating the detailed sequence labeling results. In order to conveniently show comparisons, we will make a distinction between the tag “I” appearing at the end of the symptom names, which will be denoted as “I/E”, and the tag “I” which occurs at the intermediate of the symptom names.

Comparing the results shown in Figs. 19–21
                           
                           
                            to the results revealed in Figs. 16–18 shows that after defining the clauses as the labeled sequences the labeling results (P
                           lab, R
                           lab, and FM
                           lab) of the tags “B”, “I”, “I / E”, and “O” are all improved. In addition, Figs. 19–21 also show that the improvements of the labeling results are mainly due to the contribution of the substantial increase of R
                           lab of each type of the predefined tags. Moreover, Figs. 20 and 21 show that after the adaptation to the labeled sequence, P
                           lab, R
                           lab, and FM
                           lab of each type of the predefined tags do not suffer from adverse effect due to increasing CWS. These results further demonstrate the effectiveness of the domain-specific adaptation to the labeled sequence.

A comparison of the results revealed in Figs. 22–24
                           
                           
                            to the results depicted in Figs. 16–18 clearly show that the labeling results (not only FM
                           lab but also P
                           lab and R
                           lab) of the end of the symptom names are improved dramatically. At the same time, the labeling results of the other three predefined tags are also improved. These results have demonstrated that the adaptation to the predefined tag set described in this paper can effectively boost the labeling performance of the end of the symptom names, thereby indirectly improving the labeling performance of the other predefined tags.

Detailed sequence labeling results obtained by HMM, MEMM, and CRF, after the domain-specific adaptation to the sequence labeling strategy, are shown in Figs. 25–27
                           
                           
                            respectively. The labeling of all predefined tags show significant improvements when compared to the results in Figs. 16–18, with an average increase of 1.80%. In addition, the labeling results of the predefined tag “E” show dramatic improvements without suffering any adverse effect from increasing CWS (see Figs. 26 and 27). All these results demonstrate that the domain-specific adaptation to the general sequence labeling strategy based on the characteristics of the chief complaints of TCM is appropriate and effective.

The comparison of sequence labeling results obtained by MEMM and CRF to the results achieved by HMM (see Figs. 16–27) clearly shows that MEMM and CRF with useful features can achieve higher labeling performance than HMM. These results further demonstrate that the discriminative models (MEMM and CRF) are more suitable to the SNR task than the generative model (HMM), and that they can effectively incorporate useful features into the sequence labeling procedure for SNR in the chief complaints. In addition, under the same training conditions the labeling results of CRF are better than MEMM. This improvement is due to effectively avoiding the label bias problem from which MEMM suffers.


                           Figs. 25–27 show that FM
                           lab of the predefined tag “I” is always lower than FM
                           lab of “B”, “I” and “E”. This reflects the fact that symptom names in chief complaints can be detected accurately, but that the exact boundaries of the symptom names are still difficult to identify. This result is also shown in NER in English discharge summaries of Western medicine [15]. Moreover, P
                           lab of the predefined tag “O” is always higher than its R
                           lab, but the results of “B”, “I”, and “E” are always lower. This reflects the fact that, in addition to HMM, MEMM and CRF also prefer to label Chinese characters with the “B”, “I”, and “E” rather than “O”. Accordingly, in future work on improving sequence labeling results we should focus on the problems of improving the labeling performance of the intermediate of the symptom names and modifying the sequence classifiers to more fairly treat every type of tag in the labeling procedure.

In this section, the confusion matrix [41] is used to analyze errors in sequence labeling results. In an N-by-N confusion matrix, the cell (y,
                        y′) indicates the ratio of the number of gold standard Chinese characters with y to the number of incorrectly labeled characters with y′. This ratio can be used to indicate which type of labeling error occurs frequently in the sequence labeling procedure. The confusion matrices obtained by HMM, MEMM and CRF after adapting the general sequence labeling strategy based on the domain characteristics of the chief complaints for the SNR task are listed in Appendices 2–4.


                        Appendices 2–4 show that Chinese characters whose exact corresponding tags are “B”, “I”, and “E” are frequently mislabeled with the tag “O”. By analyzing the labeling results, we see that this frequently occurs when a symptom name is an aggregation of several symptom names. For convenience, TCM doctors write such aggregations when there is a common prefix or suffix, provided that the aggregated symptom name does not change the meanings of the original symptom names. For example, to avoid the redundant effort, TCM doctors will aggregate the symptom names “痰多” (sputum is excessive), “痰稠” (sputum is thick) and “痰黏” (sputum is sticky) by letting them share their common prefix “痰” (sputum) to form a concise symptom name “痰多稠黏” (the sputum is excessive, thick and sticky), and the new aggregated symptom name would also express the meanings of the original symptom names. Similarly, “腕腹胀” (gastric and abdominal distension) is an aggregated symptom name of the symptom names “腕胀” (gastric distension) and “腹胀” (abdominal distension), both of which have a common suffix “胀” (distension). Unlike aggregated descriptions in general text or in English discharge summaries, chief complaints usually do not include conjunctions such as “和” (and) or “或” (or), before the last shared part of the aggregated symptom names. This results in the loss of an important feature for identifying the beginning or the end of aggregated symptom names. In future work, the domain-specific syntactic structure of the chief complaints could be explored. Domain-specific syntactic features can then be imported by the discriminative sequence classifiers into the SNR task, thereby helping to accurately recognize aggregated symptom names in chief complaints.


                        Appendices 2–4 also reveal that Chinese characters, whose exact corresponding tags are “O”, are usually mislabeled with tags “I” and “B”. These mistakes usually occur when symptom names include dispensable modifiers. TCM doctors have different opinions on whether to include or exclude dispensable modifiers in symptom names. For example, the Chinese character “有” (having), which is used to convey the meaning that “a patient has some symptoms”, in the symptom name “有肠鸣” (has borborygmus) could be appended by several TCM doctors. This problem can be boiled down to the symptom name normalization problem and solved by normalization methods [28]. In future, we can attempt to model SNR and symptom name normalization jointly in order to further improve sequence labeling performance.

@&#DISCUSSION@&#

With the rapid development of natural language processing, data mining, and machine learning techniques, many new state-of-the-art methods have been developed for the NER task. However, due to the distinctive characteristics of the FCRs of TCM, these general methods need domain-specific adaptation for NER in FCRs of TCM. In this paper, we study symptom name recognition in chief complaints. This is a fundamental task for other clinical research of TCM [8], such as automatic clinical diagnosis, discovering clinical knowledge from large sets of FCRs of TCM, and constructing TCM clinical expert systems. We encourage TCM researchers to pay more attention to this fundamental task.

Research on NER in FCRs of TCM is at an early stage; many practical problems remain to be solved. As shown in Section 4, we found that there are still some problems which need to be solved, for example, improving the recognition of aggregated symptom names in the chief complaints or designing an appropriately sequence classifier that could jointly model SNR and symptom name normalization. Exploration of the syntactic structures of FCRs of TCM is also a research area that may yield features useful for sequence classifiers. Our results also demonstrated that future domain-specific methods for NER in FCRs of TCM should be designed through the incorporation of more domain knowledge.

In comparing the recognition of concepts and medical information in English discharge summaries of Western medicine reported in [14,15] to the results shown in this paper, we find that the highest FM
                     rec (95.12%) of SNR in chief complaints reported in this paper is much higher than the best FM
                     rec (85.70% described in [14] and 85.20% reported in [15]) of NER in English discharge summaries of Western medicine. By comparing these two tasks and analyzing the detailed results obtained by one team (first place winners of the 2009 i2b2 medical information extraction task [16]), we find two reasons for these differences. First, unlike the naming of symptoms in English, TCM symptom names have domain-specific characteristics that usually comprise three aspects of descriptions (body location, sensation and intensity), and words used in TCM symptom names tend to have different distributions from words used in other contexts. These characteristics facilitate the SNR task as performed by sequence classifiers. Second, unlike work on NER in English discharge summaries of Western medicine, [14,15] we only recognize symptom names which eases the burden of the sequence classifiers. Several other entities remain be considered, such as treatments, temporal information, and drugs. The more entities taken into account, the greater the challenges. For example, Patrick and Min showed that FM
                     rec of the medication entities in English discharge summaries can reach 91.40%, however FM
                     rec of the reason entities only reaches 55.52% under the same experimental settings [16]. Recognition of multiple types of named entities will be needed to further exploit the FCRs of TCM.

@&#CONCLUSIONS@&#

SNR in chief complaints is an essential and fundamental task for exploiting the content of FCRs of TCM, discovering valuable clinical knowledge of TCM, and constructing clinical expert systems for TCM. It provides an opportunity to effectively and efficiently make use of an abundant clinical knowledge source – FCRs of TCM – and, consequently, further assist TCM modernization. In this paper, SNR in chief complaints is reasonably treated as a sequence labeling problem. According to the domain-specific characteristics of FCRs of TCM, the general sequence labeling strategy is appropriately adapted for the SNR task for several empirical reasons. Moreover, three typically supervised classifiers are investigated, and their specializations for the SNR task are interpreted carefully. A series of elaborate experiments were performed. The results demonstrate that the domain-specific adaptation of sequence labeling strategies is appropriate and effective, and that CRF outperforms HMM and MEMM for the SNR task.

@&#ACKNOWLEDGMENTS@&#

We would like to thank Ph.D. Ju Wang, M.S. Juan Zhao, M.S. Xuehong Zhang and M.S. Shengrong Zou for their helpful suggestions on this work and for their valuable work on manually structuring and annotating clinical records for us. The authors are also pleased to acknowledge Dr. James J. Cimino, Mr. Daniel O’Sullivan, Ms. Sky Y. Chen and Ms. Cathy F. Yu for the help in revising the paper. From all above, we are particularly thankful to Dr. James J. Cimino for taking the time out of his busy schedule to give us the constructive comments and helpful suggestions.


                  Funding: This work was supported by NSFC under Grants No. 61173182 and 61179071, as well as by support from Sichuan Province under Grant Nos. 2012HH0004, 2012HH0031 and 2008SZ0049 and Zhejiang Province under Grant No. LY12F02010.

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.jbi.2013.09.008.


                     
                        
                           Supplementary data A1
                           
                              The symptom name annotation guideline.
                           
                           
                        
                     
                     
                        
                           Supplementary data A2
                           
                              Confusion matrixes obtained by HMMs based on the domain-specific adapted sequence labeling strategy.
                           
                           
                        
                     
                     
                        
                           Supplementary data A3
                           
                              Confusion matrixes obtained by MEMMs based on the domain-specific adapted sequence labeling strategy.
                           
                           
                        
                     
                     
                        
                           Supplementary data A4
                           
                              Confusion matrixes obtained by CRFs based on the domain-specific adapted sequence labeling strategy.
                           
                           
                        
                     
                  

@&#REFERENCES@&#

