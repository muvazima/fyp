@&#MAIN-TITLE@&#Integrated concept blending with vector space models

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           A method for merging an arbitrary number of nouns, mixing their meanings.


                        
                        
                           
                           Successful model using vector representation, scantily explored for concept retrieval.


                        
                        
                           
                           Experiments with 3 semantic space models: WN, a thesaurus, and a topic based model.


                        
                        
                           
                           Good performance with an automatically obtained resource comparable to a manual one.


                        
                        
                           
                           Evaluation by qualified reviewers and comparison with a traditional dictionary.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Computational linguistics

Natural language processing

Lexicography

Vector space models

Reverse lookup dictionaries

Concept-blending

@&#ABSTRACT@&#


               
               
                  Traditional concept retrieval is based on usual word definition dictionaries with simple performance: they just map words to their definitions. This approach is mostly helpful for readers and language students, but writers sometimes need to find a word that encompasses a set of ideas that they have in mind. For this task, inverse dictionaries are ready to help; however, in some cases a sought word does not correspond to a single definition but to a composite meaning of several concepts. A language producer then tends to require a concept search that starts with a group of words or a series of related terms, looking for a target word. This paper aims to assist on this task by presenting a new approach for concept blending through the development of a search-by-concept method based on vector space representation using semantic analysis and statistical natural language processing techniques. Words are represented as numeric vectors based on different semantic similarity measures and probabilistic measures; the semantic properties of a word are captured in the vector elements determined by a given linguistic context. Three different sources are used as context for word vector construction: WordNet, a distributional thesaurus, and the Latent Dirichlet Allocation algorithm; each source is used for building a different semantic vector space.
                  The concept-blender input is then conformed by a set of n-nouns. All input members are read and substituted by their corresponding vectors. Then, a semantic space analysis including a filtering and ranking process is carried out to deploy a list of target words. A test set of 50 concepts was created in order to evaluate the system's performance. A group of 30 evaluators found our integrated concept blending model to provide better results for finding an adequate word for the provided set of concepts.
               
            

@&#INTRODUCTION@&#

Generally, traditional dictionaries are designed with readers in mind. In this scenario, a query is based on looking up single words in order to get their corresponding meanings; this is not always helpful when viewed from language producer's perspective. For them –speakers, writers, etc.– necessities are different: they may have an idea composed by several meanings or concepts, and their goal is to find the best word that is able to represent their thoughts. Asmore dictionaries in electronic format are available, searching by concept is readily at hand; but, as we will detail in Section 2, current implementations have limitations due to the presence of syntactic problems, query expansion issues, and gloss reliance. The implementation of vector-space word representations enables circumventing most of these well-known problems through their capacity to capture syntactic and semantic regularities in language (Mikolov et al., 2013). Also, working with continuous space models permits a distributed representation with good levels of generalization; moreover, semantic vector spaces have the characteristic that similar words tend to have similar vectors.

This paper presents a new approach for concept retrieval, based on vector space representation constructed with semantic similarity measures and statistical NLP techniques. A sought concept is expressed as an input of n nouns. We propose representing each noun as a numeric vector in order to allocate them in a so-called semantic space of m dimensions; these dimensions correspond to a predefined set of concepts, words or topics. Then, the value of each element in the noun's vector is given by its relation (similarity measure, or probabilistic distribution with regard to a topic) with those reference concepts, words or topics. Given the input set of points represented in the m-dimensional vector space, we find an equidistant new point, from which a sample of the nearest neighbors is taken. The words included in this sample should semantically mix the characteristics described by the original entries, representing the target words from the reverse lookup process of our concept blending method.

The semantic space is created from three different sources in order to evaluate different approaches: a supervised approach assisted by WordNet, and two unsupervised approaches using (1) a distributional thesaurus, and (2) the Latent Dirichlet Allocation (LDA) algorithm.

Once the concept blending method was ready, a test set was created to attest the performance of our proposal, and an evaluation procedure determined which one of the proposed models used for the semantic space creation was closer to human associative reasoning. Results were compared with an existing search-by-concept dictionary (OneLook), showing that our results were preferred, in the majority of cases, by master class evaluators. Additionally, we found a greater semantic association for the words provided by our method.

In Section 2 we present similar works in the state of the art; in Section 3 we present our method. In Section 4 our experiments and results can be found, and finally, our conclusions are drawn.

This work uses the representation of words as a vector. This is not a new idea, perhaps the earliest works on this were those of Hinton et al. (1986) and Hodgson (1991). In Section 2.1 we give a brief survey on this subject, while in Section 2.2 we present works related to concept retrieval.

Distributional analysis is based in structuralist linguistics (Harris, 1951), corpus linguistics (Firth, 1968), psychology (Miller and Charles, 1991), and it is based on the idea that not only the semantic properties of a lexical item are fully reflected in appropriate aspects of the relations it contracts with actual and potential contexts, but “there are good reasons for a principled limitation to linguistic contexts” Cruse (1986). Distributional hypothesis suggests that we can induce aspects of the meaning of words from their contexts; it is a “theory of meaning” that can be easily operationalized into a procedure to extract “meaning” from text corpora on a large scale.

Models that represent the meaning of words as vectors keeping track of the words’ distributional history focus on the notion of semantic similarity, measured with geometrical methods in the space inhabited by the distributional vectors. Examples of these models are LSA (Landauer and Dumais, 1997), HAL (Lund and Burgess, 1996), the work of Sahlgren (2006), Padó and Lapata (2007), and Baroni and Lenci (2010). We follow the principle of distributional semantics as models of word meaning following Landauer and Dumais (1997), and Turney et al. (2010).

Distributional semantics can model human similarity judgments, lexical priming (hospital primes doctor) synonymy (zenith-pinnacle), analogy (mason is to stone like carpenter is to wood), relation classification (exam-anxiety:CAUSE-EFFECT), etc. One of the earliest works is Hodgson's (1991). He found similar amounts of priming for different semantic relations between primes and targets (approx. 23 pairs per relation), for example: synonyms: to dread/to fear, antonyms: short/tall, coordinates: train/truck, super- and subordinate pairs: container/bottle, free association pairs: dove/peace, phrasal associates: vacant/building.

Distributional semantics in complex NLP systems include information retrieval within the broad topic of “semantic search”. Elsewhere, general-purpose distributional semantics models are not too common or too effective; lack of reliable, well-known out-of-the-box resources comparable to WordNet “Similarity” is too vague a notion for well-defined semantic needs. However, there are some successful attempts to use general-purpose distributional semantic information at least as supplementary resource in various domains, e.g. question answering (Tomás and Vicedo, 2007), bridging coreference resolution (Poesio et al., 2004; Versley, 2007), language modeling for speech recognition (Bellegarda, 1998), and textual entailment (Kotlerman et al., 2010).

Distributional semantics in the humanities, social sciences and cultural studies are a great potential, only partially explored, e.g., Sagi et al. (2009) use distributional semantics to study semantic broadening (dog from specific breed to “generic canine”) and narrowing (deer from “animal” to “deer”) in the history of English phonastemes (glance and gleam, growl and howl) and the parallel evolution of British and American literature over two centuries.

To our knowledge, the use of distributional semantics as a means to obtain a blended concept representation to aid language producers has not been particularly explored.

We could not find any works directly comparable with our concept-retrieval method because traditional tools return a concept based on a description, while our method merges meanings of several words in order to find a single one that represents their blending. Nevertheless, it is relevant to mention works related to the task of returning words given a search for a concept. In general, we find such tools under the name of reverse-lookup dictionaries.

As far as we know, there are only few printed reverse dictionaries for the English language. The reason for this is probably the complexity of their elaboration, especially the one of choosing the proper way to structure their information. The Bernstein's Reverse Dictionary (Bernstein and Wagner, 1975) was the first of its kind; in this book, the definitions of 13,390 words were reduced to their briefest form and then ordered alphabetically. In order to cover all routes to find a target word, some words have multiple references as the author re-orders the word sequence of the definitions in every possible form. However, the briefness in the definitions could be seen as a limitation, because this forced reduction may lead to information loss; albeit this is necessary because longer definitions might complicate the compiling work task.

Regarding electronic dictionaries, other complications arise; information's different orderings, organization of concept hierarchies, or varying entry structures attempts have been made in the implementation of the reverse lookup method seeking for the best performance. The first attempts on electronic reverse dictionary creation were based on Boolean operators, i.e., exact match systems. Such works receive a definition or a list of words to begin the reverse search; target words containing the exact form of the input were displayed as output; however, this scenario had few possibilities of occurrence, showing its limited performance. The Merriam Webster's Collegiate Dictionary in its first electronic versions included a reverse search option based on this procedure.

Another approach for reverse search was done in 1995 with the United States patent of Crawford et al. titled “Reverse electronic dictionary using synonyms to expand search capabilities” (Crawford et al., 1997). In this work, synonyms were used to expand search capabilities.

Reverse lookup dictionaries for languages other than English have been also constructed. For example, Bilac et al. (2004) for Japanese compares the input phrase and the definitions from a concept dictionary. Before doing the lookup process, they parse all dictionary definitions with a morphological analyzer in order to generate frequency files which reflect the term frequencies in each definition. The output consists of those words whose definitions have the highest similarity with the user input. There is also an attempt to expand dictionary definitions of a concept by adding definitions of its hypernyms. This is possible due to the characteristics of the concept dictionary used. And, following their basic principle, a direct checking for a match within the user input and the concept definition is done as another output option. All of these measures were used as part of the reverse search process but each one evaluated separately.

A different reverse lookup method was created in Dutoit and Nugues (2002). In this proposal, a lexical database of French words called ‘The Integral Dictionary’ (TID) acts as the main source for the reverse search operation. TID is a semantic network associated to a lexicon with a size comparable to WordNet (Miller, 1995), one of the most important lexical databases for English. The Integral Dictionary organizes words into a diversity of concepts, classified into categories being only used by this reverse search algorithm: classes and themes. Classes form a hierarchy and are annotated with their part-of-speech, and themes are concepts that can predicate the classes. As a graph of concepts, ontological concepts are the basic components of TID and each concept is annotated by a gloss of few words describing its content. TID also includes the implementation of different semantic lexical functions which allows the generation of word senses from another word sense given as an input.

Another proposal for reverse search tries to emulate the behavior of human mind (Zock and Bilac, 2004) assuming that knowing a word does not imply that a person is able to access it in time, regardless of having it stored in memory. People use various methods to start a search process in their mind; it could be words, concepts, partial descriptions, related terms, etc. Based on the notion of association that considers that every idea, concept or word is connected, people should have a highly connected conceptual-lexical network in their minds. As a result, any word or concept has the potential to evoke each other. A few years later, an improved system was detailed in Zock and Schwab (2008). Again, the main concern was finding a correct manner to index the dictionary in order to gain a quick and intuitive access to words. This system allows lexical access based on underspecified input through the creation of a corpus-based association matrix, which is in turn composed of target words and access keys. In detail, the association matrix consists of a lexical matrix with one axis containing all the words of the language representing the target words 
                           
                              
                                 t
                                 w
                              
                           
                        , and the other axis containing the access words 
                           
                              
                                 a
                                 w
                              
                           
                         representing the words or concepts capable and likely to evoke the target words.

The most recent reverse dictionary system we have knowledge of can be found in Shaw et al. (2013). It is called the Wordster Reverse Dictionary and was built with two considerations in mind: (a) the user input is unlikely to exactly match the definition of a dictionary word; and (b) the response time of an input query needs to be minimum in order to create an application capable of supporting on-line interaction. For them, the main challenge consists in solving a concept similarity problem in a model with concepts as single words.

We find in these aforementioned works that one of the common aspects is the usage of an electronic dictionary to build their databases upon, and the capacity of query expansion including different conceptually related terms (synonyms, antonyms, hypernyms and hyponyms) (Crawford et al., 1997; Bilac et al., 2004; Dutoit and Nugues, 2002; Zock and Schwab, 2008; Shaw et al., 2013). However, the reverse search done by Crawford et al. (1997), Bilac et al. (2004), Zock and Schwab (2008) and Shaw et al. (2013) at some point of their procedures perform a comparison between user's input phrase and dictionary's definition target words, looking for an exact matching, while Dutoit and Nugues (2002) based their reverse search on the highest similarity values measuring graph distances.

A detailed description of the methodological features contained on the systems mentioned above are shown in Table 1
                        , adding a column for the concept blending method (CBM) proposed in this work in order to show an initial review of some of its characteristics.

As shown in Table 1, great progress has been accomplished over the last years by following common methodologies (e.g., query expansion, use of glosses for concept retrieval), but other methods, such as vector representation havebeen scantily explored. In addition, our goal is not just to effect a reverse lookup, but blending several words into a single concept. We present details of our proposal in the next section.

Here we present a description of our proposed method. Roughly it consists in two stages: first, the creation of the semantic space with three different sources and then, the second stage consisting in the concept blending that is carried out in the previously created semantic space. We explain stage 1 in Section 3.1, and stage 2 in Section 3.2.

Semantic spaces are very important for successful concept blending in the proposed method. The vector-space word representation presents several advantages, such as the ability of capturing certain generalization for the input words due to their distributed representation; its capacity to capture semantic regularities in language, and the proximity in space given to similar words: Once words are represented in a vector space, it is possible to compare two words by measuring their proximity in this space. If this is fulfilled, then, in order to blend the meanings of several words, we would need only to find an equidistant point to each involved word in a vector space.

A semantic space is a way of representing words as vectors in an Euclidean space with axes determined by a given linguistic context. Three different sources were proposed for semantic space construction, having different linguistic contexts for word vectors. In this section we describe how vectors are constructed for words, based on the following sources:
                           
                              •
                              WordNet – a large lexical database of English.

Distributional thesaurus – a thesaurus generated automatically from a corpus by finding words occurring in similar contexts to each other.

Latent Dirichlet Allocation – a topic modeling algorithm.

With these three sources, the dimensionality of each semantic space can be fixed. In the following sections we will use semantic spaces of 25 dimensions.

Under the semantic space based on WordNet lies a semantic analysis of words using semantic similarity and relatedness measures to represent their vectors. When using this kind of measures on an ontology-structured resource such as WordNet, it is possible to calculate the semantic similarity or semantic relatedness only between words that belong to the same part of speech. Due to this part of speech restriction, only nouns are considered as word members of the semantic space; this is supported by the fact that concepts are expressed mostly as noun phrases (Sowa, 1984).

WordNet 3.0 includes 82,115 synsets where 117,798 nouns are distributed. WordNet describes a specific sense of a certain word as word#pos#sense where pos refers to the part of speech of the word and its sense is represented by a number.

Based on WordNet's hierarchical principle, its 25 top concepts were defined as semantic primes to represent the linguistic context that determines the dimensionality of the space. The top concepts with their specific senses are listed below. This is also the order given to the top concepts during vector representation of words mentioned further on. 
                              
                                 
                                    
                                    
                                    
                                    
                                    
                                       
                                          
                                             activity#n#1
                                          
                                          
                                             animal#n#1
                                          
                                          
                                             artifact#n#1
                                          
                                          
                                             attribute#n#2
                                          
                                       
                                       
                                          
                                             body#n#1
                                          
                                          
                                             cognition#n#1
                                          
                                          
                                             communication#n#2
                                          
                                          
                                             event#n#1
                                          
                                       
                                       
                                          
                                             feeling#n#1
                                          
                                          
                                             food#n#1
                                          
                                          
                                             group#n#1
                                          
                                          
                                             location#n#1
                                          
                                       
                                       
                                          
                                             motive#n#1
                                          
                                          
                                             natural_object#n#1
                                          
                                          
                                             natural_phenomenon#n#1
                                          
                                          
                                             human_being#n#1
                                          
                                       
                                       
                                          
                                             plant#n#2
                                          
                                          
                                             possession#n#2
                                          
                                          
                                             process#n#6
                                          
                                          
                                             quantity#n#1
                                          
                                       
                                       
                                          
                                             relation#n#1
                                          
                                          
                                             shape#n#2
                                          
                                          
                                             state#n#1
                                          
                                          
                                             substance#n#1
                                          
                                       
                                       
                                          
                                             time#n#5
                                          
                                          
                                          
                                          
                                       
                                    
                                 
                              
                           
                        

It is important to note that in this case the input to the algorithm should be given as senses, instead of words. In this case the user is presented with a list of senses for each word so that senses are manually disambiguated. For every WordNet noun, its vector was created by calculating the semantic similarity or relatedness value between the respective sense and each top concept. Three measures were considered: JCN, Lin and Lesk Patwardhan et al. (2003). The first two are similarity measures while the latter one is a relatedness measure. After reading and creating the vectors for every noun, the process ends. Details of this algorithm are given as Algorithm 1.
                              
                                 
                              
                           
                        

The process was repeated for each of the different measures mentioned above, resulting in a semantic space with JCN measured vectors, another with Lin measured vectors, and the last one with Lesk measured vectors. With the semantic spaces created, we normalized by column making the greatest value found equal to 1 and calculating the linear correspondence to the rest of values. For the Lin measure normalization was not necessary because the maximum value using the Lin semantic similarity measure is already 1.

Finally, word vectors having the following form were obtained (for the sake of simplicity, only the first 10 out of 25 values are shown): 
                              
                                 
                              
                           
                        

Having vectors in this form, the WordNet semantic space construction was completed and word vectors were ready to conduct the concept blending.

We used the publicly available distributional thesaurus published by Lin (1998). This resource lists the pairwise similarity between 5469 nouns and the top-200 most similar words for each one.

The semantic space construction based on this resource is different in comparison with that of WordNet: Two variants for vector representation of words were proposed, one that imitates the fixed nature of the top-concepts used for WordNet semantic space, and other that uses a dynamic distribution of topics. Regardless of the vector representation variant, the initial step in the semantic space construction is the vocabulary extraction from the pairwise similarity values database.

Regarding the first variant of vector representation, each word of the vocabulary was represented as a vector of 25 dimensions determined by words corresponding to the 25 top concepts in WordNet's graph (Miller, 1990). These topics were selected due to their character of semantic primes representing the most generic concepts and unique beginners of different hierarchies, aiming for an equilibrated distribution of vector's dimensions. However, in this case, the specific sense of each topic could not be considered due to the distributional thesaurus nature.

Each vector's value consists in the semantic similarity measured between the word being analyzed and each top concept. If there is no similarity value defined in the pairwise similarity database, the dimension value is zero. A word vector containing only zeros in all its dimensions was discarded from the semantic space. The remaining word vectors constituted the Thesaurus Semantic Space I (TSSI). The algorithm describing this first variant of vector representation is shown in Algorithm 2.
                              
                                 
                              
                           
                        

During the TSSI construction, a vocabulary loss (approximately 20% of the words was not found) was noticed due to the number of words with vectors full of zeros. The discarded words could have been good answers during testing, but this assumption depended totally on the input concept. In order to increase the number of words related to our input concept, we would need to generate topics dynamically. This is the second variant for the vector representation of words.

For every input concept in our method, a set of topics was generated with the most related terms of each noun included in the input concept. The set of topics represents the linguistic context determining the dimensionality of the space and received the name of dynamic topics. The dimensionality of the vectors depends on the number of nouns forming the input concept. For comparison purposes, we established that the dimensionality should be 25. So, for each input concept represented by n nouns, the highest number k satisfying the expression k * n ≤ 25 was calculated. Then, k became the number of the most related terms selected for each noun member of the input concept, getting at the end a set of topics related at least with one of the members of our input and discarding from the semantic space only those words with no relation at all, yielding a negligible loss of vocabulary that does not affect the quality of the concept blending results. In order to keep all spaces with the same dimensionality, the resulting space was completed with topics from TSSI. The algorithm corresponding to the dynamic topic generation is shown in Algorithm 3.
                              
                                 
                              
                           
                        

Once the dynamic topics were obtained, the TSSI construction algorithm was applied. The only difference is that top concepts were replaced by dynamic topics. Finally, word vectors were saved in the thesaurus semantic space II (TSSII).

For example, suppose the following input concept: “ball field stick sport”, n = 4 ; k = 6. For each noun member of the input, their six most similar terms are extracted from the Thesaurus DB being the following:


                           
                              
                                 
                              
                           
                        

The union of all similar terms would represent the linguistic context determining the dimensionality of the semantic space subsequently constructed. In the case that the extracted terms are shared between input nouns, only one is taken into account; then, as described above, the resulting space was completed with topics from TSSI.

A normalization procedure was not necessary due to the properties of the Lin's semantic similarity measure. With the semantic space construction completed, word vectors were ready to continue with the concept blending.

In order to generate word vectors using LDA, it was necessary to obtain an LDA model from a corpus. The corpus selected for this task was the Wikipedia corpus snapshot as of November 2013, which includes 4,105,489 articles in English and a vocabulary of 7,423,153 words. Wikipedia is a multilingual, web-based, free-content encyclopedia that covers a wide variety of topics making it an extraordinarily large corpus with broad scope.

During corpus processing we noticed that a large number of terms appeared just a few times in all articles (for example, “AAABBNNNNN” appearing once). Such terms would not give relevant information during word vector representation; so, words appearing less than 5 times in the corpus were removed. After removing stopwords and words appearing less than 5 times, the vocabulary of the corpus was reduced to 1,680,882 words.

The Daichi Mochihashi LDA package Mochihashi (2004) was used to process the Wikipedia corpus with each document being a Wikipedia article. With the Wikipedia corpus ready, the remaining parameter was the number of latent topics T to be assumed in the data. An assumption of 25 topics was made following traditional selection choices Blei et al. (2003).

After implementing LDA, two outputs were generated:
                              
                                 •
                                 
                                    α – T-dimensional row vector representing the parameter of prior Dirichlet distribution over the latent topics.


                                    β – [V,T]-dimensional matrix representing the set of words for each latent topic where V is the size of the vocabulary.

Analyzing LDA's output, each cell in the β matrix indicates the probability of a specific word 
                              
                                 
                                    v
                                    i
                                 
                              
                            for each one of the topics automatically generated by LDA, where 
                              
                                 
                                    v
                                    i
                                 
                                 ∈
                                 V
                              
                           . Thus, the rows from β matrix could be seen as word vectors of 25 elements as in a traditional word context matrix. Those word vectors constituted the LDA semantic space in which the linguistic context determining the axes of the space was represented by the automatically generated unlabeled topics, since LDA is an unsupervised method.

This section explains the procedure used to obtain a list of target words given an input concept. The method's input consists of a concept formed by n nouns; then the method looks for their respective vectors in the semantic space previously selected by the user and calculates the average vector resulting from the input words, giving as a result a new vector that should be located in the semantic space representing a word combining the semantics of the nouns of the input concept. However, getting an average vector located exactly over an existing word in the semantic space is highly unlikely; so, a sample of the N-nearest neighbors is taken (N may vary depending on the Euclidean distance parameter.)

According to the semantic space source, different parameters were considered for the selection of the N-nearest neighbors. The parameters were empirically found as part of our experiments, to improve the output quality by taking advantage of the semantic space's properties.

The following considerations were made during this stage:
                           
                              •
                              For the semantic space based on WordNet we used the Euclidean distance between vectors. Then, the product of the pairwise semantic similarity values between each noun member of the input concept and the candidate words is calculated.

Candidate words with a defined pairwise similarity in the distributional thesaurus DB for all nouns in the input concept have priority over other candidates.

For the semantic space based on LDA, we used the Euclidean distance between the average vector and the words existing in the semantic space.

@&#EXPERIMENTS AND RESULTS@&#

In this section we describe the experiments performed following the methodology described in Section 3, along with the obtained results for each one of the proposed semantic spaces. Additionally, a proposal to determine which model is closer to human associative reasoning is explained, and the results of an existing electronic reverse dictionary are taken into account for comparison purposes.

@&#EXPERIMENTS@&#

In order to carry out experiments on the concept blending method proposed in this work, we asked a group of 30 graduate students of literature from the National Autonomous University of Mexico. We asked each of them to provide 5 examples of composite concepts (each described with three or four words,
                           1
                        
                        
                           1
                           Note, however, that our method allows input concepts consisting of any number of nouns.
                         following Dutoit and Nugues (2002), Zock and Schwab (2008), Shaw et al. (2013)) for which they would like to find a single word. From this set of 150 concepts, 50 were chosen, considering the ones which contained the most frequent words in average.

The complete test set is shown in Table 2
                        . Note that for WordNet it was necessary to additionally specify the part of speech (always noun) and sense (manually selected) of each concept's noun.

Word vectors based on WordNet had three different types: JCN, Lin and Lesk. To determine which semantic similarity (JCN and Lin) or relatedness Lesk measure had the best quality output, the complete test set was implemented on each semantic space. To describe the concept blending process carried out for each input concept, an example for the JCN semantic space is shown (note that only the first 9 values (out of 25) are shown here): 
                              
                                 
                              
                           
                        

With this information, we can extract the t highest ranked output words. In this case, 7 are shown in Table 3
                           , where the most relevant result was meet#n#1. Note the proximity of this word's vector values with the ones of the previously calculated average vector. 
                              
                                 
                              
                           
                        


                           Table 3 shows that the weight given to the parameter represented by the product of semantic similarity values is a very important factor for the selection of output words. Even words with lower Euclidean distances from the average vector are displaced due to the impact of the product of semantic similarity. This allows discarding words that could be located near to the average vector but having no close relation with the input concept, thus becoming an irrelevant candidate. We empirically determined a threshold for each measure with experiments in a previous stage with three groups of evaluators. Each group of ten evaluators was presented with results of different thresholds. The best results were found when using the JCN measure, with a threshold of less than 0.1; for Lin, less than 0.8; and for Lesk, less than 0.1. We noticed that vectors with Euclidean distances greater than the values mentioned above tend to represent words having no relationship with the input concepts.

Then, the experiment mentioned above was repeated for each semantic space with the final group of evaluators for every input concept included in the test set. Table 4
                            shows the concept blending of three different concepts with the two highest ranked output words from each semantic space. While working with WordNet it was possible to include also the gloss of each word providing additional information. Additionally, it can be seen that in some cases output words may be shared between source measures of vector creation, especially between the JCN and the Lin measure. This may have its origins in their respective formulas of semantic similarity, since the Lin measure uses the same elements of the JCN measure. This was a constant behavior through the test set results.

Results from each semantic space seem to be correct answers for each concept; however, it was necessary to determine which semantic similarity or relatedness measure implemented for word vector creation had the best performance in concept blending. To accomplish this, complete results of each semantic space were evaluated as detailed in Méndez et al. (2013). Roughly, this consisted on running another task in mechanical turk with ten evaluators using the evaluation set of 50 concepts described in Section 4.2 in order to determine which semantic similarity or relatedness measure implemented for word vector creation had the best performance in concept blending. The conclusion was that the output words with JCN-measured vectors were the best for combining the semantics of the input concepts. Following this, we used JCN-measured vectors for the construction of the WordNet semantic space.

The distributional thesaurus was the second source for semantic space construction with two variants, TSSI and TSSII. In the case of TSSI we noticed that approximately one third of words were not found because their corresponding vectors were filled with zeros; the reason for this was the linguistic context defining the dimensionality of the space represented by WordNet top concepts. Unlike WordNet's graph, where every noun is interconnected with generic concepts ensuring the existence of a semantic similarity value between them, in a distributional thesaurus the semantic similarity depends on the features shared between words; thus, the existence of a feature between a generic concept (top concept) and another word was not as common as expected, yielding vectors with zeros in all of dimensions approximately for each 3 out of 10 words. Because of this limitations, TSSI results are not presented in detail (see Section 3.1.2).

For TSSII, we proceeded as follows. In order to obtain the vectors of each noun of the input concept, it was necessary to determine the dimensionality of the space for the dynamic topics. In this case: n = 3 and k = 8. So, for each noun member of the input, the eight most similar terms were extracted from the Thesarus DB. For example, for the input concept ”motor, wheel, driver” the dynamic topics are: 
                              
                                 
                              
                           
                           
                              
                                 
                              
                           
                        

The union of all similar terms conformed the dimensionality of the semantic space (24 dimensions) and completed to 25 by using the first dimension of TSSI. Having that, word vectors could be calculated having values consisting in the semantic similarity measured between the word being analyzed and each dynamic topic. Once the semantic space is complete, the concept blending started by calculating the vectors of each input noun (Only the first values, out of 25, are displayed here.) 
                              
                                 
                              
                           
                        

Using a similar procedure to the one described in the previous section, we determined a threshold for the Euclidean distance between vectors (for TSSII) was 0.5. Once the semantic space analysis was completed, the blending method displayed the list of seven words shown in Table 5
                            ranked in descending order.


                           Table 5 shows the results obtained from the second variant of the distributional thesaurus. We can see that the product of pairwise similarity between the target word and each input noun is important for ranking; however, the determining factor in the selection of the target words is the existence of a pairwise similarity between the candidate word and each input noun. In this experiment, all target words had pairwise similarity defined with all input nouns in the Thesaurus DB. If target words with complete pairwise similarity do not fill the top-t output, then, a back-off is carried out until the output was complete. For example, for an input concept of 3 words with a t = 10: if only 7 words were found with a pairwise similarity with all 3 concept words, then we use the next words that have a pairwise similarity with 2 of them. If the desired t is not reached, then we consider words having a pairwise similarity with at least one of the words.

The concept blendings of six different concepts with the three highest ranked output words using TSSI and TSSII are shown in Table 6
                           . Note how output words tend to be common to both variants, but in several cases TSSII included new words that improved results.

By using TSSII, the sparseness in word vectors was decreased considerably due to the use of dynamic topics (from a matrix with 80% of zeros to 27%). Also, although depending on the same parameters as TSSI, new vocabulary in the list of output words for TSSII was present. These new words contributed to the output of our method.

LDA was the third source for semantic space construction proposed in this work. Words of this semantic space had vectors of 25 elements representing 25 latent topics for the Wikipedia corpus. Each vector element stores the probability of a word being generated by a latent topic. Both the topics and the probabilities were determined automatically by LDA.

To describe the concept blending process carried out with LDA semantic spaces, an example for one input concept is explained (only the first values out of 25 are shown): 
                              
                                 
                              
                           
                        

After the semantic space analysis using t = 6, the method displayed the seven words ranked in descending order shown in Table 7
                           . Notice that for LDA semantic space the unique parameter taken into account for output selection was the Euclidean distance of every word vector against the average vector of input nouns.

The complete test set was evaluated for concept blending. As we will see in next section, using a LDA semantic space yielded a broad level of semantic association: while for some input concepts the resulting output words tended to precisely mix the input nouns’ meaning, for others the quality of output words was graded as poor by our evaluators. Table 8
                            shows the three highest ranked output words for six different concepts.

Once we have an output for each source of semantic space using the same test set, we will proceed to use these results for the general evaluation, final part of this work.

One of the goals of this work is to determine which model of concept blend is closer to human associative reasoning; in this section we report such findings. First, it was necessary to find a way to measure the quality of the obtained results. For this purpose, results containing the three highest ranked output words of the proposed concept blending method were reunited in one document. Second, comparing against existing implementations was also important to define the significance of this work regarding the state of the art. There are two publicly accessible online sites that include systems allowing this kind of search: onelook.com and dictionary.com. Based on the state of the art Shaw et al. (2013), OneLook Reverse Dictionary was selected for comparison in terms of quality.

OneLook Reverse Dictionary lets the user describe a concept and get back a list of words and phrases related to that concept. This dictionary indexes hundreds of on-line dictionaries, encyclopedias, and other reference sites. Concerning the reverse lookup, it searches in their references for words that have definitions conceptually similar to the input concept (Beeferman, 2009).

Having the results from both our concept blending method and OneLook Reverse Dictionary, it was necessary to find qualified people to be in charge of the evaluation. Amazon Mechanical Turk supported this requirement. It is a marketplace for work that requires human intelligence by providing access to a virtual community of workers being able to choose from a variety of skill and capabilities of their workforce that fulfill requesters needs. 30 categorization master workers were selected to evaluate the quality of the results; the award of master qualification refers to a demonstration of accuracy in a type of Human Intelligence (HIT), in this case categorization.

The developed HIT aimed to measure the semantic association of output words, taking into account the three highest ranked outputs from each source of concept blending, as well as OneLook. Each input concept of the test set became a numbered item and the results of the search-by-concept dictionary and OneLook Reverse Dictionary were listed below in four different sets. The evaluators were asked to mark the degree of semantic association of each set of words with respect the input concept. The marks go from zero to three, meaning:
                           
                              •
                              0 – no semantic association

1 – weak semantic association

2 – medium semantic association

3 – strong semantic association

Results for the first 9 concepts (out of 50) shown in Table 9
                         give an idea about how the HIT created on Amazon Mechanical Turk looked like, with the difference that the HIT did not had a specific order of output words sets; the sets of words position was shuffled for every input concept in order to avoid possible biasing.

Having the evaluation data of the 30 workers from Amazon Mechanical Turk, we determined the performance of each semantic space source for every input concept by computing the mean of the scores given by the workers (values going from zero to three depending on the degree of semantic association). With this information, a comparison between the different sources of semantic space and the existing reverse dictionary was possible. For each input concept a comparison of the mean values was carried out by looking for the source that had the highest score. Table 10
                         shows the results of this comparison. Each row shows the percentage of times this resource had the highest score. For example, the row DT against the LDA column shows that 63.1% of the cases the highest score was assigned by DT; while the remaining 36.9 it was assigned by LDA. Note that both WordNet and the distributional thesaurus had a higher percentage of output words with a better degree of semantic association compared to Onelook Reverse Dictionary (RD), while LDA was the unique source of semantic space with a lower performance than OneLook.

To appreciate the distribution of semantic association degrees between the different sources evaluated in this work, the values of semantic association were associated to a specific degree considering that degrees of semantic association go from zero to three, with zero denoting null association and three denoting strong association. So, all values having a decimal part greater than or equal to 0.5 were rounded to their nearest unit. The distribution is summarized in Fig. 1
                        , showing for the three sources of semantic spaces for concept blending and the existing reverse dictionary, the amount of output words for each semantic association score.

The information shown in Fig. 1 indicates that the weak and medium degrees of semantic association concentrate most of the test set results. The strong degree of semantic association was scarcely achieved by the different sources of semantic space. WordNet obtained 4% while OneLook RD got a 6% and the distributional thesaurus and LDA got a 2% of the results. OneLook RD is the source with the highest percentage of output words (a 6%) with strong semantic association; however, it is also the source with the highest percentage of output words having a weak semantic association as indicated by its 46%. For the medium degree of semantic association, WordNet and the distributional thesaurus covered more than 50% of the output words reunited from the test set, with 58% for WordNet and 62% for the distributional thesaurus; meanwhile OneLook and LDA got a 42% and 20% respectively. A weak semantic association was achieved in 36% of WordNet output words, 26% for the distributional thesaurus and 45% for OneLook RD. LDA semantic space was the source of semantic space with the lowest performance having a 44% of weak semantic association and a 32% of null semantic association, covering almost the 80% of the test set output words within the poorest levels of association. The distributional thesaurus got 10% and OneLook RD 6% of the test set output words with null semantic association. WordNet was the unique source with no output words with null semantic association.

Evaluators’ agreement was calculated using Fleiss’ kappa. We obtained κ = 0.42, which represents a moderate agreement, but shows that evaluators had different opinions regarding concept associations.

Finally, the overall performance for each source of semantic space was computed taking into account all mean values. This calculation accomplished the ultimate specific goal proposed in this work, to determine which source of semantic space is closer to human associative reasoning. Table 11
                         shows the overall degree of semantic association achieved by each source of semantic space.

According to Table 11, the best sources for concept blending are WordNet and the distributional thesaurus almost indistinctly, given a significance level of 0.05. This suggest that both WordNet and the Distributional Thesaurus are closer to human associative reasoning. The overall performance value indicates a semantic association approximate to a medium degree, being also the case of OneLook RD. LDA's overall performance indicates a semantic association slightly over a weak degree. LDA was the source of semantic space that yielded the lowest results.

The poor performance of the LDA semantic space advised an additional revision to determine the possible causes. Word vectors of this semantic space had 25 elements, each one representing the word probability with a latent topic automatically discovered by LDA. However, despite the multiple dimensions, most of their values (72% of the table) were zeros, resulting in sparse vectors. This suggested that most of the unlabeled topics generated by LDA might not be suitable for representing a word feature, or at least not for the words conforming the proposed test set. After reviewing the topics contents, we found that many of them (10 topics) were related to countries and states; also, we noticed an important presence of topics related with religion (4 topics). This linguistic context might not be useful for representing the semantics of a word, and thus, vector elements represented by these topics were filled with zeros in most of cases. Being LDA an unsupervised algorithm, this effect was difficult to avoid unless a modified version of LDA was used.

@&#CONCLUSIONS@&#

The concept blending method described in this work demonstrated a good performance after being tested with input concepts covering a wide range of subjects. Our proposal consisted of using semantic spaces for this purpose. The semantic properties of a word were captured in the vector elements determined by a given linguistic context. We experimented with three different sources for the semantic space construction: WordNet, a distributional thesaurus, and LDA. Our method showed, with two semantic spaces – WordNet and the distributional thesaurus – a better performance compared with existing implementations of reverse dictionaries used as concept blenders.

Evaluator's agreement showed that evaluators did not find an straightforward task, as some of them could grade the same output as weak semantic association, while others can grade it as medium. However, the kappa agreement value of evaluators is not unusual for tasks involving semantic similarity ratings (cf 
                     Pakhomov et al., 2011; Cramer, 2008). Evaluators were not previously trained on what to consider a weak or strong semantic association. Although a master qualification was required, a further refined qualification, such as linguistics knowledge was not specified.

The analysis of the evaluation data revealed that semantic similarity measures performed well when used as source for word vector creation. Word vectors from WordNet semantic space were created using the JCN semantic similarity measure, while word vectors from the distributional thesaurus semantic space were created using Lin's semantic similarity measure. Also, distributed representations based on WordNet top concepts achieved good levels of generalization, capturing word's semantic features properly.

On the other hand, the LDA semantic space presented poor results, mainly because of the automatic generation of topics, having low-informative topics generating zeros inside word vectors in most of the cases. Many topics related with countries, states and religion were detected. The evaluation data revealed that this linguistic context was not useful enough for representing the semantics of a word. Despite having considered a broad and relatively big corpus for creating the LDA semantic space, the WordNet vectors were denser than the vectors obtained with this model. Further experiments with the LDA semantic space should be made before generalizing that this model is not useful for this task.

Another aspect to highlight was the good performance of the semantic space analysis proposed for concept blending, which in most cases seemed to have merged properly the characteristics of the words forming the input nouns. So, the main conclusion of this work is that vector space word representation gives promising results for concept blending.

@&#FUTURE WORK@&#

Given a phrase describing a desired concept of idea, a reverse dictionary provides target words whose definitions match the entered phrase. In the concept blending method developed in this work, input phrases considered only nouns. The first future work proposal is to allow input phrases including words of any part of speech.

The WordNet semantic space used the JCN semantic similarity measure for word vector creation. The evaluation data showed that WordNet was the best resource for concept blending, followed by the distributional thesaurus, which included word vectors based on Lin's semantic similarity measure. Another future work proposal is to experiment with a distributional thesaurus based on other semantic similarity measures, such as JCN, in order to improve its semantic space performance. Also, we plan to experiment with different corpora for creating distributional thesauri with an increased vocabulary coverage.

Different sources for building the LDA model should be explored, particularly given the apparent topic mismatch between the training corpus (Wikipedia) and the test set that was built aiming to help writers’ concept selection. While the Wikipedia vocabulary is broad, it might tend to reflect technical relationships between words. Also, we could explore building particular LDA models for each subset of concepts to blend, expecting to have an increase in performance similar to that of TSSII vs. TSSI.

Finally, we plan to continue this development in order to explore more ways to expand the search area of the traditional definition-based approach.

@&#ACKNOWLEDGEMENTS@&#

Work done under support of CONACyT-SNI, SIP-IPN, COFAA-IPN, and BEIFI-IPN. The authors would like to thank the anonymous reviewers for their useful comments and discussion.

@&#REFERENCES@&#

